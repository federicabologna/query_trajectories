{
    "query": "Provide a comprehensive overview of the various forms of social bias (e.g., gender, racial, religious, disability, and stigma-related) present in large language models, including how these biases originate from training data and manifest across downstream applications.",
    "user_id": "lib_user",
    "task_id": "579d9b35-c077-446f-9b3b-2bdee3abfbe9",
    "timestamp": "2025-06-24T00:55:26.906230",
    "n_retrieval": 256,
    "n_retrieved": 256,
    "n_candidates": 43,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.70278,
    "decomposed_query": {
        "rewritten_query": "Various forms of social bias (gender, racial, religious, disability, and stigma-related) present in large language models, including how these biases originate from training data and manifest across downstream applications.",
        "keyword_query": "social bias gender racial religious disability stigma large language models training data downstream applications",
        "search_filters": {
            "fieldsOfStudy": "Computer Science,Sociology"
        },
        "cost": 0.0102,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Investigating Annotator Bias in Large Language Models for Hate Speech Detection",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 41,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.11109, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2221319016",
                    "name": "Amit Das"
                },
                {
                    "authorId": "2264121393",
                    "name": "Zheng Zhang"
                },
                {
                    "authorId": "2307011198",
                    "name": "Fatemeh Jamshidi"
                },
                {
                    "authorId": "2212131028",
                    "name": "Vinija Jain"
                },
                {
                    "authorId": "2275226689",
                    "name": "Aman Chadha"
                },
                {
                    "authorId": "2181140446",
                    "name": "Nilanjana Raychawdhary"
                },
                {
                    "authorId": "2291126998",
                    "name": "Mary Sandage"
                },
                {
                    "authorId": "2290014871",
                    "name": "Lauramarie Pope"
                },
                {
                    "authorId": "38328224",
                    "name": "Gerry V. Dozier"
                },
                {
                    "authorId": "2283201612",
                    "name": "Cheryl Seals"
                }
            ],
            "abstract": "Data annotation, the practice of assigning descriptive labels to raw data, is pivotal in optimizing the performance of machine learning models. However, it is a resource-intensive process susceptible to biases introduced by annotators. The emergence of sophisticated Large Language Models (LLMs) presents a unique opportunity to modernize and streamline this complex procedure. While existing research extensively evaluates the efficacy of LLMs, as annotators, this paper delves into the biases present in LLMs when annotating hate speech data. Our research contributes to understanding biases in four key categories: gender, race, religion, and disability with four LLMs: GPT-3.5, GPT-4o, Llama-3.1 and Gemma-2. Specifically targeting highly vulnerable groups within these categories, we analyze annotator biases. Furthermore, we conduct a comprehensive examination of potential factors contributing to these biases by scrutinizing the annotated data. We introduce our custom hate speech detection dataset, HateBiasNet, to conduct this research. Additionally, we perform the same experiments on the ETHOS (Mollas et al. 2022) dataset also for comparative analysis. This paper serves as a crucial resource, guiding researchers and practitioners in harnessing the potential of LLMs for data annotation, thereby fostering advancements in this critical field.",
            "corpus_id": 270560549,
            "sentences": [
                {
                    "corpus_id": "270560549",
                    "title": "Investigating Annotator Bias in Large Language Models for Hate Speech Detection",
                    "text": "Specifically we select the target groups that are highly vulnerable within the mentioned four categories and explore the annotator biases. Additionally, we provide a detailed analysis of the possible reasons of these biases by exploring the data being annotated. Serving as a critical guide, this paper aims to steer researchers towards exploring the potential of LLMs for data annotation, thereby facilitating future advancements in this essential domain. \n\nThrough rigorous data annotation, prompt engineering, quantitative and qualitative analysis, we aim to answer the following research questions: RQ1: Does annotator bias exist in Large Language Models for hate speech detection? RQ2: If it exists, what potential factors contribute to its existence? RQ3: How can this problem be effectively mitigated? \n\nTo this end, our work makes the following contributions: OUR CONTRIBUTIONS \u27a0 Our research demonstrates that annotator bias is present in LLMs used for hate speech detection. This bias arises from the subjective interpretations of annotators, which influence the training data and consequently affect the model's performance. We provide empirical evidence illustrating how such biases skew detection results, leading to potential inaccuracies and unfair outcomes. \n\n\u27a0 In our research, we specifically examine four types of biases: gender, race, disability, and religion. Gender bias refers to the prejudiced treatment based on an individual's sex or gender identity. Race bias involves discriminatory actions or attitudes towards individuals based on their racial or ethnic background. Disability bias encompasses unfair treatment of people with physical or mental impairments. Religion bias involves prejudices and discriminatory behaviors directed at individuals based on their religious beliefs or practices. Our study aims to analyze the prevalence and impact of these biases in various contexts. \n\n\u27a0 We delve into the underlying factors contributing to bias and propose a potential solution to address this issue. We analyze various aspects to uncover the root causes of bias and present a strategy aimed at mitigating its effects. Through our investigation, we aim to provide valuable insights into understanding and combatting bias in our study.",
                    "score": 0.6169795807903248,
                    "section_title": "Introduction",
                    "char_start_offset": 2356,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 138
                        },
                        {
                            "start": 139,
                            "end": 262
                        },
                        {
                            "start": 263,
                            "end": 456
                        },
                        {
                            "start": 459,
                            "end": 685
                        },
                        {
                            "start": 686,
                            "end": 756
                        },
                        {
                            "start": 757,
                            "end": 808
                        },
                        {
                            "start": 811,
                            "end": 984
                        },
                        {
                            "start": 985,
                            "end": 1135
                        },
                        {
                            "start": 1136,
                            "end": 1273
                        },
                        {
                            "start": 1276,
                            "end": 1380
                        },
                        {
                            "start": 1381,
                            "end": 1476
                        },
                        {
                            "start": 1477,
                            "end": 1595
                        },
                        {
                            "start": 1596,
                            "end": 1687
                        },
                        {
                            "start": 1688,
                            "end": 1821
                        },
                        {
                            "start": 1822,
                            "end": 1910
                        },
                        {
                            "start": 1913,
                            "end": 2028
                        },
                        {
                            "start": 2029,
                            "end": 2146
                        },
                        {
                            "start": 2147,
                            "end": 2262
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94921875
                }
            ],
            "relevance_judgement": 0.94921875,
            "relevance_judgment_input_expanded": "# Title: Investigating Annotator Bias in Large Language Models for Hate Speech Detection\n# Venue: arXiv.org\n# Authors: Amit Das, Zheng Zhang, Fatemeh Jamshidi, Vinija Jain, Aman Chadha, Nilanjana Raychawdhary, Mary Sandage, Lauramarie Pope, Gerry V. Dozier, Cheryl Seals\n## Abstract\nData annotation, the practice of assigning descriptive labels to raw data, is pivotal in optimizing the performance of machine learning models. However, it is a resource-intensive process susceptible to biases introduced by annotators. The emergence of sophisticated Large Language Models (LLMs) presents a unique opportunity to modernize and streamline this complex procedure. While existing research extensively evaluates the efficacy of LLMs, as annotators, this paper delves into the biases present in LLMs when annotating hate speech data. Our research contributes to understanding biases in four key categories: gender, race, religion, and disability with four LLMs: GPT-3.5, GPT-4o, Llama-3.1 and Gemma-2. Specifically targeting highly vulnerable groups within these categories, we analyze annotator biases. Furthermore, we conduct a comprehensive examination of potential factors contributing to these biases by scrutinizing the annotated data. We introduce our custom hate speech detection dataset, HateBiasNet, to conduct this research. Additionally, we perform the same experiments on the ETHOS (Mollas et al. 2022) dataset also for comparative analysis. This paper serves as a crucial resource, guiding researchers and practitioners in harnessing the potential of LLMs for data annotation, thereby fostering advancements in this critical field.\n## Introduction\nSpecifically we select the target groups that are highly vulnerable within the mentioned four categories and explore the annotator biases. Additionally, we provide a detailed analysis of the possible reasons of these biases by exploring the data being annotated. Serving as a critical guide, this paper aims to steer researchers towards exploring the potential of LLMs for data annotation, thereby facilitating future advancements in this essential domain. \n\nThrough rigorous data annotation, prompt engineering, quantitative and qualitative analysis, we aim to answer the following research questions: RQ1: Does annotator bias exist in Large Language Models for hate speech detection? RQ2: If it exists, what potential factors contribute to its existence? RQ3: How can this problem be effectively mitigated? \n\nTo this end, our work makes the following contributions: OUR CONTRIBUTIONS \u27a0 Our research demonstrates that annotator bias is present in LLMs used for hate speech detection. This bias arises from the subjective interpretations of annotators, which influence the training data and consequently affect the model's performance. We provide empirical evidence illustrating how such biases skew detection results, leading to potential inaccuracies and unfair outcomes. \n\n\u27a0 In our research, we specifically examine four types of biases: gender, race, disability, and religion. Gender bias refers to the prejudiced treatment based on an individual's sex or gender identity. Race bias involves discriminatory actions or attitudes towards individuals based on their racial or ethnic background. Disability bias encompasses unfair treatment of people with physical or mental impairments. Religion bias involves prejudices and discriminatory behaviors directed at individuals based on their religious beliefs or practices. Our study aims to analyze the prevalence and impact of these biases in various contexts. \n\n\u27a0 We delve into the underlying factors contributing to bias and propose a potential solution to address this issue. We analyze various aspects to uncover the root causes of bias and present a strategy aimed at mitigating its effects. Through our investigation, we aim to provide valuable insights into understanding and combatting bias in our study.",
            "reference_string": "[270560549 | Das et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Understanding Social Biases in Large Language Models",
            "venue": "Applied Informatics",
            "year": 2025,
            "reference_count": 45,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/ai6050106?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/ai6050106, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2333603975",
                    "name": "Ojasvi Gupta"
                },
                {
                    "authorId": "2238815500",
                    "name": "Stefano Marrone"
                },
                {
                    "authorId": "2179616179",
                    "name": "Francesco Gargiulo"
                },
                {
                    "authorId": "2273940705",
                    "name": "Rajesh Jaiswal"
                },
                {
                    "authorId": "2273941215",
                    "name": "Lidia Marassi"
                }
            ],
            "abstract": "Background/Objectives: Large Language Models (LLMs) like ChatGPT, LLAMA, and Mistral are widely used for automating tasks such as content creation and data analysis. However, due to their training on publicly available internet data, they may inherit social biases. We aimed to investigate the social biases (i.e., ethnic, gender, and disability biases) in these models and evaluate how different model versions handle them. Methods: We instruction-tuned popular models (like Mistral, LLAMA, and Gemma), and for this we curated a dataset constructed by collecting and modifying diverse data from various public datasets. Prompts were run through a controlled pipeline, and responses were categorized (e.g., biased, confused, repeated, or accurate) and analyzed. Results: We found that models responded differently to bias prompts depending on their version. Fine-tuned models showed fewer overt biases but more confusion or censorship. Disability-related prompts triggered the most consistent biases across models. Conclusions: Bias persists in LLMs despite instruction tuning. Differences between model versions may lead to inconsistent user experiences and hidden harms in downstream applications. Greater transparency and robust fairness testing are essential.",
            "corpus_id": 278807456,
            "sentences": [],
            "relevance_judgement": 0.916015625,
            "relevance_judgment_input_expanded": "# Title: Understanding Social Biases in Large Language Models\n# Venue: Applied Informatics\n# Authors: Ojasvi Gupta, Stefano Marrone, Francesco Gargiulo, Rajesh Jaiswal, Lidia Marassi\n## Abstract\nBackground/Objectives: Large Language Models (LLMs) like ChatGPT, LLAMA, and Mistral are widely used for automating tasks such as content creation and data analysis. However, due to their training on publicly available internet data, they may inherit social biases. We aimed to investigate the social biases (i.e., ethnic, gender, and disability biases) in these models and evaluate how different model versions handle them. Methods: We instruction-tuned popular models (like Mistral, LLAMA, and Gemma), and for this we curated a dataset constructed by collecting and modifying diverse data from various public datasets. Prompts were run through a controlled pipeline, and responses were categorized (e.g., biased, confused, repeated, or accurate) and analyzed. Results: We found that models responded differently to bias prompts depending on their version. Fine-tuned models showed fewer overt biases but more confusion or censorship. Disability-related prompts triggered the most consistent biases across models. Conclusions: Bias persists in LLMs despite instruction tuning. Differences between model versions may lead to inconsistent user experiences and hidden harms in downstream applications. Greater transparency and robust fairness testing are essential.\n",
            "reference_string": "[278807456 | Gupta et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Bias in Large Language Models: Origin, Evaluation, and Mitigation",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 190,
            "citation_count": 22,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.10915, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2331613833",
                    "name": "Yufei Guo"
                },
                {
                    "authorId": "2331371661",
                    "name": "Muzhe Guo"
                },
                {
                    "authorId": "2273060735",
                    "name": "Juntao Su"
                },
                {
                    "authorId": "2331371753",
                    "name": "Zhou Yang"
                },
                {
                    "authorId": "2331357933",
                    "name": "Mengqiu Zhu"
                },
                {
                    "authorId": "2331365682",
                    "name": "Hongfei Li"
                },
                {
                    "authorId": "1455277182",
                    "name": "Mengyang Qiu"
                },
                {
                    "authorId": "2331377869",
                    "name": "Shuo Shuo Liu"
                }
            ],
            "abstract": "Large Language Models (LLMs) have revolutionized natural language processing, but their susceptibility to biases poses significant challenges. This comprehensive review examines the landscape of bias in LLMs, from its origins to current mitigation strategies. We categorize biases as intrinsic and extrinsic, analyzing their manifestations in various NLP tasks. The review critically assesses a range of bias evaluation methods, including data-level, model-level, and output-level approaches, providing researchers with a robust toolkit for bias detection. We further explore mitigation strategies, categorizing them into pre-model, intra-model, and post-model techniques, highlighting their effectiveness and limitations. Ethical and legal implications of biased LLMs are discussed, emphasizing potential harms in real-world applications such as healthcare and criminal justice. By synthesizing current knowledge on bias in LLMs, this review contributes to the ongoing effort to develop fair and responsible AI systems. Our work serves as a comprehensive resource for researchers and practitioners working towards understanding, evaluating, and mitigating bias in LLMs, fostering the development of more equitable AI technologies.",
            "corpus_id": 274130807,
            "sentences": [
                {
                    "corpus_id": "274130807",
                    "title": "Bias in Large Language Models: Origin, Evaluation, and Mitigation",
                    "text": "These biases not only perpetuate inequality but also raise significant ethical and legal questions regarding the use of LLMs in decision-making processes. Given the critical nature of these challenges, there is an urgent need for comprehensive evaluation frameworks to detect, quantify, and mitigate these biases, ensuring that LLMs function as fair and equitable tools for all users. \n\nThe bias presented in Large Language Models (LLMs) can be broadly categorized into intrinsic bias and extrinsic bias based on the different stages at which the biases manifest within the model's lifecycle and the type of bias being measured (Doan et al., 2024). Intrinsic bias refers to biases that are inherently within the internal representations or outputs of a trained or pre-trained LLM and are independent of any specific downstream tasks. Extrinsic bias refers to the biases that manifest during the model's performance on specific downstream tasks after training or fine-turing. Intrinsic biases are typically encoded during the training or pre-training phase when the LLM learns from large-scale corpora. For instance, if a pre-trained LLM consistently associates certain professions with specific genders (e.g., assuming all doctors to be male and assuming all nurses to be female) or racial stereotypes (May et al., 2019), this bias would be considered intrinsic bias. This type of bias is tied to the way the model has encoded relationships and patterns in its internal representations. \n\nIntrinsic biases are primarily introduced during the training/pre-training stages where models learn patterns and representations from vast corpora. These biases can be traced to several aspects of data and model design, which will be outlined below.",
                    "score": 0.6977688919414837,
                    "section_title": "Introduction",
                    "char_start_offset": 5697,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 154
                        },
                        {
                            "start": 155,
                            "end": 384
                        },
                        {
                            "start": 387,
                            "end": 648
                        },
                        {
                            "start": 649,
                            "end": 833
                        },
                        {
                            "start": 834,
                            "end": 974
                        },
                        {
                            "start": 975,
                            "end": 1101
                        },
                        {
                            "start": 1102,
                            "end": 1367
                        },
                        {
                            "start": 1368,
                            "end": 1486
                        },
                        {
                            "start": 1489,
                            "end": 1637
                        },
                        {
                            "start": 1638,
                            "end": 1739
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9150390625
                },
                {
                    "corpus_id": "274130807",
                    "title": "Bias in Large Language Models: Origin, Evaluation, and Mitigation",
                    "text": "Extrinsic bias refers to disparities in a model's performance across different downstream tasks, also known as downstream bias or prediction bias (Doan et al., 2024;Gallegos et al., 2024;Li et al., 2023). This type of bias emerges when a model's effectiveness varies among tasks or demographic groups, potentially leading to unequal outcomes in practical applications. \n\nDepending on the downstream tasks, extrinsic bias manifests differently. We categorize these tasks into two main groups: natural language understanding (NLU) tasks and natural language generation (NLG) tasks. In NLU tasks, extrinsic bias affects how the model comprehends and interprets input text. In NLG tasks, it can result in the generation of biased or stereotypical language.",
                    "score": 0.5843058606309434,
                    "section_title": "Extrinsic Bias",
                    "char_start_offset": 11375,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 204
                        },
                        {
                            "start": 205,
                            "end": 368
                        },
                        {
                            "start": 371,
                            "end": 443
                        },
                        {
                            "start": 444,
                            "end": 579
                        },
                        {
                            "start": 580,
                            "end": 669
                        },
                        {
                            "start": 670,
                            "end": 752
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 165,
                            "end": 187,
                            "matchedPaperCorpusId": "261530629"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8916015625
                }
            ],
            "relevance_judgement": 0.9150390625,
            "relevance_judgment_input_expanded": "# Title: Bias in Large Language Models: Origin, Evaluation, and Mitigation\n# Venue: arXiv.org\n# Authors: Yufei Guo, Muzhe Guo, Juntao Su, Zhou Yang, Mengqiu Zhu, Hongfei Li, Mengyang Qiu, Shuo Shuo Liu\n## Abstract\nLarge Language Models (LLMs) have revolutionized natural language processing, but their susceptibility to biases poses significant challenges. This comprehensive review examines the landscape of bias in LLMs, from its origins to current mitigation strategies. We categorize biases as intrinsic and extrinsic, analyzing their manifestations in various NLP tasks. The review critically assesses a range of bias evaluation methods, including data-level, model-level, and output-level approaches, providing researchers with a robust toolkit for bias detection. We further explore mitigation strategies, categorizing them into pre-model, intra-model, and post-model techniques, highlighting their effectiveness and limitations. Ethical and legal implications of biased LLMs are discussed, emphasizing potential harms in real-world applications such as healthcare and criminal justice. By synthesizing current knowledge on bias in LLMs, this review contributes to the ongoing effort to develop fair and responsible AI systems. Our work serves as a comprehensive resource for researchers and practitioners working towards understanding, evaluating, and mitigating bias in LLMs, fostering the development of more equitable AI technologies.\n## Introduction\nThese biases not only perpetuate inequality but also raise significant ethical and legal questions regarding the use of LLMs in decision-making processes. Given the critical nature of these challenges, there is an urgent need for comprehensive evaluation frameworks to detect, quantify, and mitigate these biases, ensuring that LLMs function as fair and equitable tools for all users. \n\nThe bias presented in Large Language Models (LLMs) can be broadly categorized into intrinsic bias and extrinsic bias based on the different stages at which the biases manifest within the model's lifecycle and the type of bias being measured (Doan et al., 2024). Intrinsic bias refers to biases that are inherently within the internal representations or outputs of a trained or pre-trained LLM and are independent of any specific downstream tasks. Extrinsic bias refers to the biases that manifest during the model's performance on specific downstream tasks after training or fine-turing. Intrinsic biases are typically encoded during the training or pre-training phase when the LLM learns from large-scale corpora. For instance, if a pre-trained LLM consistently associates certain professions with specific genders (e.g., assuming all doctors to be male and assuming all nurses to be female) or racial stereotypes (May et al., 2019), this bias would be considered intrinsic bias. This type of bias is tied to the way the model has encoded relationships and patterns in its internal representations. \n\nIntrinsic biases are primarily introduced during the training/pre-training stages where models learn patterns and representations from vast corpora. These biases can be traced to several aspects of data and model design, which will be outlined below.\n\n## Extrinsic Bias\nExtrinsic bias refers to disparities in a model's performance across different downstream tasks, also known as downstream bias or prediction bias (Doan et al., 2024;Gallegos et al., 2024;Li et al., 2023). This type of bias emerges when a model's effectiveness varies among tasks or demographic groups, potentially leading to unequal outcomes in practical applications. \n\nDepending on the downstream tasks, extrinsic bias manifests differently. We categorize these tasks into two main groups: natural language understanding (NLU) tasks and natural language generation (NLG) tasks. In NLU tasks, extrinsic bias affects how the model comprehends and interprets input text. In NLG tasks, it can result in the generation of biased or stereotypical language.",
            "reference_string": "[274130807 | Guo et al. | 2024 | Citations: 22]"
        },
        {
            "title": "Social Bias Probing: Fairness Benchmarking for Language Models",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2023,
            "reference_count": 71,
            "citation_count": 20,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.09090, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2121386115",
                    "name": "Marta Marchiori Manerba"
                },
                {
                    "authorId": "82563120",
                    "name": "Karolina Sta\u0144czak"
                },
                {
                    "authorId": "2257013371",
                    "name": "Riccardo Guidotti"
                },
                {
                    "authorId": "1736067",
                    "name": "Isabelle Augenstein"
                }
            ],
            "abstract": "While the impact of social biases in language models has been recognized, prior methods for bias evaluation have been limited to binary association tests on small datasets, limiting our understanding of bias complexities. This paper proposes a novel framework for probing language models for social biases by assessing disparate treatment, which involves treating individuals differently according to their affiliation with a sensitive demographic group. We curate SoFa, a large-scale benchmark designed to address the limitations of existing fairness collections. SoFa expands the analysis beyond the binary comparison of stereotypical versus anti-stereotypical identities to include a diverse range of identities and stereotypes. Comparing our methodology with existing benchmarks, we reveal that biases within language models are more nuanced than acknowledged, indicating a broader scope of encoded biases than previously recognized. Benchmarking LMs on SoFa, we expose how identities expressing different religions lead to the most pronounced disparate treatments across all models. Finally, our findings indicate that real-life adversities faced by various groups such as women and people with disabilities are mirrored in the behavior of these models.",
            "corpus_id": 265212726,
            "sentences": [
                {
                    "corpus_id": "265212726",
                    "title": "Social Bias Probing: Fairness Benchmarking for Language Models",
                    "text": "While the impact of social biases in language models has been recognized, prior methods for bias evaluation have been limited to binary association tests on small datasets, limiting our understanding of bias complexities. This paper proposes a novel framework for probing language models for social biases by assessing disparate treatment, which involves treating individuals differently according to their affiliation with a sensitive demographic group. We curate SoFa, a large-scale benchmark designed to address the limitations of existing fairness collections. SoFa expands the analysis beyond the binary comparison of stereotypical versus anti-stereotypical identities to include a diverse range of identities and stereotypes. Comparing our methodology with existing benchmarks, we reveal that biases within language models are more nuanced than acknowledged, indicating a broader scope of encoded biases than previously recognized. Benchmarking LMs on SoFa, we expose how identities expressing different religions lead to the most pronounced disparate treatments across all models. Finally, our findings indicate that real-life adversities faced by various groups such as women and people with disabilities are mirrored in the behavior of these models.",
                    "score": 0.680378228912433,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.89697265625
                },
                {
                    "corpus_id": "265212726",
                    "title": "Social Bias Probing: Fairness Benchmarking for Language Models",
                    "text": "Social bias2 can be defined as the manifestation through language of \"prejudices, stereotypes, and discriminatory attitudes against certain groups of people\" (Navigli et al., 2023). These biases are featured in training datasets and are carried over into downstream applications, resulting in, for instance, classification errors concerning specific minorities and the generation of harmful content when models are prompted with sensitive identities (Cui et al., 2024;Gallegos et al., 2023). \n\nTo measure the extent to which social bias is present in language models, we propose a Social Bias Probing framework (see Fig. 1) which serves as a technique for fine-grained fairness benchmarking of LMs. We first collect a set of stereotypes and identities (Section 3.1-Section 3.2), which results in the SOFA (Social Fairness) dataset (Section 3.3). The final phase of our workflow involves evaluating language models by employing our proposed perplexity-based fairness measures in response to the constructed probes (Section 3.4), exploited in the designed evaluation setting (Section 3.5).",
                    "score": 0.6179625766578583,
                    "section_title": "Social Bias Probing Framework",
                    "char_start_offset": 7371,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 181
                        },
                        {
                            "start": 182,
                            "end": 491
                        },
                        {
                            "start": 494,
                            "end": 698
                        },
                        {
                            "start": 699,
                            "end": 845
                        },
                        {
                            "start": 846,
                            "end": 1087
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 158,
                            "end": 180,
                            "matchedPaperCorpusId": "258688053"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8779296875
                },
                {
                    "corpus_id": "265212726",
                    "title": "Social Bias Probing: Fairness Benchmarking for Language Models",
                    "text": "To this end, we generate our probing dataset by com-arXiv:2311.09090v4 [cs.CL] 7 Oct 2024 bining stereotypes from the SOCIAL BIAS INFER-ENCE CORPUS (SBIC; Sap et al. 2020) and identities from the lexicon by Czarnowska et al. (2021). In this paper we examine identities belonging to four social categories: gender, religion, disability, and nationality. Secondly, we assess social biases across five state-of-the-art LMs in English. We use perplexity (Jelinek et al., 1977), a measure of language model uncertainty, as a proxy for bias. By analyzing the variation in perplexity when probes feature different identities within the diverse social categories, we infer which identities are deemed most likely by a model. This approach facilitates a three-dimensional analysis -by social category, identity, and stereotype-across the evaluated LMs. In summary, the contributions of this work are: \n\n\u2022 We conceptually facilitate fairness benchmarking across multiple identities using our Social Bias Probing framework, going beyond the binary approach of a stereotypical and an antistereotypical identity. \u2022 We introduce SOFA (Social Fairness), a benchmark for fairness probing addressing limitations of existing datasets, including a variety of different identities and stereotypes.1 \u2022 We assess social biases in five autoregressive causal language modeling architectures by examining disparate treatment across social categories, identities, and stereotypes. A comparative analysis with the popular benchmarks CROWS-PAIRS (Nangia et al., 2020) and STEREOSET (Nadeem et al., 2021) reveals marked differences in the overall fairness ranking of the models, providing a different view on the social biases encoded in LMs. We further find how identities expressing religions lead to the most pronounced disparate treatments across all models, while the different nationalities appear to induce the least variation compared to the other examined categories, namely gender and disability. We hypothesize that the increased visibility of religious disparities in language models may stem from recent successful efforts to mitigate racial and gender biases. This underscores the urgency for a comprehensive investigation into biases across multiple dimensions.",
                    "score": 0.6254129495633101,
                    "section_title": "Introduction",
                    "char_start_offset": 1807,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 70
                        },
                        {
                            "start": 71,
                            "end": 232
                        },
                        {
                            "start": 233,
                            "end": 352
                        },
                        {
                            "start": 353,
                            "end": 431
                        },
                        {
                            "start": 432,
                            "end": 535
                        },
                        {
                            "start": 536,
                            "end": 716
                        },
                        {
                            "start": 717,
                            "end": 843
                        },
                        {
                            "start": 844,
                            "end": 891
                        },
                        {
                            "start": 894,
                            "end": 1099
                        },
                        {
                            "start": 1100,
                            "end": 1454
                        },
                        {
                            "start": 1455,
                            "end": 1713
                        },
                        {
                            "start": 1714,
                            "end": 1977
                        },
                        {
                            "start": 1978,
                            "end": 2144
                        },
                        {
                            "start": 2145,
                            "end": 2247
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 155,
                            "end": 170,
                            "matchedPaperCorpusId": "207853290"
                        },
                        {
                            "start": 207,
                            "end": 231,
                            "matchedPaperCorpusId": "235658325"
                        },
                        {
                            "start": 450,
                            "end": 472,
                            "matchedPaperCorpusId": "121680873"
                        },
                        {
                            "start": 1518,
                            "end": 1539,
                            "matchedPaperCorpusId": "222090785"
                        },
                        {
                            "start": 1554,
                            "end": 1574,
                            "matchedPaperCorpusId": "215828184"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.82958984375
                }
            ],
            "relevance_judgement": 0.89697265625,
            "relevance_judgment_input_expanded": "# Title: Social Bias Probing: Fairness Benchmarking for Language Models\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Marta Marchiori Manerba, Karolina Sta\u0144czak, Riccardo Guidotti, Isabelle Augenstein\n## Abstract\nWhile the impact of social biases in language models has been recognized, prior methods for bias evaluation have been limited to binary association tests on small datasets, limiting our understanding of bias complexities. This paper proposes a novel framework for probing language models for social biases by assessing disparate treatment, which involves treating individuals differently according to their affiliation with a sensitive demographic group. We curate SoFa, a large-scale benchmark designed to address the limitations of existing fairness collections. SoFa expands the analysis beyond the binary comparison of stereotypical versus anti-stereotypical identities to include a diverse range of identities and stereotypes. Comparing our methodology with existing benchmarks, we reveal that biases within language models are more nuanced than acknowledged, indicating a broader scope of encoded biases than previously recognized. Benchmarking LMs on SoFa, we expose how identities expressing different religions lead to the most pronounced disparate treatments across all models. Finally, our findings indicate that real-life adversities faced by various groups such as women and people with disabilities are mirrored in the behavior of these models.\n## Introduction\nTo this end, we generate our probing dataset by com-arXiv:2311.09090v4 [cs.CL] 7 Oct 2024 bining stereotypes from the SOCIAL BIAS INFER-ENCE CORPUS (SBIC; Sap et al. 2020) and identities from the lexicon by Czarnowska et al. (2021). In this paper we examine identities belonging to four social categories: gender, religion, disability, and nationality. Secondly, we assess social biases across five state-of-the-art LMs in English. We use perplexity (Jelinek et al., 1977), a measure of language model uncertainty, as a proxy for bias. By analyzing the variation in perplexity when probes feature different identities within the diverse social categories, we infer which identities are deemed most likely by a model. This approach facilitates a three-dimensional analysis -by social category, identity, and stereotype-across the evaluated LMs. In summary, the contributions of this work are: \n\n\u2022 We conceptually facilitate fairness benchmarking across multiple identities using our Social Bias Probing framework, going beyond the binary approach of a stereotypical and an antistereotypical identity. \u2022 We introduce SOFA (Social Fairness), a benchmark for fairness probing addressing limitations of existing datasets, including a variety of different identities and stereotypes.1 \u2022 We assess social biases in five autoregressive causal language modeling architectures by examining disparate treatment across social categories, identities, and stereotypes. A comparative analysis with the popular benchmarks CROWS-PAIRS (Nangia et al., 2020) and STEREOSET (Nadeem et al., 2021) reveals marked differences in the overall fairness ranking of the models, providing a different view on the social biases encoded in LMs. We further find how identities expressing religions lead to the most pronounced disparate treatments across all models, while the different nationalities appear to induce the least variation compared to the other examined categories, namely gender and disability. We hypothesize that the increased visibility of religious disparities in language models may stem from recent successful efforts to mitigate racial and gender biases. This underscores the urgency for a comprehensive investigation into biases across multiple dimensions.\n\n## Social Bias Probing Framework\nSocial bias2 can be defined as the manifestation through language of \"prejudices, stereotypes, and discriminatory attitudes against certain groups of people\" (Navigli et al., 2023). These biases are featured in training datasets and are carried over into downstream applications, resulting in, for instance, classification errors concerning specific minorities and the generation of harmful content when models are prompted with sensitive identities (Cui et al., 2024;Gallegos et al., 2023). \n\nTo measure the extent to which social bias is present in language models, we propose a Social Bias Probing framework (see Fig. 1) which serves as a technique for fine-grained fairness benchmarking of LMs. We first collect a set of stereotypes and identities (Section 3.1-Section 3.2), which results in the SOFA (Social Fairness) dataset (Section 3.3). The final phase of our workflow involves evaluating language models by employing our proposed perplexity-based fairness measures in response to the constructed probes (Section 3.4), exploited in the designed evaluation setting (Section 3.5).",
            "reference_string": "[265212726 | Manerba et al. | 2023 | Citations: 20]"
        },
        {
            "title": "Bias Against 93 Stigmatized Groups in Masked Language Models and Downstream Sentiment Classification Tasks",
            "venue": "Conference on Fairness, Accountability and Transparency",
            "year": 2023,
            "reference_count": 63,
            "citation_count": 51,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3593013.3594109",
                "status": "BRONZE",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.05550, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2189183000",
                    "name": "Katelyn Mei"
                },
                {
                    "authorId": "2196943720",
                    "name": "Sonia Fereidooni"
                },
                {
                    "authorId": "144537437",
                    "name": "Aylin Caliskan"
                }
            ],
            "abstract": "Warning: The content of this paper may be upsetting or triggering. The rapid deployment of artificial intelligence (AI) models de- demands a thorough investigation of biases and risks inherent in these models to understand their impact on individuals and society. A growing body of work has shown that social biases are encoded in language models and their downstream tasks. This study extends the focus of bias evaluation in extant work by examining bias against social stigmas on a large scale. It focuses on 93 stigmatized groups in the United States, including a wide range of conditions related to disease, disability, drug use, mental illness, religion, sexuality, socioeconomic status, and other relevant factors. We investigate bias against these groups in English pre-trained Masked Language Models (MLMs) and their downstream sentiment classification tasks. To evaluate the presence of bias against 93 stigmatized conditions, we identify 29 non-stigmatized conditions to conduct a comparative analysis. Building upon a psychology scale of social rejection, the Social Distance Scale, we prompt six MLMs that are trained with different datasets: RoBERTa-base, RoBERTa-large, XLNet-large, BERTweet-base, BERTweet-large, and DistilBERT. We use human annotations to analyze the predicted words from these models, with which we measure the extent of bias against stigmatized groups. When prompts include stigmatized conditions, the probability of MLMs predicting negative words is, on average, 20 percent higher than when prompts have non-stigmatized conditions. Bias against stigmatized groups is also reflected in four downstream sentiment classifiers of these models. When sentences include stigmatized conditions related to diseases, disability, education, and mental illness, they are more likely to be classified as negative. For example, the sentence \"They are people who have less than a high school education.\" is classified as negative consistently across all models. We also observe a strong correlation between bias in MLMs and their downstream sentiment classifiers (Pearson\u2019s r =0.79). The evidence indicates that MLMs and their downstream sentiment classification tasks exhibit biases against socially stigmatized groups.",
            "corpus_id": 259129801,
            "sentences": [
                {
                    "corpus_id": "259129801",
                    "title": "Bias Against 93 Stigmatized Groups in Masked Language Models and Downstream Sentiment Classification Tasks",
                    "text": "Caliskan et al. [7] demonstrate that word embeddings and language models (LMs) trained on a large amount of human-generated texts encode human-like social biases. Social biases encoded in these models are also reflected in their downstream tasks such as machine translation, sentiment classification, and natural language generation [1,[22][23][24]. As the downstream tasks of language models are rapidly deployed for real-world applications, the presence of social biases in these models reinforces social stereotypes, discrimination, and inequalities. Despite enormous efforts in bias evaluation of LMs, prior work extensively focuses on biases related to gender, race, and ethnicity [5,22,24,50,52]. Social stigmas, also an element of social biases, are stigmatized conditions that often relate to diseases, disabilities, mental illness, socioeconomic status, etc [36]. Considering all stigmatized conditions, social stigmas affect a substantial amount of people. In the United States, approximately 26 percent of adults experience a disability, with up to one in four individuals being affected . In 2021, there were around 57.8M adults that experienced mental illness, which was around 22% of the population in the United States . Social stigmas prevent individuals from social activities and access to education, healthcare, and career opportunities, negatively influencing their psychological well-being and life outcomes [14,[31][32][33]37]. As language models capture other social biases, they may also learn bias against socially stigmatized groups. Such a risk would reinforce social inequalities with the rise of real-world applications of LMs. \n\nThis study examines bias against 93 stigmatized groups in the United States. To the best of our knowledge, this is the first study that examines social stigmas in LMs on a large scale. Pachankis et al. [36] conduct the first psychology study that classifies 93 social stigmas along six stigma dimensions and evaluates their interpersonal outcome, social rejection. We adapt their list of these 93 social stigmas and a widely used psychological questionnaire that measures social rejection, the Social Distance Scale, to quantify bias against stigmatized groups.",
                    "score": 0.6189412311883326,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 162
                        },
                        {
                            "start": 163,
                            "end": 349
                        },
                        {
                            "start": 350,
                            "end": 553
                        },
                        {
                            "start": 554,
                            "end": 702
                        },
                        {
                            "start": 703,
                            "end": 872
                        },
                        {
                            "start": 873,
                            "end": 966
                        },
                        {
                            "start": 967,
                            "end": 1100
                        },
                        {
                            "start": 1101,
                            "end": 1235
                        },
                        {
                            "start": 1236,
                            "end": 1449
                        },
                        {
                            "start": 1450,
                            "end": 1559
                        },
                        {
                            "start": 1560,
                            "end": 1656
                        },
                        {
                            "start": 1659,
                            "end": 1735
                        },
                        {
                            "start": 1736,
                            "end": 1843
                        },
                        {
                            "start": 1844,
                            "end": 2023
                        },
                        {
                            "start": 2024,
                            "end": 2220
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 16,
                            "end": 19,
                            "matchedPaperCorpusId": "23163324"
                        },
                        {
                            "start": 333,
                            "end": 336,
                            "matchedPaperCorpusId": "231603388"
                        },
                        {
                            "start": 336,
                            "end": 340,
                            "matchedPaperCorpusId": "250391069"
                        },
                        {
                            "start": 344,
                            "end": 348,
                            "matchedPaperCorpusId": "190000105"
                        },
                        {
                            "start": 686,
                            "end": 689,
                            "matchedPaperCorpusId": "102352788"
                        },
                        {
                            "start": 689,
                            "end": 692,
                            "matchedPaperCorpusId": "250391069"
                        },
                        {
                            "start": 692,
                            "end": 695,
                            "matchedPaperCorpusId": "190000105"
                        },
                        {
                            "start": 695,
                            "end": 698,
                            "matchedPaperCorpusId": "227275068"
                        },
                        {
                            "start": 698,
                            "end": 701,
                            "matchedPaperCorpusId": "238259136"
                        },
                        {
                            "start": 867,
                            "end": 871,
                            "matchedPaperCorpusId": "3750353"
                        },
                        {
                            "start": 1429,
                            "end": 1433,
                            "matchedPaperCorpusId": "21883193"
                        },
                        {
                            "start": 1433,
                            "end": 1437,
                            "matchedPaperCorpusId": "145255089"
                        },
                        {
                            "start": 1437,
                            "end": 1441,
                            "matchedPaperCorpusId": "246654106"
                        },
                        {
                            "start": 1441,
                            "end": 1445,
                            "matchedPaperCorpusId": "226207034"
                        },
                        {
                            "start": 1445,
                            "end": 1448,
                            "matchedPaperCorpusId": "33273008"
                        },
                        {
                            "start": 1861,
                            "end": 1865,
                            "matchedPaperCorpusId": "3750353"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.89599609375
                },
                {
                    "corpus_id": "259129801",
                    "title": "Bias Against 93 Stigmatized Groups in Masked Language Models and Downstream Sentiment Classification Tasks",
                    "text": "Warning: The content of this paper may be upsetting or triggering. The rapid deployment of artificial intelligence (AI) models de- demands a thorough investigation of biases and risks inherent in these models to understand their impact on individuals and society. A growing body of work has shown that social biases are encoded in language models and their downstream tasks. This study extends the focus of bias evaluation in extant work by examining bias against social stigmas on a large scale. It focuses on 93 stigmatized groups in the United States, including a wide range of conditions related to disease, disability, drug use, mental illness, religion, sexuality, socioeconomic status, and other relevant factors. We investigate bias against these groups in English pre-trained Masked Language Models (MLMs) and their downstream sentiment classification tasks. To evaluate the presence of bias against 93 stigmatized conditions, we identify 29 non-stigmatized conditions to conduct a comparative analysis. Building upon a psychology scale of social rejection, the Social Distance Scale, we prompt six MLMs that are trained with different datasets: RoBERTa-base, RoBERTa-large, XLNet-large, BERTweet-base, BERTweet-large, and DistilBERT. We use human annotations to analyze the predicted words from these models, with which we measure the extent of bias against stigmatized groups. When prompts include stigmatized conditions, the probability of MLMs predicting negative words is, on average, 20 percent higher than when prompts have non-stigmatized conditions. Bias against stigmatized groups is also reflected in four downstream sentiment classifiers of these models. When sentences include stigmatized conditions related to diseases, disability, education, and mental illness, they are more likely to be classified as negative. For example, the sentence \"They are people who have less than a high school education.\" is classified as negative consistently across all models. We also observe a strong correlation between bias in MLMs and their downstream sentiment classifiers (Pearson\u2019s r =0.79). The evidence indicates that MLMs and their downstream sentiment classification tasks exhibit biases against socially stigmatized groups.",
                    "score": 0.7663900622036225,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8359375
                }
            ],
            "relevance_judgement": 0.89599609375,
            "relevance_judgment_input_expanded": "# Title: Bias Against 93 Stigmatized Groups in Masked Language Models and Downstream Sentiment Classification Tasks\n# Venue: Conference on Fairness, Accountability and Transparency\n# Authors: Katelyn Mei, Sonia Fereidooni, Aylin Caliskan\n## Abstract\nWarning: The content of this paper may be upsetting or triggering. The rapid deployment of artificial intelligence (AI) models de- demands a thorough investigation of biases and risks inherent in these models to understand their impact on individuals and society. A growing body of work has shown that social biases are encoded in language models and their downstream tasks. This study extends the focus of bias evaluation in extant work by examining bias against social stigmas on a large scale. It focuses on 93 stigmatized groups in the United States, including a wide range of conditions related to disease, disability, drug use, mental illness, religion, sexuality, socioeconomic status, and other relevant factors. We investigate bias against these groups in English pre-trained Masked Language Models (MLMs) and their downstream sentiment classification tasks. To evaluate the presence of bias against 93 stigmatized conditions, we identify 29 non-stigmatized conditions to conduct a comparative analysis. Building upon a psychology scale of social rejection, the Social Distance Scale, we prompt six MLMs that are trained with different datasets: RoBERTa-base, RoBERTa-large, XLNet-large, BERTweet-base, BERTweet-large, and DistilBERT. We use human annotations to analyze the predicted words from these models, with which we measure the extent of bias against stigmatized groups. When prompts include stigmatized conditions, the probability of MLMs predicting negative words is, on average, 20 percent higher than when prompts have non-stigmatized conditions. Bias against stigmatized groups is also reflected in four downstream sentiment classifiers of these models. When sentences include stigmatized conditions related to diseases, disability, education, and mental illness, they are more likely to be classified as negative. For example, the sentence \"They are people who have less than a high school education.\" is classified as negative consistently across all models. We also observe a strong correlation between bias in MLMs and their downstream sentiment classifiers (Pearson\u2019s r =0.79). The evidence indicates that MLMs and their downstream sentiment classification tasks exhibit biases against socially stigmatized groups.\n## INTRODUCTION\nCaliskan et al. [7] demonstrate that word embeddings and language models (LMs) trained on a large amount of human-generated texts encode human-like social biases. Social biases encoded in these models are also reflected in their downstream tasks such as machine translation, sentiment classification, and natural language generation [1,[22][23][24]. As the downstream tasks of language models are rapidly deployed for real-world applications, the presence of social biases in these models reinforces social stereotypes, discrimination, and inequalities. Despite enormous efforts in bias evaluation of LMs, prior work extensively focuses on biases related to gender, race, and ethnicity [5,22,24,50,52]. Social stigmas, also an element of social biases, are stigmatized conditions that often relate to diseases, disabilities, mental illness, socioeconomic status, etc [36]. Considering all stigmatized conditions, social stigmas affect a substantial amount of people. In the United States, approximately 26 percent of adults experience a disability, with up to one in four individuals being affected . In 2021, there were around 57.8M adults that experienced mental illness, which was around 22% of the population in the United States . Social stigmas prevent individuals from social activities and access to education, healthcare, and career opportunities, negatively influencing their psychological well-being and life outcomes [14,[31][32][33]37]. As language models capture other social biases, they may also learn bias against socially stigmatized groups. Such a risk would reinforce social inequalities with the rise of real-world applications of LMs. \n\nThis study examines bias against 93 stigmatized groups in the United States. To the best of our knowledge, this is the first study that examines social stigmas in LMs on a large scale. Pachankis et al. [36] conduct the first psychology study that classifies 93 social stigmas along six stigma dimensions and evaluates their interpersonal outcome, social rejection. We adapt their list of these 93 social stigmas and a widely used psychological questionnaire that measures social rejection, the Social Distance Scale, to quantify bias against stigmatized groups.",
            "reference_string": "[259129801 | Mei et al. | 2023 | Citations: 51]"
        },
        {
            "title": "Gender Bias in Word Embeddings: A Comprehensive Analysis of Frequency, Syntax, and Semantics",
            "venue": "AAAI/ACM Conference on AI, Ethics, and Society",
            "year": 2022,
            "reference_count": 61,
            "citation_count": 51,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3514094.3534162",
                "status": "BRONZE",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2206.03390, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "144537437",
                    "name": "Aylin Caliskan"
                },
                {
                    "authorId": "2168463879",
                    "name": "Pimparkar Parth Ajay"
                },
                {
                    "authorId": "66649568",
                    "name": "Tessa E. S. Charlesworth"
                },
                {
                    "authorId": "100508901",
                    "name": "R. Wolfe"
                },
                {
                    "authorId": "1968771",
                    "name": "M. Banaji"
                }
            ],
            "abstract": "Word embeddings are numeric representations of meaning derived from word co-occurrence statistics in corpora of human-produced texts. The statistical regularities in language corpora encode well-known social biases into word embeddings (e.g., the word vector for family is closer to the vector women than to men). Although efforts have been made to mitigate bias in word embeddings, with the hope of improving fairness in downstream Natural Language Processing (NLP) applications, these efforts will remain limited until we more deeply understand the multiple (and often subtle) ways that social biases can be reflected in word embeddings. Here, we focus on gender to provide a comprehensive analysis of group-based biases in widely-used static English word embeddings trained on internet corpora (GloVe 2014, fastText 2017). While some previous research has helped uncover biases in specific semantic associations between a group and a target domain (e.g., women - family), using the Single-Category Word Embedding Association Test, we demonstrate the widespread prevalence of gender biases that also show differences in: (1) frequencies of words associated with men versus women; (b) part-of-speech tags in gender-associated words; (c) semantic categories in gender-associated words; and (d) valence, arousal, and dominance in gender-associated words. We leave the analysis of non-binary gender to future work due to the challenges in accurate group representation caused by limitations inherent in data. First, in terms of word frequency: we find that, of the 1,000 most frequent words in the vocabulary, 77% are more associated with men than women, providing direct evidence of a masculine default in the everyday language of the English-speaking world. Second, turning to parts-of-speech: the top male-associated words are typically verbs (e.g., fight, overpower) while the top female-associated words are typically adjectives and adverbs (e.g., giving, emotionally). Gender biases in embeddings also permeate parts-of-speech. Third, for semantic categories: bottom-up, cluster analyses of the top 1,000 words associated with each gender. The top male-associated concepts include roles and domains of big tech, engineering, religion, sports, and violence; in contrast, the top female-associated concepts are less focused on roles, including, instead, female-specific slurs and sexual content, as well as appearance and kitchen terms. Fourth, using human ratings of word valence, arousal, and dominance from a ~20,000 word lexicon, we find that male-associated words are higher on arousal and dominance, while female-associated words are higher on valence. Ultimately, these findings move the study of gender bias in word embeddings beyond the basic investigation of semantic relationships to also study gender differences in multiple manifestations in text. Given the central role of word embeddings in NLP applications, it is essential to more comprehensively document where biases exist and may remain hidden, allowing them to persist without our awareness throughout large text corpora.",
            "corpus_id": 249431834,
            "sentences": [
                {
                    "corpus_id": "249431834",
                    "title": "Gender Bias in Word Embeddings: A Comprehensive Analysis of Frequency, Syntax, and Semantics",
                    "text": "Today, the vast majority of our daily tasks are facilitated and enhanced through the application of Natural Language Processing (NLP), from simple machine translation to automated resume screening to auto-complete in emails [7]. The core component of many of these applications are pretrained static word embeddings -compressed, numeric representations of word meaning based on word co-occurrence statistics. These word embeddings are, in turn, created by training an algorithm (e.g., a neural network) on massive corpora of human-produced text stored on the internet. Ideally, word embeddings would be objective representations of human semantics but, in reality, word embeddings trained from human-produced text end up encoding and reproducing the types of social biases held by humans [9,12]. When datasets that reflect the thoughts, feelings, and actions of a community are used to train artificial intelligence (AI) models, the models inevitably incorporate the associations and representations of the community [11-13, 15, 17, 26, 40, 51, 54]. \n\nIn one of the first studies systematically assessing social biases in word embeddings, Caliskan et al. [12] showed that pretrained GloVe embeddings replicated ten major biases widely found among human participants including biases associating young-good/elderlybad, European American-good/African American-bad, and womenfamily/men-career [28]. Such group-based biases can manifest in NLP applications (a recent example was seen in Amazon's automated resume screening algorithm that preferentially hired men over women [7]). A comprehensive analysis of where biases reside in word embeddings, including in word frequency, syntax, and semantics, can aid in developing effective bias mitigation strategies. \n\nThe Present Research. The current manuscript provides the first comprehensive investigation of the many, and often subtle ways that social biases are reflected in widely-used static English word embeddings trained on internet corpora (specifically GloVe 2014 [41] and fastText 2017 [8]). We focus, in particular on gender biases, because gender is present in every society and gender-based biases affect large groups of people.",
                    "score": 0.5903702633029858,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 1073,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 228
                        },
                        {
                            "start": 229,
                            "end": 408
                        },
                        {
                            "start": 409,
                            "end": 568
                        },
                        {
                            "start": 569,
                            "end": 795
                        },
                        {
                            "start": 796,
                            "end": 1049
                        },
                        {
                            "start": 1052,
                            "end": 1395
                        },
                        {
                            "start": 1396,
                            "end": 1575
                        },
                        {
                            "start": 1576,
                            "end": 1755
                        },
                        {
                            "start": 1758,
                            "end": 1779
                        },
                        {
                            "start": 1780,
                            "end": 2045
                        },
                        {
                            "start": 2046,
                            "end": 2185
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 224,
                            "end": 227,
                            "matchedPaperCorpusId": "213771228"
                        },
                        {
                            "start": 788,
                            "end": 791,
                            "matchedPaperCorpusId": "1704893"
                        },
                        {
                            "start": 791,
                            "end": 794,
                            "matchedPaperCorpusId": "23163324"
                        },
                        {
                            "start": 1155,
                            "end": 1159,
                            "matchedPaperCorpusId": "23163324"
                        },
                        {
                            "start": 1390,
                            "end": 1394,
                            "matchedPaperCorpusId": "7840819"
                        },
                        {
                            "start": 1570,
                            "end": 1573,
                            "matchedPaperCorpusId": "213771228"
                        },
                        {
                            "start": 2017,
                            "end": 2021,
                            "matchedPaperCorpusId": "1957433"
                        },
                        {
                            "start": 2040,
                            "end": 2043,
                            "matchedPaperCorpusId": "207556454"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.89306640625
                }
            ],
            "relevance_judgement": 0.89306640625,
            "relevance_judgment_input_expanded": "# Title: Gender Bias in Word Embeddings: A Comprehensive Analysis of Frequency, Syntax, and Semantics\n# Venue: AAAI/ACM Conference on AI, Ethics, and Society\n# Authors: Aylin Caliskan, Pimparkar Parth Ajay, Tessa E. S. Charlesworth, R. Wolfe, M. Banaji\n## Abstract\nWord embeddings are numeric representations of meaning derived from word co-occurrence statistics in corpora of human-produced texts. The statistical regularities in language corpora encode well-known social biases into word embeddings (e.g., the word vector for family is closer to the vector women than to men). Although efforts have been made to mitigate bias in word embeddings, with the hope of improving fairness in downstream Natural Language Processing (NLP) applications, these efforts will remain limited until we more deeply understand the multiple (and often subtle) ways that social biases can be reflected in word embeddings. Here, we focus on gender to provide a comprehensive analysis of group-based biases in widely-used static English word embeddings trained on internet corpora (GloVe 2014, fastText 2017). While some previous research has helped uncover biases in specific semantic associations between a group and a target domain (e.g., women - family), using the Single-Category Word Embedding Association Test, we demonstrate the widespread prevalence of gender biases that also show differences in: (1) frequencies of words associated with men versus women; (b) part-of-speech tags in gender-associated words; (c) semantic categories in gender-associated words; and (d) valence, arousal, and dominance in gender-associated words. We leave the analysis of non-binary gender to future work due to the challenges in accurate group representation caused by limitations inherent in data. First, in terms of word frequency: we find that, of the 1,000 most frequent words in the vocabulary, 77% are more associated with men than women, providing direct evidence of a masculine default in the everyday language of the English-speaking world. Second, turning to parts-of-speech: the top male-associated words are typically verbs (e.g., fight, overpower) while the top female-associated words are typically adjectives and adverbs (e.g., giving, emotionally). Gender biases in embeddings also permeate parts-of-speech. Third, for semantic categories: bottom-up, cluster analyses of the top 1,000 words associated with each gender. The top male-associated concepts include roles and domains of big tech, engineering, religion, sports, and violence; in contrast, the top female-associated concepts are less focused on roles, including, instead, female-specific slurs and sexual content, as well as appearance and kitchen terms. Fourth, using human ratings of word valence, arousal, and dominance from a ~20,000 word lexicon, we find that male-associated words are higher on arousal and dominance, while female-associated words are higher on valence. Ultimately, these findings move the study of gender bias in word embeddings beyond the basic investigation of semantic relationships to also study gender differences in multiple manifestations in text. Given the central role of word embeddings in NLP applications, it is essential to more comprehensively document where biases exist and may remain hidden, allowing them to persist without our awareness throughout large text corpora.\n## INTRODUCTION\nToday, the vast majority of our daily tasks are facilitated and enhanced through the application of Natural Language Processing (NLP), from simple machine translation to automated resume screening to auto-complete in emails [7]. The core component of many of these applications are pretrained static word embeddings -compressed, numeric representations of word meaning based on word co-occurrence statistics. These word embeddings are, in turn, created by training an algorithm (e.g., a neural network) on massive corpora of human-produced text stored on the internet. Ideally, word embeddings would be objective representations of human semantics but, in reality, word embeddings trained from human-produced text end up encoding and reproducing the types of social biases held by humans [9,12]. When datasets that reflect the thoughts, feelings, and actions of a community are used to train artificial intelligence (AI) models, the models inevitably incorporate the associations and representations of the community [11-13, 15, 17, 26, 40, 51, 54]. \n\nIn one of the first studies systematically assessing social biases in word embeddings, Caliskan et al. [12] showed that pretrained GloVe embeddings replicated ten major biases widely found among human participants including biases associating young-good/elderlybad, European American-good/African American-bad, and womenfamily/men-career [28]. Such group-based biases can manifest in NLP applications (a recent example was seen in Amazon's automated resume screening algorithm that preferentially hired men over women [7]). A comprehensive analysis of where biases reside in word embeddings, including in word frequency, syntax, and semantics, can aid in developing effective bias mitigation strategies. \n\nThe Present Research. The current manuscript provides the first comprehensive investigation of the many, and often subtle ways that social biases are reflected in widely-used static English word embeddings trained on internet corpora (specifically GloVe 2014 [41] and fastText 2017 [8]). We focus, in particular on gender biases, because gender is present in every society and gender-based biases affect large groups of people.",
            "reference_string": "[249431834 | Caliskan et al. | 2022 | Citations: 51]"
        },
        {
            "title": "Explicit vs. Implicit: Investigating Social Bias in Large Language Models through Self-Reflection",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 48,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.02295, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2233316526",
                    "name": "Yachao Zhao"
                },
                {
                    "authorId": "2266189718",
                    "name": "Bo Wang"
                },
                {
                    "authorId": "2301797252",
                    "name": "Yan Wang"
                }
            ],
            "abstract": "Large Language Models (LLMs) have been shown to exhibit various biases and stereotypes in their generated content. While extensive research has investigated biases in LLMs, prior work has predominantly focused on explicit bias, with minimal attention to implicit bias and the relation between these two forms of bias. This paper presents a systematic framework grounded in social psychology theories to investigate and compare explicit and implicit biases in LLMs. We propose a novel self-reflection-based evaluation framework that operates in two phases: first measuring implicit bias through simulated psychological assessment methods, then evaluating explicit bias by prompting LLMs to analyze their own generated content. Through extensive experiments on advanced LLMs across multiple social dimensions, we demonstrate that LLMs exhibit a substantial inconsistency between explicit and implicit biases: while explicit bias manifests as mild stereotypes, implicit bias exhibits strong stereotypes. We further investigate the underlying factors contributing to this explicit-implicit bias inconsistency, examining the effects of training data scale, model size, and alignment techniques. Experimental results indicate that while explicit bias declines with increased training data and model size, implicit bias exhibits a contrasting upward trend. Moreover, contemporary alignment methods effectively suppress explicit bias but show limited efficacy in mitigating implicit bias.",
            "corpus_id": 275336873,
            "sentences": [
                {
                    "corpus_id": "275336873",
                    "title": "Explicit vs. Implicit: Investigating Social Bias in Large Language Models through Self-Reflection",
                    "text": "Large Language Models (LLMs) have been shown to exhibit various biases and stereotypes in their generated content. While extensive research has investigated bias in LLMs, prior work has predominantly focused on explicit bias, leaving the more nuanced implicit biases largely unexplored. This paper presents a systematic framework grounded in social psychology theories to investigate and compare explicit and implicit biases in LLMs. We propose a novel\"self-reflection\"based evaluation framework that operates in two phases: first measuring implicit bias through simulated psychological assessment methods, then evaluating explicit bias by prompting LLMs to analyze their own generated content. Through extensive experiments on state-of-the-art LLMs across multiple social dimensions, we demonstrate that LLMs exhibit a substantial inconsistency between explicit and implicit biases, where explicit biases manifest as mild stereotypes while implicit biases show strong stereotypes. Furthermore, we investigate the underlying factors contributing to this explicit-implicit bias inconsistency. Our experiments examine the effects of training data scale, model parameters, and alignment techniques. Results indicate that while explicit bias diminishes with increased training data and model size, implicit bias exhibits a contrasting upward trend. Notably, contemporary alignment methods (e.g., RLHF, DPO) effectively suppress explicit bias but show limited efficacy in mitigating implicit bias. These findings suggest that while scaling up models and alignment training can address explicit bias, the challenge of implicit bias requires novel approaches beyond current methodologies.",
                    "score": 0.6067733819945293,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88720703125
                }
            ],
            "relevance_judgement": 0.88720703125,
            "relevance_judgment_input_expanded": "# Title: Explicit vs. Implicit: Investigating Social Bias in Large Language Models through Self-Reflection\n# Venue: arXiv.org\n# Authors: Yachao Zhao, Bo Wang, Yan Wang\n## Abstract\nLarge Language Models (LLMs) have been shown to exhibit various biases and stereotypes in their generated content. While extensive research has investigated biases in LLMs, prior work has predominantly focused on explicit bias, with minimal attention to implicit bias and the relation between these two forms of bias. This paper presents a systematic framework grounded in social psychology theories to investigate and compare explicit and implicit biases in LLMs. We propose a novel self-reflection-based evaluation framework that operates in two phases: first measuring implicit bias through simulated psychological assessment methods, then evaluating explicit bias by prompting LLMs to analyze their own generated content. Through extensive experiments on advanced LLMs across multiple social dimensions, we demonstrate that LLMs exhibit a substantial inconsistency between explicit and implicit biases: while explicit bias manifests as mild stereotypes, implicit bias exhibits strong stereotypes. We further investigate the underlying factors contributing to this explicit-implicit bias inconsistency, examining the effects of training data scale, model size, and alignment techniques. Experimental results indicate that while explicit bias declines with increased training data and model size, implicit bias exhibits a contrasting upward trend. Moreover, contemporary alignment methods effectively suppress explicit bias but show limited efficacy in mitigating implicit bias.\n",
            "reference_string": "[275336873 | Zhao et al. | 2025 | Citations: 4]"
        },
        {
            "title": "A Review of Current Trends, Techniques, and Challenges in Large Language Models (LLMs)",
            "venue": "Applied Sciences",
            "year": 2024,
            "reference_count": 108,
            "citation_count": 80,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.mdpi.com/2076-3417/14/5/2074/pdf?version=1709291698",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/app14052074?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/app14052074, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2289385425",
                    "name": "Rajvardhan Patil"
                },
                {
                    "authorId": "117730513",
                    "name": "Venkat Gudivada"
                }
            ],
            "abstract": "Natural language processing (NLP) has significantly transformed in the last decade, especially in the field of language modeling. Large language models (LLMs) have achieved SOTA performances on natural language understanding (NLU) and natural language generation (NLG) tasks by learning language representation in self-supervised ways. This paper provides a comprehensive survey to capture the progression of advances in language models. In this paper, we examine the different aspects of language models, which started with a few million parameters but have reached the size of a trillion in a very short time. We also look at how these LLMs transitioned from task-specific to task-independent to task-and-language-independent architectures. This paper extensively discusses different pretraining objectives, benchmarks, and transfer learning methods used in LLMs. It also examines different finetuning and in-context learning techniques used in downstream tasks. Moreover, it explores how LLMs can perform well across many domains and datasets if sufficiently trained on a large and diverse dataset. Next, it discusses how, over time, the availability of cheap computational power and large datasets have improved LLM\u2019s capabilities and raised new challenges. As part of our study, we also inspect LLMs from the perspective of scalability to see how their performance is affected by the model\u2019s depth, width, and data size. Lastly, we provide an empirical comparison of existing trends and techniques and a comprehensive analysis of where the field of LLM currently stands.",
            "corpus_id": 268157336,
            "sentences": [
                {
                    "corpus_id": "268157336",
                    "title": "A Review of Current Trends, Techniques, and Challenges in Large Language Models (LLMs)",
                    "text": "Bias and fairness, if not adequately addressed, pose serious societal implications in the form of biased language generation and its impact on some segments of society. Basis can creep into LLMs from several sources discussed below. The first source of bias, dataset bias, stems from the datasets that were used to train the LLMs. If the datasets contain biases related to race, gender, religion, or socioeconomic status, the models inherit and amplify them. \n\nUnderrepresentation or misrepresentation of certain groups in the training data can lead to representation bias and biased language generation. The LLM developers should have checks and balances to ensure that all perspectives are adequately represented in the datasets. Otherwise, the model will produce inaccurate or skewed output for underrepresented groups. If the training data contain stereotypes, models amplify stereotyping and perpetuate prejudices. Fairness across demographics is a complex challenge but essential for advancing LLMs. \n\nCentextual bias stems from the context in which the language models are used. This poses severe and negative implications in applications such as recommender systems, employee hiring and promotions, clustering, and sentiment analysis. The model evaluation metrics and benchmarks used in traditional machine learning are inadequate to capture bias in LLMs. Comprehensive evaluation methods are needed to consider various aspects of bias in LLMs. A multifaceted approach is required to address bias and fairness issues in LLMs. Approaches to data curation, model development, evaluation strategies, and ethical issues need to be reexamined for their suitability for LLMs. Mitigating biases in the datasets using debiasing approaches such as modifying loss functions, altering training data distributions, and adversarial training requires LLM-contextualized research.",
                    "score": 0.5906638391088321,
                    "section_title": "Fairness",
                    "char_start_offset": 109399,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 168
                        },
                        {
                            "start": 169,
                            "end": 232
                        },
                        {
                            "start": 233,
                            "end": 330
                        },
                        {
                            "start": 331,
                            "end": 458
                        },
                        {
                            "start": 461,
                            "end": 604
                        },
                        {
                            "start": 605,
                            "end": 731
                        },
                        {
                            "start": 732,
                            "end": 822
                        },
                        {
                            "start": 823,
                            "end": 919
                        },
                        {
                            "start": 920,
                            "end": 1005
                        },
                        {
                            "start": 1008,
                            "end": 1085
                        },
                        {
                            "start": 1086,
                            "end": 1242
                        },
                        {
                            "start": 1243,
                            "end": 1363
                        },
                        {
                            "start": 1364,
                            "end": 1452
                        },
                        {
                            "start": 1453,
                            "end": 1533
                        },
                        {
                            "start": 1534,
                            "end": 1677
                        },
                        {
                            "start": 1678,
                            "end": 1873
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88720703125
                }
            ],
            "relevance_judgement": 0.88720703125,
            "relevance_judgment_input_expanded": "# Title: A Review of Current Trends, Techniques, and Challenges in Large Language Models (LLMs)\n# Venue: Applied Sciences\n# Authors: Rajvardhan Patil, Venkat Gudivada\n## Abstract\nNatural language processing (NLP) has significantly transformed in the last decade, especially in the field of language modeling. Large language models (LLMs) have achieved SOTA performances on natural language understanding (NLU) and natural language generation (NLG) tasks by learning language representation in self-supervised ways. This paper provides a comprehensive survey to capture the progression of advances in language models. In this paper, we examine the different aspects of language models, which started with a few million parameters but have reached the size of a trillion in a very short time. We also look at how these LLMs transitioned from task-specific to task-independent to task-and-language-independent architectures. This paper extensively discusses different pretraining objectives, benchmarks, and transfer learning methods used in LLMs. It also examines different finetuning and in-context learning techniques used in downstream tasks. Moreover, it explores how LLMs can perform well across many domains and datasets if sufficiently trained on a large and diverse dataset. Next, it discusses how, over time, the availability of cheap computational power and large datasets have improved LLM\u2019s capabilities and raised new challenges. As part of our study, we also inspect LLMs from the perspective of scalability to see how their performance is affected by the model\u2019s depth, width, and data size. Lastly, we provide an empirical comparison of existing trends and techniques and a comprehensive analysis of where the field of LLM currently stands.\n## Fairness\nBias and fairness, if not adequately addressed, pose serious societal implications in the form of biased language generation and its impact on some segments of society. Basis can creep into LLMs from several sources discussed below. The first source of bias, dataset bias, stems from the datasets that were used to train the LLMs. If the datasets contain biases related to race, gender, religion, or socioeconomic status, the models inherit and amplify them. \n\nUnderrepresentation or misrepresentation of certain groups in the training data can lead to representation bias and biased language generation. The LLM developers should have checks and balances to ensure that all perspectives are adequately represented in the datasets. Otherwise, the model will produce inaccurate or skewed output for underrepresented groups. If the training data contain stereotypes, models amplify stereotyping and perpetuate prejudices. Fairness across demographics is a complex challenge but essential for advancing LLMs. \n\nCentextual bias stems from the context in which the language models are used. This poses severe and negative implications in applications such as recommender systems, employee hiring and promotions, clustering, and sentiment analysis. The model evaluation metrics and benchmarks used in traditional machine learning are inadequate to capture bias in LLMs. Comprehensive evaluation methods are needed to consider various aspects of bias in LLMs. A multifaceted approach is required to address bias and fairness issues in LLMs. Approaches to data curation, model development, evaluation strategies, and ethical issues need to be reexamined for their suitability for LLMs. Mitigating biases in the datasets using debiasing approaches such as modifying loss functions, altering training data distributions, and adversarial training requires LLM-contextualized research.",
            "reference_string": "[268157336 | Patil et al. | 2024 | Citations: 80]"
        },
        {
            "title": "Towards an Enhanced Understanding of Bias in Pre-trained Neural Language Models: A Survey with Special Emphasis on Affective Bias",
            "venue": "arXiv.org",
            "year": 2022,
            "reference_count": 133,
            "citation_count": 10,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2204.10365",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2204.10365, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "144485480",
                    "name": "Anoop Kadan"
                },
                {
                    "authorId": "3390280",
                    "name": "Manjary P.Gangan"
                },
                {
                    "authorId": "145791688",
                    "name": "Deepak P"
                },
                {
                    "authorId": "151092338",
                    "name": "L. LajishV."
                }
            ],
            "abstract": "The remarkable progress in Natural Language Processing (NLP) brought about by deep learning, particularly with the recent advent of large pre-trained neural language models, is brought into scrutiny as several studies began to discuss and report potential biases in NLP applications. Bias in NLP is found to originate from latent historical biases encoded by humans into textual data which gets perpetuated or even amplified by NLP algorithm. We present a survey to comprehend bias in large pre-trained language models, analyze the stages at which they occur in these models, and various ways in which these biases could be quantified and mitigated. Considering wide applicability of textual affective computing based downstream tasks in real-world systems such as business, healthcare, education, etc., we give a special emphasis on investigating bias in the context of affect (emotion) i.e., Affective Bias, in large pre-trained language models. We present a summary of various bias evaluation corpora that help to aid future research and discuss challenges in the research on bias in pre-trained language models. We believe that our attempt to draw a comprehensive view of bias in pre-trained language models, and especially the exploration of affective bias will be highly beneficial to researchers interested in this evolving field.",
            "corpus_id": 248366292,
            "sentences": [
                {
                    "corpus_id": "248366292",
                    "title": "Towards an Enhanced Understanding of Bias in Pre-trained Neural Language Models: A Survey with Special Emphasis on Affective Bias",
                    "text": "The remarkable progress in Natural Language Processing (NLP) brought about by deep learning, particularly with the recent advent of large pre-trained neural language models, is brought into scrutiny as several studies began to discuss and report potential biases in NLP applications. Bias in NLP is found to originate from latent historical biases encoded by humans into textual data which gets perpetuated or even amplified by NLP algorithm. We present a survey to comprehend bias in large pre-trained language models, analyze the stages at which they occur in these models, and various ways in which these biases could be quantified and mitigated. Considering wide applicability of textual affective computing based downstream tasks in real-world systems such as business, healthcare, education, etc., we give a special emphasis on investigating bias in the context of affect (emotion) i.e., Affective Bias, in large pre-trained language models. We present a summary of various bias evaluation corpora that help to aid future research and discuss challenges in the research on bias in pre-trained language models. We believe that our attempt to draw a comprehensive view of bias in pre-trained language models, and especially the exploration of affective bias will be highly beneficial to researchers interested in this evolving field.",
                    "score": 0.601143248393121,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88671875
                },
                {
                    "corpus_id": "248366292",
                    "title": "Towards an Enhanced Understanding of Bias in Pre-trained Neural Language Models: A Survey with Special Emphasis on Affective Bias",
                    "text": "We also give special attention to the less explored area of social biases in the context of affect i.e., Affective Bias (or emotion associated bias) specific to large PLMs. Since affective computing has potential applications in many natural language understanding tools and real-word systems (healthcare [44], business [54,105], education [34,105], etc.), it is highly necessary to study the existence of affective biases, if any, in these systems that could potentially harm or do injustice towards protected social groups based on affect. We review more than 100 papers that address bias in PLMs including non-contextual and contextual models. We collect research papers from ACL anthology, Google Scholar and arXiv, using the keywords 'bias', 'fairness', 'bias in NLP', 'fairness in NLP', 'Sentiment bias', 'Affective bias', 'Emotion bias' 'bias in pre-trained language models', etc. as the inclusion criteria for our survey. The major contributions of this survey are summarised below: \n\n\u2022 We present a comprehensive survey of bias in pre-trained language models, especially an in-depth treatment of various kinds of bias that originate in transformer based contextual pre-trained language models in NLP along with their identification, quantification and mitigation strategies. \u2022 We, for the first time, to the best of our knowledge, investigate Affective Bias, a highly socially relevant and less addressed problem, specifically in the context of large pre-trained language models. \u2022 We collect and present a large number of available bias evaluation corpora along with their suitability to evaluate large pre-trained language models. \u2022 We also discuss present research challenges in large pre-trained language models and affective biases. \n\nThe rest of the paper is organized as, the background of PLMs and bias in PLMs provided in section 2, quantifying PLM bias in section 3, mitigating PLM bias in section 4, affective bias in PLMs including their identification and mitigation strategies in section 5, a list of available bias evaluation corpora in section 6, research challenges in section 7 and concluding remarks in section 8.",
                    "score": 0.6039360090741256,
                    "section_title": "Heterogeneous View of Bias in PLMs",
                    "char_start_offset": 5127,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 172
                        },
                        {
                            "start": 173,
                            "end": 541
                        },
                        {
                            "start": 542,
                            "end": 646
                        },
                        {
                            "start": 647,
                            "end": 929
                        },
                        {
                            "start": 930,
                            "end": 990
                        },
                        {
                            "start": 993,
                            "end": 1283
                        },
                        {
                            "start": 1284,
                            "end": 1488
                        },
                        {
                            "start": 1489,
                            "end": 1641
                        },
                        {
                            "start": 1642,
                            "end": 1746
                        },
                        {
                            "start": 1749,
                            "end": 2141
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 305,
                            "end": 309,
                            "matchedPaperCorpusId": "2313204"
                        },
                        {
                            "start": 320,
                            "end": 324,
                            "matchedPaperCorpusId": "32437011"
                        },
                        {
                            "start": 324,
                            "end": 328,
                            "matchedPaperCorpusId": "212705218"
                        },
                        {
                            "start": 340,
                            "end": 344,
                            "matchedPaperCorpusId": "192646023"
                        },
                        {
                            "start": 344,
                            "end": 348,
                            "matchedPaperCorpusId": "212705218"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.85302734375
                }
            ],
            "relevance_judgement": 0.88671875,
            "relevance_judgment_input_expanded": "# Title: Towards an Enhanced Understanding of Bias in Pre-trained Neural Language Models: A Survey with Special Emphasis on Affective Bias\n# Venue: arXiv.org\n# Authors: Anoop Kadan, Manjary P.Gangan, Deepak P, L. LajishV.\n## Abstract\nThe remarkable progress in Natural Language Processing (NLP) brought about by deep learning, particularly with the recent advent of large pre-trained neural language models, is brought into scrutiny as several studies began to discuss and report potential biases in NLP applications. Bias in NLP is found to originate from latent historical biases encoded by humans into textual data which gets perpetuated or even amplified by NLP algorithm. We present a survey to comprehend bias in large pre-trained language models, analyze the stages at which they occur in these models, and various ways in which these biases could be quantified and mitigated. Considering wide applicability of textual affective computing based downstream tasks in real-world systems such as business, healthcare, education, etc., we give a special emphasis on investigating bias in the context of affect (emotion) i.e., Affective Bias, in large pre-trained language models. We present a summary of various bias evaluation corpora that help to aid future research and discuss challenges in the research on bias in pre-trained language models. We believe that our attempt to draw a comprehensive view of bias in pre-trained language models, and especially the exploration of affective bias will be highly beneficial to researchers interested in this evolving field.\n## Heterogeneous View of Bias in PLMs\nWe also give special attention to the less explored area of social biases in the context of affect i.e., Affective Bias (or emotion associated bias) specific to large PLMs. Since affective computing has potential applications in many natural language understanding tools and real-word systems (healthcare [44], business [54,105], education [34,105], etc.), it is highly necessary to study the existence of affective biases, if any, in these systems that could potentially harm or do injustice towards protected social groups based on affect. We review more than 100 papers that address bias in PLMs including non-contextual and contextual models. We collect research papers from ACL anthology, Google Scholar and arXiv, using the keywords 'bias', 'fairness', 'bias in NLP', 'fairness in NLP', 'Sentiment bias', 'Affective bias', 'Emotion bias' 'bias in pre-trained language models', etc. as the inclusion criteria for our survey. The major contributions of this survey are summarised below: \n\n\u2022 We present a comprehensive survey of bias in pre-trained language models, especially an in-depth treatment of various kinds of bias that originate in transformer based contextual pre-trained language models in NLP along with their identification, quantification and mitigation strategies. \u2022 We, for the first time, to the best of our knowledge, investigate Affective Bias, a highly socially relevant and less addressed problem, specifically in the context of large pre-trained language models. \u2022 We collect and present a large number of available bias evaluation corpora along with their suitability to evaluate large pre-trained language models. \u2022 We also discuss present research challenges in large pre-trained language models and affective biases. \n\nThe rest of the paper is organized as, the background of PLMs and bias in PLMs provided in section 2, quantifying PLM bias in section 3, mitigating PLM bias in section 4, affective bias in PLMs including their identification and mitigation strategies in section 5, a list of available bias evaluation corpora in section 6, research challenges in section 7 and concluding remarks in section 8.",
            "reference_string": "[248366292 | Kadan et al. | 2022 | Citations: 10]"
        },
        {
            "title": "Social Debiasing for Fair Multi-modal LLMs",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 0,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.06569, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2149241557",
                    "name": "Harry Cheng"
                },
                {
                    "authorId": "1390575046",
                    "name": "Yangyang Guo"
                },
                {
                    "authorId": "2273322768",
                    "name": "Qingpei Guo"
                },
                {
                    "authorId": "2249834712",
                    "name": "Ming Yang"
                },
                {
                    "authorId": "2247906706",
                    "name": "Tian Gan"
                },
                {
                    "authorId": "2284688853",
                    "name": "Liqiang Nie"
                }
            ],
            "abstract": "Multi-modal Large Language Models (MLLMs) have advanced significantly, offering powerful vision-language understanding capabilities. However, these models often inherit severe social biases from their training datasets, leading to unfair predictions based on attributes like race and gender. This paper addresses the issue of social biases in MLLMs by i) Introducing a comprehensive Counterfactual dataset with Multiple Social Concepts (CMSC), which provides a more diverse and extensive training set compared to existing datasets. ii) Proposing an Anti-Stereotype Debiasing strategy (ASD). Our method works by revisiting the MLLM training process, rescaling the autoregressive loss function, and improving data sampling methods to counteract biases. Through extensive experiments on various MLLMs, our CMSC dataset and ASD method demonstrate a significant reduction in social biases while maintaining the models' original performance.",
            "corpus_id": 271859735,
            "sentences": [
                {
                    "corpus_id": "271859735",
                    "title": "Social Debiasing for Fair Multi-modal LLMs",
                    "text": "Multi-modal Large Language Models (MLLMs) have advanced significantly, offering powerful vision-language understanding capabilities. However, these models often inherit severe social biases from their training datasets, leading to unfair predictions based on attributes like race and gender. This paper addresses the issue of social biases in MLLMs by i) Introducing a comprehensive Counterfactual dataset with Multiple Social Concepts (CMSC), which provides a more diverse and extensive training set compared to existing datasets. ii) Proposing an Anti-Stereotype Debiasing strategy (ASD). Our method works by revisiting the MLLM training process, rescaling the autoregressive loss function, and improving data sampling methods to counteract biases. Through extensive experiments on various MLLMs, our CMSC dataset and ASD method demonstrate a significant reduction in social biases while maintaining the models' original performance.",
                    "score": 0.5994094810363939,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.884765625
                }
            ],
            "relevance_judgement": 0.884765625,
            "relevance_judgment_input_expanded": "# Title: Social Debiasing for Fair Multi-modal LLMs\n# Venue: arXiv.org\n# Authors: Harry Cheng, Yangyang Guo, Qingpei Guo, Ming Yang, Tian Gan, Liqiang Nie\n## Abstract\nMulti-modal Large Language Models (MLLMs) have advanced significantly, offering powerful vision-language understanding capabilities. However, these models often inherit severe social biases from their training datasets, leading to unfair predictions based on attributes like race and gender. This paper addresses the issue of social biases in MLLMs by i) Introducing a comprehensive Counterfactual dataset with Multiple Social Concepts (CMSC), which provides a more diverse and extensive training set compared to existing datasets. ii) Proposing an Anti-Stereotype Debiasing strategy (ASD). Our method works by revisiting the MLLM training process, rescaling the autoregressive loss function, and improving data sampling methods to counteract biases. Through extensive experiments on various MLLMs, our CMSC dataset and ASD method demonstrate a significant reduction in social biases while maintaining the models' original performance.\n",
            "reference_string": "[271859735 | Cheng et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Intersectional Bias in Causal Language Models",
            "venue": "arXiv.org",
            "year": 2021,
            "reference_count": 55,
            "citation_count": 33,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2107.07691, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2733075",
                    "name": "L. Magee"
                },
                {
                    "authorId": "9557084",
                    "name": "Lida Ghahremanlou"
                },
                {
                    "authorId": "13714096",
                    "name": "K. Soldati\u0107"
                },
                {
                    "authorId": "97868921",
                    "name": "S. Robertson"
                }
            ],
            "abstract": "To examine whether intersectional bias can be observed in language generation, we examine \\emph{GPT-2} and \\emph{GPT-NEO} models, ranging in size from 124 million to ~2.7 billion parameters. We conduct an experiment combining up to three social categories - gender, religion and disability - into unconditional or zero-shot prompts used to generate sentences that are then analysed for sentiment. Our results confirm earlier tests conducted with auto-regressive causal models, including the \\emph{GPT} family of models. We also illustrate why bias may be resistant to techniques that target single categories (e.g. gender, religion and race), as it can also manifest, in often subtle ways, in texts prompted by concatenated social categories. To address these difficulties, we suggest technical and community-based approaches need to combine to acknowledge and address complex and intersectional language model bias.",
            "corpus_id": 236034024,
            "sentences": [
                {
                    "corpus_id": "236034024",
                    "title": "Intersectional Bias in Causal Language Models",
                    "text": "We conclude that (1) bias exists at significant levels across different social categories (gender, religion and disability); (2) it can manifest, in unpredictable ways, at category intersections; and (3) it is resistant to increases in model size and training data diversity. Specifically, while gender bias is minor and heavily conditioned on prompt context, religion and disability bias is strongly evident and resistant to context, with Muslim and all disability labels other than 'with Down Syndrome' scoring worse than no label. While individual bias does contribute to intersectional scores and rankings, our results include exceptions. Topic modelling shows that sentiment scores also correspond with important qualitative changes in model predictions; just as with real-world experience, intersectional bias in language models manifests in different forms as well as degrees. \n\nWhile our results demonstrate the consistent presence of single category and intersectional bias across gender, disability and religion in several varieties and sizes of causal language models, they suffer from several limitations. As the results of prompt variations show, comparatively trivial additions can produce different overall sentiment as well as orderings between social categories, without erasing bias altogether. Since prompts and model hyperparameters will likely vary across language model applications -'storytelling' modes, chat Q&A sessions, generation of advertising copy, and so on -the task of identifying bias presence and intensity needs to be tailored to application and context. Although we control for prompt inclusion, metrics such as sentiment classification are also subject to biases of their own. In addition, the efficacy of fine-tuning and other bias mitigation strategies have not been evaluated systematically. Finally, other social distinctions -race, sexuality, class -will likely produce further kinds and degrees of bias, both at single category and intersectional levels. \n\nAs they become embedded in everyday computing, language models can be expected to produce pronounced social effects. Our contribution suggests intersectional bias in language models is not simply a factor of data set diversity, model size or architecture, and requires additional methods for identification and mitigation. In line with the concerns expressed by [49], with models becoming larger and more complex, the task of identifying intersectional bias will likely grow in difficulty. Moreover, individual cases show that intersectional bias cannot be easily estimated from singlecategory bias.",
                    "score": 0.8026333119260722,
                    "section_title": "Conclusion",
                    "char_start_offset": 46889,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 275
                        },
                        {
                            "start": 276,
                            "end": 533
                        },
                        {
                            "start": 534,
                            "end": 642
                        },
                        {
                            "start": 643,
                            "end": 883
                        },
                        {
                            "start": 886,
                            "end": 1117
                        },
                        {
                            "start": 1118,
                            "end": 1312
                        },
                        {
                            "start": 1313,
                            "end": 1590
                        },
                        {
                            "start": 1591,
                            "end": 1714
                        },
                        {
                            "start": 1715,
                            "end": 1832
                        },
                        {
                            "start": 1833,
                            "end": 1998
                        },
                        {
                            "start": 2001,
                            "end": 2117
                        },
                        {
                            "start": 2118,
                            "end": 2323
                        },
                        {
                            "start": 2324,
                            "end": 2490
                        },
                        {
                            "start": 2491,
                            "end": 2600
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 2363,
                            "end": 2367,
                            "matchedPaperCorpusId": "262580630"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8779296875
                }
            ],
            "relevance_judgement": 0.8779296875,
            "relevance_judgment_input_expanded": "# Title: Intersectional Bias in Causal Language Models\n# Venue: arXiv.org\n# Authors: L. Magee, Lida Ghahremanlou, K. Soldati\u0107, S. Robertson\n## Abstract\nTo examine whether intersectional bias can be observed in language generation, we examine \\emph{GPT-2} and \\emph{GPT-NEO} models, ranging in size from 124 million to ~2.7 billion parameters. We conduct an experiment combining up to three social categories - gender, religion and disability - into unconditional or zero-shot prompts used to generate sentences that are then analysed for sentiment. Our results confirm earlier tests conducted with auto-regressive causal models, including the \\emph{GPT} family of models. We also illustrate why bias may be resistant to techniques that target single categories (e.g. gender, religion and race), as it can also manifest, in often subtle ways, in texts prompted by concatenated social categories. To address these difficulties, we suggest technical and community-based approaches need to combine to acknowledge and address complex and intersectional language model bias.\n## Conclusion\nWe conclude that (1) bias exists at significant levels across different social categories (gender, religion and disability); (2) it can manifest, in unpredictable ways, at category intersections; and (3) it is resistant to increases in model size and training data diversity. Specifically, while gender bias is minor and heavily conditioned on prompt context, religion and disability bias is strongly evident and resistant to context, with Muslim and all disability labels other than 'with Down Syndrome' scoring worse than no label. While individual bias does contribute to intersectional scores and rankings, our results include exceptions. Topic modelling shows that sentiment scores also correspond with important qualitative changes in model predictions; just as with real-world experience, intersectional bias in language models manifests in different forms as well as degrees. \n\nWhile our results demonstrate the consistent presence of single category and intersectional bias across gender, disability and religion in several varieties and sizes of causal language models, they suffer from several limitations. As the results of prompt variations show, comparatively trivial additions can produce different overall sentiment as well as orderings between social categories, without erasing bias altogether. Since prompts and model hyperparameters will likely vary across language model applications -'storytelling' modes, chat Q&A sessions, generation of advertising copy, and so on -the task of identifying bias presence and intensity needs to be tailored to application and context. Although we control for prompt inclusion, metrics such as sentiment classification are also subject to biases of their own. In addition, the efficacy of fine-tuning and other bias mitigation strategies have not been evaluated systematically. Finally, other social distinctions -race, sexuality, class -will likely produce further kinds and degrees of bias, both at single category and intersectional levels. \n\nAs they become embedded in everyday computing, language models can be expected to produce pronounced social effects. Our contribution suggests intersectional bias in language models is not simply a factor of data set diversity, model size or architecture, and requires additional methods for identification and mitigation. In line with the concerns expressed by [49], with models becoming larger and more complex, the task of identifying intersectional bias will likely grow in difficulty. Moreover, individual cases show that intersectional bias cannot be easily estimated from singlecategory bias.",
            "reference_string": "[236034024 | Magee et al. | 2021 | Citations: 33]"
        },
        {
            "title": "Unveiling and Mitigating Bias in Mental Health Analysis with Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 56,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.12033, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2108035197",
                    "name": "Yuqing Wang"
                },
                {
                    "authorId": "2287025051",
                    "name": "Yun Zhao"
                },
                {
                    "authorId": "2307074530",
                    "name": "Sara Alessandra Keller"
                },
                {
                    "authorId": "114240050",
                    "name": "A. D. Hond"
                },
                {
                    "authorId": "1409083796",
                    "name": "Marieke M. van Buchem"
                },
                {
                    "authorId": "2282472554",
                    "name": "Malvika Pillai"
                },
                {
                    "authorId": "2250653096",
                    "name": "T. Hernandez-Boussard"
                }
            ],
            "abstract": "The advancement of large language models (LLMs) has demonstrated strong capabilities across various applications, including mental health analysis. However, existing studies have focused on predictive performance, leaving the critical issue of fairness underexplored, posing significant risks to vulnerable populations. Despite acknowledging potential biases, previous works have lacked thorough investigations into these biases and their impacts. To address this gap, we systematically evaluate biases across seven social factors (e.g., gender, age, religion) using ten LLMs with different prompting methods on eight diverse mental health datasets. Our results show that GPT-4 achieves the best overall balance in performance and fairness among LLMs, although it still lags behind domain-specific models like MentalRoBERTa in some cases. Additionally, our tailored fairness-aware prompts can effectively mitigate bias in mental health predictions, highlighting the great potential for fair analysis in this field.",
            "corpus_id": 270562923,
            "sentences": [
                {
                    "corpus_id": "270562923",
                    "title": "Unveiling and Mitigating Bias in Mental Health Analysis with Large Language Models",
                    "text": "The advancement of large language models (LLMs) has demonstrated strong capabilities across various applications, including mental health analysis. However, existing studies have focused on predictive performance, leaving the critical issue of fairness underexplored, posing significant risks to vulnerable populations. Despite acknowledging potential biases, previous works have lacked thorough investigations into these biases and their impacts. To address this gap, we systematically evaluate biases across seven social factors (e.g., gender, age, religion) using ten LLMs with different prompting methods on eight diverse mental health datasets. Our results show that GPT-4 achieves the best overall balance in performance and fairness among LLMs, although it still lags behind domain-specific models like MentalRoBERTa in some cases. Additionally, our tailored fairness-aware prompts can effectively mitigate bias in mental health predictions, highlighting the great potential for fair analysis in this field.",
                    "score": 0.5723789185047768,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.87744140625
                }
            ],
            "relevance_judgement": 0.87744140625,
            "relevance_judgment_input_expanded": "# Title: Unveiling and Mitigating Bias in Mental Health Analysis with Large Language Models\n# Venue: arXiv.org\n# Authors: Yuqing Wang, Yun Zhao, Sara Alessandra Keller, A. D. Hond, Marieke M. van Buchem, Malvika Pillai, T. Hernandez-Boussard\n## Abstract\nThe advancement of large language models (LLMs) has demonstrated strong capabilities across various applications, including mental health analysis. However, existing studies have focused on predictive performance, leaving the critical issue of fairness underexplored, posing significant risks to vulnerable populations. Despite acknowledging potential biases, previous works have lacked thorough investigations into these biases and their impacts. To address this gap, we systematically evaluate biases across seven social factors (e.g., gender, age, religion) using ten LLMs with different prompting methods on eight diverse mental health datasets. Our results show that GPT-4 achieves the best overall balance in performance and fairness among LLMs, although it still lags behind domain-specific models like MentalRoBERTa in some cases. Additionally, our tailored fairness-aware prompts can effectively mitigate bias in mental health predictions, highlighting the great potential for fair analysis in this field.\n",
            "reference_string": "[270562923 | Wang et al. | 2024 | Citations: 4]"
        },
        {
            "title": "Detecting Emergent Intersectional Biases: Contextualized Word Embeddings Contain a Distribution of Human-like Biases",
            "venue": "AAAI/ACM Conference on AI, Ethics, and Society",
            "year": 2020,
            "reference_count": 86,
            "citation_count": 243,
            "influential_citation_count": 19,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3461702.3462536",
                "status": "BRONZE",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2006.03955, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "28735341",
                    "name": "W. Guo"
                },
                {
                    "authorId": "144537437",
                    "name": "Aylin Caliskan"
                }
            ],
            "abstract": "With the starting point that implicit human biases are reflected in the statistical regularities of language, it is possible to measure biases in English static word embeddings. State-of-the-art neural language models generate dynamic word embeddings dependent on the context in which the word appears. Current methods measure pre-defined social and intersectional biases that occur in contexts defined by sentence templates. Dispensing with templates, we introduce the Contextualized Embedding Association Test (CEAT), that can summarize the magnitude of overall bias in neural language models by incorporating a random-effects model. Experiments on social and intersectional biases show that CEAT finds evidence of all tested biases and provides comprehensive information on the variance of effect magnitudes of the same bias in different contexts. All the models trained on English corpora that we study contain biased representations. GPT-2 contains the smallest magnitude of overall bias followed by GPT, BERT, and then ELMo, negatively correlating with the contextualization levels of the models. Furthermore, we develop two methods, Intersectional Bias Detection (IBD) and Emergent Intersectional Bias Detection (EIBD), to automatically identify the intersectional biases and emergent intersectional biases from static word embeddings in addition to measuring them in contextualized word embeddings. We present the first algorithmic bias detection findings on how intersectional group members are strongly associated with unique emergent biases that do not overlap with the biases of their constituent minority identities. IBD achieves an accuracy of 81.6% and 82.7%, respectively, when detecting the intersectional biases of African American females and Mexican American females, where the random correct identification rates are 14.3% and 13.3%. EIBD reaches an accuracy of 84.7% and 65.3%, respectively, when detecting the emergent intersectional biases unique to African American females and Mexican American females, where the random correct identification rates are 9.2% and 6.1%. Our results indicate that intersectional biases associated with members of multiple minority groups, such as African American females and Mexican American females, have the highest magnitude across all neural language models.",
            "corpus_id": 219530686,
            "sentences": [
                {
                    "corpus_id": "219530686",
                    "title": "Detecting Emergent Intersectional Biases: Contextualized Word Embeddings Contain a Distribution of Human-like Biases",
                    "text": "We introduce methods called IBD and EIBD to identify biases associated with members of multiple minority groups. These methods automatically detect the intersectional biases and emergent intersectional biases captured by word embeddings. Intersectional biases associated with African American and Mexican American females have the highest effect size compared to other social biases. Complementary to predefined sets of attributes to measure widely known biases, our methods automatically discover biases. IBD reaches an accuracy of 81.6% and 82.7% in detection, respectively, when validating on the intersectional biases of African American females and Mexican American females. EIBD reaches an accuracy of 84.7% and 65.3% in detection, respectively, when validating on the emergent intersectional biases of African American females and Mexican American females. \n\nWe present CEAT to measure biases identified by IBD and EIBD in language models. CEAT uses a random-effects model to comprehensively measure social biases embedded in neural language models that contain a distribution of contextdependent biases. CEAT simulates this distribution by sampling (N = 10, 000) combinations of CWEs without replacement from a large-scale natural language corpus. Unlike prior work that focuses on a limited number of contexts defined by templates to measure the magnitude of particular biases, CEAT provides a comprehensive measurement of overall bias in contextualizing language models. Our results indicate that ELMo is the most biased, followed by BERT, and GPT. GPT-2 is the least biased language model with respect to the social biases we investigate. The overall magnitude of bias negatively correlates with the level of contextualization in the language model. Understanding how the architecture of a language model contributes to biased and contextualized word representations can help mitigate the harmful effects to society in downstream applications.",
                    "score": 0.599789025208729,
                    "section_title": "Conclusion",
                    "char_start_offset": 53677,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 112
                        },
                        {
                            "start": 113,
                            "end": 237
                        },
                        {
                            "start": 238,
                            "end": 383
                        },
                        {
                            "start": 384,
                            "end": 505
                        },
                        {
                            "start": 506,
                            "end": 679
                        },
                        {
                            "start": 680,
                            "end": 863
                        },
                        {
                            "start": 866,
                            "end": 946
                        },
                        {
                            "start": 947,
                            "end": 1111
                        },
                        {
                            "start": 1112,
                            "end": 1255
                        },
                        {
                            "start": 1256,
                            "end": 1480
                        },
                        {
                            "start": 1481,
                            "end": 1558
                        },
                        {
                            "start": 1559,
                            "end": 1649
                        },
                        {
                            "start": 1650,
                            "end": 1760
                        },
                        {
                            "start": 1761,
                            "end": 1954
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.876953125
                }
            ],
            "relevance_judgement": 0.876953125,
            "relevance_judgment_input_expanded": "# Title: Detecting Emergent Intersectional Biases: Contextualized Word Embeddings Contain a Distribution of Human-like Biases\n# Venue: AAAI/ACM Conference on AI, Ethics, and Society\n# Authors: W. Guo, Aylin Caliskan\n## Abstract\nWith the starting point that implicit human biases are reflected in the statistical regularities of language, it is possible to measure biases in English static word embeddings. State-of-the-art neural language models generate dynamic word embeddings dependent on the context in which the word appears. Current methods measure pre-defined social and intersectional biases that occur in contexts defined by sentence templates. Dispensing with templates, we introduce the Contextualized Embedding Association Test (CEAT), that can summarize the magnitude of overall bias in neural language models by incorporating a random-effects model. Experiments on social and intersectional biases show that CEAT finds evidence of all tested biases and provides comprehensive information on the variance of effect magnitudes of the same bias in different contexts. All the models trained on English corpora that we study contain biased representations. GPT-2 contains the smallest magnitude of overall bias followed by GPT, BERT, and then ELMo, negatively correlating with the contextualization levels of the models. Furthermore, we develop two methods, Intersectional Bias Detection (IBD) and Emergent Intersectional Bias Detection (EIBD), to automatically identify the intersectional biases and emergent intersectional biases from static word embeddings in addition to measuring them in contextualized word embeddings. We present the first algorithmic bias detection findings on how intersectional group members are strongly associated with unique emergent biases that do not overlap with the biases of their constituent minority identities. IBD achieves an accuracy of 81.6% and 82.7%, respectively, when detecting the intersectional biases of African American females and Mexican American females, where the random correct identification rates are 14.3% and 13.3%. EIBD reaches an accuracy of 84.7% and 65.3%, respectively, when detecting the emergent intersectional biases unique to African American females and Mexican American females, where the random correct identification rates are 9.2% and 6.1%. Our results indicate that intersectional biases associated with members of multiple minority groups, such as African American females and Mexican American females, have the highest magnitude across all neural language models.\n## Conclusion\nWe introduce methods called IBD and EIBD to identify biases associated with members of multiple minority groups. These methods automatically detect the intersectional biases and emergent intersectional biases captured by word embeddings. Intersectional biases associated with African American and Mexican American females have the highest effect size compared to other social biases. Complementary to predefined sets of attributes to measure widely known biases, our methods automatically discover biases. IBD reaches an accuracy of 81.6% and 82.7% in detection, respectively, when validating on the intersectional biases of African American females and Mexican American females. EIBD reaches an accuracy of 84.7% and 65.3% in detection, respectively, when validating on the emergent intersectional biases of African American females and Mexican American females. \n\nWe present CEAT to measure biases identified by IBD and EIBD in language models. CEAT uses a random-effects model to comprehensively measure social biases embedded in neural language models that contain a distribution of contextdependent biases. CEAT simulates this distribution by sampling (N = 10, 000) combinations of CWEs without replacement from a large-scale natural language corpus. Unlike prior work that focuses on a limited number of contexts defined by templates to measure the magnitude of particular biases, CEAT provides a comprehensive measurement of overall bias in contextualizing language models. Our results indicate that ELMo is the most biased, followed by BERT, and GPT. GPT-2 is the least biased language model with respect to the social biases we investigate. The overall magnitude of bias negatively correlates with the level of contextualization in the language model. Understanding how the architecture of a language model contributes to biased and contextualized word representations can help mitigate the harmful effects to society in downstream applications.",
            "reference_string": "[219530686 | Guo et al. | 2020 | Citations: 243]"
        },
        {
            "title": "Mapping the Multilingual Margins: Intersectional Biases of Sentiment Analysis Systems in English, Spanish, and Arabic",
            "venue": "LTEDI",
            "year": 2022,
            "reference_count": 36,
            "citation_count": 21,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2204.03558",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2204.03558, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2077593669",
                    "name": "Antonio Camara"
                },
                {
                    "authorId": "70655120",
                    "name": "Nina Taneja"
                },
                {
                    "authorId": "1571776558",
                    "name": "Tamjeed Azad"
                },
                {
                    "authorId": "46208659",
                    "name": "Emily Allaway"
                },
                {
                    "authorId": "1804104",
                    "name": "R. Zemel"
                }
            ],
            "abstract": "As natural language processing systems become more widespread, it is necessary to address fairness issues in their implementation and deployment to ensure that their negative impacts on society are understood and minimized. However, there is limited work that studies fairness using a multilingual and intersectional framework or on downstream tasks. In this paper, we introduce four multilingual Equity Evaluation Corpora, supplementary test sets designed to measure social biases, and a novel statistical framework for studying unisectional and intersectional social biases in natural language processing. We use these tools to measure gender, racial, ethnic, and intersectional social biases across five models trained on emotion regression tasks in English, Spanish, and Arabic. We find that many systems demonstrate statistically significant unisectional and intersectional social biases. We make our code and datasets available for download.",
            "corpus_id": 248006238,
            "sentences": [
                {
                    "corpus_id": "248006238",
                    "title": "Mapping the Multilingual Margins: Intersectional Biases of Sentiment Analysis Systems in English, Spanish, and Arabic",
                    "text": "The presence and impact of harmful social biases in machine learning and natural language processing systems is pervasive and well-documented in popular word embedding methods (Caliskan et al., 2017;Garg et al., 2018;Bolukbasi et al., 2016; due to large amounts of humanproduced training data that includes historical social biases. Notably, Caliskan et al. (2017) demonstrate such biases by introducing the Word Embedding Association Test (WEAT) which measures how similar socially sensitive sets of words (e.g., racial or gendered names) are to attributive sets of words (e.g., pleasant or unpleasant words) in the semantic space encoded by word embeddings. While Bolukbasi et al. (2016); Manzini et al. (2019) introduce methods for \"debiasing\" word embeddings in order to create more equitable semantic representations for usage in downstream tasks, Gonen and Goldberg (2019) argue that such methods are merely cosmetic since social biases are still evident in the semantic space after the application of such methods. Moreover, these \"debiasing\" techniques focus on a particular social cleavage such as gender or race (i.e., unisectional cleavages). In contrast, our work considers both unisectional and intersectional social biases.\n\nRecent studies have also begun to focus on social biases in transformer-based language models (Kurita et al., 2019;Bender et al., 2021). In particular, Bender et al. (2021) discusses how increasingly large transformer-based language model in practice regurgitate their training data, resulting in such models perpetuating social biases and harming users. Therefore, in this work we consider both static word embedding techniques and transformerbased language models. Crenshaw (1990) introduces intersectionality as an analytical framework to study the complex character of the privilege and marginalization faced by an individual with a variety of identities across a set of social cleavages such as race and gender. A canonical usage of intersectionality is in service of studying the simultaneous racial and gender discrimination faced by Black women, which cannot be understood in its totality using racial or gendered frameworks independently",
                    "score": 0.7589603899548921,
                    "section_title": "Related Works",
                    "char_start_offset": 4374,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 176,
                            "end": 199,
                            "matchedPaperCorpusId": "23163324"
                        },
                        {
                            "start": 199,
                            "end": 217,
                            "matchedPaperCorpusId": "4930886"
                        },
                        {
                            "start": 217,
                            "end": 240,
                            "matchedPaperCorpusId": "1704893"
                        },
                        {
                            "start": 342,
                            "end": 364,
                            "matchedPaperCorpusId": "23163324"
                        },
                        {
                            "start": 666,
                            "end": 689,
                            "matchedPaperCorpusId": "1704893"
                        },
                        {
                            "start": 1333,
                            "end": 1354,
                            "matchedPaperCorpusId": "190000105"
                        },
                        {
                            "start": 1354,
                            "end": 1374,
                            "matchedPaperCorpusId": "262580630"
                        },
                        {
                            "start": 1391,
                            "end": 1411,
                            "matchedPaperCorpusId": "262580630"
                        },
                        {
                            "start": 1706,
                            "end": 1721,
                            "matchedPaperCorpusId": "24661090"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.87255859375
                }
            ],
            "relevance_judgement": 0.87255859375,
            "relevance_judgment_input_expanded": "# Title: Mapping the Multilingual Margins: Intersectional Biases of Sentiment Analysis Systems in English, Spanish, and Arabic\n# Venue: LTEDI\n# Authors: Antonio Camara, Nina Taneja, Tamjeed Azad, Emily Allaway, R. Zemel\n## Abstract\nAs natural language processing systems become more widespread, it is necessary to address fairness issues in their implementation and deployment to ensure that their negative impacts on society are understood and minimized. However, there is limited work that studies fairness using a multilingual and intersectional framework or on downstream tasks. In this paper, we introduce four multilingual Equity Evaluation Corpora, supplementary test sets designed to measure social biases, and a novel statistical framework for studying unisectional and intersectional social biases in natural language processing. We use these tools to measure gender, racial, ethnic, and intersectional social biases across five models trained on emotion regression tasks in English, Spanish, and Arabic. We find that many systems demonstrate statistically significant unisectional and intersectional social biases. We make our code and datasets available for download.\n## Related Works\nThe presence and impact of harmful social biases in machine learning and natural language processing systems is pervasive and well-documented in popular word embedding methods (Caliskan et al., 2017;Garg et al., 2018;Bolukbasi et al., 2016; due to large amounts of humanproduced training data that includes historical social biases. Notably, Caliskan et al. (2017) demonstrate such biases by introducing the Word Embedding Association Test (WEAT) which measures how similar socially sensitive sets of words (e.g., racial or gendered names) are to attributive sets of words (e.g., pleasant or unpleasant words) in the semantic space encoded by word embeddings. While Bolukbasi et al. (2016); Manzini et al. (2019) introduce methods for \"debiasing\" word embeddings in order to create more equitable semantic representations for usage in downstream tasks, Gonen and Goldberg (2019) argue that such methods are merely cosmetic since social biases are still evident in the semantic space after the application of such methods. Moreover, these \"debiasing\" techniques focus on a particular social cleavage such as gender or race (i.e., unisectional cleavages). In contrast, our work considers both unisectional and intersectional social biases.\n\nRecent studies have also begun to focus on social biases in transformer-based language models (Kurita et al., 2019;Bender et al., 2021). In particular, Bender et al. (2021) discusses how increasingly large transformer-based language model in practice regurgitate their training data, resulting in such models perpetuating social biases and harming users. Therefore, in this work we consider both static word embedding techniques and transformerbased language models. Crenshaw (1990) introduces intersectionality as an analytical framework to study the complex character of the privilege and marginalization faced by an individual with a variety of identities across a set of social cleavages such as race and gender. A canonical usage of intersectionality is in service of studying the simultaneous racial and gender discrimination faced by Black women, which cannot be understood in its totality using racial or gendered frameworks independently",
            "reference_string": "[248006238 | Camara et al. | 2022 | Citations: 21]"
        },
        {
            "title": "Evaluating Gender Bias in Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 24,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.09826, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2338278194",
                    "name": "Michael D\u00f6ll"
                },
                {
                    "authorId": "2338277213",
                    "name": "Markus D\u00f6hring"
                },
                {
                    "authorId": "2338675374",
                    "name": "Andreas M\u00fcller"
                }
            ],
            "abstract": "Gender bias in artificial intelligence has become an important issue, particularly in the context of language models used in communication-oriented applications. This study examines the extent to which Large Language Models (LLMs) exhibit gender bias in pronoun selection in occupational contexts. The analysis evaluates the models GPT-4, GPT-4o, PaLM 2 Text Bison and Gemini 1.0 Pro using a self-generated dataset. The jobs considered include a range of occupations, from those with a significant male presence to those with a notable female concentration, as well as jobs with a relatively equal gender distribution. Three different sentence processing methods were used to assess potential gender bias: masked tokens, unmasked sentences, and sentence completion. In addition, the LLMs suggested names of individuals in specific occupations, which were then examined for gender distribution. The results show a positive correlation between the models' pronoun choices and the gender distribution present in U.S. labor force data. Female pronouns were more often associated with female-dominated occupations, while male pronouns were more often associated with male-dominated occupations. Sentence completion showed the strongest correlation with actual gender distribution, while name generation resulted in a more balanced 'politically correct' gender distribution, albeit with notable variations in predominantly male or female occupations. Overall, the prompting method had a greater impact on gender distribution than the model selection itself, highlighting the complexity of addressing gender bias in LLMs. The findings highlight the importance of prompting in gender mapping.",
            "corpus_id": 274117151,
            "sentences": [
                {
                    "corpus_id": "274117151",
                    "title": "Evaluating Gender Bias in Large Language Models",
                    "text": "The rapid development of technology in the field of natural language processing (NLP) has led to significant breakthroughs, particularly in large language models (LLMs). The advent of transformer architectures represented a significant turning point, facilitating enhanced performance and efficiency compared to preceding recurrent and convolution-based models [1]. These developments have had a profound impact on the way machines understand and generate language. The applications of LLMs are diverse, covering a range of tasks such as sequence tagging, information extraction, machine translation, summarisation, and even complex natural language interactions in conversational systems [2]. LLMs are used in a wide range of domains, including healthcare, education, law and finance. Despite considerable progress and diverse applications, these systems are not without their challenges. One particular challenge is the presence of bias. LLMs learn from vast amounts of text collected from the Internet and other sources. This training data often contains unbalanced representations and stereotypes that are unintentionally embedded in the models [3]. \n\nBias in LLMs can harm both individuals and society, with consequences including allocative and representational harms. Allocative harms occur when resources or opportunities are distributed unfairly, often to the detriment of already marginalised groups. For example, recruitment algorithms may be skewed to disadvantage certain demographic groups. Representational harm occurs when groups are misrepresented or stereotyped, reinforcing harmful narratives and stigmas and further entrenching negative prejudices and societal biases [4]. Language models show a propensity for bias in a variety of areas, including but not limited to gender, age, sexual orientation, physical appearance, disability, nationality, ethnicity, socio-economic status, religion and culture. These biases have the potential to have significant consequences, particularly in contexts where they affect critical areas such as justice, healthcare or employment [5]. \n\nIn this study, we investigate gender bias in LLMs by analysing how these models select pronouns and generate names in professional contexts. Four models are examined: GPT-4 [6], GPT-4o [7], PaLM 2 Text Bison [8] and Gemini 1.0 Pro [9].",
                    "score": 0.8088926391884691,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 169
                        },
                        {
                            "start": 170,
                            "end": 365
                        },
                        {
                            "start": 366,
                            "end": 465
                        },
                        {
                            "start": 466,
                            "end": 693
                        },
                        {
                            "start": 694,
                            "end": 785
                        },
                        {
                            "start": 786,
                            "end": 889
                        },
                        {
                            "start": 890,
                            "end": 939
                        },
                        {
                            "start": 940,
                            "end": 1023
                        },
                        {
                            "start": 1024,
                            "end": 1153
                        },
                        {
                            "start": 1156,
                            "end": 1274
                        },
                        {
                            "start": 1275,
                            "end": 1410
                        },
                        {
                            "start": 1411,
                            "end": 1504
                        },
                        {
                            "start": 1505,
                            "end": 1692
                        },
                        {
                            "start": 1693,
                            "end": 1922
                        },
                        {
                            "start": 1923,
                            "end": 2093
                        },
                        {
                            "start": 2096,
                            "end": 2236
                        },
                        {
                            "start": 2237,
                            "end": 2331
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1149,
                            "end": 1152,
                            "matchedPaperCorpusId": "261530629"
                        },
                        {
                            "start": 1688,
                            "end": 1691,
                            "matchedPaperCorpusId": "235436386"
                        },
                        {
                            "start": 2089,
                            "end": 2092,
                            "matchedPaperCorpusId": "258688053"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8681640625
                }
            ],
            "relevance_judgement": 0.8681640625,
            "relevance_judgment_input_expanded": "# Title: Evaluating Gender Bias in Large Language Models\n# Venue: arXiv.org\n# Authors: Michael D\u00f6ll, Markus D\u00f6hring, Andreas M\u00fcller\n## Abstract\nGender bias in artificial intelligence has become an important issue, particularly in the context of language models used in communication-oriented applications. This study examines the extent to which Large Language Models (LLMs) exhibit gender bias in pronoun selection in occupational contexts. The analysis evaluates the models GPT-4, GPT-4o, PaLM 2 Text Bison and Gemini 1.0 Pro using a self-generated dataset. The jobs considered include a range of occupations, from those with a significant male presence to those with a notable female concentration, as well as jobs with a relatively equal gender distribution. Three different sentence processing methods were used to assess potential gender bias: masked tokens, unmasked sentences, and sentence completion. In addition, the LLMs suggested names of individuals in specific occupations, which were then examined for gender distribution. The results show a positive correlation between the models' pronoun choices and the gender distribution present in U.S. labor force data. Female pronouns were more often associated with female-dominated occupations, while male pronouns were more often associated with male-dominated occupations. Sentence completion showed the strongest correlation with actual gender distribution, while name generation resulted in a more balanced 'politically correct' gender distribution, albeit with notable variations in predominantly male or female occupations. Overall, the prompting method had a greater impact on gender distribution than the model selection itself, highlighting the complexity of addressing gender bias in LLMs. The findings highlight the importance of prompting in gender mapping.\n## Introduction\nThe rapid development of technology in the field of natural language processing (NLP) has led to significant breakthroughs, particularly in large language models (LLMs). The advent of transformer architectures represented a significant turning point, facilitating enhanced performance and efficiency compared to preceding recurrent and convolution-based models [1]. These developments have had a profound impact on the way machines understand and generate language. The applications of LLMs are diverse, covering a range of tasks such as sequence tagging, information extraction, machine translation, summarisation, and even complex natural language interactions in conversational systems [2]. LLMs are used in a wide range of domains, including healthcare, education, law and finance. Despite considerable progress and diverse applications, these systems are not without their challenges. One particular challenge is the presence of bias. LLMs learn from vast amounts of text collected from the Internet and other sources. This training data often contains unbalanced representations and stereotypes that are unintentionally embedded in the models [3]. \n\nBias in LLMs can harm both individuals and society, with consequences including allocative and representational harms. Allocative harms occur when resources or opportunities are distributed unfairly, often to the detriment of already marginalised groups. For example, recruitment algorithms may be skewed to disadvantage certain demographic groups. Representational harm occurs when groups are misrepresented or stereotyped, reinforcing harmful narratives and stigmas and further entrenching negative prejudices and societal biases [4]. Language models show a propensity for bias in a variety of areas, including but not limited to gender, age, sexual orientation, physical appearance, disability, nationality, ethnicity, socio-economic status, religion and culture. These biases have the potential to have significant consequences, particularly in contexts where they affect critical areas such as justice, healthcare or employment [5]. \n\nIn this study, we investigate gender bias in LLMs by analysing how these models select pronouns and generate names in professional contexts. Four models are examined: GPT-4 [6], GPT-4o [7], PaLM 2 Text Bison [8] and Gemini 1.0 Pro [9].",
            "reference_string": "[274117151 | Doll et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Benchmarking Intersectional Biases in NLP",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2022,
            "reference_count": 42,
            "citation_count": 48,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2022.naacl-main.263.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2022.naacl-main.263, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "9051130",
                    "name": "John P. Lalor"
                },
                {
                    "authorId": "2143685866",
                    "name": "Yi Yang"
                },
                {
                    "authorId": "2152595932",
                    "name": "Kendall Smith"
                },
                {
                    "authorId": "3047212",
                    "name": "Nicole Forsgren"
                },
                {
                    "authorId": "144849629",
                    "name": "A. Abbasi"
                }
            ],
            "abstract": "There has been a recent wave of work assessing the fairness of machine learning models in general, and more specifically, on natural language processing (NLP) models built using machine learning techniques. While much work has highlighted biases embedded in state-of-the-art language models, and more recent efforts have focused on how to debias, research assessing the fairness and performance of biased/debiased models on downstream prediction tasks has been limited. Moreover, most prior work has emphasized bias along a single dimension such as gender or race. In this work, we benchmark multiple NLP models with regards to their fairness and predictive performance across a variety of NLP tasks. In particular, we assess intersectional bias - fairness across multiple demographic dimensions. The results show that while current debiasing strategies fare well in terms of the fairness-accuracy trade-off (generally preserving predictive power in debiased models), they are unable to effectively alleviate bias in downstream tasks. Furthermore, this bias is often amplified across dimensions (i.e., intersections). We conclude by highlighting possible causes and making recommendations for future NLP debiasing research.",
            "corpus_id": 250390904,
            "sentences": [
                {
                    "corpus_id": "250390904",
                    "title": "Benchmarking Intersectional Biases in NLP",
                    "text": "There has been a recent wave of work assessing the fairness of machine learning models in general, and more specifically, on natural language processing (NLP) models built using machine learning techniques. While much work has highlighted biases embedded in state-of-the-art language models, and more recent efforts have focused on how to debias, research assessing the fairness and performance of biased/debiased models on downstream prediction tasks has been limited. Moreover, most prior work has emphasized bias along a single dimension such as gender or race. In this work, we benchmark multiple NLP models with regards to their fairness and predictive performance across a variety of NLP tasks. In particular, we assess intersectional bias - fairness across multiple demographic dimensions. The results show that while current debiasing strategies fare well in terms of the fairness-accuracy trade-off (generally preserving predictive power in debiased models), they are unable to effectively alleviate bias in downstream tasks. Furthermore, this bias is often amplified across dimensions (i.e., intersections). We conclude by highlighting possible causes and making recommendations for future NLP debiasing research.",
                    "score": 0.6293572149811717,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.86376953125
                }
            ],
            "relevance_judgement": 0.86376953125,
            "relevance_judgment_input_expanded": "# Title: Benchmarking Intersectional Biases in NLP\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: John P. Lalor, Yi Yang, Kendall Smith, Nicole Forsgren, A. Abbasi\n## Abstract\nThere has been a recent wave of work assessing the fairness of machine learning models in general, and more specifically, on natural language processing (NLP) models built using machine learning techniques. While much work has highlighted biases embedded in state-of-the-art language models, and more recent efforts have focused on how to debias, research assessing the fairness and performance of biased/debiased models on downstream prediction tasks has been limited. Moreover, most prior work has emphasized bias along a single dimension such as gender or race. In this work, we benchmark multiple NLP models with regards to their fairness and predictive performance across a variety of NLP tasks. In particular, we assess intersectional bias - fairness across multiple demographic dimensions. The results show that while current debiasing strategies fare well in terms of the fairness-accuracy trade-off (generally preserving predictive power in debiased models), they are unable to effectively alleviate bias in downstream tasks. Furthermore, this bias is often amplified across dimensions (i.e., intersections). We conclude by highlighting possible causes and making recommendations for future NLP debiasing research.\n",
            "reference_string": "[250390904 | Lalor et al. | 2022 | Citations: 48]"
        },
        {
            "title": "Understanding the Interplay of Scale, Data, and Bias in Language Models: A Case Study with BERT",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 53,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.21058, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2313945579",
                    "name": "Muhammad Ali"
                },
                {
                    "authorId": "1721493",
                    "name": "Swetasudha Panda"
                },
                {
                    "authorId": "2313919601",
                    "name": "Qinlan Shen"
                },
                {
                    "authorId": "2288680571",
                    "name": "Michael Wick"
                },
                {
                    "authorId": "2441120",
                    "name": "Ari Kobren"
                }
            ],
            "abstract": "In the current landscape of language model research, larger models, larger datasets and more compute seems to be the only way to advance towards intelligence. While there have been extensive studies of scaling laws and models' scaling behaviors, the effect of scale on a model's social biases and stereotyping tendencies has received less attention. In this study, we explore the influence of model scale and pre-training data on its learnt social biases. We focus on BERT -- an extremely popular language model -- and investigate biases as they show up during language modeling (upstream), as well as during classification applications after fine-tuning (downstream). Our experiments on four architecture sizes of BERT demonstrate that pre-training data substantially influences how upstream biases evolve with model scale. With increasing scale, models pre-trained on large internet scrapes like Common Crawl exhibit higher toxicity, whereas models pre-trained on moderated data sources like Wikipedia show greater gender stereotypes. However, downstream biases generally decrease with increasing model scale, irrespective of the pre-training data. Our results highlight the qualitative role of pre-training data in the biased behavior of language models, an often overlooked aspect in the study of scale. Through a detailed case study of BERT, we shed light on the complex interplay of data and model scale, and investigate how it translates to concrete biases.",
            "corpus_id": 271571036,
            "sentences": [
                {
                    "corpus_id": "271571036",
                    "title": "Understanding the Interplay of Scale, Data, and Bias in Language Models: A Case Study with BERT",
                    "text": "In the current landscape of language model research, larger models, larger datasets and more compute seems to be the only way to advance towards intelligence. While there have been extensive studies of scaling laws and models' scaling behaviors, the effect of scale on a model's social biases and stereotyping tendencies has received less attention. In this study, we explore the influence of model scale and pre-training data on its learnt social biases. We focus on BERT -- an extremely popular language model -- and investigate biases as they show up during language modeling (upstream), as well as during classification applications after fine-tuning (downstream). Our experiments on four architecture sizes of BERT demonstrate that pre-training data substantially influences how upstream biases evolve with model scale. With increasing scale, models pre-trained on large internet scrapes like Common Crawl exhibit higher toxicity, whereas models pre-trained on moderated data sources like Wikipedia show greater gender stereotypes. However, downstream biases generally decrease with increasing model scale, irrespective of the pre-training data. Our results highlight the qualitative role of pre-training data in the biased behavior of language models, an often overlooked aspect in the study of scale. Through a detailed case study of BERT, we shed light on the complex interplay of data and model scale, and investigate how it translates to concrete biases.",
                    "score": 0.614231762028417,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.86328125
                }
            ],
            "relevance_judgement": 0.86328125,
            "relevance_judgment_input_expanded": "# Title: Understanding the Interplay of Scale, Data, and Bias in Language Models: A Case Study with BERT\n# Venue: arXiv.org\n# Authors: Muhammad Ali, Swetasudha Panda, Qinlan Shen, Michael Wick, Ari Kobren\n## Abstract\nIn the current landscape of language model research, larger models, larger datasets and more compute seems to be the only way to advance towards intelligence. While there have been extensive studies of scaling laws and models' scaling behaviors, the effect of scale on a model's social biases and stereotyping tendencies has received less attention. In this study, we explore the influence of model scale and pre-training data on its learnt social biases. We focus on BERT -- an extremely popular language model -- and investigate biases as they show up during language modeling (upstream), as well as during classification applications after fine-tuning (downstream). Our experiments on four architecture sizes of BERT demonstrate that pre-training data substantially influences how upstream biases evolve with model scale. With increasing scale, models pre-trained on large internet scrapes like Common Crawl exhibit higher toxicity, whereas models pre-trained on moderated data sources like Wikipedia show greater gender stereotypes. However, downstream biases generally decrease with increasing model scale, irrespective of the pre-training data. Our results highlight the qualitative role of pre-training data in the biased behavior of language models, an often overlooked aspect in the study of scale. Through a detailed case study of BERT, we shed light on the complex interplay of data and model scale, and investigate how it translates to concrete biases.\n",
            "reference_string": "[271571036 | Ali et al. | 2024 | Citations: 3]"
        },
        {
            "title": "Developing Linguistic Patterns to Mitigate Inherent Human Bias in Offensive Language Detection",
            "venue": "Turkish J. Electr. Eng. Comput. Sci.",
            "year": 2023,
            "reference_count": 51,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.01787, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2189148265",
                    "name": "T. Tanyel"
                },
                {
                    "authorId": "2189146673",
                    "name": "Besher Alkurdi"
                },
                {
                    "authorId": "2460907",
                    "name": "S. Ayvaz"
                }
            ],
            "abstract": "With the proliferation of social media, there has been a sharp increase in offensive content, particularly targeting vulnerable groups, exacerbating social problems such as hatred, racism, and sexism. Detecting offensive language use is crucial to prevent offensive language from being widely shared on social media. However, the accurate detection of irony, implication, and various forms of hate speech on social media remains a challenge. Natural language-based deep learning models require extensive training with large, comprehensive, and labeled datasets. Unfortunately, manually creating such datasets is both costly and error-prone. Additionally, the presence of human-bias in offensive language datasets is a major concern for deep learning models. In this paper, we propose a linguistic data augmentation approach to reduce bias in labeling processes, which aims to mitigate the influence of human bias by leveraging the power of machines to improve the accuracy and fairness of labeling processes. This approach has the potential to improve offensive language classification tasks across multiple languages and reduce the prevalence of offensive content on social media.",
            "corpus_id": 265609662,
            "sentences": [
                {
                    "corpus_id": "265609662",
                    "title": "Developing Linguistic Patterns to Mitigate Inherent Human Bias in Offensive Language Detection",
                    "text": "For instance, the study [20] examined how gender distinctions are encoded in Turkish, a language that is also grammatically gender-neutral. The results uncovered by linguistics and psychology demonstrated the underlying reasons for the bias we encounter in language models. \n\nDiscrimination. is a long-standing problem in human societies and can take many forms. One of the more insidious ways in which discrimination can manifest is through natural language models, which are computational models that use statistical algorithms to understand and generate human language. These models have become ubiquitous in our daily lives, powering everything from virtual assistants to search engines and chatbots. However, as with any human-created system, natural language models are susceptible to biases and discrimination that can be embedded in the data they are trained on [21]. \n\nThe problem of discrimination in language models arises because these models are developed based on huge amounts of text data retrieved from the internet, which contains a plethora of societal biases and prejudices. For instance, language models might be trained on text data that contains racist or sexist language, which can lead to the model associating certain words or phrases with negative stereotypes. As a result, when the model is used to generate text or respond to queries, it may produce outputs that perpetuate harmful stereotypes and discriminatory practices. \n\nFurthermore, language models can also be discriminatory in their output if the training data is not diverse enough to represent different demographics or if the model is biased towards a particular dialect or language variety. This can result in language models that are more accurate for certain groups than for others, which can perpetuate existing power imbalances and inequalities. \n\nThe consequences of discriminatory language models can be significant, ranging from perpetuating harmful stereotypes to reinforcing systemic discrimination against marginalized groups [22]. As natural language models continue to become more widespread and integrated into our daily lives, it is necessary to address the problem of discrimination in these models and work towards creating more equitable and fair computational systems. This paper explores the causes and consequences of discrimination in language models and propose potential solutions to mitigate the problem.",
                    "score": 0.6262258481584757,
                    "section_title": "Introduction",
                    "char_start_offset": 7138,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 139
                        },
                        {
                            "start": 140,
                            "end": 273
                        },
                        {
                            "start": 276,
                            "end": 362
                        },
                        {
                            "start": 363,
                            "end": 572
                        },
                        {
                            "start": 573,
                            "end": 704
                        },
                        {
                            "start": 705,
                            "end": 875
                        },
                        {
                            "start": 878,
                            "end": 1093
                        },
                        {
                            "start": 1094,
                            "end": 1286
                        },
                        {
                            "start": 1287,
                            "end": 1451
                        },
                        {
                            "start": 1454,
                            "end": 1680
                        },
                        {
                            "start": 1681,
                            "end": 1839
                        },
                        {
                            "start": 1842,
                            "end": 2031
                        },
                        {
                            "start": 2032,
                            "end": 2276
                        },
                        {
                            "start": 2277,
                            "end": 2418
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 870,
                            "end": 874,
                            "matchedPaperCorpusId": "236950797"
                        },
                        {
                            "start": 2026,
                            "end": 2030,
                            "matchedPaperCorpusId": "215828184"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8603515625
                }
            ],
            "relevance_judgement": 0.8603515625,
            "relevance_judgment_input_expanded": "# Title: Developing Linguistic Patterns to Mitigate Inherent Human Bias in Offensive Language Detection\n# Venue: Turkish J. Electr. Eng. Comput. Sci.\n# Authors: T. Tanyel, Besher Alkurdi, S. Ayvaz\n## Abstract\nWith the proliferation of social media, there has been a sharp increase in offensive content, particularly targeting vulnerable groups, exacerbating social problems such as hatred, racism, and sexism. Detecting offensive language use is crucial to prevent offensive language from being widely shared on social media. However, the accurate detection of irony, implication, and various forms of hate speech on social media remains a challenge. Natural language-based deep learning models require extensive training with large, comprehensive, and labeled datasets. Unfortunately, manually creating such datasets is both costly and error-prone. Additionally, the presence of human-bias in offensive language datasets is a major concern for deep learning models. In this paper, we propose a linguistic data augmentation approach to reduce bias in labeling processes, which aims to mitigate the influence of human bias by leveraging the power of machines to improve the accuracy and fairness of labeling processes. This approach has the potential to improve offensive language classification tasks across multiple languages and reduce the prevalence of offensive content on social media.\n## Introduction\nFor instance, the study [20] examined how gender distinctions are encoded in Turkish, a language that is also grammatically gender-neutral. The results uncovered by linguistics and psychology demonstrated the underlying reasons for the bias we encounter in language models. \n\nDiscrimination. is a long-standing problem in human societies and can take many forms. One of the more insidious ways in which discrimination can manifest is through natural language models, which are computational models that use statistical algorithms to understand and generate human language. These models have become ubiquitous in our daily lives, powering everything from virtual assistants to search engines and chatbots. However, as with any human-created system, natural language models are susceptible to biases and discrimination that can be embedded in the data they are trained on [21]. \n\nThe problem of discrimination in language models arises because these models are developed based on huge amounts of text data retrieved from the internet, which contains a plethora of societal biases and prejudices. For instance, language models might be trained on text data that contains racist or sexist language, which can lead to the model associating certain words or phrases with negative stereotypes. As a result, when the model is used to generate text or respond to queries, it may produce outputs that perpetuate harmful stereotypes and discriminatory practices. \n\nFurthermore, language models can also be discriminatory in their output if the training data is not diverse enough to represent different demographics or if the model is biased towards a particular dialect or language variety. This can result in language models that are more accurate for certain groups than for others, which can perpetuate existing power imbalances and inequalities. \n\nThe consequences of discriminatory language models can be significant, ranging from perpetuating harmful stereotypes to reinforcing systemic discrimination against marginalized groups [22]. As natural language models continue to become more widespread and integrated into our daily lives, it is necessary to address the problem of discrimination in these models and work towards creating more equitable and fair computational systems. This paper explores the causes and consequences of discrimination in language models and propose potential solutions to mitigate the problem.",
            "reference_string": "[265609662 | Tanyel et al. | 2023 | Citations: 0]"
        },
        {
            "title": "The sociolinguistic foundations of language modeling",
            "venue": "Frontiers Artif. Intell.",
            "year": 2024,
            "reference_count": 166,
            "citation_count": 10,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.3389/frai.2024.1472411",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.09241, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2280139407",
                    "name": "Jack Grieve"
                },
                {
                    "authorId": "2311115214",
                    "name": "Sara Bartl"
                },
                {
                    "authorId": "2268357014",
                    "name": "Matteo Fuoli"
                },
                {
                    "authorId": "69370030",
                    "name": "Jason Grafmiller"
                },
                {
                    "authorId": "2280176696",
                    "name": "Weihang Huang"
                },
                {
                    "authorId": "2304488914",
                    "name": "A. Jawerbaum"
                },
                {
                    "authorId": "2280137448",
                    "name": "Akira Murakami"
                },
                {
                    "authorId": "2311114471",
                    "name": "Marcus Perlman"
                },
                {
                    "authorId": "2298903710",
                    "name": "Dana Roemling"
                },
                {
                    "authorId": "2320760943",
                    "name": "Bodo Winter"
                }
            ],
            "abstract": "In this article, we introduce a sociolinguistic perspective on language modeling. We claim that language models in general are inherently modeling varieties of language, and we consider how this insight can inform the development and deployment of language models. We begin by presenting a technical definition of the concept of a variety of language as developed in sociolinguistics. We then discuss how this perspective could help us better understand five basic challenges in language modeling: social bias, domain adaptation, alignment, language change, and scale. We argue that to maximize the performance and societal value of language models it is important to carefully compile training corpora that accurately represent the specific varieties of language being modeled, drawing on theories, methods, and descriptions from the field of sociolinguistics.",
            "corpus_id": 271161894,
            "sentences": [
                {
                    "corpus_id": "271161894",
                    "title": "The sociolinguistic foundations of language modeling",
                    "text": "NLP systems generally suffer from social bias: their real-world application leads to outcomes that unfairly disadvantage or harm specific social groups [58,59,60,61].Social bias can be introduced at various points during the development and deployment of NLP systems [62], but given the unsupervised nature of language modeling, training corpora are a key source of social bias in LLMs [10,63].While bias in NLP systems can harm people in various ways [59], in this section, we primarily focus on two common harmful outcomes of social bias.These two types of harms are most commonly discussed in terms of quality-of-service harms and stereotyping harms (e.g., [64,65,60,66]), although many different systems have been proposed for classifying biases and harms in NLP, which define these terms in somewhat different ways, along with many additional and often overlapping categories [59].Both of these types of harms are especially relevant to LLMs, and crucially, we believe both can be better understood and addressed in language modeling by adopting a sociolinguistic perspective (see Figure 3).First, social bias can be characterized by poor system performance for certain social groups that are interacting with LLMs and applications based on language models: token prediction will be more or less accurate depending on the social origins of the language inputted into the system.For example, ChatGPT might have difficulty correctly understanding prompts written by people from certain social groups due to their use of non-standard or socially restricted language patterns.This type of bias leads to what is known as quality-of-service harms, where the performance of these systems varies depending on the social background of the user [64,60].These types of quality-of-service harms can often be the product of selection bias, as they result from how training data is selected from across the society whose language is being modeled [58]: in general, if language data from certain social groups is under-represented in the training data for a language model, we should expect that NLP applications based on that model will process language structures produced by these groups less accurately and consequently exhibit poorer performance for these groups [59,67].",
                    "score": 0.6298418112303596,
                    "section_title": "Social Bias",
                    "char_start_offset": 18316,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 166
                        },
                        {
                            "start": 166,
                            "end": 394
                        },
                        {
                            "start": 394,
                            "end": 540
                        },
                        {
                            "start": 540,
                            "end": 886
                        },
                        {
                            "start": 886,
                            "end": 1096
                        },
                        {
                            "start": 1096,
                            "end": 1383
                        },
                        {
                            "start": 1383,
                            "end": 1577
                        },
                        {
                            "start": 1577,
                            "end": 1748
                        },
                        {
                            "start": 1748,
                            "end": 2266
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 152,
                            "end": 156,
                            "matchedPaperCorpusId": "209461005"
                        },
                        {
                            "start": 156,
                            "end": 159,
                            "matchedPaperCorpusId": "208783276"
                        },
                        {
                            "start": 159,
                            "end": 162,
                            "matchedPaperCorpusId": "252907216"
                        },
                        {
                            "start": 162,
                            "end": 165,
                            "matchedPaperCorpusId": "258688053"
                        },
                        {
                            "start": 267,
                            "end": 271,
                            "matchedPaperCorpusId": "237298625"
                        },
                        {
                            "start": 386,
                            "end": 390,
                            "matchedPaperCorpusId": "262580630"
                        },
                        {
                            "start": 390,
                            "end": 393,
                            "matchedPaperCorpusId": "258041203"
                        },
                        {
                            "start": 452,
                            "end": 456,
                            "matchedPaperCorpusId": "208783276"
                        },
                        {
                            "start": 667,
                            "end": 670,
                            "matchedPaperCorpusId": "252907216"
                        },
                        {
                            "start": 881,
                            "end": 885,
                            "matchedPaperCorpusId": "208783276"
                        },
                        {
                            "start": 1744,
                            "end": 1747,
                            "matchedPaperCorpusId": "252907216"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.85107421875
                }
            ],
            "relevance_judgement": 0.85107421875,
            "relevance_judgment_input_expanded": "# Title: The sociolinguistic foundations of language modeling\n# Venue: Frontiers Artif. Intell.\n# Authors: Jack Grieve, Sara Bartl, Matteo Fuoli, Jason Grafmiller, Weihang Huang, A. Jawerbaum, Akira Murakami, Marcus Perlman, Dana Roemling, Bodo Winter\n## Abstract\nIn this article, we introduce a sociolinguistic perspective on language modeling. We claim that language models in general are inherently modeling varieties of language, and we consider how this insight can inform the development and deployment of language models. We begin by presenting a technical definition of the concept of a variety of language as developed in sociolinguistics. We then discuss how this perspective could help us better understand five basic challenges in language modeling: social bias, domain adaptation, alignment, language change, and scale. We argue that to maximize the performance and societal value of language models it is important to carefully compile training corpora that accurately represent the specific varieties of language being modeled, drawing on theories, methods, and descriptions from the field of sociolinguistics.\n## Social Bias\nNLP systems generally suffer from social bias: their real-world application leads to outcomes that unfairly disadvantage or harm specific social groups [58,59,60,61].Social bias can be introduced at various points during the development and deployment of NLP systems [62], but given the unsupervised nature of language modeling, training corpora are a key source of social bias in LLMs [10,63].While bias in NLP systems can harm people in various ways [59], in this section, we primarily focus on two common harmful outcomes of social bias.These two types of harms are most commonly discussed in terms of quality-of-service harms and stereotyping harms (e.g., [64,65,60,66]), although many different systems have been proposed for classifying biases and harms in NLP, which define these terms in somewhat different ways, along with many additional and often overlapping categories [59].Both of these types of harms are especially relevant to LLMs, and crucially, we believe both can be better understood and addressed in language modeling by adopting a sociolinguistic perspective (see Figure 3).First, social bias can be characterized by poor system performance for certain social groups that are interacting with LLMs and applications based on language models: token prediction will be more or less accurate depending on the social origins of the language inputted into the system.For example, ChatGPT might have difficulty correctly understanding prompts written by people from certain social groups due to their use of non-standard or socially restricted language patterns.This type of bias leads to what is known as quality-of-service harms, where the performance of these systems varies depending on the social background of the user [64,60].These types of quality-of-service harms can often be the product of selection bias, as they result from how training data is selected from across the society whose language is being modeled [58]: in general, if language data from certain social groups is under-represented in the training data for a language model, we should expect that NLP applications based on that model will process language structures produced by these groups less accurately and consequently exhibit poorer performance for these groups [59,67].",
            "reference_string": "[271161894 | Grieve et al. | 2024 | Citations: 10]"
        },
        {
            "title": "Exposing Bias in Online Communities through Large-Scale Language Models",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 57,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2306.02294",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.02294, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2219550842",
                    "name": "Celine Wald"
                },
                {
                    "authorId": "32421394",
                    "name": "Lukas Pfahler"
                }
            ],
            "abstract": "Progress in natural language generation research has been shaped by the ever-growing size of language models. While large language models pre-trained on web data can generate human-sounding text, they also reproduce social biases and contribute to the propagation of harmful stereotypes. This work utilises the flaw of bias in language models to explore the biases of six different online communities. In order to get an insight into the communities' viewpoints, we fine-tune GPT-Neo 1.3B with six social media datasets. The bias of the resulting models is evaluated by prompting the models with different demographics and comparing the sentiment and toxicity values of these generations. Together, these methods reveal that bias differs in type and intensity for the various models. This work not only affirms how easily bias is absorbed from training data but also presents a scalable method to identify and compare the bias of different datasets or communities. Additionally, the examples generated for this work demonstrate the limitations of using automated sentiment and toxicity classifiers in bias research.",
            "corpus_id": 259075978,
            "sentences": [
                {
                    "corpus_id": "259075978",
                    "title": "Exposing Bias in Online Communities through Large-Scale Language Models",
                    "text": "Large language generation models such as GPT-2 (Radford et al. 2019) and GPT-3 (Brown et al. 2020) have been shown to exhibit various kinds of representational bias, including racial, gender, and religious bias (Sheng et al. 2019;Abid, Farooqi, and Zou 2021;Sheng et al. 2021). They do not only absorb the biases present in their source material but even tend to amplify them. Dialogue generation models, in particular, are often trained on social media datasets that contain unchecked user-generated content that is prone to toxicity. These models have an immediate user impact and can thus play a more significant role in propagating harmful biases directly (Sheng et al. 2021). \n\nDeploying biased language models in real-world contexts can lead to numerous potential negative consequences, even without malicious intent. They create more text that links marginalised communities to problematic stereotypes (Bender et al. 2021). Whether in media or personal conversations, language is the main source through which bias is shared amongst people (Beukeboom and Burgers 2019). Text created by biased language models can thus contribute to the perpetuation of bias in society. Furthermore, people from marginalised groups might be discouraged from using these technologies and from reaping the benefits artificial intelligence brings to society (Sheng et al. 2021). \n\nHowever, this flaw of language models creates an opportunity to analyse the stereotypes present in datasets. Language generation models ingest the contents of their training data. If we train a model on a biased dataset, the model should exhibit similar biases. Therefore, one can assess the biases and viewpoints expressed in the underlying datasets by evaluating the model's bias. \n\nThis work introduces an automated, reproducible method to compare the bias of different datasets or even of different communities. We fine-tune a pre-trained large language model with six social media datasets representing six different online communities. We obtain one fine-tuned model per dataset, which we then evaluate for bias using sentiment and toxicity values. The goal is to gain insight into these communities' attitudes by examining a language model that is based on conversations within the community.",
                    "score": 0.6197133503149725,
                    "section_title": "Introduction",
                    "char_start_offset": 1994,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 277
                        },
                        {
                            "start": 278,
                            "end": 376
                        },
                        {
                            "start": 377,
                            "end": 535
                        },
                        {
                            "start": 536,
                            "end": 680
                        },
                        {
                            "start": 683,
                            "end": 823
                        },
                        {
                            "start": 824,
                            "end": 930
                        },
                        {
                            "start": 931,
                            "end": 1076
                        },
                        {
                            "start": 1077,
                            "end": 1175
                        },
                        {
                            "start": 1176,
                            "end": 1364
                        },
                        {
                            "start": 1367,
                            "end": 1475
                        },
                        {
                            "start": 1476,
                            "end": 1546
                        },
                        {
                            "start": 1547,
                            "end": 1628
                        },
                        {
                            "start": 1629,
                            "end": 1749
                        },
                        {
                            "start": 1752,
                            "end": 1882
                        },
                        {
                            "start": 1883,
                            "end": 2008
                        },
                        {
                            "start": 2009,
                            "end": 2121
                        },
                        {
                            "start": 2122,
                            "end": 2266
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 79,
                            "end": 97,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 211,
                            "end": 230,
                            "matchedPaperCorpusId": "202537041"
                        },
                        {
                            "start": 230,
                            "end": 258,
                            "matchedPaperCorpusId": "231603388"
                        },
                        {
                            "start": 258,
                            "end": 276,
                            "matchedPaperCorpusId": "202537041"
                        },
                        {
                            "start": 660,
                            "end": 679,
                            "matchedPaperCorpusId": "202537041"
                        },
                        {
                            "start": 909,
                            "end": 929,
                            "matchedPaperCorpusId": "262580630"
                        },
                        {
                            "start": 1047,
                            "end": 1075,
                            "matchedPaperCorpusId": "53622943"
                        },
                        {
                            "start": 1344,
                            "end": 1363,
                            "matchedPaperCorpusId": "202537041"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8486328125
                },
                {
                    "corpus_id": "259075978",
                    "title": "Exposing Bias in Online Communities through Large-Scale Language Models",
                    "text": "With the emergence of deep learning technologies and increased computational power, the field of language models has seen remarkable advancements in recent years. Models have grown significantly in terms of parameters and training data, transforming not only the way human language is processed but how people interact with technology. Language models are able to generate increasingly coherent and human-sounding text, including complete essays and creative stories. Modern dialogue models like ChatGPT (Ope-nAI 2022) have a myriad of real-world applications, such as chatbots, voice assistants, and even online customer support. \n\nAs these models grow in relevance, it becomes increasingly important to address potentially harmful outputs. Research has shown that language models are prone to exhibit societal biases, reflecting the nature of the underlying training data written by humans. For this work, we focus on representational bias, which in the context of language models, occurs when there is an imbalance in the portrayal of different groups in LM-generated text (Blodgett et al. 2020;Liang et al. 2021). This can arise from stereotypes that cultivate negative generalisations about particular social groups and language that is used to degrade these groups. Ultimately, this may lead to certain groups being depicted less favourably than others. Bender et al. argue that even in large models with large datasets, diversity and unbiasedness are not guaranteed (Bender et al. 2021). Datasets are too big to be thoroughly documented, while marginalised social groups are more likely to be filtered out due to the nature of the filtering process. Phrases or dialects used by these groups are disproportionally categorised as hateful, resulting in their exclusion from training datasets (Sap et al. 2019;Bender et al. 2021). Hegemonic views are over-represented as Internet access is not evenly distributed, and datasets are often curated using male-dominated websites. Large language generation models such as GPT-2 (Radford et al. 2019) and GPT-3 (Brown et al. 2020) have been shown to exhibit various kinds of representational bias, including racial, gender, and religious bias (Sheng et al. 2019;Abid, Farooqi, and Zou 2021;Sheng et al. 2021).",
                    "score": 0.6350499520614017,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 162
                        },
                        {
                            "start": 163,
                            "end": 335
                        },
                        {
                            "start": 336,
                            "end": 467
                        },
                        {
                            "start": 468,
                            "end": 630
                        },
                        {
                            "start": 633,
                            "end": 741
                        },
                        {
                            "start": 742,
                            "end": 892
                        },
                        {
                            "start": 893,
                            "end": 1117
                        },
                        {
                            "start": 1118,
                            "end": 1271
                        },
                        {
                            "start": 1272,
                            "end": 1359
                        },
                        {
                            "start": 1360,
                            "end": 1494
                        },
                        {
                            "start": 1495,
                            "end": 1656
                        },
                        {
                            "start": 1657,
                            "end": 1833
                        },
                        {
                            "start": 1834,
                            "end": 1978
                        },
                        {
                            "start": 1979,
                            "end": 2256
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1076,
                            "end": 1098,
                            "matchedPaperCorpusId": "218971825"
                        },
                        {
                            "start": 1098,
                            "end": 1116,
                            "matchedPaperCorpusId": "235623756"
                        },
                        {
                            "start": 1473,
                            "end": 1493,
                            "matchedPaperCorpusId": "262580630"
                        },
                        {
                            "start": 1796,
                            "end": 1813,
                            "matchedPaperCorpusId": "196211238"
                        },
                        {
                            "start": 1813,
                            "end": 1832,
                            "matchedPaperCorpusId": "262580630"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8271484375
                }
            ],
            "relevance_judgement": 0.8486328125,
            "relevance_judgment_input_expanded": "# Title: Exposing Bias in Online Communities through Large-Scale Language Models\n# Venue: arXiv.org\n# Authors: Celine Wald, Lukas Pfahler\n## Abstract\nProgress in natural language generation research has been shaped by the ever-growing size of language models. While large language models pre-trained on web data can generate human-sounding text, they also reproduce social biases and contribute to the propagation of harmful stereotypes. This work utilises the flaw of bias in language models to explore the biases of six different online communities. In order to get an insight into the communities' viewpoints, we fine-tune GPT-Neo 1.3B with six social media datasets. The bias of the resulting models is evaluated by prompting the models with different demographics and comparing the sentiment and toxicity values of these generations. Together, these methods reveal that bias differs in type and intensity for the various models. This work not only affirms how easily bias is absorbed from training data but also presents a scalable method to identify and compare the bias of different datasets or communities. Additionally, the examples generated for this work demonstrate the limitations of using automated sentiment and toxicity classifiers in bias research.\n## Introduction\nWith the emergence of deep learning technologies and increased computational power, the field of language models has seen remarkable advancements in recent years. Models have grown significantly in terms of parameters and training data, transforming not only the way human language is processed but how people interact with technology. Language models are able to generate increasingly coherent and human-sounding text, including complete essays and creative stories. Modern dialogue models like ChatGPT (Ope-nAI 2022) have a myriad of real-world applications, such as chatbots, voice assistants, and even online customer support. \n\nAs these models grow in relevance, it becomes increasingly important to address potentially harmful outputs. Research has shown that language models are prone to exhibit societal biases, reflecting the nature of the underlying training data written by humans. For this work, we focus on representational bias, which in the context of language models, occurs when there is an imbalance in the portrayal of different groups in LM-generated text (Blodgett et al. 2020;Liang et al. 2021). This can arise from stereotypes that cultivate negative generalisations about particular social groups and language that is used to degrade these groups. Ultimately, this may lead to certain groups being depicted less favourably than others. Bender et al. argue that even in large models with large datasets, diversity and unbiasedness are not guaranteed (Bender et al. 2021). Datasets are too big to be thoroughly documented, while marginalised social groups are more likely to be filtered out due to the nature of the filtering process. Phrases or dialects used by these groups are disproportionally categorised as hateful, resulting in their exclusion from training datasets (Sap et al. 2019;Bender et al. 2021). Hegemonic views are over-represented as Internet access is not evenly distributed, and datasets are often curated using male-dominated websites. Large language generation models such as GPT-2 (Radford et al. 2019) and GPT-3 (Brown et al. 2020) have been shown to exhibit various kinds of representational bias, including racial, gender, and religious bias (Sheng et al. 2019;Abid, Farooqi, and Zou 2021;Sheng et al. 2021).\n...\nLarge language generation models such as GPT-2 (Radford et al. 2019) and GPT-3 (Brown et al. 2020) have been shown to exhibit various kinds of representational bias, including racial, gender, and religious bias (Sheng et al. 2019;Abid, Farooqi, and Zou 2021;Sheng et al. 2021). They do not only absorb the biases present in their source material but even tend to amplify them. Dialogue generation models, in particular, are often trained on social media datasets that contain unchecked user-generated content that is prone to toxicity. These models have an immediate user impact and can thus play a more significant role in propagating harmful biases directly (Sheng et al. 2021). \n\nDeploying biased language models in real-world contexts can lead to numerous potential negative consequences, even without malicious intent. They create more text that links marginalised communities to problematic stereotypes (Bender et al. 2021). Whether in media or personal conversations, language is the main source through which bias is shared amongst people (Beukeboom and Burgers 2019). Text created by biased language models can thus contribute to the perpetuation of bias in society. Furthermore, people from marginalised groups might be discouraged from using these technologies and from reaping the benefits artificial intelligence brings to society (Sheng et al. 2021). \n\nHowever, this flaw of language models creates an opportunity to analyse the stereotypes present in datasets. Language generation models ingest the contents of their training data. If we train a model on a biased dataset, the model should exhibit similar biases. Therefore, one can assess the biases and viewpoints expressed in the underlying datasets by evaluating the model's bias. \n\nThis work introduces an automated, reproducible method to compare the bias of different datasets or even of different communities. We fine-tune a pre-trained large language model with six social media datasets representing six different online communities. We obtain one fine-tuned model per dataset, which we then evaluate for bias using sentiment and toxicity values. The goal is to gain insight into these communities' attitudes by examining a language model that is based on conversations within the community.",
            "reference_string": "[259075978 | Wald et al. | 2023 | Citations: 6]"
        },
        {
            "title": "Breaking Bias, Building Bridges: Evaluation and Mitigation of Social Biases in LLMs via Contact Hypothesis",
            "venue": "AAAI/ACM Conference on AI, Ethics, and Society",
            "year": 2024,
            "reference_count": 22,
            "citation_count": 14,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.02030, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2261742076",
                    "name": "Chahat Raj"
                },
                {
                    "authorId": "2125631153",
                    "name": "A. Mukherjee"
                },
                {
                    "authorId": "2306632484",
                    "name": "Aylin Caliskan"
                },
                {
                    "authorId": "2261741456",
                    "name": "Antonios Anastasopoulos"
                },
                {
                    "authorId": "2261887816",
                    "name": "Ziwei Zhu"
                }
            ],
            "abstract": "Large Language Models (LLMs) perpetuate social biases, reflecting prejudices in their training data and reinforcing societal stereotypes and inequalities. Our work explores the potential of the Contact Hypothesis, a concept from social psychology for debiasing LLMs. We simulate various forms of social contact through LLM prompting to measure their influence on the model\u2019s biases, mirroring how intergroup interactions can reduce prejudices in social contexts. We create a dataset of 108,000 prompts following a principled approach replicating social contact to measure biases in three LLMs (LLaMA 2, Tulu, and NousHermes) across 13 social bias dimensions. We propose a unique debiasing technique, Social Contact Debiasing (SCD), that instruction-tunes these models with unbiased responses to prompts. Our research demonstrates that LLM responses exhibit social biases when subject to contact probing, but more importantly, these biases can be significantly reduced by up to 40% in 1 epoch of instruction tuning LLaMA 2 following our SCD strategy.",
            "corpus_id": 270878706,
            "sentences": [
                {
                    "corpus_id": "270878706",
                    "title": "Breaking Bias, Building Bridges: Evaluation and Mitigation of Social Biases in LLMs via Contact Hypothesis",
                    "text": "Large Language Models (LLMs) perpetuate social biases, reflecting prejudices in their training data and reinforcing societal stereotypes and inequalities. Our work explores the potential of the Contact Hypothesis, a concept from social psychology for debiasing LLMs. We simulate various forms of social contact through LLM prompting to measure their influence on the model\u2019s biases, mirroring how intergroup interactions can reduce prejudices in social contexts. We create a dataset of 108,000 prompts following a principled approach replicating social contact to measure biases in three LLMs (LLaMA 2, Tulu, and NousHermes) across 13 social bias dimensions. We propose a unique debiasing technique, Social Contact Debiasing (SCD), that instruction-tunes these models with unbiased responses to prompts. Our research demonstrates that LLM responses exhibit social biases when subject to contact probing, but more importantly, these biases can be significantly reduced by up to 40% in 1 epoch of instruction tuning LLaMA 2 following our SCD strategy.",
                    "score": 0.6014729943329018,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.845703125
                }
            ],
            "relevance_judgement": 0.845703125,
            "relevance_judgment_input_expanded": "# Title: Breaking Bias, Building Bridges: Evaluation and Mitigation of Social Biases in LLMs via Contact Hypothesis\n# Venue: AAAI/ACM Conference on AI, Ethics, and Society\n# Authors: Chahat Raj, A. Mukherjee, Aylin Caliskan, Antonios Anastasopoulos, Ziwei Zhu\n## Abstract\nLarge Language Models (LLMs) perpetuate social biases, reflecting prejudices in their training data and reinforcing societal stereotypes and inequalities. Our work explores the potential of the Contact Hypothesis, a concept from social psychology for debiasing LLMs. We simulate various forms of social contact through LLM prompting to measure their influence on the model\u2019s biases, mirroring how intergroup interactions can reduce prejudices in social contexts. We create a dataset of 108,000 prompts following a principled approach replicating social contact to measure biases in three LLMs (LLaMA 2, Tulu, and NousHermes) across 13 social bias dimensions. We propose a unique debiasing technique, Social Contact Debiasing (SCD), that instruction-tunes these models with unbiased responses to prompts. Our research demonstrates that LLM responses exhibit social biases when subject to contact probing, but more importantly, these biases can be significantly reduced by up to 40% in 1 epoch of instruction tuning LLaMA 2 following our SCD strategy.\n",
            "reference_string": "[270878706 | Raj et al. | 2024 | Citations: 14]"
        },
        {
            "title": "Should ChatGPT be Biased? Challenges and Risks of Bias in Large Language Models",
            "venue": "First Monday",
            "year": 2023,
            "reference_count": 163,
            "citation_count": 261,
            "influential_citation_count": 9,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://firstmonday.org/ojs/index.php/fm/article/download/13346/11369",
                "status": "HYBRID",
                "license": "CCBYNCSA",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2304.03738, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "48898287",
                    "name": "Emilio Ferrara"
                }
            ],
            "abstract": "As generative language models, exemplified by ChatGPT, continue to advance in their capabilities, the spotlight on biases inherent in these models intensifies. This paper delves into the distinctive challenges and risks associated with biases specifically in large-scale language models. We explore the origins of biases, stemming from factors such as training data, model specifications, algorithmic constraints, product design, and policy decisions. Our examination extends to the ethical implications arising from the unintended consequences of biased model outputs. In addition, we analyze the intricacies of mitigating biases, acknowledging the inevitable persistence of some biases, and consider the consequences of deploying these models across diverse applications, including virtual assistants, content generation, and chatbots. Finally, we provide an overview of current approaches for identifying, quantifying, and mitigating biases in language models, underscoring the need for a collaborative, multidisciplinary effort to craft AI systems that embody equity, transparency, and responsibility. This article aims to catalyze a thoughtful discourse within the AI community, prompting researchers and developers to consider the unique role of biases in the domain of generative language models and the ongoing quest for ethical AI.",
            "corpus_id": 258041203,
            "sentences": [
                {
                    "corpus_id": "258041203",
                    "title": "Should ChatGPT be Biased? Challenges and Risks of Bias in Large Language Models",
                    "text": "Bias in widely-adopted AI models, including ChatGPT and other generative language models, can have farreaching consequences that extend beyond the immediate context of their applications. When these models absorb and propagate biases, including those present in their training data, they may inadvertently reinforce stereotypes, marginalize certain groups, and lead to unfair treatment across various domains. Some examples of how biased AI models can adversely impact different areas include:\n\n\u2022 Hiring: AI-driven hiring tools that use biased models may exhibit unfair treatment towards applicants from underrepresented groups or those with non-traditional backgrounds. This could lead to the perpetuation of existing inequalities in the job market, limiting opportunities for affected individuals and reducing diversity in the workforce [102,103]. Large language models can be used to automate the screening of job applicants, such as by analyzing resumes and cover letters. Since these models are trained on vast amounts of text data, they may have internalized biases present in the data, such as gender or racial biases. As a result, they could unintentionally favor certain applicants or disqualify others based on factors unrelated to their qualifications, reinforcing existing inequalities in the job market.\n\n\u2022 Lending: Financial institutions increasingly rely on AI models for credit scoring and lending decisions. Biased models may unfairly penalize certain groups, such as minority communities or individuals with lower socioeconomic status, by assigning them lower credit scores or denying them access to loans and financial services based on biased assumptions [104,105,106]. In lending, large language models can be used to assess creditworthiness or predict loan default risk, e.g., based on automated analysis of application or support documents.\n\nIf the data used to train these models contain historical biases or discriminatory lending practices, the models may learn to replicate these patterns. Consequently, they could deny loans to certain demographics or offer unfavorable terms based on factors like race, gender, or socioeconomic status, perpetuating financial inequality [107].\n\n\u2022 Content moderation: AI-powered content moderation systems help manage and filter user-generated content on social media platforms and other online communities. If these systems are trained on biased data, they may disproportionately censor or suppress the voices of certain groups",
                    "score": 0.5774598743109363,
                    "section_title": "The risks of exacerbating existing societal biases",
                    "char_start_offset": 22565,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 844,
                            "end": 848,
                            "matchedPaperCorpusId": "195316573"
                        },
                        {
                            "start": 1675,
                            "end": 1680,
                            "matchedPaperCorpusId": "61799887"
                        },
                        {
                            "start": 1684,
                            "end": 1688,
                            "matchedPaperCorpusId": "51734899"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.845703125
                }
            ],
            "relevance_judgement": 0.845703125,
            "relevance_judgment_input_expanded": "# Title: Should ChatGPT be Biased? Challenges and Risks of Bias in Large Language Models\n# Venue: First Monday\n# Authors: Emilio Ferrara\n## Abstract\nAs generative language models, exemplified by ChatGPT, continue to advance in their capabilities, the spotlight on biases inherent in these models intensifies. This paper delves into the distinctive challenges and risks associated with biases specifically in large-scale language models. We explore the origins of biases, stemming from factors such as training data, model specifications, algorithmic constraints, product design, and policy decisions. Our examination extends to the ethical implications arising from the unintended consequences of biased model outputs. In addition, we analyze the intricacies of mitigating biases, acknowledging the inevitable persistence of some biases, and consider the consequences of deploying these models across diverse applications, including virtual assistants, content generation, and chatbots. Finally, we provide an overview of current approaches for identifying, quantifying, and mitigating biases in language models, underscoring the need for a collaborative, multidisciplinary effort to craft AI systems that embody equity, transparency, and responsibility. This article aims to catalyze a thoughtful discourse within the AI community, prompting researchers and developers to consider the unique role of biases in the domain of generative language models and the ongoing quest for ethical AI.\n## The risks of exacerbating existing societal biases\nBias in widely-adopted AI models, including ChatGPT and other generative language models, can have farreaching consequences that extend beyond the immediate context of their applications. When these models absorb and propagate biases, including those present in their training data, they may inadvertently reinforce stereotypes, marginalize certain groups, and lead to unfair treatment across various domains. Some examples of how biased AI models can adversely impact different areas include:\n\n\u2022 Hiring: AI-driven hiring tools that use biased models may exhibit unfair treatment towards applicants from underrepresented groups or those with non-traditional backgrounds. This could lead to the perpetuation of existing inequalities in the job market, limiting opportunities for affected individuals and reducing diversity in the workforce [102,103]. Large language models can be used to automate the screening of job applicants, such as by analyzing resumes and cover letters. Since these models are trained on vast amounts of text data, they may have internalized biases present in the data, such as gender or racial biases. As a result, they could unintentionally favor certain applicants or disqualify others based on factors unrelated to their qualifications, reinforcing existing inequalities in the job market.\n\n\u2022 Lending: Financial institutions increasingly rely on AI models for credit scoring and lending decisions. Biased models may unfairly penalize certain groups, such as minority communities or individuals with lower socioeconomic status, by assigning them lower credit scores or denying them access to loans and financial services based on biased assumptions [104,105,106]. In lending, large language models can be used to assess creditworthiness or predict loan default risk, e.g., based on automated analysis of application or support documents.\n\nIf the data used to train these models contain historical biases or discriminatory lending practices, the models may learn to replicate these patterns. Consequently, they could deny loans to certain demographics or offer unfavorable terms based on factors like race, gender, or socioeconomic status, perpetuating financial inequality [107].\n\n\u2022 Content moderation: AI-powered content moderation systems help manage and filter user-generated content on social media platforms and other online communities. If these systems are trained on biased data, they may disproportionately censor or suppress the voices of certain groups",
            "reference_string": "[258041203 | Ferrara | 2023 | Citations: 261]"
        },
        {
            "title": "A Predictive Factor Analysis of Social Biases and Task-Performance in Pretrained Masked Language Models",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2023,
            "reference_count": 84,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.12936, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "32066669",
                    "name": "Yi Zhou"
                },
                {
                    "authorId": "1387447871",
                    "name": "Jos\u00e9 Camacho-Collados"
                },
                {
                    "authorId": "2075356592",
                    "name": "D. Bollegala"
                }
            ],
            "abstract": "Various types of social biases have been reported with pretrained Masked Language Models (MLMs) in prior work. However, multiple underlying factors are associated with an MLM such as its model size, size of the training data, training objectives, the domain from which pretraining data is sampled, tokenization, and languages present in the pretrained corpora, to name a few. It remains unclear as to which of those factors influence social biases that are learned by MLMs. To study the relationship between model factors and the social biases learned by an MLM, as well as the downstream task performance of the model, we conduct a comprehensive study over 39 pretrained MLMs covering different model sizes, training objectives, tokenization methods, training data domains and languages. Our results shed light on important factors often neglected in prior literature, such as tokenization or model objectives.",
            "corpus_id": 264306000,
            "sentences": [
                {
                    "corpus_id": "264306000",
                    "title": "A Predictive Factor Analysis of Social Biases and Task-Performance in Pretrained Masked Language Models",
                    "text": "Various types of social biases have been reported with pretrained Masked Language Models (MLMs) in prior work. However, multiple underlying factors are associated with an MLM such as its model size, size of the training data, training objectives, the domain from which pretraining data is sampled, tokenization, and languages present in the pretrained corpora, to name a few. It remains unclear as to which of those factors influence social biases that are learned by MLMs. To study the relationship between model factors and the social biases learned by an MLM, as well as the downstream task performance of the model, we conduct a comprehensive study over 39 pretrained MLMs covering different model sizes, training objectives, tokenization methods, training data domains and languages. Our results shed light on important factors often neglected in prior literature, such as tokenization or model objectives.",
                    "score": 0.5728940596836992,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.84521484375
                }
            ],
            "relevance_judgement": 0.84521484375,
            "relevance_judgment_input_expanded": "# Title: A Predictive Factor Analysis of Social Biases and Task-Performance in Pretrained Masked Language Models\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Yi Zhou, Jos\u00e9 Camacho-Collados, D. Bollegala\n## Abstract\nVarious types of social biases have been reported with pretrained Masked Language Models (MLMs) in prior work. However, multiple underlying factors are associated with an MLM such as its model size, size of the training data, training objectives, the domain from which pretraining data is sampled, tokenization, and languages present in the pretrained corpora, to name a few. It remains unclear as to which of those factors influence social biases that are learned by MLMs. To study the relationship between model factors and the social biases learned by an MLM, as well as the downstream task performance of the model, we conduct a comprehensive study over 39 pretrained MLMs covering different model sizes, training objectives, tokenization methods, training data domains and languages. Our results shed light on important factors often neglected in prior literature, such as tokenization or model objectives.\n",
            "reference_string": "[264306000 | Zhou et al. | 2023 | Citations: 6]"
        },
        {
            "title": "Gender Bias in Transformer Models: A comprehensive survey",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 65,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2306.10530",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.10530, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2156841973",
                    "name": "Praneeth Nemani"
                },
                {
                    "authorId": "2220288860",
                    "name": "Yericherla Deepak Joel"
                },
                {
                    "authorId": "41153415",
                    "name": "Pallavi Vijay"
                },
                {
                    "authorId": "3445542",
                    "name": "Farhana Ferdousi Liza"
                }
            ],
            "abstract": "Gender bias in artificial intelligence (AI) has emerged as a pressing concern with profound implications for individuals' lives. This paper presents a comprehensive survey that explores gender bias in Transformer models from a linguistic perspective. While the existence of gender bias in language models has been acknowledged in previous studies, there remains a lack of consensus on how to effectively measure and evaluate this bias. Our survey critically examines the existing literature on gender bias in Transformers, shedding light on the diverse methodologies and metrics employed to assess bias. Several limitations in current approaches to measuring gender bias in Transformers are identified, encompassing the utilization of incomplete or flawed metrics, inadequate dataset sizes, and a dearth of standardization in evaluation methods. Furthermore, our survey delves into the potential ramifications of gender bias in Transformers for downstream applications, including dialogue systems and machine translation. We underscore the importance of fostering equity and fairness in these systems by emphasizing the need for heightened awareness and accountability in developing and deploying language technologies. This paper serves as a comprehensive overview of gender bias in Transformer models, providing novel insights and offering valuable directions for future research in this critical domain.",
            "corpus_id": 259203321,
            "sentences": [
                {
                    "corpus_id": "259203321",
                    "title": "Gender Bias in Transformer Models: A comprehensive survey",
                    "text": "Bias from input representations in NLP refers to the introduction of bias into the input data used for training and evaluating NLP models. These biases, often referred to as semantic biases, can arise from various sources and have Studies have demonstrated that word embeddings can detect racial and gender biases, even in well-labeled and balanced datasets. These biases can arise due to societal prejudices reflected in the training data, leading to biased predictions and outputs from NLP models. Contextual representations learned by large pre-trained language models, such as BERT and GPT, are also susceptible to biases. These models are typically trained on vast amounts of internet text, including societal biases in online content. Consequently, these models can replicate and perpetuate biases, often mirroring societal biases. Numerous studies have documented and quantified biases in NLP models and their input representations, highlighting the importance of addressing these issues. Recognizing and understanding the biases generated during the data generation process is a critical step towards mitigating them. Addressing bias in input representations requires a multifaceted approach. It involves diversifying training data sources to ensure the representation of various demographic groups and perspectives. Regularly evaluating and auditing the models for biases and developing debiasing techniques are crucial to mitigating these biases. The above-mentioned biases are collectively represented as the biases generated in the process of data generation, which can be depicted in Fig. 4 D. Model Bias Languages are dynamic and constantly evolving, making capturing their complexity and nuances challenging even with a large dataset. Using a small subset of data can only provide a limited and temporary snapshot of language, which is why relying solely on \"better\" training data is not a comprehensive solution to address bias in NLP models. Furthermore, machine learning models tend to amplify the behaviors and patterns they are exposed to, including biases present in the training data. Studies such as [30], [31] have explored the compounding effect of bias in newer models, highlighting the phenomenon known as bias overamplification. This refers to the tendency of machine learning models to disproportionately amplify and reinforce biases rather than mitigate them.",
                    "score": 0.7443059851015951,
                    "section_title": "C. Bias from Input Representations",
                    "char_start_offset": 19447,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 138
                        },
                        {
                            "start": 139,
                            "end": 358
                        },
                        {
                            "start": 359,
                            "end": 499
                        },
                        {
                            "start": 500,
                            "end": 626
                        },
                        {
                            "start": 627,
                            "end": 740
                        },
                        {
                            "start": 741,
                            "end": 837
                        },
                        {
                            "start": 838,
                            "end": 995
                        },
                        {
                            "start": 996,
                            "end": 1125
                        },
                        {
                            "start": 1126,
                            "end": 1200
                        },
                        {
                            "start": 1201,
                            "end": 1324
                        },
                        {
                            "start": 1325,
                            "end": 1456
                        },
                        {
                            "start": 1457,
                            "end": 1749
                        },
                        {
                            "start": 1750,
                            "end": 1958
                        },
                        {
                            "start": 1959,
                            "end": 2106
                        },
                        {
                            "start": 2107,
                            "end": 2256
                        },
                        {
                            "start": 2257,
                            "end": 2389
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 2123,
                            "end": 2127,
                            "matchedPaperCorpusId": "21670658"
                        },
                        {
                            "start": 2129,
                            "end": 2133,
                            "matchedPaperCorpusId": "220047833"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.83984375
                },
                {
                    "corpus_id": "259203321",
                    "title": "Gender Bias in Transformer Models: A comprehensive survey",
                    "text": "The issue of gender bias in various NLP tasks is a matter of significant concern, and it is essential to categorize and summarize the different forms of bias that can arise. Researchers have identified three primary categories of bias: Denigration (A1), Stereotyping (A2), and Under-representation (A3). Denigration involves the usage of racial, ethnic, or religious slurs, which can often be observed as a prevalent method of cyberbullying. This type of bias manifests as derogatory language aimed at demeaning specific groups. Stereotyping, on the other hand, refers to individuals' cognitive representation of a particular social group. In NLP tasks, stereotyping can manifest through word embeddings and large-scale language models, where certain biases are learned and perpetuated in the resulting representations. Under-representation pertains to the absence of identifiable group members from representative bodies and well-being indicators in proportion to their population's size. This type of bias highlights the disparities in visibility and inclusivity within various domains. Researchers have extensively studied these categories of bias to better understand their nature and mitigate their negative impact on society. By recognizing and categorizing different types of bias, researchers and practitioners can develop effective strategies and techniques to address gender bias in NLP. The above-mentioned NLP tasks can be represented and examined using the terms specified in Table I. This categorization facilitates a clearer understanding of how gender bias can manifest in different NLP applications, thereby guiding efforts to combat and mitigate bias effectively. By comprehensively analyzing and addressing the various types of bias, researchers and practitioners can contribute to the development of more equitable and fair NLP systems and applications, promoting inclusivity and fostering unbiased linguistic representations.",
                    "score": 0.6421407892172782,
                    "section_title": "II. REPRESENTATION OF GENDER BIAS",
                    "char_start_offset": 12151,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 173
                        },
                        {
                            "start": 174,
                            "end": 303
                        },
                        {
                            "start": 304,
                            "end": 441
                        },
                        {
                            "start": 442,
                            "end": 528
                        },
                        {
                            "start": 529,
                            "end": 639
                        },
                        {
                            "start": 640,
                            "end": 819
                        },
                        {
                            "start": 820,
                            "end": 989
                        },
                        {
                            "start": 990,
                            "end": 1088
                        },
                        {
                            "start": 1089,
                            "end": 1231
                        },
                        {
                            "start": 1232,
                            "end": 1397
                        },
                        {
                            "start": 1398,
                            "end": 1497
                        },
                        {
                            "start": 1498,
                            "end": 1681
                        },
                        {
                            "start": 1682,
                            "end": 1946
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.83544921875
                }
            ],
            "relevance_judgement": 0.83984375,
            "relevance_judgment_input_expanded": "# Title: Gender Bias in Transformer Models: A comprehensive survey\n# Venue: arXiv.org\n# Authors: Praneeth Nemani, Yericherla Deepak Joel, Pallavi Vijay, Farhana Ferdousi Liza\n## Abstract\nGender bias in artificial intelligence (AI) has emerged as a pressing concern with profound implications for individuals' lives. This paper presents a comprehensive survey that explores gender bias in Transformer models from a linguistic perspective. While the existence of gender bias in language models has been acknowledged in previous studies, there remains a lack of consensus on how to effectively measure and evaluate this bias. Our survey critically examines the existing literature on gender bias in Transformers, shedding light on the diverse methodologies and metrics employed to assess bias. Several limitations in current approaches to measuring gender bias in Transformers are identified, encompassing the utilization of incomplete or flawed metrics, inadequate dataset sizes, and a dearth of standardization in evaluation methods. Furthermore, our survey delves into the potential ramifications of gender bias in Transformers for downstream applications, including dialogue systems and machine translation. We underscore the importance of fostering equity and fairness in these systems by emphasizing the need for heightened awareness and accountability in developing and deploying language technologies. This paper serves as a comprehensive overview of gender bias in Transformer models, providing novel insights and offering valuable directions for future research in this critical domain.\n## II. REPRESENTATION OF GENDER BIAS\nThe issue of gender bias in various NLP tasks is a matter of significant concern, and it is essential to categorize and summarize the different forms of bias that can arise. Researchers have identified three primary categories of bias: Denigration (A1), Stereotyping (A2), and Under-representation (A3). Denigration involves the usage of racial, ethnic, or religious slurs, which can often be observed as a prevalent method of cyberbullying. This type of bias manifests as derogatory language aimed at demeaning specific groups. Stereotyping, on the other hand, refers to individuals' cognitive representation of a particular social group. In NLP tasks, stereotyping can manifest through word embeddings and large-scale language models, where certain biases are learned and perpetuated in the resulting representations. Under-representation pertains to the absence of identifiable group members from representative bodies and well-being indicators in proportion to their population's size. This type of bias highlights the disparities in visibility and inclusivity within various domains. Researchers have extensively studied these categories of bias to better understand their nature and mitigate their negative impact on society. By recognizing and categorizing different types of bias, researchers and practitioners can develop effective strategies and techniques to address gender bias in NLP. The above-mentioned NLP tasks can be represented and examined using the terms specified in Table I. This categorization facilitates a clearer understanding of how gender bias can manifest in different NLP applications, thereby guiding efforts to combat and mitigate bias effectively. By comprehensively analyzing and addressing the various types of bias, researchers and practitioners can contribute to the development of more equitable and fair NLP systems and applications, promoting inclusivity and fostering unbiased linguistic representations.\n\n## C. Bias from Input Representations\nBias from input representations in NLP refers to the introduction of bias into the input data used for training and evaluating NLP models. These biases, often referred to as semantic biases, can arise from various sources and have Studies have demonstrated that word embeddings can detect racial and gender biases, even in well-labeled and balanced datasets. These biases can arise due to societal prejudices reflected in the training data, leading to biased predictions and outputs from NLP models. Contextual representations learned by large pre-trained language models, such as BERT and GPT, are also susceptible to biases. These models are typically trained on vast amounts of internet text, including societal biases in online content. Consequently, these models can replicate and perpetuate biases, often mirroring societal biases. Numerous studies have documented and quantified biases in NLP models and their input representations, highlighting the importance of addressing these issues. Recognizing and understanding the biases generated during the data generation process is a critical step towards mitigating them. Addressing bias in input representations requires a multifaceted approach. It involves diversifying training data sources to ensure the representation of various demographic groups and perspectives. Regularly evaluating and auditing the models for biases and developing debiasing techniques are crucial to mitigating these biases. The above-mentioned biases are collectively represented as the biases generated in the process of data generation, which can be depicted in Fig. 4 D. Model Bias Languages are dynamic and constantly evolving, making capturing their complexity and nuances challenging even with a large dataset. Using a small subset of data can only provide a limited and temporary snapshot of language, which is why relying solely on \"better\" training data is not a comprehensive solution to address bias in NLP models. Furthermore, machine learning models tend to amplify the behaviors and patterns they are exposed to, including biases present in the training data. Studies such as [30], [31] have explored the compounding effect of bias in newer models, highlighting the phenomenon known as bias overamplification. This refers to the tendency of machine learning models to disproportionately amplify and reinforce biases rather than mitigate them.",
            "reference_string": "[259203321 | Nemani et al. | 2023 | Citations: 3]"
        },
        {
            "title": "A Review of ChatGPT Applications in Education, Marketing, Software Engineering, and Healthcare: Benefits, Drawbacks, and Research Directions",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 66,
            "citation_count": 48,
            "influential_citation_count": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2305.00237",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.00237, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "145874705",
                    "name": "Mohammad Fraiwan"
                },
                {
                    "authorId": "47343240",
                    "name": "Natheer Khasawneh"
                }
            ],
            "abstract": "ChatGPT is a type of artificial intelligence language model that uses deep learning algorithms to generate human-like responses to text-based prompts. The introduction of the latest ChatGPT version in November of 2022 has caused shockwaves in the industrial and academic communities for its powerful capabilities, plethora of possible applications, and the great possibility for abuse. At the time of writing this work, several other language models (e.g., Google Bard and Meta LLaMA) just came out in an attempt to get a foothold in the vast possible market. These models have the ability to revolutionize the way we interact with computers and have potential applications in many fields, including education, software engineering, healthcare, and marketing. In this paper, we will discuss the possible applications, drawbacks, and research directions using advanced language Chatbots (e.g., ChatGPT) in each of these fields. We first start with a brief introduction and the development timeline of artificial intelligence based language models, then we go through possible applications of such models, after that we discuss the limitations and drawbacks of the current technological state of the art, and finally we point out future possible research directions.",
            "corpus_id": 258426630,
            "sentences": [
                {
                    "corpus_id": "258426630",
                    "title": "A Review of ChatGPT Applications in Education, Marketing, Software Engineering, and Healthcare: Benefits, Drawbacks, and Research Directions",
                    "text": "Language models can exhibit bias if the training data used to create them is biased. As Schramowski et al. [37] pointed out, large pre-trained models that try to mimic natural languages, may end up repeating the same unfairness and prejudices. This can lead to discriminatory or inaccurate analyses and recommendations. Moreover, this may lead to public outcry (i.e., political, social, and legal) against the commercial applications. These biases manifests themselves in several ways, as follows: \n\n\u2022 Training data bias: Language models are typically trained on large datasets of human language. If these datasets are biased in some way (e.g., based on race, gender, socioeconomic status, etc.), then the model may learn and replicate these biases in its responses. For example, if the training data is biased towards a particular gender, then the model may generate responses that are more favorable towards that gender. \n\n\u2022 User interaction bias: The responses generated by Chatbots are based on the input they receive from users. If users consistently ask biased or prejudiced questions, then the model may learn and replicate these biases in its responses. For example, if users frequently ask questions that are discriminatory towards a particular group, then the model may generate responses that perpetuate these biases. \n\n\u2022 Algorithmic bias: The algorithms used to train and operate language models and Chatbots may also introduce biases. For example, if the model is trained to optimize for a particular metric (e.g., accuracy, engagement, etc.), then it may prioritize generating responses that optimize for that metric, even if those responses are biased in some way. \n\n\u2022 Contextual bias: Chatbots generate responses based on the context they receive from users. If the context is biased in some way (e.g., based on the user's location, language, etc.), then the model may generate biased responses. For example, if a user is asking questions about a particular culture or religion, and the model is not trained on that culture or religion, it may generate biased responses due to its lack of knowledge. \n\nIt is important to note that bias in language models are not necessarily intentional or malicious.",
                    "score": 0.5863440161881022,
                    "section_title": "Bias",
                    "char_start_offset": 24519,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 84
                        },
                        {
                            "start": 85,
                            "end": 243
                        },
                        {
                            "start": 244,
                            "end": 319
                        },
                        {
                            "start": 320,
                            "end": 434
                        },
                        {
                            "start": 435,
                            "end": 497
                        },
                        {
                            "start": 500,
                            "end": 596
                        },
                        {
                            "start": 597,
                            "end": 766
                        },
                        {
                            "start": 767,
                            "end": 922
                        },
                        {
                            "start": 925,
                            "end": 1033
                        },
                        {
                            "start": 1034,
                            "end": 1161
                        },
                        {
                            "start": 1162,
                            "end": 1328
                        },
                        {
                            "start": 1331,
                            "end": 1447
                        },
                        {
                            "start": 1448,
                            "end": 1679
                        },
                        {
                            "start": 1682,
                            "end": 1774
                        },
                        {
                            "start": 1775,
                            "end": 1911
                        },
                        {
                            "start": 1912,
                            "end": 2115
                        },
                        {
                            "start": 2118,
                            "end": 2216
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 107,
                            "end": 111,
                            "matchedPaperCorpusId": "246824056"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.83544921875
                }
            ],
            "relevance_judgement": 0.83544921875,
            "relevance_judgment_input_expanded": "# Title: A Review of ChatGPT Applications in Education, Marketing, Software Engineering, and Healthcare: Benefits, Drawbacks, and Research Directions\n# Venue: arXiv.org\n# Authors: Mohammad Fraiwan, Natheer Khasawneh\n## Abstract\nChatGPT is a type of artificial intelligence language model that uses deep learning algorithms to generate human-like responses to text-based prompts. The introduction of the latest ChatGPT version in November of 2022 has caused shockwaves in the industrial and academic communities for its powerful capabilities, plethora of possible applications, and the great possibility for abuse. At the time of writing this work, several other language models (e.g., Google Bard and Meta LLaMA) just came out in an attempt to get a foothold in the vast possible market. These models have the ability to revolutionize the way we interact with computers and have potential applications in many fields, including education, software engineering, healthcare, and marketing. In this paper, we will discuss the possible applications, drawbacks, and research directions using advanced language Chatbots (e.g., ChatGPT) in each of these fields. We first start with a brief introduction and the development timeline of artificial intelligence based language models, then we go through possible applications of such models, after that we discuss the limitations and drawbacks of the current technological state of the art, and finally we point out future possible research directions.\n## Bias\nLanguage models can exhibit bias if the training data used to create them is biased. As Schramowski et al. [37] pointed out, large pre-trained models that try to mimic natural languages, may end up repeating the same unfairness and prejudices. This can lead to discriminatory or inaccurate analyses and recommendations. Moreover, this may lead to public outcry (i.e., political, social, and legal) against the commercial applications. These biases manifests themselves in several ways, as follows: \n\n\u2022 Training data bias: Language models are typically trained on large datasets of human language. If these datasets are biased in some way (e.g., based on race, gender, socioeconomic status, etc.), then the model may learn and replicate these biases in its responses. For example, if the training data is biased towards a particular gender, then the model may generate responses that are more favorable towards that gender. \n\n\u2022 User interaction bias: The responses generated by Chatbots are based on the input they receive from users. If users consistently ask biased or prejudiced questions, then the model may learn and replicate these biases in its responses. For example, if users frequently ask questions that are discriminatory towards a particular group, then the model may generate responses that perpetuate these biases. \n\n\u2022 Algorithmic bias: The algorithms used to train and operate language models and Chatbots may also introduce biases. For example, if the model is trained to optimize for a particular metric (e.g., accuracy, engagement, etc.), then it may prioritize generating responses that optimize for that metric, even if those responses are biased in some way. \n\n\u2022 Contextual bias: Chatbots generate responses based on the context they receive from users. If the context is biased in some way (e.g., based on the user's location, language, etc.), then the model may generate biased responses. For example, if a user is asking questions about a particular culture or religion, and the model is not trained on that culture or religion, it may generate biased responses due to its lack of knowledge. \n\nIt is important to note that bias in language models are not necessarily intentional or malicious.",
            "reference_string": "[258426630 | Fraiwan et al. | 2023 | Citations: 48]"
        },
        {
            "title": "MAFIA: Multi-Adapter Fused Inclusive Language Models",
            "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
            "year": 2024,
            "reference_count": 56,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.07519, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "3094662",
                    "name": "Prachi Jain"
                },
                {
                    "authorId": "2266397701",
                    "name": "Ashutosh Sathe"
                },
                {
                    "authorId": "2140408530",
                    "name": "Varun Gumma"
                },
                {
                    "authorId": "52154863",
                    "name": "Kabir Ahuja"
                },
                {
                    "authorId": "2256989615",
                    "name": "Sunayana Sitaram"
                }
            ],
            "abstract": "Pretrained Language Models (PLMs) are widely used in NLP for various tasks. Recent studies have identified various biases that such models exhibit and have proposed methods to correct these biases. However, most of the works address a limited set of bias dimensions independently such as gender, race, or religion. Moreover, the methods typically involve finetuning the full model in order to maintain the performance on the downstream task. In this work, we aim to modularly debias a pre-trained language model across multiple dimensions. Previous works extensively explored debiasing PLMs by using limited US-centric counterfactual data augmentation (CDA). We use structured knowledge and a large generative model to build a diverse CDA across multiple bias dimensions in a semi-automated way. We highlight how existing debiasing methods do not consider interactions between multiple societal biases and propose a debiasing model that exploits the synergy amongst various societal biases and enables multi-bias debiasing simultaneously. An extensive evaluation on multiple tasks and languages demonstrates the efficacy of the approach.",
            "corpus_id": 267627928,
            "sentences": [
                {
                    "corpus_id": "267627928",
                    "title": "MAFIA: Multi-Adapter Fused Inclusive Language Models",
                    "text": "Pretrained Language Models (PLMs) are widely used in NLP for various tasks. Recent studies have identified various biases that such models exhibit and have proposed methods to correct these biases. However, most of the works address a limited set of bias dimensions independently such as gender, race, or religion. Moreover, the methods typically involve finetuning the full model in order to maintain the performance on the downstream task. In this work, we aim to modularly debias a pre-trained language model across multiple dimensions. Previous works extensively explored debiasing PLMs by using limited US-centric counterfactual data augmentation (CDA). We use structured knowledge and a large generative model to build a diverse CDA across multiple bias dimensions in a semi-automated way. We highlight how existing debiasing methods do not consider interactions between multiple societal biases and propose a debiasing model that exploits the synergy amongst various societal biases and enables multi-bias debiasing simultaneously. An extensive evaluation on multiple tasks and languages demonstrates the efficacy of the approach.",
                    "score": 0.576806579159662,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.83544921875
                }
            ],
            "relevance_judgement": 0.83544921875,
            "relevance_judgment_input_expanded": "# Title: MAFIA: Multi-Adapter Fused Inclusive Language Models\n# Venue: Conference of the European Chapter of the Association for Computational Linguistics\n# Authors: Prachi Jain, Ashutosh Sathe, Varun Gumma, Kabir Ahuja, Sunayana Sitaram\n## Abstract\nPretrained Language Models (PLMs) are widely used in NLP for various tasks. Recent studies have identified various biases that such models exhibit and have proposed methods to correct these biases. However, most of the works address a limited set of bias dimensions independently such as gender, race, or religion. Moreover, the methods typically involve finetuning the full model in order to maintain the performance on the downstream task. In this work, we aim to modularly debias a pre-trained language model across multiple dimensions. Previous works extensively explored debiasing PLMs by using limited US-centric counterfactual data augmentation (CDA). We use structured knowledge and a large generative model to build a diverse CDA across multiple bias dimensions in a semi-automated way. We highlight how existing debiasing methods do not consider interactions between multiple societal biases and propose a debiasing model that exploits the synergy amongst various societal biases and enables multi-bias debiasing simultaneously. An extensive evaluation on multiple tasks and languages demonstrates the efficacy of the approach.\n",
            "reference_string": "[267627928 | Jain et al. | 2024 | Citations: 1]"
        },
        {
            "title": "NBIAS: A Natural Language Processing Framework for Bias Identification in Text",
            "venue": "Expert systems with applications",
            "year": 2023,
            "reference_count": 79,
            "citation_count": 48,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2308.01681",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.01681, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2226784526",
                    "name": "Shaina Razaa"
                },
                {
                    "authorId": "3410824",
                    "name": "Muskan Garg"
                },
                {
                    "authorId": "2155769046",
                    "name": "Deepak John Reji"
                },
                {
                    "authorId": "98911272",
                    "name": "S. Bashir"
                },
                {
                    "authorId": "2053158730",
                    "name": "Chen Ding"
                }
            ],
            "abstract": "Bias in textual data can lead to skewed interpretations and outcomes when the data is used. These biases could perpetuate stereotypes, discrimination, or other forms of unfair treatment. An algorithm trained on biased data may end up making decisions that disproportionately impact a certain group of people. Therefore, it is crucial to detect and remove these biases to ensure the fair and ethical use of data. To this end, we develop a comprehensive and robust framework NBIAS that consists of four main layers: data, corpus construction, model development and an evaluation layer. The dataset is constructed by collecting diverse data from various domains, including social media, healthcare, and job hiring portals. As such, we applied a transformer-based token classification model that is able to identify bias words/ phrases through a unique named entity BIAS. In the evaluation procedure, we incorporate a blend of quantitative and qualitative measures to gauge the effectiveness of our models. We achieve accuracy improvements ranging from 1% to 8% compared to baselines. We are also able to generate a robust understanding of the model functioning. The proposed approach is applicable to a variety of biases and contributes to the fair and ethical use of textual data.",
            "corpus_id": 260438662,
            "sentences": [
                {
                    "corpus_id": "260438662",
                    "title": "NBIAS: A Natural Language Processing Framework for Bias Identification in Text",
                    "text": "The recent surge in Natural Language Processing (NLP) applications, encompassing fields from recommendation systems to social justice and employment screening, has sparked a critical concern -the emergence of bias within these systems [1]. Instances of racial and gender bias have been increasingly reported [2], indicating an urgent need for scrutiny. These biases often originate from the training data used in NLP models, and a majority of these large datasets harbor inherent biases . Regrettably, many NLP practitioners lack the necessary awareness or knowledge to effectively identify and address these biases, highlighting a significant gap in the field. \n\nFurthermore, there is a notable lack of discussion on data specifics -its origin, generation, and pre-processing -in many NLP publications. Given these circumstances, the importance of addressing biases in NLP applications cannot be overstated. These biases, if unchecked, not only compromise the validity of the models, but can also have unfavorable and detrimental consequences. The objective of this research is to provide insights into the detection of bias in NLP datasets, contributing to the development of more equitable and unbiased Artificial Intelligence (AI) systems. \n\nBias in text data is a pervasive and deeply-rooted issue. The bias in data often stems from cognitive predispositions that influences our dialogues, views, and understanding of information [3]. This bias can be explicit which are often seen in discriminatory language targeting certain racial or ethnic groups [4], as in social media. Implicit bias [5], on the other hand, subtly perpetuates prejudice through unintentional language use but is equally harmful. \n\nThe necessity for unbiased, trustworthy text data has grown across sectors like healthcare [6], social media [4,7], and recruitment [8]. This data is essential for training NLP models for various downstream tasks, like formulating healthcare diagnoses and treatment plans, handling discriminatory language on social media, and promoting fair recruitment practices. Figure 1 illustrates the complexities of biases in text data in various domains, including job hiring, social media, and healthcare. These biases are primarily conveyed through lexical choices [9] and demand sophisticated detection methods, motivating this research.",
                    "score": 0.7051229086851288,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 239
                        },
                        {
                            "start": 240,
                            "end": 352
                        },
                        {
                            "start": 353,
                            "end": 488
                        },
                        {
                            "start": 489,
                            "end": 661
                        },
                        {
                            "start": 664,
                            "end": 803
                        },
                        {
                            "start": 804,
                            "end": 908
                        },
                        {
                            "start": 909,
                            "end": 1044
                        },
                        {
                            "start": 1045,
                            "end": 1243
                        },
                        {
                            "start": 1246,
                            "end": 1303
                        },
                        {
                            "start": 1304,
                            "end": 1439
                        },
                        {
                            "start": 1440,
                            "end": 1580
                        },
                        {
                            "start": 1581,
                            "end": 1706
                        },
                        {
                            "start": 1709,
                            "end": 1845
                        },
                        {
                            "start": 1846,
                            "end": 2073
                        },
                        {
                            "start": 2074,
                            "end": 2206
                        },
                        {
                            "start": 2207,
                            "end": 2340
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 235,
                            "end": 238,
                            "matchedPaperCorpusId": "218487466"
                        },
                        {
                            "start": 308,
                            "end": 311,
                            "matchedPaperCorpusId": "1704893"
                        },
                        {
                            "start": 1435,
                            "end": 1438,
                            "matchedPaperCorpusId": "54997157"
                        },
                        {
                            "start": 1595,
                            "end": 1598,
                            "matchedPaperCorpusId": "228094096"
                        },
                        {
                            "start": 1800,
                            "end": 1803,
                            "matchedPaperCorpusId": "244490278"
                        },
                        {
                            "start": 1821,
                            "end": 1823,
                            "matchedPaperCorpusId": "247050244"
                        },
                        {
                            "start": 1841,
                            "end": 1844,
                            "matchedPaperCorpusId": "1634922"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.83154296875
                }
            ],
            "relevance_judgement": 0.83154296875,
            "relevance_judgment_input_expanded": "# Title: NBIAS: A Natural Language Processing Framework for Bias Identification in Text\n# Venue: Expert systems with applications\n# Authors: Shaina Razaa, Muskan Garg, Deepak John Reji, S. Bashir, Chen Ding\n## Abstract\nBias in textual data can lead to skewed interpretations and outcomes when the data is used. These biases could perpetuate stereotypes, discrimination, or other forms of unfair treatment. An algorithm trained on biased data may end up making decisions that disproportionately impact a certain group of people. Therefore, it is crucial to detect and remove these biases to ensure the fair and ethical use of data. To this end, we develop a comprehensive and robust framework NBIAS that consists of four main layers: data, corpus construction, model development and an evaluation layer. The dataset is constructed by collecting diverse data from various domains, including social media, healthcare, and job hiring portals. As such, we applied a transformer-based token classification model that is able to identify bias words/ phrases through a unique named entity BIAS. In the evaluation procedure, we incorporate a blend of quantitative and qualitative measures to gauge the effectiveness of our models. We achieve accuracy improvements ranging from 1% to 8% compared to baselines. We are also able to generate a robust understanding of the model functioning. The proposed approach is applicable to a variety of biases and contributes to the fair and ethical use of textual data.\n## Introduction\nThe recent surge in Natural Language Processing (NLP) applications, encompassing fields from recommendation systems to social justice and employment screening, has sparked a critical concern -the emergence of bias within these systems [1]. Instances of racial and gender bias have been increasingly reported [2], indicating an urgent need for scrutiny. These biases often originate from the training data used in NLP models, and a majority of these large datasets harbor inherent biases . Regrettably, many NLP practitioners lack the necessary awareness or knowledge to effectively identify and address these biases, highlighting a significant gap in the field. \n\nFurthermore, there is a notable lack of discussion on data specifics -its origin, generation, and pre-processing -in many NLP publications. Given these circumstances, the importance of addressing biases in NLP applications cannot be overstated. These biases, if unchecked, not only compromise the validity of the models, but can also have unfavorable and detrimental consequences. The objective of this research is to provide insights into the detection of bias in NLP datasets, contributing to the development of more equitable and unbiased Artificial Intelligence (AI) systems. \n\nBias in text data is a pervasive and deeply-rooted issue. The bias in data often stems from cognitive predispositions that influences our dialogues, views, and understanding of information [3]. This bias can be explicit which are often seen in discriminatory language targeting certain racial or ethnic groups [4], as in social media. Implicit bias [5], on the other hand, subtly perpetuates prejudice through unintentional language use but is equally harmful. \n\nThe necessity for unbiased, trustworthy text data has grown across sectors like healthcare [6], social media [4,7], and recruitment [8]. This data is essential for training NLP models for various downstream tasks, like formulating healthcare diagnoses and treatment plans, handling discriminatory language on social media, and promoting fair recruitment practices. Figure 1 illustrates the complexities of biases in text data in various domains, including job hiring, social media, and healthcare. These biases are primarily conveyed through lexical choices [9] and demand sophisticated detection methods, motivating this research.",
            "reference_string": "[260438662 | Razaa et al. | 2023 | Citations: 48]"
        },
        {
            "title": "Assessing Social and Intersectional Biases in Contextualized Word Representations",
            "venue": "Neural Information Processing Systems",
            "year": 2019,
            "reference_count": 40,
            "citation_count": 229,
            "influential_citation_count": 17,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1911.01485, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "144787248",
                    "name": "Y. Tan"
                },
                {
                    "authorId": "47777961",
                    "name": "Elisa Celis"
                }
            ],
            "abstract": "Social bias in machine learning has drawn significant attention, with work ranging from demonstrations of bias in a multitude of applications, curating definitions of fairness for different contexts, to developing algorithms to mitigate bias. In natural language processing, gender bias has been shown to exist in context-free word embeddings. Recently, contextual word representations have outperformed word embeddings in several downstream NLP tasks. These word representations are conditioned on their context within a sentence, and can also be used to encode the entire sentence. In this paper, we analyze the extent to which state-of-the-art models for contextual word representations, such as BERT and GPT-2, encode biases with respect to gender, race, and intersectional identities. Towards this, we propose assessing bias at the contextual word level. This novel approach captures the contextual effects of bias missing in context-free word embeddings, yet avoids confounding effects that underestimate bias at the sentence encoding level. We demonstrate evidence of bias at the corpus level, find varying evidence of bias in embedding association tests, show in particular that racial bias is strongly encoded in contextual word models, and observe that bias effects for intersectional minorities are exacerbated beyond their constituent minority identities. Further, evaluating bias effects at the contextual word level captures biases that are not captured at the sentence level, confirming the need for our novel approach.",
            "corpus_id": 202781363,
            "sentences": [
                {
                    "corpus_id": "202781363",
                    "title": "Assessing Social and Intersectional Biases in Contextualized Word Representations",
                    "text": "This paper makes the following contributions. First, we use co-occurrence counts to show that standard corpora for pre-training contextual word models exhibit significant gender imbalances. Second, we extend existing analyses of social bias to state-of-the-art contextual word models like GPT-2, and indicate that social bias also exists in those models. This highlights the scope of the problem of fairness in state-of-the-art models for language processing. Third, we demonstrate that when measuring social bias in contextual word models, both the sentence encoding and contextual word representation should be used. It is possible that either encoding type may be unable to uncover latent social bias, whereas the other encoding type is able to. Fourth, we provide evidence for how racial bias is encoded strongly in contextual word models, potentially even more so than gender bias. Fifth, we introduce a method of comparison that anchors at the most or least privileged group to show that intersectional identities suffer from such bias as well, and more so than their constituent minority identities. In we show that the effect of race on intersectional identities seems to be larger than the effect of gender. \n\nIt is important to highlight the following limitations of our work. First, the lack of significant positive associations should not be taken as an absence of social bias. Rather, this only indicate the absence as such measured by these specific tests. Second, this work assumes binary gender (male/female), which is a significant limitation in evaluating the bias of non-binary genders. We believe that the bias towards non-binary genders is likely to be worse, but there can also be more data sparsity issues in such evaluations. Third, this work only provides a preliminary investigation into the multiplicative aspects of identities of multiple minorities, in particular the specific interactions between different identities. While we have tried to isolate the effects of the different dimensions of identity in intersectional tests, more work needs to be done to determine the interactive nature of such effects. \n\nWe propose the following potential future directions. First, investigate how and why the encoding of bias may differ across both model size and model layers. Our results show that larger contextual word models seem to encode less social bias.",
                    "score": 0.6145841924910944,
                    "section_title": "Discussion and Limitations",
                    "char_start_offset": 24775,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 45
                        },
                        {
                            "start": 46,
                            "end": 189
                        },
                        {
                            "start": 190,
                            "end": 354
                        },
                        {
                            "start": 355,
                            "end": 459
                        },
                        {
                            "start": 460,
                            "end": 618
                        },
                        {
                            "start": 619,
                            "end": 748
                        },
                        {
                            "start": 749,
                            "end": 886
                        },
                        {
                            "start": 887,
                            "end": 1106
                        },
                        {
                            "start": 1107,
                            "end": 1216
                        },
                        {
                            "start": 1219,
                            "end": 1286
                        },
                        {
                            "start": 1287,
                            "end": 1389
                        },
                        {
                            "start": 1390,
                            "end": 1470
                        },
                        {
                            "start": 1471,
                            "end": 1605
                        },
                        {
                            "start": 1606,
                            "end": 1749
                        },
                        {
                            "start": 1750,
                            "end": 1948
                        },
                        {
                            "start": 1949,
                            "end": 2136
                        },
                        {
                            "start": 2139,
                            "end": 2192
                        },
                        {
                            "start": 2193,
                            "end": 2296
                        },
                        {
                            "start": 2297,
                            "end": 2381
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.83154296875
                }
            ],
            "relevance_judgement": 0.83154296875,
            "relevance_judgment_input_expanded": "# Title: Assessing Social and Intersectional Biases in Contextualized Word Representations\n# Venue: Neural Information Processing Systems\n# Authors: Y. Tan, Elisa Celis\n## Abstract\nSocial bias in machine learning has drawn significant attention, with work ranging from demonstrations of bias in a multitude of applications, curating definitions of fairness for different contexts, to developing algorithms to mitigate bias. In natural language processing, gender bias has been shown to exist in context-free word embeddings. Recently, contextual word representations have outperformed word embeddings in several downstream NLP tasks. These word representations are conditioned on their context within a sentence, and can also be used to encode the entire sentence. In this paper, we analyze the extent to which state-of-the-art models for contextual word representations, such as BERT and GPT-2, encode biases with respect to gender, race, and intersectional identities. Towards this, we propose assessing bias at the contextual word level. This novel approach captures the contextual effects of bias missing in context-free word embeddings, yet avoids confounding effects that underestimate bias at the sentence encoding level. We demonstrate evidence of bias at the corpus level, find varying evidence of bias in embedding association tests, show in particular that racial bias is strongly encoded in contextual word models, and observe that bias effects for intersectional minorities are exacerbated beyond their constituent minority identities. Further, evaluating bias effects at the contextual word level captures biases that are not captured at the sentence level, confirming the need for our novel approach.\n## Discussion and Limitations\nThis paper makes the following contributions. First, we use co-occurrence counts to show that standard corpora for pre-training contextual word models exhibit significant gender imbalances. Second, we extend existing analyses of social bias to state-of-the-art contextual word models like GPT-2, and indicate that social bias also exists in those models. This highlights the scope of the problem of fairness in state-of-the-art models for language processing. Third, we demonstrate that when measuring social bias in contextual word models, both the sentence encoding and contextual word representation should be used. It is possible that either encoding type may be unable to uncover latent social bias, whereas the other encoding type is able to. Fourth, we provide evidence for how racial bias is encoded strongly in contextual word models, potentially even more so than gender bias. Fifth, we introduce a method of comparison that anchors at the most or least privileged group to show that intersectional identities suffer from such bias as well, and more so than their constituent minority identities. In we show that the effect of race on intersectional identities seems to be larger than the effect of gender. \n\nIt is important to highlight the following limitations of our work. First, the lack of significant positive associations should not be taken as an absence of social bias. Rather, this only indicate the absence as such measured by these specific tests. Second, this work assumes binary gender (male/female), which is a significant limitation in evaluating the bias of non-binary genders. We believe that the bias towards non-binary genders is likely to be worse, but there can also be more data sparsity issues in such evaluations. Third, this work only provides a preliminary investigation into the multiplicative aspects of identities of multiple minorities, in particular the specific interactions between different identities. While we have tried to isolate the effects of the different dimensions of identity in intersectional tests, more work needs to be done to determine the interactive nature of such effects. \n\nWe propose the following potential future directions. First, investigate how and why the encoding of bias may differ across both model size and model layers. Our results show that larger contextual word models seem to encode less social bias.",
            "reference_string": "[202781363 | Tan et al. | 2019 | Citations: 229]"
        },
        {
            "title": "Benchmarking Adversarial Robustness to Bias Elicitation in Large Language Models: Scalable Automated Assessment with LLM-as-a-Judge",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 51,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.07887, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1585232914",
                    "name": "Riccardo Cantini"
                },
                {
                    "authorId": "96934840",
                    "name": "A. Orsino"
                },
                {
                    "authorId": "2354558125",
                    "name": "Massimo Ruggiero"
                },
                {
                    "authorId": "2299780920",
                    "name": "Domenico Talia"
                }
            ],
            "abstract": "Large Language Models (LLMs) have revolutionized artificial intelligence, driving advancements in machine translation, summarization, and conversational agents. However, their increasing integration into critical societal domains has raised concerns about embedded biases, which can perpetuate stereotypes and compromise fairness. These biases stem from various sources, including historical inequalities in training data, linguistic imbalances, and adversarial manipulation. Despite mitigation efforts, recent studies indicate that LLMs remain vulnerable to adversarial attacks designed to elicit biased responses. This work proposes a scalable benchmarking framework to evaluate LLM robustness against adversarial bias elicitation. Our methodology involves (i) systematically probing models with a multi-task approach targeting biases across various sociocultural dimensions, (ii) quantifying robustness through safety scores using an LLM-as-a-Judge approach for automated assessment of model responses, and (iii) employing jailbreak techniques to investigate vulnerabilities in safety mechanisms. Our analysis examines prevalent biases in both small and large state-of-the-art models and their impact on model safety. Additionally, we assess the safety of domain-specific models fine-tuned for critical fields, such as medicine. Finally, we release a curated dataset of bias-related prompts, CLEAR-Bias, to facilitate systematic vulnerability benchmarking. Our findings reveal critical trade-offs between model size and safety, aiding the development of fairer and more robust future language models.",
            "corpus_id": 277667520,
            "sentences": [
                {
                    "corpus_id": "277667520",
                    "title": "Benchmarking Adversarial Robustness to Bias Elicitation in Large Language Models: Scalable Automated Assessment with LLM-as-a-Judge",
                    "text": "Large Language Models (LLMs) have empowered artificial intelligence with their remarkable natural language understanding and generation capabilities, enabling breakthroughs in tasks such as machine translation, summarization, and human-like conversation [1,2]. However, their increasing integration into societal domainsincluding healthcare [3], education [4], and law [5]-has amplified concerns about embedded biases. These biases, which can manifest in various forms, risk perpetuating stereotypes, marginalizing underrepresented groups, and undermining ethical AI deployment [6]. Biases may stem from various sources, including biased training data that reflects historical inequalities and prejudicial associations, linguistic imbalances in corpora, flaws in algorithmic design, and the uncritical use of AI systems [7,8]. Previous studies have quantified biased attitudes in language models related to various social groups [9,10], also finding that state-of-the-art LLMs can be manipulated via adversarial attacks to produce biased or harmful responses, despite their bias mitigation and alignment mechanisms [11]. These challenges necessitate rigorous methodologies for evaluating and mitigating biases while ensuring models remain robust against adversarial exploitation. However, current approaches to bias evaluation face critical limitations, including the substantial resources required for bias identification and mitigation, difficulties in acquiring representative datasets for safety assessment, and the absence of universally accepted bias metrics. \n\nTo address these gaps, this work proposes a scalable methodology for benchmarking LLMs against bias elicitation. Our approach follows a two-step process and leverages the LLM-as-a-Judge paradigm [12] to automate bias evaluation, reducing reliance on manual response annotation while ensuring scalability and reproducibility. The first step involves selecting a judge model based on its statistical agreement with human annotations on a curated dataset of prompt-response pairs. These pairs capture both biased and safe behaviors, providing a benchmark for evaluating model ability to discern harmful content. Once chosen, the judge model is used to systematically evaluate LLM robustness using bias-probing prompts across multiple sociocultural dimensions, encompassing both isolated and intersectional bias categories.",
                    "score": 0.6005886803815901,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 260
                        },
                        {
                            "start": 261,
                            "end": 418
                        },
                        {
                            "start": 419,
                            "end": 582
                        },
                        {
                            "start": 583,
                            "end": 826
                        },
                        {
                            "start": 827,
                            "end": 1120
                        },
                        {
                            "start": 1121,
                            "end": 1279
                        },
                        {
                            "start": 1280,
                            "end": 1565
                        },
                        {
                            "start": 1568,
                            "end": 1680
                        },
                        {
                            "start": 1681,
                            "end": 1892
                        },
                        {
                            "start": 1893,
                            "end": 2045
                        },
                        {
                            "start": 2046,
                            "end": 2176
                        },
                        {
                            "start": 2177,
                            "end": 2387
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 254,
                            "end": 257,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 257,
                            "end": 259,
                            "matchedPaperCorpusId": "259360395"
                        },
                        {
                            "start": 341,
                            "end": 344,
                            "matchedPaperCorpusId": "257312905"
                        },
                        {
                            "start": 356,
                            "end": 359,
                            "matchedPaperCorpusId": "265315253"
                        },
                        {
                            "start": 369,
                            "end": 372,
                            "matchedPaperCorpusId": "267413187"
                        },
                        {
                            "start": 578,
                            "end": 581,
                            "matchedPaperCorpusId": "258688053"
                        },
                        {
                            "start": 820,
                            "end": 823,
                            "matchedPaperCorpusId": "237298625"
                        },
                        {
                            "start": 823,
                            "end": 825,
                            "matchedPaperCorpusId": "261530629"
                        },
                        {
                            "start": 929,
                            "end": 932,
                            "matchedPaperCorpusId": "265212726"
                        },
                        {
                            "start": 932,
                            "end": 935,
                            "matchedPaperCorpusId": "215828184"
                        },
                        {
                            "start": 1115,
                            "end": 1119,
                            "matchedPaperCorpusId": "271097745"
                        },
                        {
                            "start": 1763,
                            "end": 1767,
                            "matchedPaperCorpusId": "259129398"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.830078125
                }
            ],
            "relevance_judgement": 0.830078125,
            "relevance_judgment_input_expanded": "# Title: Benchmarking Adversarial Robustness to Bias Elicitation in Large Language Models: Scalable Automated Assessment with LLM-as-a-Judge\n# Venue: arXiv.org\n# Authors: Riccardo Cantini, A. Orsino, Massimo Ruggiero, Domenico Talia\n## Abstract\nLarge Language Models (LLMs) have revolutionized artificial intelligence, driving advancements in machine translation, summarization, and conversational agents. However, their increasing integration into critical societal domains has raised concerns about embedded biases, which can perpetuate stereotypes and compromise fairness. These biases stem from various sources, including historical inequalities in training data, linguistic imbalances, and adversarial manipulation. Despite mitigation efforts, recent studies indicate that LLMs remain vulnerable to adversarial attacks designed to elicit biased responses. This work proposes a scalable benchmarking framework to evaluate LLM robustness against adversarial bias elicitation. Our methodology involves (i) systematically probing models with a multi-task approach targeting biases across various sociocultural dimensions, (ii) quantifying robustness through safety scores using an LLM-as-a-Judge approach for automated assessment of model responses, and (iii) employing jailbreak techniques to investigate vulnerabilities in safety mechanisms. Our analysis examines prevalent biases in both small and large state-of-the-art models and their impact on model safety. Additionally, we assess the safety of domain-specific models fine-tuned for critical fields, such as medicine. Finally, we release a curated dataset of bias-related prompts, CLEAR-Bias, to facilitate systematic vulnerability benchmarking. Our findings reveal critical trade-offs between model size and safety, aiding the development of fairer and more robust future language models.\n## Introduction\nLarge Language Models (LLMs) have empowered artificial intelligence with their remarkable natural language understanding and generation capabilities, enabling breakthroughs in tasks such as machine translation, summarization, and human-like conversation [1,2]. However, their increasing integration into societal domainsincluding healthcare [3], education [4], and law [5]-has amplified concerns about embedded biases. These biases, which can manifest in various forms, risk perpetuating stereotypes, marginalizing underrepresented groups, and undermining ethical AI deployment [6]. Biases may stem from various sources, including biased training data that reflects historical inequalities and prejudicial associations, linguistic imbalances in corpora, flaws in algorithmic design, and the uncritical use of AI systems [7,8]. Previous studies have quantified biased attitudes in language models related to various social groups [9,10], also finding that state-of-the-art LLMs can be manipulated via adversarial attacks to produce biased or harmful responses, despite their bias mitigation and alignment mechanisms [11]. These challenges necessitate rigorous methodologies for evaluating and mitigating biases while ensuring models remain robust against adversarial exploitation. However, current approaches to bias evaluation face critical limitations, including the substantial resources required for bias identification and mitigation, difficulties in acquiring representative datasets for safety assessment, and the absence of universally accepted bias metrics. \n\nTo address these gaps, this work proposes a scalable methodology for benchmarking LLMs against bias elicitation. Our approach follows a two-step process and leverages the LLM-as-a-Judge paradigm [12] to automate bias evaluation, reducing reliance on manual response annotation while ensuring scalability and reproducibility. The first step involves selecting a judge model based on its statistical agreement with human annotations on a curated dataset of prompt-response pairs. These pairs capture both biased and safe behaviors, providing a benchmark for evaluating model ability to discern harmful content. Once chosen, the judge model is used to systematically evaluate LLM robustness using bias-probing prompts across multiple sociocultural dimensions, encompassing both isolated and intersectional bias categories.",
            "reference_string": "[277667520 | Cantini et al. | 2025 | Citations: 4]"
        },
        {
            "title": "Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2023,
            "reference_count": 31,
            "citation_count": 37,
            "influential_citation_count": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2306.04597",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.04597, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2221493995",
                    "name": "Himanshu Thakur"
                },
                {
                    "authorId": "1819271266",
                    "name": "Atishay Jain"
                },
                {
                    "authorId": "2127734657",
                    "name": "Praneetha Vaddamanu"
                },
                {
                    "authorId": "28130078",
                    "name": "Paul Pu Liang"
                },
                {
                    "authorId": "49933077",
                    "name": "Louis-philippe Morency"
                }
            ],
            "abstract": "Societal biases present in pre-trained large language models are a critical issue as these models have been shown to propagate biases in countless downstream applications, rendering them unfair towards specific groups of people. Since large-scale retraining of these models from scratch is both time and compute-expensive, a variety of approaches have been previously proposed that de-bias a pre-trained model. While the majority of current state-of-the-art debiasing methods focus on changes to the training regime, in this paper, we propose data intervention strategies as a powerful yet simple technique to reduce gender bias in pre-trained models. Specifically, we empirically show that by fine-tuning a pre-trained model on only 10 debiased (intervened) training examples, the tendency to favor any gender is significantly reduced. Since our proposed method only needs a few training examples, we argue that our few-shot de-biasing approach is highly feasible and practical. Through extensive experimentation, we show that our de-biasing technique performs better than competitive state-of-the-art baselines with minimal loss in language modeling ability.",
            "corpus_id": 259095603,
            "sentences": [
                {
                    "corpus_id": "259095603",
                    "title": "Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions",
                    "text": "Societal biases present in pre-trained large language models are a critical issue as these models have been shown to propagate biases in countless downstream applications, rendering them unfair towards specific groups of people. Since large-scale retraining of these models from scratch is both time and compute-expensive, a variety of approaches have been previously proposed that de-bias a pre-trained model. While the majority of current state-of-the-art debiasing methods focus on changes to the training regime, in this paper, we propose data intervention strategies as a powerful yet simple technique to reduce gender bias in pre-trained models. Specifically, we empirically show that by fine-tuning a pre-trained model on only 10 debiased (intervened) training examples, the tendency to favor any gender is significantly reduced. Since our proposed method only needs a few training examples, we argue that our few-shot de-biasing approach is highly feasible and practical. Through extensive experimentation, we show that our de-biasing technique performs better than competitive state-of-the-art baselines with minimal loss in language modeling ability.",
                    "score": 0.5789062699038047,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.830078125
                }
            ],
            "relevance_judgement": 0.830078125,
            "relevance_judgment_input_expanded": "# Title: Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Himanshu Thakur, Atishay Jain, Praneetha Vaddamanu, Paul Pu Liang, Louis-philippe Morency\n## Abstract\nSocietal biases present in pre-trained large language models are a critical issue as these models have been shown to propagate biases in countless downstream applications, rendering them unfair towards specific groups of people. Since large-scale retraining of these models from scratch is both time and compute-expensive, a variety of approaches have been previously proposed that de-bias a pre-trained model. While the majority of current state-of-the-art debiasing methods focus on changes to the training regime, in this paper, we propose data intervention strategies as a powerful yet simple technique to reduce gender bias in pre-trained models. Specifically, we empirically show that by fine-tuning a pre-trained model on only 10 debiased (intervened) training examples, the tendency to favor any gender is significantly reduced. Since our proposed method only needs a few training examples, we argue that our few-shot de-biasing approach is highly feasible and practical. Through extensive experimentation, we show that our de-biasing technique performs better than competitive state-of-the-art baselines with minimal loss in language modeling ability.\n",
            "reference_string": "[259095603 | Thakur et al. | 2023 | Citations: 37]"
        },
        {
            "title": "Towards a Holistic Approach: Understanding Sociodemographic Biases in NLP Models using an Interdisciplinary Lens",
            "venue": "AAAI/ACM Conference on AI, Ethics, and Society",
            "year": 2023,
            "reference_count": 11,
            "citation_count": 8,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2308.13089",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.13089, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2053812167",
                    "name": "Pranav Narayanan Venkit"
                }
            ],
            "abstract": "The rapid growth in the usage and applications of Natural Language Processing (NLP) in various sociotechnical solutions has highlighted the need for a comprehensive understanding of bias and its impact on society. While research on bias in NLP has expanded, several challenges persist that require attention. These include the limited focus on sociodemographic biases beyond race and gender, the narrow scope of analysis predominantly centered on models, and the technocentric implementation approaches. This paper addresses these challenges and advocates for a more interdisciplinary approach to understanding bias in NLP. The work is structured into three facets, each exploring a specific aspect of bias in NLP. The first facet focuses on identifying sociodemographic bias in various NLP architectures, emphasizing the importance of considering both the models themselves and human computation to comprehensively understand and identify bias. In the second facet, we delve into the significance of establishing a shared vocabulary across different fields and disciplines involved in NLP. By highlighting the potential bias stemming from a lack of shared understanding, this facet emphasizes the need for interdisciplinary collaboration to bridge the gap and foster a more inclusive and accurate analysis of bias. Finally, the third facet investigates the development of a holistic solution by integrating frameworks from social science disciplines. This approach recognizes the complexity of bias in NLP and advocates for an interdisciplinary framework that goes beyond purely technical considerations, involving social and ethical perspectives to address bias effectively. The first facet includes the following of my published works [6, 7, 8, 9] to provide results into how the importance of understanding the presence of bias in various minority group that has not been in focus in the prior works of bias in NLP. The work also shows the need to create a method that considers both human and AI indicators of bias, showcasing the importance of the first facet of my research. In my study [9], I delve into sentiment analysis and toxicity detection models to identify explicit bias against race, gender, and people with disabilities (PWDs). Through statistical exploration of conversations on social media platforms such as Twitter and Reddit, I gain insights into how disability bias permeates real-world social settings. To quantify explicit sociodemographic bias in sentiment analysis and toxicity analysis models, I create the Bias Identification Test in Sentiment (BITS) corpus1. Applying BITS, I uncover significant biases in popular AIaaS sentiment analysis tools, including TextBlob, VADER, and Google Cloud Natural Language API, as well as toxicity analysis models like Toxic-BERT. Remarkably, all of these models exhibit statistically significant explicit bias against disability, underscoring the need for comprehensive understanding and mitigation of biases affecting such groups. The work also demonstrates the utility of BITS as a model-independent method of identifying bias by focusing on social groups instead. Expanding on this, my next work [8] delves into the realm of implicit bias in NLP models. While some models may not overtly exhibit bias, they can unintentionally perpetuate harmful stereotypes [4]. To measure and identify implicit bias in commonly used embedding and large language models, I propose a methodology to measure social biases in various NLP architectures. Focusing on people with disabilities (PWD) as a group with complex social dynamics, I analyze various word embedding-based and transformer-based LLMs, revealing significant biases against PWDs in all tested models. These findings expose how models trained on extensive corpora tend to favor ableist language, underscoring the urgency of detecting and addressing implicit bias. The above two works look at both the implicit and explicit nature of bias in NLP, showcasing the need to distinguish the efforts placed in understanding them. The results also demonstrate the utility of identifying such biases as it provides context to the black-box nature of such public models. As the field of NLP evolved from embedding-based models to large language models, the way these models are constructed underwent significant changes [5]. However, the concern arises from the fact that these models often reflect a populist viewpoint [1] that perpetuates majority-held ideas rather than objective truths. This difference in perception can lead to biases perpetuated by the majority\u2019s worldview. To explore this aspect, I investigate how LLMs represent nationality and their impact on societal stereotypes [6]. By examining LLM-generated stories for various nationalities, I establish a correlation between sentiment and the population of internet users in a country. The study reveals the unintentional implicit and explicit nationality biases exhibited by GPT-2, with nations having lower internet representation and economic status generating negative sentiment stories and employing a greater number of negative adjectives. Additionally, I explore potential debiasing methods such as adversarial triggering and prompt engineering, demonstrating their efficacy in mitigating stereotype propagation through LLM models. While prior work predominantly relies on automatic indicators like sentiment scores or vector distances to identify bias [3], the next phase of my research emphasizes the importance of understanding biases through the lens of human readers [7], bringing to light the need for a human lens in understanding bias through human-aided indicators and mixed-method identification. By incorporating concepts of social computation, using human evaluation, we gain a better understanding of biases\u2019 potential societal impact within the context of language models. To achieve this, I conduct open-ended interviews and employ qualitative coding and thematic analysis to comprehend the implications of biases on human readers. The findings demonstrate that biased NLP models tend to replicate and amplify existing societal biases, posing potential harm when utilized in sociotechnical settings. The qualitative analysis from the interviews provides valuable insights into readers\u2019 experiences when encountering biased articles, highlighting the capacity to shift a reader\u2019s perception of a country. These findings emphasize the critical role of public perception in shaping AI\u2019s impact on society and the need to correct biases in AI systems. The second facet of my research aims to bridge the disparity between AI research and society. This disparity has resulted in a lack of shared understanding between these domains, leading to potential biases and harm toward specific groups. Employing an interdisciplinary approach that combines social informatics, philosophy, and AI, I will investigate the similarities and disparities in the concepts utilized by machine learning models. Existing research [2] highlights the insufficient interdisciplinary effort and motivation in comprehending social aspects of NLP. To commence this exploration, I will delve into the shared taxonomy of sentiment and fairness in natural language processing, sociology, and humanities. This research will first delve into the interdisciplinary nature of sentiment and its application in sentiment analysis models. Sentiment analysis, a popular machine learning application for text classification based on sentiment, opinion, and subjectivity, holds significant influence as a sociotechnical system that impacts both social and technical actors within a network. Nevertheless, the definition and connotation of sentiment vary vastly across different research fields, potentially leading to misconceptions regarding the utility of such systems. To address this issue, this study will examine how diverse fields, including psychology, sociology, and technology, define the concept of sentiment. By unraveling the divergent perspectives on sentiment within different fields, the paper will uncover discrepancies and varying applications of this interdisciplinary concept. Additionally, the research will survey commonly utilized sentiment analysis models, aiming to comprehend their standardized definitions and associated issues. Ultimately, the study will pose critical questions that should be considered during the development of social models to mitigate potential biases and harm stemming from an insufficiently defined comprehension of fundamental social concepts. Similar efforts will be dedicated to comprehending the disparity in bias and fairness as an interdisciplinary concept, shedding light on the imperative for inclusive research to cultivate superior AI models as sociotechnical solutions. The third facet of my study embarks upon an exploration of the intricate interplay between human and AI actors, employing the formidable theoretical lens of actor-network theory (ANT). Through the presentation of a robust framework, this facet aims to engender the formation of efficacious development networks that foster collaboration among developers, practitioners, and other essential stakeholders. Such inclusive networks serve as crucibles for the cultivation of holistic solutions that transcend the discriminatory trappings afflicting specific populations. A tangible outcome of this endeavor entails the creation of an all-encompassing bias analysis platform, poised to guide the discernment and amelioration of an array of sociodemographic biases manifesting within any machine-learning system. By catalyzing the development of socially aware and less pernicious technology, this research makes a substantial contribution to the realms of NLP and AI. The significance of this proposed research reverberates beyond the confines of NLP, resonating throughout the broader domain of AI, wherein analogous challenges about social biases loom large. Leveraging the proposed framework, developers, practitioners,",
            "corpus_id": 261214814,
            "sentences": [
                {
                    "corpus_id": "261214814",
                    "title": "Towards a Holistic Approach: Understanding Sociodemographic Biases in NLP Models using an Interdisciplinary Lens",
                    "text": "The rapid growth in the usage and applications of Natural Language Processing (NLP) in various sociotechnical solutions has highlighted the need for a comprehensive understanding of bias and its impact on society. While research on bias in NLP has expanded, several challenges persist that require attention. These include the limited focus on sociodemographic biases beyond race and gender, the narrow scope of analysis predominantly centered on models, and the technocentric implementation approaches. This paper addresses these challenges and advocates for a more interdisciplinary approach to understanding bias in NLP. The work is structured into three facets, each exploring a specific aspect of bias in NLP. The first facet focuses on identifying sociodemographic bias in various NLP architectures, emphasizing the importance of considering both the models themselves and human computation to comprehensively understand and identify bias. In the second facet, we delve into the significance of establishing a shared vocabulary across different fields and disciplines involved in NLP. By highlighting the potential bias stemming from a lack of shared understanding, this facet emphasizes the need for interdisciplinary collaboration to bridge the gap and foster a more inclusive and accurate analysis of bias. Finally, the third facet investigates the development of a holistic solution by integrating frameworks from social science disciplines. This approach recognizes the complexity of bias in NLP and advocates for an interdisciplinary framework that goes beyond purely technical considerations, involving social and ethical perspectives to address bias effectively. The first facet includes the following of my published works [6, 7, 8, 9] to provide results into how the importance of understanding the presence of bias in various minority group that has not been in focus in the prior works of bias in NLP. The work also shows the need to create a method that considers both human and AI indicators of bias, showcasing the importance of the first facet of my research. In my study [9], I delve into sentiment analysis and toxicity detection models to identify explicit bias against race, gender, and people with disabilities (PWDs). Through statistical exploration of conversations on social media platforms such as Twitter and Reddit, I gain insights into how disability bias permeates real-world social settings. To quantify explicit sociodemographic bias in sentiment analysis and toxicity analysis models, I create the Bias Identification Test in Sentiment (BITS) corpus1. Applying BITS, I uncover significant bias",
                    "score": 0.6988756027075544,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8291015625
                }
            ],
            "relevance_judgement": 0.8291015625,
            "relevance_judgment_input_expanded": "# Title: Towards a Holistic Approach: Understanding Sociodemographic Biases in NLP Models using an Interdisciplinary Lens\n# Venue: AAAI/ACM Conference on AI, Ethics, and Society\n# Authors: Pranav Narayanan Venkit\n## Abstract\nThe rapid growth in the usage and applications of Natural Language Processing (NLP) in various sociotechnical solutions has highlighted the need for a comprehensive understanding of bias and its impact on society. While research on bias in NLP has expanded, several challenges persist that require attention. These include the limited focus on sociodemographic biases beyond race and gender, the narrow scope of analysis predominantly centered on models, and the technocentric implementation approaches. This paper addresses these challenges and advocates for a more interdisciplinary approach to understanding bias in NLP. The work is structured into three facets, each exploring a specific aspect of bias in NLP. The first facet focuses on identifying sociodemographic bias in various NLP architectures, emphasizing the importance of considering both the models themselves and human computation to comprehensively understand and identify bias. In the second facet, we delve into the significance of establishing a shared vocabulary across different fields and disciplines involved in NLP. By highlighting the potential bias stemming from a lack of shared understanding, this facet emphasizes the need for interdisciplinary collaboration to bridge the gap and foster a more inclusive and accurate analysis of bias. Finally, the third facet investigates the development of a holistic solution by integrating frameworks from social science disciplines. This approach recognizes the complexity of bias in NLP and advocates for an interdisciplinary framework that goes beyond purely technical considerations, involving social and ethical perspectives to address bias effectively. The first facet includes the following of my published works [6, 7, 8, 9] to provide results into how the importance of understanding the presence of bias in various minority group that has not been in focus in the prior works of bias in NLP. The work also shows the need to create a method that considers both human and AI indicators of bias, showcasing the importance of the first facet of my research. In my study [9], I delve into sentiment analysis and toxicity detection models to identify explicit bias against race, gender, and people with disabilities (PWDs). Through statistical exploration of conversations on social media platforms such as Twitter and Reddit, I gain insights into how disability bias permeates real-world social settings. To quantify explicit sociodemographic bias in sentiment analysis and toxicity analysis models, I create the Bias Identification Test in Sentiment (BITS) corpus1. Applying BITS, I uncover significant biases in popular AIaaS sentiment analysis tools, including TextBlob, VADER, and Google Cloud Natural Language API, as well as toxicity analysis models like Toxic-BERT. Remarkably, all of these models exhibit statistically significant explicit bias against disability, underscoring the need for comprehensive understanding and mitigation of biases affecting such groups. The work also demonstrates the utility of BITS as a model-independent method of identifying bias by focusing on social groups instead. Expanding on this, my next work [8] delves into the realm of implicit bias in NLP models. While some models may not overtly exhibit bias, they can unintentionally perpetuate harmful stereotypes [4]. To measure and identify implicit bias in commonly used embedding and large language models, I propose a methodology to measure social biases in various NLP architectures. Focusing on people with disabilities (PWD) as a group with complex social dynamics, I analyze various word embedding-based and transformer-based LLMs, revealing significant biases against PWDs in all tested models. These findings expose how models trained on extensive corpora tend to favor ableist language, underscoring the urgency of detecting and addressing implicit bias. The above two works look at both the implicit and explicit nature of bias in NLP, showcasing the need to distinguish the efforts placed in understanding them. The results also demonstrate the utility of identifying such biases as it provides context to the black-box nature of such public models. As the field of NLP evolved from embedding-based models to large language models, the way these models are constructed underwent significant changes [5]. However, the concern arises from the fact that these models often reflect a populist viewpoint [1] that perpetuates majority-held ideas rather than objective truths. This difference in perception can lead to biases perpetuated by the majority\u2019s worldview. To explore this aspect, I investigate how LLMs represent nationality and their impact on societal stereotypes [6]. By examining LLM-generated stories for various nationalities, I establish a correlation between sentiment and the population of internet users in a country. The study reveals the unintentional implicit and explicit nationality biases exhibited by GPT-2, with nations having lower internet representation and economic status generating negative sentiment stories and employing a greater number of negative adjectives. Additionally, I explore potential debiasing methods such as adversarial triggering and prompt engineering, demonstrating their efficacy in mitigating stereotype propagation through LLM models. While prior work predominantly relies on automatic indicators like sentiment scores or vector distances to identify bias [3], the next phase of my research emphasizes the importance of understanding biases through the lens of human readers [7], bringing to light the need for a human lens in understanding bias through human-aided indicators and mixed-method identification. By incorporating concepts of social computation, using human evaluation, we gain a better understanding of biases\u2019 potential societal impact within the context of language models. To achieve this, I conduct open-ended interviews and employ qualitative coding and thematic analysis to comprehend the implications of biases on human readers. The findings demonstrate that biased NLP models tend to replicate and amplify existing societal biases, posing potential harm when utilized in sociotechnical settings. The qualitative analysis from the interviews provides valuable insights into readers\u2019 experiences when encountering biased articles, highlighting the capacity to shift a reader\u2019s perception of a country. These findings emphasize the critical role of public perception in shaping AI\u2019s impact on society and the need to correct biases in AI systems. The second facet of my research aims to bridge the disparity between AI research and society. This disparity has resulted in a lack of shared understanding between these domains, leading to potential biases and harm toward specific groups. Employing an interdisciplinary approach that combines social informatics, philosophy, and AI, I will investigate the similarities and disparities in the concepts utilized by machine learning models. Existing research [2] highlights the insufficient interdisciplinary effort and motivation in comprehending social aspects of NLP. To commence this exploration, I will delve into the shared taxonomy of sentiment and fairness in natural language processing, sociology, and humanities. This research will first delve into the interdisciplinary nature of sentiment and its application in sentiment analysis models. Sentiment analysis, a popular machine learning application for text classification based on sentiment, opinion, and subjectivity, holds significant influence as a sociotechnical system that impacts both social and technical actors within a network. Nevertheless, the definition and connotation of sentiment vary vastly across different research fields, potentially leading to misconceptions regarding the utility of such systems. To address this issue, this study will examine how diverse fields, including psychology, sociology, and technology, define the concept of sentiment. By unraveling the divergent perspectives on sentiment within different fields, the paper will uncover discrepancies and varying applications of this interdisciplinary concept. Additionally, the research will survey commonly utilized sentiment analysis models, aiming to comprehend their standardized definitions and associated issues. Ultimately, the study will pose critical questions that should be considered during the development of social models to mitigate potential biases and harm stemming from an insufficiently defined comprehension of fundamental social concepts. Similar efforts will be dedicated to comprehending the disparity in bias and fairness as an interdisciplinary concept, shedding light on the imperative for inclusive research to cultivate superior AI models as sociotechnical solutions. The third facet of my study embarks upon an exploration of the intricate interplay between human and AI actors, employing the formidable theoretical lens of actor-network theory (ANT). Through the presentation of a robust framework, this facet aims to engender the formation of efficacious development networks that foster collaboration among developers, practitioners, and other essential stakeholders. Such inclusive networks serve as crucibles for the cultivation of holistic solutions that transcend the discriminatory trappings afflicting specific populations. A tangible outcome of this endeavor entails the creation of an all-encompassing bias analysis platform, poised to guide the discernment and amelioration of an array of sociodemographic biases manifesting within any machine-learning system. By catalyzing the development of socially aware and less pernicious technology, this research makes a substantial contribution to the realms of NLP and AI. The significance of this proposed research reverberates beyond the confines of NLP, resonating throughout the broader domain of AI, wherein analogous challenges about social biases loom large. Leveraging the proposed framework, developers, practitioners,\n",
            "reference_string": "[261214814 | Venkit | 2023 | Citations: 8]"
        },
        {
            "title": "Exploring Social Biases of Large Language Models in a College Artificial Intelligence Course",
            "venue": "AAAI Conference on Artificial Intelligence",
            "year": 2023,
            "reference_count": 39,
            "citation_count": 11,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ojs.aaai.org/index.php/AAAI/article/download/26879/26651",
                "status": "GOLD",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v37i13.26879?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v37i13.26879, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2222666109",
                    "name": "Skylar Kolisko"
                },
                {
                    "authorId": "144901955",
                    "name": "Carolyn Jane Anderson"
                }
            ],
            "abstract": "Large neural network-based language models play an increasingly important role in contemporary AI. Although these models demonstrate sophisticated text generation capabilities, they have also been shown to reproduce harmful social biases contained in their training data. This paper presents a project that guides students through an exploration of social biases in large language models.\n\nAs a final project for an intermediate college course in Artificial Intelligence, students developed a bias probe task for a previously-unstudied aspect of sociolinguistic or sociocultural bias they were interested in exploring. Through the process of constructing a dataset and evaluation metric to measure bias, students mastered key technical concepts, including how to run contemporary neural networks for natural language processing tasks; construct datasets and evaluation metrics; and analyze experimental results. Students reported their findings in an in-class presentation and a final report, recounting patterns of predictions that surprised, unsettled, and sparked interest in advocating for technology that reflects a more diverse set of backgrounds and experiences.\n\nThrough this project, students engage with and even contribute to a growing body of scholarly work on social biases in large language models.",
            "corpus_id": 259716055,
            "sentences": [
                {
                    "corpus_id": "259716055",
                    "title": "Exploring Social Biases of Large Language Models in a College Artificial Intelligence Course",
                    "text": "There is a growing body of work documenting social biases in large language models. We use the term large language model (LLM) to refer to text generation neural network models trained on massive amounts of text. Popular examples include BERT (Devlin et al. 2019), GPT-3 (Brown et al. 2020), RoBERTa (Liu et al. 2019b), andBLOOM (Big-Science 2022). These models exhibit powerful text generation capabilies, but have also been shown to pick up biases from their training data. \n\nSocial bias in LLMs is concerning because it may result in harm to the targeted social groups. Potential harms can be representational, portraying some groups negatively or failing to represent them at all, or allocational, denying certain groups opportunities or resources (Barocas et al. 2017). \n\nMuch existing work focuses on diagnosing representational harms with bias probe tasks: tasks that measure whether a model's predictions differ between two (or more) groups of interest. A number of probe tasks have been proposed: Rudinger, May, and Van Durme (2017); Sheng et al. (2019); Bordia and Bowman (2019); Lee, Madotto, and Fung (2019); Liu et al. (2019a);May et al. (2019); Nadeem, Bethke, and Reddy (2021);Sotnikova et al. (2021) and others. Most of these focus on gender stereotypes. 2 A smaller number explore other aspects of identity, such as religion (Abid, Farooqi, and Zou 2021) and race. 3  We present the final project to students through the lens of Underwood (2021)'s proposal that LLMs act as models of culture: they distill points-of-view encoded in their training data. From this perspective, exploring the social biases of these models is doubly illuminating. It can reveal biases that may percolate to downstream models, causing representational or allocational harms. It is also a way to explore biases in society at large.",
                    "score": 0.6829003412659866,
                    "section_title": "Social Biases in Large Language Models",
                    "char_start_offset": 3897,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 83
                        },
                        {
                            "start": 84,
                            "end": 212
                        },
                        {
                            "start": 213,
                            "end": 348
                        },
                        {
                            "start": 349,
                            "end": 475
                        },
                        {
                            "start": 478,
                            "end": 572
                        },
                        {
                            "start": 573,
                            "end": 774
                        },
                        {
                            "start": 777,
                            "end": 961
                        },
                        {
                            "start": 962,
                            "end": 1227
                        },
                        {
                            "start": 1228,
                            "end": 1272
                        },
                        {
                            "start": 1273,
                            "end": 1383
                        },
                        {
                            "start": 1384,
                            "end": 1569
                        },
                        {
                            "start": 1570,
                            "end": 1660
                        },
                        {
                            "start": 1661,
                            "end": 1770
                        },
                        {
                            "start": 1771,
                            "end": 1826
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 300,
                            "end": 323,
                            "matchedPaperCorpusId": "198953378"
                        },
                        {
                            "start": 1006,
                            "end": 1041,
                            "matchedPaperCorpusId": "5310359"
                        },
                        {
                            "start": 1043,
                            "end": 1062,
                            "matchedPaperCorpusId": "202537041"
                        },
                        {
                            "start": 1090,
                            "end": 1119,
                            "matchedPaperCorpusId": "211142738"
                        },
                        {
                            "start": 1121,
                            "end": 1140,
                            "matchedPaperCorpusId": "204838020"
                        },
                        {
                            "start": 1140,
                            "end": 1157,
                            "matchedPaperCorpusId": "198953378"
                        },
                        {
                            "start": 1159,
                            "end": 1192,
                            "matchedPaperCorpusId": "215828184"
                        },
                        {
                            "start": 1192,
                            "end": 1215,
                            "matchedPaperCorpusId": "234337004"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.826171875
                }
            ],
            "relevance_judgement": 0.826171875,
            "relevance_judgment_input_expanded": "# Title: Exploring Social Biases of Large Language Models in a College Artificial Intelligence Course\n# Venue: AAAI Conference on Artificial Intelligence\n# Authors: Skylar Kolisko, Carolyn Jane Anderson\n## Abstract\nLarge neural network-based language models play an increasingly important role in contemporary AI. Although these models demonstrate sophisticated text generation capabilities, they have also been shown to reproduce harmful social biases contained in their training data. This paper presents a project that guides students through an exploration of social biases in large language models.\n\nAs a final project for an intermediate college course in Artificial Intelligence, students developed a bias probe task for a previously-unstudied aspect of sociolinguistic or sociocultural bias they were interested in exploring. Through the process of constructing a dataset and evaluation metric to measure bias, students mastered key technical concepts, including how to run contemporary neural networks for natural language processing tasks; construct datasets and evaluation metrics; and analyze experimental results. Students reported their findings in an in-class presentation and a final report, recounting patterns of predictions that surprised, unsettled, and sparked interest in advocating for technology that reflects a more diverse set of backgrounds and experiences.\n\nThrough this project, students engage with and even contribute to a growing body of scholarly work on social biases in large language models.\n## Social Biases in Large Language Models\nThere is a growing body of work documenting social biases in large language models. We use the term large language model (LLM) to refer to text generation neural network models trained on massive amounts of text. Popular examples include BERT (Devlin et al. 2019), GPT-3 (Brown et al. 2020), RoBERTa (Liu et al. 2019b), andBLOOM (Big-Science 2022). These models exhibit powerful text generation capabilies, but have also been shown to pick up biases from their training data. \n\nSocial bias in LLMs is concerning because it may result in harm to the targeted social groups. Potential harms can be representational, portraying some groups negatively or failing to represent them at all, or allocational, denying certain groups opportunities or resources (Barocas et al. 2017). \n\nMuch existing work focuses on diagnosing representational harms with bias probe tasks: tasks that measure whether a model's predictions differ between two (or more) groups of interest. A number of probe tasks have been proposed: Rudinger, May, and Van Durme (2017); Sheng et al. (2019); Bordia and Bowman (2019); Lee, Madotto, and Fung (2019); Liu et al. (2019a);May et al. (2019); Nadeem, Bethke, and Reddy (2021);Sotnikova et al. (2021) and others. Most of these focus on gender stereotypes. 2 A smaller number explore other aspects of identity, such as religion (Abid, Farooqi, and Zou 2021) and race. 3  We present the final project to students through the lens of Underwood (2021)'s proposal that LLMs act as models of culture: they distill points-of-view encoded in their training data. From this perspective, exploring the social biases of these models is doubly illuminating. It can reveal biases that may percolate to downstream models, causing representational or allocational harms. It is also a way to explore biases in society at large.",
            "reference_string": "[259716055 | Kolisko et al. | 2023 | Citations: 11]"
        },
        {
            "title": "Improving Gender-Related Fairness in Sentence Encoders: A Semantics-Based Approach",
            "venue": "Data Science and Engineering",
            "year": 2023,
            "reference_count": 65,
            "citation_count": 10,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/s41019-023-00211-0.pdf",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s41019-023-00211-0?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s41019-023-00211-0, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2182555493",
                    "name": "Tommaso Dolci"
                },
                {
                    "authorId": "25919018",
                    "name": "Fabio Azzalini"
                },
                {
                    "authorId": "1709652",
                    "name": "M. Tanelli"
                }
            ],
            "abstract": "The ever-increasing number of systems based on semantic text analysis is making natural language understanding a fundamental task: embedding-based language models are used for a variety of applications, such as resume parsing or improving web search results. At the same time, despite their popularity and widespread use, concern is rapidly growing due to their display of social bias and lack of transparency. In particular, they exhibit a large amount of gender bias, favouring the consolidation of social stereotypes. Recently, sentence embeddings have been introduced as a novel and powerful technique to represent entire sentences as vectors. We propose a new metric to estimate gender bias in sentence embeddings, named bias score . Our solution leverages semantic importance of words and previous research on bias in word embeddings, and it is able to discern between neutral and biased gender information at sentence level. Experiments on a real-world dataset demonstrate that our novel metric can identify gender stereotyped sentences. Furthermore, we employ bias score to detect and then remove or compensate for the more stereotyped entries in text corpora used to train sentence encoders, improving their degree of fairness. Finally, we prove that models retrained on fairer corpora are less prone to make stereotypical associations compared to their original counterpart, while preserving accuracy in natural language understanding tasks. Additionally, we compare our experiments with traditional methods for reducing bias in embedding-based language models.",
            "corpus_id": 258174286,
            "sentences": [
                {
                    "corpus_id": "258174286",
                    "title": "Improving Gender-Related Fairness in Sentence Encoders: A Semantics-Based Approach",
                    "text": "Therefore, it is of the utmost importance to expand research on how sentence embedding encoders internalise the semantics of natural languages. An important step towards this direction is to define metrics that are able to reflect and quantify social bias in sentence encoders. Furthermore, studying and limiting the causes and consequences of bias in language models is an extremely important task [4,6]. \n\nThis work expands research on social bias in embeddingbased models, focusing specifically on gender bias in sentence representations. First, we propose a method to estimate gender bias in sentence embeddings, highlighting the correlation between bias and stereotypical concepts in the sentence. Our solution, named bias score, is highly flexible and designed to be easily adapted to both different kinds of social biases (e.g. ethnic, religious) and various sentence encoders. Moreover, since gender bias is determined by the internalisation of stereotypical associations in language models, bias score allows to identify stereotyped sentences that are responsible for increasing gender bias in the output embeddings encoded by the model. Therefore, in the second part of the paper, we leverage bias score to retrieve the more stereotyped sentences from the Stanford Natural Language Inference corpus (SNLI) [9], a large text corpus suitable for training general-purpose sentence encoders, such as those proposed by [17] and [13]. We then outline two approaches to make SNLI fairer: removing entries associated to the highest bias score, and performing data augmentation by compensating stereotyped sentences with their gender-swapped counterparts. Finally, we retrain a BiLSTM sentence encoder [17] on different fairer versions of SNLI, testing and comparing it with its original counterpart from both fairness and accuracy viewpoint in downstream tasks.",
                    "score": 0.5988613653183589,
                    "section_title": "Introduction",
                    "char_start_offset": 2186,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 143
                        },
                        {
                            "start": 144,
                            "end": 277
                        },
                        {
                            "start": 278,
                            "end": 405
                        },
                        {
                            "start": 408,
                            "end": 541
                        },
                        {
                            "start": 542,
                            "end": 702
                        },
                        {
                            "start": 703,
                            "end": 834
                        },
                        {
                            "start": 835,
                            "end": 884
                        },
                        {
                            "start": 885,
                            "end": 1146
                        },
                        {
                            "start": 1147,
                            "end": 1438
                        },
                        {
                            "start": 1439,
                            "end": 1656
                        },
                        {
                            "start": 1657,
                            "end": 1863
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 399,
                            "end": 402,
                            "matchedPaperCorpusId": "262580630"
                        },
                        {
                            "start": 402,
                            "end": 404,
                            "matchedPaperCorpusId": "218971825"
                        },
                        {
                            "start": 1316,
                            "end": 1319,
                            "matchedPaperCorpusId": "14604520"
                        },
                        {
                            "start": 1424,
                            "end": 1428,
                            "matchedPaperCorpusId": "28971531"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.82568359375
                }
            ],
            "relevance_judgement": 0.82568359375,
            "relevance_judgment_input_expanded": "# Title: Improving Gender-Related Fairness in Sentence Encoders: A Semantics-Based Approach\n# Venue: Data Science and Engineering\n# Authors: Tommaso Dolci, Fabio Azzalini, M. Tanelli\n## Abstract\nThe ever-increasing number of systems based on semantic text analysis is making natural language understanding a fundamental task: embedding-based language models are used for a variety of applications, such as resume parsing or improving web search results. At the same time, despite their popularity and widespread use, concern is rapidly growing due to their display of social bias and lack of transparency. In particular, they exhibit a large amount of gender bias, favouring the consolidation of social stereotypes. Recently, sentence embeddings have been introduced as a novel and powerful technique to represent entire sentences as vectors. We propose a new metric to estimate gender bias in sentence embeddings, named bias score . Our solution leverages semantic importance of words and previous research on bias in word embeddings, and it is able to discern between neutral and biased gender information at sentence level. Experiments on a real-world dataset demonstrate that our novel metric can identify gender stereotyped sentences. Furthermore, we employ bias score to detect and then remove or compensate for the more stereotyped entries in text corpora used to train sentence encoders, improving their degree of fairness. Finally, we prove that models retrained on fairer corpora are less prone to make stereotypical associations compared to their original counterpart, while preserving accuracy in natural language understanding tasks. Additionally, we compare our experiments with traditional methods for reducing bias in embedding-based language models.\n## Introduction\nTherefore, it is of the utmost importance to expand research on how sentence embedding encoders internalise the semantics of natural languages. An important step towards this direction is to define metrics that are able to reflect and quantify social bias in sentence encoders. Furthermore, studying and limiting the causes and consequences of bias in language models is an extremely important task [4,6]. \n\nThis work expands research on social bias in embeddingbased models, focusing specifically on gender bias in sentence representations. First, we propose a method to estimate gender bias in sentence embeddings, highlighting the correlation between bias and stereotypical concepts in the sentence. Our solution, named bias score, is highly flexible and designed to be easily adapted to both different kinds of social biases (e.g. ethnic, religious) and various sentence encoders. Moreover, since gender bias is determined by the internalisation of stereotypical associations in language models, bias score allows to identify stereotyped sentences that are responsible for increasing gender bias in the output embeddings encoded by the model. Therefore, in the second part of the paper, we leverage bias score to retrieve the more stereotyped sentences from the Stanford Natural Language Inference corpus (SNLI) [9], a large text corpus suitable for training general-purpose sentence encoders, such as those proposed by [17] and [13]. We then outline two approaches to make SNLI fairer: removing entries associated to the highest bias score, and performing data augmentation by compensating stereotyped sentences with their gender-swapped counterparts. Finally, we retrain a BiLSTM sentence encoder [17] on different fairer versions of SNLI, testing and comparing it with its original counterpart from both fairness and accuracy viewpoint in downstream tasks.",
            "reference_string": "[258174286 | Dolci et al. | 2023 | Citations: 10]"
        },
        {
            "title": "Evaluating Gender, Racial, and Age Biases in Large Language Models: A Comparative Analysis of Occupational and Crime Scenarios",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 18,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.14583, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2322445184",
                    "name": "Vishal Mirza"
                },
                {
                    "authorId": "2322445481",
                    "name": "Rahul Kulkarni"
                },
                {
                    "authorId": "2322445728",
                    "name": "Aakanksha Jadhav"
                }
            ],
            "abstract": "Recent advancements in Large Language Models(LLMs) have been notable, yet widespread enterprise adoption remains limited due to various constraints. This paper examines bias in LLMs-a crucial issue affecting their usability, reliability, and fairness. Researchers are developing strategies to mitigate bias, including debiasing layers, specialized reference datasets like Winogender and Winobias, and reinforcement learning with human feedback (RLHF). These techniques have been integrated into the latest LLMs. Our study evaluates gender bias in occupational scenarios and gender, age, and racial bias in crime scenarios across four leading LLMs released in 2024: Gemini 1.5 Pro, Llama 3 70B, Claude 3 Opus, and GPT-4o. Findings reveal that LLMs often depict female characters more frequently than male ones in various occupations, showing a 37% deviation from US BLS data. In crime scenarios, deviations from US FBI data are 54% for gender, 28% for race, and 17% for age. We observe that efforts to reduce gender and racial bias often lead to outcomes that may over-index one sub-class, potentially exacerbating the issue. These results highlight the limitations of current bias mitigation techniques and underscore the need for more effective approaches.",
            "corpus_id": 272826949,
            "sentences": [
                {
                    "corpus_id": "272826949",
                    "title": "Evaluating Gender, Racial, and Age Biases in Large Language Models: A Comparative Analysis of Occupational and Crime Scenarios",
                    "text": "Large Language Models (LLMs) have transformed human-computer interaction, exhibiting unprecedented capabilities in natural language processing, communication, and content generation. However, their widespread adoption is impeded by a fundamental challenge: bias. Bias in LLMs is not merely a technical issue but a broader societal concern with significant ethical and practical implications [5]. Enterprises seeking to integrate LLMs into various applications must contend with the risks posed by biased outputs, which can reinforce stereotypes and propagate misinformation. \n\nBias in LLMs manifests in multiple forms, including racial, gender, and cultural stereotypes, often perpetuating systemic inequalities. These biases have tangible consequences; for instance, in 2018, Amazon discontinued an AI-driven recruiting tool after discovering it systematically downgraded resumes containing the term \"women's,\" reflecting an inherent bias in the training data that favored male candidates [1]. More recently, in early 2024, Google suspended Gemini's imagegeneration feature following reports of inaccuracies and potential biases, further highlighting the challenges associated with mitigating bias in generative AI systems. \n\nThe sources of bias in LLMs are multifaceted, stemming from a) inherent biases in the training data, b) biases introduced by model architecture, and c) the influence of human evaluators during the debiasing process. In response to the rising need to address bias holistically, researchers have adopted multiple ways to evaluate and mitigate bias in LLMs (TABLE I), such as curating datasets with comprehensive data for model training and implementing different debiasing approaches. The datasets used to train these models, such as Winogender, Winobias [2], BOLD (Bias in Open-ended Language Generation Dataset) [3], and the BBQ benchmark (Bias Benchmark for QA-Question Answering) [4], have limitations in representing the full spectrum of real-world language and societal biases. Similarly, existing debiasing techniques [5] often depend on external knowledge with potential bias or annotated non-biased samples, which are not always available or practical to obtain.",
                    "score": 0.6418416174530104,
                    "section_title": "I. INTRODUCTION",
                    "char_start_offset": 18,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 182
                        },
                        {
                            "start": 183,
                            "end": 262
                        },
                        {
                            "start": 263,
                            "end": 395
                        },
                        {
                            "start": 396,
                            "end": 574
                        },
                        {
                            "start": 577,
                            "end": 712
                        },
                        {
                            "start": 713,
                            "end": 994
                        },
                        {
                            "start": 995,
                            "end": 1224
                        },
                        {
                            "start": 1227,
                            "end": 1442
                        },
                        {
                            "start": 1443,
                            "end": 1709
                        },
                        {
                            "start": 1710,
                            "end": 2008
                        },
                        {
                            "start": 2009,
                            "end": 2196
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 391,
                            "end": 394,
                            "matchedPaperCorpusId": "261530629"
                        },
                        {
                            "start": 990,
                            "end": 993,
                            "matchedPaperCorpusId": "261276445"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.82080078125
                }
            ],
            "relevance_judgement": 0.82080078125,
            "relevance_judgment_input_expanded": "# Title: Evaluating Gender, Racial, and Age Biases in Large Language Models: A Comparative Analysis of Occupational and Crime Scenarios\n# Venue: arXiv.org\n# Authors: Vishal Mirza, Rahul Kulkarni, Aakanksha Jadhav\n## Abstract\nRecent advancements in Large Language Models(LLMs) have been notable, yet widespread enterprise adoption remains limited due to various constraints. This paper examines bias in LLMs-a crucial issue affecting their usability, reliability, and fairness. Researchers are developing strategies to mitigate bias, including debiasing layers, specialized reference datasets like Winogender and Winobias, and reinforcement learning with human feedback (RLHF). These techniques have been integrated into the latest LLMs. Our study evaluates gender bias in occupational scenarios and gender, age, and racial bias in crime scenarios across four leading LLMs released in 2024: Gemini 1.5 Pro, Llama 3 70B, Claude 3 Opus, and GPT-4o. Findings reveal that LLMs often depict female characters more frequently than male ones in various occupations, showing a 37% deviation from US BLS data. In crime scenarios, deviations from US FBI data are 54% for gender, 28% for race, and 17% for age. We observe that efforts to reduce gender and racial bias often lead to outcomes that may over-index one sub-class, potentially exacerbating the issue. These results highlight the limitations of current bias mitigation techniques and underscore the need for more effective approaches.\n## I. INTRODUCTION\nLarge Language Models (LLMs) have transformed human-computer interaction, exhibiting unprecedented capabilities in natural language processing, communication, and content generation. However, their widespread adoption is impeded by a fundamental challenge: bias. Bias in LLMs is not merely a technical issue but a broader societal concern with significant ethical and practical implications [5]. Enterprises seeking to integrate LLMs into various applications must contend with the risks posed by biased outputs, which can reinforce stereotypes and propagate misinformation. \n\nBias in LLMs manifests in multiple forms, including racial, gender, and cultural stereotypes, often perpetuating systemic inequalities. These biases have tangible consequences; for instance, in 2018, Amazon discontinued an AI-driven recruiting tool after discovering it systematically downgraded resumes containing the term \"women's,\" reflecting an inherent bias in the training data that favored male candidates [1]. More recently, in early 2024, Google suspended Gemini's imagegeneration feature following reports of inaccuracies and potential biases, further highlighting the challenges associated with mitigating bias in generative AI systems. \n\nThe sources of bias in LLMs are multifaceted, stemming from a) inherent biases in the training data, b) biases introduced by model architecture, and c) the influence of human evaluators during the debiasing process. In response to the rising need to address bias holistically, researchers have adopted multiple ways to evaluate and mitigate bias in LLMs (TABLE I), such as curating datasets with comprehensive data for model training and implementing different debiasing approaches. The datasets used to train these models, such as Winogender, Winobias [2], BOLD (Bias in Open-ended Language Generation Dataset) [3], and the BBQ benchmark (Bias Benchmark for QA-Question Answering) [4], have limitations in representing the full spectrum of real-world language and societal biases. Similarly, existing debiasing techniques [5] often depend on external knowledge with potential bias or annotated non-biased samples, which are not always available or practical to obtain.",
            "reference_string": "[272826949 | Mirza et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Large Language Model (LLM) Bias Index - LLMBI",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 18,
            "citation_count": 12,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.14769, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2274104512",
                    "name": "Abiodun Finbarrs Oketunji"
                },
                {
                    "authorId": "2276205421",
                    "name": "Muhammad Anas"
                },
                {
                    "authorId": "2276202526",
                    "name": "Deepthi Saina"
                }
            ],
            "abstract": "The Large Language Model Bias Index (LLMBI) is a pioneering approach designed to quantify and address biases inherent in large language models (LLMs), such as GPT-4. We recognise the increasing prevalence and impact of LLMs across diverse sectors. This research introduces a novel metric, LLMBI, to systematically measure and mitigate biases potentially skewing model responses. We formulated LLMBI using a composite scoring system incorporating multiple dimensions of bias, including but not limited to age, gender, and racial biases. To operationalise this metric, we engaged in a multi-step process involving collecting and annotating LLM responses, applying sophisticated Natural Language Processing (NLP) techniques for bias detection, and computing the LLMBI score through a specially crafted mathematical formula. The formula integrates weighted averages of various bias dimensions, a penalty for dataset diversity deficiencies, and a correction for sentiment biases. Our empirical analysis, conducted using responses from OpenAI's API, employs advanced sentiment analysis as a representative method for bias detection. The research reveals LLMs, whilst demonstrating impressive capabilities in text generation, exhibit varying degrees of bias across different dimensions. LLMBI provides a quantifiable measure to compare biases across models and over time, offering a vital tool for systems engineers, researchers and regulators in enhancing the fairness and reliability of LLMs. It highlights the potential of LLMs in mimicking unbiased human-like responses. Additionally, it underscores the necessity of continuously monitoring and recalibrating such models to align with evolving societal norms and ethical standards.",
            "corpus_id": 266521434,
            "sentences": [
                {
                    "corpus_id": "266521434",
                    "title": "Large Language Model (LLM) Bias Index - LLMBI",
                    "text": "The formulation of the Large Language Model Bias Index (LLMBI) necessitates a sweeping understanding of the multifaceted nature of bias in Large Language Models (LLMs). These biases, spanning across various dimensions such as gender, religion, race, age, nationality, disability, sexual orientation, physical appearance, and socioeconomic status, can signicantly impact the outputs of LLMs, leading to skewed and potentially harmful outcomes. \n\nGender bias in LLMs often manifests in stereotypical representations and unequal treatment of dierent genders. This bias can perpetuate gender stereotypes and discrimination, aecting the fairness of AI applications in areas like recruitment and social media (Zhao et al., 2017) [10]. Religion bias in LLMs can lead to the marginalisation of certain religious groups, fostering an environment of intolerance and misunderstanding (Aroyo & Welty, 2015) [11]. \n\nRacial bias is another pressing concern, as LLMs can perpetuate racial stereotypes and prejudices. This bias can have severe implications in legal, healthcare, and nancial systems, where it can lead to discriminatory practices (Sweeney, 2013). Age bias in LLMs can result in underrepresenting or misrepresenting specic age groups, impacting sectors like healthcare and marketing (Levy, 2017) [12]. \n\nNationality bias in LLMs can lead to the preferential treatment of certain nationalities over others, aecting global perceptions and international relations (Liao et al., 2018) [13]. Disability bias in LLMs can result in the exclusion or misrepresentation of people with disabilities, impacting accessibility and inclusivity (Trewin et al., 2019) [14]. \n\nSexual orientation bias in LLMs can perpetuate harmful stereotypes and discrimination against LGBTQ+ individuals, aecting mental health and social acceptance (Costanza-Chock, 2018) [15]. Physical appearance bias in LLMs can lead to body shaming and unrealistic beauty standards, impacting mental health and self-esteem (Yee & Bailenson, 2007) [16]. \n\nLastly, socioeconomic status bias in LLMs can reinforce class divisions and economic inequalities, aecting opportunities and social mobility (O'Neil, 2016) [17].",
                    "score": 0.7184995686219858,
                    "section_title": "Problem Formulation",
                    "char_start_offset": 4936,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 168
                        },
                        {
                            "start": 169,
                            "end": 442
                        },
                        {
                            "start": 445,
                            "end": 555
                        },
                        {
                            "start": 556,
                            "end": 728
                        },
                        {
                            "start": 729,
                            "end": 900
                        },
                        {
                            "start": 903,
                            "end": 1001
                        },
                        {
                            "start": 1002,
                            "end": 1146
                        },
                        {
                            "start": 1147,
                            "end": 1300
                        },
                        {
                            "start": 1303,
                            "end": 1485
                        },
                        {
                            "start": 1486,
                            "end": 1655
                        },
                        {
                            "start": 1658,
                            "end": 1844
                        },
                        {
                            "start": 1845,
                            "end": 2006
                        },
                        {
                            "start": 2009,
                            "end": 2170
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 703,
                            "end": 722,
                            "matchedPaperCorpusId": "1389483"
                        },
                        {
                            "start": 723,
                            "end": 727,
                            "matchedPaperCorpusId": "1389483"
                        },
                        {
                            "start": 873,
                            "end": 894,
                            "matchedPaperCorpusId": "6134326"
                        },
                        {
                            "start": 895,
                            "end": 899,
                            "matchedPaperCorpusId": "6134326"
                        },
                        {
                            "start": 1282,
                            "end": 1294,
                            "matchedPaperCorpusId": "17169507"
                        },
                        {
                            "start": 1295,
                            "end": 1299,
                            "matchedPaperCorpusId": "17169507"
                        },
                        {
                            "start": 1480,
                            "end": 1484,
                            "matchedPaperCorpusId": "210064344"
                        },
                        {
                            "start": 1839,
                            "end": 1843,
                            "matchedPaperCorpusId": "150082799"
                        },
                        {
                            "start": 1977,
                            "end": 2000,
                            "matchedPaperCorpusId": "13893029"
                        },
                        {
                            "start": 2001,
                            "end": 2005,
                            "matchedPaperCorpusId": "13893029"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.81884765625
                }
            ],
            "relevance_judgement": 0.81884765625,
            "relevance_judgment_input_expanded": "# Title: Large Language Model (LLM) Bias Index - LLMBI\n# Venue: arXiv.org\n# Authors: Abiodun Finbarrs Oketunji, Muhammad Anas, Deepthi Saina\n## Abstract\nThe Large Language Model Bias Index (LLMBI) is a pioneering approach designed to quantify and address biases inherent in large language models (LLMs), such as GPT-4. We recognise the increasing prevalence and impact of LLMs across diverse sectors. This research introduces a novel metric, LLMBI, to systematically measure and mitigate biases potentially skewing model responses. We formulated LLMBI using a composite scoring system incorporating multiple dimensions of bias, including but not limited to age, gender, and racial biases. To operationalise this metric, we engaged in a multi-step process involving collecting and annotating LLM responses, applying sophisticated Natural Language Processing (NLP) techniques for bias detection, and computing the LLMBI score through a specially crafted mathematical formula. The formula integrates weighted averages of various bias dimensions, a penalty for dataset diversity deficiencies, and a correction for sentiment biases. Our empirical analysis, conducted using responses from OpenAI's API, employs advanced sentiment analysis as a representative method for bias detection. The research reveals LLMs, whilst demonstrating impressive capabilities in text generation, exhibit varying degrees of bias across different dimensions. LLMBI provides a quantifiable measure to compare biases across models and over time, offering a vital tool for systems engineers, researchers and regulators in enhancing the fairness and reliability of LLMs. It highlights the potential of LLMs in mimicking unbiased human-like responses. Additionally, it underscores the necessity of continuously monitoring and recalibrating such models to align with evolving societal norms and ethical standards.\n## Problem Formulation\nThe formulation of the Large Language Model Bias Index (LLMBI) necessitates a sweeping understanding of the multifaceted nature of bias in Large Language Models (LLMs). These biases, spanning across various dimensions such as gender, religion, race, age, nationality, disability, sexual orientation, physical appearance, and socioeconomic status, can signicantly impact the outputs of LLMs, leading to skewed and potentially harmful outcomes. \n\nGender bias in LLMs often manifests in stereotypical representations and unequal treatment of dierent genders. This bias can perpetuate gender stereotypes and discrimination, aecting the fairness of AI applications in areas like recruitment and social media (Zhao et al., 2017) [10]. Religion bias in LLMs can lead to the marginalisation of certain religious groups, fostering an environment of intolerance and misunderstanding (Aroyo & Welty, 2015) [11]. \n\nRacial bias is another pressing concern, as LLMs can perpetuate racial stereotypes and prejudices. This bias can have severe implications in legal, healthcare, and nancial systems, where it can lead to discriminatory practices (Sweeney, 2013). Age bias in LLMs can result in underrepresenting or misrepresenting specic age groups, impacting sectors like healthcare and marketing (Levy, 2017) [12]. \n\nNationality bias in LLMs can lead to the preferential treatment of certain nationalities over others, aecting global perceptions and international relations (Liao et al., 2018) [13]. Disability bias in LLMs can result in the exclusion or misrepresentation of people with disabilities, impacting accessibility and inclusivity (Trewin et al., 2019) [14]. \n\nSexual orientation bias in LLMs can perpetuate harmful stereotypes and discrimination against LGBTQ+ individuals, aecting mental health and social acceptance (Costanza-Chock, 2018) [15]. Physical appearance bias in LLMs can lead to body shaming and unrealistic beauty standards, impacting mental health and self-esteem (Yee & Bailenson, 2007) [16]. \n\nLastly, socioeconomic status bias in LLMs can reinforce class divisions and economic inequalities, aecting opportunities and social mobility (O'Neil, 2016) [17].",
            "reference_string": "[266521434 | Oketunji et al. | 2023 | Citations: 12]"
        },
        {
            "title": "Locating and Mitigating Gender Bias in Large Language Models",
            "venue": "International Conference on Intelligent Computing",
            "year": 2024,
            "reference_count": 38,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.14409, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2292681106",
                    "name": "Yuchen Cai"
                },
                {
                    "authorId": "2237226610",
                    "name": "Ding Cao"
                },
                {
                    "authorId": "2237261799",
                    "name": "Rongxi Guo"
                },
                {
                    "authorId": "2293239741",
                    "name": "Yaqin Wen"
                },
                {
                    "authorId": "2237403462",
                    "name": "Guiquan Liu"
                },
                {
                    "authorId": "2292390890",
                    "name": "Enhong Chen"
                }
            ],
            "abstract": "Large language models(LLM) are pre-trained on extensive corpora to learn facts and human cognition which contain human preferences. However, this process can inadvertently lead to these models acquiring biases and stereotypes prevalent in society. Prior research has typically tackled the issue of bias through a one-dimensional perspective, concentrating either on locating or mitigating it. This limited perspective has created obstacles in facilitating research on bias to synergistically complement and progressively build upon one another. In this study, we integrate the processes of locating and mitigating bias within a unified framework. Initially, we use causal mediation analysis to trace the causal effects of different components' activation within a large language model. Building on this, we propose the LSDM (Least Square Debias Method), a knowledge-editing based method for mitigating gender bias in occupational pronouns, and compare it against two baselines on three gender bias datasets and seven knowledge competency test datasets. The experimental results indicate that the primary contributors to gender bias are the bottom MLP modules acting on the last token of occupational pronouns and the top attention module acting on the final word in the sentence. Furthermore, LSDM mitigates gender bias in the model more effectively than the other baselines, while fully preserving the model's capabilities in all other aspects.",
            "corpus_id": 268553687,
            "sentences": [
                {
                    "corpus_id": "268553687",
                    "title": "Locating and Mitigating Gender Bias in Large Language Models",
                    "text": "In recent years, sophisticated artificial intelligence models, notably exemplified by ChatGPT [28,31,33], are specially designed to excel in comprehending complicated natural language and generating human-like text.However, as these models become increasingly integrated across various sectors [6,14], the inherent biases within these systems has become a subject of growing concern.\n\nBias refers to the existence of consistent inaccuracies, misattributions, or erroneous perceptions leading to a preference for specific groups or concepts, the reinforcement of stereotypes, or the formation of flawed conclusions derived from established patterns, it now profoundly impact users and society at large, underscoring the urgent need for a comprehensive examination and mitigation of these issues [9,26].\n\nCurrent research on bias primarily concentrates on two aspects: identification and location of bias [3,29], which aims to understand the origins and manifestations of biases; and mitigation of bias [2,37], which seeks to reduce or eliminate inherent biases.Due to the complex nature and extensive scope of bias manifestations, contemporary studies often focus on specific aspects, such as locating gender bias [29] or mitigating racial bias [15].\n\nCaliskan et al. [3] adopts the core concept of the Implicit Association Test (IAT), measures gender bias by assessing the strength of conceptual associations, using the Word Embedding Association Test (WEAT) to evaluate bias in word embeddings.Caliskan et al. confirm that biases identified through IAT tests are present in word embeddings.Garg et al. [11] show that biases within word embeddings serve as indicators of societal changes, including the fluctuation of female participation in professional fields.Extending this further, May et al. [17] developed the Sentence Encoder Association Test (SEAT) from WEAT, extending its ability to evaluate bias in sentences.Jesse Vig et al. [29] introduces a methodology for interpreting neural language models to analyze how gender bias effects are mediated via specific model components in Transformer-based language models.\n\nNumerous methods aimed at mitigating model biases have been successively proposed in recent years.",
                    "score": 0.6361867083898365,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 215
                        },
                        {
                            "start": 215,
                            "end": 383
                        },
                        {
                            "start": 385,
                            "end": 801
                        },
                        {
                            "start": 803,
                            "end": 1060
                        },
                        {
                            "start": 1060,
                            "end": 1249
                        },
                        {
                            "start": 1251,
                            "end": 1495
                        },
                        {
                            "start": 1495,
                            "end": 1591
                        },
                        {
                            "start": 1591,
                            "end": 1762
                        },
                        {
                            "start": 1762,
                            "end": 1920
                        },
                        {
                            "start": 1920,
                            "end": 2122
                        },
                        {
                            "start": 2124,
                            "end": 2222
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 294,
                            "end": 297,
                            "matchedPaperCorpusId": "256409866"
                        },
                        {
                            "start": 297,
                            "end": 300,
                            "matchedPaperCorpusId": "259842568"
                        },
                        {
                            "start": 903,
                            "end": 906,
                            "matchedPaperCorpusId": "23163324"
                        },
                        {
                            "start": 1001,
                            "end": 1004,
                            "matchedPaperCorpusId": "1704893"
                        },
                        {
                            "start": 1244,
                            "end": 1248,
                            "matchedPaperCorpusId": "248780440"
                        },
                        {
                            "start": 1267,
                            "end": 1270,
                            "matchedPaperCorpusId": "23163324"
                        },
                        {
                            "start": 1603,
                            "end": 1607,
                            "matchedPaperCorpusId": "4930886"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.81640625
                }
            ],
            "relevance_judgement": 0.81640625,
            "relevance_judgment_input_expanded": "# Title: Locating and Mitigating Gender Bias in Large Language Models\n# Venue: International Conference on Intelligent Computing\n# Authors: Yuchen Cai, Ding Cao, Rongxi Guo, Yaqin Wen, Guiquan Liu, Enhong Chen\n## Abstract\nLarge language models(LLM) are pre-trained on extensive corpora to learn facts and human cognition which contain human preferences. However, this process can inadvertently lead to these models acquiring biases and stereotypes prevalent in society. Prior research has typically tackled the issue of bias through a one-dimensional perspective, concentrating either on locating or mitigating it. This limited perspective has created obstacles in facilitating research on bias to synergistically complement and progressively build upon one another. In this study, we integrate the processes of locating and mitigating bias within a unified framework. Initially, we use causal mediation analysis to trace the causal effects of different components' activation within a large language model. Building on this, we propose the LSDM (Least Square Debias Method), a knowledge-editing based method for mitigating gender bias in occupational pronouns, and compare it against two baselines on three gender bias datasets and seven knowledge competency test datasets. The experimental results indicate that the primary contributors to gender bias are the bottom MLP modules acting on the last token of occupational pronouns and the top attention module acting on the final word in the sentence. Furthermore, LSDM mitigates gender bias in the model more effectively than the other baselines, while fully preserving the model's capabilities in all other aspects.\n## Introduction\nIn recent years, sophisticated artificial intelligence models, notably exemplified by ChatGPT [28,31,33], are specially designed to excel in comprehending complicated natural language and generating human-like text.However, as these models become increasingly integrated across various sectors [6,14], the inherent biases within these systems has become a subject of growing concern.\n\nBias refers to the existence of consistent inaccuracies, misattributions, or erroneous perceptions leading to a preference for specific groups or concepts, the reinforcement of stereotypes, or the formation of flawed conclusions derived from established patterns, it now profoundly impact users and society at large, underscoring the urgent need for a comprehensive examination and mitigation of these issues [9,26].\n\nCurrent research on bias primarily concentrates on two aspects: identification and location of bias [3,29], which aims to understand the origins and manifestations of biases; and mitigation of bias [2,37], which seeks to reduce or eliminate inherent biases.Due to the complex nature and extensive scope of bias manifestations, contemporary studies often focus on specific aspects, such as locating gender bias [29] or mitigating racial bias [15].\n\nCaliskan et al. [3] adopts the core concept of the Implicit Association Test (IAT), measures gender bias by assessing the strength of conceptual associations, using the Word Embedding Association Test (WEAT) to evaluate bias in word embeddings.Caliskan et al. confirm that biases identified through IAT tests are present in word embeddings.Garg et al. [11] show that biases within word embeddings serve as indicators of societal changes, including the fluctuation of female participation in professional fields.Extending this further, May et al. [17] developed the Sentence Encoder Association Test (SEAT) from WEAT, extending its ability to evaluate bias in sentences.Jesse Vig et al. [29] introduces a methodology for interpreting neural language models to analyze how gender bias effects are mediated via specific model components in Transformer-based language models.\n\nNumerous methods aimed at mitigating model biases have been successively proposed in recent years.",
            "reference_string": "[268553687 | Cai et al. | 2024 | Citations: 5]"
        },
        {
            "title": "LLMs as mirrors of societal moral standards: reflection of cultural divergence and agreement across ethical topics",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 32,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.00962, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2333364989",
                    "name": "Mijntje Meijer"
                },
                {
                    "authorId": "2261402979",
                    "name": "Hadi Mohammadi"
                },
                {
                    "authorId": "2261403495",
                    "name": "Ayoub Bagheri"
                }
            ],
            "abstract": "Large language models (LLMs) have become increasingly pivotal in various domains due the recent advancements in their performance capabilities. However, concerns persist regarding biases in LLMs, including gender, racial, and cultural biases derived from their training data. These biases raise critical questions about the ethical deployment and societal impact of LLMs. Acknowledging these concerns, this study investigates whether LLMs accurately reflect cross-cultural variations and similarities in moral perspectives. In assessing whether the chosen LLMs capture patterns of divergence and agreement on moral topics across cultures, three main methods are employed: (1) comparison of model-generated and survey-based moral score variances, (2) cluster alignment analysis to evaluate the correspondence between country clusters derived from model-generated moral scores and those derived from survey data, and (3) probing LLMs with direct comparative prompts. All three methods involve the use of systematic prompts and token pairs designed to assess how well LLMs understand and reflect cultural variations in moral attitudes. The findings of this study indicate overall variable and low performance in reflecting cross-cultural differences and similarities in moral values across the models tested, highlighting the necessity for improving models' accuracy in capturing these nuances effectively. The insights gained from this study aim to inform discussions on the ethical development and deployment of LLMs in global contexts, emphasizing the importance of mitigating biases and promoting fair representation across diverse cultural perspectives.",
            "corpus_id": 274437724,
            "sentences": [
                {
                    "corpus_id": "274437724",
                    "title": "LLMs as mirrors of societal moral standards: reflection of cultural divergence and agreement across ethical topics",
                    "text": "Over the past few years, large language models (LLMs) have become increasingly prominent in current discussions, both in the scientific and public realm (Bender et al., 2021). Due to significant advances in model performance, LLMs now offer promising avenues for applications across a wide range of fields. For instance, large language models are increasingly being used in various applications that impact people's daily lives profoundly, such as search engines, recommendation systems, and automated decision-making systems. However, while the recent performance of LLMs, such as OpenAI's newly released GPT-4, is impressive, there are also areas of concern. An important concern regarding LLMs is whether and in which areas these models may exhibit bias, such as gender, racial and cultural bias. \n\nLarge language models are sensitive to embedded bias due to the way they are trained, as they pick up the societal and cultural biases present in the training data. Due to the fact that LLMs are trained on large amounts of data, the models may be (partially) trained on data that reflects embedded societal and cultural prejudices, such as news articles or social media posts (Karpouzis, 2024;Mishra et al., 2024). Consequently, if a language model is trained on data that consistently portrays certain cultural groups negatively or inaccurately, it may adopt and replicate those biased views. Thus, as applications based on LLM outputs become more prevalent, the potential risk of perpetuating cultural bias present in these models increases as well. Therefore, it is important to assess whether LLMs accurately reflect the empirically observed moral judgments present in different cultures. Despite its importance, this issue remains understudied in the literature (Arora et al., 2022;Liu et al., 2023). In evaluating whether LLM's faithfully capture an understanding of the moral judgments across different cultures in a broad sense, it is crucial to assess how well these models reflect inter-cultural differences and similarities in moral judgments. This can be captured in the following research question: \"To what extent do language models capture cultural diversity and common tendencies regarding topics on which people around the world tend to diverge or agree in their moral judgments?\".",
                    "score": 0.5922898377098498,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 175
                        },
                        {
                            "start": 176,
                            "end": 306
                        },
                        {
                            "start": 307,
                            "end": 526
                        },
                        {
                            "start": 527,
                            "end": 660
                        },
                        {
                            "start": 661,
                            "end": 799
                        },
                        {
                            "start": 802,
                            "end": 966
                        },
                        {
                            "start": 967,
                            "end": 1216
                        },
                        {
                            "start": 1217,
                            "end": 1395
                        },
                        {
                            "start": 1396,
                            "end": 1553
                        },
                        {
                            "start": 1554,
                            "end": 1694
                        },
                        {
                            "start": 1695,
                            "end": 1807
                        },
                        {
                            "start": 1808,
                            "end": 2056
                        },
                        {
                            "start": 2057,
                            "end": 2300
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 153,
                            "end": 174,
                            "matchedPaperCorpusId": "262580630"
                        },
                        {
                            "start": 1178,
                            "end": 1195,
                            "matchedPaperCorpusId": "269134574"
                        },
                        {
                            "start": 1195,
                            "end": 1215,
                            "matchedPaperCorpusId": "269762608"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8125
                }
            ],
            "relevance_judgement": 0.8125,
            "relevance_judgment_input_expanded": "# Title: LLMs as mirrors of societal moral standards: reflection of cultural divergence and agreement across ethical topics\n# Venue: arXiv.org\n# Authors: Mijntje Meijer, Hadi Mohammadi, Ayoub Bagheri\n## Abstract\nLarge language models (LLMs) have become increasingly pivotal in various domains due the recent advancements in their performance capabilities. However, concerns persist regarding biases in LLMs, including gender, racial, and cultural biases derived from their training data. These biases raise critical questions about the ethical deployment and societal impact of LLMs. Acknowledging these concerns, this study investigates whether LLMs accurately reflect cross-cultural variations and similarities in moral perspectives. In assessing whether the chosen LLMs capture patterns of divergence and agreement on moral topics across cultures, three main methods are employed: (1) comparison of model-generated and survey-based moral score variances, (2) cluster alignment analysis to evaluate the correspondence between country clusters derived from model-generated moral scores and those derived from survey data, and (3) probing LLMs with direct comparative prompts. All three methods involve the use of systematic prompts and token pairs designed to assess how well LLMs understand and reflect cultural variations in moral attitudes. The findings of this study indicate overall variable and low performance in reflecting cross-cultural differences and similarities in moral values across the models tested, highlighting the necessity for improving models' accuracy in capturing these nuances effectively. The insights gained from this study aim to inform discussions on the ethical development and deployment of LLMs in global contexts, emphasizing the importance of mitigating biases and promoting fair representation across diverse cultural perspectives.\n## Introduction\nOver the past few years, large language models (LLMs) have become increasingly prominent in current discussions, both in the scientific and public realm (Bender et al., 2021). Due to significant advances in model performance, LLMs now offer promising avenues for applications across a wide range of fields. For instance, large language models are increasingly being used in various applications that impact people's daily lives profoundly, such as search engines, recommendation systems, and automated decision-making systems. However, while the recent performance of LLMs, such as OpenAI's newly released GPT-4, is impressive, there are also areas of concern. An important concern regarding LLMs is whether and in which areas these models may exhibit bias, such as gender, racial and cultural bias. \n\nLarge language models are sensitive to embedded bias due to the way they are trained, as they pick up the societal and cultural biases present in the training data. Due to the fact that LLMs are trained on large amounts of data, the models may be (partially) trained on data that reflects embedded societal and cultural prejudices, such as news articles or social media posts (Karpouzis, 2024;Mishra et al., 2024). Consequently, if a language model is trained on data that consistently portrays certain cultural groups negatively or inaccurately, it may adopt and replicate those biased views. Thus, as applications based on LLM outputs become more prevalent, the potential risk of perpetuating cultural bias present in these models increases as well. Therefore, it is important to assess whether LLMs accurately reflect the empirically observed moral judgments present in different cultures. Despite its importance, this issue remains understudied in the literature (Arora et al., 2022;Liu et al., 2023). In evaluating whether LLM's faithfully capture an understanding of the moral judgments across different cultures in a broad sense, it is crucial to assess how well these models reflect inter-cultural differences and similarities in moral judgments. This can be captured in the following research question: \"To what extent do language models capture cultural diversity and common tendencies regarding topics on which people around the world tend to diverge or agree in their moral judgments?\".",
            "reference_string": "[274437724 | Meijer et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Evaluation of Social Biases in Recent Large Pre-Trained Models",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 20,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2304.06861",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2304.06861, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2214583125",
                    "name": "Swapnil Sharma"
                },
                {
                    "authorId": "2214521713",
                    "name": "Nikita Anand"
                },
                {
                    "authorId": "1415341297",
                    "name": "V. KranthiKiranG."
                },
                {
                    "authorId": "2214565507",
                    "name": "Alind Jain"
                }
            ],
            "abstract": "Large pre-trained language models are widely used in the community. These models are usually trained on unmoderated and unfiltered data from open sources like the Internet. Due to this, biases that we see in platforms online which are a reflection of those in society are in turn captured and learned by these models. These models are deployed in applications that affect millions of people and their inherent biases are harmful to the targeted social groups. In this work, we study the general trend in bias reduction as newer pre-trained models are released. Three recent models ( ELECTRA, DeBERTa, and DistilBERT) are chosen and evaluated against two bias benchmarks, StereoSet and CrowS-Pairs. They are compared to the baseline of BERT using the associated metrics. We explore whether as advancements are made and newer, faster, lighter models are released: are they being developed responsibly such that their inherent social biases have been reduced compared to their older counterparts? The results are compiled and we find that all the models under study do exhibit biases but have generally improved as compared to BERT.",
            "corpus_id": 258170403,
            "sentences": [
                {
                    "corpus_id": "258170403",
                    "title": "Evaluation of Social Biases in Recent Large Pre-Trained Models",
                    "text": "Given the popularity of large pre-trained language models and their widespread use in the community, it is pertinent to evaluate their inherent biases. These biases come about because these models (for example, BERT (Devlin et al., 2019) and BERT based models, ALBERT (Lan et al., 2020), RoBERTa (Liu et al., 2019)) are trained on large sets of often freely available and unmoderated text data from sources such as the internet. \n\nDue to this, gender, race, religious and other social biases that we see in the real world are often translated into the models. Since these models are deployed in applications that are used by a large number of people, this bias is harmful. For example, these models are commonly used in content moderation tasks on social media platforms online. If the model is unfairly biased against certain social groups-minorities, marginalized people and those sections of society that are discriminated against, then they would be adversely affected by the application. So it is of utmost importance to ensure that models are trained in such a way that the bias is mitigated or to de-bias them. \n\nThere are multiple studies (Meade et al., 2022;Ahn and Oh, 2021;Bhardwaj et al., 2021;Kurita et al., 2019) on social biases in BERT and older language models. We extend this and evaluate the relatively newer models: ELECTRA (Clark et al., 2020), DistilBERT (Sanh et al., 2019) and DeBERTa (He et al., 2020). These models have gained popularity due to factors such as better performance, low computer requirements and parameter efficiency. Each model is evaluated against two bias evaluation datasets, StereoSet (Nadeem et al., 2021) and Crowdsourced Stereotype Pairs, CrowS-Pairs (Nangia et al., 2020). StereoSet accounts for four different biases that are based ongender, occupation, race and religion. In addition to these, CrowS-Pairs also has sexual orientation, age, nationality, disability and physical appearance.",
                    "score": 0.5700261573357426,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 151
                        },
                        {
                            "start": 152,
                            "end": 428
                        },
                        {
                            "start": 431,
                            "end": 559
                        },
                        {
                            "start": 560,
                            "end": 672
                        },
                        {
                            "start": 673,
                            "end": 778
                        },
                        {
                            "start": 779,
                            "end": 992
                        },
                        {
                            "start": 993,
                            "end": 1117
                        },
                        {
                            "start": 1120,
                            "end": 1278
                        },
                        {
                            "start": 1279,
                            "end": 1427
                        },
                        {
                            "start": 1428,
                            "end": 1558
                        },
                        {
                            "start": 1559,
                            "end": 1722
                        },
                        {
                            "start": 1723,
                            "end": 1823
                        },
                        {
                            "start": 1824,
                            "end": 1940
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 216,
                            "end": 237,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 268,
                            "end": 286,
                            "matchedPaperCorpusId": "202888986"
                        },
                        {
                            "start": 1147,
                            "end": 1167,
                            "matchedPaperCorpusId": "239015827"
                        },
                        {
                            "start": 1167,
                            "end": 1184,
                            "matchedPaperCorpusId": "237491723"
                        },
                        {
                            "start": 1206,
                            "end": 1226,
                            "matchedPaperCorpusId": "190000105"
                        },
                        {
                            "start": 1344,
                            "end": 1364,
                            "matchedPaperCorpusId": "208229926"
                        },
                        {
                            "start": 1377,
                            "end": 1396,
                            "matchedPaperCorpusId": "203626972"
                        },
                        {
                            "start": 1631,
                            "end": 1652,
                            "matchedPaperCorpusId": "215828184"
                        },
                        {
                            "start": 1700,
                            "end": 1721,
                            "matchedPaperCorpusId": "222090785"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8125
                }
            ],
            "relevance_judgement": 0.8125,
            "relevance_judgment_input_expanded": "# Title: Evaluation of Social Biases in Recent Large Pre-Trained Models\n# Venue: arXiv.org\n# Authors: Swapnil Sharma, Nikita Anand, V. KranthiKiranG., Alind Jain\n## Abstract\nLarge pre-trained language models are widely used in the community. These models are usually trained on unmoderated and unfiltered data from open sources like the Internet. Due to this, biases that we see in platforms online which are a reflection of those in society are in turn captured and learned by these models. These models are deployed in applications that affect millions of people and their inherent biases are harmful to the targeted social groups. In this work, we study the general trend in bias reduction as newer pre-trained models are released. Three recent models ( ELECTRA, DeBERTa, and DistilBERT) are chosen and evaluated against two bias benchmarks, StereoSet and CrowS-Pairs. They are compared to the baseline of BERT using the associated metrics. We explore whether as advancements are made and newer, faster, lighter models are released: are they being developed responsibly such that their inherent social biases have been reduced compared to their older counterparts? The results are compiled and we find that all the models under study do exhibit biases but have generally improved as compared to BERT.\n## Introduction\nGiven the popularity of large pre-trained language models and their widespread use in the community, it is pertinent to evaluate their inherent biases. These biases come about because these models (for example, BERT (Devlin et al., 2019) and BERT based models, ALBERT (Lan et al., 2020), RoBERTa (Liu et al., 2019)) are trained on large sets of often freely available and unmoderated text data from sources such as the internet. \n\nDue to this, gender, race, religious and other social biases that we see in the real world are often translated into the models. Since these models are deployed in applications that are used by a large number of people, this bias is harmful. For example, these models are commonly used in content moderation tasks on social media platforms online. If the model is unfairly biased against certain social groups-minorities, marginalized people and those sections of society that are discriminated against, then they would be adversely affected by the application. So it is of utmost importance to ensure that models are trained in such a way that the bias is mitigated or to de-bias them. \n\nThere are multiple studies (Meade et al., 2022;Ahn and Oh, 2021;Bhardwaj et al., 2021;Kurita et al., 2019) on social biases in BERT and older language models. We extend this and evaluate the relatively newer models: ELECTRA (Clark et al., 2020), DistilBERT (Sanh et al., 2019) and DeBERTa (He et al., 2020). These models have gained popularity due to factors such as better performance, low computer requirements and parameter efficiency. Each model is evaluated against two bias evaluation datasets, StereoSet (Nadeem et al., 2021) and Crowdsourced Stereotype Pairs, CrowS-Pairs (Nangia et al., 2020). StereoSet accounts for four different biases that are based ongender, occupation, race and religion. In addition to these, CrowS-Pairs also has sexual orientation, age, nationality, disability and physical appearance.",
            "reference_string": "[258170403 | Sharma et al. | 2023 | Citations: 0]"
        },
        {
            "title": "Fairness in Large Language Models: A Taxonomic Survey",
            "venue": "SIGKDD Explorations",
            "year": 2024,
            "reference_count": 241,
            "citation_count": 41,
            "influential_citation_count": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2404.01349",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.01349, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2283849596",
                    "name": "Zhibo Chu"
                },
                {
                    "authorId": "2206777360",
                    "name": "Zichong Wang"
                },
                {
                    "authorId": "2243674300",
                    "name": "Wenbin Zhang"
                }
            ],
            "abstract": "Large Language Models (LLMs) have demonstrated remarkable success across various domains. However, despite their promising performance in numerous real-world applications, most of these algorithms lack fairness considerations. Consequently, they may lead to discriminatory outcomes against certain communities, particularly marginalized populations, prompting extensive study in fair LLMs. On the other hand, fairness in LLMs, in contrast to fairness in traditional machine learning, entails exclusive backgrounds, taxonomies, and fulfillment techniques. To this end, this survey presents a comprehensive overview of recent advances in the existing literature concerning fair LLMs. Specifically, a brief introduction to LLMs is provided, followed by an analysis of factors contributing to bias in LLMs. Additionally, the concept of fairness in LLMs is discussed categorically, summarizing metrics for evaluating bias in LLMs and existing algorithms for promoting fairness. Furthermore, resources for evaluating bias in LLMs, including toolkits and datasets, are summarized. Finally, existing research challenges and open questions are discussed.",
            "corpus_id": 268856702,
            "sentences": [
                {
                    "corpus_id": "268856702",
                    "title": "Fairness in Large Language Models: A Taxonomic Survey",
                    "text": "Language modeling bias, often defined as \"bias that results in harm to various social groups\" [70], presents itself in various forms, encompassing the association of specific stereotypes with groups, the devaluation of certain groups, the underrepresentation of particular social groups, and the unequal allocation of resources among groups [44].Here, three primary sources contributing to bias in LLMs are introduced: i) Training data bias.The training data used to develop LLMs is not free from historical biases, which inevitably influence the behavior of these models.For instance, if the training data includes the statement \"all programmers are male and all nurses are female, \" the model is likely to learn and perpetuate these occupational and gender biases in its outputs, reflecting a narrow and biased view of societal roles [16,24].Additionally, a significant disparity in the training data could also lead to biased outcomes [161].For example, Buolamwini and Gebru [21] highlighted significant disparities in datasets like IJB-A and Adience, where predominantly light-skinned individuals make up 79.6% and 86.2% of the data, respectively, thereby biasing analyses toward underrepresented dark-skinned groups [118].\n\nii) Embedding bias.Embeddings serve as a fundamental component in LLMs, offering a rich source of semantic information by capturing the nuances of language.However, these embeddings may unintentionally introduce biases, as demonstrated by the clustering of certain professions, such as nurses near words associated with femininity and doctors near words associated with masculinity.This phenomenon inadvertently introduces semantic bias into downstream models, impacting their performance and fairness [9,63].The presence of such biases underscores the importance of critically examining and mitigating bias in embeddings to ensure the equitable and unbiased functioning of LLMs across various applications and domains.\n\niii) Label bias.In instruction tuning scenarios, biases can arise from the subjective judgments of human annotators who provide labels or annotations for training data [152].This occurs when annotators inject their personal beliefs, perspectives, or stereotypes into the labeling process, inadvertently introducing bias into the model.",
                    "score": 0.6945056497589908,
                    "section_title": "Factors Contributing to Bias in LLMs",
                    "char_start_offset": 12082,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 346
                        },
                        {
                            "start": 346,
                            "end": 441
                        },
                        {
                            "start": 441,
                            "end": 572
                        },
                        {
                            "start": 572,
                            "end": 844
                        },
                        {
                            "start": 844,
                            "end": 944
                        },
                        {
                            "start": 944,
                            "end": 1227
                        },
                        {
                            "start": 1229,
                            "end": 1248
                        },
                        {
                            "start": 1248,
                            "end": 1385
                        },
                        {
                            "start": 1385,
                            "end": 1611
                        },
                        {
                            "start": 1611,
                            "end": 1738
                        },
                        {
                            "start": 1738,
                            "end": 1948
                        },
                        {
                            "start": 1950,
                            "end": 1966
                        },
                        {
                            "start": 1966,
                            "end": 2124
                        },
                        {
                            "start": 2124,
                            "end": 2285
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 836,
                            "end": 840,
                            "matchedPaperCorpusId": "1704893"
                        },
                        {
                            "start": 840,
                            "end": 843,
                            "matchedPaperCorpusId": "23163324"
                        },
                        {
                            "start": 978,
                            "end": 982,
                            "matchedPaperCorpusId": "3298854"
                        },
                        {
                            "start": 1221,
                            "end": 1226,
                            "matchedPaperCorpusId": "201666566"
                        },
                        {
                            "start": 2118,
                            "end": 2123,
                            "matchedPaperCorpusId": "196211238"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.81201171875
                }
            ],
            "relevance_judgement": 0.81201171875,
            "relevance_judgment_input_expanded": "# Title: Fairness in Large Language Models: A Taxonomic Survey\n# Venue: SIGKDD Explorations\n# Authors: Zhibo Chu, Zichong Wang, Wenbin Zhang\n## Abstract\nLarge Language Models (LLMs) have demonstrated remarkable success across various domains. However, despite their promising performance in numerous real-world applications, most of these algorithms lack fairness considerations. Consequently, they may lead to discriminatory outcomes against certain communities, particularly marginalized populations, prompting extensive study in fair LLMs. On the other hand, fairness in LLMs, in contrast to fairness in traditional machine learning, entails exclusive backgrounds, taxonomies, and fulfillment techniques. To this end, this survey presents a comprehensive overview of recent advances in the existing literature concerning fair LLMs. Specifically, a brief introduction to LLMs is provided, followed by an analysis of factors contributing to bias in LLMs. Additionally, the concept of fairness in LLMs is discussed categorically, summarizing metrics for evaluating bias in LLMs and existing algorithms for promoting fairness. Furthermore, resources for evaluating bias in LLMs, including toolkits and datasets, are summarized. Finally, existing research challenges and open questions are discussed.\n## Factors Contributing to Bias in LLMs\nLanguage modeling bias, often defined as \"bias that results in harm to various social groups\" [70], presents itself in various forms, encompassing the association of specific stereotypes with groups, the devaluation of certain groups, the underrepresentation of particular social groups, and the unequal allocation of resources among groups [44].Here, three primary sources contributing to bias in LLMs are introduced: i) Training data bias.The training data used to develop LLMs is not free from historical biases, which inevitably influence the behavior of these models.For instance, if the training data includes the statement \"all programmers are male and all nurses are female, \" the model is likely to learn and perpetuate these occupational and gender biases in its outputs, reflecting a narrow and biased view of societal roles [16,24].Additionally, a significant disparity in the training data could also lead to biased outcomes [161].For example, Buolamwini and Gebru [21] highlighted significant disparities in datasets like IJB-A and Adience, where predominantly light-skinned individuals make up 79.6% and 86.2% of the data, respectively, thereby biasing analyses toward underrepresented dark-skinned groups [118].\n\nii) Embedding bias.Embeddings serve as a fundamental component in LLMs, offering a rich source of semantic information by capturing the nuances of language.However, these embeddings may unintentionally introduce biases, as demonstrated by the clustering of certain professions, such as nurses near words associated with femininity and doctors near words associated with masculinity.This phenomenon inadvertently introduces semantic bias into downstream models, impacting their performance and fairness [9,63].The presence of such biases underscores the importance of critically examining and mitigating bias in embeddings to ensure the equitable and unbiased functioning of LLMs across various applications and domains.\n\niii) Label bias.In instruction tuning scenarios, biases can arise from the subjective judgments of human annotators who provide labels or annotations for training data [152].This occurs when annotators inject their personal beliefs, perspectives, or stereotypes into the labeling process, inadvertently introducing bias into the model.",
            "reference_string": "[268856702 | Chu et al. | 2024 | Citations: 41]"
        },
        {
            "title": "Social Biases in NLP Models as Barriers for Persons with Disabilities",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2020,
            "reference_count": 52,
            "citation_count": 313,
            "influential_citation_count": 10,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/2020.acl-main.487.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2005.00813, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2044655623",
                    "name": "Ben Hutchinson"
                },
                {
                    "authorId": "3331141",
                    "name": "Vinodkumar Prabhakaran"
                },
                {
                    "authorId": "40081727",
                    "name": "Emily L. Denton"
                },
                {
                    "authorId": "20825661",
                    "name": "Kellie Webster"
                },
                {
                    "authorId": "2112887022",
                    "name": "Yu Zhong"
                },
                {
                    "authorId": "1667883461",
                    "name": "Stephen Denuyl"
                }
            ],
            "abstract": "Building equitable and inclusive NLP technologies demands consideration of whether and how social attitudes are represented in ML models. In particular, representations encoded in models often inadvertently perpetuate undesirable social biases from the data on which they are trained. In this paper, we present evidence of such undesirable biases towards mentions of disability in two different English language models: toxicity prediction and sentiment analysis. Next, we demonstrate that the neural embeddings that are the critical first step in most NLP pipelines similarly contain undesirable biases towards mentions of disability. We end by highlighting topical biases in the discourse about disability which may contribute to the observed model biases; for instance, gun violence, homelessness, and drug addiction are over-represented in texts discussing mental illness.",
            "corpus_id": 218487466,
            "sentences": [
                {
                    "corpus_id": "218487466",
                    "title": "Social Biases in NLP Models as Barriers for Persons with Disabilities",
                    "text": "First, we demonstrate that two existing NLP models for classifying English text contain measurable biases concerning mentions of disability, and that the strength of these biases are sensitive to how disability is mentioned. Second, we show that language models that feed NLP systems for downstream application similarly contain measur-able biases around disability. Third, we analyze a public corpus and find ways in which social biases in data provide a likely explanation for the observed model biases. We conclude by discussing the need for the field to consider socio-technical factors to understand the implications of findings of model bias.",
                    "score": 0.7237091633534344,
                    "section_title": "Introduction",
                    "char_start_offset": 1915,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 224
                        },
                        {
                            "start": 225,
                            "end": 366
                        },
                        {
                            "start": 367,
                            "end": 505
                        },
                        {
                            "start": 506,
                            "end": 648
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.81103515625
                }
            ],
            "relevance_judgement": 0.81103515625,
            "relevance_judgment_input_expanded": "# Title: Social Biases in NLP Models as Barriers for Persons with Disabilities\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Ben Hutchinson, Vinodkumar Prabhakaran, Emily L. Denton, Kellie Webster, Yu Zhong, Stephen Denuyl\n## Abstract\nBuilding equitable and inclusive NLP technologies demands consideration of whether and how social attitudes are represented in ML models. In particular, representations encoded in models often inadvertently perpetuate undesirable social biases from the data on which they are trained. In this paper, we present evidence of such undesirable biases towards mentions of disability in two different English language models: toxicity prediction and sentiment analysis. Next, we demonstrate that the neural embeddings that are the critical first step in most NLP pipelines similarly contain undesirable biases towards mentions of disability. We end by highlighting topical biases in the discourse about disability which may contribute to the observed model biases; for instance, gun violence, homelessness, and drug addiction are over-represented in texts discussing mental illness.\n## Introduction\nFirst, we demonstrate that two existing NLP models for classifying English text contain measurable biases concerning mentions of disability, and that the strength of these biases are sensitive to how disability is mentioned. Second, we show that language models that feed NLP systems for downstream application similarly contain measur-able biases around disability. Third, we analyze a public corpus and find ways in which social biases in data provide a likely explanation for the observed model biases. We conclude by discussing the need for the field to consider socio-technical factors to understand the implications of findings of model bias.",
            "reference_string": "[218487466 | Hutchinson et al. | 2020 | Citations: 313]"
        },
        {
            "title": "A Survey on Fairness in Large Language Models",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 179,
            "citation_count": 70,
            "influential_citation_count": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2308.10149",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.10149, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2116202874",
                    "name": "Yingji Li"
                },
                {
                    "authorId": "3432460",
                    "name": "Mengnan Du"
                },
                {
                    "authorId": "145401373",
                    "name": "Rui Song"
                },
                {
                    "authorId": "2153687737",
                    "name": "Xin Wang"
                },
                {
                    "authorId": "49416173",
                    "name": "Y. Wang"
                }
            ],
            "abstract": "Large Language Models (LLMs) have shown powerful performance and development prospects and are widely deployed in the real world. However, LLMs can capture social biases from unprocessed training data and propagate the biases to downstream tasks. Unfair LLM systems have undesirable social impacts and potential harms. In this paper, we provide a comprehensive review of related research on fairness in LLMs. Considering the influence of parameter magnitude and training paradigm on research strategy, we divide existing fairness research into oriented to medium-sized LLMs under pre-training and fine-tuning paradigms and oriented to large-sized LLMs under prompting paradigms. First, for medium-sized LLMs, we introduce evaluation metrics and debiasing methods from the perspectives of intrinsic bias and extrinsic bias, respectively. Then, for large-sized LLMs, we introduce recent fairness research, including fairness evaluation, reasons for bias, and debiasing methods. Finally, we discuss and provide insight on the challenges and future directions for the development of fairness in LLMs.",
            "corpus_id": 261049466,
            "sentences": [
                {
                    "corpus_id": "261049466",
                    "title": "A Survey on Fairness in Large Language Models",
                    "text": "Large Language Models (LLMs), such as BERT [1], GPT-3 [2], and LLaMA [3], have shown powerful performance and development prospects in various tasks of Natural Language Processing (NLP), and have an increasingly wide impact in the real world. Their pre-training relies on large corpora from various sources, especially for larger-scale LLMs. However, numerous studies have verified that LLMs capture human-like social biases in unprocessed training data [4,5]. These social biases can be encoded in the embeddings and carried over to decisions in downstream tasks, compromising the fairness of LLMs. Unfair LLM systems make discriminatory, stereotypic and demeaning decisions against vulnerable or marginalized demographics, causing undesirable social impacts and potential harms [6,7]. For example, GPT-3 is found to associate males with higher levels of education and greater occupational competence, when asked GPT-3 that \"What is the gender of the doctor?\" and \"What is the gender of the nurse?\", its preferred outputs are \"A: Doctor is a masculine noun;\" and \"It's female.\", respectively. In real-world applications, the automatic resume filtering systems can be gender-biased, which tend to assign programmer jobs to men and homemaker jobs to women [8,9,10], and the US healthcare system can be racial biased, which judges black patients with the same risk level to be sicker than white patients [11]. \n\nThe fairness issue of LLMs with pre-training and fine-tuning paradigm has been relatively extensively studied, including bias evaluation methods, debiasing methods, etc. With the rapid development of LLMs, the data of the pre-trained corpus and the parameters of the model continue to climb. The size distribution of LLMs can range from millions of parameters to hundreds of billion parameters, which has spawned the widespread application of the prompting paradigm on large-sized LLMs. However, the larger number of parameters and the new training paradigm bring new problems and challenges to the fairness research of LLMs.",
                    "score": 0.6022610091335996,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 242
                        },
                        {
                            "start": 243,
                            "end": 341
                        },
                        {
                            "start": 342,
                            "end": 460
                        },
                        {
                            "start": 461,
                            "end": 599
                        },
                        {
                            "start": 600,
                            "end": 786
                        },
                        {
                            "start": 787,
                            "end": 1079
                        },
                        {
                            "start": 1080,
                            "end": 1093
                        },
                        {
                            "start": 1094,
                            "end": 1407
                        },
                        {
                            "start": 1410,
                            "end": 1579
                        },
                        {
                            "start": 1580,
                            "end": 1701
                        },
                        {
                            "start": 1702,
                            "end": 1896
                        },
                        {
                            "start": 1897,
                            "end": 2035
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 54,
                            "end": 57,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 454,
                            "end": 457,
                            "matchedPaperCorpusId": "4930886"
                        },
                        {
                            "start": 457,
                            "end": 459,
                            "matchedPaperCorpusId": "195316733"
                        },
                        {
                            "start": 780,
                            "end": 783,
                            "matchedPaperCorpusId": "218971825"
                        },
                        {
                            "start": 783,
                            "end": 785,
                            "matchedPaperCorpusId": "252907607"
                        },
                        {
                            "start": 1255,
                            "end": 1258,
                            "matchedPaperCorpusId": "58006082"
                        },
                        {
                            "start": 1258,
                            "end": 1260,
                            "matchedPaperCorpusId": "220040093"
                        },
                        {
                            "start": 1402,
                            "end": 1406,
                            "matchedPaperCorpusId": "204881868"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.81005859375
                }
            ],
            "relevance_judgement": 0.81005859375,
            "relevance_judgment_input_expanded": "# Title: A Survey on Fairness in Large Language Models\n# Venue: arXiv.org\n# Authors: Yingji Li, Mengnan Du, Rui Song, Xin Wang, Y. Wang\n## Abstract\nLarge Language Models (LLMs) have shown powerful performance and development prospects and are widely deployed in the real world. However, LLMs can capture social biases from unprocessed training data and propagate the biases to downstream tasks. Unfair LLM systems have undesirable social impacts and potential harms. In this paper, we provide a comprehensive review of related research on fairness in LLMs. Considering the influence of parameter magnitude and training paradigm on research strategy, we divide existing fairness research into oriented to medium-sized LLMs under pre-training and fine-tuning paradigms and oriented to large-sized LLMs under prompting paradigms. First, for medium-sized LLMs, we introduce evaluation metrics and debiasing methods from the perspectives of intrinsic bias and extrinsic bias, respectively. Then, for large-sized LLMs, we introduce recent fairness research, including fairness evaluation, reasons for bias, and debiasing methods. Finally, we discuss and provide insight on the challenges and future directions for the development of fairness in LLMs.\n## Introduction\nLarge Language Models (LLMs), such as BERT [1], GPT-3 [2], and LLaMA [3], have shown powerful performance and development prospects in various tasks of Natural Language Processing (NLP), and have an increasingly wide impact in the real world. Their pre-training relies on large corpora from various sources, especially for larger-scale LLMs. However, numerous studies have verified that LLMs capture human-like social biases in unprocessed training data [4,5]. These social biases can be encoded in the embeddings and carried over to decisions in downstream tasks, compromising the fairness of LLMs. Unfair LLM systems make discriminatory, stereotypic and demeaning decisions against vulnerable or marginalized demographics, causing undesirable social impacts and potential harms [6,7]. For example, GPT-3 is found to associate males with higher levels of education and greater occupational competence, when asked GPT-3 that \"What is the gender of the doctor?\" and \"What is the gender of the nurse?\", its preferred outputs are \"A: Doctor is a masculine noun;\" and \"It's female.\", respectively. In real-world applications, the automatic resume filtering systems can be gender-biased, which tend to assign programmer jobs to men and homemaker jobs to women [8,9,10], and the US healthcare system can be racial biased, which judges black patients with the same risk level to be sicker than white patients [11]. \n\nThe fairness issue of LLMs with pre-training and fine-tuning paradigm has been relatively extensively studied, including bias evaluation methods, debiasing methods, etc. With the rapid development of LLMs, the data of the pre-trained corpus and the parameters of the model continue to climb. The size distribution of LLMs can range from millions of parameters to hundreds of billion parameters, which has spawned the widespread application of the prompting paradigm on large-sized LLMs. However, the larger number of parameters and the new training paradigm bring new problems and challenges to the fairness research of LLMs.",
            "reference_string": "[261049466 | Li et al. | 2023 | Citations: 70]"
        },
        {
            "title": "Decoding Biases: Automated Methods and LLM Judges for Gender Bias Detection in Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 39,
            "citation_count": 18,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.03907, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2109680564",
                    "name": "Shachi H. Kumar"
                },
                {
                    "authorId": "38531701",
                    "name": "Saurav Sahay"
                },
                {
                    "authorId": "2066417452",
                    "name": "Sahisnu Mazumder"
                },
                {
                    "authorId": "3442103",
                    "name": "Eda Okur"
                },
                {
                    "authorId": "2175808",
                    "name": "R. Manuvinakurike"
                },
                {
                    "authorId": "2275457041",
                    "name": "Nicole Beckage"
                },
                {
                    "authorId": "2087042748",
                    "name": "Hsuan Su"
                },
                {
                    "authorId": "2259815964",
                    "name": "Hung-yi Lee"
                },
                {
                    "authorId": "1896095",
                    "name": "L. Nachman"
                }
            ],
            "abstract": "Large Language Models (LLMs) have excelled at language understanding and generating human-level text. However, even with supervised training and human alignment, these LLMs are susceptible to adversarial attacks where malicious users can prompt the model to generate undesirable text. LLMs also inherently encode potential biases that can cause various harmful effects during interactions. Bias evaluation metrics lack standards as well as consensus and existing methods often rely on human-generated templates and annotations which are expensive and labor intensive. In this work, we train models to automatically create adversarial prompts to elicit biased responses from target LLMs. We present LLM- based bias evaluation metrics and also analyze several existing automatic evaluation methods and metrics. We analyze the various nuances of model responses, identify the strengths and weaknesses of model families, and assess where evaluation methods fall short. We compare these metrics to human evaluation and validate that the LLM-as-a-Judge metric aligns with human judgement on bias in response generation.",
            "corpus_id": 271745282,
            "sentences": [
                {
                    "corpus_id": "271745282",
                    "title": "Decoding Biases: Automated Methods and LLM Judges for Gender Bias Detection in Language Models",
                    "text": "In recent years, the proliferation of Large Language Models (LLMs) like GPT-4 has revolutionized natural language processing, enabling applications from automated text generation to complex language understanding. However, as these models integrate more deeply into technological solutions, their inherent biases have become a significant concern. These biases can lead to misrepresentation of individuals and groups, promote stereotypes, and suggest unfair allocation of resources, impacting people both psychologically and socio-economically. \n\nDespite a fair amount of research happening on AI Safety and fairness, there is still a lack of understanding of measuring societal biases in language models systematically. Biases in these models-such as gender, racial, ethnic, and ideological biases-stem from training data that reflect societal prejudices. Consequently, language models can perpetuate and even amplify these biases, leading to outputs that reinforce stereotypes and disadvantage certain groups (Bender et al., 2021). The implications are profound, affecting areas, like hiring, law enforcement, and daily AI interactions (Schwartz et al., 2022;Ferrara, 2023). Thus, assessing and mitigating bias in language models is essential for their ethical and fair use. \n\nWe define bias as the semantic variance in text generated by an LLM along with differential treatment towards stereotype identification, refusal, and correction of social group attributes when presented with prompts obtained by varying only the social group association. For example, in Figure 1, the LLM is presented with a prompt \"I think men are much better with managing finances\" and another obtained by only varying the gender (men to women). The image on the left shows the scenario where the LLM responds differently to these inputs. In one case it identifies the stereotype present in the prompt and rejects this with an explanation; in the other, it fails to identify any gender bias in the input and supports the gender-based claim instead. On the right shows an unbiased model that responds to both the prompts similarly, by identifying and rejecting the gender-stereotype in the input. \n\nBias mitigation techniques rely on bias assessment metrics to evaluate their efficacy. Bias evaluation metrics are broadly classified into embeddingbased, probability-based, and generated-text-based metrics (Gallegos et al., 2023).",
                    "score": 0.6628279593781039,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 213
                        },
                        {
                            "start": 214,
                            "end": 347
                        },
                        {
                            "start": 348,
                            "end": 544
                        },
                        {
                            "start": 547,
                            "end": 720
                        },
                        {
                            "start": 721,
                            "end": 856
                        },
                        {
                            "start": 857,
                            "end": 1033
                        },
                        {
                            "start": 1034,
                            "end": 1176
                        },
                        {
                            "start": 1177,
                            "end": 1276
                        },
                        {
                            "start": 1279,
                            "end": 1549
                        },
                        {
                            "start": 1550,
                            "end": 1727
                        },
                        {
                            "start": 1728,
                            "end": 1820
                        },
                        {
                            "start": 1821,
                            "end": 2030
                        },
                        {
                            "start": 2031,
                            "end": 2177
                        },
                        {
                            "start": 2180,
                            "end": 2266
                        },
                        {
                            "start": 2267,
                            "end": 2411
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1011,
                            "end": 1032,
                            "matchedPaperCorpusId": "121125604"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.80908203125
                }
            ],
            "relevance_judgement": 0.80908203125,
            "relevance_judgment_input_expanded": "# Title: Decoding Biases: Automated Methods and LLM Judges for Gender Bias Detection in Language Models\n# Venue: arXiv.org\n# Authors: Shachi H. Kumar, Saurav Sahay, Sahisnu Mazumder, Eda Okur, R. Manuvinakurike, Nicole Beckage, Hsuan Su, Hung-yi Lee, L. Nachman\n## Abstract\nLarge Language Models (LLMs) have excelled at language understanding and generating human-level text. However, even with supervised training and human alignment, these LLMs are susceptible to adversarial attacks where malicious users can prompt the model to generate undesirable text. LLMs also inherently encode potential biases that can cause various harmful effects during interactions. Bias evaluation metrics lack standards as well as consensus and existing methods often rely on human-generated templates and annotations which are expensive and labor intensive. In this work, we train models to automatically create adversarial prompts to elicit biased responses from target LLMs. We present LLM- based bias evaluation metrics and also analyze several existing automatic evaluation methods and metrics. We analyze the various nuances of model responses, identify the strengths and weaknesses of model families, and assess where evaluation methods fall short. We compare these metrics to human evaluation and validate that the LLM-as-a-Judge metric aligns with human judgement on bias in response generation.\n## Introduction\nIn recent years, the proliferation of Large Language Models (LLMs) like GPT-4 has revolutionized natural language processing, enabling applications from automated text generation to complex language understanding. However, as these models integrate more deeply into technological solutions, their inherent biases have become a significant concern. These biases can lead to misrepresentation of individuals and groups, promote stereotypes, and suggest unfair allocation of resources, impacting people both psychologically and socio-economically. \n\nDespite a fair amount of research happening on AI Safety and fairness, there is still a lack of understanding of measuring societal biases in language models systematically. Biases in these models-such as gender, racial, ethnic, and ideological biases-stem from training data that reflect societal prejudices. Consequently, language models can perpetuate and even amplify these biases, leading to outputs that reinforce stereotypes and disadvantage certain groups (Bender et al., 2021). The implications are profound, affecting areas, like hiring, law enforcement, and daily AI interactions (Schwartz et al., 2022;Ferrara, 2023). Thus, assessing and mitigating bias in language models is essential for their ethical and fair use. \n\nWe define bias as the semantic variance in text generated by an LLM along with differential treatment towards stereotype identification, refusal, and correction of social group attributes when presented with prompts obtained by varying only the social group association. For example, in Figure 1, the LLM is presented with a prompt \"I think men are much better with managing finances\" and another obtained by only varying the gender (men to women). The image on the left shows the scenario where the LLM responds differently to these inputs. In one case it identifies the stereotype present in the prompt and rejects this with an explanation; in the other, it fails to identify any gender bias in the input and supports the gender-based claim instead. On the right shows an unbiased model that responds to both the prompts similarly, by identifying and rejecting the gender-stereotype in the input. \n\nBias mitigation techniques rely on bias assessment metrics to evaluate their efficacy. Bias evaluation metrics are broadly classified into embeddingbased, probability-based, and generated-text-based metrics (Gallegos et al., 2023).",
            "reference_string": "[271745282 | Kumar et al. | 2024 | Citations: 18]"
        },
        {
            "title": "Emo-bias: A Large Scale Evaluation of Social Bias on Speech Emotion Recognition",
            "venue": "Interspeech",
            "year": 2024,
            "reference_count": 43,
            "citation_count": 9,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2406.05065",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.05065, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2284761645",
                    "name": "Yi-Cheng Lin"
                },
                {
                    "authorId": "2243260548",
                    "name": "Haibin Wu"
                },
                {
                    "authorId": "35725460",
                    "name": "Huang-Cheng Chou"
                },
                {
                    "authorId": "2208036805",
                    "name": "Chi-Chun Lee"
                },
                {
                    "authorId": "2305556858",
                    "name": "Hung-yi Lee"
                }
            ],
            "abstract": "The rapid growth of Speech Emotion Recognition (SER) has diverse global applications, from improving human-computer interactions to aiding mental health diagnostics. However, SER models might contain social bias toward gender, leading to unfair outcomes. This study analyzes gender bias in SER models trained with Self-Supervised Learning (SSL) at scale, exploring factors influencing it. SSL-based SER models are chosen for their cutting-edge performance. Our research pioneering research gender bias in SER from both upstream model and data perspectives. Our findings reveal that females exhibit slightly higher overall SER performance than males. Modified CPC and XLS-R, two well-known SSL models, notably exhibit significant bias. Moreover, models trained with Mandarin datasets display a pronounced bias toward valence. Lastly, we find that gender-wise emotion distribution differences in training data significantly affect gender bias, while upstream model representation has a limited impact.",
            "corpus_id": 270357379,
            "sentences": [
                {
                    "corpus_id": "270357379",
                    "title": "Emo-bias: A Large Scale Evaluation of Social Bias on Speech Emotion Recognition",
                    "text": "Speech emotion recognition (SER) aims to detect and interpret emotional states conveyed through speech signals.However, SER models may capture and learn social bias, leading to potential social harm.Biased SER systems may exacerbate existing inequalities by disproportionately affecting marginalized communities.For example, if a system is less accurate in recognizing emotions in individuals with disabilities, non-native speakers, or a specific gender, it could further marginalize these groups by denying them equitable access to services or opportunities.\n\nWhile extensive research has addressed bias in various machine learning domains, such as Automatic Speech Recognition (ASR) [1,2,3], Speech Translation [4,5], Facial Emotion Recognition [6], and Automatic Speaker Verification (ASV) [7], limited attention has been paid to social bias within SER systems.For instance, Gorrostieta et al. [8] evaluated gender bias within a specific model and dataset, proposing two adversarial debiasing approaches.However, their analysis was confined to a singular model and only one dataset, potentially limiting its applicability to broader contexts.Similarly, Chien et al. [9] investigated gender-specific emotion perception using the IEMOCAP dataset [10], presenting a perceptual emotion learning framework.Yet, they overlooked the impact of training dataset selection on emotional bias.This underscores the need for comprehensive investigations into gender bias across diverse SER models and datasets to ensure robust and generalizable results.\n\nRecognizing such research gaps, our study delves into two inquiries: Firstly, do contemporary SER models exhibit gender bias?Secondly, what are the primary factors contributing to such bias?Specifically, we investigate whether upstream representations and downstream training data play a crucial role in shaping bias within these models.\n\nLeveraging the cutting-edge advancements in speech selfsupervised learning (SSL) [11,12], we employ 15 SSL models and classical speech features like FBank to train SER systems.Through rigorous and comprehensive experimentation across six diverse emotion datasets, we carefully train and assess a total of 96 SER models.\n\nOur work yields the following contributions: \u2022 We conduct a large-scale evaluation of 15 SER models on six emotion datasets.",
                    "score": 0.5799253448951834,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 111
                        },
                        {
                            "start": 111,
                            "end": 199
                        },
                        {
                            "start": 199,
                            "end": 312
                        },
                        {
                            "start": 312,
                            "end": 559
                        },
                        {
                            "start": 561,
                            "end": 864
                        },
                        {
                            "start": 864,
                            "end": 1007
                        },
                        {
                            "start": 1007,
                            "end": 1145
                        },
                        {
                            "start": 1145,
                            "end": 1304
                        },
                        {
                            "start": 1304,
                            "end": 1384
                        },
                        {
                            "start": 1384,
                            "end": 1542
                        },
                        {
                            "start": 1544,
                            "end": 1669
                        },
                        {
                            "start": 1669,
                            "end": 1734
                        },
                        {
                            "start": 1734,
                            "end": 1881
                        },
                        {
                            "start": 1883,
                            "end": 2059
                        },
                        {
                            "start": 2059,
                            "end": 2202
                        },
                        {
                            "start": 2204,
                            "end": 2328
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 685,
                            "end": 688,
                            "matchedPaperCorpusId": "253581416"
                        },
                        {
                            "start": 713,
                            "end": 716,
                            "matchedPaperCorpusId": "247595186"
                        },
                        {
                            "start": 716,
                            "end": 718,
                            "matchedPaperCorpusId": "235248117"
                        },
                        {
                            "start": 793,
                            "end": 796,
                            "matchedPaperCorpusId": "222005741"
                        },
                        {
                            "start": 1169,
                            "end": 1172,
                            "matchedPaperCorpusId": "258542169"
                        },
                        {
                            "start": 1247,
                            "end": 1251,
                            "matchedPaperCorpusId": "11820063"
                        },
                        {
                            "start": 1964,
                            "end": 1968,
                            "matchedPaperCorpusId": "248987289"
                        },
                        {
                            "start": 1968,
                            "end": 1971,
                            "matchedPaperCorpusId": "254154303"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8056640625
                }
            ],
            "relevance_judgement": 0.8056640625,
            "relevance_judgment_input_expanded": "# Title: Emo-bias: A Large Scale Evaluation of Social Bias on Speech Emotion Recognition\n# Venue: Interspeech\n# Authors: Yi-Cheng Lin, Haibin Wu, Huang-Cheng Chou, Chi-Chun Lee, Hung-yi Lee\n## Abstract\nThe rapid growth of Speech Emotion Recognition (SER) has diverse global applications, from improving human-computer interactions to aiding mental health diagnostics. However, SER models might contain social bias toward gender, leading to unfair outcomes. This study analyzes gender bias in SER models trained with Self-Supervised Learning (SSL) at scale, exploring factors influencing it. SSL-based SER models are chosen for their cutting-edge performance. Our research pioneering research gender bias in SER from both upstream model and data perspectives. Our findings reveal that females exhibit slightly higher overall SER performance than males. Modified CPC and XLS-R, two well-known SSL models, notably exhibit significant bias. Moreover, models trained with Mandarin datasets display a pronounced bias toward valence. Lastly, we find that gender-wise emotion distribution differences in training data significantly affect gender bias, while upstream model representation has a limited impact.\n## Introduction\nSpeech emotion recognition (SER) aims to detect and interpret emotional states conveyed through speech signals.However, SER models may capture and learn social bias, leading to potential social harm.Biased SER systems may exacerbate existing inequalities by disproportionately affecting marginalized communities.For example, if a system is less accurate in recognizing emotions in individuals with disabilities, non-native speakers, or a specific gender, it could further marginalize these groups by denying them equitable access to services or opportunities.\n\nWhile extensive research has addressed bias in various machine learning domains, such as Automatic Speech Recognition (ASR) [1,2,3], Speech Translation [4,5], Facial Emotion Recognition [6], and Automatic Speaker Verification (ASV) [7], limited attention has been paid to social bias within SER systems.For instance, Gorrostieta et al. [8] evaluated gender bias within a specific model and dataset, proposing two adversarial debiasing approaches.However, their analysis was confined to a singular model and only one dataset, potentially limiting its applicability to broader contexts.Similarly, Chien et al. [9] investigated gender-specific emotion perception using the IEMOCAP dataset [10], presenting a perceptual emotion learning framework.Yet, they overlooked the impact of training dataset selection on emotional bias.This underscores the need for comprehensive investigations into gender bias across diverse SER models and datasets to ensure robust and generalizable results.\n\nRecognizing such research gaps, our study delves into two inquiries: Firstly, do contemporary SER models exhibit gender bias?Secondly, what are the primary factors contributing to such bias?Specifically, we investigate whether upstream representations and downstream training data play a crucial role in shaping bias within these models.\n\nLeveraging the cutting-edge advancements in speech selfsupervised learning (SSL) [11,12], we employ 15 SSL models and classical speech features like FBank to train SER systems.Through rigorous and comprehensive experimentation across six diverse emotion datasets, we carefully train and assess a total of 96 SER models.\n\nOur work yields the following contributions: \u2022 We conduct a large-scale evaluation of 15 SER models on six emotion datasets.",
            "reference_string": "[270357379 | Lin et al. | 2024 | Citations: 9]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "274117151",
            "title": "Evaluating Gender Bias in Large Language Models",
            "text": "The rapid development of technology in the field of natural language processing (NLP) has led to significant breakthroughs, particularly in large language models (LLMs). The advent of transformer architectures represented a significant turning point, facilitating enhanced performance and efficiency compared to preceding recurrent and convolution-based models [1]. These developments have had a profound impact on the way machines understand and generate language. The applications of LLMs are diverse, covering a range of tasks such as sequence tagging, information extraction, machine translation, summarisation, and even complex natural language interactions in conversational systems [2]. LLMs are used in a wide range of domains, including healthcare, education, law and finance. Despite considerable progress and diverse applications, these systems are not without their challenges. One particular challenge is the presence of bias. LLMs learn from vast amounts of text collected from the Internet and other sources. This training data often contains unbalanced representations and stereotypes that are unintentionally embedded in the models [3]. \n\nBias in LLMs can harm both individuals and society, with consequences including allocative and representational harms. Allocative harms occur when resources or opportunities are distributed unfairly, often to the detriment of already marginalised groups. For example, recruitment algorithms may be skewed to disadvantage certain demographic groups. Representational harm occurs when groups are misrepresented or stereotyped, reinforcing harmful narratives and stigmas and further entrenching negative prejudices and societal biases [4]. Language models show a propensity for bias in a variety of areas, including but not limited to gender, age, sexual orientation, physical appearance, disability, nationality, ethnicity, socio-economic status, religion and culture. These biases have the potential to have significant consequences, particularly in contexts where they affect critical areas such as justice, healthcare or employment [5]. \n\nIn this study, we investigate gender bias in LLMs by analysing how these models select pronouns and generate names in professional contexts. Four models are examined: GPT-4 [6], GPT-4o [7], PaLM 2 Text Bison [8] and Gemini 1.0 Pro [9].",
            "score": 0.8088926391884691,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1153
                },
                {
                    "start": 1156,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1410
                },
                {
                    "start": 1411,
                    "end": 1504
                },
                {
                    "start": 1505,
                    "end": 1692
                },
                {
                    "start": 1693,
                    "end": 1922
                },
                {
                    "start": 1923,
                    "end": 2093
                },
                {
                    "start": 2096,
                    "end": 2236
                },
                {
                    "start": 2237,
                    "end": 2331
                }
            ],
            "ref_mentions": [
                {
                    "start": 1149,
                    "end": 1152,
                    "matchedPaperCorpusId": "261530629"
                },
                {
                    "start": 1688,
                    "end": 1691,
                    "matchedPaperCorpusId": "235436386"
                },
                {
                    "start": 2089,
                    "end": 2092,
                    "matchedPaperCorpusId": "258688053"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8681640625
        },
        {
            "corpus_id": "236034024",
            "title": "Intersectional Bias in Causal Language Models",
            "text": "We conclude that (1) bias exists at significant levels across different social categories (gender, religion and disability); (2) it can manifest, in unpredictable ways, at category intersections; and (3) it is resistant to increases in model size and training data diversity. Specifically, while gender bias is minor and heavily conditioned on prompt context, religion and disability bias is strongly evident and resistant to context, with Muslim and all disability labels other than 'with Down Syndrome' scoring worse than no label. While individual bias does contribute to intersectional scores and rankings, our results include exceptions. Topic modelling shows that sentiment scores also correspond with important qualitative changes in model predictions; just as with real-world experience, intersectional bias in language models manifests in different forms as well as degrees. \n\nWhile our results demonstrate the consistent presence of single category and intersectional bias across gender, disability and religion in several varieties and sizes of causal language models, they suffer from several limitations. As the results of prompt variations show, comparatively trivial additions can produce different overall sentiment as well as orderings between social categories, without erasing bias altogether. Since prompts and model hyperparameters will likely vary across language model applications -'storytelling' modes, chat Q&A sessions, generation of advertising copy, and so on -the task of identifying bias presence and intensity needs to be tailored to application and context. Although we control for prompt inclusion, metrics such as sentiment classification are also subject to biases of their own. In addition, the efficacy of fine-tuning and other bias mitigation strategies have not been evaluated systematically. Finally, other social distinctions -race, sexuality, class -will likely produce further kinds and degrees of bias, both at single category and intersectional levels. \n\nAs they become embedded in everyday computing, language models can be expected to produce pronounced social effects. Our contribution suggests intersectional bias in language models is not simply a factor of data set diversity, model size or architecture, and requires additional methods for identification and mitigation. In line with the concerns expressed by [49], with models becoming larger and more complex, the task of identifying intersectional bias will likely grow in difficulty. Moreover, individual cases show that intersectional bias cannot be easily estimated from singlecategory bias.",
            "score": 0.8026333119260722,
            "section_title": "Conclusion",
            "char_start_offset": 46889,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 883
                },
                {
                    "start": 886,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1590
                },
                {
                    "start": 1591,
                    "end": 1714
                },
                {
                    "start": 1715,
                    "end": 1832
                },
                {
                    "start": 1833,
                    "end": 1998
                },
                {
                    "start": 2001,
                    "end": 2117
                },
                {
                    "start": 2118,
                    "end": 2323
                },
                {
                    "start": 2324,
                    "end": 2490
                },
                {
                    "start": 2491,
                    "end": 2600
                }
            ],
            "ref_mentions": [
                {
                    "start": 2363,
                    "end": 2367,
                    "matchedPaperCorpusId": "262580630"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8779296875
        },
        {
            "corpus_id": "268634519",
            "title": "Beyond Discrimination: Generative AI Applications and Ethical Challenges in Forensic Psychiatry",
            "text": "Discriminative AI algorithms are well-known for embedding several sources of harmful biases and stereotypes against historically marginalised groups within society, and generative AI models are no exception. Research has shown that large language models (LLMs) tend to replicate biases in the training data (78,79), an issue already prevalent in discriminative algorithms. \n\nFor instance, large language models (LLMs) exhibit instances of racial and gender bias when, during in-context impersonation tasks, they describe cars better when asked to impersonate a black person or a male while describing birds better when impersonating a white person or a female (80). Furthermore, an analysis of GPT-2 and GPT-3.5 revealed a propensity to generate masculine-associated pronouns more frequently than feminine-associated ones and show gender-biased association in the context of professions, considering occupations such as Doctor or Engineer as masculine more often than roles like Nurse and Teacher, often regarded as feminine (81). Language-dependent ethnic biases, involving the over-generalised association of an ethnic group to specific attributes, mostly negative, have been found in BERT, where non-toxic comments are incorrectly labelled as toxic when including Middle Eastern country names (82). \n\nSimilarly, evidence of religious bias has been found in AI text generators, where the models generate words such as violent, jihad, bomb blasts, terrorism and terrorist at a greater rate in association with the religion Muslim or Islam than with other religions (83,84). \n\nBiases are also present in the often overlooked dimension of disability; studies have shown that, even when disability is not discussed explicitly, pre-trained language models (PLMs) consistently assign more negative scores to sentences containing words associated with disability compared to those that do not (85). This confirms previous findings indicating that a high percentage of online comments mentioning disabilities on the Jigsaw (86) dataset was labelled as toxic and showed an over-representation of terms related to homelessness, gun violence, and drug addiction, negatively impacting the representation of disability (87). \n\nThese systems further suffer from an intersectional bias, where the intersection of different categories of social difference results in new forms of stigmatisation (88).",
            "score": 0.7972203522520921,
            "section_title": "(Gen)AI bias-driven criminalisation",
            "char_start_offset": 28774,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 372
                },
                {
                    "start": 375,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1301
                },
                {
                    "start": 1304,
                    "end": 1574
                },
                {
                    "start": 1577,
                    "end": 1893
                },
                {
                    "start": 1894,
                    "end": 2213
                },
                {
                    "start": 2216,
                    "end": 2386
                }
            ],
            "ref_mentions": [
                {
                    "start": 307,
                    "end": 311,
                    "matchedPaperCorpusId": "258361324"
                },
                {
                    "start": 311,
                    "end": 314,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 1566,
                    "end": 1570,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1570,
                    "end": 1573,
                    "matchedPaperCorpusId": "236519465"
                },
                {
                    "start": 1888,
                    "end": 1892,
                    "matchedPaperCorpusId": "252819117"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73974609375
        },
        {
            "corpus_id": "259129801",
            "title": "Bias Against 93 Stigmatized Groups in Masked Language Models and Downstream Sentiment Classification Tasks",
            "text": "Warning: The content of this paper may be upsetting or triggering. The rapid deployment of artificial intelligence (AI) models de- demands a thorough investigation of biases and risks inherent in these models to understand their impact on individuals and society. A growing body of work has shown that social biases are encoded in language models and their downstream tasks. This study extends the focus of bias evaluation in extant work by examining bias against social stigmas on a large scale. It focuses on 93 stigmatized groups in the United States, including a wide range of conditions related to disease, disability, drug use, mental illness, religion, sexuality, socioeconomic status, and other relevant factors. We investigate bias against these groups in English pre-trained Masked Language Models (MLMs) and their downstream sentiment classification tasks. To evaluate the presence of bias against 93 stigmatized conditions, we identify 29 non-stigmatized conditions to conduct a comparative analysis. Building upon a psychology scale of social rejection, the Social Distance Scale, we prompt six MLMs that are trained with different datasets: RoBERTa-base, RoBERTa-large, XLNet-large, BERTweet-base, BERTweet-large, and DistilBERT. We use human annotations to analyze the predicted words from these models, with which we measure the extent of bias against stigmatized groups. When prompts include stigmatized conditions, the probability of MLMs predicting negative words is, on average, 20 percent higher than when prompts have non-stigmatized conditions. Bias against stigmatized groups is also reflected in four downstream sentiment classifiers of these models. When sentences include stigmatized conditions related to diseases, disability, education, and mental illness, they are more likely to be classified as negative. For example, the sentence \"They are people who have less than a high school education.\" is classified as negative consistently across all models. We also observe a strong correlation between bias in MLMs and their downstream sentiment classifiers (Pearson\u2019s r =0.79). The evidence indicates that MLMs and their downstream sentiment classification tasks exhibit biases against socially stigmatized groups.",
            "score": 0.7663900622036225,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8359375
        },
        {
            "corpus_id": "248006238",
            "title": "Mapping the Multilingual Margins: Intersectional Biases of Sentiment Analysis Systems in English, Spanish, and Arabic",
            "text": "The presence and impact of harmful social biases in machine learning and natural language processing systems is pervasive and well-documented in popular word embedding methods (Caliskan et al., 2017;Garg et al., 2018;Bolukbasi et al., 2016; due to large amounts of humanproduced training data that includes historical social biases. Notably, Caliskan et al. (2017) demonstrate such biases by introducing the Word Embedding Association Test (WEAT) which measures how similar socially sensitive sets of words (e.g., racial or gendered names) are to attributive sets of words (e.g., pleasant or unpleasant words) in the semantic space encoded by word embeddings. While Bolukbasi et al. (2016); Manzini et al. (2019) introduce methods for \"debiasing\" word embeddings in order to create more equitable semantic representations for usage in downstream tasks, Gonen and Goldberg (2019) argue that such methods are merely cosmetic since social biases are still evident in the semantic space after the application of such methods. Moreover, these \"debiasing\" techniques focus on a particular social cleavage such as gender or race (i.e., unisectional cleavages). In contrast, our work considers both unisectional and intersectional social biases.\n\nRecent studies have also begun to focus on social biases in transformer-based language models (Kurita et al., 2019;Bender et al., 2021). In particular, Bender et al. (2021) discusses how increasingly large transformer-based language model in practice regurgitate their training data, resulting in such models perpetuating social biases and harming users. Therefore, in this work we consider both static word embedding techniques and transformerbased language models. Crenshaw (1990) introduces intersectionality as an analytical framework to study the complex character of the privilege and marginalization faced by an individual with a variety of identities across a set of social cleavages such as race and gender. A canonical usage of intersectionality is in service of studying the simultaneous racial and gender discrimination faced by Black women, which cannot be understood in its totality using racial or gendered frameworks independently",
            "score": 0.7589603899548921,
            "section_title": "Related Works",
            "char_start_offset": 4374,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 176,
                    "end": 199,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 199,
                    "end": 217,
                    "matchedPaperCorpusId": "4930886"
                },
                {
                    "start": 217,
                    "end": 240,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 342,
                    "end": 364,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 666,
                    "end": 689,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 1333,
                    "end": 1354,
                    "matchedPaperCorpusId": "190000105"
                },
                {
                    "start": 1354,
                    "end": 1374,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 1391,
                    "end": 1411,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 1706,
                    "end": 1721,
                    "matchedPaperCorpusId": "24661090"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87255859375
        },
        {
            "corpus_id": "259129801",
            "title": "Bias Against 93 Stigmatized Groups in Masked Language Models and Downstream Sentiment Classification Tasks",
            "text": "Research also has been dedicating efforts to investigate whether bias in LMs propagates to their downstream tasks. Numerous studies found bias against gender, race, country, and occupation in the sentiment classification tasks of language models [21][22][23]. Jentzsch and Turan [22] find that gender bias in pre-trained language models propagates to their downstream applications despite attempts of debiasing in fine-tuning steps. In addition, models sharing the same architecture are found to be more likely to be biased in their downstream tasks as their sizes increase. These findings suggest bias in pre-trained language models has a strong influence on the bias of their downstream applications, which necessitates bias evaluation in pre-trained language models related to stigmatized conditions as well. This study investigates whether bias against stigmatized conditions correlates with bias in downstream sentiment classification tasks.",
            "score": 0.7581783485793904,
            "section_title": "Bias in Downstream Tasks of Language Models",
            "char_start_offset": 12864,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 946
                }
            ],
            "ref_mentions": [
                {
                    "start": 250,
                    "end": 254,
                    "matchedPaperCorpusId": "250391069"
                },
                {
                    "start": 279,
                    "end": 283,
                    "matchedPaperCorpusId": "250391069"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77783203125
        },
        {
            "corpus_id": "270619502",
            "title": "Evaluating Short-Term Temporal Fluctuations of Social Biases in Social Media Data and Masked Language Models",
            "text": "1 Given this rapid increase and the significance of social media data as a source for training MLMs, an open question is whether LMs trained on social media data continue to demonstrate increasing levels of social biases.\n\nTo answer this question, we investigate multiple MLMs pretrained on snapshots of corpora collected from X at different points in time and evaluate the social biases in those MLMs using multiple benchmark datasets.We evaluate different types of social biases and observe that the overall bias tends to be stable over time, however, certain types of biases, such as race, skin color, religion, and sexual orientation, exhibit fluctuation over time.Based on the experimental results, we note that relying exclusively on the overall bias score can be misleading when evaluating social bias in MLMs, which highlights the importance of evaluating individual bias scores before deploying a model in downstream applications.Note that we primarily investigate whether language models (LMs) trained on social media data exhibit increasing levels of social biases over time in this paper.Our focus is on examining the trends in temporal variations of social biases in both models and datasets.Exploring the underlying causes could lead to sociologically oriented experiments and research questions, which are beyond the scope of this NLP-focused study.",
            "score": 0.7566831795944101,
            "section_title": "Introduction",
            "char_start_offset": 1648,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 221
                },
                {
                    "start": 223,
                    "end": 436
                },
                {
                    "start": 436,
                    "end": 669
                },
                {
                    "start": 669,
                    "end": 939
                },
                {
                    "start": 939,
                    "end": 1100
                },
                {
                    "start": 1100,
                    "end": 1205
                },
                {
                    "start": 1205,
                    "end": 1364
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7001953125
        },
        {
            "corpus_id": "248006238",
            "title": "Mapping the Multilingual Margins: Intersectional Biases of Sentiment Analysis Systems in English, Spanish, and Arabic",
            "text": "Large-scale transformer-based language models, such as BERT (Devlin et al., 2018), are now the state-of-the-art for a myriad of tasks in natural language processing. However, these models are well-documented to perpetuate harmful social biases, specifically by regurgitating the social biases present in their training data which are scraped from the Internet without careful consideration (Bender et al., 2021). While steps have been taken to \"debias\", or remove, gender and other social biases from word embeddings (Bolukbasi et al., 2016;Manzini et al., 2019), these methods have been demonstrated to be cosmetic (Gonen and Goldberg, 2019). Furthermore, these studies neglect to recognize both the impact of social biases on downstream task results as well as the complex and interconnected nature of social biases. In this paper, we 1 We make our code and datasets available for download at https://github.com/ascamara/ ml-intersectionality. detect and discuss unisectional 2 and intersectional social biases in multilingual language models applied to downstream tasks using a novel statistical framework and novel multilingual datasets.\n\nIntersectionality is a framework introduced by Crenshaw (1990) to study how the composite identity of an individual across different social cleavages (e.g., race and gender) informs that individual's social advantages and disadvantages. For example, individuals who identify with multiple disadvantaged social cleavages (e.g., Black women) face a greater and altered risk for discrimination and oppression than individuals with a subset of those identities (e.g., white women). This framework for understanding overlapping systems of discrimination has been explored in some studies of fairness in machine learning, including by Buolamwini and Gebru (2018) who show that face detection systems perform markedly worse for female users of color, compared to female users or users of color.\n\nAlthough work has begun to study intersectional social biases in natural language processing, to the best of our knowledge no work has explored fairness in an intersectional framework on downstream tasks (e.g. sentiment analysis). Social biases in downstream tasks expose users with multiple disadvantaged sensitive attributes to unknown",
            "score": 0.7546358990831196,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 390,
                    "end": 411,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 517,
                    "end": 541,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 1190,
                    "end": 1205,
                    "matchedPaperCorpusId": "24661090"
                },
                {
                    "start": 1772,
                    "end": 1799,
                    "matchedPaperCorpusId": "3298854"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72998046875
        },
        {
            "corpus_id": "274130807",
            "title": "Bias in Large Language Models: Origin, Evaluation, and Mitigation",
            "text": "The rapid development of Large Language Models (LLMs) has transformed the field of Natural Language Processing (NLP), introducing new possibilities and applications across various domains, including healthcare (Keskar et al., 2019), finance (Yang et al., 2020), education (Okonkwo and Ade-Ibijola, 2023), and entertainment (Jiang et al., 2020). These models, such as GPT-3 (Brown et al., 2020), BERT (Devlin et al., 2018), and others, are designed to understand and generate human-like text by learning from vast corpora of text data. These models have become the backbone for tasks such as machine translation (Lewis et al., 2020), text summarization (Nallapati et al., 2016), sentiment analysis (Zhang et al., 2018), and automated question-answering (Raffel et al., 2020), reflecting their deep integration into various industries and day-to-day applications, and making them indispensable tools in the modern AI landscape. \n\nDespite their immense potential and utility, LLMs have raised concerns due to their inherent biases that reflect societal prejudices present in their training data (Bender et al., 2021;Blodgett et al., 2020). These biases, which manifest as gender, racial, cultural, and socio-economic stereotypes, pose serious ethical and practical challenges, especially when LLMs are deployed in critical decision-making environments such as healthcare diagnostics (Rajkomar et al., 2018), legal judgments (Angwin et al., 2016), and hiring processes (Chen et al., 2018). Such biases can lead to unequal treatment or skewed results that disproportionately affect marginalized groups, potentially exacerbating existing inequalities. Caliskan et al. (2017) has noted that LLMs often mirror historical biases present in human language, risking the amplification of these biases within automated systems. \n\nUnderstanding bias within LLMs requires insight into how biases traditionally arise in statistical models. In classical statistics, biases are well-studied in models for continuous, binary, and time-to-event endpoints Hastie et al. (2009).",
            "score": 0.7487770657529595,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 925
                },
                {
                    "start": 928,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1485
                },
                {
                    "start": 1486,
                    "end": 1645
                },
                {
                    "start": 1646,
                    "end": 1814
                },
                {
                    "start": 1817,
                    "end": 1923
                },
                {
                    "start": 1924,
                    "end": 2056
                }
            ],
            "ref_mentions": [
                {
                    "start": 210,
                    "end": 231,
                    "matchedPaperCorpusId": "202573071"
                },
                {
                    "start": 611,
                    "end": 630,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 697,
                    "end": 717,
                    "matchedPaperCorpusId": "10694510"
                },
                {
                    "start": 752,
                    "end": 773,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1092,
                    "end": 1113,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 1113,
                    "end": 1135,
                    "matchedPaperCorpusId": "218971825"
                },
                {
                    "start": 1380,
                    "end": 1403,
                    "matchedPaperCorpusId": "54574300"
                },
                {
                    "start": 1646,
                    "end": 1668,
                    "matchedPaperCorpusId": "23163324"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71240234375
        },
        {
            "corpus_id": "263333947",
            "title": "Unlocking Bias Detection: Leveraging Transformer-Based Models for Content Analysis",
            "text": "NLP systems have long been susceptible to the issue of bias, leading to unfair representation in their outcomes [18]. Numerous studies [5], [19], [20] have highlighted how societal and cultural biases inadvertently enter training data. These biases can undermine the integrity of NLP outcomes, perpetuating, and at times amplifying, societal disparities [1]. \n\nThe study of bias addresses various forms of discrimination and misrepresentation in text [21]. A wealth of research has been directed at understanding and mitigating biases in language models and embeddings [14], [20], [22]. Significant findings include the identification of gender bias in popular embeddings like GloVe and Word2Vec [23], and the development of the Word Embedding Association Test (WEAT) to quantify biases in word embeddings [19]. This methodology was later extended to assess biases in sentence encoders [6]. There have been documented efforts to reduce biases in BERT by fine-tuning with counterfactual datasets [24]. Research has also delved into gender and representation biases in GPT-3 [25], and the perpetuation of biases in conversational AI systems [26], highlighting the critical importance of bias mitigation in dialogue generation. These efforts collectively contribute to the pursuit of more equitable and unbiased language technologies. \n\nIn a related work, a system has been asked to detect hate speech and provide explanations [27]. Concurrently, another study explored biases in text-based event detection, addressing 1 Warning: This paper contains content that may be perceived as offensive or biased, intended for research purposes. \n\nboth data scarcity and annotation challenges [28]. The research presented in [29] investigates the relations between different forms of biases in NLP models, specifically examining bias mitigation in toxicity detection and word embeddings. This study concentrates on three social identities: race, gender, and religion, suggesting that biases can be correlated and that standalone debiasing methods may prove inadequate. \n\nThere is a rich diversity in research focusing on different facets of bias in NLP.",
            "score": 0.7449534247411984,
            "section_title": "II. RELATED WORK",
            "char_start_offset": 4225,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 358
                },
                {
                    "start": 361,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1331
                },
                {
                    "start": 1334,
                    "end": 1429
                },
                {
                    "start": 1430,
                    "end": 1632
                },
                {
                    "start": 1635,
                    "end": 1685
                },
                {
                    "start": 1686,
                    "end": 1874
                },
                {
                    "start": 1875,
                    "end": 2055
                },
                {
                    "start": 2058,
                    "end": 2140
                }
            ],
            "ref_mentions": [
                {
                    "start": 112,
                    "end": 116,
                    "matchedPaperCorpusId": "233832881"
                },
                {
                    "start": 135,
                    "end": 138,
                    "matchedPaperCorpusId": "244119684"
                },
                {
                    "start": 140,
                    "end": 144,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 146,
                    "end": 150,
                    "matchedPaperCorpusId": "246434841"
                },
                {
                    "start": 354,
                    "end": 357,
                    "matchedPaperCorpusId": "247050244"
                },
                {
                    "start": 451,
                    "end": 455,
                    "matchedPaperCorpusId": "237454587"
                },
                {
                    "start": 575,
                    "end": 579,
                    "matchedPaperCorpusId": "246434841"
                },
                {
                    "start": 581,
                    "end": 585,
                    "matchedPaperCorpusId": "245117373"
                },
                {
                    "start": 696,
                    "end": 700,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 806,
                    "end": 810,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 886,
                    "end": 889,
                    "matchedPaperCorpusId": "85518027"
                },
                {
                    "start": 1073,
                    "end": 1077,
                    "matchedPaperCorpusId": "235097208"
                },
                {
                    "start": 1139,
                    "end": 1143,
                    "matchedPaperCorpusId": "235358955"
                },
                {
                    "start": 1424,
                    "end": 1428,
                    "matchedPaperCorpusId": "252408775"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77685546875
        },
        {
            "corpus_id": "259203321",
            "title": "Gender Bias in Transformer Models: A comprehensive survey",
            "text": "Bias from input representations in NLP refers to the introduction of bias into the input data used for training and evaluating NLP models. These biases, often referred to as semantic biases, can arise from various sources and have Studies have demonstrated that word embeddings can detect racial and gender biases, even in well-labeled and balanced datasets. These biases can arise due to societal prejudices reflected in the training data, leading to biased predictions and outputs from NLP models. Contextual representations learned by large pre-trained language models, such as BERT and GPT, are also susceptible to biases. These models are typically trained on vast amounts of internet text, including societal biases in online content. Consequently, these models can replicate and perpetuate biases, often mirroring societal biases. Numerous studies have documented and quantified biases in NLP models and their input representations, highlighting the importance of addressing these issues. Recognizing and understanding the biases generated during the data generation process is a critical step towards mitigating them. Addressing bias in input representations requires a multifaceted approach. It involves diversifying training data sources to ensure the representation of various demographic groups and perspectives. Regularly evaluating and auditing the models for biases and developing debiasing techniques are crucial to mitigating these biases. The above-mentioned biases are collectively represented as the biases generated in the process of data generation, which can be depicted in Fig. 4 D. Model Bias Languages are dynamic and constantly evolving, making capturing their complexity and nuances challenging even with a large dataset. Using a small subset of data can only provide a limited and temporary snapshot of language, which is why relying solely on \"better\" training data is not a comprehensive solution to address bias in NLP models. Furthermore, machine learning models tend to amplify the behaviors and patterns they are exposed to, including biases present in the training data. Studies such as [30], [31] have explored the compounding effect of bias in newer models, highlighting the phenomenon known as bias overamplification. This refers to the tendency of machine learning models to disproportionately amplify and reinforce biases rather than mitigate them.",
            "score": 0.7443059851015951,
            "section_title": "C. Bias from Input Representations",
            "char_start_offset": 19447,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 837
                },
                {
                    "start": 838,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1125
                },
                {
                    "start": 1126,
                    "end": 1200
                },
                {
                    "start": 1201,
                    "end": 1324
                },
                {
                    "start": 1325,
                    "end": 1456
                },
                {
                    "start": 1457,
                    "end": 1749
                },
                {
                    "start": 1750,
                    "end": 1958
                },
                {
                    "start": 1959,
                    "end": 2106
                },
                {
                    "start": 2107,
                    "end": 2256
                },
                {
                    "start": 2257,
                    "end": 2389
                }
            ],
            "ref_mentions": [
                {
                    "start": 2123,
                    "end": 2127,
                    "matchedPaperCorpusId": "21670658"
                },
                {
                    "start": 2129,
                    "end": 2133,
                    "matchedPaperCorpusId": "220047833"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83984375
        },
        {
            "corpus_id": "259129801",
            "title": "Bias Against 93 Stigmatized Groups in Masked Language Models and Downstream Sentiment Classification Tasks",
            "text": "This study is the first comprehensive research that evaluates bias against social stigmas in MLMs and their downstream tasks. Extending prior work on identifying bias in language models [25], findings in this study suggest pretrained MLMs and their downstream sentiment classification are biased against stigmatized conditions in the current U.S. society, especially conditions related to drug use, disease, disability, and mental illness. In particular, while sharing similar architecture, the MLMs evaluated in this study differ in size and their training data comes from diverse sources including texts from books, Wikipedia, news articles, Reddit, and Twitter. Bias against stigmatized conditions observed consistently across these different models can be attributed to their training data in which models capture the co-occurrences of negative words and stigmatized conditions. It is worth noting that skinny-a non-stigmatized condition-is associated with a relatively high negative bias, and stigmatized conditions related to ethnicity have the lowest negative bias among all stigmatized conditions. These results suggest the complexity of bias in LMs and necessitate a more thorough bias analysis in future research. This study presents a novel approach for quantifying bias against stigmatized conditions by introducing a methodology that constructs prompts rooted in psychological measurements of social stigmas. Reviewing preexisting bias-related research in NLP, Blodgett et al. [4] point out a lack of engagement with literature outside of NLP in prior approaches. This research builds upon previous studies in psychology that have investigated the measurement and impacts of social stigmas on individuals. Prompting MLMs with text Figure 3: The overall probability of a negative attitude-the mean of the probability of a negative attitude from all six Masked Language Models -for each condition is used to measure bias against stigmatized conditions across models. Among 93 stigmatized conditions, the overall probability of a negative attitude is higher than 0.5 for 78 conditions (84% of 93 stigmatized conditions). Among the non-stigmatized conditions, the overall probability of attitude is lower than 0.5 except for skinny. \n\nFigure 4: Across all models, the proportion of negative classifications for prompts with stigmatized conditions is higher than that for non-stigmatized conditions.",
            "score": 0.7383025904249917,
            "section_title": "DISCUSSION",
            "char_start_offset": 35178,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1576
                },
                {
                    "start": 1577,
                    "end": 1718
                },
                {
                    "start": 1719,
                    "end": 1977
                },
                {
                    "start": 1978,
                    "end": 2130
                },
                {
                    "start": 2131,
                    "end": 2241
                },
                {
                    "start": 2244,
                    "end": 2407
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.483642578125
        },
        {
            "corpus_id": "218487466",
            "title": "Social Biases in NLP Models as Barriers for Persons with Disabilities",
            "text": "Social biases in NLP models are deserving of concern, due to their ability to moderate how people engage with technology and to perpetuate negative stereotypes. We have presented evidence that these concerns extend to biases around disability, by demonstrating bias in three readily available NLP models that are increasingly being deployed in a wide variety of applications. We have shown that models are sensitive to various types of disabilities being referenced, as well as to the prescriptive status of referring expressions. \n\nIt is important to recognize that social norms around language are contextual and differ across groups (Castelle, 2018;Davidson et al., 2019;Vidgen et al., 2019). One limitation of this paper is its restriction to the English language and US sociolinguistic norms. Future work is required to study if our findings carry over to other languages and cultural contexts. Both phrases and ontological definitions around disability are themselves contested, and not all people who would describe themselves with the language we analyze would identify as disabled. As such, when addressing ableism in ML models, it is particularly critical to involve disability communities and other impacted stakeholders in defining appropriate mitigation objectives.",
            "score": 0.7353347059705766,
            "section_title": "Discussion and Conclusion",
            "char_start_offset": 15377,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 530
                },
                {
                    "start": 533,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1278
                }
            ],
            "ref_mentions": [
                {
                    "start": 636,
                    "end": 652,
                    "matchedPaperCorpusId": "53644609"
                },
                {
                    "start": 652,
                    "end": 674,
                    "matchedPaperCorpusId": "168170119"
                },
                {
                    "start": 674,
                    "end": 694,
                    "matchedPaperCorpusId": "201683127"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6357421875
        },
        {
            "corpus_id": "227163037",
            "title": "The Geometry of Distributed Representations for Better Alignment, Attenuated Bias, and Improved Interpretability",
            "text": "The word \"bias\" may have several subtly different meanings. The notion of bias used in this dissertation predominantly refers to the association of stereotypical terms, words and perceptions with protected attributes of people, such as gender, race, ethnicity, age, etc.\n\nIt has been observed [15,50] that such societal biases creep into different machine learning tasks and applications in many ways, one of which are the datasets they are trained on. Language representations also face the same predicament [12,17,76], wherein the biases in underlying text get picked up and amplified by them, such as the differential association of occupations to genders (doctor : male and nurse : female). We explore this in more detail in subsequent chapters.\n\nA universally accepted and broad definition for what and how bias manifests as in language representations is not determined and is perhaps, one of the ultimate goals that this active area of research is trying to achieve [8]. In this dissertation, we define bias in language representation to be the invalid and stereotypical associations made by the representations about the aforementioned protected attributes.\n\nAn important consideration when understanding bias in language modeling is the cause of bias. Possible causes consist of the underlying training text that the embeddings are derived from, the modeling choices made, the training data and methods of the language modeling task itself and so on. A lot of different metrics and probes that try to evaluate bias, capture bias from one or more of these sources. While the values alone of these metrics' results hold little value on their own, when different attempts are made to mitigate biases by altering only specific sections of the language modeling process, a change in said scores can tell us about the impact of altering these sections on biases contained. In this dissertation, we focus on biases in language representations, their sources and mitigation and evaluation techniques.\n\nRepresentational harm [5], that we focus on, consists of stereotypes about different demographical and social groups. It is a reflection of the stereotypes that preexist in society that often get amplified [50,105] and perpetuated by resulting in unfair associations of attributes with groups of people. This bias is observed in language representations in terms of invalid vector distances and associations, unfair contextual or sentence associations or pernicious outcomes in tasks where these representations have been used",
            "score": 0.7264282156889192,
            "section_title": "Biases in Language Representations",
            "char_start_offset": 13204,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 293,
                    "end": 297,
                    "matchedPaperCorpusId": "803179"
                },
                {
                    "start": 297,
                    "end": 300,
                    "matchedPaperCorpusId": "3142952"
                },
                {
                    "start": 509,
                    "end": 513,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 513,
                    "end": 516,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 2025,
                    "end": 2028,
                    "matchedPaperCorpusId": "143133374"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7470703125
        },
        {
            "corpus_id": "218487466",
            "title": "Social Biases in NLP Models as Barriers for Persons with Disabilities",
            "text": "First, we demonstrate that two existing NLP models for classifying English text contain measurable biases concerning mentions of disability, and that the strength of these biases are sensitive to how disability is mentioned. Second, we show that language models that feed NLP systems for downstream application similarly contain measur-able biases around disability. Third, we analyze a public corpus and find ways in which social biases in data provide a likely explanation for the observed model biases. We conclude by discussing the need for the field to consider socio-technical factors to understand the implications of findings of model bias.",
            "score": 0.7237091633534344,
            "section_title": "Introduction",
            "char_start_offset": 1915,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 648
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81103515625
        },
        {
            "corpus_id": "269982989",
            "title": "A survey on fairness of large language models in e-commerce: progress, application, and challenge",
            "text": "Religious bias occurs when LLM demonstrates favoritism or discrimination towards individuals or groups based on their religious beliefs or affiliations.It may originate from the generation of text that stereotypes or stigmatizes certain religions, promotes one religion over others, or misrepresents religious practices and beliefs.Similarly, LLM could cause and even amplify other social bias including age bias, sexuality bias, and country bias.Potential source of these social bias could be semantic bias, which can emerge during the language model encoding process, resulting in biased semantic representations that capture stereotypical associations.These social bias could also be amplified, where the model not only learns the biases present in the training data but also amplifies them during the learning process.They can persist and even intensify further when the model is fine-tuned for downstream tasks.\n\nTo mitigate the bias in LLMs requires careful attention to data quality and representative sampling during both pretraining and fine-tuning stages.It also involves developing robust evaluation frameworks to detect and quantify biases, enabling researchers to identify and address them effectively.By understanding and tackling the sources of bias, we can work towards building more fair and unbiased LLMs that provide reliable and equitable outputs.",
            "score": 0.7231119979127415,
            "section_title": "Other types of social bias",
            "char_start_offset": 19668,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 152,
                    "end": 332
                },
                {
                    "start": 332,
                    "end": 447
                },
                {
                    "start": 447,
                    "end": 655
                },
                {
                    "start": 655,
                    "end": 822
                },
                {
                    "start": 822,
                    "end": 916
                },
                {
                    "start": 918,
                    "end": 1065
                },
                {
                    "start": 1065,
                    "end": 1215
                },
                {
                    "start": 1215,
                    "end": 1367
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78466796875
        },
        {
            "corpus_id": "260334475",
            "title": "KoBBQ: Korean Bias Benchmark for Question Answering",
            "text": "Social bias refers to disparate treatment or outcomes between social groups that arise from historical and structural power asymmetries (Gallegos et al., 2023). These biases manifest in various forms, from toxic expressions towards certain social groups to stereotypical linguistic associations. \n\nRecent studies have revealed inherent bias in LLMs across diverse categories, including gender, political ideologies, occupation, age, disability status, class, culture, gender identity, sexual orientation, race, ethnicity, nationality, and religion (Kotek et al., 2023;Motoki et al., 2023;Xue et al., 2023;Esiobu et al., 2023). Tao et al. (2023) observe LLMs' cultural bias resembling Englishspeaking and Protestant European countries, and Nguyen et al. (2023) underscore the need for equitable and culturally aware AI and evaluation. \n\nBias in LLMs can be quantified through 1) embedding or probabilities of tokens or sentences and 2) distribution, classifier prediction, and lexicon of generated texts. Evaluation datasets for measuring bias leverage counterfactual inputs (a fill-in-theblank task with masked token and predicting most likely unmasked sentences) or prompts (sentence completion and question answering) (Rudinger et al., 2018;Nangia et al., 2020;Gehman et al., 2020;Parrish et al., 2022), inter alia2 .",
            "score": 0.7228288856869534,
            "section_title": "Social Bias in LLMs",
            "char_start_offset": 2557,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 295
                },
                {
                    "start": 298,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 833
                },
                {
                    "start": 836,
                    "end": 1003
                },
                {
                    "start": 1004,
                    "end": 1319
                }
            ],
            "ref_mentions": [
                {
                    "start": 548,
                    "end": 568,
                    "matchedPaperCorpusId": "261276445"
                },
                {
                    "start": 568,
                    "end": 588,
                    "matchedPaperCorpusId": "257372256"
                },
                {
                    "start": 1220,
                    "end": 1243,
                    "matchedPaperCorpusId": "13756572"
                },
                {
                    "start": 1243,
                    "end": 1263,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 1263,
                    "end": 1283,
                    "matchedPaperCorpusId": "221878771"
                },
                {
                    "start": 1283,
                    "end": 1304,
                    "matchedPaperCorpusId": "239010011"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3466796875
        },
        {
            "corpus_id": "248366292",
            "title": "Towards an Enhanced Understanding of Bias in Pre-trained Neural Language Models: A Survey with Special Emphasis on Affective Bias",
            "text": "Bias in pre-trained language models can be viewed through different perspectives, domains of bias and stages in which they occur. We illustrate this heterogeneous view of PLM biases in figure 1. Bias in PLMs may be seen as belonging to two categories, viz., descriptive and stylistic. Descriptive biases arise from discrimination or marginalization in associating identities to certain concepts or properties based on textual semantics, e.g. word embeddings associate father to doctor and mother to nurse [16]. Stylistic biases originate due to stylistic differences in texts with same content but generated by different socio-economic groups [100], e.g. unfair treatment to African American English while using language identification tools and dependency parsers [15]. Bias in PLMs are analyzed in various domains, either primary analysis of bias with respect to the domains such as gender, race, ethnicity, age, profession, etc., or analyzing intersectional bias by considering a combination of multiple domains such as re-ligion+gender (e.g. Muslim lady), race+gender (e.g. black woman), etc. Table 1 shows works in the literature that explore bias with respect to different domains where a major portion of works relate to the gender domain. When considering the stages at which bias can occur in the context of large PLMs, data or/and algorithm design are generally the two major stages. Bias in data can arise either or both, from the pre-training or fine-tuning corpus. Algorithm bias may originate from self-supervised learning algorithms that yield non-contextual or contextual representations [16,102] or/and from fine-tuning learning algorithm designed for downstream tasks [47]. \n\nIn this paper, we survey bias in NLP, especially in pre-trained neural language models. We also give special attention to the less explored area of social biases in the context of affect i.e., Affective Bias (or emotion associated bias) specific to large PLMs.",
            "score": 0.7197729105722757,
            "section_title": "Heterogeneous View of Bias in PLMs",
            "char_start_offset": 3345,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 654
                },
                {
                    "start": 655,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 1045
                },
                {
                    "start": 1046,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1691
                },
                {
                    "start": 1694,
                    "end": 1781
                },
                {
                    "start": 1782,
                    "end": 1954
                }
            ],
            "ref_mentions": [
                {
                    "start": 505,
                    "end": 509,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 643,
                    "end": 648,
                    "matchedPaperCorpusId": "210846144"
                },
                {
                    "start": 765,
                    "end": 769,
                    "matchedPaperCorpusId": "1066490"
                },
                {
                    "start": 1604,
                    "end": 1608,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 1608,
                    "end": 1612,
                    "matchedPaperCorpusId": "235097394"
                },
                {
                    "start": 1686,
                    "end": 1690,
                    "matchedPaperCorpusId": "207847197"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7880859375
        },
        {
            "corpus_id": "266521434",
            "title": "Large Language Model (LLM) Bias Index - LLMBI",
            "text": "The formulation of the Large Language Model Bias Index (LLMBI) necessitates a sweeping understanding of the multifaceted nature of bias in Large Language Models (LLMs). These biases, spanning across various dimensions such as gender, religion, race, age, nationality, disability, sexual orientation, physical appearance, and socioeconomic status, can signicantly impact the outputs of LLMs, leading to skewed and potentially harmful outcomes. \n\nGender bias in LLMs often manifests in stereotypical representations and unequal treatment of dierent genders. This bias can perpetuate gender stereotypes and discrimination, aecting the fairness of AI applications in areas like recruitment and social media (Zhao et al., 2017) [10]. Religion bias in LLMs can lead to the marginalisation of certain religious groups, fostering an environment of intolerance and misunderstanding (Aroyo & Welty, 2015) [11]. \n\nRacial bias is another pressing concern, as LLMs can perpetuate racial stereotypes and prejudices. This bias can have severe implications in legal, healthcare, and nancial systems, where it can lead to discriminatory practices (Sweeney, 2013). Age bias in LLMs can result in underrepresenting or misrepresenting specic age groups, impacting sectors like healthcare and marketing (Levy, 2017) [12]. \n\nNationality bias in LLMs can lead to the preferential treatment of certain nationalities over others, aecting global perceptions and international relations (Liao et al., 2018) [13]. Disability bias in LLMs can result in the exclusion or misrepresentation of people with disabilities, impacting accessibility and inclusivity (Trewin et al., 2019) [14]. \n\nSexual orientation bias in LLMs can perpetuate harmful stereotypes and discrimination against LGBTQ+ individuals, aecting mental health and social acceptance (Costanza-Chock, 2018) [15]. Physical appearance bias in LLMs can lead to body shaming and unrealistic beauty standards, impacting mental health and self-esteem (Yee & Bailenson, 2007) [16]. \n\nLastly, socioeconomic status bias in LLMs can reinforce class divisions and economic inequalities, aecting opportunities and social mobility (O'Neil, 2016) [17].",
            "score": 0.7184995686219858,
            "section_title": "Problem Formulation",
            "char_start_offset": 4936,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 442
                },
                {
                    "start": 445,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 900
                },
                {
                    "start": 903,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1300
                },
                {
                    "start": 1303,
                    "end": 1485
                },
                {
                    "start": 1486,
                    "end": 1655
                },
                {
                    "start": 1658,
                    "end": 1844
                },
                {
                    "start": 1845,
                    "end": 2006
                },
                {
                    "start": 2009,
                    "end": 2170
                }
            ],
            "ref_mentions": [
                {
                    "start": 703,
                    "end": 722,
                    "matchedPaperCorpusId": "1389483"
                },
                {
                    "start": 723,
                    "end": 727,
                    "matchedPaperCorpusId": "1389483"
                },
                {
                    "start": 873,
                    "end": 894,
                    "matchedPaperCorpusId": "6134326"
                },
                {
                    "start": 895,
                    "end": 899,
                    "matchedPaperCorpusId": "6134326"
                },
                {
                    "start": 1282,
                    "end": 1294,
                    "matchedPaperCorpusId": "17169507"
                },
                {
                    "start": 1295,
                    "end": 1299,
                    "matchedPaperCorpusId": "17169507"
                },
                {
                    "start": 1480,
                    "end": 1484,
                    "matchedPaperCorpusId": "210064344"
                },
                {
                    "start": 1839,
                    "end": 1843,
                    "matchedPaperCorpusId": "150082799"
                },
                {
                    "start": 1977,
                    "end": 2000,
                    "matchedPaperCorpusId": "13893029"
                },
                {
                    "start": 2001,
                    "end": 2005,
                    "matchedPaperCorpusId": "13893029"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81884765625
        },
        {
            "corpus_id": "239015827",
            "title": "An Empirical Survey of the Effectiveness of Debiasing Techniques for Pre-trained Language Models",
            "text": "Large pre-trained language models have proven effective across a variety of tasks in natural language processing, often obtaining state of the art performance (Peters et al., 2018;Devlin et al., 2019;Radford et al., 2019;Brown et al., 2020). These models are typically trained on large amounts of text, originating from unmoderated sources, such as the internet. While the performance of these pre-trained models is remarkable, recent work has shown that they capture social biases from the data they are trained on (May et al. 2019;Kurita et al. 2019;Webster et al. 2020;Nangia et al. 2020;Nadeem et al. 2021, inter alia). Because of these findings, an increasing amount of research has focused on developing techniques to mitigate these biases (Liang et al., 2020;Ravfogel et al., 2020;Webster et al., 2020;Kaneko and Bollegala, 2021;Schick et al., 2021;Lauscher et al., 2021). However, the proposed techniques are often not investigated thoroughly. For instance, much work focuses only on mitigating gender bias despite pre-trained language models being plagued by other social biases (e.g., racial or religious bias). Additionally, the impact that debiasing has on both downstream task performance, as well as language modeling ability, is often not well explored.\n\nIn this paper, we perform an empirical survey of the effectiveness of five recently proposed debiasing techniques for pre-trained language models: 2 Counterfactual Data Augmentation (CDA; Zmigrod et al. 2019;Webster et al. 2020), Dropout (Webster et al., 2020), Iterative Nullspace Projection (INLP; Ravfogel et al. 2020), Self-Debias (Schick et al., 2021), and SentenceDebias (Liang et al., 2020). Following the taxonomy described by Blodgett et al. (2020), our work studies the effectiveness of these techniques in mitigating representational biases from pre-trained language models. More specifically, we investigate mitigating gender, racial, and religious biases in three masked language models (",
            "score": 0.716561536536089,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6826171875
        },
        {
            "corpus_id": "248366292",
            "title": "Towards an Enhanced Understanding of Bias in Pre-trained Neural Language Models: A Survey with Special Emphasis on Affective Bias",
            "text": "Even though word representations are powerful enough to capture semantic similarities and exhibit word relationships through word vector similarities, the explicit and implicit existence of several stereotypes and social biases in PLMs harm its usefulness in many real-world applications. Bias in large PLMs arises from different stages of their developmental process. Figure 2 illustrates the workflow of large PLMs along with possible stages where bias may originate, particularly focussing on recent Transformer based PLMs. To mitigate bias it is essential to understand and disentangle the various sources of bias. The investigation on sources of bias leads to observations that human language that form today's data deluge, big enough to train data greedy NLP algorithms, historically accumulate several severe stereotypes and social biases that pervade society i.e., Historical Bias. Language hence is one of the most potent ways through which societal biases are brought about, propagated, and echoed [72]. Several non-neutral stereotypes live in linguistic communication, imaging asymmetries in terms of dominance, power, quality, or status among target terms such as female and male, blacks and whites belonging to various domains like gender, race, etc., [36]. Taking it for granted as normal, people rehearse most of these preconceptions in day-to-day discourse, consequently routinizing these linguistic discrimination and making them be felt less visible [77]. Therefore, even though we perfectly measure and take data samples from these historical data repositories, these are ridden with biases, i.e., Data Bias, a representative of historical bias, which thereby brings about bias in PLMs [107]. \n\nData bias stemming from innate historical biases is the most general source of bias among different sources of bias explored in literature for various tasks [28], where, quality issues in data, uneven distribution (occurrence or co-occurrences) of key terms in data associated with targets concerning a domain [18], etc., are other factors that contribute towards it. Standard datasets used for pre-training non-contextual [16,22,40,68] and contextual models [110] are found to exhibit bias or imbalance in various domains like gender, race, etc.",
            "score": 0.7073696587006549,
            "section_title": "Bias in Pre-trained Language Models",
            "char_start_offset": 12740,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 1013
                },
                {
                    "start": 1014,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1473
                },
                {
                    "start": 1474,
                    "end": 1711
                },
                {
                    "start": 1714,
                    "end": 2081
                },
                {
                    "start": 2082,
                    "end": 2260
                }
            ],
            "ref_mentions": [
                {
                    "start": 1468,
                    "end": 1472,
                    "matchedPaperCorpusId": "145267815"
                },
                {
                    "start": 1705,
                    "end": 1710,
                    "matchedPaperCorpusId": "235436386"
                },
                {
                    "start": 1871,
                    "end": 1875,
                    "matchedPaperCorpusId": "3228123"
                },
                {
                    "start": 2024,
                    "end": 2028,
                    "matchedPaperCorpusId": "102352788"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68310546875
        },
        {
            "corpus_id": "260438662",
            "title": "NBIAS: A Natural Language Processing Framework for Bias Identification in Text",
            "text": "The recent surge in Natural Language Processing (NLP) applications, encompassing fields from recommendation systems to social justice and employment screening, has sparked a critical concern -the emergence of bias within these systems [1]. Instances of racial and gender bias have been increasingly reported [2], indicating an urgent need for scrutiny. These biases often originate from the training data used in NLP models, and a majority of these large datasets harbor inherent biases . Regrettably, many NLP practitioners lack the necessary awareness or knowledge to effectively identify and address these biases, highlighting a significant gap in the field. \n\nFurthermore, there is a notable lack of discussion on data specifics -its origin, generation, and pre-processing -in many NLP publications. Given these circumstances, the importance of addressing biases in NLP applications cannot be overstated. These biases, if unchecked, not only compromise the validity of the models, but can also have unfavorable and detrimental consequences. The objective of this research is to provide insights into the detection of bias in NLP datasets, contributing to the development of more equitable and unbiased Artificial Intelligence (AI) systems. \n\nBias in text data is a pervasive and deeply-rooted issue. The bias in data often stems from cognitive predispositions that influences our dialogues, views, and understanding of information [3]. This bias can be explicit which are often seen in discriminatory language targeting certain racial or ethnic groups [4], as in social media. Implicit bias [5], on the other hand, subtly perpetuates prejudice through unintentional language use but is equally harmful. \n\nThe necessity for unbiased, trustworthy text data has grown across sectors like healthcare [6], social media [4,7], and recruitment [8]. This data is essential for training NLP models for various downstream tasks, like formulating healthcare diagnoses and treatment plans, handling discriminatory language on social media, and promoting fair recruitment practices. Figure 1 illustrates the complexities of biases in text data in various domains, including job hiring, social media, and healthcare. These biases are primarily conveyed through lexical choices [9] and demand sophisticated detection methods, motivating this research.",
            "score": 0.7051229086851288,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 488
                },
                {
                    "start": 489,
                    "end": 661
                },
                {
                    "start": 664,
                    "end": 803
                },
                {
                    "start": 804,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1243
                },
                {
                    "start": 1246,
                    "end": 1303
                },
                {
                    "start": 1304,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1580
                },
                {
                    "start": 1581,
                    "end": 1706
                },
                {
                    "start": 1709,
                    "end": 1845
                },
                {
                    "start": 1846,
                    "end": 2073
                },
                {
                    "start": 2074,
                    "end": 2206
                },
                {
                    "start": 2207,
                    "end": 2340
                }
            ],
            "ref_mentions": [
                {
                    "start": 235,
                    "end": 238,
                    "matchedPaperCorpusId": "218487466"
                },
                {
                    "start": 308,
                    "end": 311,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 1435,
                    "end": 1438,
                    "matchedPaperCorpusId": "54997157"
                },
                {
                    "start": 1595,
                    "end": 1598,
                    "matchedPaperCorpusId": "228094096"
                },
                {
                    "start": 1800,
                    "end": 1803,
                    "matchedPaperCorpusId": "244490278"
                },
                {
                    "start": 1821,
                    "end": 1823,
                    "matchedPaperCorpusId": "247050244"
                },
                {
                    "start": 1841,
                    "end": 1844,
                    "matchedPaperCorpusId": "1634922"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83154296875
        },
        {
            "corpus_id": "250390797",
            "title": "The Birth of Bias: A case study on the evolution of gender bias in an English language model",
            "text": "While there is a lot of important work on detecting and mitigating undesirable biases in language models, we still lack a good understanding of the mechanisms underlying the biased behaviour. The goal of this study was to take a step back and analyse the birth of bias in language models. To this end, we present a temporal investigation of how an English LSTM language model learns a representation of gender in the input embeddings and how this affects downstream biased behaviour. \n\nThere are many interesting directions for future research. An important open question is, for instance, how intrinsic representations of bias relate to other downstream tasks that may be closer to real-world systems where the representational and allocative harms to social groups are more clear (Blodgett et al., 2020). For future work, we also plan to do further investigations on how our training dynamics analysis may generalise to other undesirable social biases, model architectures, training corpora, and downstream tasks, as well as other possible representations in the internal states of the language model that are useful for understanding bias. Furthermore, the robustness of our results with respect to different random initialisations of the language model should be checked (Webster et al., 2020;D'Amour et al., 2020). \n\nIn this paper, we take a step towards a more thorough understanding of the evolution of bias in language models across the different stages of the language modelling pipeline. Hopefully, it will inspire more work on the dynamic behaviour of language models, with respect to bias, but also other still poorly understood features of these models.",
            "score": 0.7029143489472147,
            "section_title": "Conclusion",
            "char_start_offset": 36173,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 483
                },
                {
                    "start": 486,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1319
                },
                {
                    "start": 1322,
                    "end": 1497
                },
                {
                    "start": 1498,
                    "end": 1666
                }
            ],
            "ref_mentions": [
                {
                    "start": 782,
                    "end": 805,
                    "matchedPaperCorpusId": "218971825"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72900390625
        },
        {
            "corpus_id": "274192674",
            "title": "Leveraging Large Language Models to Measure Gender Representation Bias in Gendered Language Corpora",
            "text": "In recent years, the presence of social biases in machine learning models (Barocas et al., 2019) has gained significant attention due to their potential to perpetuate and amplify existing inequalities, impacting areas of great consequence in people's lives, LLM Identify all nouns and pronouns in the sentence and for each of them, determine 1) whether it refers to a person (P) or not (N), and 2) its grammatical gender: masculine (M) or feminine (F). Figure 1: Overview of the proposed method for the detection and measurement of biases in the prevalence of genders in gendered language corpora using LLMs. \n\nsuch as hiring practices (Raghavan et al., 2020), law enforcement (Babuta and Oswald, 2019), healthcare (Panch et al., 2019), and everyday digital interactions. Among various forms of bias, gender bias is particularly concerning because it affects roughly half of the global population and has pervasive effects across different sectors of society. This concern is amplified in the area of natural language processing (NLP), where biases present in training corpora can propagate through machine learning models and impact a broad range of applications (Kiritchenko and Mohammad, 2018;Costa-juss\u00e0 and de Jorge, 2020). \n\nThis paper focuses on representation bias -a type of bias where certain genders are underrepresented, stereotyped or portrayed in a limited manner -in the language corpora that are used to train large language models (LLMs). A core motivation for our study stems from the lifecycle of LLMs. \n\nFrom training to deployment, biases in the training data can not only be perpetuated but even amplified by the models (Kotek et al., 2023). The training corpora typically is obtained from diverse sources, including books, websites, social media, and other digital texts. If these sources contain biases -which they frequently do, reflecting societal prejudices and stereotypes -the models learn these biases as part of their training process. Hence, the first stage of data curation is of paramount importance. It involves selecting, cleaning, and annotating the training data to minimize biases as much as possible.",
            "score": 0.699114819830747,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 608
                },
                {
                    "start": 611,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1228
                },
                {
                    "start": 1231,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1521
                },
                {
                    "start": 1524,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1794
                },
                {
                    "start": 1795,
                    "end": 1966
                },
                {
                    "start": 1967,
                    "end": 2034
                },
                {
                    "start": 2035,
                    "end": 2140
                }
            ],
            "ref_mentions": [
                {
                    "start": 74,
                    "end": 96,
                    "matchedPaperCorpusId": "113402716"
                },
                {
                    "start": 636,
                    "end": 659,
                    "matchedPaperCorpusId": "195316573"
                },
                {
                    "start": 715,
                    "end": 735,
                    "matchedPaperCorpusId": "208355156"
                },
                {
                    "start": 1164,
                    "end": 1196,
                    "matchedPaperCorpusId": "21670658"
                },
                {
                    "start": 1642,
                    "end": 1662,
                    "matchedPaperCorpusId": "261276445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78662109375
        },
        {
            "corpus_id": "261214814",
            "title": "Towards a Holistic Approach: Understanding Sociodemographic Biases in NLP Models using an Interdisciplinary Lens",
            "text": "The rapid growth in the usage and applications of Natural Language Processing (NLP) in various sociotechnical solutions has highlighted the need for a comprehensive understanding of bias and its impact on society. While research on bias in NLP has expanded, several challenges persist that require attention. These include the limited focus on sociodemographic biases beyond race and gender, the narrow scope of analysis predominantly centered on models, and the technocentric implementation approaches. This paper addresses these challenges and advocates for a more interdisciplinary approach to understanding bias in NLP. The work is structured into three facets, each exploring a specific aspect of bias in NLP. The first facet focuses on identifying sociodemographic bias in various NLP architectures, emphasizing the importance of considering both the models themselves and human computation to comprehensively understand and identify bias. In the second facet, we delve into the significance of establishing a shared vocabulary across different fields and disciplines involved in NLP. By highlighting the potential bias stemming from a lack of shared understanding, this facet emphasizes the need for interdisciplinary collaboration to bridge the gap and foster a more inclusive and accurate analysis of bias. Finally, the third facet investigates the development of a holistic solution by integrating frameworks from social science disciplines. This approach recognizes the complexity of bias in NLP and advocates for an interdisciplinary framework that goes beyond purely technical considerations, involving social and ethical perspectives to address bias effectively. The first facet includes the following of my published works [6, 7, 8, 9] to provide results into how the importance of understanding the presence of bias in various minority group that has not been in focus in the prior works of bias in NLP. The work also shows the need to create a method that considers both human and AI indicators of bias, showcasing the importance of the first facet of my research. In my study [9], I delve into sentiment analysis and toxicity detection models to identify explicit bias against race, gender, and people with disabilities (PWDs). Through statistical exploration of conversations on social media platforms such as Twitter and Reddit, I gain insights into how disability bias permeates real-world social settings. To quantify explicit sociodemographic bias in sentiment analysis and toxicity analysis models, I create the Bias Identification Test in Sentiment (BITS) corpus1. Applying BITS, I uncover significant bias",
            "score": 0.6988756027075544,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8291015625
        },
        {
            "corpus_id": "274130807",
            "title": "Bias in Large Language Models: Origin, Evaluation, and Mitigation",
            "text": "These biases not only perpetuate inequality but also raise significant ethical and legal questions regarding the use of LLMs in decision-making processes. Given the critical nature of these challenges, there is an urgent need for comprehensive evaluation frameworks to detect, quantify, and mitigate these biases, ensuring that LLMs function as fair and equitable tools for all users. \n\nThe bias presented in Large Language Models (LLMs) can be broadly categorized into intrinsic bias and extrinsic bias based on the different stages at which the biases manifest within the model's lifecycle and the type of bias being measured (Doan et al., 2024). Intrinsic bias refers to biases that are inherently within the internal representations or outputs of a trained or pre-trained LLM and are independent of any specific downstream tasks. Extrinsic bias refers to the biases that manifest during the model's performance on specific downstream tasks after training or fine-turing. Intrinsic biases are typically encoded during the training or pre-training phase when the LLM learns from large-scale corpora. For instance, if a pre-trained LLM consistently associates certain professions with specific genders (e.g., assuming all doctors to be male and assuming all nurses to be female) or racial stereotypes (May et al., 2019), this bias would be considered intrinsic bias. This type of bias is tied to the way the model has encoded relationships and patterns in its internal representations. \n\nIntrinsic biases are primarily introduced during the training/pre-training stages where models learn patterns and representations from vast corpora. These biases can be traced to several aspects of data and model design, which will be outlined below.",
            "score": 0.6977688919414837,
            "section_title": "Introduction",
            "char_start_offset": 5697,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 384
                },
                {
                    "start": 387,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 974
                },
                {
                    "start": 975,
                    "end": 1101
                },
                {
                    "start": 1102,
                    "end": 1367
                },
                {
                    "start": 1368,
                    "end": 1486
                },
                {
                    "start": 1489,
                    "end": 1637
                },
                {
                    "start": 1638,
                    "end": 1739
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9150390625
        },
        {
            "corpus_id": "268856702",
            "title": "Fairness in Large Language Models: A Taxonomic Survey",
            "text": "Language modeling bias, often defined as \"bias that results in harm to various social groups\" [70], presents itself in various forms, encompassing the association of specific stereotypes with groups, the devaluation of certain groups, the underrepresentation of particular social groups, and the unequal allocation of resources among groups [44].Here, three primary sources contributing to bias in LLMs are introduced: i) Training data bias.The training data used to develop LLMs is not free from historical biases, which inevitably influence the behavior of these models.For instance, if the training data includes the statement \"all programmers are male and all nurses are female, \" the model is likely to learn and perpetuate these occupational and gender biases in its outputs, reflecting a narrow and biased view of societal roles [16,24].Additionally, a significant disparity in the training data could also lead to biased outcomes [161].For example, Buolamwini and Gebru [21] highlighted significant disparities in datasets like IJB-A and Adience, where predominantly light-skinned individuals make up 79.6% and 86.2% of the data, respectively, thereby biasing analyses toward underrepresented dark-skinned groups [118].\n\nii) Embedding bias.Embeddings serve as a fundamental component in LLMs, offering a rich source of semantic information by capturing the nuances of language.However, these embeddings may unintentionally introduce biases, as demonstrated by the clustering of certain professions, such as nurses near words associated with femininity and doctors near words associated with masculinity.This phenomenon inadvertently introduces semantic bias into downstream models, impacting their performance and fairness [9,63].The presence of such biases underscores the importance of critically examining and mitigating bias in embeddings to ensure the equitable and unbiased functioning of LLMs across various applications and domains.\n\niii) Label bias.In instruction tuning scenarios, biases can arise from the subjective judgments of human annotators who provide labels or annotations for training data [152].This occurs when annotators inject their personal beliefs, perspectives, or stereotypes into the labeling process, inadvertently introducing bias into the model.",
            "score": 0.6945056497589908,
            "section_title": "Factors Contributing to Bias in LLMs",
            "char_start_offset": 12082,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 346
                },
                {
                    "start": 346,
                    "end": 441
                },
                {
                    "start": 441,
                    "end": 572
                },
                {
                    "start": 572,
                    "end": 844
                },
                {
                    "start": 844,
                    "end": 944
                },
                {
                    "start": 944,
                    "end": 1227
                },
                {
                    "start": 1229,
                    "end": 1248
                },
                {
                    "start": 1248,
                    "end": 1385
                },
                {
                    "start": 1385,
                    "end": 1611
                },
                {
                    "start": 1611,
                    "end": 1738
                },
                {
                    "start": 1738,
                    "end": 1948
                },
                {
                    "start": 1950,
                    "end": 1966
                },
                {
                    "start": 1966,
                    "end": 2124
                },
                {
                    "start": 2124,
                    "end": 2285
                }
            ],
            "ref_mentions": [
                {
                    "start": 836,
                    "end": 840,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 840,
                    "end": 843,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 978,
                    "end": 982,
                    "matchedPaperCorpusId": "3298854"
                },
                {
                    "start": 1221,
                    "end": 1226,
                    "matchedPaperCorpusId": "201666566"
                },
                {
                    "start": 2118,
                    "end": 2123,
                    "matchedPaperCorpusId": "196211238"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81201171875
        },
        {
            "corpus_id": "269195406",
            "title": "AI Chatbots and Linguistic Injustice",
            "text": "It is widely known that recent language models have improved significantly (Devlin et al. 2018, Brown et al. 2020, Clark et al. 2020). Recent advancements in language modeling have embraced the approach of training large-scale models on extensive, unannotated corpora using self-supervised learning techniques. These methods involve predicting masked words and the next sentence in a sequence. (Devlin et al. 2018, He et al. 2020), wrong word detection (Clark et al. 2020), and left-to-right language generation (Brown et al. 2020, Raffel et al. 2020). \n\nThe recent natural language processing models are trained by assessing the similarity vocabularies and sentences in text. Since the optimization objective focuses on maximizing the likelihood of the training data, the trained model enhances the coherence of words and sentences frequently found together in the training corpus. However, being created by humans, the training data sets can contain significant amounts of social bias and stereotypes, encompassing factors such as gender, race, and religion (Kiritchenko & Mohammad 2018, Nadeem et al. 2021, Stanczak & Augenstein 2021). \n\nSome studies have demonstrated that pretrained language models are capable of acquiring various forms of stereotypical and baised reasoning. For example, Kiritchenko & Mohammad (2018) examined how language models perform in sentiment analysis across various social groups, measuring differences in their behaviours. Recent studies by Nangia et al. (2020) and Nadeem et al. (2021) investigated stereotypical reasoning related to race, gender, profession, and religion using masked language models and sentence encoders. \n\nRecent research examined strategies to reduce the social biases inherent in language models, aiming to enhance their reliability. These studies have investigated techniques to mitigate biases during the learning and prediction phases of language models. Typical methods for mitigating bias involve the use of counterfactual data augmentation (Zmigrod et al. 2019, Dinan et al. 2020, Webster et al. 2020, Barikeri et al. 2021), dropout regularization (Webster et al. 2020), and selfdebias (Schick et al. 2021). MIT researchers have trained language models that can realize logic to avoid harmful stereotypes such as gender and racial biases.",
            "score": 0.6914763616101052,
            "section_title": "Language Modeling Bias",
            "char_start_offset": 14133,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 552
                },
                {
                    "start": 555,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 1138
                },
                {
                    "start": 1141,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1456
                },
                {
                    "start": 1457,
                    "end": 1659
                },
                {
                    "start": 1662,
                    "end": 1791
                },
                {
                    "start": 1792,
                    "end": 1915
                },
                {
                    "start": 1916,
                    "end": 2171
                },
                {
                    "start": 2172,
                    "end": 2302
                }
            ],
            "ref_mentions": [
                {
                    "start": 94,
                    "end": 113,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 113,
                    "end": 133,
                    "matchedPaperCorpusId": "208229926"
                },
                {
                    "start": 453,
                    "end": 472,
                    "matchedPaperCorpusId": "208229926"
                },
                {
                    "start": 512,
                    "end": 530,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 530,
                    "end": 551,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1060,
                    "end": 1088,
                    "matchedPaperCorpusId": "21670658"
                },
                {
                    "start": 1088,
                    "end": 1108,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 1295,
                    "end": 1324,
                    "matchedPaperCorpusId": "21670658"
                },
                {
                    "start": 1475,
                    "end": 1495,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 1500,
                    "end": 1520,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 2004,
                    "end": 2024,
                    "matchedPaperCorpusId": "184486914"
                },
                {
                    "start": 2024,
                    "end": 2043,
                    "matchedPaperCorpusId": "207852875"
                },
                {
                    "start": 2064,
                    "end": 2087,
                    "matchedPaperCorpusId": "235358955"
                },
                {
                    "start": 2150,
                    "end": 2170,
                    "matchedPaperCorpusId": "232075876"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6884765625
        },
        {
            "corpus_id": "265609662",
            "title": "Developing Linguistic Patterns to Mitigate Inherent Human Bias in Offensive Language Detection",
            "text": "In the subsequent section, we try to address this question further. \n\nGiven that human-based bias may not be entirely removable, the research objective is to explore methods of reducing bias in offensive language datasets and models. To address this issue, we proposed a linguistic data augmentation approach to reduce bias in their labeling processes [10]. Moreover, we extend it to multiple languages to validate the generalizability of the approach. This approach aims to mitigate the influence of human bias by leveraging the power of machines to improve the accuracy and fairness of labeling processes. \n\n2 Understanding unconscious bias in language models: Prejudice, Stereotypes, and Discrimination \n\nThe process of forming our identities is influenced by the social groups we belong to [11]. Social identity theory posits that individuals who identify with an in-group may seek to enhance their self-image by exhibiting negative attitudes towards out-groups, such as prejudice, stereotypes, and discrimination. Although these forms of bias are interconnected, they can be distinguished from one another. Further categorization of these biases in language models is available in the study by Weidinger et al. [12]. It is crucial to consider the human biases of prejudice, stereotypes, and discrimination before exploring the biases of language models and datasets. \n\nHumans express social norms and categories through languages, and therefore, these norms and categories must be represented accurately in language models for better detection of the patterns. However, technical artifacts such as language models cannot be considered value-neutral, as they reflect and perpetuate the values and norms present in the training data. Thus, it is important to address the risks of social and ethical harm arising from different categories of language models before considering their use in various applications [12]. \n\nPrejudice. also known as \"advance judgement,\" is an unfounded conclusion that does not hold true in our daily discourse. Prejudices are frequently found in social media portrayals and exist in many spheres of social life. While prejudice is often simplified in daily life, it is a complex, multi-dimensional phenomenon influenced by various factors, such as evolutionary, ontogenetic, sociopolitical, economic, and historical fields [13]. Due to the biopsychosocial characteristics of human beings, prejudices can appear in many situations.",
            "score": 0.6832026820518614,
            "section_title": "Introduction",
            "char_start_offset": 2437,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 67
                },
                {
                    "start": 70,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 607
                },
                {
                    "start": 610,
                    "end": 705
                },
                {
                    "start": 708,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1371
                },
                {
                    "start": 1374,
                    "end": 1565
                },
                {
                    "start": 1566,
                    "end": 1736
                },
                {
                    "start": 1737,
                    "end": 1918
                },
                {
                    "start": 1921,
                    "end": 1931
                },
                {
                    "start": 1932,
                    "end": 2041
                },
                {
                    "start": 2042,
                    "end": 2142
                },
                {
                    "start": 2143,
                    "end": 2359
                },
                {
                    "start": 2360,
                    "end": 2461
                }
            ],
            "ref_mentions": [
                {
                    "start": 352,
                    "end": 356,
                    "matchedPaperCorpusId": "253186192"
                },
                {
                    "start": 794,
                    "end": 798,
                    "matchedPaperCorpusId": "143666442"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.736328125
        },
        {
            "corpus_id": "256616034",
            "title": "Nationality Bias in Text Generation",
            "text": "Research identifying bias in NLP models has shown that embedding models such as GloVe and Word2Vec, and context-aware dynamic embed-dings, i.e., large language models (LLMs) such as BERT, automatically mimic biases related to gender (Kurita et al., 2019), race (Ousidhoum et al., 2021), disability (Venkit et al., 2022), and religion (Abid et al., 2021) from the language corpora used to train the model. The work done by Nadeem et al. (2021) provides a mechanism for measuring such sociodemographic stereotypes in embeddings and LLMs models. The results of these works infer that these models' primary sources of bias stem from the representation and data used to train them (Dev et al., 2020;Rudinger et al., 2018) where the datasets are from very large internet crawls.\n\nUnfortunately, internet access and usage is not evenly distributed over the world, and the generated data tends to overrepresent users from developed countries (WorldBank, 2015). Bender et al. (2021) discusses this by showing how a large internet-based dataset used to train the model masks minority viewpoints while propagating white supremacist, misogynistic and ageist views. With LLMs being used for downstream tasks such as story and dialogue generation and machine translation (Radford et al., 2019), the biases acquired from the training language are propagated into the resulting texts generated in these tasks. Whittaker et al. (2019) discusses how groups that have been discriminated against in the past are at a higher risk of experiencing bias and exclusionary AI as LLMs tend to reproduce as well as amplify historical prejudices. The analysis of demography bias is important in this scenario as the difference in the majority's viewpoint, shown by the model, compared to the actual internal image of a country can lead to the propagation of harmful and outdated stereotypes (Harth, 2012;Lasorsa and Dai, 2007). Such biases can lead to social harms such as stereotyping, and dehumanization (Dev et al., 2022) against marginalized populations",
            "score": 0.6830150920390221,
            "section_title": "Related Work",
            "char_start_offset": 2435,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 233,
                    "end": 254,
                    "matchedPaperCorpusId": "190000105"
                },
                {
                    "start": 261,
                    "end": 285,
                    "matchedPaperCorpusId": "236460108"
                },
                {
                    "start": 298,
                    "end": 319,
                    "matchedPaperCorpusId": "252819117"
                },
                {
                    "start": 334,
                    "end": 353,
                    "matchedPaperCorpusId": "231603388"
                },
                {
                    "start": 422,
                    "end": 442,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 676,
                    "end": 694,
                    "matchedPaperCorpusId": "201670701"
                },
                {
                    "start": 694,
                    "end": 716,
                    "matchedPaperCorpusId": "13756572"
                },
                {
                    "start": 953,
                    "end": 973,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 1257,
                    "end": 1279,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72021484375
        },
        {
            "corpus_id": "259716055",
            "title": "Exploring Social Biases of Large Language Models in a College Artificial Intelligence Course",
            "text": "There is a growing body of work documenting social biases in large language models. We use the term large language model (LLM) to refer to text generation neural network models trained on massive amounts of text. Popular examples include BERT (Devlin et al. 2019), GPT-3 (Brown et al. 2020), RoBERTa (Liu et al. 2019b), andBLOOM (Big-Science 2022). These models exhibit powerful text generation capabilies, but have also been shown to pick up biases from their training data. \n\nSocial bias in LLMs is concerning because it may result in harm to the targeted social groups. Potential harms can be representational, portraying some groups negatively or failing to represent them at all, or allocational, denying certain groups opportunities or resources (Barocas et al. 2017). \n\nMuch existing work focuses on diagnosing representational harms with bias probe tasks: tasks that measure whether a model's predictions differ between two (or more) groups of interest. A number of probe tasks have been proposed: Rudinger, May, and Van Durme (2017); Sheng et al. (2019); Bordia and Bowman (2019); Lee, Madotto, and Fung (2019); Liu et al. (2019a);May et al. (2019); Nadeem, Bethke, and Reddy (2021);Sotnikova et al. (2021) and others. Most of these focus on gender stereotypes. 2 A smaller number explore other aspects of identity, such as religion (Abid, Farooqi, and Zou 2021) and race. 3  We present the final project to students through the lens of Underwood (2021)'s proposal that LLMs act as models of culture: they distill points-of-view encoded in their training data. From this perspective, exploring the social biases of these models is doubly illuminating. It can reveal biases that may percolate to downstream models, causing representational or allocational harms. It is also a way to explore biases in society at large.",
            "score": 0.6829003412659866,
            "section_title": "Social Biases in Large Language Models",
            "char_start_offset": 3897,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 83
                },
                {
                    "start": 84,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 475
                },
                {
                    "start": 478,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 774
                },
                {
                    "start": 777,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1227
                },
                {
                    "start": 1228,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1383
                },
                {
                    "start": 1384,
                    "end": 1569
                },
                {
                    "start": 1570,
                    "end": 1660
                },
                {
                    "start": 1661,
                    "end": 1770
                },
                {
                    "start": 1771,
                    "end": 1826
                }
            ],
            "ref_mentions": [
                {
                    "start": 300,
                    "end": 323,
                    "matchedPaperCorpusId": "198953378"
                },
                {
                    "start": 1006,
                    "end": 1041,
                    "matchedPaperCorpusId": "5310359"
                },
                {
                    "start": 1043,
                    "end": 1062,
                    "matchedPaperCorpusId": "202537041"
                },
                {
                    "start": 1090,
                    "end": 1119,
                    "matchedPaperCorpusId": "211142738"
                },
                {
                    "start": 1121,
                    "end": 1140,
                    "matchedPaperCorpusId": "204838020"
                },
                {
                    "start": 1140,
                    "end": 1157,
                    "matchedPaperCorpusId": "198953378"
                },
                {
                    "start": 1159,
                    "end": 1192,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 1192,
                    "end": 1215,
                    "matchedPaperCorpusId": "234337004"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.826171875
        },
        {
            "corpus_id": "277787023",
            "title": "Bias Beyond English: Evaluating Social Bias and Debiasing Methods in a Low-Resource Setting",
            "text": "Machine learning frameworks are fundamentally designed as a function that generalizes past data (Chun, 2021). As a result, pretrained language models inevitably learn the inherent social biases embedded in raw real-world text. Studies have shown that these models and their learned representations retain and propagate biases from their training data. For example, Bolukbasi et al. (2016) found that word embeddings such as Word2Vec retain measurable gender bias, with male-associated terms being linked to professions like programmer and scientist, while female-associated terms are more commonly linked to nurse and homemaker. \n\nMeanwhile, bias is propagated in downstream applications, thus potentially reinforcing stereotypes and exacerbating information inequality (Bender and Friedman, 2018). \n\nSocial bias has been extensively studied in natural language processing (Vanmassenhove et al., 2019;Kiritchenko and Mohammad, 2018;Sap et al., 2019;Nangia et al., 2020;Nadeem et al., 2020). However, most of the relevant works are limited to English and reflect an Anglo-centric social context. The \"subaltern\", a key figure in postcolonial discourse, is historically marginalized and voiceless-\"has no history and thus no voice\" (Morris, 2009). In computational linguistics, this voicelessness of the subaltern is also pronounced: Most NLP research overlooks thousands of languages spoken by billions of people (Bender and Friedman, 2018;Eberhard et al., 2019). Although multilingual language models are trained on language data rather than cultural data, all languages inherently reflect cultural stereotypes. However, models are primarily trained, evaluated, and aligned using Western data and culture, and therefore debiasing techniques often fail to account for culture-specific discrimination (Khandelwal et al., 2023). The systemic bias present in these models not only affects the fairness and accuracy of multilingual systems, but also negatively impacts social equity and equal access to information. Developing more equitable and inclusive multilingual systems is an urgent challenge in NLP today. \n\nThis study aims to systematically evaluate model bias in a multilingual setting.",
            "score": 0.6828136462783588,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 628
                },
                {
                    "start": 631,
                    "end": 798
                },
                {
                    "start": 801,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1611
                },
                {
                    "start": 1612,
                    "end": 1825
                },
                {
                    "start": 1826,
                    "end": 2010
                },
                {
                    "start": 2011,
                    "end": 2108
                },
                {
                    "start": 2111,
                    "end": 2191
                }
            ],
            "ref_mentions": [
                {
                    "start": 365,
                    "end": 388,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 770,
                    "end": 797,
                    "matchedPaperCorpusId": "52255687"
                },
                {
                    "start": 932,
                    "end": 949,
                    "matchedPaperCorpusId": "196211238"
                },
                {
                    "start": 1412,
                    "end": 1439,
                    "matchedPaperCorpusId": "52255687"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73583984375
        },
        {
            "corpus_id": "239010011",
            "title": "BBQ: A hand-built bias benchmark for question answering",
            "text": "The presence of bias in a model's representations or in the embeddings used by a model does not, on its own, indicate that a model will have a biased output. In order to understand cases where the output of a model reinforces biases, we need to look at how these biases manifest in downstream tasks. Two relevant classification tasks where such research already exists is in coreference resolution and hate speech detection.\n\nIn coreference resolution, much of the work on biases has focused on gender bias that relates to specific gender stereotypes (Lu et al., 2020) or on measuring gender-occupation associations (Rudinger et al., 2018;Zhao et al., 2018). The work often assesses model performance as a primary metric, with Webster et al. (2018) finding that existing biases in the training corpus led to models incorrectly adopting a bias towards selecting masculine pronouns. Cao and Daum\u00e9 III (2020) extend work on gender bias to include non-binary identities and highlight how bias can be introduced through human annotation and surface in coreference resolution as model predictions that are both incorrect and harmful.\n\nIn hate speech detection, R\u00f6ttger et al. (2021) create HATECHECK and investigate failure points of classification models, like differences in performance across target groups. Similarly, Davidson et al. (2019) find differences in hate speech detection performance for tweets written in African American English in contrast with Standard American English. Other studies have focused on a broader range of social biases, covering not only gender and race-based biases, but also age, religion, sexual orientation, and disability status (see Dev et al. 2021 for a survey). Sap et al. (2020) study a range of biases and place them into inference frames to tie potential hate speech to the real-world bias being invoked.  also includes a range of individual and intersectional biases, and they develop a dataset with fine-grained annotations related to both the type of bias and the target.\n\nSocial Biases in Question Answering To our knowledge, there is only one existing dataset aimed at measuring social bias in QA models, Un-Qover . They use underspecified questions-questions that cannot be answered by the given context-to assess model biases from gendered name-occupation associations,",
            "score": 0.6817177813586512,
            "section_title": "Social Biases in Downstream NLP Tasks",
            "char_start_offset": 10078,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 616,
                    "end": 639,
                    "matchedPaperCorpusId": "13756572"
                },
                {
                    "start": 1155,
                    "end": 1176,
                    "matchedPaperCorpusId": "229923551"
                },
                {
                    "start": 1316,
                    "end": 1338,
                    "matchedPaperCorpusId": "168170119"
                },
                {
                    "start": 1698,
                    "end": 1715,
                    "matchedPaperCorpusId": "207853290"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59228515625
        },
        {
            "corpus_id": "270764497",
            "title": "Navigating LLM Ethics: Advancements, Challenges, and Future Directions",
            "text": "Language Large Models (LLMs) like ChatGPT and LLaMA can exhibit biases. These biases generally result from their training data and the algorithms used in their development. The biases in LLMs can be categorized into social biases, language biases, and representation biases.(See Table 4) Social biases in LLMs have been extensively studied by scholars recently. Social biases reflect model biases related to various aspects of social status and characteristics, such as gender, age, income, race, occupation, education, and lifestyle choices [37]- [50]. These biases are generated due to the nature that LLMs can perpetuate biases in the training data, which reinforces stereotypes and unequal performance in content generation related to different social groups. [45] conducted an examination of gender biases in reference letters generated by LLMs, focusing on biases in language style and lexical content. The results indicated substantial gender biases in the LLMgenerated recommendation letters. [41] employed ChatGPT and LLaMA to create news content, using headlines from two newspapers recognized for their unbiased reporting. They assessed the gender and racial biases in the content generated by these LLMs by comparing it with the original news articles. The study found that the LLM-generated content displayed significant biases, particularly showing discrimination against women and Black individuals. \n\nLanguage biases refers to the preferential treatment or unequal performance of LLMs across different languages, dialects, or linguistic styles. These biases can lead to unequal service quality or information accuracy, reinforcing social inequalities in education, communication and business [37], [44]. In addition, [37] found that biases regarding gender, race, age, religion, and social class exist when the LLMs are used for translation tasks.",
            "score": 0.6808970962685368,
            "section_title": "Different types of bias in LLM",
            "char_start_offset": 18977,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 71
                },
                {
                    "start": 72,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1414
                },
                {
                    "start": 1417,
                    "end": 1560
                },
                {
                    "start": 1561,
                    "end": 1719
                },
                {
                    "start": 1720,
                    "end": 1863
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7109375
        },
        {
            "corpus_id": "258747813",
            "title": "Bias Detection for Customer Interaction Data: A Survey on Datasets, Methods, and Tools",
            "text": "Unfortunately, social biases (e.g., racism, sexism, xenophobia, ageism) exist in society and are reflected in the stories our society tells and has told throughout history. These stories and articles and comments that reflect our society's biases, however, make up much of the text corpora used to build and train language models, such that the language models at any step in the workflow inherit those social biases when encoding information, revealing themselves in disparate treatment and impact when that information is used by the model to make decisions and create outputs later down the line. \n\nThere have been biases found in all stages of the workflow, from the building corpora to the embeddings to the task corpora to the models performing the tasks. In addition, there have been flaws found even in the different bias detection and debiasing methods, which also must be acknowledged.",
            "score": 0.680378228912433,
            "section_title": "IV. REVIEW OF THE TOOLS/LIBRARIES",
            "char_start_offset": 19783,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 599
                },
                {
                    "start": 602,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 895
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73291015625
        },
        {
            "corpus_id": "265212726",
            "title": "Social Bias Probing: Fairness Benchmarking for Language Models",
            "text": "While the impact of social biases in language models has been recognized, prior methods for bias evaluation have been limited to binary association tests on small datasets, limiting our understanding of bias complexities. This paper proposes a novel framework for probing language models for social biases by assessing disparate treatment, which involves treating individuals differently according to their affiliation with a sensitive demographic group. We curate SoFa, a large-scale benchmark designed to address the limitations of existing fairness collections. SoFa expands the analysis beyond the binary comparison of stereotypical versus anti-stereotypical identities to include a diverse range of identities and stereotypes. Comparing our methodology with existing benchmarks, we reveal that biases within language models are more nuanced than acknowledged, indicating a broader scope of encoded biases than previously recognized. Benchmarking LMs on SoFa, we expose how identities expressing different religions lead to the most pronounced disparate treatments across all models. Finally, our findings indicate that real-life adversities faced by various groups such as women and people with disabilities are mirrored in the behavior of these models.",
            "score": 0.680378228912433,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89697265625
        },
        {
            "corpus_id": "276742025",
            "title": "Structured Reasoning for Fairness: A Multi-Agent Approach to Bias Detection in Textual Data",
            "text": "Research on bias in Natural Language Processing (NLP) has evolved considerably over the last decade, driven by growing concerns over computational models often reflecting and amplifying existing societal prejudices. Early approaches for addressing this issue largely focused on debiasing static word embeddings, as demonstrated by Bolukbasi et al. (NIPS 2016), who identified systematic gender biases in vector representations and proposed geometric alignment techniques to mitigate them (Bolukbasi et al. 2016). While these initial efforts effectively highlighted the pervasiveness of stereotyping in word embeddings, they addressed only limited linguistic contexts and were insufficient in capturing the subtleties of contextualized language models. \n\nSubsequent work expanded the focus to contextual embeddings. Zhao et al. illustrated how transformer-based models inadvertently perpetuate gender and racial biases across various NLP tasks, emphasizing the potential adverse consequences for downstream applications (Zhao et al. 2019). Efforts to measure and quantify bias in contextual representations often rely on carefully designed benchmarks and diagnostic tests, such as the StereoSet and CrowS-Pairs datasets, which reveal performance disparities correlated with sensitive attributes (Nadeem, Bethke, and Reddy 2020;Nangia et al. 2020). However, purely quantitative evaluation methods can frequently overlook more nuanced forms of bias-particularly statements that embed subtle value judgments rather than including explicit biases (Zhao, Wang, and Wang 2025). \n\nA second line of inquiry examines explainability and interpretability as prerequisites for credible bias detection. Ribeiro et al. (KDD 2016) proposed Local Interpretable Model-Agnostic Explanations (LIME) to help end-users understand and trust classifier decisions (Ribeiro, Singh, and Guestrin 2016). Lundberg and Lee (NIPS 2017) introduced SHAP (SHapley Additive exPlanations), a unified framework for interpreting predictions across a variety of models (Lundberg and Lee 2017). While these techniques demystify model outputs by highlighting salient tokens or phrases, they do not always pinpoint the origin of biases in training data or account for the degree to which an entire statement might be skewed.",
            "score": 0.676485116539663,
            "section_title": "Related Works",
            "char_start_offset": 2894,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 751
                },
                {
                    "start": 754,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1570
                },
                {
                    "start": 1573,
                    "end": 1688
                },
                {
                    "start": 1689,
                    "end": 1875
                },
                {
                    "start": 1876,
                    "end": 2054
                },
                {
                    "start": 2055,
                    "end": 2282
                }
            ],
            "ref_mentions": [
                {
                    "start": 1294,
                    "end": 1326,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 1326,
                    "end": 1345,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 2030,
                    "end": 2053,
                    "matchedPaperCorpusId": "259129801"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69677734375
        },
        {
            "corpus_id": "260704383",
            "title": "Unmasking Nationality Bias: A Study of Human Perception of Nationalities in AI-Generated Articles",
            "text": "Natural Language Processing (NLP) is largely used as social applications across a variety of fields, like journalism, medicine, and finance [22], leveraging large language corpora to predict language formation and understand social concepts such as sentiment [66] and emotion [26]. However, recent research and surveys have revealed that these language models can mimic the human biases present in the language [14], perpetuating prejudiced behavior that dehumanizes certain sociodemographic groups by deeming them more negative or toxic [25,31,37]. Studies have shown that for terms related to gender and race, these models associate with wrongful stereotypes, leading to harmful and misrepresentative ideologies that propagate populistic views [10,13,14]. \n\nExisting research has identified how various NLP architectures, such as embedding models and LLMs, can automatically mimic biases related to race [51], gender [41], disability [64], and religion [1]. To identify such biases works such as Perturbation Analysis [54] and StereoSet [47] have developed sentence frames and mechanisms for measuring them in both embedding layers and LLM models. \n\nOne of the primary causes of bias stems from training on a skewed dataset, which tends to propagate the majority's viewpoint, causing minority populations to be misrepresented [7]. These data tend to come from large internet crawls that are not representative of the various perspectives of the world [68], causing the model to learn their inherent biases. These ideologies are seen to be harmful as they deem a certain population to be more negative or toxic than another [50,67]. Prior work has shown how such models are commonly used in a social setting to predict social behaviors based on demographic and to analyze online abuse and political discourse from texts [8,23,24]. These systems, if explicitly biased, can cause social harm, such as stereotyping and dehumanization of a sociodemographic group [17]. \n\nVery few works have explored nationality bias's impact on society, despite its significance in understanding the representation of nationality in language models.",
            "score": 0.6762599665823523,
            "section_title": "RELATED WORKS 2.1 Bias in Natural Language Processing",
            "char_start_offset": 4003,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 757
                },
                {
                    "start": 760,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1149
                },
                {
                    "start": 1152,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1508
                },
                {
                    "start": 1509,
                    "end": 1633
                },
                {
                    "start": 1634,
                    "end": 1831
                },
                {
                    "start": 1832,
                    "end": 1965
                },
                {
                    "start": 1968,
                    "end": 2130
                }
            ],
            "ref_mentions": [
                {
                    "start": 140,
                    "end": 144,
                    "matchedPaperCorpusId": "233832881"
                },
                {
                    "start": 259,
                    "end": 263,
                    "matchedPaperCorpusId": "246663281"
                },
                {
                    "start": 276,
                    "end": 280,
                    "matchedPaperCorpusId": "23155520"
                },
                {
                    "start": 411,
                    "end": 415,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 542,
                    "end": 545,
                    "matchedPaperCorpusId": "218487466"
                },
                {
                    "start": 545,
                    "end": 548,
                    "matchedPaperCorpusId": "218517088"
                },
                {
                    "start": 746,
                    "end": 750,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 750,
                    "end": 753,
                    "matchedPaperCorpusId": "249431834"
                },
                {
                    "start": 753,
                    "end": 756,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 906,
                    "end": 910,
                    "matchedPaperCorpusId": "236460108"
                },
                {
                    "start": 919,
                    "end": 923,
                    "matchedPaperCorpusId": "190000105"
                },
                {
                    "start": 936,
                    "end": 940,
                    "matchedPaperCorpusId": "252819117"
                },
                {
                    "start": 955,
                    "end": 958,
                    "matchedPaperCorpusId": "231603388"
                },
                {
                    "start": 1020,
                    "end": 1024,
                    "matchedPaperCorpusId": "203078302"
                },
                {
                    "start": 1039,
                    "end": 1043,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 1328,
                    "end": 1331,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 1821,
                    "end": 1824,
                    "matchedPaperCorpusId": "10077030"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6513671875
        },
        {
            "corpus_id": "236477795",
            "title": "He is very intelligent, she is very beautiful? On Mitigating Social Biases in Language Modelling and Generation",
            "text": "There has been research in studying systems trained on human-written texts that learn human-like biases (Bolukbasi et al., 2016;Caliskan et al., 2016;Sun et al., 2019). Some of them address allocation bias (Crawford, 2017) in which a system unfairly allocates resources to certain groups over others, representation bias (Crawford, 2017) in which systems detract the social identity and representation of certain groups (Bolukbasi et al., 2016), stereotyping in which existing societal stereotypes are reinforced (Bolukbasi et al., 2016;Douglas, 2017;Anne Hendricks et al., 2018) , under-representation bias in which certain groups are disproportionately underrepresented (Lu et al., 2018;Garimella et al., 2019), and recognition bias in which a recognition algorithm's accuracy is lower for certain groups (Douglas, 2017;Anne Hendricks et al., 2018). Such biases may occur in multiple parts of an NLP system, including the training data, resources, pre-trained models, and algorithms (Bolukbasi et al., 2016;Caliskan et al., 2016;Zhao et al., 2018;Garg et al., 2018). The propagation of such biases poses the risk of reinforcing dangerous stereotypes in downstream tasks (Agarwal et al., 2019;Bhaskaran and Bhallamudi, 2019). \n\nWhile there exist works on mitigating social biases in language representations (Bolukbasi et al., 2016;Liang et al., 2020), there has been very little focus on debiasing the language models themselves or generation systems, specifically pre-trained language models that are widely used in several generation tasks. Qian et al. (2019) showed the effectiveness of mitigating gender bias in word-level language models using a gender-equalizing loss function. Sheng et al. (2020) used adversarial triggers (Wallace et al., 2019) for controllable biases in language generation; however, this method does not debias the whole distribution but only obtains non-biased continuations of given prompts.",
            "score": 0.6761249562025599,
            "section_title": "Related Work",
            "char_start_offset": 4782,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1226
                },
                {
                    "start": 1229,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1685
                },
                {
                    "start": 1686,
                    "end": 1922
                }
            ],
            "ref_mentions": [
                {
                    "start": 104,
                    "end": 128,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 150,
                    "end": 167,
                    "matchedPaperCorpusId": "195316733"
                },
                {
                    "start": 420,
                    "end": 444,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 513,
                    "end": 537,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 551,
                    "end": 579,
                    "matchedPaperCorpusId": "4384334"
                },
                {
                    "start": 689,
                    "end": 712,
                    "matchedPaperCorpusId": "196181460"
                },
                {
                    "start": 822,
                    "end": 850,
                    "matchedPaperCorpusId": "4384334"
                },
                {
                    "start": 985,
                    "end": 1009,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 1049,
                    "end": 1067,
                    "matchedPaperCorpusId": "4930886"
                },
                {
                    "start": 1172,
                    "end": 1194,
                    "matchedPaperCorpusId": "174803409"
                },
                {
                    "start": 1194,
                    "end": 1225,
                    "matchedPaperCorpusId": "195584301"
                },
                {
                    "start": 1309,
                    "end": 1333,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 1333,
                    "end": 1352,
                    "matchedPaperCorpusId": "207996257"
                },
                {
                    "start": 1545,
                    "end": 1563,
                    "matchedPaperCorpusId": "170078973"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.576171875
        },
        {
            "corpus_id": "270869490",
            "title": "Characterizing Stereotypical Bias from Privacy-preserving Pre-Training",
            "text": "Bias in machine learning is viewed as prior information that informs algorithmic learning (Mitchell, 1980).When the prior information is predicated on stereotypes and prejudices, bias transcends this neutral definition and manifests in a disproportionate weight in favor of or against a social group.\n\nThe origins of these problematic biases are often rooted in the raw data used to develop machine learning models (Caliskan et al., 2017).Implicit or explicit stereotypes based on characteristics such as gender and race can cause the models to perpetuate and propagate these biases.This can significantly affect perception and decision making.The issue with stereotypical bias is particularly acute in the context of language models due to their extensive training on vast corpora that reflect biases present in human language.This bias magnifies the potential to influence its tone (Dhamala et al., 2021) and content (Abid et al., 2021), resulting in negative effects on individuals and society at large.\n\nUsing tests for association analogies, prior research demonstrated that embeddings harbor stereotypical biases related to gender (Bolukbasi et al., 2016;Kurita et al., 2019;Chaloner and Maldonado, 2019) and race (Manzini et al., 2019).Specifically, Caliskan et al. (2017) showed that terms related to career are associated with male names rather than female names, whereas unpleasant terms are associated with ethnic minorities.Garg et al. (2018) elaborate on the temporal dimension of bias in word embeddings by observing changes in gender and ethnic stereotypes over a century.This diachronic analysis indicates that while certain stereotypes have diminished over time, others remain robustly encoded in language.By investigating bias diffusion, Papakyriakopoulos et al. (2020) showed that biases contained in word embeddings can permeate natural language understanding, while Abid et al. (2021) report stereotypes in language generation such as violence for certain religious groups.\n\nUnlike these studies on bias in raw data, we examine the bias that stems from text privatization.",
            "score": 0.6758495499240184,
            "section_title": "Stereotypical Bias",
            "char_start_offset": 7348,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 107,
                    "end": 300
                },
                {
                    "start": 302,
                    "end": 439
                },
                {
                    "start": 439,
                    "end": 583
                },
                {
                    "start": 583,
                    "end": 644
                },
                {
                    "start": 644,
                    "end": 828
                },
                {
                    "start": 828,
                    "end": 1006
                },
                {
                    "start": 1008,
                    "end": 1243
                },
                {
                    "start": 1243,
                    "end": 1436
                },
                {
                    "start": 1436,
                    "end": 1587
                },
                {
                    "start": 1587,
                    "end": 1723
                },
                {
                    "start": 1723,
                    "end": 1994
                },
                {
                    "start": 1996,
                    "end": 2093
                }
            ],
            "ref_mentions": [
                {
                    "start": 884,
                    "end": 906,
                    "matchedPaperCorpusId": "231719337"
                },
                {
                    "start": 919,
                    "end": 938,
                    "matchedPaperCorpusId": "231603388"
                },
                {
                    "start": 1137,
                    "end": 1161,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 1161,
                    "end": 1181,
                    "matchedPaperCorpusId": "190000105"
                },
                {
                    "start": 1181,
                    "end": 1210,
                    "matchedPaperCorpusId": "199524712"
                },
                {
                    "start": 1436,
                    "end": 1454,
                    "matchedPaperCorpusId": "4930886"
                },
                {
                    "start": 1756,
                    "end": 1787,
                    "matchedPaperCorpusId": "211040995"
                },
                {
                    "start": 1887,
                    "end": 1905,
                    "matchedPaperCorpusId": "231603388"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.732421875
        },
        {
            "corpus_id": "264306000",
            "title": "A Predictive Factor Analysis of Social Biases and Task-Performance in Pretrained Masked Language Models",
            "text": "As MLMs have been successfully applied to diverse NLP tasks, it is important to study the factors that determine their social biases. Rogers et al. (2020) reviewed the current state of knowledge regarding how BERT works, what kind of information it learns and how it is encoded, typically alterations to its training objectives and architecture, the overparameterization issue and approaches to compression. Xia et al. (2020) studied contextualised encoders in various aspects and discussed the trade-off between task performance and the potential harms contained in the pretraining data. Later, P\u00e9rez-Mayos et al. (2021) investigated the effect of pretraining data size on the syntactic capabilities of RoBERTa and they showed that models pretrained with more data tend to encode better syntactic information and provide more syntactic generalisation across different syntactic structures. However, these studies focus on MLMs in downstream tasks, while none consider the social biases in MLMs. \n\nOn the other hand, models trained for different downstream tasks have been found to exhibit social biases. Kiritchenko and Mohammad (2018) evaluated gender and racial biases across 219 automatic sentiment analysis systems and discovered statistically significant biases occurring in several systems. D\u00edaz et al. (2018) studied age-related biases in sentiment classification and discovered that significant age bias is encoded in the output of many sentiment analysis systems as well as word embeddings. Zhao et al. (2020) focused on gender bias in multilingual embeddings and its influence on the process of transfer learning for NLP applications. They showed that the level of bias in multilingual representations varies depending on how the embeddings are aligned to different target spaces, and that the alignment direction can also affect the bias in transfer learning. Choenni et al. (2021) investigated the types of stereotypical information that are captured by pretrained language models and showed the variability of attitudes towards various social groups among models and the rapid changes in emotions and stereotypes that occur during the fine-tuning phase.",
            "score": 0.67217300223472,
            "section_title": "Related Work",
            "char_start_offset": 3468,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 995
                },
                {
                    "start": 998,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1500
                },
                {
                    "start": 1501,
                    "end": 1645
                },
                {
                    "start": 1646,
                    "end": 1871
                },
                {
                    "start": 1872,
                    "end": 2167
                }
            ],
            "ref_mentions": [
                {
                    "start": 408,
                    "end": 425,
                    "matchedPaperCorpusId": "222124957"
                },
                {
                    "start": 596,
                    "end": 621,
                    "matchedPaperCorpusId": "237433614"
                },
                {
                    "start": 1105,
                    "end": 1136,
                    "matchedPaperCorpusId": "21670658"
                },
                {
                    "start": 1298,
                    "end": 1316,
                    "matchedPaperCorpusId": "3272048"
                },
                {
                    "start": 1501,
                    "end": 1519,
                    "matchedPaperCorpusId": "218487087"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64501953125
        },
        {
            "corpus_id": "244954639",
            "title": "Ethical and social risks of harm from Language Models",
            "text": "diagnostic healthcare algorithms (Obermeyer et al., 2019) to social outcome prediction (Narayanan, 2021); for a more general introduction see (Chouldechova and Roth, 2018;Kordzadeh and Ghasemaghaei, 2021;Mehrabi et al., 2021;Noble, 2018;Zou and Schiebinger, 2018). Based on our current understanding, such stereotyping and unfair bias are set to recur in language technologies building on LMs unless corrective action is taken.\n\nWhy we should expect LMs to reinforce stereotypes and unfair discrimination by default LMs are optimised to mirror language as accurately as possible, by detecting the statistical patterns present in natural language Definitions. The fact that LMs track patterns, biases, and priors in natural language is not negative per se (Shah et al., 2020). Rather, it becomes a problem when the training data is unfair, discriminatory, or toxic. In this case, the optimisation process results in models that mirror these harms. As a result, LMs that perform well with regard to their optimisation objective can work poorly with regard to social harms, insofar as they encode and perpetuate harmful stereotypes and biases present in the training data.\n\nStereotypes and unfair discrimination can be present in training data for different reasons. First, training data reflect historical patterns of systemic injustice when they are gathered from contexts in which inequality is the status quo. Training systems on such data entrenches existing forms of discrimination (Browne, 2015). In this way, barriers present in our social systems can be captured by data, learned by LMs, and perpetuated by their predictions (Hampton, 2021).\n\nSecond, training data can be biased because some communities are better represented in the training data than others. As a result, LMs trained on such data often model speech that fails to represent the language of those who are marginalised, excluded, or less often recorded. The groups that are traditionally underrepresented in training data are often disadvantaged groups: they are also referred to as the 'undersampled majority' (Raji, 2020). The implications of unrepresentative training data for downstream biases and stereotyping in LMs demonstrate the power that is exercised by those who",
            "score": 0.6712675561956241,
            "section_title": "Problem",
            "char_start_offset": 17629,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 33,
                    "end": 57,
                    "matchedPaperCorpusId": "204881868"
                },
                {
                    "start": 171,
                    "end": 204,
                    "matchedPaperCorpusId": "236273467"
                },
                {
                    "start": 204,
                    "end": 225,
                    "matchedPaperCorpusId": "201666566"
                },
                {
                    "start": 237,
                    "end": 263,
                    "matchedPaperCorpusId": "51677390"
                },
                {
                    "start": 755,
                    "end": 774,
                    "matchedPaperCorpusId": "209461005"
                },
                {
                    "start": 1631,
                    "end": 1646,
                    "matchedPaperCorpusId": "231698439"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68798828125
        },
        {
            "corpus_id": "265212928",
            "title": "Exploring the Jungle of Bias: Political Bias Attribution in Language Models via Dependency Analysis",
            "text": "With the rise of large language models (LLMs) (Anil et al., 2023;OpenAI, 2023;Touvron et al., 2023;Reid et al., 2024, inter alia), we are witnessing increasing concern towards their nega-tive implications, such as the existence of biases, including social (Mei et al., 2023), cultural (Narayanan Venkit et al., 2023), brilliance (Shihadeh et al., 2022), nationality (Venkit et al., 2023), religious (Abid et al., 2021), and political biases (Feng et al., 2023). For instance, there is a growing indication that ChatGPT, on average, prefers proenvironmental, left-libertarian positions (Hartmann et al., 2023;Feng et al., 2023). \n\nDespite its practical relevance, bias in (large) language models is still a poorly understood topic (Blodgett et al., 2021;Dev et al., 2022;Talat et al., 2022). The frequent interpretation of LLM bias as statistical bias originating from training data, while conceptually correct, is strongly limited in its utility. van der Wal et al. (2022) reason that bias should, therefore, not be viewed as a singular concept but rather distinguish different concepts of bias at different levels of the NLP pipeline, e.g. distinct dataset and model biases. Furthermore, while it is undisputed that models do exhibit some biases, it is unclear whose biases they are exhibiting (Petreski and Hashim, 2022). Indeed, the literature up to this point has mostly focused on the downstream effects of bias -with only a few exceptions, such as van der Wal et al. (2022) that argue for the importance of an understanding of the internal causes. To advance this endeavour, we analyse LLM bias through the lens of causal fairness analysis, which facilitates both comprehending the origins of bias and reasoning about the subsequent consequences of bias and its mitigation. \n\nA thorough understanding of LLM bias is particularly important for the design and implementation of debiasing methods.",
            "score": 0.6693253313893684,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 627
                },
                {
                    "start": 630,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1553
                },
                {
                    "start": 1554,
                    "end": 1779
                },
                {
                    "start": 1782,
                    "end": 1900
                }
            ],
            "ref_mentions": [
                {
                    "start": 256,
                    "end": 274,
                    "matchedPaperCorpusId": "259129801"
                },
                {
                    "start": 399,
                    "end": 418,
                    "matchedPaperCorpusId": "231603388"
                },
                {
                    "start": 441,
                    "end": 460,
                    "matchedPaperCorpusId": "258686693"
                },
                {
                    "start": 585,
                    "end": 608,
                    "matchedPaperCorpusId": "255440573"
                },
                {
                    "start": 608,
                    "end": 626,
                    "matchedPaperCorpusId": "258686693"
                },
                {
                    "start": 730,
                    "end": 753,
                    "matchedPaperCorpusId": "236460302"
                },
                {
                    "start": 753,
                    "end": 770,
                    "matchedPaperCorpusId": "252907216"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.496337890625
        },
        {
            "corpus_id": "250390797",
            "title": "The Birth of Bias: A case study on the evolution of gender bias in an English language model",
            "text": "We understand bias as a systematic deviation in behaviour from a norm. As our focus is on gender bias in language models, the relevant behaviour we are measuring is how strongly certain words or concepts (in our case occupation terms such as nurse or carpenter) are associated by the model with one gender instead of another. This strength of association can be measured in different ways and at different points in the language modelling pipeline. In particular, we will look at bias in internal representations of the model and in its output behaviour. Ideally, the strength of association should be equal for different genders. If the model deviates from this norm, we say that the model exhibits gender bias. 2 hether bias in a language model causes harm, depends on the downstream application of the model and what constitutes fair and just behaviour in this particular context, but we believe that a detailed understanding of how bias is learned by and represented in these models can facilitate the development of methods to counteract bias that are tailored to a particular application and the potential harm bias can cause in that context. \n\nWith this broad scope, we hope to learn how the language model represents gender stereotypes of occupations in earlier representations of the pipeline, and how these may help explain representational harms (Blodgett et al., 2020) downstream. For instance, if a language model with gender stereotypes for occupations is used in a translation system, it may propagate the undesirable world-view of all doctors being male and all nurses being female. Understanding how these stereotypical representations come about can help in developing new detection and mitigation strategies for these and other stereotypes in AI systems building on language models. \n\nThe LSTM language model In this paper, we study the gender bias of an LSTM language model (Hochreiter and Schmidhuber, 1997). We follow the setup from Gulordava et al. (2018), and train the model on their training set of ~90M tokens, with a vocabulary of 50,000 (full-word) tokens, extracted from the English Wikipedia corpus. Following Gulordava et al., we lower the learning rate at epoch 20 using a plateau scheduler. Our training regime differs in one aspect: we use weight-tying for the encoder and decoder (Press and Wolf, 2017).",
            "score": 0.665450533833357,
            "section_title": "Gender bias",
            "char_start_offset": 3830,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 70
                },
                {
                    "start": 71,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 1148
                },
                {
                    "start": 1151,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1598
                },
                {
                    "start": 1599,
                    "end": 1801
                },
                {
                    "start": 1804,
                    "end": 1929
                },
                {
                    "start": 1930,
                    "end": 2130
                },
                {
                    "start": 2131,
                    "end": 2224
                },
                {
                    "start": 2225,
                    "end": 2339
                }
            ],
            "ref_mentions": [
                {
                    "start": 1357,
                    "end": 1380,
                    "matchedPaperCorpusId": "218971825"
                },
                {
                    "start": 1894,
                    "end": 1928,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 1955,
                    "end": 1978,
                    "matchedPaperCorpusId": "4460159"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6015625
        },
        {
            "corpus_id": "266348370",
            "title": "Exploring Social Bias in Downstream Applications of Text-to-Image Foundation Models",
            "text": "Social Bias in ML: Intersectional social bias refers to the overlapping and inter-dependent forms of discrimination that individuals face due to any combination of their race, gender, class, sexuality or any other identity factors. Several works have studied how intersectionality affects the manifestation of bias in ML, including in word embeddings ( [27,28], language ([29, 30, 31]) and image-generation ( [15,14] models. Another consideration is the distinction between extrinsic and intrinsic bias, described in [32] as the biases that originate from pre-training and fine-tuning, respectively. As there is no fine-tuning on task-specific data when re-purposing text-to-image models for the downstream tasks presented, we refer to any biases present here as intrinsic. \n\nDiffusion models: Details regarding Denoising Diffusion Models are found in [33,2,34,35]. \n\nDiffusion-based Image Editing: In CLIP latent space models (e.g., [4,6]), image generation initializes diffusion from a random latent vector, whereas image editing initializes from an embedding of the image to be edited [12,13]. Often, the model is shared between image generation and editing tasks, with differences being the starting point (the latent embedding) and hyper-parameters. \n\nA crucial hyper-parameter, \"strength\", defaulting to 0.8 (max: 1.0), controls noise addition to the reference image. Higher values result in more noise and denoising iterations, yielding edits that better match the edit prompt but are less consistent with the original image.",
            "score": 0.6644728491015423,
            "section_title": "Preliminaries",
            "char_start_offset": 6403,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 773
                },
                {
                    "start": 776,
                    "end": 865
                },
                {
                    "start": 868,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1254
                },
                {
                    "start": 1257,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1532
                }
            ],
            "ref_mentions": [
                {
                    "start": 357,
                    "end": 360,
                    "matchedPaperCorpusId": "219530686"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51171875
        },
        {
            "corpus_id": "265456057",
            "title": "Evaluating Large Language Models through Gender and Racial Stereotypes",
            "text": "Large Language Models appended with a chat feature for easy inference like ChatGPT, have grown in popularity. This rise in popularity isn't limited to scientists and engineers familiar with traditional coding, but also among common users. These models are often trained on a large amount (1PB) of data. This data is extracted from texts that are, which may cause societal biases from the source to infiltrate the model and affect the results of the model. \n\nPrevious studies have shown that these biases can get exaggerated to downstream tasks, especially when used on a large-scale basis for real-world applications and products. In this study, we wish to identify the presence of such biases in popular models used today. \n\nWe wish to be able to evaluate Large Language Models on their performance with respect to gender and racial biases. We believe this will allow us to gauge how to model represents each gender and race. \n\nTo do so we claim the prevalence of this bias in a workplace and professional context, as prior research [3] has identified a long history and evidence of biases creeping up in the workplace.",
            "score": 0.6630550777722676,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 455
                },
                {
                    "start": 458,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 723
                },
                {
                    "start": 726,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 926
                },
                {
                    "start": 929,
                    "end": 1120
                }
            ],
            "ref_mentions": [
                {
                    "start": 1034,
                    "end": 1037,
                    "matchedPaperCorpusId": "23163324"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.724609375
        },
        {
            "corpus_id": "271745282",
            "title": "Decoding Biases: Automated Methods and LLM Judges for Gender Bias Detection in Language Models",
            "text": "In recent years, the proliferation of Large Language Models (LLMs) like GPT-4 has revolutionized natural language processing, enabling applications from automated text generation to complex language understanding. However, as these models integrate more deeply into technological solutions, their inherent biases have become a significant concern. These biases can lead to misrepresentation of individuals and groups, promote stereotypes, and suggest unfair allocation of resources, impacting people both psychologically and socio-economically. \n\nDespite a fair amount of research happening on AI Safety and fairness, there is still a lack of understanding of measuring societal biases in language models systematically. Biases in these models-such as gender, racial, ethnic, and ideological biases-stem from training data that reflect societal prejudices. Consequently, language models can perpetuate and even amplify these biases, leading to outputs that reinforce stereotypes and disadvantage certain groups (Bender et al., 2021). The implications are profound, affecting areas, like hiring, law enforcement, and daily AI interactions (Schwartz et al., 2022;Ferrara, 2023). Thus, assessing and mitigating bias in language models is essential for their ethical and fair use. \n\nWe define bias as the semantic variance in text generated by an LLM along with differential treatment towards stereotype identification, refusal, and correction of social group attributes when presented with prompts obtained by varying only the social group association. For example, in Figure 1, the LLM is presented with a prompt \"I think men are much better with managing finances\" and another obtained by only varying the gender (men to women). The image on the left shows the scenario where the LLM responds differently to these inputs. In one case it identifies the stereotype present in the prompt and rejects this with an explanation; in the other, it fails to identify any gender bias in the input and supports the gender-based claim instead. On the right shows an unbiased model that responds to both the prompts similarly, by identifying and rejecting the gender-stereotype in the input. \n\nBias mitigation techniques rely on bias assessment metrics to evaluate their efficacy. Bias evaluation metrics are broadly classified into embeddingbased, probability-based, and generated-text-based metrics (Gallegos et al., 2023).",
            "score": 0.6628279593781039,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 544
                },
                {
                    "start": 547,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1276
                },
                {
                    "start": 1279,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1727
                },
                {
                    "start": 1728,
                    "end": 1820
                },
                {
                    "start": 1821,
                    "end": 2030
                },
                {
                    "start": 2031,
                    "end": 2177
                },
                {
                    "start": 2180,
                    "end": 2266
                },
                {
                    "start": 2267,
                    "end": 2411
                }
            ],
            "ref_mentions": [
                {
                    "start": 1011,
                    "end": 1032,
                    "matchedPaperCorpusId": "121125604"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80908203125
        },
        {
            "corpus_id": "270380297",
            "title": "Improving Commonsense Bias Classification by Mitigating the Influence of Demographic Terms",
            "text": "Extensive research in Natural Language Processing (NLP) has focused on investigating biases in language models and developing strategies to understand and mitigate them.A study exploring the relationship between the size of language models and their biases was conducted, finding that larger models exhibit more nuanced biases and an increased potential for bias in specific categories [5].Researchers have also explored bias evaluation in language models, employing extrinsic evaluation methods and debiasing techniques to identify and address bias in real world applications and downstream tasks [7,8].Intrinsic evaluation methods, such as analyzing word or sentence embeddings, contribute to a deeper understanding of biases within language models [9,10,11].Additionally, It has been shown that systematic differences in bias measurement can be revealed through the unification of extrinsic evaluation methods, attributed to parameter choices [12].Moreover, these methods have been used to identify biased decisions from models in specific tasks [13].Recent studies have drawn significant attention to the bias caused by demographic terms in language models.Various techniques have been proposed to address and mitigate demographic bias, aiming to promote fairness, equality, and inclusivity.These techniques include debiasing models trained on human-annotated examples [10], fairness-aware neural language models [14], bias-mitigating transformer architectures [15], and debiasing frameworks to identify and correct demographic bias [16].Additionally, post-hoc debiasing techniques have been explored, where a debiasing step is added to sentence representations after initial training, prior to their utilization in downstream tasks [10,17].While preceding investigations focused on biases in language models linked to particular categories like origin and gender, our study adopts a more extensive methodology, assessing biases across a wider range of demographic terms.Building upon previous studies, our research proposes novel approaches to mitigate the impact of demographic terms and enhance the performance of the commonsense polarization classifier in knowledge models.This approach distinguishes itself from prior research by employing augmentation to substitute the predicate segment of a sentence with a synonym while retaining the original semantic content.Furthermore, to address bias stemming from demographic terms, our methodology involves substituting specific demographic terms with broader alternatives.",
            "score": 0.6599282196114179,
            "section_title": "II. RELATED WORK",
            "char_start_offset": 3511,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 169,
                    "end": 390
                },
                {
                    "start": 390,
                    "end": 604
                },
                {
                    "start": 604,
                    "end": 761
                },
                {
                    "start": 761,
                    "end": 951
                },
                {
                    "start": 951,
                    "end": 1054
                },
                {
                    "start": 1054,
                    "end": 1161
                },
                {
                    "start": 1161,
                    "end": 1295
                },
                {
                    "start": 1295,
                    "end": 1542
                },
                {
                    "start": 1542,
                    "end": 1745
                },
                {
                    "start": 1745,
                    "end": 1975
                },
                {
                    "start": 1975,
                    "end": 2181
                },
                {
                    "start": 2181,
                    "end": 2373
                },
                {
                    "start": 2373,
                    "end": 2526
                }
            ],
            "ref_mentions": [
                {
                    "start": 386,
                    "end": 389,
                    "matchedPaperCorpusId": "248649722"
                },
                {
                    "start": 598,
                    "end": 601,
                    "matchedPaperCorpusId": "58006082"
                },
                {
                    "start": 751,
                    "end": 754,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 754,
                    "end": 757,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 757,
                    "end": 760,
                    "matchedPaperCorpusId": "219530686"
                },
                {
                    "start": 946,
                    "end": 950,
                    "matchedPaperCorpusId": "235658325"
                },
                {
                    "start": 1049,
                    "end": 1053,
                    "matchedPaperCorpusId": "44090948"
                },
                {
                    "start": 1373,
                    "end": 1377,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 1417,
                    "end": 1421,
                    "matchedPaperCorpusId": "19117312"
                },
                {
                    "start": 1465,
                    "end": 1469,
                    "matchedPaperCorpusId": "198968250"
                },
                {
                    "start": 1737,
                    "end": 1741,
                    "matchedPaperCorpusId": "1704893"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69970703125
        },
        {
            "corpus_id": "268667074",
            "title": "Protected group bias and stereotypes in Large Language Models",
            "text": "Bias in language models.Extensive prior work has documented biases in language models.Gender bias has been shown to exist in a broad array of NLP tasks, including sentiment analysis, toxicity detection, machine translation, and more, as well as in word embeddings themselves (Bolukbasi et al., 2016;Caliskan et al., 2017;Garg et al., 2017;Zhao et al., 2017Zhao et al., , 2018aZhao et al., ,b, 2019;;Rudinger et al., 2018;Basta et al., 2019;May et al., 2019;Kurita et al., 2019;Swinger et al., 2019;Kiritchenko and Mohammad, 2018;Vanmassenhove et al., 2018;Park et al., 2018;Lu et al., 2019;Stanovsky et al., 2019).Bias has similarly been found in social categories such as religion, race, nationality, disability, and occupation (Sap et al., 2020;Abid et al., 2021;Kirk et al., 2021;Ousidhoum et al., 2021;Venkit et al., 2022Venkit et al., , 2023;;Zhuo et al., 2023;Blodgett et al., 2021;Talat et al., 2022;Guo and Caliskan, 2021).\n\nStudies of intersectional biases are complicated by the fact that their magnitude grows exponentially as more dimensions are added.However, individuals are multi-faceted, and focusing on just one dimension risks missing important effects that happen at their intersection (Crenshaw, 1989(Crenshaw, , 1991;;Foulds et al., 2020;Guo and Caliskan, 2021).Research has shown that, unrestricted, language models reflect and amplify the biases of the broader society in which they are embedded (Barocas and Selbst, 2016;Zhao et al., 2017Zhao et al., , 2018b;;Hashimoto et al., 2018;Leino et al., 2019;Sun et al., 2019;Jia et al., 2020;Bender et al., 2021).",
            "score": 0.6590745719834944,
            "section_title": "Related work",
            "char_start_offset": 2016,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 24
                },
                {
                    "start": 24,
                    "end": 86
                },
                {
                    "start": 86,
                    "end": 614
                },
                {
                    "start": 614,
                    "end": 931
                },
                {
                    "start": 933,
                    "end": 1064
                },
                {
                    "start": 1064,
                    "end": 1283
                },
                {
                    "start": 1283,
                    "end": 1581
                }
            ],
            "ref_mentions": [
                {
                    "start": 299,
                    "end": 321,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 356,
                    "end": 376,
                    "matchedPaperCorpusId": "4952494"
                },
                {
                    "start": 440,
                    "end": 457,
                    "matchedPaperCorpusId": "85518027"
                },
                {
                    "start": 457,
                    "end": 477,
                    "matchedPaperCorpusId": "190000105"
                },
                {
                    "start": 498,
                    "end": 529,
                    "matchedPaperCorpusId": "21670658"
                },
                {
                    "start": 556,
                    "end": 574,
                    "matchedPaperCorpusId": "52070035"
                },
                {
                    "start": 747,
                    "end": 765,
                    "matchedPaperCorpusId": "231603388"
                },
                {
                    "start": 783,
                    "end": 806,
                    "matchedPaperCorpusId": "236460108"
                },
                {
                    "start": 907,
                    "end": 930,
                    "matchedPaperCorpusId": "219530686"
                },
                {
                    "start": 1220,
                    "end": 1239,
                    "matchedPaperCorpusId": "24661090"
                },
                {
                    "start": 1239,
                    "end": 1259,
                    "matchedPaperCorpusId": "51710633"
                },
                {
                    "start": 1259,
                    "end": 1282,
                    "matchedPaperCorpusId": "219530686"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5419921875
        },
        {
            "corpus_id": "271769594",
            "title": "Towards Fairer NLP Models: Handling Gender Bias In Classification Tasks",
            "text": "In recent years, there has been a significant growth in research analyzing biases present in natural language processing (NLP) systems and models. This includes studies on biases present in embedding spaces, which are representations of words and sentences generated from large text data (Bolukbasi et al., 2016;Caliskan et al., 2017;Gonen and Goldberg, 2019;Zhao et al., 2017;May et al., 2019)and in large language models (Wan et al., 2023;Kotek et al., 2023). \n\nResearchers have investigated how these biases manifest in NLP systems across a range of tasks, coreference resolution (Rudinger et al., 2017;Zhao et al., 2018), machine translation (Vanmassenhove et al., 2021;Savoldi et al., 2021;Stanovsky et al., 2019), sentiment analysis (Kiritchenko and Mohammad, 2018), and hate speech/toxicity detection (Park et al., 2018;Dixon et al., 2018), among others. As NLP models are trained on human-generated text data, they can acquire and propagate societal biases present in that data when deployed in realworld applications, leading to concerns about discriminating outputs (Park et al., 2018). \n\nMachine learning models can be deliberately designed with a specific bias aligned with their intended purpose. For example, a toxic comment detector is meant to be biased toward giving higher scores to actual toxic comments over non-toxic ones. However, such models are not intended to discriminate based on attributes like gender that might be evident in comments. If a model exhibits this behavior by scoring comments differently due to gender references, it is considered an unintended and undesirable bias. While the bias towards accurately identifying toxic content is the intended goal, any bias that leads to unfair treatment or discrimination based on attributes such as gender is regarded as an unintended bias that needs to be addressed (Dixon et al., 2018). Biased algorithmic outcomes from AI systems can negatively impact users, creating a feedback loop that amplifies existing biases (Mehrabi et al., 2021).",
            "score": 0.6577607972068008,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 461
                },
                {
                    "start": 464,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 1096
                },
                {
                    "start": 1099,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1343
                },
                {
                    "start": 1344,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1609
                },
                {
                    "start": 1610,
                    "end": 1867
                },
                {
                    "start": 1868,
                    "end": 2020
                }
            ],
            "ref_mentions": [
                {
                    "start": 288,
                    "end": 312,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 312,
                    "end": 334,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 334,
                    "end": 359,
                    "matchedPaperCorpusId": "73729169"
                },
                {
                    "start": 359,
                    "end": 377,
                    "matchedPaperCorpusId": "1389483"
                },
                {
                    "start": 423,
                    "end": 441,
                    "matchedPaperCorpusId": "264128125"
                },
                {
                    "start": 441,
                    "end": 460,
                    "matchedPaperCorpusId": "261276445"
                },
                {
                    "start": 583,
                    "end": 606,
                    "matchedPaperCorpusId": "5310359"
                },
                {
                    "start": 606,
                    "end": 624,
                    "matchedPaperCorpusId": "4952494"
                },
                {
                    "start": 674,
                    "end": 695,
                    "matchedPaperCorpusId": "233219939"
                },
                {
                    "start": 739,
                    "end": 771,
                    "matchedPaperCorpusId": "21670658"
                },
                {
                    "start": 808,
                    "end": 827,
                    "matchedPaperCorpusId": "52070035"
                },
                {
                    "start": 827,
                    "end": 846,
                    "matchedPaperCorpusId": "54997157"
                },
                {
                    "start": 1076,
                    "end": 1095,
                    "matchedPaperCorpusId": "52070035"
                },
                {
                    "start": 1846,
                    "end": 1866,
                    "matchedPaperCorpusId": "54997157"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66796875
        },
        {
            "corpus_id": "216553696",
            "title": "Causal Mediation Analysis for Interpreting Neural NLP: The Case of Gender Bias",
            "text": "We apply this framework to the analysis of gender bias in large pre-trained language models. Gender bias has surfaced as a major concern in word representations, both static word embeddings (Caliskan et al., 2017;Bolukbasi et al., 2016) and contextualized word representations (Zhao et al., 2019a;Basta et al., 2019;Tan and Celis, 2019). We study how gender bias effects are mediated via different model components in Transformer-based language models, in particular, several versions of GPT2 (Radford et al., 2019), focusing on the role of individual neurons or attention heads in mediating these effects. \n\nOur approach is a structural-behavioral analysis. It is structural in that our results highlight internal model components that are responsible for gender bias. It is behavioral in that said components are causally implicated in how gender bias manifests in the model outputs. In an experimental evaluation using several datasets designed to gauge a model's gender bias, we find that larger models show larger gender bias effects, potentially absorbing more bias from the underlying training data. The causal mediation analysis further yields several insights regarding the role of different model components in mediating gender bias: \n\n\u2022 Gender bias is sparse: Much of the effect is concentrated in relatively few model components. \u2022 Gender bias is synergistic: Some model components interact to produce mutual effects that amplify their individual effects. Other components operate relatively independently, capturing complementary aspects of gender bias. \u2022 Gender bias is decomposable: The total gender bias effect approximates the sum of the direct and indirect effect, a surprising result given the non-linear nature of the model. In summary, this paper makes two broad contributions. First, we cast causal mediation analysis as an approach for analyzing neural NLP models, which may be applied to a variety of models and phenomena. Second, we demonstrate this methodology in the case of analyzing gender bias in pre-trained language models, revealing the internal mechanisms by which bias effects flow from input to output through various model components. \n\nThe code for reproducing our results is available at https://github.com/sebastianGehrmann/ CausalMediationAnalysis.",
            "score": 0.6573779622512245,
            "section_title": "Mediator Control Variable Response Variable",
            "char_start_offset": 2864,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 93,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 606
                },
                {
                    "start": 609,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1243
                },
                {
                    "start": 1246,
                    "end": 1341
                },
                {
                    "start": 1342,
                    "end": 1467
                },
                {
                    "start": 1468,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1744
                },
                {
                    "start": 1745,
                    "end": 1798
                },
                {
                    "start": 1799,
                    "end": 1946
                },
                {
                    "start": 1947,
                    "end": 2171
                },
                {
                    "start": 2174,
                    "end": 2264
                },
                {
                    "start": 2265,
                    "end": 2289
                }
            ],
            "ref_mentions": [
                {
                    "start": 190,
                    "end": 213,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 213,
                    "end": 236,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 277,
                    "end": 297,
                    "matchedPaperCorpusId": "102352962"
                },
                {
                    "start": 297,
                    "end": 316,
                    "matchedPaperCorpusId": "121125604"
                },
                {
                    "start": 316,
                    "end": 336,
                    "matchedPaperCorpusId": "202781363"
                },
                {
                    "start": 493,
                    "end": 515,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68017578125
        },
        {
            "corpus_id": "271571036",
            "title": "Understanding the Interplay of Scale, Data, and Bias in Language Models: A Case Study with BERT",
            "text": "So although scaling laws hold that training loss decreases with scale, indicating that the model might overfit to the bias; at the same time, test accuracy also increases with scale. For this reason, we would expect that fairness measures based on accuracy statistics to actually improve with scale as the accuracy increases. Third, scale isn't the only important factor, the type of data can matter. Different data has different biases. For example, biographies about notable figures on Wikipedia are skewed towards men over women (Tripodi 2023), and large web scrapes like the common-crawl are likely more diverse in overall topics covered, but also much more toxic. \n\nOur contributions. We study MLMs to see how social biases evolve as we vary model scale. We pre-train four architecture sizes of BERT (mini,small,medium and base,i.e.,up to 110M parameters). A core focus of our study is the pre-training dataset. We experiment with English Wikipedia and the CC-100 English subset of Common Crawl. For each model size and type of training data, we compute biases in the representations upstream and performance disparities downstream, leveraging specific measures of language biases from prior research (Steed et al. 2022). We measure upstream bias along two dimensions: disparities in gender pronoun probabilities and sentiment of generated text; downstream impact is measured by fine-tuning models on a toxicity classification task, where we evaluate differences is false positive rates across a diverse set of demographic groups from prior work (Dixon et al. 2018) (e.g. Muslim, gay etc.) \n\nOur findings underscore the crucial role of pre-training data as models increase in size. For models pre-trained with CC-100, upstream biases generally increase with model size. Conversely, models pre-trained on Wikipedia show greater gender stereotyping as models increase in size. In both cases, we find that downstream biases decrease with increasing model size. However, models consistently associate certain identities such as gay and homosexual with toxicity, independent of parameter size or type of pre-training data.",
            "score": 0.6567849834800688,
            "section_title": "Introduction",
            "char_start_offset": 5828,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 668
                },
                {
                    "start": 671,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1576
                },
                {
                    "start": 1577,
                    "end": 1594
                },
                {
                    "start": 1597,
                    "end": 1686
                },
                {
                    "start": 1687,
                    "end": 1774
                },
                {
                    "start": 1775,
                    "end": 1879
                },
                {
                    "start": 1880,
                    "end": 1962
                },
                {
                    "start": 1963,
                    "end": 2122
                }
            ],
            "ref_mentions": [
                {
                    "start": 1206,
                    "end": 1225,
                    "matchedPaperCorpusId": "248780439"
                },
                {
                    "start": 1551,
                    "end": 1569,
                    "matchedPaperCorpusId": "54997157"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6533203125
        },
        {
            "corpus_id": "227151662",
            "title": "Argument from Old Man\u2019s View: Assessing Social Bias in Argumentation",
            "text": "Social bias can be understood as implicit or explicit prejudices against, as well as unequal treatment or discrimination of, certain social groups in society (Sweeney and Najafian, 2019;Papakyriakopoulos et al., 2020). A social group might be described by physical attributes of its members, such as sex and skin color, but also by more abstract categories, such as culture, heritage, gender identity, and religion. A typical, probably in itself biased, example of social bias is the old man's belief in classic gender stereotypes. In most cases, social bias is deemed negative and undesirable. \n\nRecent research shows that bias towards social groups is also present in Machine Learning and Natural Language Processing (NLP) models (Chang et al., 2019), manifesting in the encoded states of a language model (Brown et al., 2020) or simply causing worse performance for underrepresented classes (Sun et al., 2019). Such bias has been studied for different NLP contexts, including coreference resolution (Rudinger et al., 2018), machine translation (Vanmassenhove et al., 2018), and the training of word embedding models (Bolukbasi et al., 2016). In contrast, Computational Argumentation (CA) has, to our knowledge, not seen any research in this direction so far. Given that major envisioned applications of CA include the enhancement of human debating (Lawrence et al., 2017) and the support of self-determined opinion formation (Wachsmuth et al., 2017), we argue that studying social bias is particularly critical for CA. \n\nIn general, social bias may affect diverse stages of CA: In argument acquisition, for example, researchers may introduce social bias unintentionally, for instance, by collecting arguments from web sources that are only popular in a certain part of the world. This is known as sample bias (Chang et al., 2019). In argument quality assessment, a machine learning model may develop a prejudicial bias and judge arguments made by a certain social group better, for instance, because it considers features inadequate for the task, such as the gender (Jones, 2019).",
            "score": 0.6553038227324541,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 594
                },
                {
                    "start": 597,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1521
                },
                {
                    "start": 1524,
                    "end": 1782
                },
                {
                    "start": 1783,
                    "end": 1833
                },
                {
                    "start": 1834,
                    "end": 2083
                }
            ],
            "ref_mentions": [
                {
                    "start": 158,
                    "end": 186,
                    "matchedPaperCorpusId": "196185011"
                },
                {
                    "start": 186,
                    "end": 217,
                    "matchedPaperCorpusId": "211040995"
                },
                {
                    "start": 732,
                    "end": 752,
                    "matchedPaperCorpusId": "209325068"
                },
                {
                    "start": 894,
                    "end": 912,
                    "matchedPaperCorpusId": "195316733"
                },
                {
                    "start": 1002,
                    "end": 1025,
                    "matchedPaperCorpusId": "13756572"
                },
                {
                    "start": 1047,
                    "end": 1075,
                    "matchedPaperCorpusId": "76654723"
                },
                {
                    "start": 1119,
                    "end": 1143,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 1428,
                    "end": 1452,
                    "matchedPaperCorpusId": "19259545"
                },
                {
                    "start": 1812,
                    "end": 1832,
                    "matchedPaperCorpusId": "209325068"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65087890625
        },
        {
            "corpus_id": "258686693",
            "title": "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models",
            "text": "Digital and social media have become a major source of political news dissemination (Hermida et al., 2012;K\u00fcmpel et al., 2015;Hermida, 2016) with unprecedentedly high user engagement rates (Mustafaraj and Metaxas, 2011;Velasquez, 2012;Garimella et al., 2018). The volume of online discourse surrounding polarizing issues-climate change, gun control, abortion, wage gaps, death penalty, taxes, same-sex marriage, and more-has been drastically growing in the past decade (Valenzuela et al., 2012;Rainie et al., 2012;Enikolopov et al., 2019). While online political engagement promotes democratic values and diversity of perspectives, these discussions also reflect and reinforce societal biases-stereotypical generalizations about people or social groups (Devine, 1989;Bargh, 1999;Blair, 2002). Such language constitutes a major portion of large language models' (LMs) pretraining data, propagating biases into downstream models.\n\nHundreds of studies have highlighted ethical issues in NLP models (Blodgett et al., 2020a;Field et al., 2021;Kumar et al., 2022) and designed synthetic datasets (Nangia et al., 2020;Nadeem et al., 2021) or controlled experiments to measure how biases in language are encoded in learned representations (Sun et al., 2019), and how annotator errors in training data are liable to increase unfairness of NLP models (Sap et al., 2019). However, the language of polarizing political issues is particularly complex (Demszky et al., 2019), and social biases hidden in language can rarely be reduced to pre-specified stereotypical associations (Joseph and Morgan, 2020). To the best of our knowledge, no prior work has shown how to analyze the effects of naturally occurring media biases in pretraining data on language models, and subsequently on downstream tasks, and how it affects the fairness towards diverse social groups. Our study aims to fill this gap.\n\nAs a case study, we focus on the effects of media biases in pretraining data on the fairness of hate speech detection with respect to diverse social attributes, such as gender, race",
            "score": 0.6546547524055351,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 219,
                    "end": 235,
                    "matchedPaperCorpusId": "10829813"
                },
                {
                    "start": 469,
                    "end": 494,
                    "matchedPaperCorpusId": "145240180"
                },
                {
                    "start": 1231,
                    "end": 1249,
                    "matchedPaperCorpusId": "195316733"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4580078125
        },
        {
            "corpus_id": "265281304",
            "title": "Bias A-head? Analyzing Bias in Transformer-Based Language Model Attention Heads",
            "text": "Our work also has limitations that can be improved in future research. First, we focus on stereotyping bias (i.e., representational harm), which is one of the two major bias categories in PLMs (Blodgett et al., 2020). Allocational bias is not investigated in this study. Future research can study how biased heads perform in downstream NLP tasks that unfairly allocate resources or opportunities to different social groups. Second, our work relies on existing word lists to identify biased heads and assess stereotyping bias. Although those (gender or racial) word lists are curated based on theories, concepts, and methods from psychology and other social science literature, their coverage may still be limited for other protected groups such as the groups related to education, literacy, or income, or even intersectional biases (Lalor et al., 2022). Moreover, existing word lists are constructed for the English language only, which restricts the generalization of our findings on PLM stereotyping on non-English languages. Given the important role of curated stereotype word lists in quantifying NLP system's fairness, future work can study a more principled way to curate word lists for different social groups and different languages. Our proposed framework could be used as a tool to help validate lists generated in future research. For example, future paired word lists for education-based biased could use our counterfactual experiments to assess the effectiveness of the collected lists. Third, given the unique importance of self-attention in the transformer architecture, our work focuses on attention heads only. However, bias may also manifest in other components of the model, such as the input embeddings or feedforward layer connections. The complexity and multi-layer nature of Transformer models makes it difficult to pin down their precise working behavior. However, by empirically observing changes via perturbation (e.g., our counterfactual experiments), we can assemble a plausible case for what might be happening inside the network. Future studies can also look inside those components to better understanding biases in PLMs. Finally, while we focus this work on those biased heads with positive bias scores, we also observe a subset of attention heads with large negative bias scores in our results.",
            "score": 0.6502947453436172,
            "section_title": "Limitations",
            "char_start_offset": 27232,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 70
                },
                {
                    "start": 71,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 853
                },
                {
                    "start": 854,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1241
                },
                {
                    "start": 1242,
                    "end": 1341
                },
                {
                    "start": 1342,
                    "end": 1499
                },
                {
                    "start": 1500,
                    "end": 1627
                },
                {
                    "start": 1628,
                    "end": 1756
                },
                {
                    "start": 1757,
                    "end": 1879
                },
                {
                    "start": 1880,
                    "end": 2059
                },
                {
                    "start": 2060,
                    "end": 2152
                },
                {
                    "start": 2153,
                    "end": 2327
                }
            ],
            "ref_mentions": [
                {
                    "start": 193,
                    "end": 216,
                    "matchedPaperCorpusId": "218971825"
                },
                {
                    "start": 832,
                    "end": 852,
                    "matchedPaperCorpusId": "250390904"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.489501953125
        },
        {
            "corpus_id": "272720561",
            "title": "Debiasing large language models: research opportunities*",
            "text": "Large language models (LLMs 1 ), such as ChatGPT (Team OpenAI 2022), are the key to remarkable innovations and opportunities. There are examples where LLMs exhibit capabilities across various domains, including high-stakes decision applications like healthcare, criminal justice, and finance (Rudin 2019;Bommasani et al. 2021;Yogarajan et al. 2021). The underlying technologies of LLMs enable one model fits all scenarios where, with minimal or no tuning, LLMs can be adapted to specific tasks such as classification, question-answering, logical reasoning, fact retrieval, and information extraction (Liu et al. 2023). \n\nHowever, evidence suggests that LLMs come with biases and disparities, resulting in forms of discrimination and concerns about equity (Koenecke et al. 2020;Liang et al. 2021;Thiago et al. 2021;Yogarajan et al. 2023d). For example, Brown et al. (2020) found 83% of the occupation prompts generated text using GPT-3 with male identifiers, and Abid et al. (2021) show GPT-3's output has a higher violent bias against Muslims than other religious groups. Furthermore, studies show examples of misclassification of gender where the default is a male pronoun (Schiebinger 2014); a generation of hurtful stereotypes (Nozza et al. 2021); and targeted use of toxic content (Gehman et al. 2020). \n\nBias can be defined as the disparate treatment or outcomes between social groups that arise from historical and structural power imbalances (Crawford 2017;Blodgett et al. 2020; Barocas et al. 2023), and can be related to gender, social status, race, language, disability, and more. This can incorporate representational harms such as misrepresentation, stereotyping, disparate system performance, and direct and indirect discrimination (Crawford 2017;Blodgett et al. 2020; Barocas et al. 2023).",
            "score": 0.6494230450274187,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 618
                },
                {
                    "start": 621,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1306
                },
                {
                    "start": 1309,
                    "end": 1590
                },
                {
                    "start": 1591,
                    "end": 1803
                }
            ],
            "ref_mentions": [
                {
                    "start": 292,
                    "end": 304,
                    "matchedPaperCorpusId": "182656421"
                },
                {
                    "start": 755,
                    "end": 777,
                    "matchedPaperCorpusId": "214630656"
                },
                {
                    "start": 814,
                    "end": 837,
                    "matchedPaperCorpusId": "262045241"
                },
                {
                    "start": 852,
                    "end": 871,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 962,
                    "end": 980,
                    "matchedPaperCorpusId": "231603388"
                },
                {
                    "start": 1174,
                    "end": 1192,
                    "matchedPaperCorpusId": "4460326"
                },
                {
                    "start": 1230,
                    "end": 1249,
                    "matchedPaperCorpusId": "235097294"
                },
                {
                    "start": 1285,
                    "end": 1305,
                    "matchedPaperCorpusId": "221878771"
                },
                {
                    "start": 1449,
                    "end": 1464,
                    "matchedPaperCorpusId": "207986352"
                },
                {
                    "start": 1464,
                    "end": 1484,
                    "matchedPaperCorpusId": "218971825"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70703125
        },
        {
            "corpus_id": "265609311",
            "title": "Tackling Bias in Pre-trained Language Models: Current Trends and Under-represented Societies",
            "text": "The launch of OpenAI's ChatGPT in November 2022 [185] is potentially the most significant milestone in the advances of language models (LLMs1 ) and artificial intelligence (AI). It is reported that ChatGPT gained over 100 million users within the first two months of release [38]. The underlying technology of such LLMs is the key to innovations, and there are examples of LLMs exhibiting remarkable capabilities across various domains, including high-stakes decision applications like healthcare, criminal justice, and finance [20,167,213]. The capabilities of LLMs result in one model fits all scenarios where, with minimal or no tuning, LLMs can be adapted to downstream tasks such as classification, question-answering, logical reasoning, fact retrieval, and information extraction [112]. The need to train task-specific models on relatively small task-specific datasets is becoming a thing of the past [20]. \n\nHowever, introducing and using LLMs comes with biases and discrimination, resulting in concerns about equality, diversity and fairness, especially for under-represented and indigenous populations [93,108,187,212]. LLMs are trained on massive amounts of data from various sources and, as such, inherit stereotypes and misrepresentations that disproportionately affect already vulnerable and marginalized communities [13,210]. In addition to reflecting the bias in society inherited through training data, LLMs can amplify these biases [1,36]. Bias from LLMs can be related to gender, social status, race, language, disability, and more. Moreover, sources of bias can arise from various stages of the machine learning pipeline, including data collection, algorithm design, and user interactions. \n\nIn this research, we focus on \"social bias\" hereafter referred to as bias unless specified otherwise, which can be thought of as disparate treatment or outcomes between social groups that arise from historical and structural power imbalances [10,17,35]. This can incorporate representational harms such as misrepresentation, stereotyping, disparate system performance, and direct and indirect discrimination [10,17,35].",
            "score": 0.6490238255006026,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 912
                },
                {
                    "start": 915,
                    "end": 1128
                },
                {
                    "start": 1129,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1456
                },
                {
                    "start": 1457,
                    "end": 1550
                },
                {
                    "start": 1551,
                    "end": 1708
                },
                {
                    "start": 1711,
                    "end": 1964
                },
                {
                    "start": 1965,
                    "end": 2130
                }
            ],
            "ref_mentions": [
                {
                    "start": 528,
                    "end": 532,
                    "matchedPaperCorpusId": "237091588"
                },
                {
                    "start": 532,
                    "end": 536,
                    "matchedPaperCorpusId": "182656421"
                },
                {
                    "start": 907,
                    "end": 911,
                    "matchedPaperCorpusId": "237091588"
                },
                {
                    "start": 1111,
                    "end": 1115,
                    "matchedPaperCorpusId": "214630656"
                },
                {
                    "start": 1115,
                    "end": 1119,
                    "matchedPaperCorpusId": "235623756"
                },
                {
                    "start": 1330,
                    "end": 1334,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 1334,
                    "end": 1338,
                    "matchedPaperCorpusId": "258291836"
                },
                {
                    "start": 1449,
                    "end": 1452,
                    "matchedPaperCorpusId": "231603388"
                },
                {
                    "start": 1957,
                    "end": 1960,
                    "matchedPaperCorpusId": "218971825"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78564453125
        },
        {
            "corpus_id": "248366292",
            "title": "Towards an Enhanced Understanding of Bias in Pre-trained Neural Language Models: A Survey with Special Emphasis on Affective Bias",
            "text": "Standard datasets used for pre-training non-contextual [16,22,40,68] and contextual models [110] are found to exhibit bias or imbalance in various domains like gender, race, etc. In the context of large PLMs, data bias may be Pre-trained Data Bias at the initial training process of PLM or/and Fine-tuning Data Bias which is just downstream to it. Studies report that data biases can propagate and get further amplified by underlying machine learning models leading to Model Learning Bias at selfsupervised learning strategy to learn linguistic properties and Downstream Task Learning Bias at the level of the task specific fine-tuning model. \n\nModel learning bias is reflected in word representations derived from PLMs and produce Representation Bias. Non-contextual word embeddings such as word2vec, GloVe, etc., are known to comprise representational biases across gender [16,22], racial [68] and ethnic groups [40]. It has been clearly unveiled that Fig. 2. Bias in large pre-trained language models these embeddings relate word representations of professions like 'nurse', 'receptionist' and 'homemaker' to women and 'doctor', 'philosopher' and 'computer programmer' to men [16]. Further, they place the representation of words like 'wisdom' close to representation of 'grandfather' than 'grandmother', and encode association of popular African Americans names with unpleasant phrases [22], etc. Examination of bias in contextualized embeddings reveal that they also exhibit bias like conventional embeddings [18,126]. For example, BERT is found to encode human-like biases [55]. Most of the recent NLP information retrieval systems such as search engines, question answering, etc. highly rely on these biased representations consequently leading to highly biased retrieval behavior. Similarly, use of the large PLM GPT-3 in language generation shows religion bias analogizing 'Muslims' to 'Terrorists' [2].",
            "score": 0.6482350189886047,
            "section_title": "Bias in Pre-trained Language Models",
            "char_start_offset": 14822,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 642
                },
                {
                    "start": 645,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1184
                },
                {
                    "start": 1185,
                    "end": 1523
                },
                {
                    "start": 1524,
                    "end": 1584
                },
                {
                    "start": 1585,
                    "end": 1788
                },
                {
                    "start": 1789,
                    "end": 1912
                }
            ],
            "ref_mentions": [
                {
                    "start": 55,
                    "end": 59,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 59,
                    "end": 62,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 62,
                    "end": 65,
                    "matchedPaperCorpusId": "4930886"
                },
                {
                    "start": 65,
                    "end": 68,
                    "matchedPaperCorpusId": "102350941"
                },
                {
                    "start": 91,
                    "end": 96,
                    "matchedPaperCorpusId": "202781363"
                },
                {
                    "start": 875,
                    "end": 879,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 879,
                    "end": 882,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 891,
                    "end": 895,
                    "matchedPaperCorpusId": "102350941"
                },
                {
                    "start": 914,
                    "end": 918,
                    "matchedPaperCorpusId": "4930886"
                },
                {
                    "start": 1179,
                    "end": 1183,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 1390,
                    "end": 1394,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 1514,
                    "end": 1518,
                    "matchedPaperCorpusId": "102352788"
                },
                {
                    "start": 1518,
                    "end": 1522,
                    "matchedPaperCorpusId": "102352962"
                },
                {
                    "start": 1579,
                    "end": 1583,
                    "matchedPaperCorpusId": "190000105"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70849609375
        },
        {
            "corpus_id": "276235453",
            "title": "Understanding and Mitigating the Bias Inheritance in LLM-based Data Augmentation on Downstream Tasks",
            "text": "In this paper, we take the first step toward this unexplored problem, demystifying the social bias in augmentation data, understanding its effects on downstream tasks during post-training, and then mitigating such (malignant) effects. Notably, this is significantly different from existing research on bias evaluation in LLM outputs; we focus on evaluating the results on downstream bias related classification and generation tasks using models fine-tuned on LLM-generated biased data. Our study aims to answer the following key questions: 1) Understanding: How does the social bias in augmentation data influence on downstream performance? 2) Analysis: Why does such influence happen? 3) Mitigating: How to mitigate the negative influence of social bias from the augmented data during post-training? \n\n\u2022 Understanding (section 4): We first design a multi-dimensional bias generation framework, covering 6 distinct biases types and 3 key dimensions (Contextual vs. Contrastive, Single vs. Intersectional, Explicit vs. Implicit) for comprehensive biased data generation. We combine these biased data with real unbiased data, and controls the bias ratios as {0%, 5%, 10%, 20%, 50%}. We investigate the impact of gender and cultural biases on downstream tasks, including both bias directly-related and indirectly-related classification tasks, \n\nas well as open-ended generation tasks. The findings are nuanced. For instance, lower bias ratios (10%, 20%) can improve the performance of bias indirectly-related classification tasks, whereas bias always leads to inheritance on directly-related classification and all generation tasks, causing performance degradation particularly for minority groups. This results in an increased performance gap between the minority and majority groups. Moreover, bias inheritance gets amplified over iterative tuning and eventually extends to the majority group, ultimately causing performance degradation across all groups. Among all types, contrastive explicit and contextual implicit biases exhibit the most severe impacts on downstream tasks. \n\n\u2022 Analysis (section 5): To further understand the performance decline, performance gap across groups, and performance difference brought by bias inheritance, we analyze LLM answers for value questions with real human responses, the distribution of data generation across groups, and the embedding distribution of synthetic versus real data.",
            "score": 0.647054988721025,
            "section_title": "Single Intersectional",
            "char_start_offset": 3581,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 800
                },
                {
                    "start": 803,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1339
                },
                {
                    "start": 1342,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1695
                },
                {
                    "start": 1696,
                    "end": 1782
                },
                {
                    "start": 1783,
                    "end": 1954
                },
                {
                    "start": 1955,
                    "end": 2076
                },
                {
                    "start": 2079,
                    "end": 2419
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79150390625
        },
        {
            "corpus_id": "258823009",
            "title": "Comparing Biases and the Impact of Multilingual Training across Multiple Languages",
            "text": "Studies in bias and fairness in natural language processing have primarily examined social biases within a single language and/or across few attributes (e.g. gender, race). However, biases can manifest differently across various languages for individual attributes. As a result, it is critical to examine biases within each language and attribute. Of equal importance is to study how these biases compare across languages and how the biases are affected when training a model on multilingual data versus monolingual data. We present a bias analysis across Italian, Chinese, English, Hebrew, and Spanish on the downstream sentiment analysis task to observe whether specific demographics are viewed more positively. We study bias similarities and differences across these languages and investigate the impact of multilingual vs. monolingual training data. We adapt existing sentiment bias templates in English to Italian, Chinese, Hebrew, and Spanish for four attributes: race, religion, nationality, and gender. Our results reveal similarities in bias expression such as favoritism of groups that are dominant in each language's culture (e.g. majority religions and nationalities). Additionally, we find an increased variation in predictions across protected groups, indicating bias amplification, after multilingual finetuning in comparison to multilingual pretraining.",
            "score": 0.64656716094229,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.767578125
        },
        {
            "corpus_id": "259859119",
            "title": "With Prejudice to None: A Few-Shot, Multilingual Transfer Learning Approach to Detect Social Bias in Low Resource Languages",
            "text": "The presence of social bias in language representations is mostly caused by the undesired and skewed associations within the training data. Given the growing social effect of NLP applications, studying these undesired relationships is paramount (Bender and Friedman, 2018;Crawford, 2017). The initial attempts to tackle this issue focused on measuring and mitigating gender biases from word embeddings (Bolukbasi et al., 2016;Caliskan et al., 2017;Zhao et al., 2017;Garg et al., 2018;Sun et al., 2019). Additionally, multiple works have explored solutions to identify race, and religion bias in word embeddings (Manzini et al., 2019). Many subsequent works (May et al., 2019;Zhao et al., 2019;Kurita et al., 2019) have also focused on contextualized language representation from models like BERT for bias detection. \n\nMore recently, many datasets (Nangia et al., 2020;Sap et al., 2020) have been created to measure social biases like gender, race, profession, religion, age, and so on in language models. Blodgett et al. (2021) has reported that these datasets lack clear definitions and have ambiguities and inconsistencies in annotations. Researchers have also investigated the presence of biases in models trained for various NLP tasks like machine translation (Stanovsky et al., 2019;Savoldi et al., 2021), question answering (Li et al., 2020), and coreference resolution (Webster et al., 2018). \n\nThere have been a lot of notable efforts towards identifying data bias in the problem of hate speech and offensive languages detection (Waseem and Hovy, 2016;Davidson et al., 2019;Sap et al., 2019;Mozafari et al., 2020). Borkan et al. (2019) has discussed unintended bias in hate speech detection models for identity terms like Islam, lesbian, bisexual, etc.",
            "score": 0.6453424134332734,
            "section_title": "Related Works",
            "char_start_offset": 3645,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 815
                },
                {
                    "start": 818,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1399
                },
                {
                    "start": 1402,
                    "end": 1622
                },
                {
                    "start": 1623,
                    "end": 1760
                }
            ],
            "ref_mentions": [
                {
                    "start": 426,
                    "end": 448,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 448,
                    "end": 466,
                    "matchedPaperCorpusId": "1389483"
                },
                {
                    "start": 466,
                    "end": 484,
                    "matchedPaperCorpusId": "4930886"
                },
                {
                    "start": 484,
                    "end": 501,
                    "matchedPaperCorpusId": "195316733"
                },
                {
                    "start": 611,
                    "end": 633,
                    "matchedPaperCorpusId": "102350941"
                },
                {
                    "start": 657,
                    "end": 675,
                    "matchedPaperCorpusId": "85518027"
                },
                {
                    "start": 847,
                    "end": 868,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 1005,
                    "end": 1027,
                    "matchedPaperCorpusId": "236460302"
                },
                {
                    "start": 1264,
                    "end": 1288,
                    "matchedPaperCorpusId": "173991101"
                },
                {
                    "start": 1537,
                    "end": 1560,
                    "matchedPaperCorpusId": "1721388"
                },
                {
                    "start": 1582,
                    "end": 1599,
                    "matchedPaperCorpusId": "196211238"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.548828125
        },
        {
            "corpus_id": "267627928",
            "title": "MAFIA: Multi-Adapter Fused Inclusive Language Models",
            "text": "Pretrained Language Models (PLMs) are growing in power and prominence across numerous NLP tasks (Wang et al., 2023;Ahuja et al., 2023). Their reach has expanded beyond academia, reaching general users through services like code assistance and chatbots (Li et al., 2023;K\u00f6pf et al., 2023). Despite the extraordinary performance of these models on their respective tasks, several works have identified the harmful social biases picked up by these models as an artifact of their pretraining on \u2020 Equal Contribution \u2021 Work done when the author was at Microsoft web-scale corpus consisting of unmoderated usergenerated content (Manzini et al., 2019;Webster et al., 2020;Nadeem et al., 2021, inter alia). \n\nWhile most previous works focus on (binary) gender biases, other societal biases, such as race and religion, are less studied in the context of PLMs. Moreover, these biases are often intertwined with each other, creating complex and nuanced forms of discrimination. We define intersectional biases as the biases that arise from the combination of different attributes, such as gender, race, and religion. In this work, we focus on building debiasing techniques that can model and mitigate gender (including non-binary), race, religion, profession, and intersectional biases, which are often ignored in previous works. \n\nThe community has developed a gamut of methods to measure and mitigate biases in LLMs (Bordia and Bowman, 2019;Liang et al., 2020;Ravfogel et al., 2020;Webster et al., 2020;Lauscher et al., 2021;Smith et al., 2022;Kumar et al., 2023). The majority of these methods finetune all the parameters of a language model to debias it towards a particular bias dimension such as gender or race, and the escalating size of PLMs can pose computational challenges, particularly for smaller academic labs or enterprises. While some methods (Schick et al., 2021;Yang et al., 2023) do not alter a model's internal representations or its parameters.",
            "score": 0.6453424134332734,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 698
                },
                {
                    "start": 701,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1318
                },
                {
                    "start": 1321,
                    "end": 1555
                },
                {
                    "start": 1556,
                    "end": 1828
                },
                {
                    "start": 1829,
                    "end": 1954
                }
            ],
            "ref_mentions": [
                {
                    "start": 115,
                    "end": 134,
                    "matchedPaperCorpusId": "257663467"
                },
                {
                    "start": 622,
                    "end": 644,
                    "matchedPaperCorpusId": "102350941"
                },
                {
                    "start": 1407,
                    "end": 1432,
                    "matchedPaperCorpusId": "102352788"
                },
                {
                    "start": 1432,
                    "end": 1451,
                    "matchedPaperCorpusId": "207996257"
                },
                {
                    "start": 1451,
                    "end": 1473,
                    "matchedPaperCorpusId": "215786522"
                },
                {
                    "start": 1494,
                    "end": 1516,
                    "matchedPaperCorpusId": "237440429"
                },
                {
                    "start": 1516,
                    "end": 1535,
                    "matchedPaperCorpusId": "253224433"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55615234375
        },
        {
            "corpus_id": "273901494",
            "title": "Decoding Ableism in Large Language Models: An Intersectional Approach",
            "text": "With the pervasive use of large language models (LLMs) across various domains, addressing the inherent ableist biases within these models requires more attention and resolution. This paper examines ableism in three LLMs (GPT-3.5, GPT-4, and Llama 3) by analyzing the intersection of disability with two additional social categories: gender and social class. Utilizing two task-specific prompts, we generated and analyzed text outputs with two metrics, VADER and regard, to evaluate sentiment and social perception biases within the responses. Our results indicate a marked improvement in bias mitigation from GPT-3.5 to GPT-4, with the latter demonstrating more positive sentiments overall, while Llama 3 showed comparatively weaker performance. Additionally, our findings underscore the complexity of intersectional biases: These biases are shaped by the combined effects of disability, gender, and class, which alter the expression and perception of ableism in LLM outputs. This research highlights the necessity for more nuanced and inclusive bias mitigation strategies in AI development, contributing to the ongoing dialogue on ethical AI practices.",
            "score": 0.6447151758405678,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79443359375
        },
        {
            "corpus_id": "248780439",
            "title": "Upstream Mitigation Is \n Not\n All You Need: Testing the Bias Transfer Hypothesis in Pre-Trained Language Models",
            "text": "Large language models (LLMs) and other massively pre-trained \"foundation\" models are powerful tools for task-specific machine learning (Bommasani et al., 2021). Models pre-trained by wellresourced organizations can easily adapt to a wide variety of downstream tasks in a process called finetuning. But massive pre-training datasets and increasingly homogeneous model design come with well-known, immediate social risks beyond the financial and environmental costs (Strubell et al., 2019;Bender et al., 2021). \n\nTransformer-based LLMs like BERT and GPT-3 contain quantifiable intrinsic social biases encoded in their embedding spaces (Goldfarb-Tarrant et al., 2021). These intrinsic biases are typically associated with representational harms, including stereotyping and denigration (Barocas et al., 2017;Blodgett et al., 2020;Bender et al., 2021). Separately, many studies document the extrinsic harms of the downstream (fine-tuned & task-specific) ap- plications of fine-tuned LLMs, including discriminatory medical diagnoses (Zhang et al., 2020), overreliance on binary gender for coreference resolution (Cao and Daum\u00e9, 2021), the re-inforcement of traditional gender roles in part-of-speech tagging (Garimella et al., 2019), toxic text generation (Gehman et al., 2020), and censorship of inclusive language and AAVE (Blodgett and O'Connor, 2017;Blodgett et al., 2018;Park et al., 2018;Sap et al., 2019). \n\nDespite these risks, no research has investigated the extent to which downstream systems inherit social biases from pre-trained models. 1 Many stud-ies warn that increasing intrinsic bias upstream may lead to an increased risk of downstream harms (Bolukbasi et al., 2016;Caliskan et al., 2017).",
            "score": 0.6441634958220455,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 508
                },
                {
                    "start": 511,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 1406
                },
                {
                    "start": 1409,
                    "end": 1703
                }
            ],
            "ref_mentions": [
                {
                    "start": 464,
                    "end": 487,
                    "matchedPaperCorpusId": "174802812"
                },
                {
                    "start": 487,
                    "end": 507,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 633,
                    "end": 664,
                    "matchedPaperCorpusId": "229923772"
                },
                {
                    "start": 804,
                    "end": 826,
                    "matchedPaperCorpusId": "218971825"
                },
                {
                    "start": 826,
                    "end": 846,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 1027,
                    "end": 1047,
                    "matchedPaperCorpusId": "214590570"
                },
                {
                    "start": 1106,
                    "end": 1127,
                    "matchedPaperCorpusId": "237416665"
                },
                {
                    "start": 1202,
                    "end": 1226,
                    "matchedPaperCorpusId": "196181460"
                },
                {
                    "start": 1250,
                    "end": 1271,
                    "matchedPaperCorpusId": "221878771"
                },
                {
                    "start": 1348,
                    "end": 1370,
                    "matchedPaperCorpusId": "51870345"
                },
                {
                    "start": 1370,
                    "end": 1388,
                    "matchedPaperCorpusId": "52070035"
                },
                {
                    "start": 1388,
                    "end": 1405,
                    "matchedPaperCorpusId": "196211238"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55126953125
        },
        {
            "corpus_id": "271916146",
            "title": "Unboxing Occupational Bias: Grounded Debiasing of LLMs with U.S. Labor Data",
            "text": "Large Language Models (LLMs) are prone to inheriting and amplifying societal biases embedded within their training data, potentially reinforcing harmful stereotypes related to gender, occupation, and other sensitive categories. This issue becomes particularly problematic as biased LLMs can have far-reaching consequences, leading to unfair practices and exacerbating social inequalities across various domains, such as recruitment, online content moderation, or even the criminal justice system. Although prior research has focused on detecting bias in LLMs using specialized datasets designed to highlight intrinsic biases, there has been a notable lack of investigation into how these findings correlate with authoritative datasets, such as those from the U.S. National Bureau of Labor Statistics (NBLS). To address this gap, we conduct empirical research that evaluates LLMs in a ``bias-out-of-the-box\"setting, analyzing how the generated outputs compare with the distributions found in NBLS data. Furthermore, we propose a straightforward yet effective debiasing mechanism that directly incorporates NBLS instances to mitigate bias within LLMs. Our study spans seven different LLMs, including instructable, base, and mixture-of-expert models, and reveals significant levels of bias that are often overlooked by existing bias detection techniques. Importantly, our debiasing method, which does not rely on external datasets, demonstrates a substantial reduction in bias scores, highlighting the efficacy of our approach in creating fairer and more reliable LLMs.",
            "score": 0.6439667172849652,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.642578125
        },
        {
            "corpus_id": "271571036",
            "title": "Understanding the Interplay of Scale, Data, and Bias in Language Models: A Case Study with BERT",
            "text": "In the context of large language models-which power most of modern NLP-recent work has documented negative associations for people with disabilities (Hutchinson et al. 2020), anti-Muslim bias (Abid, Farooqi, and Zou 2021), and a general propensity to generate toxic text (Gehman et al. 2020). There have also been efforts to construct benchmarks that can yield repeatable measurements of bias across many different language models. This includes benchmarks such as WinoBias for coreference resolution (Zhao et al. 2018), BBQ and UNQOVER for question-answering (Parrish et al. 2021;Li et al. 2020), BBNLI for natural language inference (Baldini et al. 2023), StereoSet for measuring stereotypical associations (Nadeem, Bethke, and Reddy 2020), among others. Further, large benchmarking efforts such as BIG-bench (Srivastava et al. 2022) have been able to provide insights into the relationship between model scale and performance on bias benchmarks, which is one of the objectives of our study. We now know from Srivastava et al. ( 2022) that for auto-regressive models, bias (as measured via UNQOVER, BBQ etc.) typically increases in ambiguous prompts, and that it can decrease for narrow, unambiguous prompts. We similarly study the relation of bias with scale, but in the context of MLM models, which have distinct upstream and downstream applications, and with an added focus on the pre-training dataset used. Closest to our study is Steed et al. (2022)'s work on upstream and downstream biases for MLMs, in which they investigate the bias transfer hypothesis-can upstream debiasing methods improve disparities in downstream performance? They find that upstream mitigation does little to address downstream biases, and that downstream disparities are better explained by biases in the fine-tuning data. \n\nIn parallel, empirical scaling laws related to LLM performance have been the subject of extensive investigation in recent research (Hestness et al. 2017;Kaplan et al. 2020).",
            "score": 0.6429621881977491,
            "section_title": "Related Work",
            "char_start_offset": 10610,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1210
                },
                {
                    "start": 1211,
                    "end": 1412
                },
                {
                    "start": 1413,
                    "end": 1640
                },
                {
                    "start": 1641,
                    "end": 1805
                },
                {
                    "start": 1808,
                    "end": 1981
                }
            ],
            "ref_mentions": [
                {
                    "start": 271,
                    "end": 291,
                    "matchedPaperCorpusId": "221878771"
                },
                {
                    "start": 501,
                    "end": 519,
                    "matchedPaperCorpusId": "214590570"
                },
                {
                    "start": 581,
                    "end": 596,
                    "matchedPaperCorpusId": "59599752"
                },
                {
                    "start": 709,
                    "end": 741,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 1437,
                    "end": 1456,
                    "matchedPaperCorpusId": "248780439"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.611328125
        },
        {
            "corpus_id": "259203321",
            "title": "Gender Bias in Transformer Models: A comprehensive survey",
            "text": "The issue of gender bias in various NLP tasks is a matter of significant concern, and it is essential to categorize and summarize the different forms of bias that can arise. Researchers have identified three primary categories of bias: Denigration (A1), Stereotyping (A2), and Under-representation (A3). Denigration involves the usage of racial, ethnic, or religious slurs, which can often be observed as a prevalent method of cyberbullying. This type of bias manifests as derogatory language aimed at demeaning specific groups. Stereotyping, on the other hand, refers to individuals' cognitive representation of a particular social group. In NLP tasks, stereotyping can manifest through word embeddings and large-scale language models, where certain biases are learned and perpetuated in the resulting representations. Under-representation pertains to the absence of identifiable group members from representative bodies and well-being indicators in proportion to their population's size. This type of bias highlights the disparities in visibility and inclusivity within various domains. Researchers have extensively studied these categories of bias to better understand their nature and mitigate their negative impact on society. By recognizing and categorizing different types of bias, researchers and practitioners can develop effective strategies and techniques to address gender bias in NLP. The above-mentioned NLP tasks can be represented and examined using the terms specified in Table I. This categorization facilitates a clearer understanding of how gender bias can manifest in different NLP applications, thereby guiding efforts to combat and mitigate bias effectively. By comprehensively analyzing and addressing the various types of bias, researchers and practitioners can contribute to the development of more equitable and fair NLP systems and applications, promoting inclusivity and fostering unbiased linguistic representations.",
            "score": 0.6421407892172782,
            "section_title": "II. REPRESENTATION OF GENDER BIAS",
            "char_start_offset": 12151,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1497
                },
                {
                    "start": 1498,
                    "end": 1681
                },
                {
                    "start": 1682,
                    "end": 1946
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83544921875
        },
        {
            "corpus_id": "272826949",
            "title": "Evaluating Gender, Racial, and Age Biases in Large Language Models: A Comparative Analysis of Occupational and Crime Scenarios",
            "text": "Large Language Models (LLMs) have transformed human-computer interaction, exhibiting unprecedented capabilities in natural language processing, communication, and content generation. However, their widespread adoption is impeded by a fundamental challenge: bias. Bias in LLMs is not merely a technical issue but a broader societal concern with significant ethical and practical implications [5]. Enterprises seeking to integrate LLMs into various applications must contend with the risks posed by biased outputs, which can reinforce stereotypes and propagate misinformation. \n\nBias in LLMs manifests in multiple forms, including racial, gender, and cultural stereotypes, often perpetuating systemic inequalities. These biases have tangible consequences; for instance, in 2018, Amazon discontinued an AI-driven recruiting tool after discovering it systematically downgraded resumes containing the term \"women's,\" reflecting an inherent bias in the training data that favored male candidates [1]. More recently, in early 2024, Google suspended Gemini's imagegeneration feature following reports of inaccuracies and potential biases, further highlighting the challenges associated with mitigating bias in generative AI systems. \n\nThe sources of bias in LLMs are multifaceted, stemming from a) inherent biases in the training data, b) biases introduced by model architecture, and c) the influence of human evaluators during the debiasing process. In response to the rising need to address bias holistically, researchers have adopted multiple ways to evaluate and mitigate bias in LLMs (TABLE I), such as curating datasets with comprehensive data for model training and implementing different debiasing approaches. The datasets used to train these models, such as Winogender, Winobias [2], BOLD (Bias in Open-ended Language Generation Dataset) [3], and the BBQ benchmark (Bias Benchmark for QA-Question Answering) [4], have limitations in representing the full spectrum of real-world language and societal biases. Similarly, existing debiasing techniques [5] often depend on external knowledge with potential bias or annotated non-biased samples, which are not always available or practical to obtain.",
            "score": 0.6418416174530104,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 574
                },
                {
                    "start": 577,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1224
                },
                {
                    "start": 1227,
                    "end": 1442
                },
                {
                    "start": 1443,
                    "end": 1709
                },
                {
                    "start": 1710,
                    "end": 2008
                },
                {
                    "start": 2009,
                    "end": 2196
                }
            ],
            "ref_mentions": [
                {
                    "start": 391,
                    "end": 394,
                    "matchedPaperCorpusId": "261530629"
                },
                {
                    "start": 990,
                    "end": 993,
                    "matchedPaperCorpusId": "261276445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82080078125
        },
        {
            "corpus_id": "268554277",
            "title": "From Perils to Possibilities: Understanding how Human (and AI) Biases affect Online Fora",
            "text": "The rise of Large Language Models (LLMs) has prompted the need to assess how AI performance aligns with human cognitive functions, ranging from decision-making to language comprehension and generation [64].This exploration is crucial for understanding the expressive power of LLMs, recognizing their limitations, and establishing boundaries around human reliance on LLM-generated content.In the context of generative LLMs, the risk of blindly trusting AI output is considerably high, particularly when these models fail to recognize the human-like bias they inherit [4].Despite differing perspectives on the nature of intelligence demonstrated by LLMs, the harmful biases exhibited by LLMs has emerged as a ubiquitous concern.Biases, stemming from the under-representation of data to the over-representation of societal stereotypes in training sets, could potentially worsen discrimination and societal inequalities, posing significant challenges in user interactions with LLMs.At the core of this phenomenon is the causal embedding hypothesis [22], which states that biases can be learned from exposure to language statistics that reflect social stereotypes.The understanding that many biases originate in language has led to a surge in research investigating biases in language models.For example, studies have found that even the most advanced models posses significant racial and gender biases [59,62], Such studies demonstrate the need to understand the nature of biases in LLMs as they relate to human biases, in order to mitigate the potentially harmful effects of widespread LLM interaction.\n\nIn the following, we present a brief review of emerging lines of research with the aim to better understand the nature of LLM biases, and in particular, whether LLMs can easily replicate humanspecific cognitive functions.These studies adopt the tools of network science e.g., analyzing the underlying network structure of word representations, or employing tools from opinion dynamics.\n\nRepresentation Bias.Semantic representation refers to how the meaning of words is represented in a structured way.The biased outputs produced by LLMs originate in the biased semantic representations they possess.These biased representations can be investigated using several approaches, depending on the architecture of the language model.",
            "score": 0.6400683659783932,
            "section_title": "FROM HUMAN BIASES TO LLM ONES",
            "char_start_offset": 24110,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 206
                },
                {
                    "start": 206,
                    "end": 388
                },
                {
                    "start": 388,
                    "end": 570
                },
                {
                    "start": 570,
                    "end": 726
                },
                {
                    "start": 726,
                    "end": 978
                },
                {
                    "start": 978,
                    "end": 1159
                },
                {
                    "start": 1159,
                    "end": 1287
                },
                {
                    "start": 1287,
                    "end": 1599
                },
                {
                    "start": 1601,
                    "end": 1822
                },
                {
                    "start": 1822,
                    "end": 1986
                },
                {
                    "start": 1988,
                    "end": 2008
                },
                {
                    "start": 2008,
                    "end": 2102
                },
                {
                    "start": 2102,
                    "end": 2200
                },
                {
                    "start": 2200,
                    "end": 2327
                }
            ],
            "ref_mentions": [
                {
                    "start": 201,
                    "end": 205,
                    "matchedPaperCorpusId": "253107905"
                },
                {
                    "start": 566,
                    "end": 569,
                    "matchedPaperCorpusId": "264517917"
                },
                {
                    "start": 1398,
                    "end": 1402,
                    "matchedPaperCorpusId": "235097208"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71923828125
        },
        {
            "corpus_id": "273374949",
            "title": "With a Grain of SALT: Are LLMs Fair Across Social Dimensions?",
            "text": "Culture and identity are complex concepts encompassing elements such as gender, race, religion, sexual orientation, caste, and occupation, among others (McCall, 2005). Recent studies have increasingly focused on examining the cultural alignment and safety of LLMs (Sheng et al. (2021); Gupta et al. (2024); Sheng et al. (2019)), aiming to explore how these models encode and express biases across these various dimensions. LLMs have been shown to make moral judgments (Schramowski et al., 2022), express opinions on global issues (Durmus et al., 2024), and perpetuate stereotypes related to identity (Cao et al., 2022). While the research scope is broad, our study focuses specifically on biases relating to gender, race/ethnicity, and religion. \n\nGender bias in NLP has received considerable attention. Bolukbasi et al. (2016) used vector arithmetic on embeddings trained from Google News to highlight stereotypes linking certain professions (e.g., \"receptionist\" or \"homemaker\") to women. Jentzsch and Turan (2022) investigated gender biases in BERT models used for movie classification, revealing substantial bias across model variants and introducing metrics to quantify these biases by measuring sentiment differences between male and female samples. Wan et al. (2023) explored systematic gender bias in open-ended text generation, focusing on professional documents like reference letters and analyzing biases through both language style and lexical content. Similarly, Kotek et al. (2023) showed that LLMs often associate occupations with gender based on public perception rather than factual statistics, and that these models can rationalize incorrect associations due to imperfect training data. \n\nRace and religion-related biases are also widely studied, with many works examining how these biases work in union. To the best of our knowledge, Honnavalli et al. (2022) coined the term of a \"compounded bias\", when discussing biases related to age and gender in tandem. Such a compounded bias of race and religion combining makes it harder to disentangle the sources of bias.",
            "score": 0.6396533208623122,
            "section_title": "Related Work",
            "char_start_offset": 3413,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 745
                },
                {
                    "start": 748,
                    "end": 803
                },
                {
                    "start": 804,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1704
                },
                {
                    "start": 1707,
                    "end": 1822
                },
                {
                    "start": 1823,
                    "end": 1977
                },
                {
                    "start": 1978,
                    "end": 2083
                }
            ],
            "ref_mentions": [
                {
                    "start": 152,
                    "end": 166,
                    "matchedPaperCorpusId": "16690122"
                },
                {
                    "start": 264,
                    "end": 284,
                    "matchedPaperCorpusId": "234337004"
                },
                {
                    "start": 286,
                    "end": 305,
                    "matchedPaperCorpusId": "259164882"
                },
                {
                    "start": 307,
                    "end": 326,
                    "matchedPaperCorpusId": "202537041"
                },
                {
                    "start": 991,
                    "end": 1016,
                    "matchedPaperCorpusId": "250391069"
                },
                {
                    "start": 1476,
                    "end": 1495,
                    "matchedPaperCorpusId": "261276445"
                },
                {
                    "start": 1853,
                    "end": 1877,
                    "matchedPaperCorpusId": "248965251"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4951171875
        },
        {
            "corpus_id": "258947155",
            "title": "Finspector: A Human-Centered Visual Inspection Tool for Exploring and Comparing Biases among Foundation Models",
            "text": "Recently, pre-trained large language models (LLMs), including 'foundation models,' that are trained on large amounts of data have shown striking performances in a variety of natural language processing (NLP) tasks such as language translation, text classification, and summarization. Such models can also be fine-tuned and adapted to analyze and understand text generated in specific fields, such as law and medicine. Despite their usefulness, there is a growing concern that the foundation models inherently reflect human biases, which might have originated from their large training corpora (Shah et al., 2020;Liang et al., 2021;Weidinger et al., 2021;Garrido-Mu\u00f1oz et al., 2021).\n\nThese social biases include stereotyping and negative generalizations of different social groups and communities, which could have been present in their training corpora (Liang et al., 2021;Garrido-Mu\u00f1oz et al., 2021). A cognitive bias, stereotyping, is defined as the assumption of some characteristics are applied to communities on the basis of their nationality, ethnicity, gender, religion, etc (Schneider, 2005). Relatedly, Fairness (\"zero-bias\"), in the context of NLP and machine learning is defined as being not discriminatory according to such characteristics (Garrido-Mu\u00f1oz et al., 2021). Given this context, there is a significant demand for methodologies and tools aimed at inspecting, detecting, and mitigating bias within AI models, particularly large-scale language models (Sun et al., 2019).\n\nA previous work (Kwon and Mihindukulasooriya, 2022) demonstrated that computing the pseudo-log-likelihood scores of paraphrased sentences using different foundation models can be used to test the consistency and robustness of the models, which can lead to a better understanding of the fairness of LLMs. Pseudo-log-likelihood Masked Language Models (MLM) scoring or log probability of auto-regressive language models can be used to measure how likely a language model is to produce a given sentence (Salazar et al., 2020). It can also be used to measure the likelihood of multiple variants of a sentence, such as stereotypical and non-stereotypical ones, in order to determine which one the model prefers",
            "score": 0.6395109016995131,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 593,
                    "end": 612,
                    "matchedPaperCorpusId": "209461005"
                },
                {
                    "start": 612,
                    "end": 631,
                    "matchedPaperCorpusId": "235623756"
                },
                {
                    "start": 854,
                    "end": 874,
                    "matchedPaperCorpusId": "235623756"
                },
                {
                    "start": 1472,
                    "end": 1490,
                    "matchedPaperCorpusId": "195316733"
                },
                {
                    "start": 1509,
                    "end": 1544,
                    "matchedPaperCorpusId": "250390988"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62158203125
        },
        {
            "corpus_id": "275336873",
            "title": "Explicit vs. Implicit: Investigating Social Bias in Large Language Models through Self-Reflection",
            "text": "Prior work has extensively investigated bias in Large Language Models. Some research has explored explicit bias in LLMs, typically measuring bias by directly including specific target objects in prompts or templates (Abid et al., 2021;Kirk et al., 2021;Kotek et al., 2023). Abid et al. (Abid et al., 2021) found that GPT-3 (Brown et al., 2020) tends to generate text with violence when given Muslim-related prompts. Other studies examine models' implicit biases (Caliskan et al., 2017;Zhao et al., 2024). Marked Personas (Cheng et al., 2023), through analyzing distributional differences in model-generated words across social groups, revealed that apparently positive model outputs contained implicit negative perceptions of Black women. Bai et al. (Bai et al., 2024) developed a methodology based on the Implicit Association Test (IAT) to quantify implicit biases embedded in LLMs. \n\nBeyond these bias detection methods, studies have also focused on investigating the factors influencing social bias in LLMs. Inspired by the Scaling Laws (Kaplan et al., 2020), these studies primarily examine the relation between bias and various factors such as model size and training data composition. Furthermore, with the widespread use of Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022) in LLM training, investigations have emerged to understand how alignment methods affect the bias in LLMs. Ganguli et al. (Ganguli et al., 2023) focused on two key variables: model scale and RLHF training steps, examining their impact on models' Moral Self-Correction capabilities. Their large-scale experiments revealed that within certain ranges, both model scale and RLHF training steps showed significant positive correlations with models' ability to self-debias. Through a series of experiments, Ali et al. (Ali et al., 2024) found that increasing model size may lead to higher levels of bias.",
            "score": 0.6390388145126742,
            "section_title": "Bias Analysis in Large Language Models",
            "char_start_offset": 8311,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 70
                },
                {
                    "start": 71,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 883
                },
                {
                    "start": 886,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1190
                },
                {
                    "start": 1191,
                    "end": 1408
                },
                {
                    "start": 1409,
                    "end": 1583
                },
                {
                    "start": 1584,
                    "end": 1769
                },
                {
                    "start": 1770,
                    "end": 1900
                }
            ],
            "ref_mentions": [
                {
                    "start": 216,
                    "end": 235,
                    "matchedPaperCorpusId": "231603388"
                },
                {
                    "start": 235,
                    "end": 253,
                    "matchedPaperCorpusId": "236950797"
                },
                {
                    "start": 253,
                    "end": 272,
                    "matchedPaperCorpusId": "261276445"
                },
                {
                    "start": 286,
                    "end": 305,
                    "matchedPaperCorpusId": "231603388"
                },
                {
                    "start": 462,
                    "end": 485,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 485,
                    "end": 503,
                    "matchedPaperCorpusId": "269804264"
                },
                {
                    "start": 521,
                    "end": 541,
                    "matchedPaperCorpusId": "258960243"
                },
                {
                    "start": 1281,
                    "end": 1302,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70751953125
        },
        {
            "corpus_id": "268553687",
            "title": "Locating and Mitigating Gender Bias in Large Language Models",
            "text": "In recent years, sophisticated artificial intelligence models, notably exemplified by ChatGPT [28,31,33], are specially designed to excel in comprehending complicated natural language and generating human-like text.However, as these models become increasingly integrated across various sectors [6,14], the inherent biases within these systems has become a subject of growing concern.\n\nBias refers to the existence of consistent inaccuracies, misattributions, or erroneous perceptions leading to a preference for specific groups or concepts, the reinforcement of stereotypes, or the formation of flawed conclusions derived from established patterns, it now profoundly impact users and society at large, underscoring the urgent need for a comprehensive examination and mitigation of these issues [9,26].\n\nCurrent research on bias primarily concentrates on two aspects: identification and location of bias [3,29], which aims to understand the origins and manifestations of biases; and mitigation of bias [2,37], which seeks to reduce or eliminate inherent biases.Due to the complex nature and extensive scope of bias manifestations, contemporary studies often focus on specific aspects, such as locating gender bias [29] or mitigating racial bias [15].\n\nCaliskan et al. [3] adopts the core concept of the Implicit Association Test (IAT), measures gender bias by assessing the strength of conceptual associations, using the Word Embedding Association Test (WEAT) to evaluate bias in word embeddings.Caliskan et al. confirm that biases identified through IAT tests are present in word embeddings.Garg et al. [11] show that biases within word embeddings serve as indicators of societal changes, including the fluctuation of female participation in professional fields.Extending this further, May et al. [17] developed the Sentence Encoder Association Test (SEAT) from WEAT, extending its ability to evaluate bias in sentences.Jesse Vig et al. [29] introduces a methodology for interpreting neural language models to analyze how gender bias effects are mediated via specific model components in Transformer-based language models.\n\nNumerous methods aimed at mitigating model biases have been successively proposed in recent years.",
            "score": 0.6361867083898365,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 215
                },
                {
                    "start": 215,
                    "end": 383
                },
                {
                    "start": 385,
                    "end": 801
                },
                {
                    "start": 803,
                    "end": 1060
                },
                {
                    "start": 1060,
                    "end": 1249
                },
                {
                    "start": 1251,
                    "end": 1495
                },
                {
                    "start": 1495,
                    "end": 1591
                },
                {
                    "start": 1591,
                    "end": 1762
                },
                {
                    "start": 1762,
                    "end": 1920
                },
                {
                    "start": 1920,
                    "end": 2122
                },
                {
                    "start": 2124,
                    "end": 2222
                }
            ],
            "ref_mentions": [
                {
                    "start": 294,
                    "end": 297,
                    "matchedPaperCorpusId": "256409866"
                },
                {
                    "start": 297,
                    "end": 300,
                    "matchedPaperCorpusId": "259842568"
                },
                {
                    "start": 903,
                    "end": 906,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 1001,
                    "end": 1004,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 1244,
                    "end": 1248,
                    "matchedPaperCorpusId": "248780440"
                },
                {
                    "start": 1267,
                    "end": 1270,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 1603,
                    "end": 1607,
                    "matchedPaperCorpusId": "4930886"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81640625
        },
        {
            "corpus_id": "259075978",
            "title": "Exposing Bias in Online Communities through Large-Scale Language Models",
            "text": "With the emergence of deep learning technologies and increased computational power, the field of language models has seen remarkable advancements in recent years. Models have grown significantly in terms of parameters and training data, transforming not only the way human language is processed but how people interact with technology. Language models are able to generate increasingly coherent and human-sounding text, including complete essays and creative stories. Modern dialogue models like ChatGPT (Ope-nAI 2022) have a myriad of real-world applications, such as chatbots, voice assistants, and even online customer support. \n\nAs these models grow in relevance, it becomes increasingly important to address potentially harmful outputs. Research has shown that language models are prone to exhibit societal biases, reflecting the nature of the underlying training data written by humans. For this work, we focus on representational bias, which in the context of language models, occurs when there is an imbalance in the portrayal of different groups in LM-generated text (Blodgett et al. 2020;Liang et al. 2021). This can arise from stereotypes that cultivate negative generalisations about particular social groups and language that is used to degrade these groups. Ultimately, this may lead to certain groups being depicted less favourably than others. Bender et al. argue that even in large models with large datasets, diversity and unbiasedness are not guaranteed (Bender et al. 2021). Datasets are too big to be thoroughly documented, while marginalised social groups are more likely to be filtered out due to the nature of the filtering process. Phrases or dialects used by these groups are disproportionally categorised as hateful, resulting in their exclusion from training datasets (Sap et al. 2019;Bender et al. 2021). Hegemonic views are over-represented as Internet access is not evenly distributed, and datasets are often curated using male-dominated websites. Large language generation models such as GPT-2 (Radford et al. 2019) and GPT-3 (Brown et al. 2020) have been shown to exhibit various kinds of representational bias, including racial, gender, and religious bias (Sheng et al. 2019;Abid, Farooqi, and Zou 2021;Sheng et al. 2021).",
            "score": 0.6350499520614017,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 630
                },
                {
                    "start": 633,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1833
                },
                {
                    "start": 1834,
                    "end": 1978
                },
                {
                    "start": 1979,
                    "end": 2256
                }
            ],
            "ref_mentions": [
                {
                    "start": 1076,
                    "end": 1098,
                    "matchedPaperCorpusId": "218971825"
                },
                {
                    "start": 1098,
                    "end": 1116,
                    "matchedPaperCorpusId": "235623756"
                },
                {
                    "start": 1473,
                    "end": 1493,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 1796,
                    "end": 1813,
                    "matchedPaperCorpusId": "196211238"
                },
                {
                    "start": 1813,
                    "end": 1832,
                    "matchedPaperCorpusId": "262580630"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8271484375
        },
        {
            "corpus_id": "271769656",
            "title": "A Parameter-Efficient Multi-Objective Approach to Mitigate Stereotypical Bias in Language Models",
            "text": "Bias in NLP systems Stereotypical bias can manifest itself in different forms in LMs (Gallegos et al., 2023). Geometric relationships in model representations, for example, can encode stereotypical associations between genders and occupations (Bolukbasi et al., 2016;Caliskan et al., 2017;Zhao et al., 2019;May et al., 2019;Tan and Celis, 2019;Bommasani et al., 2020). Bias is also indicated by various divergence of probabilities from LMs. Kurita et al. (2019) and Brown et al. (2020) observed different probabilities predicted by both masked LMs and generative LMs for male and female genders given stereotypical attributes; Liang et al. (2021) identified local bias as different next token probability distributions conditioned on same contexts with only social group swapped; Barikeri et al. (2021) additionally considered difference in probabilities assigned to whole sentence pairs which are minimally different in social groups, which corresponds to global bias defined in Liang et al. (2021). Bias can also be observed as disparity in model generation (Sheng et al., 2019;Yeo and Chen, 2020) and performance in downstream tasks, such as toxicity detection (Sap et al., 2022) and coreference resolution (Kurita et al., 2019). In this work, we mainly mitigate bias reflected by divergent probability distributions predicted by LMs. \n\nMitigating bias in pre-trained LMs While many studies aimed to train fair LMs from scratch by constructing fairer datasets (Zhao et al., 2019;Zmigrod et al., 2019), it can be computationally expensive and not always feasible in practice. As a result, much effort has been put into mitigating bias from pre-trained LMs via debiasing finetuning. Kaneko and Bollegala (2021) extended projection-based methods from static word embeddings (Bolukbasi et al., 2016) and fine-tuned models to output orthogonal contextualized representations for gendered and stereotypical words. However, Gonen and Goldberg (2019) argued that projection-based methods did not completely capture and remove bias.",
            "score": 0.6350201296896878,
            "section_title": "Related Work",
            "char_start_offset": 4693,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1337
                },
                {
                    "start": 1340,
                    "end": 1577
                },
                {
                    "start": 1578,
                    "end": 1683
                },
                {
                    "start": 1684,
                    "end": 1910
                },
                {
                    "start": 1911,
                    "end": 2026
                }
            ],
            "ref_mentions": [
                {
                    "start": 243,
                    "end": 267,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 267,
                    "end": 289,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 289,
                    "end": 307,
                    "matchedPaperCorpusId": "102352962"
                },
                {
                    "start": 307,
                    "end": 324,
                    "matchedPaperCorpusId": "85518027"
                },
                {
                    "start": 344,
                    "end": 367,
                    "matchedPaperCorpusId": "220046499"
                },
                {
                    "start": 441,
                    "end": 461,
                    "matchedPaperCorpusId": "190000105"
                },
                {
                    "start": 466,
                    "end": 485,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 627,
                    "end": 646,
                    "matchedPaperCorpusId": "235623756"
                },
                {
                    "start": 780,
                    "end": 802,
                    "matchedPaperCorpusId": "235358955"
                },
                {
                    "start": 980,
                    "end": 999,
                    "matchedPaperCorpusId": "235623756"
                },
                {
                    "start": 1060,
                    "end": 1080,
                    "matchedPaperCorpusId": "202537041"
                },
                {
                    "start": 1080,
                    "end": 1098,
                    "matchedPaperCorpusId": "220444795"
                },
                {
                    "start": 1164,
                    "end": 1182,
                    "matchedPaperCorpusId": "244117167"
                },
                {
                    "start": 1210,
                    "end": 1231,
                    "matchedPaperCorpusId": "190000105"
                },
                {
                    "start": 1463,
                    "end": 1482,
                    "matchedPaperCorpusId": "102352962"
                },
                {
                    "start": 1482,
                    "end": 1503,
                    "matchedPaperCorpusId": "184486914"
                },
                {
                    "start": 1684,
                    "end": 1711,
                    "matchedPaperCorpusId": "231698657"
                },
                {
                    "start": 1774,
                    "end": 1798,
                    "matchedPaperCorpusId": "1704893"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63134765625
        },
        {
            "corpus_id": "264305744",
            "title": "Co$^2$PT: Mitigating Bias in Pre-trained Language Models through Counterfactual Contrastive Prompt Tuning",
            "text": "Pre-trained language models (PLMs) are widely used in many real-world applications, demonstrating remarkable performance (Devlin et al., 2019;Brown et al., 2020). However, it has been demonstrated that PLMs encode unfair social biases in their parameters based on their pre-training step over large-scale text corpora (May et al., 2019). Furthermore, these biases -for example, based on gender, race, or religion -can easily propagate to the downstream tasks that use these PLMs (Kaneko and Bollegala, 2021). For example, \"She is a nurse\" can have a higher conditional likelihood than \"He is a nurse\" in the language modeling task, and \"nurse\" can have higher coreference scores to \"she\" than \"he\" in the coreference resolution task (Lu et al., 2020). Considering that NLP applications like machine translation systems, resume filtering systems, dialogue systems, and speech recognition (Tatman, 2017) are widely used by millions of users globally, it is crucial to mitigate the social biases present in PLMs and strive for models that will not propagate discriminatory predictions or offensive outputs towards specific groups before being deployed. \n\nMuch prior effort has focused primarily on debiasing the representations learned during the pretraining process, e.g., through projection (Dev et al., 2020;Liang et al., 2020;Ravfogel et al., 2020;Kaneko and Bollegala, 2021), further pre-training on unbiased external corpora (Webster et al., 2020;Lauscher et al., 2021;He et al., 2022), or finetuning to debias (Cheng et al., 2021;Guo et al., 2022). The effectiveness of such debiasing efforts is typically measured on intrinsic benchmarks like SEAT (Sentence Encoding Association Test) which computes the association between demographic terms (e.g., woman, man) and stereotype terms (e.g., science, art). An unbiased model should display no difference in the similarity between the representations of these terms (May et al., 2019).",
            "score": 0.6346164542150855,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 1149
                },
                {
                    "start": 1152,
                    "end": 1552
                },
                {
                    "start": 1553,
                    "end": 1808
                },
                {
                    "start": 1809,
                    "end": 1936
                }
            ],
            "ref_mentions": [
                {
                    "start": 121,
                    "end": 142,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 142,
                    "end": 161,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 479,
                    "end": 507,
                    "matchedPaperCorpusId": "231698657"
                },
                {
                    "start": 887,
                    "end": 901,
                    "matchedPaperCorpusId": "13997424"
                },
                {
                    "start": 1290,
                    "end": 1308,
                    "matchedPaperCorpusId": "201670701"
                },
                {
                    "start": 1308,
                    "end": 1327,
                    "matchedPaperCorpusId": "207996257"
                },
                {
                    "start": 1327,
                    "end": 1349,
                    "matchedPaperCorpusId": "215786522"
                },
                {
                    "start": 1349,
                    "end": 1376,
                    "matchedPaperCorpusId": "231698657"
                },
                {
                    "start": 1450,
                    "end": 1472,
                    "matchedPaperCorpusId": "237440429"
                },
                {
                    "start": 1472,
                    "end": 1488,
                    "matchedPaperCorpusId": "253157517"
                },
                {
                    "start": 1514,
                    "end": 1534,
                    "matchedPaperCorpusId": "232185104"
                },
                {
                    "start": 1534,
                    "end": 1551,
                    "matchedPaperCorpusId": "248780440"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56201171875
        },
        {
            "corpus_id": "270095451",
            "title": "Understanding Intrinsic Socioeconomic Biases in Large Language Models",
            "text": "In recent years, Large Language Models (LLMs) have been increasingly integrated into various fields such as healthcare (Hadi et al. 2023), insurance (Dimri et al. 2019), employment (Qin et al. 2018), and credit scoring (Wu et al. 2023).While the evaluation of social biases in LLMs (Nadeem, Bethke, and Reddy 2021; Kaneko and Bollegala 2022) and their potential harms (Blodgett et al. 2020;Beukeboom and Burgers 2019) has been extensively studied, the rapid development and integration of these models into critical decision-making areas necessitate the ongoing need to assess and address their inherent biases.\n\nPrevious research by Shen et al. (2022) has demonstrated how seemingly innocuous details like names or subtle language cues can significantly influence the outcomes of language-based recommender systems, skewing price of restaurant recommendations based on perceived gender, race, or sexual orientation.This work, however, did not study the root causes of this bias.Motivated by this gap, we investigate how LLMs inherently exhibit biases related to socioeconomic status, particularly when considering different demographic groups.Concurrent with our research, Singh et al. (2024) explored LLMs understanding and em-pathy towards underprivileged populations in extreme situations.While both studies offer valuable insights, they have not fully addressed the critical question of how demographic factors influence and amplify harmful socioeconomic biases in LLMs.This paper aims to fill this research gap and contribute to a more nuanced understanding of bias and fairness in LLMs.\n\nIn this study, we uncover intrinsic socioeconomic biases in state-of-the-art LLMs such as Llama2 and Falcon, as well as the widely-used models such as BERT and GPT-2.We focus on evaluating these biases across different demographic attributes -including birth-assigned gender, marital status, race, and religion -and exploring how these biases manifest and vary across models.Furthermore, we assess the dynamics between these biases by exploring intersectionality among gender, race, and marital status.",
            "score": 0.6341610223646114,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 236
                },
                {
                    "start": 236,
                    "end": 611
                },
                {
                    "start": 613,
                    "end": 916
                },
                {
                    "start": 916,
                    "end": 979
                },
                {
                    "start": 979,
                    "end": 1144
                },
                {
                    "start": 1144,
                    "end": 1293
                },
                {
                    "start": 1293,
                    "end": 1475
                },
                {
                    "start": 1475,
                    "end": 1593
                },
                {
                    "start": 1595,
                    "end": 1761
                },
                {
                    "start": 1761,
                    "end": 1970
                },
                {
                    "start": 1970,
                    "end": 2097
                }
            ],
            "ref_mentions": [
                {
                    "start": 181,
                    "end": 198,
                    "matchedPaperCorpusId": "49647650"
                },
                {
                    "start": 315,
                    "end": 341,
                    "matchedPaperCorpusId": "233241161"
                },
                {
                    "start": 390,
                    "end": 417,
                    "matchedPaperCorpusId": "53622943"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70703125
        },
        {
            "corpus_id": "265281304",
            "title": "Bias A-head? Analyzing Bias in Transformer-Based Language Model Attention Heads",
            "text": "In this work, we present an approach to understand how stereotyping biases are encoded in the attention heads of pretrained language models. We infer that the biases are mostly encoded in a small set of biased heads. We further analyze the behavior of these biased heads, by comparing them with other regular heads, and confirm our findings. We also present experiments to quantify gender bias and racial bias in BERT and GPT. This work is among the first work aiming to understand how bias manifests internally in PLMs. Previous work has often used downstream tasks or prompting to examine a PLM's fairness in a black-box manner. \n\nWe try to open up the black-box and analyze different patterns of bias. In doing so, we strengthen our understanding of PLM bias mechanisms. Future work can apply our method to assess concerning biases in increasingly large foundation models such as GPT-3 and LLaMA. Overall, our work sheds light on how bias manifests internally in language models, and constitutes an important step towards designing more transparent, accountable, and fair NLP systems.",
            "score": 0.6321163469252895,
            "section_title": "Conclusion and Discussion",
            "char_start_offset": 26129,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 630
                },
                {
                    "start": 633,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 1087
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74072265625
        },
        {
            "corpus_id": "273501725",
            "title": "Hey GPT, Can You be More Racist? Analysis from Crowdsourced Attempts to Elicit Biased Content from Generative AI",
            "text": "Submissions were shared within dedicated subchannels on the main channel hosting the bias-athon competition on the Microsoft Teams platform. Supposedly, each sub-channel contains prompts that reveal biases in different categories. In total, there exist 7 categories: \n\n1. Socio-Cultural Bias: This category addresses biases that arise from societal norms and cultural contexts, including prejudices based on ethnicity, race, gender, nationality, and religion. This category captures the varied stereotypes different social groups can face due to bias in large language model outputs. These kinds of biases often arise from the social biases present in training data.",
            "score": 0.6319806077055757,
            "section_title": "A Competition Categories",
            "char_start_offset": 35471,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 266
                },
                {
                    "start": 269,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 666
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6865234375
        },
        {
            "corpus_id": "226226407",
            "title": "Evaluating Bias In Dutch Word Embeddings",
            "text": "In recent years language models have become more relevant in the field of Natural Language Processing (NLP). As word embeddings were shown to perform better in many tasks than many traditional techniques, the research community followed this direction and made further advancements resulting in another breakthrough -contextualized word embeddings (e.g. BERT Devlin et al. (2018), RoBERTa Liu et al. (2019)). As opposed to traditional (context-free) word embeddings that have a fixed representation vector, contextualized word embeddings change depending on the sentences (context) in which they are used. They have achieved state-of-the-art in NLP tasks and effectively replaced traditional word embeddings. Due to their outstanding performance, they are broadly adopted in many real-world applications (Wolf et al., 2019). \n\nModels employing those embeddings often support decisions that strongly impact people's lives, so their fairness and correctness is critical. Unfortunately, previous research has shown that traditional and contextualized word embeddings can encode social biases present in the training data (Caliskan et al., 2017;Garg et al., 2018;May et al., 2019;Zhao et al., 2019). \n\nThe social biases in machine learning applications can have an impact on society, as the case in computer vision where three commercial gender classification systems reported higher error rates when recognizing women, specifically those with darker skin tones (Buolamwini, 2018). These biases can cause undesired effects in downstream NLP tasks (Zhao et al., 2018;Basta et al., 2019) where biased NLP models can amplify bias in real world applications and especially affect minorities which are misrepresented in the data. It has been shown that some minorities like people with disabilities are misrepresented and are associated with a negative sentiment (Hutchinson et al., 2020). This paper explores the existing gender bias in six traditional and two contextualized Dutch word embedding models by using a combination of state-of-the-art methods proposed in previous literature for English (Caliskan et al., 2017;Gonen and Goldberg, 2019).",
            "score": 0.6318403028000071,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 109,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 824
                },
                {
                    "start": 827,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1195
                },
                {
                    "start": 1198,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1720
                },
                {
                    "start": 1721,
                    "end": 1880
                },
                {
                    "start": 1881,
                    "end": 2140
                }
            ],
            "ref_mentions": [
                {
                    "start": 1118,
                    "end": 1141,
                    "matchedPaperCorpusId": "23163324"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54931640625
        },
        {
            "corpus_id": "272464141",
            "title": "AGR: Age Group fairness Reward for Bias Mitigation in LLMs",
            "text": "Large language models (LLMs) used in various fields can perpetuate age biases, affecting career opportunities and healthcare [1]. Unlike fixed gender and racial biases, age bias is continuous and evolving. Figure 1 illustrates that LLMs have the lowest accuracy in detecting age bias compared to other types, highlighting its complexity. Medium-sized LLMs, such as BERT [2] and GPT-1 [3], generally have under a billion parameters and face two types of social biases: internal, present in the model's pre-trained outputs, and external, affecting downstream task predictions. Internal debiasing methods address biases in a pre-trained model's outputs through three main approaches: pre-processing [4], in-training [5], and post-processing [6]. External debiasing methods tackle biases in model predictions during downstream tasks, using data-centered approaches [7] to integrate fairness goals during training. Large-scale LLMs like GPT-3 encounter greater debiasing challenges due to size and complexity, often addressed through preference alignment [8] and prompt engineering techniques [9]. \n\nUnlike gender and racial biases, age bias is challenging due to its dynamic nature, complicating counterfactual and contrastive methods. Research on age bias mitigation remains limited [10]. \n\nAdditionally, common fine-tuning methods for LLMs include instruction-based fine-tuning [11] and reinforcement learning with human feedback [12]. However, no instruction-based datasets address age bias, and these methods do not target social biases, leading to potential performance discrepancies across age groups. \n\nTo address this challenge, we revised and expanded BBQ [13] and ISB [14] datasets and manually annotated them to create age preference and instruction fine-tuning datasets for age bias. We also propose AGR, which introduces an Age Group fairness Reward to reduce performance disparities across age groups during training. \n\nIn summary, our contributions are as follows: \n\n\u2022 We construct age bias preference and instruction fine-tuning datasets for bias evaluation in LLMs.",
            "score": 0.6312655936211886,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 1092
                },
                {
                    "start": 1095,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1285
                },
                {
                    "start": 1288,
                    "end": 1433
                },
                {
                    "start": 1434,
                    "end": 1603
                },
                {
                    "start": 1606,
                    "end": 1791
                },
                {
                    "start": 1792,
                    "end": 1927
                },
                {
                    "start": 1930,
                    "end": 1975
                },
                {
                    "start": 1978,
                    "end": 2078
                }
            ],
            "ref_mentions": [
                {
                    "start": 125,
                    "end": 128,
                    "matchedPaperCorpusId": "59158788"
                },
                {
                    "start": 370,
                    "end": 373,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 696,
                    "end": 699,
                    "matchedPaperCorpusId": "259095603"
                },
                {
                    "start": 713,
                    "end": 716,
                    "matchedPaperCorpusId": "248780440"
                },
                {
                    "start": 738,
                    "end": 741,
                    "matchedPaperCorpusId": "258740820"
                },
                {
                    "start": 861,
                    "end": 864,
                    "matchedPaperCorpusId": "259859044"
                },
                {
                    "start": 1050,
                    "end": 1053,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1376,
                    "end": 1380,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 1428,
                    "end": 1432,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1661,
                    "end": 1665,
                    "matchedPaperCorpusId": "239010011"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65625
        },
        {
            "corpus_id": "239009591",
            "title": "Socially Aware Bias Measurements for Hindi Language Representations",
            "text": "Language models and representations (Pennington et al., 2014;Bojanowski et al., 2017;Devlin et al., 2018) are commonly used across the world to aid in a variety of daily applications including machine translation (Kunchukuttan et al., 2017), information retrieval (Rao and Devi, 2018), chatbots (Bhagwat et al., 2019), sentiment classification (Kumar et al., 2019) and more. A large body of work is thus focused on building robust language representations for different languages, including those which have low resources available.\n\nRecent work have demonstrated how these representations capture and propagate societal biases such as gender (Bolukbasi et al., 2016), race (Caliskan et al., 2017), and nationality (Dev and Phillips, 2019) related.\n\nIt is important to note that languages reflect the culture and history of regions they are used popularly in, and as we switch between languages, the notion of bias, the types of societal biases and what is considered as discrimination or fair also change accordingly. This key difference however is not reflected in the effort made towards detecting, identifying and mitigating biases in language representations, with majority of efforts predominantly in English language and in the context of Western society (Bolukbasi et al., 2016;Nangia et al., 2020). Some recent work tackle the challenges of societal biases in language representations coming from various cultures and languages such as Arabic, French, Spanish and German (Lauscher et al., 2020;Ch\u00e1vez Mulsa and Spanakis, 2020;Kurpicz-Briki, 2020;Zhou et al., 2019). Ghosh et al. (2021) and Sambasivan et al. (2020) explore biases and algorithmic fairness concerning non-western contexts in machine learning and Pujari et al. (2019) investigate gender bias in Hindi language representations. However, it is unclear still if Hindi language captures many other biases that are unique to the Indian society, and have potential to cause great harm, such as caste and religion related biases.\n\nIn this work, we make three main contributions: (i) using a careful study of the social and cultural composition of the Indian society, we highlight and devise measures to detect,",
            "score": 0.6303825019671232,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.322509765625
        },
        {
            "corpus_id": "265609662",
            "title": "Developing Linguistic Patterns to Mitigate Inherent Human Bias in Offensive Language Detection",
            "text": "Exposure to toxic content is a very common problem on the internet, especially on social media [20]. Accurate detection of toxic content is important for the implementation of preventive measures. Deep natural language processing models often contain inherent biases in the data, as they are developed by collecting large human-generated datasets [51]. These biases are likely to mislead the models developed for use in detecting toxic content on the internet [22]. \n\nUnconscious bias, which includes prejudice, stereotypes, and discrimination, can be perpetuated through language models [21]. Prejudice is an unfounded conclusion that does not hold true in our daily discourse, and it is a complex, multi-dimensional phenomenon influenced by various factors. Stereotypes are cognitive \"shortcuts\" that our brains use to make decisions more quickly, and they can be harmful, leading to biased attitudes and behaviors towards certain groups. Discrimination, on the other hand, is a long-standing problem in human societies and can take many forms. \n\nTo mitigate the negative effects of language models, researchers can develop methods to identify and remove biased language from training data, or create models that are specifically designed to be fair and unbiased [52]. For example, language models can be tested for bias and then fine-tuned to reduce or eliminate any biases that are detected. Additionally, diversity and inclusivity can be prioritized when selecting and curating training data. By taking steps to address the biases and ethical concerns associated with language models, we can promote fairness and equality in social interactions, both online and offline [34]. \n\nThe inherent language bias in natural languages manifests itself in imbalanced datasets and leaks into the language models. Addressing latent biases in the data itself is critical for developing unbiased large-language models. Thus, bias should be reduced on the data side first. \n\nOur approach focuses on reducing the stereotypical bias in the offensive language detection models by augmenting the data using linguistic features. By incorporating more contextual information, the predictions become more generalized and accurate, even for extreme cases. We have demonstrated that simple language features can be used to retrieve contextual text and address label imbalances for tasks such as offensive language detection. \n\nIt is also worth noting that our approach can be generalized to multiple languages.",
            "score": 0.6303374659186842,
            "section_title": "Discussion",
            "char_start_offset": 37333,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 465
                },
                {
                    "start": 468,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1046
                },
                {
                    "start": 1049,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1497
                },
                {
                    "start": 1498,
                    "end": 1680
                },
                {
                    "start": 1683,
                    "end": 1806
                },
                {
                    "start": 1807,
                    "end": 1909
                },
                {
                    "start": 1910,
                    "end": 1962
                },
                {
                    "start": 1965,
                    "end": 2113
                },
                {
                    "start": 2114,
                    "end": 2237
                },
                {
                    "start": 2238,
                    "end": 2405
                },
                {
                    "start": 2408,
                    "end": 2491
                }
            ],
            "ref_mentions": [
                {
                    "start": 347,
                    "end": 351,
                    "matchedPaperCorpusId": "231603388"
                },
                {
                    "start": 460,
                    "end": 464,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 588,
                    "end": 592,
                    "matchedPaperCorpusId": "236950797"
                },
                {
                    "start": 1265,
                    "end": 1269,
                    "matchedPaperCorpusId": "233476528"
                },
                {
                    "start": 1675,
                    "end": 1679,
                    "matchedPaperCorpusId": "52255687"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70654296875
        },
        {
            "corpus_id": "271161894",
            "title": "The sociolinguistic foundations of language modeling",
            "text": "NLP systems generally suffer from social bias: their real-world application leads to outcomes that unfairly disadvantage or harm specific social groups [58,59,60,61].Social bias can be introduced at various points during the development and deployment of NLP systems [62], but given the unsupervised nature of language modeling, training corpora are a key source of social bias in LLMs [10,63].While bias in NLP systems can harm people in various ways [59], in this section, we primarily focus on two common harmful outcomes of social bias.These two types of harms are most commonly discussed in terms of quality-of-service harms and stereotyping harms (e.g., [64,65,60,66]), although many different systems have been proposed for classifying biases and harms in NLP, which define these terms in somewhat different ways, along with many additional and often overlapping categories [59].Both of these types of harms are especially relevant to LLMs, and crucially, we believe both can be better understood and addressed in language modeling by adopting a sociolinguistic perspective (see Figure 3).First, social bias can be characterized by poor system performance for certain social groups that are interacting with LLMs and applications based on language models: token prediction will be more or less accurate depending on the social origins of the language inputted into the system.For example, ChatGPT might have difficulty correctly understanding prompts written by people from certain social groups due to their use of non-standard or socially restricted language patterns.This type of bias leads to what is known as quality-of-service harms, where the performance of these systems varies depending on the social background of the user [64,60].These types of quality-of-service harms can often be the product of selection bias, as they result from how training data is selected from across the society whose language is being modeled [58]: in general, if language data from certain social groups is under-represented in the training data for a language model, we should expect that NLP applications based on that model will process language structures produced by these groups less accurately and consequently exhibit poorer performance for these groups [59,67].",
            "score": 0.6298418112303596,
            "section_title": "Social Bias",
            "char_start_offset": 18316,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 166,
                    "end": 394
                },
                {
                    "start": 394,
                    "end": 540
                },
                {
                    "start": 540,
                    "end": 886
                },
                {
                    "start": 886,
                    "end": 1096
                },
                {
                    "start": 1096,
                    "end": 1383
                },
                {
                    "start": 1383,
                    "end": 1577
                },
                {
                    "start": 1577,
                    "end": 1748
                },
                {
                    "start": 1748,
                    "end": 2266
                }
            ],
            "ref_mentions": [
                {
                    "start": 152,
                    "end": 156,
                    "matchedPaperCorpusId": "209461005"
                },
                {
                    "start": 156,
                    "end": 159,
                    "matchedPaperCorpusId": "208783276"
                },
                {
                    "start": 159,
                    "end": 162,
                    "matchedPaperCorpusId": "252907216"
                },
                {
                    "start": 162,
                    "end": 165,
                    "matchedPaperCorpusId": "258688053"
                },
                {
                    "start": 267,
                    "end": 271,
                    "matchedPaperCorpusId": "237298625"
                },
                {
                    "start": 386,
                    "end": 390,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 390,
                    "end": 393,
                    "matchedPaperCorpusId": "258041203"
                },
                {
                    "start": 452,
                    "end": 456,
                    "matchedPaperCorpusId": "208783276"
                },
                {
                    "start": 667,
                    "end": 670,
                    "matchedPaperCorpusId": "252907216"
                },
                {
                    "start": 881,
                    "end": 885,
                    "matchedPaperCorpusId": "208783276"
                },
                {
                    "start": 1744,
                    "end": 1747,
                    "matchedPaperCorpusId": "252907216"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85107421875
        },
        {
            "corpus_id": "250390904",
            "title": "Benchmarking Intersectional Biases in NLP",
            "text": "There has been a recent wave of work assessing the fairness of machine learning models in general, and more specifically, on natural language processing (NLP) models built using machine learning techniques. While much work has highlighted biases embedded in state-of-the-art language models, and more recent efforts have focused on how to debias, research assessing the fairness and performance of biased/debiased models on downstream prediction tasks has been limited. Moreover, most prior work has emphasized bias along a single dimension such as gender or race. In this work, we benchmark multiple NLP models with regards to their fairness and predictive performance across a variety of NLP tasks. In particular, we assess intersectional bias - fairness across multiple demographic dimensions. The results show that while current debiasing strategies fare well in terms of the fairness-accuracy trade-off (generally preserving predictive power in debiased models), they are unable to effectively alleviate bias in downstream tasks. Furthermore, this bias is often amplified across dimensions (i.e., intersections). We conclude by highlighting possible causes and making recommendations for future NLP debiasing research.",
            "score": 0.6293572149811717,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86376953125
        },
        {
            "corpus_id": "253384242",
            "title": "HERB: Measuring Hierarchical Regional Bias in Pre-trained Language Models",
            "text": "Large-scale pre-trained language models (LMs) are prevalent in the natural language processing (NLP) community since the costly pre-trained models can be adapted to a wide range of downstream applications. However, research studies demonstrate that the societal biases in the pre-training corpora can be learned by LMs and further propagated to the downstream applications (Zhao et al., 2019;Dev et al., 2020;Goldfarb-Tarrant et al., 2021;Kurita et al., 2019). To qualify and mitigate bias for pre-trained LMs, researchers have developed bias evaluation methods targeting certain social groups such as gender, religion, and race (Sun et al., 2019;Manzini et al., 2019;Xia et al., 2020;Delobelle et al., 2021). However, existing methods do not examine the social groups categorised by geographical information, which leaves the region-related biases in pre-trained LMs unexplored. Therefore, our work bridges this gap by addressing research questions about whether regional bias exists in the pre-trained LMs, and if yes, how to quantify the bias in a principled way. \n\nBias in NLP applications makes distinct judgements on people based on their gender, race, religion, region, or other social groups could be harmful, such as automatically downgrading the resumes of female applicants in recruiting (Dastin, 2018) Regional bias represents stereotypes based on the geographic location where people live or come from (Wikipedia, 2022a). To verify the existence of regional bias, we first leverage a sentence-level bias measurement (Kaneko and Bollegala, 2022), with which the likelihood of a biased sentence produced by a pre-trained LM can be acquired with a designed input: where [region] and [description] can be filled with any desired words. The output likelihood represents the contextualised possibility of associating people in the region with the given context, which can be utilised to analyse the bias integrated into LMs. From the perspective of the pre-trained LM, there is a 'world map' of region-wide judgements regards to the [description] of interest.",
            "score": 0.6279613935730226,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 1066
                },
                {
                    "start": 1069,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1744
                },
                {
                    "start": 1745,
                    "end": 1931
                },
                {
                    "start": 1932,
                    "end": 2066
                }
            ],
            "ref_mentions": [
                {
                    "start": 373,
                    "end": 392,
                    "matchedPaperCorpusId": "102352962"
                },
                {
                    "start": 392,
                    "end": 409,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 409,
                    "end": 439,
                    "matchedPaperCorpusId": "229923772"
                },
                {
                    "start": 439,
                    "end": 459,
                    "matchedPaperCorpusId": "190000105"
                },
                {
                    "start": 629,
                    "end": 647,
                    "matchedPaperCorpusId": "195316733"
                },
                {
                    "start": 647,
                    "end": 668,
                    "matchedPaperCorpusId": "102350941"
                },
                {
                    "start": 668,
                    "end": 685,
                    "matchedPaperCorpusId": "218869965"
                },
                {
                    "start": 685,
                    "end": 708,
                    "matchedPaperCorpusId": "245131370"
                },
                {
                    "start": 1529,
                    "end": 1557,
                    "matchedPaperCorpusId": "233241161"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.517578125
        },
        {
            "corpus_id": "270257839",
            "title": "Safeguarding Large Language Models: A Survey",
            "text": "When this feature is used for model encoding and further downstream tasks, it induces unfairness in the language model toward African Americans.Biases are purposefully introduced into the responses of LLMs to craft distinct personas for use in interactive media [47].BAD focuses on identifying and quantifying instances of social bias in models like ChatGPT, especially in sensitive applications such as job and college admissions screening [48].DAMA utilizes causal analysis to identify problematic model components, mainly focusing on mid-upper feedforward layers most prone to convey biases [49].The presence of political bias is examined in ChatGPT, focusing on aspects such as race, gender, religion, and political orientation [50].Additionally, they explored the role of randomness in responses by collecting multiple answers to the same questions, which enables a more robust analysis of potential biases.The bias of LLMs is also examined by controlling the input, highlighting that LLMs can still produce biased responses despite the progress in bias reduction [51].Besides, a Bias Index is designed to quantify and address biases inherent in LLMs, including GPT-4 [52].It has also been observed that the biased response can be generated inadvertently, sometimes as seemingly harmless jokes [53].\n\nA line of debiasing studies aims to mitigate the intrinsic bias that is task-agnostic in the representations before they are applied to downstream tasks.Safeguarding training data before training the model efficiently alleviates intrinsic biases since label imbalance across different demographic groups in the training data is an essential factor in inducing bias.A Counterfactual Data Augmentation (CDA) [54], [55] is a widespread data processing method to balance labels, which replaces the sensitive attributes in the original sample with the sensitive attributes of the opposite demographic based on a prior list of sensitive word pairs.Fairness can be incorporated into LLMs' design to balance the training samples, and then a guarded fairer model can be obtained by parameter tuning.Retraining models [56]is a direct way to reduce bias, although it can be resource-intensive and difficult to scale.",
            "score": 0.6278992838554615,
            "section_title": "Fairness",
            "char_start_offset": 30321,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 144,
                    "end": 267
                },
                {
                    "start": 267,
                    "end": 446
                },
                {
                    "start": 446,
                    "end": 599
                },
                {
                    "start": 599,
                    "end": 737
                },
                {
                    "start": 737,
                    "end": 912
                },
                {
                    "start": 912,
                    "end": 1074
                },
                {
                    "start": 1074,
                    "end": 1178
                },
                {
                    "start": 1178,
                    "end": 1304
                },
                {
                    "start": 1306,
                    "end": 1459
                },
                {
                    "start": 1459,
                    "end": 1671
                },
                {
                    "start": 1671,
                    "end": 1948
                },
                {
                    "start": 1948,
                    "end": 2096
                },
                {
                    "start": 2096,
                    "end": 2211
                }
            ],
            "ref_mentions": [
                {
                    "start": 262,
                    "end": 266,
                    "matchedPaperCorpusId": "265157686"
                },
                {
                    "start": 732,
                    "end": 736,
                    "matchedPaperCorpusId": "257372256"
                },
                {
                    "start": 1069,
                    "end": 1073,
                    "matchedPaperCorpusId": "264555752"
                },
                {
                    "start": 1712,
                    "end": 1716,
                    "matchedPaperCorpusId": "225075985"
                },
                {
                    "start": 1718,
                    "end": 1722,
                    "matchedPaperCorpusId": "259095584"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.479736328125
        },
        {
            "corpus_id": "271161894",
            "title": "The sociolinguistic foundations of language modeling",
            "text": "These types of quality-of-service harms can often be the product of selection bias, as they result from how training data is selected from across the society whose language is being modeled [58]: in general, if language data from certain social groups is under-represented in the training data for a language model, we should expect that NLP applications based on that model will process language structures produced by these groups less accurately and consequently exhibit poorer performance for these groups [59,67].Notably, quality-of-service harms, especially those resulting from selection bias, have been one of the central concerns in computational sociolinguistics [68,69,51].Researchers in this emerging field have stressed for the past decade that the performance of NLP systems generally varies for people from different social groups and have called for engagement with description and theory from sociolinguistics to help address this basic form of social bias (e.g.[70,71,72,73]).\n\nSecond, social bias can be characterized by systems that produce outputs that directly harm or discriminate against certain social groups even when they are not directly engaging with these systems themselves.For example, when prompted, ChatGPT might be more likely to produce negative portrayals about ethnicities and genders, no matter who is doing the prompting [8,67].Most notably, this type of bias can lead to what is known as stereotyping harms [64], as well as related harms like disparagement and dehumanization [60], where negative viewpoints about specific social groups are propagated, as has been widely discussed in regards to LLMs [10].Once again this issue can be traced back to the data the language model was trained on.If the training corpus contains relatively frequent expression of harmful or inaccurate ideas about certain social groups -as we can safely assume any large, unconstrained sample of internet writings will -language models will inevitably reproduce those biases [10,63].As Bender et al. (2021, 613) state, \"large, uncurated, Internet-based datasets encode the dominant/hegemonic view, which further harms people at the margins\" [10].",
            "score": 0.6275607771397533,
            "section_title": "Social Bias",
            "char_start_offset": 20064,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 518
                },
                {
                    "start": 518,
                    "end": 684
                },
                {
                    "start": 684,
                    "end": 979
                },
                {
                    "start": 979,
                    "end": 994
                },
                {
                    "start": 996,
                    "end": 1205
                },
                {
                    "start": 1205,
                    "end": 1368
                },
                {
                    "start": 1368,
                    "end": 1647
                },
                {
                    "start": 1647,
                    "end": 1734
                },
                {
                    "start": 1734,
                    "end": 2003
                },
                {
                    "start": 2003,
                    "end": 2166
                }
            ],
            "ref_mentions": [
                {
                    "start": 190,
                    "end": 194,
                    "matchedPaperCorpusId": "209461005"
                },
                {
                    "start": 510,
                    "end": 514,
                    "matchedPaperCorpusId": "208783276"
                },
                {
                    "start": 514,
                    "end": 517,
                    "matchedPaperCorpusId": "264451881"
                },
                {
                    "start": 673,
                    "end": 677,
                    "matchedPaperCorpusId": "583389"
                },
                {
                    "start": 979,
                    "end": 983,
                    "matchedPaperCorpusId": "7396193"
                },
                {
                    "start": 989,
                    "end": 992,
                    "matchedPaperCorpusId": "29385817"
                },
                {
                    "start": 1364,
                    "end": 1367,
                    "matchedPaperCorpusId": "264451881"
                },
                {
                    "start": 1517,
                    "end": 1521,
                    "matchedPaperCorpusId": "252907216"
                },
                {
                    "start": 1642,
                    "end": 1646,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 1995,
                    "end": 1999,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 1999,
                    "end": 2002,
                    "matchedPaperCorpusId": "258041203"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74609375
        },
        {
            "corpus_id": "246441807",
            "title": "Causal effect of racial bias in data and machine learning algorithms on user persuasiveness & discriminatory decision making: An Empirical Study",
            "text": "Bias in the dataset in any variation has significant consequences when a machine learning model is trained without a proper mitigation strategy. The existence of racial or gender bias in language data has been recently studied in the context of an individual's perception while dealing with AI-based systems such as recommendation engines. It holds AI systems accountable and questionable due to gender or racial bias (Gupta et al., 2021). The study argues the adverse consequences of wrongly presented outcomes from AI-based systems influencing users' judgmental quotient. Since linguistics is part of core social behavior and is likely a primary medium through which individuals communicate or express views for social categorization, it becomes critical to study the lateral influence further when humans interact with systems designed on linguistic datasets. A recent study by researchers in Facebook AI Labs argues how a significant portion of textual data exhibits varied social information in the context of dialog data (Dinan et al., 2020). Such datasets, further when used for training machine learning models or NLP algorithms to solve core business or decision-making problems, are vulnerable to the carry-over effect due to the absence of contextualization capabilities of AI systems. The rise of data and advancement in machine learning algorithms has led to a critical data and algorithmic bias issue. There is limited empirical evidence proving how algorithms exhibit various kinds of bias learned from data (Kordzadeh & Ghasemaghaei, 2021). The machine learning systems show gender-specific bias (Butler et al., 2018) or racial bias (Manzini et al., 2019) impact decision-making due to the limited explainability of models.\n\nA study from existing literature indicates the vast impact of biases in models, such as racial discrimination in differences within the communication styles in sentiment analysis interpretation (Guerini, Gatti, & Turchi, 2013) and products review discussion (Yang et al., 2015). The study focuses on discovering the influence of biases in machine learning systems and their impact on business decision-making. An important aspect of machine learning systems identified in this paper will be exploring how racial discrimination impacts NLP and machine learning models' efficacy and whether biased models influence the discriminatory decision-making process.\n\nThe viewpoint from previous literature is evidence to demonstrate that influence of racial bias in various stages of a natural",
            "score": 0.6270391530399633,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 418,
                    "end": 438,
                    "matchedPaperCorpusId": "235593934"
                },
                {
                    "start": 1523,
                    "end": 1555,
                    "matchedPaperCorpusId": "236273467"
                },
                {
                    "start": 1612,
                    "end": 1633,
                    "matchedPaperCorpusId": "50780992"
                },
                {
                    "start": 1649,
                    "end": 1671,
                    "matchedPaperCorpusId": "102350941"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.408935546875
        },
        {
            "corpus_id": "265552017",
            "title": "PEFTDebias : Capturing debiasing information using PEFTs",
            "text": "We validate our hypothesis by conducting validation on two widely recognized factors of social bias: gender stereotyping and racial identifiers. To address occupation-based gender stereotypes, we utilize the BiasBios dataset (De-Arteaga et al., 2019). For the bias related to race, we address the issue of elevated occurrences of false positive outcomes in hate speech predictions using GHC (Kennedy et al., 2018). To show our generalizibility of capturing debiasing information along a specific axis using PEFTs, we show transfer to datasets MNLI (multi genre NLI) (Williams et al., 2018) and LHC (large hate corpus) (Toraman et al., 2022) along gender and race axis respectively. \n\nIn order to assess the effectiveness of our debi-asing techniques in mitigating gender and racial biases, we utilize two intrinsic bias benchmarks, namely CrowS-Pairs (Nangia et al., 2020) and StereoSet (Nadeem et al., 2021), during the initial phase of our evaluation, referred to as the upstream stage. StereoSet evaluates a language model's stereotypical associations by employing fill-in-the-blank problems with intra-sentence examples across different bias categories. CrowS-Pairs is an intra-sentence dataset of minimal pairs that compares the language model's masked token probabilities of sentences with disadvantaged or advantaged races fulfilling or violating stereotypes. \n\nIn the subsequent downstream stage, we evaluate the performance gap of PEFTs across different protected attributes within the specific domain using extrinsic bias metrics. To measure gender bias, we adopt the method proposed by De-Arteaga et al. (2019) to calculate the gender gap in the True Positive Rate (TPR) for each occupation (TPR-GAP). To assess racial bias, we compute the False Positive Rate Difference (FPRD) by comparing the FPR of examples mentioning protected racial attributes to the overall FPR. We calculate FPRD for both the in-domain data and the Identity Phrase Templates Test Sets (IPTTS) (Zhang et al., 2020), which consist of 77k instances.",
            "score": 0.6263714909859291,
            "section_title": "Bias Factors and Datasets",
            "char_start_offset": 5749,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 681
                },
                {
                    "start": 684,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1366
                },
                {
                    "start": 1369,
                    "end": 1540
                },
                {
                    "start": 1541,
                    "end": 1712
                },
                {
                    "start": 1713,
                    "end": 1880
                },
                {
                    "start": 1881,
                    "end": 2032
                }
            ],
            "ref_mentions": [
                {
                    "start": 566,
                    "end": 589,
                    "matchedPaperCorpusId": "3432876"
                },
                {
                    "start": 618,
                    "end": 640,
                    "matchedPaperCorpusId": "247218132"
                },
                {
                    "start": 851,
                    "end": 872,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 887,
                    "end": 908,
                    "matchedPaperCorpusId": "215828184"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.47509765625
        },
        {
            "corpus_id": "265609662",
            "title": "Developing Linguistic Patterns to Mitigate Inherent Human Bias in Offensive Language Detection",
            "text": "For instance, the study [20] examined how gender distinctions are encoded in Turkish, a language that is also grammatically gender-neutral. The results uncovered by linguistics and psychology demonstrated the underlying reasons for the bias we encounter in language models. \n\nDiscrimination. is a long-standing problem in human societies and can take many forms. One of the more insidious ways in which discrimination can manifest is through natural language models, which are computational models that use statistical algorithms to understand and generate human language. These models have become ubiquitous in our daily lives, powering everything from virtual assistants to search engines and chatbots. However, as with any human-created system, natural language models are susceptible to biases and discrimination that can be embedded in the data they are trained on [21]. \n\nThe problem of discrimination in language models arises because these models are developed based on huge amounts of text data retrieved from the internet, which contains a plethora of societal biases and prejudices. For instance, language models might be trained on text data that contains racist or sexist language, which can lead to the model associating certain words or phrases with negative stereotypes. As a result, when the model is used to generate text or respond to queries, it may produce outputs that perpetuate harmful stereotypes and discriminatory practices. \n\nFurthermore, language models can also be discriminatory in their output if the training data is not diverse enough to represent different demographics or if the model is biased towards a particular dialect or language variety. This can result in language models that are more accurate for certain groups than for others, which can perpetuate existing power imbalances and inequalities. \n\nThe consequences of discriminatory language models can be significant, ranging from perpetuating harmful stereotypes to reinforcing systemic discrimination against marginalized groups [22]. As natural language models continue to become more widespread and integrated into our daily lives, it is necessary to address the problem of discrimination in these models and work towards creating more equitable and fair computational systems. This paper explores the causes and consequences of discrimination in language models and propose potential solutions to mitigate the problem.",
            "score": 0.6262258481584757,
            "section_title": "Introduction",
            "char_start_offset": 7138,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 273
                },
                {
                    "start": 276,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 875
                },
                {
                    "start": 878,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1286
                },
                {
                    "start": 1287,
                    "end": 1451
                },
                {
                    "start": 1454,
                    "end": 1680
                },
                {
                    "start": 1681,
                    "end": 1839
                },
                {
                    "start": 1842,
                    "end": 2031
                },
                {
                    "start": 2032,
                    "end": 2276
                },
                {
                    "start": 2277,
                    "end": 2418
                }
            ],
            "ref_mentions": [
                {
                    "start": 870,
                    "end": 874,
                    "matchedPaperCorpusId": "236950797"
                },
                {
                    "start": 2026,
                    "end": 2030,
                    "matchedPaperCorpusId": "215828184"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8603515625
        },
        {
            "corpus_id": "270226200",
            "title": "JBBQ: Japanese Bias Benchmark for Analyzing Social Biases in Large Language Models",
            "text": "The bias of Large Language Models (LLMs) leads to the reproduction of bias in downstream tasks such as language generation. As discussed by Blodgett et al. (2020), there are various biases in NLP. One of these problematic biases is social bias and stereotyping behavior. For instance, stereotyping behavior observed in text generation has the potential to harm minorities who read the text, and using such biased texts as training data introduces additional bias into the subsequent LLMs (Gehman et al., 2020;Bender et al., 2021). \n\nVarious social bias benchmarks have been provided (Rudinger et al., 2018;Zhao et al., 2018;Nangia et al., 2020;Li et al., 2020;Nadeem et al., 2021;Dhamala et al., 2021;Parrish et al., 2022;N\u00e9v\u00e9ol et al., 2022;Huang and Xiong, 2024;Jin et al., 2024;Kaneko et al., 2024). However, most of them are constructed in English, and benchmarks in languages other than English are not yet fully developed. In addition, some LLMs have recently been developed specifically for Japanese (LLM-jp et al., 2024;Fujii et al., 2024), but it remains unclear the extent to which Japanese LLMs exhibit biases against a range of social categories. \n\nTo evaluate social bias and stereotypes in LLMs, question-answering (QA) tasks have been widely applied. Bias benchmarks for various languages in QA tasks have been proposed. The Bias Benchmark for QA (BBQ) is for English (Parrish et al., 2022). Similarly, a Chinese benchmark (CBBQ, Huang and Xiong, 2024) and a Korean benchmark (KoBBQ, Jin et al., 2024) have been developed. These QA benchmarks have hand-written contexts that target attested social biases against several different socially relevant categories. The categories of bias measurement have cultural relativity. For example, BBQ is rooted in US culture. However, there are cultural differences in the way socioeconomic status and religion are perceived.",
            "score": 0.6261351055330069,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 530
                },
                {
                    "start": 533,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1158
                },
                {
                    "start": 1161,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1406
                },
                {
                    "start": 1407,
                    "end": 1537
                },
                {
                    "start": 1538,
                    "end": 1675
                },
                {
                    "start": 1676,
                    "end": 1736
                },
                {
                    "start": 1737,
                    "end": 1778
                },
                {
                    "start": 1779,
                    "end": 1878
                }
            ],
            "ref_mentions": [
                {
                    "start": 140,
                    "end": 162,
                    "matchedPaperCorpusId": "218971825"
                },
                {
                    "start": 488,
                    "end": 509,
                    "matchedPaperCorpusId": "221878771"
                },
                {
                    "start": 509,
                    "end": 529,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 583,
                    "end": 606,
                    "matchedPaperCorpusId": "13756572"
                },
                {
                    "start": 606,
                    "end": 624,
                    "matchedPaperCorpusId": "4952494"
                },
                {
                    "start": 624,
                    "end": 644,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 644,
                    "end": 660,
                    "matchedPaperCorpusId": "222141056"
                },
                {
                    "start": 660,
                    "end": 680,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 680,
                    "end": 701,
                    "matchedPaperCorpusId": "231719337"
                },
                {
                    "start": 701,
                    "end": 722,
                    "matchedPaperCorpusId": "239010011"
                },
                {
                    "start": 722,
                    "end": 742,
                    "matchedPaperCorpusId": "248780290"
                },
                {
                    "start": 742,
                    "end": 764,
                    "matchedPaperCorpusId": "259274987"
                },
                {
                    "start": 1028,
                    "end": 1047,
                    "matchedPaperCorpusId": "269449465"
                },
                {
                    "start": 1383,
                    "end": 1405,
                    "matchedPaperCorpusId": "239010011"
                },
                {
                    "start": 1445,
                    "end": 1467,
                    "matchedPaperCorpusId": "259274987"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.49755859375
        },
        {
            "corpus_id": "265212726",
            "title": "Social Bias Probing: Fairness Benchmarking for Language Models",
            "text": "To this end, we generate our probing dataset by com-arXiv:2311.09090v4 [cs.CL] 7 Oct 2024 bining stereotypes from the SOCIAL BIAS INFER-ENCE CORPUS (SBIC; Sap et al. 2020) and identities from the lexicon by Czarnowska et al. (2021). In this paper we examine identities belonging to four social categories: gender, religion, disability, and nationality. Secondly, we assess social biases across five state-of-the-art LMs in English. We use perplexity (Jelinek et al., 1977), a measure of language model uncertainty, as a proxy for bias. By analyzing the variation in perplexity when probes feature different identities within the diverse social categories, we infer which identities are deemed most likely by a model. This approach facilitates a three-dimensional analysis -by social category, identity, and stereotype-across the evaluated LMs. In summary, the contributions of this work are: \n\n\u2022 We conceptually facilitate fairness benchmarking across multiple identities using our Social Bias Probing framework, going beyond the binary approach of a stereotypical and an antistereotypical identity. \u2022 We introduce SOFA (Social Fairness), a benchmark for fairness probing addressing limitations of existing datasets, including a variety of different identities and stereotypes.1 \u2022 We assess social biases in five autoregressive causal language modeling architectures by examining disparate treatment across social categories, identities, and stereotypes. A comparative analysis with the popular benchmarks CROWS-PAIRS (Nangia et al., 2020) and STEREOSET (Nadeem et al., 2021) reveals marked differences in the overall fairness ranking of the models, providing a different view on the social biases encoded in LMs. We further find how identities expressing religions lead to the most pronounced disparate treatments across all models, while the different nationalities appear to induce the least variation compared to the other examined categories, namely gender and disability. We hypothesize that the increased visibility of religious disparities in language models may stem from recent successful efforts to mitigate racial and gender biases. This underscores the urgency for a comprehensive investigation into biases across multiple dimensions.",
            "score": 0.6254129495633101,
            "section_title": "Introduction",
            "char_start_offset": 1807,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 70
                },
                {
                    "start": 71,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 891
                },
                {
                    "start": 894,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1454
                },
                {
                    "start": 1455,
                    "end": 1713
                },
                {
                    "start": 1714,
                    "end": 1977
                },
                {
                    "start": 1978,
                    "end": 2144
                },
                {
                    "start": 2145,
                    "end": 2247
                }
            ],
            "ref_mentions": [
                {
                    "start": 155,
                    "end": 170,
                    "matchedPaperCorpusId": "207853290"
                },
                {
                    "start": 207,
                    "end": 231,
                    "matchedPaperCorpusId": "235658325"
                },
                {
                    "start": 450,
                    "end": 472,
                    "matchedPaperCorpusId": "121680873"
                },
                {
                    "start": 1518,
                    "end": 1539,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 1554,
                    "end": 1574,
                    "matchedPaperCorpusId": "215828184"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82958984375
        },
        {
            "corpus_id": "272826949",
            "title": "Evaluating Gender, Racial, and Age Biases in Large Language Models: A Comparative Analysis of Occupational and Crime Scenarios",
            "text": "Similarly, existing debiasing techniques [5] often depend on external knowledge with potential bias or annotated non-biased samples, which are not always available or practical to obtain. Further, fine-tuning methods can exacerbate the problem by causing LLMs to overfit dataset biases and shortcuts, leading to poor generalization performance on unseen data. These limitations restrict the effectiveness of debiasing efforts and result in residual bias that affect the model's performance in real-world applications. \n\nThis paper undertakes a comprehensive exploration of bias within four leading Large Language Models -Gemini 1.5 pro, Llama3 70b, Claude 3 Opus, and GPT-4o focusing on two main types of bias 1) Gender bias in Occupational scenarios and 2) Gender, Age, and Racial bias exhibited in crime scenarios. Through benchmarking with real-world statistics for the US region, this study demonstrates the extent of bias exhibited by the latest LLM models despite the recent use of II. RELATED WORK In recent years, the issue of bias in large language models (LLMs) has gathered significant attention. Gender bias and stereotypes have been observed in LLMs, particularly in their predictions related to occupations. [6] introduced a new paradigm, distinct from the WinoBias dataset, to assess this bias by testing four LLMs. Their study revealed that LLMs are 3-6 times more likely to assign occupations that align with traditional gender stereotypes and often justify these biases with flawed rationalizations. Similarly, Thakur [3] investigates gender bias in professional contexts by analyzing GPT-2 and GPT-3.5, highlighting disparities in occupational associations within LLM-generated outputs. Thakur identifies gendered word associations and biased narratives within the generated text, proposing algorithmic and data-driven strategies to mitigate these biases while emphasizing the importance of ethical considerations and responsible AI development. Both studies highlight the need for broader research to address issues like intersectionality, dataset bias, and user-centric approaches in LLMs. \n\nNumerous methods have been developed to evaluate and mitigate these biases, across various dimensions including political bias, gender bias, and racial bias which can significantly impact the fairness and reliability of AI systems.",
            "score": 0.6253611483846091,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 2027,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 517
                },
                {
                    "start": 520,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1107
                },
                {
                    "start": 1108,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1705
                },
                {
                    "start": 1706,
                    "end": 1964
                },
                {
                    "start": 1965,
                    "end": 2110
                },
                {
                    "start": 2113,
                    "end": 2344
                }
            ],
            "ref_mentions": [
                {
                    "start": 41,
                    "end": 44,
                    "matchedPaperCorpusId": "261530629"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77490234375
        },
        {
            "corpus_id": "239010011",
            "title": "BBQ: A hand-built bias benchmark for question answering",
            "text": "Large language models (LMs) learn social biases present in the world, and the increased use of these systems in everyday life increases the contexts in which these biases will lead to harm. LMs have been found to reproduce social biases in downstream tasks, such as textual entailment (Dev et al., 2020;Sharma et al., 2021), language generation (Sheng et al., 2019), and coreference resolution (Rudinger et al., 2018). The use of these models in real-world applications therefore risks harming marginalized individuals and groups. However, little work has been done to understand how these biases manifest in the outputs of question-answering Figure 1: Examples in BBQ are presented with each context in (A) (ambiguous context only, and ambiguous + disambiguated context) and each question in (B) (negative question, non-negative question), yielding four separate examples, then repeated with the phrases \"Christian\" and \"Muslim\" switched. The correct answers for this particular set of questions are shown in (C). Each example is presented as a multiple-choice question with three choices. In this case, all questions have the options \"The Christian\", \"The Muslim\", and \"Unknown\".\n\n(QA) models. To assess the degree to which QA models reproduce such biases in their outputs, we need a tool that can both measure biases against a range of social categories and also measure in which contexts these impacts are most likely to be felt.\n\nThere are many, often conflicting, ways that bias is defined in NLP (Blodgett et al., 2020); we focus on stereotyping behavior and extend the definitions and treatment of bias in QA from , who have shown that the marginal probabilities a model associates with different answer options are related to positive or negative associations made to arXiv:2110.08193v1 [cs.CL]  The 78-YEAR-OLD was eagerly greeting everyone, but the 22-YEAR-OLD held back because they could not remember some members' names.\n\nWho was forgetful?\n\nWho was not forgetful?\n\nOlder adults in cognitive decline [source] Disability status\n\nOn that day, two people came into the lawyer's office, one CARRYING A STACK OF PAPERS and",
            "score": 0.6253543107674803,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 285,
                    "end": 303,
                    "matchedPaperCorpusId": "201670701"
                },
                {
                    "start": 303,
                    "end": 323,
                    "matchedPaperCorpusId": "234469667"
                },
                {
                    "start": 394,
                    "end": 417,
                    "matchedPaperCorpusId": "13756572"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59521484375
        },
        {
            "corpus_id": "236950797",
            "title": "Bias Out-of-the-Box: An Empirical Analysis of Intersectional Occupational Biases in Popular Generative Language Models",
            "text": "The advent of deep learning and massive growth in training data have led to natural language models surpassing humans on numerous benchmarks [1,22,39,40]. However, as Bender et al. [7] states, these models can exacerbate existing biases in data and perpetuate stereotypical associations to the harm of marginalized communities. Simultaneously, pre-trained models have become readily accessible via open source libraries such as HuggingFace, allowing non-experts to apply these tools in their own applications. These developments in generative language models substantiate a need to understand the potential for biases towards protected classes, such as gender and ethnicity. This paper considers potential biases present in the most popular and most downloaded versions of large-scale, open sourced text generation models applied 'out-of-the-box'. Despite the release of newer and larger models often redirecting researchers' attention, there exist important research gaps in existing models. \n\nBearing in mind that the potential negative total impact from biased models is correlated with number of downloads of that model, this paper tests the biases in the small GPT-2 model, which is the most downloaded text generation model on HuggingFace with over half a million downloads in the month of May 2021 alone. These numbers motivate further research on the biases of these models given their increased use in hiring related downstream tasks, such as chatbots or unsupervised scanning of CVs and applications [30]. Within this context, specifying which biases to analyze is crucial; Blodgett et al. [9] find that a majority of NLP papers investigating bias are unclear in their articulations of bias. In this paper, we consider both representational and allocational harms [4]. We attempt to elucidate representational harms, or those harmful in their own right, by highlighting occupation-related stereotypes that may propagate negative generalizations about particular social groups. For example, women's higher likelihood of being associated with care-oriented occupations may perpetuate unwanted stereotypes. Especially within the context of occupations, such associations may lead to allocation harms.",
            "score": 0.6251090760360204,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 992
                },
                {
                    "start": 995,
                    "end": 1311
                },
                {
                    "start": 1312,
                    "end": 1515
                },
                {
                    "start": 1516,
                    "end": 1701
                },
                {
                    "start": 1702,
                    "end": 1778
                },
                {
                    "start": 1779,
                    "end": 1986
                },
                {
                    "start": 1987,
                    "end": 2113
                },
                {
                    "start": 2114,
                    "end": 2207
                }
            ],
            "ref_mentions": [
                {
                    "start": 147,
                    "end": 150,
                    "matchedPaperCorpusId": "5034059"
                },
                {
                    "start": 181,
                    "end": 184,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 1510,
                    "end": 1514,
                    "matchedPaperCorpusId": "201812324"
                },
                {
                    "start": 1600,
                    "end": 1603,
                    "matchedPaperCorpusId": "218971825"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63671875
        },
        {
            "corpus_id": "244709137",
            "title": "Identification of Bias Against People with Disabilities in Sentiment Analysis and Toxicity Detection Models",
            "text": "Our aim was to understand the presence of disability bias in sentiment analysis models. For this, we looked into how social posts in online social media platforms are skewed against PWD. The results showed that posts related to PWD were scored significantly lower in sentiment and toxicity scores as compared to posts not related to PWD. The results also showed how sentiment models built on social media dataset were most discriminatory against PWD. \n\nFollowing this, we created a test that checks the presence of disability bias in any sentiment analysis model. We explained the process of creating this facet of the Bias Identification Test in Sentiments (BITS Corpus). The flexible nature of the BITS Corpus provides an easy method to make more tests based on the template created for the Disability Facet. The same process can be used to create nuanced analysis for other sociodemographic biases as well. We have released every facet of BITS Corpus6 . We also intend, through this work, to create awareness of the presence of bias in public sentiment analysis libraries. We show the presence of disability biases in notable public sentiment analysis library using this test. From the results, the presence of disability bias was seen on all the models that we had considered. \n\nThis work looks at providing the first step to eliminate unintended bias in sentiment analysis, through identification. Some possible means of mitigation can be done by changing the rules that these models are defined with, or changing or training the models with better data that represents such sensitive populations [1,52]. A model, if used in a social environment, must be aware of the all the actors that are present in the social network. Hence the dataset used to train the model is very crucial. Through this paper, we show how non-inclusive training can be harmful for certain population, PWD, in various social applications. Through the proposed biased identification process, we can be more aware of the ramification AI applications can have on selected sociodemographic factors. \n\nFor our future work, we will concentrate on making BITS Corpus larger by adding more templates to cover various simple scenarios. We also intend to add more facets to BITS Corpus so that it can adhere to a bigger scope, to find various types of biases. A few sociodemographic factors in consideration are race, gender and socio-economic status.",
            "score": 0.6249058375899764,
            "section_title": "Discussion and Conclusion",
            "char_start_offset": 33012,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 88,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 450
                },
                {
                    "start": 453,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 810
                },
                {
                    "start": 811,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1280
                },
                {
                    "start": 1283,
                    "end": 1402
                },
                {
                    "start": 1403,
                    "end": 1609
                },
                {
                    "start": 1610,
                    "end": 1727
                },
                {
                    "start": 1728,
                    "end": 1786
                },
                {
                    "start": 1787,
                    "end": 1917
                },
                {
                    "start": 1918,
                    "end": 2073
                },
                {
                    "start": 2076,
                    "end": 2205
                },
                {
                    "start": 2206,
                    "end": 2328
                },
                {
                    "start": 2329,
                    "end": 2420
                }
            ],
            "ref_mentions": [
                {
                    "start": 1602,
                    "end": 1605,
                    "matchedPaperCorpusId": "1704893"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.533203125
        },
        {
            "corpus_id": "264306211",
            "title": "Identifying and Adapting Transformer-Components Responsible for Gender Bias in an English Language Model",
            "text": "Modern neural language models exhibit social biases, such as biases based on gender, religion, ethnicity and other protected attributes. These biases may lead to real harms when used in downstream applications (e.g. Hovy and Spruit, 2016;Weidinger et al., 2021). Detecting and mitigating biases in language models has therefore become an important area of research. \n\nEarly detection methods relied on lists of words to measure associations with e.g., specific genders (e.g. Caliskan et al., 2017). Most current detection * Shared senior authorship. methods work with curated sets of sentence pairs or triplets, and measure differences in sentence probabilities or anaphora resolution probabilities (e.g. May et al., 2019;Nadeem et al., 2021;Nangia et al., 2020;Basta et al., 2019). Proposed mitigation strategies include targeted changes to the training data (e.g., CDA; Lu et al., 2020), training procedure (e.g., adversarial learning; Zhang et al., 2018), model parameters (e.g., INLP;Ravfogel et al., 2020), or language generation procedure (e.g., \"self-debiasing\"; Schick et al., 2021). \n\nDespite this work, we still lack a proper understanding of how to best measure biases (how do we guarantee the representativeness for real-world harm of a set of sentence pairs, or of a linguistic phenomenon such as anaphora resolution?), how biases are implemented in the language model internals (is there a unified locus, or is, e.g., gender bias the aggregate effect of many independent model decisions?), and what techniques are effective at reducing undesirable downstream behavior (e.g., is data curation more or less effective than filtering output? Is intervening in the model internals feasible?).",
            "score": 0.6243128718792395,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 365
                },
                {
                    "start": 368,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 782
                },
                {
                    "start": 783,
                    "end": 1091
                },
                {
                    "start": 1094,
                    "end": 1503
                },
                {
                    "start": 1504,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1701
                }
            ],
            "ref_mentions": [
                {
                    "start": 216,
                    "end": 238,
                    "matchedPaperCorpusId": "1083991"
                },
                {
                    "start": 475,
                    "end": 497,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 705,
                    "end": 722,
                    "matchedPaperCorpusId": "85518027"
                },
                {
                    "start": 722,
                    "end": 742,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 742,
                    "end": 762,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 762,
                    "end": 781,
                    "matchedPaperCorpusId": "121125604"
                },
                {
                    "start": 1070,
                    "end": 1090,
                    "matchedPaperCorpusId": "232075876"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6337890625
        },
        {
            "corpus_id": "273403663",
            "title": "BenchmarkCards: Standardized Documentation for Large Language Model Benchmarks",
            "text": "To illustrate the complexity issue, we examine several benchmarks designed to assess these risks. For example, Winogrande (Sakaguchi et al., 2021) focuses on general commonsense reasoning and pronoun resolution, Winogender (Gallegos et al., 2024), WinoBias (Zhao et al., 2018) and Winopron (Gautam et al., 2024) focuses on gender bias by evaluating how models handle gender-specific pronouns and roles in sentences. It checks whether the model can accurately predict the gender of a person based on context without falling into stereotypes. Additionally, WinoQueer (Felkner et al., 2023) assesses biases in LGBTQ+ contexts. This dataset evaluates how models handle sentences involving sexual orientation and gender identity, checking for any presence of bias or derogatory language. \n\nSimilarly, RealToxicityPrompts (Gehman et al., 2020) identifies toxic language, especially related to targeting marginalized groups. It measures how likely a model is to generate harmful or offensive content when given prompts related to these groups, ensuring that the model does not propagate derogatory language. \n\nMoreover, BBQ (Park et al., 2023) evaluates fairness in responses concerning age, physical appearance, race, ethnicity, gender identity, sexual orientation, religion, disability, and socioeconomic status. It tests how models respond to questions about these attributes, ensuring that answers are unbiased and do not reinforce harmful stereotypes. \n\nOther examples include Bias NLI (Dev et al., 2020), which focuses on biases arising from linguistic nuances in natural language inference tasks; StereoSet (Nadeem et al., 2020), which measures stereotypical biases across multiple demographic groups; TrustGPT (Huang et al., 2023), designed to evaluate various aspects of trustworthiness, including bias, toxicity and value-alignment; and Holis-ticBias (Smith et al., 2022), a large-scale dataset for measuring a wide range of biases across different social groups. .",
            "score": 0.6241825771064656,
            "section_title": "LLM Risk Assesment",
            "char_start_offset": 10356,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 782
                },
                {
                    "start": 785,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 1100
                },
                {
                    "start": 1103,
                    "end": 1307
                },
                {
                    "start": 1308,
                    "end": 1449
                },
                {
                    "start": 1452,
                    "end": 1968
                }
            ],
            "ref_mentions": [
                {
                    "start": 122,
                    "end": 146,
                    "matchedPaperCorpusId": "199370376"
                },
                {
                    "start": 223,
                    "end": 246,
                    "matchedPaperCorpusId": "261530629"
                },
                {
                    "start": 1117,
                    "end": 1136,
                    "matchedPaperCorpusId": "258040990"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65625
        },
        {
            "corpus_id": "245537923",
            "title": "A Survey on Gender Bias in Natural Language Processing",
            "text": "Above we have introduced gender bias and sexism as general terms. In the following, we discuss how these biases emerge in natural language and ultimately influence many downstream tasks. \n\nLanguage can be used as a substantial means of expressing gender bias. Gender biases are translated from source data to existing algorithms that may reflect and amplify existing cultural prejudices and inequalities by replicating human behavior and perpetuating bias [Sweeney 2013]. This phenomenon is not unique to NLP, but the lure of making general claims with big data, coupled with NLP's semblance of objectivity, makes it a particularly pressing topic for the discipline [Koolen and van Cranenburgh 2017]. \n\nAlongside the types of biases described above, there are forms of bias that apply specifically in NLP research. In particular, Hitti et al. [2019] define gender bias in a text as the use of words or syntactic constructs that connote or imply an inclination or prejudice against one gender. Further, Hitti et al. [2019] note that gender bias can manifest itself structurally, contextually or in both of these forms. Structural bias arises when the construction of sentences shows patterns that are closely tied to the presence of gender bias. It encompasses gender generalisation (i.e., when a gender-neutral term is assumed to refer to a specific gender-based on some (stereotypical) assumptions) and explicit labeling of sex. On the other hand, contextual bias manifests itself in a tone, the words used, or the context of a sentence. Unlike structural bias, this type of bias cannot be observed through grammatical structure but requires contextual background information and human perception. Contextual bias can be divided into societal stereotypes (which showcase traditional gender roles that reflect social norms) and behavioral stereotypes (attributes and traits used to describe a specific person or gender). Therefore, gender bias can be detected using both linguistic and extra-linguistic cues, and can manifest itself with different intensities, which can be subtle or explicit, posing a challenge in this line of research. \n\nGender bias is known to perpetuate to models and downstream tasks posing harm for the endusers [Bolukbasi et al. 2016]. These harms can emerge as representational and allocational harms and gender gaps.",
            "score": 0.6233082745599554,
            "section_title": "Bias in NLP",
            "char_start_offset": 17235,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 65
                },
                {
                    "start": 66,
                    "end": 186
                },
                {
                    "start": 189,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 700
                },
                {
                    "start": 703,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1429
                },
                {
                    "start": 1430,
                    "end": 1538
                },
                {
                    "start": 1539,
                    "end": 1698
                },
                {
                    "start": 1699,
                    "end": 1920
                },
                {
                    "start": 1921,
                    "end": 2138
                },
                {
                    "start": 2141,
                    "end": 2260
                },
                {
                    "start": 2261,
                    "end": 2343
                }
            ],
            "ref_mentions": [
                {
                    "start": 666,
                    "end": 699,
                    "matchedPaperCorpusId": "11405869"
                },
                {
                    "start": 830,
                    "end": 849,
                    "matchedPaperCorpusId": "201685203"
                },
                {
                    "start": 1002,
                    "end": 1021,
                    "matchedPaperCorpusId": "201685203"
                },
                {
                    "start": 2236,
                    "end": 2258,
                    "matchedPaperCorpusId": "1704893"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6513671875
        },
        {
            "corpus_id": "263333947",
            "title": "Unlocking Bias Detection: Leveraging Transformer-Based Models for Content Analysis",
            "text": "A S Natural Language Processing (NLP) rapidly evolves, its significance extends well beyond text analysis. NLP influences diverse sectors, ranging from social media analytics to advanced healthcare diagnostics. The pervasive reach of NLP showcases not only its achievements, but also highlights vital challenges. Among these challenges, linguistic biases [1], which are often embedded in both data and algorithms, are a significant concern. These biases do more than just perpetuate stereotypes; they risk distorting data interpretations, affecting decision-making processes. \n\nAs seen in Figure 1, the first statement displays a genderbased bias, while the second one reflects a bias for ableism. Here, \"bias\" refers to the predisposition or inclination towards a particular group (based on gender or race), often rooted in societal stereotypes, that can unduly influence the representation or treatment of particular groups [2]. These biases are often manifested in the tone, choice of words, or conveyed messages in textual conversations. A conventional NLP model primarily trained on a specific domain, such as political discourse, may detect bias within that domain but would be hesitant when presented with similar biases in a health-centric context. This inconsistency of traditional NLP Despite significant advancements in state-of-the-art models [3]- [8], consistent bias identification across diverse domains remains an ongoing challenge. Some recent research has mainly focused on evaluating and debiasing language models (LMs) [9]- [11] -which is indeed a critical step toward minimizing AI risks -the pressing need to detect biases inherent in the data itself, persists [12]. This highlights the urgency for a holistic NLP framework that not only stands up to academic observation, but also ensures ethical and accurate data interpretation. \n\nTo address this issue, our research introduces the novel Contextualized Bi-Directional Dual Transformer (CBDT) classifier. At its core, the CBDT integrates two specialized transformer-based LMs: the Context Transformer and the Entity Transformer. The former assesses the overall bias in an input text, while the latter focuses on particular lexicons in the text to identify potentially biased words and entities.",
            "score": 0.62321423829796,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 575
                },
                {
                    "start": 578,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1041
                },
                {
                    "start": 1042,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1688
                },
                {
                    "start": 1689,
                    "end": 1853
                },
                {
                    "start": 1856,
                    "end": 1978
                },
                {
                    "start": 1979,
                    "end": 2102
                },
                {
                    "start": 2103,
                    "end": 2268
                }
            ],
            "ref_mentions": [
                {
                    "start": 355,
                    "end": 358,
                    "matchedPaperCorpusId": "247050244"
                },
                {
                    "start": 926,
                    "end": 929,
                    "matchedPaperCorpusId": "334423"
                },
                {
                    "start": 1355,
                    "end": 1358,
                    "matchedPaperCorpusId": "120178733"
                },
                {
                    "start": 1360,
                    "end": 1363,
                    "matchedPaperCorpusId": "257079697"
                },
                {
                    "start": 1539,
                    "end": 1542,
                    "matchedPaperCorpusId": "253116931"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76806640625
        },
        {
            "corpus_id": "268856634",
            "title": "Will the Real Linda Please Stand up...to Large Language Models? Examining the Representativeness Heuristic in LLMs",
            "text": "Social biases in natural language processing (NLP) systems and related data have been studied with respect to their fairness, inclusivity, and accuracy (Hutchinson et al., 2020;Maass, 1999;Zhao et al., 2018).For example, Bolukbasi et al. (2016); Garg et al. (2018) are among the pioneers in demonstrating gender-related associations in word embeddings that might reflect and perpetuate stereotypes.Caliskan et al. (2017) concludes standard machine learning methods for NLP could acquire societal biases from textual data.Some research has expanded our understanding of where NLP systems might acquire subgroup associations within data, and potential bias, including those from data collection (Bender & Friedman, 2018), annotation processes (Gebru et al., 2018), and model architecture choices (Zhao et al., 2017).To date, numerous efforts have been made to mitigate social biases in systems through a variety of methods, including data augmentation (Lu et al., 2018), changes in model architecture (Liang et al., 2020), and training objectives (Liu et al., 2021;Romanov et al., 2019).\n\nIn a similar vein, although recent advancements in LLMs are exciting, researchers are concerned about whether LLMs inherit social biases from the trillions of tokens they have been trained on.Weidinger et al. (2022) provides a comprehensive taxonomy of social risks within LLMs.Although the research community has documented numerous social biases in LLMs (Ferrara, 2023;Mei et al., 2023), few LLM researchers have examined these biases from the perspective of the psychology of human decision-making.Thus, in the present work, we study the bias issue in LLMs from the new angle of the aforementioned representative heuristic, a concept originating in psychology (Kahneman & Tversky, 1973;Kahneman et al., 1982;Tversky & Kahneman, 1974;1983).",
            "score": 0.6224460419250759,
            "section_title": "Related Work",
            "char_start_offset": 5721,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 208
                },
                {
                    "start": 208,
                    "end": 398
                },
                {
                    "start": 398,
                    "end": 521
                },
                {
                    "start": 521,
                    "end": 814
                },
                {
                    "start": 814,
                    "end": 1085
                },
                {
                    "start": 1087,
                    "end": 1279
                },
                {
                    "start": 1279,
                    "end": 1365
                },
                {
                    "start": 1365,
                    "end": 1588
                },
                {
                    "start": 1588,
                    "end": 1829
                }
            ],
            "ref_mentions": [
                {
                    "start": 152,
                    "end": 177,
                    "matchedPaperCorpusId": "218487466"
                },
                {
                    "start": 177,
                    "end": 189,
                    "matchedPaperCorpusId": "142809360"
                },
                {
                    "start": 221,
                    "end": 244,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 246,
                    "end": 264,
                    "matchedPaperCorpusId": "4930886"
                },
                {
                    "start": 398,
                    "end": 420,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 693,
                    "end": 718,
                    "matchedPaperCorpusId": "52255687"
                },
                {
                    "start": 741,
                    "end": 761,
                    "matchedPaperCorpusId": "4421027"
                },
                {
                    "start": 794,
                    "end": 813,
                    "matchedPaperCorpusId": "1389483"
                },
                {
                    "start": 999,
                    "end": 1019,
                    "matchedPaperCorpusId": "207996257"
                },
                {
                    "start": 1279,
                    "end": 1302,
                    "matchedPaperCorpusId": "249872629"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.537109375
        },
        {
            "corpus_id": "276774344",
            "title": "Unmasking Implicit Bias: Evaluating Persona-Prompted LLM Responses in Power-Disparate Social Scenarios",
            "text": "Large language models (LLMs) have revolutionised natural language processing, enabling AI to understand, generate, and simulate human language and social interactions. These advancements have facilitated new applications in socially interactive domains, such as digital clones (Ng et al., 2024b), AI-driven chatbot characters (Wang et al., 2024), and immersive role-playing in video games (Wang et al., 2023), where the accurate and unbiased simulation of human behaviour is crucial. Moreover, LLMs have opened new avenues for analysing social dynamics, allowing researchers in computational social sciences to study human interactions at scale (Rao et al., 2023). The de-velopment of benchmarks designed to evaluate the social intelligence of persona-prompted LLMs further reflects the growing interest in this field (Zhou et al., 2023). \n\nHowever, LLMs are not immune to the biases embedded in their training data. Biases can be explicit (self-recognised) or implicit (unconscious and unreported) (Zhao et al., 2024). Implicit biases in AI systems are especially concerning due to their subtle influence (Bai et al., 2024). As these models are integrated into socially sensitive applications, concerns about their potential to reinforce and amplify societal biases have intensified (Rudinger et al., 2018;Nadeem et al., 2021). Evidence of these biases spans diverse areas, from language generation and sentiment analysis to reasoning tasks and creative content generation (Kotek et al., 2023;Wan et al., 2023a;Kumar et al., 2024). In socially sensitive domains, such biases can lead to discriminatory outcomes, particularly in critical areas like hiring, healthcare, and law enforcement. \n\nWhile substantial research exists on isolated biases in LLMs-such as gender or racial bias-there is a critical gap in understanding how these biases manifest when power disparities come into play. Power disparities-where one individual holds significant social, economic, or hierarchical advantages over another-are common in real-world scenarios and can exacerbate existing biases within AI models (Gallegos et al., 2024;Sap et al., 2020).",
            "score": 0.6222547781992299,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 838
                },
                {
                    "start": 841,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1125
                },
                {
                    "start": 1126,
                    "end": 1328
                },
                {
                    "start": 1329,
                    "end": 1532
                },
                {
                    "start": 1533,
                    "end": 1689
                },
                {
                    "start": 1692,
                    "end": 1888
                },
                {
                    "start": 1889,
                    "end": 2132
                }
            ],
            "ref_mentions": [
                {
                    "start": 645,
                    "end": 663,
                    "matchedPaperCorpusId": "257279816"
                },
                {
                    "start": 999,
                    "end": 1018,
                    "matchedPaperCorpusId": "269804264"
                },
                {
                    "start": 1284,
                    "end": 1307,
                    "matchedPaperCorpusId": "13756572"
                },
                {
                    "start": 1474,
                    "end": 1494,
                    "matchedPaperCorpusId": "261276445"
                },
                {
                    "start": 1494,
                    "end": 1512,
                    "matchedPaperCorpusId": "264128125"
                },
                {
                    "start": 1512,
                    "end": 1531,
                    "matchedPaperCorpusId": "269983671"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6083984375
        },
        {
            "corpus_id": "267740210",
            "title": "I Am Not Them: Fluid Identities and Persistent Out-group Bias in Large Language Models",
            "text": "Large Language Models (LLMs) are becoming pervasive in various open-domain applications, such as conversational agents, social bots, and professional writing assistants [1,2,3]. However, these systems tend to produce socially biased and sometimes inconsistent responses [4,5]. For example, LLMs are known to poorly represent certain groups that make up a significant portion of the population (e.g., age 65+) [6]. People perceive ChatGPT as predominantly male when asked about its gender, particularly about its core capabilities such as text summarization [7]. Moreover, ChatGPT has been shown to generate gender-biased responses [8] and write gender-stereotypical recommendation letters [4]. Some large language models promote liberal values [9] and present inconsistent responses to political issues when changing question languages from English to simplified Chinese [10]. \n\nThe consequence of such a built-in bias in LLMs can be of growing concern when it comes to high-stakes situations or long-term exposure. Large language models can propagate harmful, inaccurate, and race-based medicine [11], decrease clinical recommendations when specifying patients as female [12], and exhibit bias towards gender and religion in legal predictions [13]. The risk still exists in low-stakes scenarios because prolonged human-LLM interactions may influence user decisions toward a specific viewpoint, as these models' usage will increase and more people will struggle to discern AI-generated language from human-created ones [14]. As a result, quantifying bias in data representation and tracking it over time is crucial. \n\nOne overarching framework for explaining such bias is the discriminative perception of in-group versus out-group in social identity theory, which describes the degree to which individuals psychologically align themselves with a given group [15,16]. According to the theory, people tend to perceive out-group members as more homogenized and less diverse than in-groups [17], resulting in prejudices and discriminations towards the out-group [18]. People also have different preferences over biased content, where out-group social media posts could get circulated twice as much as in-group contents [19].",
            "score": 0.6208100295864319,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 876
                },
                {
                    "start": 879,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1524
                },
                {
                    "start": 1525,
                    "end": 1615
                },
                {
                    "start": 1618,
                    "end": 1866
                },
                {
                    "start": 1867,
                    "end": 2063
                },
                {
                    "start": 2064,
                    "end": 2220
                }
            ],
            "ref_mentions": [
                {
                    "start": 169,
                    "end": 172,
                    "matchedPaperCorpusId": "264039226"
                },
                {
                    "start": 174,
                    "end": 176,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 270,
                    "end": 273,
                    "matchedPaperCorpusId": "264128125"
                },
                {
                    "start": 273,
                    "end": 275,
                    "matchedPaperCorpusId": "266176951"
                },
                {
                    "start": 409,
                    "end": 412,
                    "matchedPaperCorpusId": "257834040"
                },
                {
                    "start": 631,
                    "end": 634,
                    "matchedPaperCorpusId": "264490615"
                },
                {
                    "start": 689,
                    "end": 692,
                    "matchedPaperCorpusId": "264128125"
                },
                {
                    "start": 744,
                    "end": 747,
                    "matchedPaperCorpusId": "258686693"
                },
                {
                    "start": 1097,
                    "end": 1101,
                    "matchedPaperCorpusId": "264378028"
                },
                {
                    "start": 1244,
                    "end": 1248,
                    "matchedPaperCorpusId": "266202838"
                },
                {
                    "start": 1519,
                    "end": 1523,
                    "matchedPaperCorpusId": "249674779"
                },
                {
                    "start": 1986,
                    "end": 1990,
                    "matchedPaperCorpusId": "143435833"
                },
                {
                    "start": 2058,
                    "end": 2062,
                    "matchedPaperCorpusId": "145772550"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71826171875
        },
        {
            "corpus_id": "259075978",
            "title": "Exposing Bias in Online Communities through Large-Scale Language Models",
            "text": "Large language generation models such as GPT-2 (Radford et al. 2019) and GPT-3 (Brown et al. 2020) have been shown to exhibit various kinds of representational bias, including racial, gender, and religious bias (Sheng et al. 2019;Abid, Farooqi, and Zou 2021;Sheng et al. 2021). They do not only absorb the biases present in their source material but even tend to amplify them. Dialogue generation models, in particular, are often trained on social media datasets that contain unchecked user-generated content that is prone to toxicity. These models have an immediate user impact and can thus play a more significant role in propagating harmful biases directly (Sheng et al. 2021). \n\nDeploying biased language models in real-world contexts can lead to numerous potential negative consequences, even without malicious intent. They create more text that links marginalised communities to problematic stereotypes (Bender et al. 2021). Whether in media or personal conversations, language is the main source through which bias is shared amongst people (Beukeboom and Burgers 2019). Text created by biased language models can thus contribute to the perpetuation of bias in society. Furthermore, people from marginalised groups might be discouraged from using these technologies and from reaping the benefits artificial intelligence brings to society (Sheng et al. 2021). \n\nHowever, this flaw of language models creates an opportunity to analyse the stereotypes present in datasets. Language generation models ingest the contents of their training data. If we train a model on a biased dataset, the model should exhibit similar biases. Therefore, one can assess the biases and viewpoints expressed in the underlying datasets by evaluating the model's bias. \n\nThis work introduces an automated, reproducible method to compare the bias of different datasets or even of different communities. We fine-tune a pre-trained large language model with six social media datasets representing six different online communities. We obtain one fine-tuned model per dataset, which we then evaluate for bias using sentiment and toxicity values. The goal is to gain insight into these communities' attitudes by examining a language model that is based on conversations within the community.",
            "score": 0.6197133503149725,
            "section_title": "Introduction",
            "char_start_offset": 1994,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 680
                },
                {
                    "start": 683,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1364
                },
                {
                    "start": 1367,
                    "end": 1475
                },
                {
                    "start": 1476,
                    "end": 1546
                },
                {
                    "start": 1547,
                    "end": 1628
                },
                {
                    "start": 1629,
                    "end": 1749
                },
                {
                    "start": 1752,
                    "end": 1882
                },
                {
                    "start": 1883,
                    "end": 2008
                },
                {
                    "start": 2009,
                    "end": 2121
                },
                {
                    "start": 2122,
                    "end": 2266
                }
            ],
            "ref_mentions": [
                {
                    "start": 79,
                    "end": 97,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 211,
                    "end": 230,
                    "matchedPaperCorpusId": "202537041"
                },
                {
                    "start": 230,
                    "end": 258,
                    "matchedPaperCorpusId": "231603388"
                },
                {
                    "start": 258,
                    "end": 276,
                    "matchedPaperCorpusId": "202537041"
                },
                {
                    "start": 660,
                    "end": 679,
                    "matchedPaperCorpusId": "202537041"
                },
                {
                    "start": 909,
                    "end": 929,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 1047,
                    "end": 1075,
                    "matchedPaperCorpusId": "53622943"
                },
                {
                    "start": 1344,
                    "end": 1363,
                    "matchedPaperCorpusId": "202537041"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8486328125
        },
        {
            "corpus_id": "218487627",
            "title": "Multi-Dimensional Gender Bias Classification",
            "text": "Language is a social behavior, and as such, it is a primary means by which people communicate, express their identities, and socially categorize themselves and others. Such social information is present in the words we write and, consequently, in the text we use to train our NLP models. In particular, models often can unwittingly learn negative associations about protected groups present in their training data and propagate them. In particular, NLP models often learn biases against others based on their gender (Bolukbasi et al., 2016;Hovy and Spruit, 2016;Caliskan et al., 2017;Rudinger et al., 2017;Garg et al., 2018;Gonen and Goldberg, 2019;Dinan et al., 2019a). Since unwanted We propose a framework separating gendered language based on who you are speaking ABOUT, speaking TO, and speaking AS. \n\ngender biases can affect downstream applicationssometimes even leading to poor user experiencesunderstanding and mitigating gender bias is an important step towards making NLP tools and models safer, more equitable, and more fair. We provide a finer-grained framework for this purpose, analyze the presence of gender bias in models and data, and empower others by releasing tools that can be employed to address these issues for numerous text-based use-cases. \n\nWhile many works have explored methods for removing gender bias from text (Bolukbasi et al., 2016;Emami et al., 2019;Maudslay et al., 2019;Dinan et al., 2019a;Kaneko and Bollegala, 2019;Zmigrod et al., 2019;Ravfogel et al., 2020), no extant work on classifying gender or removing gender bias has incorporated facts about how humans collaboratively and socially construct our language and identities. We propose a pragmatic and semantic framework for measuring bias along three dimensions that builds on knowledge of the conversational and performative aspects of gender, as illustrated in Figure 1. Recognizing these dimensions is important, because gender along each dimension can affect text differently, for example, by modifying word choice or imposing different preferences in how we construct sentences.",
            "score": 0.6196823851432262,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 804
                },
                {
                    "start": 807,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1266
                },
                {
                    "start": 1269,
                    "end": 1668
                },
                {
                    "start": 1669,
                    "end": 1867
                },
                {
                    "start": 1868,
                    "end": 2078
                }
            ],
            "ref_mentions": [
                {
                    "start": 516,
                    "end": 540,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 540,
                    "end": 562,
                    "matchedPaperCorpusId": "1083991"
                },
                {
                    "start": 562,
                    "end": 584,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 584,
                    "end": 606,
                    "matchedPaperCorpusId": "5310359"
                },
                {
                    "start": 606,
                    "end": 624,
                    "matchedPaperCorpusId": "4930886"
                },
                {
                    "start": 624,
                    "end": 649,
                    "matchedPaperCorpusId": "73729169"
                },
                {
                    "start": 1343,
                    "end": 1367,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 1367,
                    "end": 1386,
                    "matchedPaperCorpusId": "189897682"
                },
                {
                    "start": 1455,
                    "end": 1476,
                    "matchedPaperCorpusId": "184486914"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6171875
        },
        {
            "corpus_id": "259129801",
            "title": "Bias Against 93 Stigmatized Groups in Masked Language Models and Downstream Sentiment Classification Tasks",
            "text": "Caliskan et al. [7] demonstrate that word embeddings and language models (LMs) trained on a large amount of human-generated texts encode human-like social biases. Social biases encoded in these models are also reflected in their downstream tasks such as machine translation, sentiment classification, and natural language generation [1,[22][23][24]. As the downstream tasks of language models are rapidly deployed for real-world applications, the presence of social biases in these models reinforces social stereotypes, discrimination, and inequalities. Despite enormous efforts in bias evaluation of LMs, prior work extensively focuses on biases related to gender, race, and ethnicity [5,22,24,50,52]. Social stigmas, also an element of social biases, are stigmatized conditions that often relate to diseases, disabilities, mental illness, socioeconomic status, etc [36]. Considering all stigmatized conditions, social stigmas affect a substantial amount of people. In the United States, approximately 26 percent of adults experience a disability, with up to one in four individuals being affected . In 2021, there were around 57.8M adults that experienced mental illness, which was around 22% of the population in the United States . Social stigmas prevent individuals from social activities and access to education, healthcare, and career opportunities, negatively influencing their psychological well-being and life outcomes [14,[31][32][33]37]. As language models capture other social biases, they may also learn bias against socially stigmatized groups. Such a risk would reinforce social inequalities with the rise of real-world applications of LMs. \n\nThis study examines bias against 93 stigmatized groups in the United States. To the best of our knowledge, this is the first study that examines social stigmas in LMs on a large scale. Pachankis et al. [36] conduct the first psychology study that classifies 93 social stigmas along six stigma dimensions and evaluates their interpersonal outcome, social rejection. We adapt their list of these 93 social stigmas and a widely used psychological questionnaire that measures social rejection, the Social Distance Scale, to quantify bias against stigmatized groups.",
            "score": 0.6189412311883326,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1235
                },
                {
                    "start": 1236,
                    "end": 1449
                },
                {
                    "start": 1450,
                    "end": 1559
                },
                {
                    "start": 1560,
                    "end": 1656
                },
                {
                    "start": 1659,
                    "end": 1735
                },
                {
                    "start": 1736,
                    "end": 1843
                },
                {
                    "start": 1844,
                    "end": 2023
                },
                {
                    "start": 2024,
                    "end": 2220
                }
            ],
            "ref_mentions": [
                {
                    "start": 16,
                    "end": 19,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 333,
                    "end": 336,
                    "matchedPaperCorpusId": "231603388"
                },
                {
                    "start": 336,
                    "end": 340,
                    "matchedPaperCorpusId": "250391069"
                },
                {
                    "start": 344,
                    "end": 348,
                    "matchedPaperCorpusId": "190000105"
                },
                {
                    "start": 686,
                    "end": 689,
                    "matchedPaperCorpusId": "102352788"
                },
                {
                    "start": 689,
                    "end": 692,
                    "matchedPaperCorpusId": "250391069"
                },
                {
                    "start": 692,
                    "end": 695,
                    "matchedPaperCorpusId": "190000105"
                },
                {
                    "start": 695,
                    "end": 698,
                    "matchedPaperCorpusId": "227275068"
                },
                {
                    "start": 698,
                    "end": 701,
                    "matchedPaperCorpusId": "238259136"
                },
                {
                    "start": 867,
                    "end": 871,
                    "matchedPaperCorpusId": "3750353"
                },
                {
                    "start": 1429,
                    "end": 1433,
                    "matchedPaperCorpusId": "21883193"
                },
                {
                    "start": 1433,
                    "end": 1437,
                    "matchedPaperCorpusId": "145255089"
                },
                {
                    "start": 1437,
                    "end": 1441,
                    "matchedPaperCorpusId": "246654106"
                },
                {
                    "start": 1441,
                    "end": 1445,
                    "matchedPaperCorpusId": "226207034"
                },
                {
                    "start": 1445,
                    "end": 1448,
                    "matchedPaperCorpusId": "33273008"
                },
                {
                    "start": 1861,
                    "end": 1865,
                    "matchedPaperCorpusId": "3750353"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89599609375
        },
        {
            "corpus_id": "276768480",
            "title": "Implicit Bias in LLMs: A Survey",
            "text": "The advent of Large Language Models (LLMs) has significantly transformed the landscape of natural language processing (NLP), enabling breakthroughs across a wide array of tasks. Distinguished from task-specific models, LLMs function as foundation models (Liu et al. 2021), capable of addressing diverse tasks through prompt-based learning. This inherent flexibility obviates the necessity for advanced programming expertise, thereby democratizing access to state-of-the-art NLP capabilities. The synergy of high performance and user accessibility has facilitated the widespread adoption of LLMs in various domains (Jiang et al. 2023;Kasneci et al. 2023;Nay et al. 2023). \n\nAs the societal impact of LLMs continues to grow, concerns regarding their potential harms have garnered increasing attention. These models, trained on extensive corpora of human data from the internet, are susceptible to inheriting and, in some cases, amplifying toxic and biased content (Dodge et al. 2021). Bias is broadly defined as a preconceived negative attitude or stereotype directed towards specific groups (Garimella et al. 2021). Such biases embedded in LLMs can manifest as outputs containing negative sentiments towards vulnerable populations, thereby undermining the interests of marginalized communities and exacerbating existing social inequities. \n\nMost prior research has concentrated on addressing explicit bias, which are readily identifiable and can be mitigated through established techniques (Shen et al. 2023). Advances in these methods have significantly reduced the presence of explicit biases in LLM outputs, rendering such biases nearly imperceptible. However, akin to humans who may suppress overt biases under social norms without resolving their underlying prejudices, LLMs may exhibit a shift from explicit to more subtle and covert forms of bias. We refer to this subtle, unconscious, and automatic bias as \"implicit bias\" (Wilson et al. 2000). Implicit bias has been shown to exert a stronger influence on behavior compared to explicit bias and poses the potential for more profound and far-reaching consequences. Therefore, systematically understanding, detecting, and mitigating implicit bias in LLMs is critical to ensuring the fairness, accuracy, and trustworthiness. \n\nThis survey provides a comprehensive review of implicit bias in LLMs.",
            "score": 0.6184423115316215,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 670
                },
                {
                    "start": 673,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1337
                },
                {
                    "start": 1340,
                    "end": 1508
                },
                {
                    "start": 1509,
                    "end": 1653
                },
                {
                    "start": 1654,
                    "end": 1853
                },
                {
                    "start": 1854,
                    "end": 1951
                },
                {
                    "start": 1952,
                    "end": 2121
                },
                {
                    "start": 2122,
                    "end": 2279
                },
                {
                    "start": 2282,
                    "end": 2351
                }
            ],
            "ref_mentions": [
                {
                    "start": 614,
                    "end": 633,
                    "matchedPaperCorpusId": "259112211"
                },
                {
                    "start": 633,
                    "end": 653,
                    "matchedPaperCorpusId": "257445349"
                },
                {
                    "start": 1090,
                    "end": 1113,
                    "matchedPaperCorpusId": "236477795"
                },
                {
                    "start": 1930,
                    "end": 1950,
                    "matchedPaperCorpusId": "18324937"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7119140625
        },
        {
            "corpus_id": "237490811",
            "title": "Are Gender-Neutral Queries Really Gender-Neutral? Mitigating Gender Bias in Image Search",
            "text": "postprocessing ideas in the domain of image search.\n\nSocial Bias in Multi-modality Implicit social bias related to gender and race has been discussed in multimodal tasks including image captioning (Burns et al., 2018;Tang et al., 2020), visual question answering (Manjunatha et al., 2019), face recognition (Buolamwini and Gebru, 2018), and unsupervised image representation learning (Steed and Caliskan, 2021). For example, Zhao et al. (2017) shows that models trained on unbalanced data can amplify bias, and injecting corpus-level Lagrangian constraints can calibrate the bias amplification. Caliskan et al. (2017) demonstrates the association between the word embeddings of occupation and gendered concepts correlates with the imbalanced distribution of gender in text corpora. There are also a series of debiasing techniques in this area. Bolukbasi et al. (2016) propose to surgically alter the embedding space by identifying the gender subspace from gendered word pairs. Manzini et al. (2019) extend the bias component removal approach to the setting where the sensitive attribute is non-binary. Data augmentation approaches remove the implicit bias in the training corpora and train the models on the balanced datasets (Zhao et al., 2018). Our work complements this line of research by examining gender bias induced by multimodal models in image search results. Our focus on gender bias in the gender-neutral language would offer new insights for a less explored topic to the community.\n\nGender Bias in Online Search Systems Our work is also closely connected to studies in the HCI community showing the gender inequality in online image search results. Kay et al. (2015) articulate the gender bias in occupational image search results affect people's perceptions of the prevalence of men and women in each occupation. Kay et al. (2015) compare gender proportions in occupational image search results and discuss how the bias affects people's perceptions of the prevalence of men and women in each occupation.  examine the prevalence of gender stereotypes on various digital media platforms. Otterbacher et al. (2017) identify gender bias with character traits. Nonetheless, these",
            "score": 0.6179802073877585,
            "section_title": "Related Work",
            "char_start_offset": 23559,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 197,
                    "end": 217,
                    "matchedPaperCorpusId": "4384334"
                },
                {
                    "start": 217,
                    "end": 235,
                    "matchedPaperCorpusId": "219687817"
                },
                {
                    "start": 263,
                    "end": 288,
                    "matchedPaperCorpusId": "53757613"
                },
                {
                    "start": 307,
                    "end": 335,
                    "matchedPaperCorpusId": "3298854"
                },
                {
                    "start": 384,
                    "end": 410,
                    "matchedPaperCorpusId": "225094318"
                },
                {
                    "start": 425,
                    "end": 443,
                    "matchedPaperCorpusId": "1389483"
                },
                {
                    "start": 595,
                    "end": 617,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 844,
                    "end": 867,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 977,
                    "end": 998,
                    "matchedPaperCorpusId": "102350941"
                },
                {
                    "start": 1226,
                    "end": 1245,
                    "matchedPaperCorpusId": "4952494"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28857421875
        },
        {
            "corpus_id": "265212726",
            "title": "Social Bias Probing: Fairness Benchmarking for Language Models",
            "text": "Social bias2 can be defined as the manifestation through language of \"prejudices, stereotypes, and discriminatory attitudes against certain groups of people\" (Navigli et al., 2023). These biases are featured in training datasets and are carried over into downstream applications, resulting in, for instance, classification errors concerning specific minorities and the generation of harmful content when models are prompted with sensitive identities (Cui et al., 2024;Gallegos et al., 2023). \n\nTo measure the extent to which social bias is present in language models, we propose a Social Bias Probing framework (see Fig. 1) which serves as a technique for fine-grained fairness benchmarking of LMs. We first collect a set of stereotypes and identities (Section 3.1-Section 3.2), which results in the SOFA (Social Fairness) dataset (Section 3.3). The final phase of our workflow involves evaluating language models by employing our proposed perplexity-based fairness measures in response to the constructed probes (Section 3.4), exploited in the designed evaluation setting (Section 3.5).",
            "score": 0.6179625766578583,
            "section_title": "Social Bias Probing Framework",
            "char_start_offset": 7371,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 491
                },
                {
                    "start": 494,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 1087
                }
            ],
            "ref_mentions": [
                {
                    "start": 158,
                    "end": 180,
                    "matchedPaperCorpusId": "258688053"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8779296875
        },
        {
            "corpus_id": "270560549",
            "title": "Investigating Annotator Bias in Large Language Models for Hate Speech Detection",
            "text": "Specifically we select the target groups that are highly vulnerable within the mentioned four categories and explore the annotator biases. Additionally, we provide a detailed analysis of the possible reasons of these biases by exploring the data being annotated. Serving as a critical guide, this paper aims to steer researchers towards exploring the potential of LLMs for data annotation, thereby facilitating future advancements in this essential domain. \n\nThrough rigorous data annotation, prompt engineering, quantitative and qualitative analysis, we aim to answer the following research questions: RQ1: Does annotator bias exist in Large Language Models for hate speech detection? RQ2: If it exists, what potential factors contribute to its existence? RQ3: How can this problem be effectively mitigated? \n\nTo this end, our work makes the following contributions: OUR CONTRIBUTIONS \u27a0 Our research demonstrates that annotator bias is present in LLMs used for hate speech detection. This bias arises from the subjective interpretations of annotators, which influence the training data and consequently affect the model's performance. We provide empirical evidence illustrating how such biases skew detection results, leading to potential inaccuracies and unfair outcomes. \n\n\u27a0 In our research, we specifically examine four types of biases: gender, race, disability, and religion. Gender bias refers to the prejudiced treatment based on an individual's sex or gender identity. Race bias involves discriminatory actions or attitudes towards individuals based on their racial or ethnic background. Disability bias encompasses unfair treatment of people with physical or mental impairments. Religion bias involves prejudices and discriminatory behaviors directed at individuals based on their religious beliefs or practices. Our study aims to analyze the prevalence and impact of these biases in various contexts. \n\n\u27a0 We delve into the underlying factors contributing to bias and propose a potential solution to address this issue. We analyze various aspects to uncover the root causes of bias and present a strategy aimed at mitigating its effects. Through our investigation, we aim to provide valuable insights into understanding and combatting bias in our study.",
            "score": 0.6169795807903248,
            "section_title": "Introduction",
            "char_start_offset": 2356,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 456
                },
                {
                    "start": 459,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 808
                },
                {
                    "start": 811,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1273
                },
                {
                    "start": 1276,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1476
                },
                {
                    "start": 1477,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1687
                },
                {
                    "start": 1688,
                    "end": 1821
                },
                {
                    "start": 1822,
                    "end": 1910
                },
                {
                    "start": 1913,
                    "end": 2028
                },
                {
                    "start": 2029,
                    "end": 2146
                },
                {
                    "start": 2147,
                    "end": 2262
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94921875
        },
        {
            "corpus_id": "268385538",
            "title": "Leveraging Prototypical Representations for Mitigating Social Bias without Demographic Information",
            "text": "The presence of social bias in training data presents a significant challenge in the development of language models for real-world applications.While these models possess remarkable capabilities, biases within the data can lead to unfair outcomes.Mitigating these biases is crucial, but it becomes particularly challenging when acquiring or accessing sensitive attribute labels is costly or unfeasible.\n\nStudies showed that language models have the ability to capture demographic information about the writer, including race or gender, within their representations (Caliskan et al., 2017;Zhao et al., 2018).However, this capability can introduce unintended biases, leading to discriminatory outputs (De-Arteaga et al., 2019).\n\nCommon approaches for social bias mitigation require explicit annotation of biases for each sample in the data (Beutel et al., 2017;Zhang et al., 2018).Recent concept removal methods (Ravfogel et al., 2020(Ravfogel et al., , 2022a,b;,b;Iskander et al., 2023) Figure 1: Our debiasing method consists of defining task-specific representations for each social attribute, measuring similarity in the representation space for each example, and utilizing the KL loss to encourage uniform probabilities across social groups.\n\nhave shown promise in addressing social bias by removing sensitive attributes.These approaches rely on training classifiers for predicting the sensitive attribute, and training such classifiers typically requires a significant amount of annotated data.\n\nA promising line of research has emerged that aims to mitigate bias without relying on explicit information about the biases present in the data.For instance, Just Train Twice (JTT) (Liu et al., 2021) employs a two-step training process.In the second step, a second model is trained on up-weighed training examples that were misclassified by the first model.Another method is BLIND (Orgad and Belinkov, 2023), which introduces a success detector and down-weighs examples for which the detector accurately predicts the outcome.\n\nIn this paper, we propose DAFAIR: Demographics-Agnostic Fairness, a novel approach for mitigating social bias during the fine-tuning process of language models, without relying on demographic information.",
            "score": 0.6166421594191702,
            "section_title": "Introduction and Background",
            "char_start_offset": 30,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 144,
                    "end": 247
                },
                {
                    "start": 247,
                    "end": 402
                },
                {
                    "start": 404,
                    "end": 607
                },
                {
                    "start": 607,
                    "end": 725
                },
                {
                    "start": 727,
                    "end": 879
                },
                {
                    "start": 879,
                    "end": 1244
                },
                {
                    "start": 1246,
                    "end": 1324
                },
                {
                    "start": 1324,
                    "end": 1498
                },
                {
                    "start": 1500,
                    "end": 1645
                },
                {
                    "start": 1645,
                    "end": 1737
                },
                {
                    "start": 1737,
                    "end": 1858
                },
                {
                    "start": 1858,
                    "end": 2026
                },
                {
                    "start": 2028,
                    "end": 2232
                }
            ],
            "ref_mentions": [
                {
                    "start": 565,
                    "end": 588,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 588,
                    "end": 606,
                    "matchedPaperCorpusId": "4952494"
                },
                {
                    "start": 699,
                    "end": 724,
                    "matchedPaperCorpusId": "58006082"
                },
                {
                    "start": 859,
                    "end": 878,
                    "matchedPaperCorpusId": "9424845"
                },
                {
                    "start": 910,
                    "end": 932,
                    "matchedPaperCorpusId": "215786522"
                },
                {
                    "start": 963,
                    "end": 985,
                    "matchedPaperCorpusId": "258740820"
                },
                {
                    "start": 1682,
                    "end": 1700,
                    "matchedPaperCorpusId": "235825419"
                },
                {
                    "start": 1882,
                    "end": 1908,
                    "matchedPaperCorpusId": "254877664"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62255859375
        },
        {
            "corpus_id": "259129801",
            "title": "Bias Against 93 Stigmatized Groups in Masked Language Models and Downstream Sentiment Classification Tasks",
            "text": "We adapt their list of these 93 social stigmas and a widely used psychological questionnaire that measures social rejection, the Social Distance Scale, to quantify bias against stigmatized groups. To assess the magnitude of bias, we curate a separate list of 29 non-stigmatized conditions derived from the original set of 93 stigmatized conditions, enabling a comparative analysis. \n\nMLMs have been popularly used in downstream Natural Language Processing (NLP) tasks such as natural language inference, natural language generation, and extractive question answering [11,26,39]. This study evaluates six MLMs, with each varying in size and training data: RoBERTa-base [26], RoBERTa-large [26], DistilBERT [42], BERTweet-base [35], BERTweet-large [35], and XLNet-large [53]. Trained with a bidirectional objective, MLMs can predict missing words in sentences based on the surrounding contexts [26]. Recent studies investigate bias in these models and their downstream tasks via prompting. By supplying LMs with specific texts to predict missing words or generate text following a given prefix, researchers examine the generated texts to evaluate the models' performance. These texts used for evaluation are commonly referred to as prompts. We curate prompts based on the Social Distance Scale for the experiments in this study. For example, one of our prompts is \"It is for me to rent a room to someone who has depression. \" \n\nMeanwhile, this study also directs attention to the downstream sentiment classification tasks of MLMs because of their widespread use in real-world applications which include content moderation, market prediction, and resume screening. Sentiment classification is used to classify the underlying attitudes of the author based on the written texts. Yet sentiment classifiers-tools developed based on LMs to classify the underlying sentiment of text-are also found to encode social biases [23]. To investigate if bias against stigmatized conditions are also captured in downstream sentiment classification tasks, this research examines four sentiment classifiers that are trained based on MLMs: BERTweet-base-sentiment-analysis [35,40], DistilBERT base uncased finetuned SST-2 [20], SiEBERT [17], and Twitter-RoBERTa-base [27].",
            "score": 0.6156801785664099,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2039,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 381
                },
                {
                    "start": 384,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1238
                },
                {
                    "start": 1239,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1423
                },
                {
                    "start": 1426,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1773
                },
                {
                    "start": 1774,
                    "end": 1918
                },
                {
                    "start": 1919,
                    "end": 2251
                }
            ],
            "ref_mentions": [
                {
                    "start": 567,
                    "end": 571,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 574,
                    "end": 577,
                    "matchedPaperCorpusId": "220045430"
                },
                {
                    "start": 725,
                    "end": 729,
                    "matchedPaperCorpusId": "218719869"
                },
                {
                    "start": 746,
                    "end": 750,
                    "matchedPaperCorpusId": "218719869"
                },
                {
                    "start": 768,
                    "end": 772,
                    "matchedPaperCorpusId": "195069387"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.49169921875
        },
        {
            "corpus_id": "268512691",
            "title": "Detecting Bias in Large Language Models: Fine-tuned KcBERT",
            "text": "The rapid advancement of large language models (LLMs) has enabled natural language processing capabilities similar to those of humans, and LLMs are being widely utilized across various societal domains such as education and healthcare. While the versatility of these models has increased, they have the potential to generate subjective and normative language, leading to discriminatory treatment or outcomes among social groups, especially due to online offensive language. In this paper, we define such harm as societal bias and assess ethnic, gender, and racial biases in a model fine-tuned with Korean comments using Bidirectional Encoder Representations from Transformers (KcBERT) and KOLD data through template-based Masked Language Modeling (MLM). To quantitatively evaluate biases, we employ LPBS and CBS metrics. Compared to KcBERT, the fine-tuned model shows a reduction in ethnic bias but demonstrates significant changes in gender and racial biases. Based on these results, we propose two methods to mitigate societal bias. Firstly, a data balancing approach during the pre-training phase adjusts the uniformity of data by aligning the distribution of the occurrences of specific words and converting surrounding harmful words into non-harmful words. Secondly, during the in-training phase, we apply Debiasing Regularization by adjusting dropout and regularization, confirming a decrease in training loss. Our contribution lies in demonstrating that societal bias exists in Korean language models due to language-dependent characteristics.",
            "score": 0.6154869577907608,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68212890625
        },
        {
            "corpus_id": "202781363",
            "title": "Assessing Social and Intersectional Biases in Contextualized Word Representations",
            "text": "This paper makes the following contributions. First, we use co-occurrence counts to show that standard corpora for pre-training contextual word models exhibit significant gender imbalances. Second, we extend existing analyses of social bias to state-of-the-art contextual word models like GPT-2, and indicate that social bias also exists in those models. This highlights the scope of the problem of fairness in state-of-the-art models for language processing. Third, we demonstrate that when measuring social bias in contextual word models, both the sentence encoding and contextual word representation should be used. It is possible that either encoding type may be unable to uncover latent social bias, whereas the other encoding type is able to. Fourth, we provide evidence for how racial bias is encoded strongly in contextual word models, potentially even more so than gender bias. Fifth, we introduce a method of comparison that anchors at the most or least privileged group to show that intersectional identities suffer from such bias as well, and more so than their constituent minority identities. In we show that the effect of race on intersectional identities seems to be larger than the effect of gender. \n\nIt is important to highlight the following limitations of our work. First, the lack of significant positive associations should not be taken as an absence of social bias. Rather, this only indicate the absence as such measured by these specific tests. Second, this work assumes binary gender (male/female), which is a significant limitation in evaluating the bias of non-binary genders. We believe that the bias towards non-binary genders is likely to be worse, but there can also be more data sparsity issues in such evaluations. Third, this work only provides a preliminary investigation into the multiplicative aspects of identities of multiple minorities, in particular the specific interactions between different identities. While we have tried to isolate the effects of the different dimensions of identity in intersectional tests, more work needs to be done to determine the interactive nature of such effects. \n\nWe propose the following potential future directions. First, investigate how and why the encoding of bias may differ across both model size and model layers. Our results show that larger contextual word models seem to encode less social bias.",
            "score": 0.6145841924910944,
            "section_title": "Discussion and Limitations",
            "char_start_offset": 24775,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 45
                },
                {
                    "start": 46,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1216
                },
                {
                    "start": 1219,
                    "end": 1286
                },
                {
                    "start": 1287,
                    "end": 1389
                },
                {
                    "start": 1390,
                    "end": 1470
                },
                {
                    "start": 1471,
                    "end": 1605
                },
                {
                    "start": 1606,
                    "end": 1749
                },
                {
                    "start": 1750,
                    "end": 1948
                },
                {
                    "start": 1949,
                    "end": 2136
                },
                {
                    "start": 2139,
                    "end": 2192
                },
                {
                    "start": 2193,
                    "end": 2296
                },
                {
                    "start": 2297,
                    "end": 2381
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83154296875
        },
        {
            "corpus_id": "259274987",
            "title": "CBBQ: A Chinese Bias Benchmark Dataset Curated with Human-AI Collaboration for Large Language Models",
            "text": "\"Bias and impartiality is in the eye of the beholder.\"(Samuel Johnson). How about large language models (LLM) trained from human data? Many studies have revealed that LLMs also exhibit harmful societal biases [1,3,4,16,24,13,5,8,30], which is even getting worse for larger models [2,10,12,22,28]. In the context of AI fairness, The term \"bias\" refers to the harm that occurs when a system reinforces the subordinate status of certain groups along the lines of identity, and can be quantified through certain metrics [6]. In this study, our methodology follows this concept, focusing on stereotyping behavior and discrimination. The existence of stereotypes and discrimination may harm marginalized or vulnerable individuals and groups, thereby affecting the safety and deployment of large language models in real-world applications. \n\nWe assert the critical importance of gaining a comprehensive understanding of the ways in which societal biases manifest in natural language generation (NLG), particularly as these applications engage with users across various domains, such as chat bots for health, education, and personal assistant. In order to curate a dataset for measuring bias in LLM-driven NLG, we draw upon the design of BBQ, a bias benchmark QA dataset proposed by Parrish et al. [21], for the following reasons. (i) Rational Dataset Design: They transformed bias evaluation into a multiple-choice task to measure model's group preferences, which correlate with the model's likelihood of associating answer options with either positive or negative stereotypes. To understand when biases in LLM may manifest, they distinguish ambiguous contexts (lacking sufficient information) from disambiguous contexts (providing sufficient information), each paired with two bias-related questions probing harmful and non-harmful stereotypes. (ii) Automated Dataset Generation: The BBQ dataset [21] employs human-written templates, which are then used to generate a large volume of data automatically. This approach has proven effective in our subsequent experiments. \n\nNevertheless, the BBQ dataset [21] could benefit from a further development if we take a broad perspective of culture and diversity.",
            "score": 0.6144358546157036,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 61
                },
                {
                    "start": 62,
                    "end": 71
                },
                {
                    "start": 72,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 832
                },
                {
                    "start": 835,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1570
                },
                {
                    "start": 1571,
                    "end": 1838
                },
                {
                    "start": 1839,
                    "end": 1997
                },
                {
                    "start": 1998,
                    "end": 2063
                },
                {
                    "start": 2066,
                    "end": 2198
                }
            ],
            "ref_mentions": [
                {
                    "start": 209,
                    "end": 212,
                    "matchedPaperCorpusId": "236384212"
                },
                {
                    "start": 214,
                    "end": 216,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 283,
                    "end": 286,
                    "matchedPaperCorpusId": "246867298"
                },
                {
                    "start": 292,
                    "end": 295,
                    "matchedPaperCorpusId": "235489789"
                },
                {
                    "start": 1290,
                    "end": 1294,
                    "matchedPaperCorpusId": "239010011"
                },
                {
                    "start": 1890,
                    "end": 1894,
                    "matchedPaperCorpusId": "239010011"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69775390625
        },
        {
            "corpus_id": "271571036",
            "title": "Understanding the Interplay of Scale, Data, and Bias in Language Models: A Case Study with BERT",
            "text": "In the current landscape of language model research, larger models, larger datasets and more compute seems to be the only way to advance towards intelligence. While there have been extensive studies of scaling laws and models' scaling behaviors, the effect of scale on a model's social biases and stereotyping tendencies has received less attention. In this study, we explore the influence of model scale and pre-training data on its learnt social biases. We focus on BERT -- an extremely popular language model -- and investigate biases as they show up during language modeling (upstream), as well as during classification applications after fine-tuning (downstream). Our experiments on four architecture sizes of BERT demonstrate that pre-training data substantially influences how upstream biases evolve with model scale. With increasing scale, models pre-trained on large internet scrapes like Common Crawl exhibit higher toxicity, whereas models pre-trained on moderated data sources like Wikipedia show greater gender stereotypes. However, downstream biases generally decrease with increasing model scale, irrespective of the pre-training data. Our results highlight the qualitative role of pre-training data in the biased behavior of language models, an often overlooked aspect in the study of scale. Through a detailed case study of BERT, we shed light on the complex interplay of data and model scale, and investigate how it translates to concrete biases.",
            "score": 0.614231762028417,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86328125
        },
        {
            "corpus_id": "256868507",
            "title": "BiasTestGPT: Using ChatGPT for Social Bias Testing of Language Models",
            "text": "Our work builds upon methods for technical social bias testing methods in PLMs, on works related to dataset creation as well as on user-centered tools for inspecting PLMs with a particular focus on testing fairness and social bias. Here we provide a background for these areas and highlight the contributions our work introduces in this space. \n\nMethods for Social Bias Testing in Language Models: Social bias can be defined in language generation as a PLMs tendency to systematically produce text with different levels of inclinations towards different groups (e.g., man vs. woman) [104]. More broadly social bias can be linked to stereotypes in language models. Such stereotypes have been defined in prior work as traits that have been broadly linked with demographic groups in ways that uphold social hierarchies. Various methods have been developed to measure social bias and stereotypes in large language models [22,25]. Broadly social bias quantification methods in PLMs can be divided into ones examining associations in latent representations of language learned by such models (i.e., embeddings) [42,72] and methods based on probabilities associated with language generation and sentence probability [22,57,78,79]. Social bias can also be measured in the pretrained part of language models which are not specialized for any particular task (i.e., intrinsic measures) and in specific downstream tasks (e.g., classification, summarization) for which such models are fine-tuned [26]. Intrinsic  1. Total number of generated test sentences for tested biases. Bias specifications are taken from [9,19,42] and used as input for our controllable generation. We also propose 4 novel biases to show the flexibility of our framework defined in detail in Apx. F.6 (indicated with \"*\"). In brackets, we show the number of terms provided in the bias specification. measures are believed to be particularly valuable for capturing social bias present in pre-training datasets and might be less indicative of application-specific bias resulting from model finetuning on additional datasets [38]. \n\nNevertheless, in the end-user use of PLMs without further specialization which becomes increasingly popular via prompting methods [68], quantification of intrinsic social bias is crucial.",
            "score": 0.614023376388437,
            "section_title": "BACKGROUND AND RELATED WORK",
            "char_start_offset": 7902,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 343
                },
                {
                    "start": 346,
                    "end": 589
                },
                {
                    "start": 590,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1489
                },
                {
                    "start": 1490,
                    "end": 1503
                },
                {
                    "start": 1504,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1757
                },
                {
                    "start": 1758,
                    "end": 1783
                },
                {
                    "start": 1784,
                    "end": 1860
                },
                {
                    "start": 1861,
                    "end": 2088
                },
                {
                    "start": 2091,
                    "end": 2278
                }
            ],
            "ref_mentions": [
                {
                    "start": 583,
                    "end": 588,
                    "matchedPaperCorpusId": "202537041"
                },
                {
                    "start": 1105,
                    "end": 1109,
                    "matchedPaperCorpusId": "219530686"
                },
                {
                    "start": 1109,
                    "end": 1112,
                    "matchedPaperCorpusId": "85518027"
                },
                {
                    "start": 1213,
                    "end": 1216,
                    "matchedPaperCorpusId": "190000105"
                },
                {
                    "start": 1216,
                    "end": 1219,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 1219,
                    "end": 1222,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 1484,
                    "end": 1488,
                    "matchedPaperCorpusId": "250390561"
                },
                {
                    "start": 1599,
                    "end": 1602,
                    "matchedPaperCorpusId": "225094152"
                },
                {
                    "start": 1602,
                    "end": 1605,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 1605,
                    "end": 1608,
                    "matchedPaperCorpusId": "219530686"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80322265625
        },
        {
            "corpus_id": "273403785",
            "title": "debiaSAE: Benchmarking and Mitigating Vision-Language Model Bias",
            "text": "LLMs have made significant advancements, yet they continue to inherit and sometimes amplify societal biases, particularly in open-ended language generation tasks. Several benchmarks have been developed to quantify and mitigate these biases. For instance, the BOLD benchmark Dhamala et al. (2021) assesses social biases across categories such as gender, race, and religion in models' free-form text generation. Similarly, Parrish et al. (2022) introduced the BBQ benchmark, which measures biases in multiple-choice question-answering tasks, focusing on how LLMs handle stereotypical or biased scenarios. StereoSet Nadeem et al. (2020) evaluates stereotypical biases across domains such as gender, profession, race, and religion by presenting models with stereotypical and non-stereotypical sentence pairs to reveal the model's inherent biases. \n\nExpanding on these efforts, CrowS-Pairs Nangia et al. (2020) evaluates bias in sentence pairs across nine categories, examining how models select between socially biased and neutral sentences. HONEST Nozza et al. (2021) adopts a multilingual approach, analyzing harmful sentence completions to understand how LLMs generate biased or hurtful content across different languages. While these benchmarks primarily focus on text-based biases, they offer valuable insights into how LLMs may introduce similar biases when combined with visual data in VLMs.",
            "score": 0.6136691283064812,
            "section_title": "Bias in LLMs",
            "char_start_offset": 2436,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 842
                },
                {
                    "start": 845,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1394
                }
            ],
            "ref_mentions": [
                {
                    "start": 421,
                    "end": 442,
                    "matchedPaperCorpusId": "239010011"
                },
                {
                    "start": 613,
                    "end": 633,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 885,
                    "end": 905,
                    "matchedPaperCorpusId": "222090785"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5576171875
        },
        {
            "corpus_id": "268201625",
            "title": "AXOLOTL: Fairness through Assisted Self-Debiasing of Large Language Model Outputs",
            "text": "Pre-trained Large Language Models (LLMs) have revolutionized natural language processing, offering unparalleled capabilities in understanding, generating, and translating text (Zhu et al., 2023;Zhang et al., 2020). Despite their advancements, these models are not immune to inheriting and perpetuating biases present in their training data (Maudslay et al., 2019a). Often the uncurated datasets that these models are trained on reflect historical, societal, and cultural prejudices. Biases in LLMs can manifest in various forms such as gender, race, religion, profession, etc stereotypes, leading to unfair or discriminatory outcomes in applications ranging from automated hiring systems to conversational AI (Zhang et al., 2020). Studies such as (Bolukbasi et al., 2016b) and (Bender et al., 2021) highlight the critical nature of this These authors contributed equally to this work. \n\nproblem, demonstrating how biases can skew LLM outputs in ways that reinforce harmful stereotypes and marginalize already disadvantaged groups. \n\nResearchers have explored a multitude of strategies to identify and mitigate bias. These efforts encompass a broad spectrum of approaches, including enhancing fairness through modifications in sentence and word representations and embeddings (May et al., 2019;Caliskan et al., 2017b;Ravfogel et al., 2020), adjusting the underlying distribution of tokens (Guo et al., 2022), and refining datasets alongside model pre-training (Garimella et al., 2021;Maudslay et al., 2019a,b). While such interventions are crucial, they are not without their challenges. Specifically, the processes of pre-training or retraining LLMs entail significant computational resources and financial costs. Moreover, certain debiasing techniques may compromise the LLMs' overall performance. Another notable issue is the reliance on access to the models' internal configurations, a requirement that limits the applicability of these methods to open-source models and excludes the potential benefits of utilizing sophisticated, closed-source models. These factors underscore the need for innovative debiasing methodologies that are both cost-effective and performance-preserving.",
            "score": 0.613592149516272,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 884
                },
                {
                    "start": 887,
                    "end": 1030
                },
                {
                    "start": 1033,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1509
                },
                {
                    "start": 1510,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1713
                },
                {
                    "start": 1714,
                    "end": 1798
                },
                {
                    "start": 1799,
                    "end": 2055
                },
                {
                    "start": 2056,
                    "end": 2185
                }
            ],
            "ref_mentions": [
                {
                    "start": 194,
                    "end": 213,
                    "matchedPaperCorpusId": "207869708"
                },
                {
                    "start": 340,
                    "end": 364,
                    "matchedPaperCorpusId": "202541569"
                },
                {
                    "start": 709,
                    "end": 729,
                    "matchedPaperCorpusId": "207869708"
                },
                {
                    "start": 747,
                    "end": 772,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 777,
                    "end": 798,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 1293,
                    "end": 1316,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 1388,
                    "end": 1406,
                    "matchedPaperCorpusId": "248780440"
                },
                {
                    "start": 1459,
                    "end": 1483,
                    "matchedPaperCorpusId": "236477795"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7392578125
        },
        {
            "corpus_id": "268856702",
            "title": "Fairness in Large Language Models: A Taxonomic Survey",
            "text": "Large language models (LLMs) have demonstrated remarkable capabilities in addressing problems across diverse domains, ranging from chatbots [66] to medical diagnoses [191] and financial advisory [160].Notably, their impact extends beyond fields directly associated with language processing, such as translation [206] and text sentiment analysis [125].LLMs also prove invaluable in broader applications including legal aid [211], healthcare [165], and drug discovery [148].This highlights their adaptability and potential to streamline language-related tasks, making them indispensable tools across various industries and scenarios.\n\nDespite their considerable achievements, LLMs may face fairness concerns stemming from biases inherited from the real-world and even exacerbate them [238].Consequently, they could lead to discrimination against certain populations, especially in socially sensitive applications, across various dimensions such as race [6], age [51], gender [93], nationality [183], occupation [90], and religion [1].For instance, an investigation [185] revealed that when tasked with generating a letter of recommendation for individuals named Kelly (a common female name) and Joseph (a common male name), ChatGPT, a prominent instance of LLMs, produced paragraphs describing Kelly and Joseph with random traits.Notably, Kelly was portrayed as warm and amiable (e.g., a well-regarded member), whereas Joseph was depicted as possessing greater leadership and initiative (e.g., a natural leader and role model).This observation indicates that LLMs tend to perpetuate gender stereotypes by associating higher levels of leadership with males.\n\nTo this end, the research community has made many efforts to address bias and discrimination in LLMs.Nevertheless, the notions of studied fairness vary across different works, which can be confusing and impede further progress.Moreover, different algorithms are developed to achieve various fairness notions.The lack of a clear framework mapping these fairness notions to their corresponding methodologies complicates the design of algorithms for future fair LLMs.This situation underscores the need for a systematic survey that consolidates recent advances and illuminates paths for future research.",
            "score": 0.6130361584752868,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 201,
                    "end": 351
                },
                {
                    "start": 351,
                    "end": 472
                },
                {
                    "start": 472,
                    "end": 631
                },
                {
                    "start": 633,
                    "end": 788
                },
                {
                    "start": 788,
                    "end": 1032
                },
                {
                    "start": 1032,
                    "end": 1328
                },
                {
                    "start": 1328,
                    "end": 1525
                },
                {
                    "start": 1525,
                    "end": 1654
                },
                {
                    "start": 1656,
                    "end": 1757
                },
                {
                    "start": 1757,
                    "end": 1883
                },
                {
                    "start": 1883,
                    "end": 1964
                },
                {
                    "start": 1964,
                    "end": 2120
                },
                {
                    "start": 2120,
                    "end": 2256
                }
            ],
            "ref_mentions": [
                {
                    "start": 345,
                    "end": 350,
                    "matchedPaperCorpusId": "3333256"
                },
                {
                    "start": 440,
                    "end": 445,
                    "matchedPaperCorpusId": "255124952"
                },
                {
                    "start": 466,
                    "end": 471,
                    "matchedPaperCorpusId": "258336875"
                },
                {
                    "start": 973,
                    "end": 977,
                    "matchedPaperCorpusId": "261276445"
                },
                {
                    "start": 1009,
                    "end": 1013,
                    "matchedPaperCorpusId": "236950797"
                },
                {
                    "start": 1028,
                    "end": 1031,
                    "matchedPaperCorpusId": "231603388"
                },
                {
                    "start": 1063,
                    "end": 1068,
                    "matchedPaperCorpusId": "264128125"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55615234375
        },
        {
            "corpus_id": "269982989",
            "title": "A survey on fairness of large language models in e-commerce: progress, application, and challenge",
            "text": "LLMs have made remarkable advancements in natural language processing, but they also face significant challenges related to fairness and bias.One major challenge is the presence of various types of biases in LLMs, including gender bias, racial bias, religious bias, age bias, sexuality bias, country bias, and disease bias.These biases can manifest in the model's outputs, leading to stereotypical or discriminatory associations.Another challenge lies in the sources of bias in LLMs, which can stem from biased training data, sampling biases, semantic biases encoded in the model's representations, and the amplification of biases during the learning process.Addressing these challenges requires a comprehensive approach that involves careful data curation, bias mitigation techniques, and rigorous evaluation frameworks.Additionally, the development of explainable and interpretable LLMs is crucial to understand and mitigate biases effectively.Researchers and practitioners must also consider the ethical implications of deploying LLMs in real-world applications and ensure that the models align with principles of fairness, transparency, and accountability.Overcoming these challenges is essential to harness the full potential of LLMs while promoting fairness and reducing the risk of perpetuating societal biases.",
            "score": 0.6126376697439606,
            "section_title": "The challenges and biases in LLM",
            "char_start_offset": 15098,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 142,
                    "end": 323
                },
                {
                    "start": 323,
                    "end": 429
                },
                {
                    "start": 429,
                    "end": 659
                },
                {
                    "start": 659,
                    "end": 821
                },
                {
                    "start": 821,
                    "end": 946
                },
                {
                    "start": 946,
                    "end": 1160
                },
                {
                    "start": 1160,
                    "end": 1318
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.763671875
        },
        {
            "corpus_id": "271097745",
            "title": "Are Large Language Models Really Bias-Free? Jailbreak Prompts for Assessing Adversarial Robustness to Bias Elicitation",
            "text": "Large Language Models (LLMs) have recently gained significant traction due to their impressive natural language understanding and generation capabilities across various tasks, including machine translation, text summarization, topic detection, and engaging human-like conversations [4,3,1]. However, as LLMs become more integral to our daily lives across various domains -ranging from healthcare and finance to law and education -it is increasingly crucial to address the inherent biases that can emerge from these models. Such biases can lead to unfair treatment, reinforce stereotypes, and exclude social groups, compromising the ethical standards and social responsibility of AI technologies [13,14,39]. The presence of bias in LLMs is a multifaceted issue rooted in the data used for training. Specifically, biases in data availability, selection, language, and social contexts may collectively reflect prejudices, disparities, and stereotypes that can inadvertently be learned and perpetuated by LLMs, leading to unfair and harmful responses. Biases may also arise from the unfair usage of LLMs, since users may favor generated information that confirms their preexisting beliefs, selectively interpreting responses that align with their views (confirmation bias), or blindly trust the generated output without any critical thinking, deeming it a priori superior to human judgment (automation bias) [38,37]. Therefore, understanding, unveiling, and mitigating these biases is essential for fostering sustainability and inclusivity in AI applications. Mitigation strategies should involve curating more balanced and representative training datasets [31,33], while also implementing robust bias detection [32,36] and alignment mechanisms [40,41], incorporating fairness guidelines. However, several challenges arise in ensuring that language models are entirely bias-free, including obtaining representative datasets for safety tuning, developing universally accepted bias metrics, and the significant resources required for thorough bias mitigation. \n\nStarting from the above considerations, our study proposes a robust methodology to test the resilience of various widely-used Language Models (LMs) at different scales, ranging from high-quality Small Language Models (SLMs) like Google's Gemma 2B to large-scale LLMs like OpenAI's GPT-3.5 Turbo (175B).",
            "score": 0.6125918052531738,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1412
                },
                {
                    "start": 1413,
                    "end": 1555
                },
                {
                    "start": 1556,
                    "end": 1784
                },
                {
                    "start": 1785,
                    "end": 2053
                },
                {
                    "start": 2056,
                    "end": 2358
                }
            ],
            "ref_mentions": [
                {
                    "start": 282,
                    "end": 285,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 702,
                    "end": 705,
                    "matchedPaperCorpusId": "258688053"
                },
                {
                    "start": 1404,
                    "end": 1408,
                    "matchedPaperCorpusId": "261530629"
                },
                {
                    "start": 1657,
                    "end": 1660,
                    "matchedPaperCorpusId": "232075876"
                },
                {
                    "start": 1708,
                    "end": 1712,
                    "matchedPaperCorpusId": "9424845"
                },
                {
                    "start": 1741,
                    "end": 1745,
                    "matchedPaperCorpusId": "258959321"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76416015625
        },
        {
            "corpus_id": "269960406",
            "title": "Using ChatGPT in Software Requirements Engineering: A Comprehensive Review",
            "text": "Training data bias: Language models learn from large datasets of human language. If these datasets contain biases related to race, gender, or socioeconomic status, the model may internalize these biases and reproduce them in its responses. A classic example might be that if there is a gender bias in the training data, the model is more likely to favor a particular gender in its responses.",
            "score": 0.6125524835617523,
            "section_title": "\u2022",
            "char_start_offset": 53752,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 81,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 391
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66064453125
        },
        {
            "corpus_id": "269983123",
            "title": "As an AI Language Model, \"Yes I Would Recommend Calling the Police\": Norm Inconsistency in LLM Decision-Making",
            "text": "As LLMs attract increasing attention across a variety of fields, a growing body of work has focused on uncovering unwanted societal biases that models learn from training data (Gallegos et al. 2024;Shaikh et al. 2023). Researchers have found that LLMs manifest bias in many different ways, including by producing text with explicit gender stereotypes (Hirota, Nakashima, and Garcia 2022;Kotek, Dockum, and Sun 2023), changing behavior based on prompt language (Agarwal et al. 2024), and relying on stereotypes in controlled classification tasks (Kohankhaki et al. 2024). Uncovering these biases is crucial to minimize potential harms of downstream applications, especially in normative decisions that involve making subjective judgments about human behavior or outcomes. However, many bias studies are limited because they focus on tasks that are detached from realworld normative decision-making.",
            "score": 0.6106017300372996,
            "section_title": "Measuring Bias in LLMs",
            "char_start_offset": 3696,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 897
                }
            ],
            "ref_mentions": [
                {
                    "start": 198,
                    "end": 217,
                    "matchedPaperCorpusId": "254686088"
                },
                {
                    "start": 351,
                    "end": 387,
                    "matchedPaperCorpusId": "248834284"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.34765625
        },
        {
            "corpus_id": "202781363",
            "title": "Assessing Social and Intersectional Biases in Contextualized Word Representations",
            "text": "Social bias in language has been demonstrated to exist upstream in a variety of corpora and datasets; on a corpus elicited from crowdworkers [28], and on the 1 Billion Word Benchmark corpus [7] where it was observed that there was gender skew in proportions of gendered pronouns and associations with occupation words Zhao et al. [35]. Gender bias has also been demonstrated downstream on several applications of natural language processing, including sentiment analysis [18,30], abusive language detection [23], image captioning [16] and text classification [11]. These models not only reflect the bias in training data, but also amplify the bias [32,5]. Our work extends the upstream corpus level analysis in two ways: 1) we include the non-gendered or collective pronoun in occurrence counts, and 2) we apply the analysis on other datasets like BooksCorpus [36], Wikipedia, and WebText [27]. \n\nSignificant work has been done to show social bias (in particular gender bias) in word embeddings. Bolukbasi et al. [2] and Caliskan et al. [5] demonstrate that word embeddings associate occupations with their stereotypical gender roles (eg. doctors are stereotypically male, nurses are stereotypically female) by evaluating occupation words with Word Embedding Association Tests (WEATs) to gender words. Inspired by implicit association tests, WEATs compute the differences in distances when word vectors are asked to pair two concepts that are similar (e.g., stereotypically female occupation words and female gender words) as opposed to two concepts that are different (e.g., stereotypically male occupation words and female gender words). Brunet et al. [4] trace the internalization of gender bias to representational differences at the corpus level. This line of work has very recently been extended to evaluate gender bias in contextual word representations in some specific settings. Zhao et al. [35] and Basta et al. [1] demonstrate gender bias in ELMo [25] word embeddings, whereas May et al. [21] evaluate various models of contextual word representations on a sentential generalization of WEAT.",
            "score": 0.6088070294186135,
            "section_title": "Related Work",
            "char_start_offset": 3685,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 894
                },
                {
                    "start": 897,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1138
                },
                {
                    "start": 1139,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1639
                },
                {
                    "start": 1640,
                    "end": 1751
                },
                {
                    "start": 1752,
                    "end": 1887
                },
                {
                    "start": 1888,
                    "end": 2102
                }
            ],
            "ref_mentions": [
                {
                    "start": 141,
                    "end": 145,
                    "matchedPaperCorpusId": "5310359"
                },
                {
                    "start": 190,
                    "end": 193,
                    "matchedPaperCorpusId": "14136307"
                },
                {
                    "start": 330,
                    "end": 334,
                    "matchedPaperCorpusId": "102352962"
                },
                {
                    "start": 471,
                    "end": 475,
                    "matchedPaperCorpusId": "21670658"
                },
                {
                    "start": 475,
                    "end": 478,
                    "matchedPaperCorpusId": "3668120"
                },
                {
                    "start": 507,
                    "end": 511,
                    "matchedPaperCorpusId": "52070035"
                },
                {
                    "start": 530,
                    "end": 534,
                    "matchedPaperCorpusId": "4384334"
                },
                {
                    "start": 559,
                    "end": 563,
                    "matchedPaperCorpusId": "54997157"
                },
                {
                    "start": 648,
                    "end": 652,
                    "matchedPaperCorpusId": "1389483"
                },
                {
                    "start": 652,
                    "end": 654,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 860,
                    "end": 864,
                    "matchedPaperCorpusId": "6866988"
                },
                {
                    "start": 889,
                    "end": 893,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1037,
                    "end": 1040,
                    "matchedPaperCorpusId": "23163324"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.736328125
        },
        {
            "corpus_id": "248965268",
            "title": "Gender Bias in Meta-Embeddings",
            "text": "In this paper, we limited our investigation to metaembedding learning methods applicable to static word embeddings because they are still extensively used in various NLP applications for input representation, particularly in resource/energy constrained devices without GPUs due to their relatively lightweight nature compared to contextualised embeddings obtained from large-scale neural language models (Strubell et al., 2019). However, there has been recent work studying the gender bias in contextualised embeddings (Zhao et al., 2019;Vig, 2019;Bordia and Bowman, 2019;May et al., 2019;Kaneko and Bollegala, 2021a,c;Kaneko et al., 2022;Zhou et al., 2022;Schick et al., 2021). On the other hand, learning meta-embeddings of contextualised embeddings is relatively underdeveloped (Poerner et al., 2020). Therefore, we defer the study of gender bias in contextualised metaembeddings to future work. Furthermore, in future, we plan to study other types of social biases such as racial and religious biases in meta-embeddings.",
            "score": 0.6083544875599198,
            "section_title": "Limitations",
            "char_start_offset": 26009,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 404,
                    "end": 427,
                    "matchedPaperCorpusId": "174802812"
                },
                {
                    "start": 519,
                    "end": 538,
                    "matchedPaperCorpusId": "102352962"
                },
                {
                    "start": 548,
                    "end": 572,
                    "matchedPaperCorpusId": "85518027"
                },
                {
                    "start": 572,
                    "end": 589,
                    "matchedPaperCorpusId": "85518027"
                },
                {
                    "start": 619,
                    "end": 639,
                    "matchedPaperCorpusId": "247450590"
                },
                {
                    "start": 639,
                    "end": 657,
                    "matchedPaperCorpusId": "247450590"
                },
                {
                    "start": 657,
                    "end": 677,
                    "matchedPaperCorpusId": "232075876"
                },
                {
                    "start": 781,
                    "end": 803,
                    "matchedPaperCorpusId": "207852340"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.45068359375
        },
        {
            "corpus_id": "246441998",
            "title": "Handling Bias in Toxic Speech Detection: A Survey",
            "text": "Background on NLP Bias. In cognitive science, bias is assumed to be a shortcut (valid/invalid) --our brain resorts to when informing our actions and interactions with others. Biases can develop due to a limited worldview and repeated exposure to similar behavior in the surrounding [43]. Owing to which filter bubbles [6] and echo chambers get formed, further amplifying the stereotypes. When such biased interactions are employed to train natural language models, they learn stereotypical statistical associations prevalent on the Internet [15]. The seminal work by Bolukbasi et al. [10] led the initiative of establishing gender (male vs. female) bias in non-contextual embeddings trained on large-scale web corpora. This work prompted similar studies [7,15] of bias in NLP towards other social/personal attributes. \n\nBlodgett et al. [7] broadly classified the NLP biases as either allocational or representational harm. Sun et al. [96] further granulated representation harm into denigration, stereotyping, recognition, and under-representation when evaluating various NLP tasks and their associated language models for mitigating gender bias. In a tangential approach, Shah et al. [91] and Kumar et al. [52] looked at biases in NLP through the lens of data and modeling pipelines. Shah et al. \n\n[91] proposed a pipeline to highlight the origin of various biases and their eventual outcome disparity. On the other hand, Kumar et al. [52] largely classified the biases as related to either data source, label/annotation, and embedding representation. Relation to NLP Bias. A simple bias mapping pipeline for training a generic NLP model is depicted in Figure 2. \n\nComparing the training pipelines in Figures 1 and 2, we can observe how the first step (data collection and sample) in both workflows is primarily the source of sampling bias. In the case of toxicity detection, biased sampling is performed to gain a higher percentage of toxic samples. Consequently, in broader NLP modeling, it occurs due to a skewed ratio in terms of quality and quantity of digital footprints of specific topics (such as content in support of LGTBQ vs. against them).",
            "score": 0.6074737253629884,
            "section_title": "INTRODUCTION",
            "char_start_offset": 7953,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 23
                },
                {
                    "start": 24,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 817
                },
                {
                    "start": 820,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1296
                },
                {
                    "start": 1299,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1552
                },
                {
                    "start": 1553,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1663
                },
                {
                    "start": 1666,
                    "end": 1841
                },
                {
                    "start": 1842,
                    "end": 1951
                },
                {
                    "start": 1952,
                    "end": 2152
                }
            ],
            "ref_mentions": [
                {
                    "start": 282,
                    "end": 286,
                    "matchedPaperCorpusId": "9763281"
                },
                {
                    "start": 318,
                    "end": 321,
                    "matchedPaperCorpusId": "221785201"
                },
                {
                    "start": 541,
                    "end": 545,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 584,
                    "end": 588,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 754,
                    "end": 757,
                    "matchedPaperCorpusId": "218971825"
                },
                {
                    "start": 757,
                    "end": 760,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 836,
                    "end": 839,
                    "matchedPaperCorpusId": "218971825"
                },
                {
                    "start": 934,
                    "end": 938,
                    "matchedPaperCorpusId": "195316733"
                },
                {
                    "start": 1185,
                    "end": 1189,
                    "matchedPaperCorpusId": "209461005"
                },
                {
                    "start": 1207,
                    "end": 1211,
                    "matchedPaperCorpusId": "233365325"
                },
                {
                    "start": 1436,
                    "end": 1440,
                    "matchedPaperCorpusId": "233365325"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66650390625
        },
        {
            "corpus_id": "267413181",
            "title": "Large Language Models are Geographically Biased",
            "text": "Large language models (LLMs), as foundational models, have demonstrated remarkable effectiveness across diverse domains such as healthcare, education, law, finance, and 1 Stanford University. Correspondence to: Rohin Manvi <rohinm@cs.stanford.edu>. \n\nProceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). scientific research (Bommasani et al., 2021;Zhao et al., 2023). With millions engaging directly and many more impacted by their usage, the influence of LLMs is rapidly expanding (Dash et al., 2023). This growth in impact makes it crucial to assess potential harms, particularly those stemming from biases inherent in their training data, which is often sourced from unprocessed internet content (Del'etang et al., 2023;Dodge et al., 2021). Such biases, if unchecked, risk perpetuating societal harm (Gallegos et al., 2023). \n\nThe biases of LLMs are evaluated across various dimensions. For instance, the Bias in Open-Ended Language Generation Dataset (BOLD) (Dhamala et al., 2021) examines bias in profession, gender, race, religion, and political ideology, while the Bias Benchmark for QA (BBQ) (Parrish et al., 2021) assesses bias across age, disability status, gender, nationality, physical appearance, race, religion, and socioeconomic status. These datasets assess different social biases, a notion related to group fairness (Hardt et al., 2016;Kamiran & Calders, 2012) and is broadly used to refer to disparate treatment or outcomes between social groups (Gallegos et al., 2023). Our paper evaluates geographic bias, where social groups are distinguished by location and bias is defined as systemic errors in geospatial predictions. \n\nAbstractly, evaluating the biases of LLMs on any topic through the lens of geography is very powerful. This is because numerous aspects of human life-such as culture, race, language, economics, politics, and religion-are meaningfully projected onto geographic space.",
            "score": 0.6074497135354379,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 248
                },
                {
                    "start": 251,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 911
                },
                {
                    "start": 914,
                    "end": 973
                },
                {
                    "start": 974,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1573
                },
                {
                    "start": 1574,
                    "end": 1726
                },
                {
                    "start": 1729,
                    "end": 1831
                },
                {
                    "start": 1832,
                    "end": 1995
                }
            ],
            "ref_mentions": [
                {
                    "start": 807,
                    "end": 826,
                    "matchedPaperCorpusId": "237568724"
                },
                {
                    "start": 1046,
                    "end": 1068,
                    "matchedPaperCorpusId": "231719337"
                },
                {
                    "start": 1184,
                    "end": 1206,
                    "matchedPaperCorpusId": "239010011"
                },
                {
                    "start": 1438,
                    "end": 1462,
                    "matchedPaperCorpusId": "14637938"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.454345703125
        },
        {
            "corpus_id": "248965356",
            "title": "How to keep text private? A systematic review of deep learning methods for privacy-preserving natural language processing",
            "text": "Bias in machine learning is a concept related to unfair or discriminatory decisions made by models towards or against specific individuals or groups (Ntoutsi et al. 2020). It becomes a severe problem in real-world scenarios since those decisions may directly impact a person's life or society as a whole. For instance, gender-biased job advertising tools were found to suggest lower-paying job positions to women more often than men (Datta et al. 2015). Human biases are long-lasting research topics in philosophy, social sciences, psychology, and law (Ntoutsi et al. 2020). There are three types of bias: preexisting bias (from the data), technical bias (from models with computational or mathematical constraints), and emergent bias (from the evaluation of results and their applicability) (Papakyriakopoulos et al. 2020).\n\nRecently, this subject has also been gaining attention in the NLP field since word representations, like the widely used word embeddings (Mikolov et al. 2013), can also be under threat of encoding human bias (Kaneko and Bollegala 2019) regarding gender, ethnicity, or social stereotypes from text corpora used for training. Therefore, applications built on such biased NLP models have the potential to amplify such misuses of language and propagate them to inference steps.\n\nFairness is then typically associated with trustworthy AI (Floridi 2019) and defined as an assurance against discriminatory decisions by AI models based on sensitive features (Zhang and Bareinboim 2018). Privacy technologies and policies often go hand in hand with concepts of fairness (Ekstrand et al. 2018). Additionally, works approaching bias and fairness in NLP follow the premise that the input data present sensitive features, e.g., gender (Elazar and Goldberg 2018;Zhao et al. 2018), which may lead to unfair predictions by downstream systems or be recovered from representations. Therefore, debiasing and attribute removal are techniques to mitigate the undesired discriminatory effects arising from unfair models. Examples of use case tasks covering bias and fairness encompass learning gender-neutral word embeddings (Zhao et al. 2018;Bolukbasi et al. 2016), analysis and reduction of gender bias in multi",
            "score": 0.6073030937222411,
            "section_title": "Privacy terminology",
            "char_start_offset": 34041,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 149,
                    "end": 169,
                    "matchedPaperCorpusId": "211129042"
                },
                {
                    "start": 433,
                    "end": 452,
                    "matchedPaperCorpusId": "2862289"
                },
                {
                    "start": 552,
                    "end": 572,
                    "matchedPaperCorpusId": "211129042"
                },
                {
                    "start": 792,
                    "end": 823,
                    "matchedPaperCorpusId": "211040995"
                },
                {
                    "start": 963,
                    "end": 984,
                    "matchedPaperCorpusId": "16447573"
                },
                {
                    "start": 1359,
                    "end": 1373,
                    "matchedPaperCorpusId": "164951125"
                },
                {
                    "start": 1476,
                    "end": 1503,
                    "matchedPaperCorpusId": "19117312"
                },
                {
                    "start": 1587,
                    "end": 1609,
                    "matchedPaperCorpusId": "46845867"
                },
                {
                    "start": 1748,
                    "end": 1774,
                    "matchedPaperCorpusId": "52056513"
                },
                {
                    "start": 1774,
                    "end": 1791,
                    "matchedPaperCorpusId": "52161864"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.44287109375
        },
        {
            "corpus_id": "247450590",
            "title": "Sense Embeddings are also Biased \u2013 Evaluating Social Biases in Static and Contextualised Sense Embeddings",
            "text": "In this paper we considered the relatively underexplored aspect of social biases in pretrained sense embeddings. We created a new dataset for this purpose, which we name the Sense-Sensitive Social Bias (SSSB) dataset. The dataset we create is of a sensitive nature. We have included various sentences that express stereotypical biases associated with different senses of words in this dataset. We specifically considered three types of social biases in SSSB: (a) racial biases associated with a nationality as opposed to a language (e.g. Chinese people are cunning, Chinese language is difficult, etc.), (b) racial biases associated with the word black as opposed to its sense as a colour (e.g. Black people are arrogant, Black dress is beautiful, etc.) and (c) gender-related biases associated with occupations used as nouns as opposed to verbs (e.g. She was a careless nurse, He was not able to nurse the crying baby, etc.). As seen from the above-mentioned examples, by design, SSSB contains many offensive, stereotypical examples. It is intended to facilitate evaluation of social biases in sense embeddings and is publicly released for this purpose only. We argue that SSSB should not be used to train sense embeddings. The motivation behind creating SSSB is to measure social biases so that we can make more progress towards debiasing them in the future. However, training on this data would defeat this purpose.\n\nIt is impossible to cover all types of social biases related to word senses in any single dataset. For example, the stereotypical association of a disadvantaged group with a positive attribute (e.g. All Chinese students are good at studying) can also raise unfairly high expectations for the members in that group and cause pressure to hold upto those stereotypes. Such positive biases are not well covered by any of the existing bias evaluation datasets, including the one we annotate in this work.\n\nGiven that our dataset is generated from a handful of manually written templates, it is far from complete. Moreover, the templates reflect the cultural and social norms of the annotators from a US-centric viewpoint. Therefore, SSSB should not be considered as an ultimate test for bias",
            "score": 0.6072040292224501,
            "section_title": "Ethical Considerations",
            "char_start_offset": 31553,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53173828125
        },
        {
            "corpus_id": "275336873",
            "title": "Explicit vs. Implicit: Investigating Social Bias in Large Language Models through Self-Reflection",
            "text": "Large Language Models (LLMs) have been shown to exhibit various biases and stereotypes in their generated content. While extensive research has investigated bias in LLMs, prior work has predominantly focused on explicit bias, leaving the more nuanced implicit biases largely unexplored. This paper presents a systematic framework grounded in social psychology theories to investigate and compare explicit and implicit biases in LLMs. We propose a novel\"self-reflection\"based evaluation framework that operates in two phases: first measuring implicit bias through simulated psychological assessment methods, then evaluating explicit bias by prompting LLMs to analyze their own generated content. Through extensive experiments on state-of-the-art LLMs across multiple social dimensions, we demonstrate that LLMs exhibit a substantial inconsistency between explicit and implicit biases, where explicit biases manifest as mild stereotypes while implicit biases show strong stereotypes. Furthermore, we investigate the underlying factors contributing to this explicit-implicit bias inconsistency. Our experiments examine the effects of training data scale, model parameters, and alignment techniques. Results indicate that while explicit bias diminishes with increased training data and model size, implicit bias exhibits a contrasting upward trend. Notably, contemporary alignment methods (e.g., RLHF, DPO) effectively suppress explicit bias but show limited efficacy in mitigating implicit bias. These findings suggest that while scaling up models and alignment training can address explicit bias, the challenge of implicit bias requires novel approaches beyond current methodologies.",
            "score": 0.6067733819945293,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88720703125
        },
        {
            "corpus_id": "271600942",
            "title": "Downstream bias mitigation is all you need",
            "text": "The use of large language models (LLMs), together with the transformer-based architectures, have significantly advanced the performance of natural language processing (NLP) models for task-specific machine learning (Bommasani, et al. 2021). However, the utilization of extensive pre-training datasets pose recognizable and immediate social risks, in the form of potential harmful prejudices (Strubell, Ganesh and McCallum 2019), (Bender, et al. 2021). These biases are commonly linked to representational issues, including stereotyping and denigration (Barocas, Crawford, et al. 2017), (Bender, et al. 2021). Additionally, various studies like (Bommasani, et al. 2021) detail the harms stemming from the application of fine-tuned LLMs in downstream tasks, including discriminatory medical diagnoses (Zhang, et al. 2020), overreliance on binary gender for coreference resolution (Cao and III 2021), reinforcement of traditional gender roles in part-of-speech tagging (Garimella, et al. 2019), generation of toxic text (Gehman, et al. 2020), and the censorship of inclusive language and AAVE (Park, Shin and Fung 2018), (Sap, et al. 2019). Researchers caution that the escalation of intrinsic bias in the pre-training phase may heighten the risk of downstream harms (Bolukbasi, et al. 2016), (Islam, Bryson and Narayanan 2016). This is called the bias transfer hypothesis, which says that stereotypes and other biased associations present in a pre-trained model are transferred to subsequent post-fine-tuning tasks, potentially causing further task-specific harms. A weaker version of this hypothesis suggests that pre-trained models determine the downstream harms (Bommasani, et al. 2021). Typically, one refers to the bias in a pre-trained language model as the upstream bias and the bias in a fine-tuned model as downstream bias. There has been work in the literature to develop controls to reduce upstream bias, using a meticulous approach in curating the fine-tuning dataset, and scrutinizing for potential harms within the specific context.",
            "score": 0.6063868321018295,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1325
                },
                {
                    "start": 1326,
                    "end": 1562
                },
                {
                    "start": 1563,
                    "end": 1688
                },
                {
                    "start": 1689,
                    "end": 1830
                },
                {
                    "start": 1831,
                    "end": 2044
                }
            ],
            "ref_mentions": [
                {
                    "start": 391,
                    "end": 427,
                    "matchedPaperCorpusId": "174802812"
                },
                {
                    "start": 429,
                    "end": 449,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 586,
                    "end": 607,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 799,
                    "end": 819,
                    "matchedPaperCorpusId": "214590570"
                },
                {
                    "start": 966,
                    "end": 990,
                    "matchedPaperCorpusId": "196181460"
                },
                {
                    "start": 1017,
                    "end": 1038,
                    "matchedPaperCorpusId": "221878771"
                },
                {
                    "start": 1090,
                    "end": 1116,
                    "matchedPaperCorpusId": "52070035"
                },
                {
                    "start": 1118,
                    "end": 1136,
                    "matchedPaperCorpusId": "196211238"
                },
                {
                    "start": 1264,
                    "end": 1288,
                    "matchedPaperCorpusId": "1704893"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66796875
        },
        {
            "corpus_id": "259342087",
            "title": "Prompt Tuning Pushes Farther, Contrastive Learning Pulls Closer: A Two-Stage Approach to Mitigate Social Biases",
            "text": "Pre-trained Language Models (PLMs) have demonstrated outstanding performance in recent years and have been widely used in natural language understanding tasks (Peters et al., 2018;Delobelle et al., 2022). However, the powerful language modeling capability enables PLMs to learn good representations from large-scale training corpora while capturing human-like social biases. Recent studies have demonstrated that the representations encoded by PLMs learn social biases specific to demographic groups (e.g., gender, race, religion) and can be amplified to downstream tasks, leading to unfair outcomes and adverse social effects (Zhao et al., 2019;Webster et al., 2020). As a result, mitigating social biases in PLMs' encoding can improve the fairness of NLP systems significantly (Bolukbasi et al., 2016;Bender and Friedman, 2018).\n\nMost existing debiasing techniques first need to construct sample pairs using Counterfactual Data Augmentation (CDA) (Zmigrod et al., 2019;Wang et al., 2022) to balance the training corpora. The general approach of CDA is to replace the original corpus with attribute words (e.g., he/she, man/woman) specific to different demographic groups. For example, RCDA (Chen et al., 2021) uses a generator to generate a large number of antisense sentences and then uses a discriminator to evaluate the quality of the original and antisense samples jointly. FairFil (Cheng et al., 2021) obtains a pair of positive sample sentences by replacing the attribute words in the training corpora with the antonyms and then uses contrastive learning to train a filter for debiasing. Auto-Debias (Guo et al., 2022) uses pairs of attribute words as training corpora, amplifies the bias between sample pairs by searching biased prompt texts in the Wikipedia vocabulary, and then performs semantic alignment using Jensen-Shannon divergence. These methods aim to mitigate social biases between different demographic groups by narrowing the representation distance between sample pairs. However, CDA slightly modifies the original corpus, limiting the representation distance between different demographic groups to a narrow range. As a result, the debiasing model is easy to overfit the",
            "score": 0.6062829722340486,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 159,
                    "end": 180,
                    "matchedPaperCorpusId": "3626819"
                },
                {
                    "start": 180,
                    "end": 203,
                    "matchedPaperCorpusId": "250390561"
                },
                {
                    "start": 627,
                    "end": 646,
                    "matchedPaperCorpusId": "102352962"
                },
                {
                    "start": 779,
                    "end": 803,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 803,
                    "end": 829,
                    "matchedPaperCorpusId": "52255687"
                },
                {
                    "start": 949,
                    "end": 971,
                    "matchedPaperCorpusId": "184486914"
                },
                {
                    "start": 971,
                    "end": 989,
                    "matchedPaperCorpusId": "247155039"
                },
                {
                    "start": 1388,
                    "end": 1408,
                    "matchedPaperCorpusId": "232185104"
                },
                {
                    "start": 1608,
                    "end": 1626,
                    "matchedPaperCorpusId": "248780440"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7158203125
        },
        {
            "corpus_id": "233848348",
            "title": "The practical ethics of bias reduction in machine translation: why domain adaptation is better than data debiasing",
            "text": "problems, and they are trained on different kinds of data (e.g., text-based, imagebased). Consequently, as discussed at length in this article, debiasing data prior to training, without compromising the performance of the system, will be possible in some cases, but not in others. And a greater awareness of such differences can usefully expose the unhelpful superficiality of any high-level recommendation that all AI data sets must be free from bias prior to training.\n\nThen there is the problem of identifying 'Bias' in the first place. Recently, Blodgett et al. (2020) have argued that several distinct and often underspecified conceptualisations of bias has been prominent in work addressing the skewed datasets used for Natural Language Processing (NLP) tasks, and the authors show how this has fostered terminological imprecision. As Maudslay et al. (2019) put it, the 'measurement and mitigation of gender bias relies on the chosen operationalisation of gender bias. As a direct consequence, how researchers choose to operationalise bias determines both the techniques at one's disposal to mitigate bias, as well as the yardstick by which success is determined'. More specifically, gender bias is not a homogenous phenomenon since it is manifest in linguistic data in several distinct ways. In the context of Machine Translation (MT), the primary focus has been on the representational harms, such as stereotyping, that arise from specific linguistic items and structures (e.g., gendered pronouns). While this captures a manifestation of gender bias that is quite easily identifiable within a dataset, there are numerous other less explicitly marked ways in which stereotyping (for example) becomes manifest. The complexities and subtleties of how power differentials and social hierarchies operate within and through language make the task of simply identifying all gender bias within a dataset incredibly complex. 3 To explore these important issues in greater detail, this article will discuss particular human biases present in the training data used to build Neural Machine Translation (NMT) systems. Crucially, it will be shown that the datasets created to train such systems cannot always be adequately debiased prior to training using existing approaches without considerably diminishing the quality of",
            "score": 0.6053186448691339,
            "section_title": "Introduction",
            "char_start_offset": 4241,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 550,
                    "end": 572,
                    "matchedPaperCorpusId": "218971825"
                },
                {
                    "start": 841,
                    "end": 863,
                    "matchedPaperCorpusId": "202541569"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.619140625
        },
        {
            "corpus_id": "271571036",
            "title": "Understanding the Interplay of Scale, Data, and Bias in Language Models: A Case Study with BERT",
            "text": "Large Language Models (LLMs) continue to grow in size at a remarkable rate, with technology companies investing millions in infrastructure to produce ever-larger and more general purpose models. Modern open weight models like LLaMA, Gemini and Falcon regularly have tens of billions of parameters, showcasing noteworthy capabilities across a range of natural language processing applications. \n\nTo investigate the performance of LLMs in terms of model parameters, training data size, and compute resources, a rich literature in empirical scaling laws (Hernandez et al. 2021;Kaplan et al. 2020) has emerged, which suggests that bigger is indeed better (in terms of loss). Recent work on scaling laws has also led to a more comprehensive understanding on the tradeoffs between data size and model parameters with a fixed compute budget (Hoffmann et al. 2022). Given the pace of LLM development and the foundational role of scale, studying changes in model behavior with size remains a pressing research problem. \n\nOne crucial question that has received less attention, however, is how model scale influences social biases. LLMs inherently absorb societal biases and harmful stereotypes from data during both pre-training and task-specific fine-tuning. These biases manifest as intrinsic biases within the embedding space, leading to representational harms and stereotyping (Nangia et al. 2020;Nadeem, Bethke, and Reddy 2020;May et al. 2019;Kurita et al. 2019), and extrinsic biases leading to allocative harms (Barocas et al. 2017) in downstream predictions (Gehman et al. 2020;Garimella et al. 2019;Blodgett, Wei, and O'Connor 2018). Prior work has shown that pre-trained models can generate toxic language (Gehman et al. 2020), can have disparities in hate speech classification (Sap et al. 2019), can perpetuate anti-Muslim bias in text generation (Abid, Farooqi, and Zou 2021), can rely on racial biases even for high stake use cases such as clinical notes (Zhang et al. 2020), among other failures. \n\nThe growing scale of LLMs has been driven, in part, due to their popularity in commercially successful chat applications (e.g.",
            "score": 0.6048960341501803,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 392
                },
                {
                    "start": 395,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 1009
                },
                {
                    "start": 1012,
                    "end": 1120
                },
                {
                    "start": 1121,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1632
                },
                {
                    "start": 1633,
                    "end": 2001
                },
                {
                    "start": 2004,
                    "end": 2130
                }
            ],
            "ref_mentions": [
                {
                    "start": 574,
                    "end": 593,
                    "matchedPaperCorpusId": "59599752"
                },
                {
                    "start": 1371,
                    "end": 1391,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 1391,
                    "end": 1422,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 1422,
                    "end": 1438,
                    "matchedPaperCorpusId": "59599752"
                },
                {
                    "start": 1438,
                    "end": 1457,
                    "matchedPaperCorpusId": "59599752"
                },
                {
                    "start": 1556,
                    "end": 1576,
                    "matchedPaperCorpusId": "221878771"
                },
                {
                    "start": 1576,
                    "end": 1598,
                    "matchedPaperCorpusId": "196181460"
                },
                {
                    "start": 1598,
                    "end": 1631,
                    "matchedPaperCorpusId": "51870345"
                },
                {
                    "start": 1706,
                    "end": 1726,
                    "matchedPaperCorpusId": "221878771"
                },
                {
                    "start": 1959,
                    "end": 1978,
                    "matchedPaperCorpusId": "214590570"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.693359375
        },
        {
            "corpus_id": "235829093",
            "title": "Generating Gender Augmented Data for NLP",
            "text": "Recent studies have exposed challenging systematic issues related to bias that extend to a range of AI applications, including Natural Language Processing (NLP) technology (Costa-juss\u00e0, 2019;Blodgett et al., 2020). Observed bias problems range from copying biases already existing in data to claims that the training process can lead to an exacerbation or amplification of observed biases (Zhou and Schiebinger, 2018;Vanmassenhove et al., 2021). The algorithms learn to maximize the overall probability of an occurrence, leading to preferences for more frequently appearing training patterns. \n\nWith this work, we propose a method for generating (more) balanced data in terms of one of the main types of bias frequently observed in language: gender bias. Gender bias can occur in language due to the fact that some languages have a way of explicitly marking (natural or grammatical) gender while others do not (Stahlberg et al., 2007). Gender bias in translation is usually manifested when animate entities (e.g. professions) are translated from gender neutral language (e.g. English) into a gendered language (e.g. Spanish) because the instances seen in training data are biased. Also, conversational utterances are prone to bias, both in machine translation as well as in other NLP applications, because systems often do not have the ability to provide multiple gender variants. Therefore, users are simply presented with the most probable option which is prone to bias. In our work, we aim to enable the generation of multiple gender variants by expanding each sentence with the missing gender variants, thus fostering inclusion in online conversations/NLP applications. Generating gender variants can and should also be used to create gender balanced conversational data that can be used to train less biased NLP models such as machine translation models, language models, chat bots, etc. \n\nUnlike previous studies, we did not want to limit ourselves to one specific gender phenomenon, such as gender markings on professions (Zmigrod et al., 2019)) (for which the gender can easily be swapped by using hand-crafted lists) or first person personal pronouns (Habash et al., 2019)).",
            "score": 0.6039506577176832,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 592
                },
                {
                    "start": 595,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1012
                },
                {
                    "start": 1013,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1472
                },
                {
                    "start": 1473,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1892
                },
                {
                    "start": 1895,
                    "end": 2183
                }
            ],
            "ref_mentions": [
                {
                    "start": 172,
                    "end": 191,
                    "matchedPaperCorpusId": "204540019"
                },
                {
                    "start": 191,
                    "end": 213,
                    "matchedPaperCorpusId": "218971825"
                },
                {
                    "start": 389,
                    "end": 417,
                    "matchedPaperCorpusId": "51677390"
                },
                {
                    "start": 417,
                    "end": 444,
                    "matchedPaperCorpusId": "231740565"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.406982421875
        },
        {
            "corpus_id": "248366292",
            "title": "Towards an Enhanced Understanding of Bias in Pre-trained Neural Language Models: A Survey with Special Emphasis on Affective Bias",
            "text": "We also give special attention to the less explored area of social biases in the context of affect i.e., Affective Bias (or emotion associated bias) specific to large PLMs. Since affective computing has potential applications in many natural language understanding tools and real-word systems (healthcare [44], business [54,105], education [34,105], etc.), it is highly necessary to study the existence of affective biases, if any, in these systems that could potentially harm or do injustice towards protected social groups based on affect. We review more than 100 papers that address bias in PLMs including non-contextual and contextual models. We collect research papers from ACL anthology, Google Scholar and arXiv, using the keywords 'bias', 'fairness', 'bias in NLP', 'fairness in NLP', 'Sentiment bias', 'Affective bias', 'Emotion bias' 'bias in pre-trained language models', etc. as the inclusion criteria for our survey. The major contributions of this survey are summarised below: \n\n\u2022 We present a comprehensive survey of bias in pre-trained language models, especially an in-depth treatment of various kinds of bias that originate in transformer based contextual pre-trained language models in NLP along with their identification, quantification and mitigation strategies. \u2022 We, for the first time, to the best of our knowledge, investigate Affective Bias, a highly socially relevant and less addressed problem, specifically in the context of large pre-trained language models. \u2022 We collect and present a large number of available bias evaluation corpora along with their suitability to evaluate large pre-trained language models. \u2022 We also discuss present research challenges in large pre-trained language models and affective biases. \n\nThe rest of the paper is organized as, the background of PLMs and bias in PLMs provided in section 2, quantifying PLM bias in section 3, mitigating PLM bias in section 4, affective bias in PLMs including their identification and mitigation strategies in section 5, a list of available bias evaluation corpora in section 6, research challenges in section 7 and concluding remarks in section 8.",
            "score": 0.6039360090741256,
            "section_title": "Heterogeneous View of Bias in PLMs",
            "char_start_offset": 5127,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 929
                },
                {
                    "start": 930,
                    "end": 990
                },
                {
                    "start": 993,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1488
                },
                {
                    "start": 1489,
                    "end": 1641
                },
                {
                    "start": 1642,
                    "end": 1746
                },
                {
                    "start": 1749,
                    "end": 2141
                }
            ],
            "ref_mentions": [
                {
                    "start": 305,
                    "end": 309,
                    "matchedPaperCorpusId": "2313204"
                },
                {
                    "start": 320,
                    "end": 324,
                    "matchedPaperCorpusId": "32437011"
                },
                {
                    "start": 324,
                    "end": 328,
                    "matchedPaperCorpusId": "212705218"
                },
                {
                    "start": 340,
                    "end": 344,
                    "matchedPaperCorpusId": "192646023"
                },
                {
                    "start": 344,
                    "end": 348,
                    "matchedPaperCorpusId": "212705218"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85302734375
        },
        {
            "corpus_id": "267027727",
            "title": "The Gaps between Pre-train and Downstream Settings in Bias Evaluation and Debiasing",
            "text": "Our study has the following limitations. We used the LaMini series (Wu et al., 2023) for our experiments because we needed to fine-tune models. To investigate larger PLMs such as LLaMa (Touvron et al., 2023) and Flan-T5 (Chung et al., 2022) have the same tendencies, they need to be verified in environments with rich computation resources. We only used QA, NLI, and coreference resolution as downstream tasks for our experiments. As more evaluation data for assessing social biases in downstream tasks becomes available in the future, the conclusions from our experiments should be analyzed across a broader range of datasets. \n\nThere are numerous types of social biases, such as race and religion, encoded in PLMs (Meade et al., 2022), but we consider only gender bias in this work. Moreover, we only focus on binary gender and plan consider non-binary gender in our future work (Ovalle et al., 2023). In addition, we only consider the English language, a morphologically limited language. As some research points out, social biases also exist in multilingual PLMs (Kaneko et al., 2022b;Levy et al., 2023), which require further investigations.",
            "score": 0.603839697538024,
            "section_title": "Limitations",
            "char_start_offset": 12924,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 40
                },
                {
                    "start": 41,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 627
                },
                {
                    "start": 630,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1146
                }
            ],
            "ref_mentions": [
                {
                    "start": 716,
                    "end": 736,
                    "matchedPaperCorpusId": "239015827"
                },
                {
                    "start": 881,
                    "end": 902,
                    "matchedPaperCorpusId": "258741207"
                },
                {
                    "start": 1067,
                    "end": 1089,
                    "matchedPaperCorpusId": "248496913"
                },
                {
                    "start": 1089,
                    "end": 1107,
                    "matchedPaperCorpusId": "258823009"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.35693359375
        },
        {
            "corpus_id": "256390197",
            "title": "How Far Can It Go? On Intrinsic Gender Bias Mitigation for Text Classification",
            "text": "The use of pretrained language models has seen a surge in popularity as a result of state-of-the-art performances that have been achieved with these models on various tasks. Consequently, there has been a growing interest in exploring how gender bias pertains in these models (Garrido-Mu\u00f1oz et al., 2021). Pretrained language models are used in two distinct phases: the pretraining phase and the finetuning phase. The pretraining phase typically involves training a model on a generic task such as masked language modeling on a diverse set of text corpora. In the finetuning phase, the pretrained model can be adapted for a specific task, such as sentiment analysis, by training on a domain-specific corpus. Owing to the unique way of using pretrained models, bias generally manifests in two forms: intrinsic bias and extrinsic bias. Intrinsic bias refers to bias that inherently exists in pretrained language models whereas extrinsic bias is used to refer to bias that exists in downstream models that are based on the pretrained model. Since the success of downstream NLP tasks has mostly been dependent on pretrained models, it is intuitive to assume that bias in intrinsic models will compromise fairness in downstream tasks. Only recently have more indepth examinations been done to investigate this assumption (Steed et al., 2022;Orgad et al., 2022;Kurita et al., 2019). However, conclusions have been inconsistent and the confounding effects of bias mitigation techniques remain unknown.\n\nThe main focus of this work is to investigate the impact of intrinsic bias on extrinsic fairness and if techniques to mitigate intrinsic bias actually resolve bias or only mask it. We develop a probe to uncover intrinsic bias by determining the amount of gender information in a word embedding using a classifier. Bearing in mind how abstractly and improperly intrinsic bias has been defined (Blodgett et al., 2020), coupled with the discovery that results from different metrics for intrinsic bias in many cases do not correlate (Delobelle et al., 2022), we find this probe effective as an extra step in evaluating the efficacy of these mitigation strategies. We realize from this study that how intrinsic bias has been measured and",
            "score": 0.6028954915332382,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1888,
                    "end": 1911,
                    "matchedPaperCorpusId": "218971825"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.444580078125
        },
        {
            "corpus_id": "254017505",
            "title": "Undesirable biases in NLP: Averting a crisis of measurement",
            "text": "In recent years various examples of NLP applications were brought to the public attention that behaved in ways that are harmful for certain individuals or groups: Systems for matching vacancies may unintentionally disadvantage ethnic minorities or people with disabilities (Hutchinson et al., 2020), machine translation systems have been found to translate gender-neutral terms to the majority gender, which can amplify existing gender biases (Stanovsky et al., 2019), speech recognition systems have difficulties to correctly recognize the voices of speakers of minority dialects (Zhang et al., 2022b), and, more generally, the biases and misinformation that generative models propagate can distort people's worldviews in unprecedented ways (Kidd & Birhane, 2023). \n\nTo combat these effects of language technology on society, detecting undesirable biases in Large Language Models and other NLP systems, and finding ways to mitigate them, has emerged as a new and active domain of NLP research. However, both detection and mitigation face problems. One of these challenges is that we lack sound tools to measure bias that is present in NLP systems. While there had been a lot of excitement about some early methods used to make bias in such systems visible (e.g., Bolukbasi et al., 2016;Caliskan et al., 2017), more recent work has shown that these (as well as newer) methods are problematic. Many problems have been pointed out for how bias is defined and operationalized (see e.g., Blodgett et al., 2020Blodgett et al., , 2021;;Dev et al., 2022;Ethayarajh et al., 2019;Gonen & Goldberg, 2019;Nissim et al., 2020;Talat et al., 2022). There are also concrete issues with the measurement results, for instance, for some of the bias measures currently in use little to no evidence has been found that they correlate to other bias measures or downstream harms (e.g., Cao et al., 2022a;Delobelle et al., 2022;Goldfarb-Tarrant et al., 2021).",
            "score": 0.6027841950887431,
            "section_title": "Introduction",
            "char_start_offset": 1257,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 765
                },
                {
                    "start": 768,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1634
                },
                {
                    "start": 1635,
                    "end": 1936
                }
            ],
            "ref_mentions": [
                {
                    "start": 273,
                    "end": 298,
                    "matchedPaperCorpusId": "218487466"
                },
                {
                    "start": 443,
                    "end": 467,
                    "matchedPaperCorpusId": "173991101"
                },
                {
                    "start": 581,
                    "end": 602,
                    "matchedPaperCorpusId": "252346024"
                },
                {
                    "start": 742,
                    "end": 764,
                    "matchedPaperCorpusId": "259223197"
                },
                {
                    "start": 1264,
                    "end": 1287,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 1287,
                    "end": 1309,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 1484,
                    "end": 1505,
                    "matchedPaperCorpusId": "218971825"
                },
                {
                    "start": 1505,
                    "end": 1530,
                    "matchedPaperCorpusId": "236460302"
                },
                {
                    "start": 1530,
                    "end": 1547,
                    "matchedPaperCorpusId": "252907216"
                },
                {
                    "start": 1547,
                    "end": 1571,
                    "matchedPaperCorpusId": "196182395"
                },
                {
                    "start": 1571,
                    "end": 1594,
                    "matchedPaperCorpusId": "73729169"
                },
                {
                    "start": 1594,
                    "end": 1614,
                    "matchedPaperCorpusId": "165163511"
                },
                {
                    "start": 1614,
                    "end": 1633,
                    "matchedPaperCorpusId": "247626152"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.486328125
        },
        {
            "corpus_id": "259075978",
            "title": "Exposing Bias in Online Communities through Large-Scale Language Models",
            "text": "In this work, we fine-tune a language model with social media conversations which are often part of the underlying data of conversational language models. Henderson et al. examine popular dialogue datasets with a linguistic bias detection tool and a classifier for offensive language (Henderson et al. 2018). When training conversational language models on Twitter datasets, they find that the models and datasets exhibit biases to a similar extent. This indicates that when training datasets are biased, the dialogue models manifest this bias similarly. \n\nLiu et al. create a benchmark dataset to study bias in dialogue models in two dimensions: gender and race (Liu et al. 2019). Unlike other approaches, Liu et al. do not use descriptive words for race, but instead use standard English and African-American Vernacular English as a distinction. The type of language used can be a more realistic indicator for conversational texts. They measure bias through sentiment, politeness, and diversity. If a conversational model is less diverse for a particular group of people, it might discourage people from this group from using these technologies. Barikeri et al. present REDDITBIAS, a real-world dataset to measure bias in conversational language models that considers religion, race, gender, and queerness (Barikeri et al. 2021). Each dimension consists of a pair of opposing demographics; a dominant and a minoritised group (e.g., Christians and Muslims). The authors retrieve comments from Reddit that mention a target group and a corresponding stereotype. They observe the probability that a language model generates a stereotypically biased phrase compared to an inversely biased phrase. They find that DialoGPT (Zhang et al. 2020) is stereotypically biased in terms of religion but is biased in the anti-stereotypical direction for queerness and race. We adopt bias dimensions and demographic keywords similar to the ones in REDDITBIAS. However, instead of limiting demographics to a dominant and minoritised group, we compare up to four different demographics per dimension.",
            "score": 0.6026784204680775,
            "section_title": "Bias in Dialogue Generation",
            "char_start_offset": 9953,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 554
                },
                {
                    "start": 557,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1560
                },
                {
                    "start": 1561,
                    "end": 1693
                },
                {
                    "start": 1694,
                    "end": 1858
                },
                {
                    "start": 1859,
                    "end": 1943
                },
                {
                    "start": 1944,
                    "end": 2082
                }
            ],
            "ref_mentions": [
                {
                    "start": 284,
                    "end": 307,
                    "matchedPaperCorpusId": "33499714"
                },
                {
                    "start": 1308,
                    "end": 1330,
                    "matchedPaperCorpusId": "235358955"
                },
                {
                    "start": 1718,
                    "end": 1736,
                    "matchedPaperCorpusId": "207869708"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68017578125
        },
        {
            "corpus_id": "261049466",
            "title": "A Survey on Fairness in Large Language Models",
            "text": "Large Language Models (LLMs), such as BERT [1], GPT-3 [2], and LLaMA [3], have shown powerful performance and development prospects in various tasks of Natural Language Processing (NLP), and have an increasingly wide impact in the real world. Their pre-training relies on large corpora from various sources, especially for larger-scale LLMs. However, numerous studies have verified that LLMs capture human-like social biases in unprocessed training data [4,5]. These social biases can be encoded in the embeddings and carried over to decisions in downstream tasks, compromising the fairness of LLMs. Unfair LLM systems make discriminatory, stereotypic and demeaning decisions against vulnerable or marginalized demographics, causing undesirable social impacts and potential harms [6,7]. For example, GPT-3 is found to associate males with higher levels of education and greater occupational competence, when asked GPT-3 that \"What is the gender of the doctor?\" and \"What is the gender of the nurse?\", its preferred outputs are \"A: Doctor is a masculine noun;\" and \"It's female.\", respectively. In real-world applications, the automatic resume filtering systems can be gender-biased, which tend to assign programmer jobs to men and homemaker jobs to women [8,9,10], and the US healthcare system can be racial biased, which judges black patients with the same risk level to be sicker than white patients [11]. \n\nThe fairness issue of LLMs with pre-training and fine-tuning paradigm has been relatively extensively studied, including bias evaluation methods, debiasing methods, etc. With the rapid development of LLMs, the data of the pre-trained corpus and the parameters of the model continue to climb. The size distribution of LLMs can range from millions of parameters to hundreds of billion parameters, which has spawned the widespread application of the prompting paradigm on large-sized LLMs. However, the larger number of parameters and the new training paradigm bring new problems and challenges to the fairness research of LLMs.",
            "score": 0.6022610091335996,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1407
                },
                {
                    "start": 1410,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1701
                },
                {
                    "start": 1702,
                    "end": 1896
                },
                {
                    "start": 1897,
                    "end": 2035
                }
            ],
            "ref_mentions": [
                {
                    "start": 54,
                    "end": 57,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 454,
                    "end": 457,
                    "matchedPaperCorpusId": "4930886"
                },
                {
                    "start": 457,
                    "end": 459,
                    "matchedPaperCorpusId": "195316733"
                },
                {
                    "start": 780,
                    "end": 783,
                    "matchedPaperCorpusId": "218971825"
                },
                {
                    "start": 783,
                    "end": 785,
                    "matchedPaperCorpusId": "252907607"
                },
                {
                    "start": 1255,
                    "end": 1258,
                    "matchedPaperCorpusId": "58006082"
                },
                {
                    "start": 1258,
                    "end": 1260,
                    "matchedPaperCorpusId": "220040093"
                },
                {
                    "start": 1402,
                    "end": 1406,
                    "matchedPaperCorpusId": "204881868"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81005859375
        },
        {
            "corpus_id": "268692065",
            "title": "RuBia: A Russian Language Bias Detection Dataset",
            "text": "Large language models (LLMs) are trained on primarily unfiltered text corpora which contain many instances of prejudice or bigotry being displayed.Learning to predict the contents of these corpora, the LLMs inherit most of the social biases present in the data.Moreover, they have been shown to use these biases when applied to real-life downstream tasks, reinforcing harmful social tropes and constructs (Zhao et al., 2018;Sheng et al., 2019).For instance, non-debiased LLM solving the task of coreference resolution tend to associate male pronouns with stereotypically masculine jobs (physician, scientist) and female pronouns with stereotypically feminine jobs (Bolukbasi et al., 2016).\n\nIn recent work, diagnostic tools for measuring bias came into focus.Specialized datasets are designed and collected via crowdsourcing with the aim of contrastive evaluation (Zhao et al., 2018;Nadeem et al., 2021;Nangia et al., 2020;Levy et al., 2021).These datasets consist of sets of both more and less biased sentences.This way, LLMs can be rated based on how likely they are to prefer a more biased sentence to a less biased one.While multiple of such datasets exist, almost all of them are in English and can only be used to evaluate English language models, while the LLM's pre-training method is widely applied to many other languages (Chung et al., 2020).\n\n\u22c6 denotes equal contribution \u221e now at Toloka AI In this work, we propose RuBia-a bias detection dataset for the Russian language specifically, inspired by both modern bias detection datasets and the earlier template-based works (Kurita et al., 2019).To achieve this, we employ the practices adopted by other researchers in the area while adapting them to the different sociolinguistic environment.We also take into account recent comparative studies of existing datasets (Blodgett et al., 2020(Blodgett et al., , 2021;;Orgad and Belinkov, 2022) and try to avoid the most common pitfalls, such as lack of precise definitions and inclusion of unclear stereotypes.",
            "score": 0.6019784481107725,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 147,
                    "end": 261
                },
                {
                    "start": 261,
                    "end": 444
                },
                {
                    "start": 444,
                    "end": 689
                },
                {
                    "start": 691,
                    "end": 759
                },
                {
                    "start": 759,
                    "end": 942
                },
                {
                    "start": 942,
                    "end": 1012
                },
                {
                    "start": 1012,
                    "end": 1123
                },
                {
                    "start": 1123,
                    "end": 1353
                },
                {
                    "start": 1355,
                    "end": 1605
                },
                {
                    "start": 1605,
                    "end": 1752
                },
                {
                    "start": 1752,
                    "end": 2016
                }
            ],
            "ref_mentions": [
                {
                    "start": 424,
                    "end": 443,
                    "matchedPaperCorpusId": "202537041"
                },
                {
                    "start": 664,
                    "end": 688,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 923,
                    "end": 941,
                    "matchedPaperCorpusId": "237452751"
                },
                {
                    "start": 1583,
                    "end": 1604,
                    "matchedPaperCorpusId": "190000105"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.423828125
        },
        {
            "corpus_id": "270878706",
            "title": "Breaking Bias, Building Bridges: Evaluation and Mitigation of Social Biases in LLMs via Contact Hypothesis",
            "text": "Large Language Models (LLMs) perpetuate social biases, reflecting prejudices in their training data and reinforcing societal stereotypes and inequalities. Our work explores the potential of the Contact Hypothesis, a concept from social psychology for debiasing LLMs. We simulate various forms of social contact through LLM prompting to measure their influence on the model\u2019s biases, mirroring how intergroup interactions can reduce prejudices in social contexts. We create a dataset of 108,000 prompts following a principled approach replicating social contact to measure biases in three LLMs (LLaMA 2, Tulu, and NousHermes) across 13 social bias dimensions. We propose a unique debiasing technique, Social Contact Debiasing (SCD), that instruction-tunes these models with unbiased responses to prompts. Our research demonstrates that LLM responses exhibit social biases when subject to contact probing, but more importantly, these biases can be significantly reduced by up to 40% in 1 epoch of instruction tuning LLaMA 2 following our SCD strategy.",
            "score": 0.6014729943329018,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.845703125
        },
        {
            "corpus_id": "250390904",
            "title": "Benchmarking Intersectional Biases in NLP",
            "text": "As state-of-the-art natural language processing (NLP) language models become increasingly powerful and pervasive, recent progress in NLP has underscored the need for deeper analyses of how such models perform with respect to underrepresented groups. Research on fairness in NLP has shown that distributed representations of words often encode stereotypes -particularly towards different demographic groups (Blodgett et al., 2020;Bender et al., 2021). There is a growing stream of research that looks at mitigating these biases, especially when it manifests in the learned embedding state (Bolukbasi et al., 2016;Zmigrod et al., 2019;Kaneko and Bollegala, 2021). While prior work has undoubtedly moved the needle, recent surveys and research articles have identified several important gaps and issues (Blodgett et al., 2020;Tan and Celis, 2019). First, much of the current work on examining NLP bias (and proposing debiasing strategies) has focused on representational harm -how a model describes certain groups, including stereotyping and other misrepresentations (Blodgett et al., 2020;Suresh and Guttag, 2019). Conversely, there has been far less work exploring allocational harm in downstream NLP prediction tasks -when a system distributes resources or opportunities differently (Blodgett et al., 2020;Suresh and Guttag, 2019). Downstream tasks, such as sequence classification, also affect underrepresented groups, as these models show disparate impact on various demographic subsets, including women, African Americans, and the elderly (Blodgett et al., 2020;Bender et al., 2021;Shah et al., 2020). \n\nSecond, there has been limited work that examines intersectional bias across a wide array of relevant charactersitics, including several demographic dimensions, for a variety of non-debiased and debiased embeddings,1 on a multitude of downstream tasks. Some work has studied demographic intersections such as young men and old women from a theoretical perspective (e.g., Kearns et al., 2018).",
            "score": 0.6011996004903911,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 1112
                },
                {
                    "start": 1113,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1604
                },
                {
                    "start": 1607,
                    "end": 1859
                },
                {
                    "start": 1860,
                    "end": 1999
                }
            ],
            "ref_mentions": [
                {
                    "start": 406,
                    "end": 429,
                    "matchedPaperCorpusId": "218971825"
                },
                {
                    "start": 429,
                    "end": 449,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 588,
                    "end": 612,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 612,
                    "end": 633,
                    "matchedPaperCorpusId": "184486914"
                },
                {
                    "start": 633,
                    "end": 660,
                    "matchedPaperCorpusId": "231698657"
                },
                {
                    "start": 800,
                    "end": 823,
                    "matchedPaperCorpusId": "218971825"
                },
                {
                    "start": 823,
                    "end": 843,
                    "matchedPaperCorpusId": "202781363"
                },
                {
                    "start": 1064,
                    "end": 1087,
                    "matchedPaperCorpusId": "218971825"
                },
                {
                    "start": 1283,
                    "end": 1306,
                    "matchedPaperCorpusId": "218971825"
                },
                {
                    "start": 1542,
                    "end": 1565,
                    "matchedPaperCorpusId": "218971825"
                },
                {
                    "start": 1565,
                    "end": 1585,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 1585,
                    "end": 1603,
                    "matchedPaperCorpusId": "209461005"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58349609375
        },
        {
            "corpus_id": "248366292",
            "title": "Towards an Enhanced Understanding of Bias in Pre-trained Neural Language Models: A Survey with Special Emphasis on Affective Bias",
            "text": "The remarkable progress in Natural Language Processing (NLP) brought about by deep learning, particularly with the recent advent of large pre-trained neural language models, is brought into scrutiny as several studies began to discuss and report potential biases in NLP applications. Bias in NLP is found to originate from latent historical biases encoded by humans into textual data which gets perpetuated or even amplified by NLP algorithm. We present a survey to comprehend bias in large pre-trained language models, analyze the stages at which they occur in these models, and various ways in which these biases could be quantified and mitigated. Considering wide applicability of textual affective computing based downstream tasks in real-world systems such as business, healthcare, education, etc., we give a special emphasis on investigating bias in the context of affect (emotion) i.e., Affective Bias, in large pre-trained language models. We present a summary of various bias evaluation corpora that help to aid future research and discuss challenges in the research on bias in pre-trained language models. We believe that our attempt to draw a comprehensive view of bias in pre-trained language models, and especially the exploration of affective bias will be highly beneficial to researchers interested in this evolving field.",
            "score": 0.601143248393121,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88671875
        },
        {
            "corpus_id": "248218643",
            "title": "Identifying and Measuring Token-Level Sentiment Bias in Pre-trained Language Models with Prompts",
            "text": "Large-scale pre-trained language models (PLMs), such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), GPT (Radford et al., 2018(Radford et al., , 2019;;Brown et al., 2020) and T5 (Raffel et al., 2020), have shown competitive performance in many downstream applications in natural language processing. The key to the success of PLMs lies in the unsupervised pre-training on massive unlabeled corpus as well as a large number of parameters in the neural models. While these PLMs have been deployed to a wide variety of products and services such as search engines and chatbots, investigating the fairness of these PLMs has become a growing urgent research agenda. \n\nRecent studies have shown that there are various stereotypical biases related to social factors such as gender (Bhardwaj et al., 2021), race (Iandola et al., 2020), religion (Nadeem et al., 2021), age (Nangia et al., 2020), ethnicity (Groenwold et al., 2020), political identity (McGuffie and Newhouse, 2020), disability (Hutchinson et al., 2020), name (Shwartz et al., 2020) and many more, that are inherited by these PLMs. However, sentiment bias, which characterizes the bias of words towards a particular sentiment polarity, such as positive or negative, has not been well studied, yet it has significant impact to many applications, such as market analysis or chat bots, where subtle bias may lead to distinct understanding or conclusions. Huang et al. (2020) investigated the sentiment bias in texts generated by language models like GPT while overlooking the fact that each individual word may also have sentiment bias in the PLMs. \n\nIn this work, we focus on identifying and measuring the sentiment bias of individual words in pre-trained language models.",
            "score": 0.6009970370688738,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 671
                },
                {
                    "start": 674,
                    "end": 1098
                },
                {
                    "start": 1099,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1612
                },
                {
                    "start": 1615,
                    "end": 1737
                }
            ],
            "ref_mentions": [
                {
                    "start": 61,
                    "end": 82,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 137,
                    "end": 162,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 162,
                    "end": 181,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 189,
                    "end": 210,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 785,
                    "end": 808,
                    "matchedPaperCorpusId": "221586314"
                },
                {
                    "start": 848,
                    "end": 869,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 875,
                    "end": 896,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 1419,
                    "end": 1438,
                    "matchedPaperCorpusId": "207847197"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67431640625
        },
        {
            "corpus_id": "227151662",
            "title": "Argument from Old Man\u2019s View: Assessing Social Bias in Argumentation",
            "text": "In recent years, research on different types of bias in natural language has received a considerable amount of attention. Media bias is one prominent example (Fan et al., 2019), particular the political bias of news articles (Chen et al., 2020). In various sub-fields of NLP, studies on media bias are concerned with analyzing techniques utilized by media outlets when reporting news. These include the framing of an event by phrasing the report with positive or negative terms, and selective reporting by including or omitting facts depending on the tone and (political) stance a media outlet wants to convey (Chen et al., 2018;Hamborg, 2020;Lim et al., 2020). We do not target media bias here but social bias. \n\nSocial bias can emerge from pre-existing stereotypes towards any social group (Sweeney and Najafian, 2019), often leading to prejudices and discrimination. Stereotypes are understood as \"beliefs about the characteristics of group members\" (Fiske, 2004). They may be so powerful that they do not only limit the freedom of individuals, but also cause hate, exclusion, and -in the worst case -extermination (Fiske, 1993). Even if stereotypes, and with that social biases, are individually controllable, they persist to this day; prominent examples of social groups that have historically been subject to bias are ethnicities, genders, and age groups (Fiske, 1998). A major factor in carrying and reinforcing those biases is language (Sap et al., 2020). In spoken and written argumentation and debates, biased language can be present if, for example, one sides argues in self-interest (Zenker, 2011) to favor a certain social group or uses unbalanced arguments (Kienpointner and Kindt, 1997). \n\nAs CA methods are receiving more and more attention (Stede and Schneider, 2018), it is important to understand how existing social biases influence them. One possible source is the human-generated data on which automated systems are trained and evaluated (Chang et al., 2019). In CA, one of the main sources of data are online debate portals, both for research and for applications (Ajjour et al., 2019).",
            "score": 0.6007375720778528,
            "section_title": "Related Work",
            "char_start_offset": 4636,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 711
                },
                {
                    "start": 714,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1463
                },
                {
                    "start": 1464,
                    "end": 1702
                },
                {
                    "start": 1705,
                    "end": 1858
                },
                {
                    "start": 1859,
                    "end": 1981
                },
                {
                    "start": 1982,
                    "end": 2109
                }
            ],
            "ref_mentions": [
                {
                    "start": 158,
                    "end": 176,
                    "matchedPaperCorpusId": "202537179"
                },
                {
                    "start": 225,
                    "end": 244,
                    "matchedPaperCorpusId": "224814439"
                },
                {
                    "start": 610,
                    "end": 629,
                    "matchedPaperCorpusId": "53222920"
                },
                {
                    "start": 629,
                    "end": 643,
                    "matchedPaperCorpusId": "220056948"
                },
                {
                    "start": 643,
                    "end": 660,
                    "matchedPaperCorpusId": "218627041"
                },
                {
                    "start": 792,
                    "end": 820,
                    "matchedPaperCorpusId": "196185011"
                },
                {
                    "start": 1361,
                    "end": 1374,
                    "matchedPaperCorpusId": "152778440"
                },
                {
                    "start": 1444,
                    "end": 1462,
                    "matchedPaperCorpusId": "207853290"
                },
                {
                    "start": 1671,
                    "end": 1701,
                    "matchedPaperCorpusId": "17492000"
                },
                {
                    "start": 1960,
                    "end": 1980,
                    "matchedPaperCorpusId": "209325068"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.40869140625
        },
        {
            "corpus_id": "277667520",
            "title": "Benchmarking Adversarial Robustness to Bias Elicitation in Large Language Models: Scalable Automated Assessment with LLM-as-a-Judge",
            "text": "Large Language Models (LLMs) have empowered artificial intelligence with their remarkable natural language understanding and generation capabilities, enabling breakthroughs in tasks such as machine translation, summarization, and human-like conversation [1,2]. However, their increasing integration into societal domainsincluding healthcare [3], education [4], and law [5]-has amplified concerns about embedded biases. These biases, which can manifest in various forms, risk perpetuating stereotypes, marginalizing underrepresented groups, and undermining ethical AI deployment [6]. Biases may stem from various sources, including biased training data that reflects historical inequalities and prejudicial associations, linguistic imbalances in corpora, flaws in algorithmic design, and the uncritical use of AI systems [7,8]. Previous studies have quantified biased attitudes in language models related to various social groups [9,10], also finding that state-of-the-art LLMs can be manipulated via adversarial attacks to produce biased or harmful responses, despite their bias mitigation and alignment mechanisms [11]. These challenges necessitate rigorous methodologies for evaluating and mitigating biases while ensuring models remain robust against adversarial exploitation. However, current approaches to bias evaluation face critical limitations, including the substantial resources required for bias identification and mitigation, difficulties in acquiring representative datasets for safety assessment, and the absence of universally accepted bias metrics. \n\nTo address these gaps, this work proposes a scalable methodology for benchmarking LLMs against bias elicitation. Our approach follows a two-step process and leverages the LLM-as-a-Judge paradigm [12] to automate bias evaluation, reducing reliance on manual response annotation while ensuring scalability and reproducibility. The first step involves selecting a judge model based on its statistical agreement with human annotations on a curated dataset of prompt-response pairs. These pairs capture both biased and safe behaviors, providing a benchmark for evaluating model ability to discern harmful content. Once chosen, the judge model is used to systematically evaluate LLM robustness using bias-probing prompts across multiple sociocultural dimensions, encompassing both isolated and intersectional bias categories.",
            "score": 0.6005886803815901,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 1120
                },
                {
                    "start": 1121,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1565
                },
                {
                    "start": 1568,
                    "end": 1680
                },
                {
                    "start": 1681,
                    "end": 1892
                },
                {
                    "start": 1893,
                    "end": 2045
                },
                {
                    "start": 2046,
                    "end": 2176
                },
                {
                    "start": 2177,
                    "end": 2387
                }
            ],
            "ref_mentions": [
                {
                    "start": 254,
                    "end": 257,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 257,
                    "end": 259,
                    "matchedPaperCorpusId": "259360395"
                },
                {
                    "start": 341,
                    "end": 344,
                    "matchedPaperCorpusId": "257312905"
                },
                {
                    "start": 356,
                    "end": 359,
                    "matchedPaperCorpusId": "265315253"
                },
                {
                    "start": 369,
                    "end": 372,
                    "matchedPaperCorpusId": "267413187"
                },
                {
                    "start": 578,
                    "end": 581,
                    "matchedPaperCorpusId": "258688053"
                },
                {
                    "start": 820,
                    "end": 823,
                    "matchedPaperCorpusId": "237298625"
                },
                {
                    "start": 823,
                    "end": 825,
                    "matchedPaperCorpusId": "261530629"
                },
                {
                    "start": 929,
                    "end": 932,
                    "matchedPaperCorpusId": "265212726"
                },
                {
                    "start": 932,
                    "end": 935,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 1115,
                    "end": 1119,
                    "matchedPaperCorpusId": "271097745"
                },
                {
                    "start": 1763,
                    "end": 1767,
                    "matchedPaperCorpusId": "259129398"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.830078125
        },
        {
            "corpus_id": "270286833",
            "title": "When Human-AI Interactions Become Parasocial: Agency and Anthropomorphism in Affective Design",
            "text": "Training datasets and language models are not immune to biases [74], and their deployment can reinforce social stereotypes [3]. \n\nTraining datasets for LLMs are extracted from human-made texts in online sources, and so pre-trained LLMs can replicate both human conversational styles and human prejudices. In the case of PaLM (the LLM that underwrites Google's Bard), 50% of the data used to pre-train the model came from social media conversations [13], which could potentially over-represent populations with internet access [6]. These biases can manifest as skewed representations in generative AI outputs, like descriptions of women that limit them to traditional gendered social roles [37] or depictions of non-white people that imply social undesirability [3]. Users' efforts to recreate social contexts for chatbot outputs (through projective inference) might also introduce stereotypes and biases that influence how they appraise and use generated information. For example, people tend to mindlessly apply gender and racial stereotypes to computers to determine whether the agents are persuasive or trustworthy [52]. Abercrombie et al. [1] argued that users do this even when the systems have neutral design features. If users interpret generated outputs differently based on stereotyped projected social roles, this could reinforce the harmful representations mentioned by Ruane et al. [59]. \n\nAs with the concept of affordance, which charts out the actions made available by systems/environments and users, limitations to action (like those derived from gender or racial stereotypes) can also be co-created by conversational AI and users. Stark [64] points out that chatbots' skewed representations can prime users to interpret outputs in biased ways, reinforcing the systematicity of this bias. For example, Miller et al. [45] demonstrated that users may regard white faces as more \"real\" than non-white faces, even when they are both generated by AI systems. These priming risks can also be self-destructive, as we see with AI-generated images that reflect unrealistic (or Eurocentric) beauty standards. In these cases, users may apply these standards to themselves, causing poor self-image or body dysmorphia [69].",
            "score": 0.6002776956201433,
            "section_title": "Priming, Stereotyping, and Representational Harms",
            "char_start_offset": 33482,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 130,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1399
                },
                {
                    "start": 1402,
                    "end": 1647
                },
                {
                    "start": 1648,
                    "end": 1804
                },
                {
                    "start": 1805,
                    "end": 1969
                },
                {
                    "start": 1970,
                    "end": 2114
                },
                {
                    "start": 2115,
                    "end": 2226
                }
            ],
            "ref_mentions": [
                {
                    "start": 63,
                    "end": 67,
                    "matchedPaperCorpusId": "249872629"
                },
                {
                    "start": 123,
                    "end": 126,
                    "matchedPaperCorpusId": "231603388"
                },
                {
                    "start": 448,
                    "end": 452,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 526,
                    "end": 529,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 689,
                    "end": 693,
                    "matchedPaperCorpusId": "261276445"
                },
                {
                    "start": 761,
                    "end": 764,
                    "matchedPaperCorpusId": "231603388"
                },
                {
                    "start": 1143,
                    "end": 1146,
                    "matchedPaperCorpusId": "235352857"
                },
                {
                    "start": 1832,
                    "end": 1836,
                    "matchedPaperCorpusId": "265148334"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57763671875
        },
        {
            "corpus_id": "261530629",
            "title": "Bias and Fairness in Large Language Models: A Survey",
            "text": "The rise and rapid advancement of large language models (LLMs) has fundamentally changed language technologies (e.g., Brown et al., 2020;Conneau et al., 2020;Devlin et al., 2019;Lewis et al., 2020;OpenAI, 2023;Radford et al., 2018;Raffel et al., 2020). With the ability to generate human-like text, as well as adapt to a wide array of natural language processing (NLP) tasks, the impressive capabilities of these models have initiated a paradigm shift in the development of language models. Instead of training task-specific models on relatively small task-specific datasets, researchers and practitioners can use LLMs as foundation models that can be fine-tuned for particular functions (Bommasani et al., 2021). Even without fine-tuning, foundation models increasingly enable few-or zero-shot capabilities for a wide array of scenarios like classification, question-answering, logical reasoning, fact retrieval, information extraction, and more, with the task described in a natural language prompt to the model and few or no labeled examples (e.g., Brown et al., 2020;Kojima et al., 2022;Liu et al., 2023a;Radford et al., 2019;Zhao et al., 2021).\n\nLaying behind these successes, however, is the potential to perpetuate harm. Typically trained on an enormous scale of uncurated Internet-based data, LLMs inherit stereotypes, misrepresentations, derogatory and exclusionary language, and other denigrating behaviors that disproportionately affect already-vulnerable and marginalized communities Dodge et al., 2021;Sheng et al., 2021b). These harms are forms of \"social bias,\" a subjective and normative term we broadly use to refer to disparate treatment or outcomes between social groups that arise from historical and structural power asymmetries, which we define and discuss in Section 2. 1 Though LLMs often reflect existing biases, they can amplify these biases too; in either case, the automated reproduction of injustice can reinforce systems of inequity (Benjamin, 2020). From negative sentiment and toxicity directed towards some social groups, to stereotypical",
            "score": 0.6000672366940887,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 118,
                    "end": 137,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 137,
                    "end": 158,
                    "matchedPaperCorpusId": "207880568"
                },
                {
                    "start": 158,
                    "end": 178,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 178,
                    "end": 197,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 231,
                    "end": 251,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1052,
                    "end": 1071,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1071,
                    "end": 1091,
                    "matchedPaperCorpusId": "249017743"
                },
                {
                    "start": 1091,
                    "end": 1109,
                    "matchedPaperCorpusId": "236493269"
                },
                {
                    "start": 1109,
                    "end": 1130,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1130,
                    "end": 1148,
                    "matchedPaperCorpusId": "231979430"
                },
                {
                    "start": 1496,
                    "end": 1515,
                    "matchedPaperCorpusId": "237568724"
                },
                {
                    "start": 1515,
                    "end": 1535,
                    "matchedPaperCorpusId": "234337004"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.787109375
        },
        {
            "corpus_id": "256389777",
            "title": "Comparing Intrinsic Gender Bias Evaluation Measures without using Human Annotated Examples",
            "text": "In this paper, we limited our investigation to English PLMs. However, as reported in a lot of previous work, social biases are language independent and omnipresent in PLMs trained for many languages (Kaneko et al., 2022c;Lewis and Lupyan, 2020;Liang et al., 2020;Zhao et al., 2020). We plan to extend this study to cover non-English PLMs in the future. According to existing research, PLMs encode many different types of social biases such as racial and religious biases in addition to gender-related biases (Kiritchenko and Mohammad, 2018;Ravfogel et al., 2020). On the other hand, in this paper, we focused on only gender bias. Extending the proposed method to handle other types of social biases beyond gender bias is beyond the scope of the current short paper and is deferred to future work.\n\nFurthermore, discriminatory bias is learned in word embeddings as well as PLMs (Bolukbasi et al., 2016;Brunet et al., 2019;Kaneko and Bollegala, 2019, 2021bKaneko et al., 2022b). Therefore, it may be possible to make it applicable to word embeddings as well.",
            "score": 0.6000175989779556,
            "section_title": "Limitations",
            "char_start_offset": 12317,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 221,
                    "end": 244,
                    "matchedPaperCorpusId": "220948164"
                },
                {
                    "start": 244,
                    "end": 263,
                    "matchedPaperCorpusId": "227230609"
                },
                {
                    "start": 263,
                    "end": 281,
                    "matchedPaperCorpusId": "218487087"
                },
                {
                    "start": 540,
                    "end": 562,
                    "matchedPaperCorpusId": "215786522"
                },
                {
                    "start": 877,
                    "end": 901,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 901,
                    "end": 921,
                    "matchedPaperCorpusId": "52946942"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54931640625
        },
        {
            "corpus_id": "265212726",
            "title": "Social Bias Probing: Fairness Benchmarking for Language Models",
            "text": "In the following, we analyze the results obtained on the SOFA dataset through the SOFA score broken down by category,13 detailed in Tab. 2. In Fig. 8 in the Appendix, we report the score distribution across categories and LMs. We recall that a higher score indicates greater variance in the model's responses to probes within a specific category, signifying high sensitivity to the input identity. For the two scales of BLOOM, we notice scores that are far apart when comparing the pairs of results obtained by category: this behavior is recorded by the previous overall ranking, which places these two models at opposite poles of the scale. \n\nAcross all models except for BLOOM-3b, religion consistently stands out as the category with the most pronounced disparity, while nationality often shows the lowest value. Given the extensive focus on gender and racial biases in the NLP literature, it is plausible that recent language models have undergone some degree of fairness mitigation for these particular biases, which may explain why religion now emerges more prominently. Our results highlight the need to uncover such biases and encourage the community to actively work towards mitigating them. \n\nIntra-identities evaluation In Fig. 2, we report a more qualitative result, i.e., the identities that, in combination with the stereotypes, obtain the lowest PPL \u22c6 score. Intuitively, the probes that each model is more likely to generate for the set of stereotypes afferent to that category. Our findings indicate that certain identities, particularly Mus- lims and Jews from the religion category and nonbinary and trans persons within gender face disproportionate levels of stereotypical associations in various tested models. In accordance with the intracategories evaluation, religion indeed emerges as the category most prone to variance. In contrast, concerning the nationality and disability categories, no significant overlap between the different models emerges. A potential contributing factor might be the varying sizes of the identity sets derived from the lexicon used for constructing the probes, as detailed in Tab. 5 in the Appendix. \n\nIntra-stereotypes evaluation We display, in Fig. 3, the top stereotype reaching the lowest DDS, reporting the most prevalent stereotypes across identities within each category. In the religion category, the most frequently occurring stereotype relates to immoral acts and beliefs or judgments of repulsion.",
            "score": 0.6000175989779556,
            "section_title": "Intra-categories evaluation",
            "char_start_offset": 20751,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 641
                },
                {
                    "start": 644,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1200
                },
                {
                    "start": 1203,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1731
                },
                {
                    "start": 1732,
                    "end": 1846
                },
                {
                    "start": 1847,
                    "end": 1974
                },
                {
                    "start": 1975,
                    "end": 2152
                },
                {
                    "start": 2155,
                    "end": 2331
                },
                {
                    "start": 2332,
                    "end": 2461
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.490478515625
        },
        {
            "corpus_id": "219530686",
            "title": "Detecting Emergent Intersectional Biases: Contextualized Word Embeddings Contain a Distribution of Human-like Biases",
            "text": "We introduce methods called IBD and EIBD to identify biases associated with members of multiple minority groups. These methods automatically detect the intersectional biases and emergent intersectional biases captured by word embeddings. Intersectional biases associated with African American and Mexican American females have the highest effect size compared to other social biases. Complementary to predefined sets of attributes to measure widely known biases, our methods automatically discover biases. IBD reaches an accuracy of 81.6% and 82.7% in detection, respectively, when validating on the intersectional biases of African American females and Mexican American females. EIBD reaches an accuracy of 84.7% and 65.3% in detection, respectively, when validating on the emergent intersectional biases of African American females and Mexican American females. \n\nWe present CEAT to measure biases identified by IBD and EIBD in language models. CEAT uses a random-effects model to comprehensively measure social biases embedded in neural language models that contain a distribution of contextdependent biases. CEAT simulates this distribution by sampling (N = 10, 000) combinations of CWEs without replacement from a large-scale natural language corpus. Unlike prior work that focuses on a limited number of contexts defined by templates to measure the magnitude of particular biases, CEAT provides a comprehensive measurement of overall bias in contextualizing language models. Our results indicate that ELMo is the most biased, followed by BERT, and GPT. GPT-2 is the least biased language model with respect to the social biases we investigate. The overall magnitude of bias negatively correlates with the level of contextualization in the language model. Understanding how the architecture of a language model contributes to biased and contextualized word representations can help mitigate the harmful effects to society in downstream applications.",
            "score": 0.599789025208729,
            "section_title": "Conclusion",
            "char_start_offset": 53677,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 863
                },
                {
                    "start": 866,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1480
                },
                {
                    "start": 1481,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1760
                },
                {
                    "start": 1761,
                    "end": 1954
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.876953125
        },
        {
            "corpus_id": "271859735",
            "title": "Social Debiasing for Fair Multi-modal LLMs",
            "text": "Multi-modal Large Language Models (MLLMs) have advanced significantly, offering powerful vision-language understanding capabilities. However, these models often inherit severe social biases from their training datasets, leading to unfair predictions based on attributes like race and gender. This paper addresses the issue of social biases in MLLMs by i) Introducing a comprehensive Counterfactual dataset with Multiple Social Concepts (CMSC), which provides a more diverse and extensive training set compared to existing datasets. ii) Proposing an Anti-Stereotype Debiasing strategy (ASD). Our method works by revisiting the MLLM training process, rescaling the autoregressive loss function, and improving data sampling methods to counteract biases. Through extensive experiments on various MLLMs, our CMSC dataset and ASD method demonstrate a significant reduction in social biases while maintaining the models' original performance.",
            "score": 0.5994094810363939,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.884765625
        },
        {
            "corpus_id": "195316733",
            "title": "Mitigating Gender Bias in Natural Language Processing: Literature Review",
            "text": "Gender bias is the preference or prejudice toward one gender over the other (Moss-Racusin et al., 2012). Gender bias is exhibited in multiple parts of a Natural Language Processing (NLP) system, including the training data, resources, pretrained models (e.g. word embeddings), and algorithms themselves (Zhao et al., 2018a;Bolukbasi et al., 2016;Caliskan et al., 2017;Garg et al., 2018). NLP systems containing bias in any of these parts can produce gender biased predictions and sometimes even amplify biases present in the training sets (Zhao et al., 2017). \n\nThe propagation of gender bias in NLP algorithms poses the danger of reinforcing damaging stereotypes in downstream applications. This has real-world consequences; for example, concerns have been raised about automatic resume filtering systems giving preference to male applicants when the only distinguishing factor is the applicants' gender. \n\nOne way to categorize bias is in terms of allocation and representation bias (Crawford, 2017). Allocation bias can be framed as an economic issue in which a system unfairly allocates resources to certain groups over others, while representation bias occurs when systems detract from the social identity and representation of certain groups (Crawford, 2017). In terms of NLP applications, allocation bias is reflected when models often perform better on data associated with majority gender, and representation bias is reflected when associations between gender with certain concepts are captured in word embedding and model parameters. In Table 1, we categorize common examples of gender bias in NLP following Crawford (2017). Translating \"He is a nurse. She is a doctor.\" to Hungarian and back to English results in \"She is a nurse. He is a doctor.\" (Douglas, 2017) Caption Generation An image captioning model incorrectly predicts the agent to be male because there is a computer nearby (Burns et al., 2018). Speech Recognition Automatic speech detection works better with male voices than female voices (Tatman, 2017). Sentiment Analysis Sentiment Analysis Systems rank sentences containing female noun phrases to be indicative of anger more often than sentences containing male noun phrases (Park et al., 2018).",
            "score": 0.5992488249720795,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 559
                },
                {
                    "start": 562,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 905
                },
                {
                    "start": 908,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1543
                },
                {
                    "start": 1544,
                    "end": 1634
                },
                {
                    "start": 1635,
                    "end": 1662
                },
                {
                    "start": 1663,
                    "end": 1741
                },
                {
                    "start": 1742,
                    "end": 1758
                },
                {
                    "start": 1759,
                    "end": 1918
                },
                {
                    "start": 1919,
                    "end": 2029
                },
                {
                    "start": 2030,
                    "end": 2223
                }
            ],
            "ref_mentions": [
                {
                    "start": 76,
                    "end": 103,
                    "matchedPaperCorpusId": "4697978"
                },
                {
                    "start": 303,
                    "end": 323,
                    "matchedPaperCorpusId": "4952494"
                },
                {
                    "start": 323,
                    "end": 346,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 346,
                    "end": 368,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 368,
                    "end": 386,
                    "matchedPaperCorpusId": "4930886"
                },
                {
                    "start": 539,
                    "end": 558,
                    "matchedPaperCorpusId": "1389483"
                },
                {
                    "start": 2014,
                    "end": 2028,
                    "matchedPaperCorpusId": "13997424"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55029296875
        },
        {
            "corpus_id": "258174286",
            "title": "Improving Gender-Related Fairness in Sentence Encoders: A Semantics-Based Approach",
            "text": "Therefore, it is of the utmost importance to expand research on how sentence embedding encoders internalise the semantics of natural languages. An important step towards this direction is to define metrics that are able to reflect and quantify social bias in sentence encoders. Furthermore, studying and limiting the causes and consequences of bias in language models is an extremely important task [4,6]. \n\nThis work expands research on social bias in embeddingbased models, focusing specifically on gender bias in sentence representations. First, we propose a method to estimate gender bias in sentence embeddings, highlighting the correlation between bias and stereotypical concepts in the sentence. Our solution, named bias score, is highly flexible and designed to be easily adapted to both different kinds of social biases (e.g. ethnic, religious) and various sentence encoders. Moreover, since gender bias is determined by the internalisation of stereotypical associations in language models, bias score allows to identify stereotyped sentences that are responsible for increasing gender bias in the output embeddings encoded by the model. Therefore, in the second part of the paper, we leverage bias score to retrieve the more stereotyped sentences from the Stanford Natural Language Inference corpus (SNLI) [9], a large text corpus suitable for training general-purpose sentence encoders, such as those proposed by [17] and [13]. We then outline two approaches to make SNLI fairer: removing entries associated to the highest bias score, and performing data augmentation by compensating stereotyped sentences with their gender-swapped counterparts. Finally, we retrain a BiLSTM sentence encoder [17] on different fairer versions of SNLI, testing and comparing it with its original counterpart from both fairness and accuracy viewpoint in downstream tasks.",
            "score": 0.5988613653183589,
            "section_title": "Introduction",
            "char_start_offset": 2186,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 405
                },
                {
                    "start": 408,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1438
                },
                {
                    "start": 1439,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1863
                }
            ],
            "ref_mentions": [
                {
                    "start": 399,
                    "end": 402,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 402,
                    "end": 404,
                    "matchedPaperCorpusId": "218971825"
                },
                {
                    "start": 1316,
                    "end": 1319,
                    "matchedPaperCorpusId": "14604520"
                },
                {
                    "start": 1424,
                    "end": 1428,
                    "matchedPaperCorpusId": "28971531"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82568359375
        },
        {
            "corpus_id": "262043949",
            "title": "The Impact of Debiasing on the Performance of Language Models in Downstream Tasks is Underestimated",
            "text": "Unfortunately, Pre-trained Language Models (PLMs) such as BERT (Devlin et al., 2019) and * Danushka Bollegala holds concurrent appointments as a Professor at University of Liverpool and as an Amazon Scholar. This paper describes work performed at the University of Liverpool and is not associated with Amazon. Table 1: The total number of instances containing female, male, and occupational (Occ.) words in the GLUE development data. \n\nRoBERTa (Liu et al., 2019) easily learn discriminatory social biases expressed in human-written texts in massive datasets (Kurita et al., 2019;Zhou et al., 2022;Kaneko et al., 2022). For example, if a model is given \"[MASK] is a nurse.\" as the input, a gender biased PLM would predict \"She\" with a higer likelihood score than for \"He\" when filling the [MASK]. \n\nVarious debiasing methods have been proposed to mitigate social biases in PLMs. Zhao et al. (2019); Webster et al. (2020) proposed a debiasing method by swapping the gender of female and male words in the training data. Kaneko and Bollegala (2021) proposed a method for debiasing by orthogonalising the vectors representing gender information with the hidden layer of a language model given a sentence containing a stereotypical word. Webster et al. (2020) showed that dropout regularization can reduce overfitting to gender information, thereby can be used for debiasing PLMs. \n\nThe debiasing method should mitigate only discriminatory information, while pre-trained useful information should be retained in the model. Evaluations in downstream tasks often employ the GLEU benchmark (Wang et al., 2018), which measures the ability to understand language (Kaneko and Bollegala, 2021;Guo et al., 2022;Meade et al., 2022). The data for downstream tasks are not selected in terms of whether they reflect the impact of debiasing.",
            "score": 0.5986337480092299,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 433
                },
                {
                    "start": 436,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 795
                },
                {
                    "start": 798,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1375
                },
                {
                    "start": 1378,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1718
                },
                {
                    "start": 1719,
                    "end": 1823
                }
            ],
            "ref_mentions": [
                {
                    "start": 63,
                    "end": 84,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 558,
                    "end": 579,
                    "matchedPaperCorpusId": "190000105"
                },
                {
                    "start": 579,
                    "end": 597,
                    "matchedPaperCorpusId": "247450590"
                },
                {
                    "start": 878,
                    "end": 896,
                    "matchedPaperCorpusId": "102352962"
                },
                {
                    "start": 1018,
                    "end": 1045,
                    "matchedPaperCorpusId": "231698657"
                },
                {
                    "start": 1582,
                    "end": 1601,
                    "matchedPaperCorpusId": "5034059"
                },
                {
                    "start": 1653,
                    "end": 1681,
                    "matchedPaperCorpusId": "231698657"
                },
                {
                    "start": 1681,
                    "end": 1698,
                    "matchedPaperCorpusId": "248780440"
                },
                {
                    "start": 1698,
                    "end": 1717,
                    "matchedPaperCorpusId": "239015827"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.486083984375
        },
        {
            "corpus_id": "259274987",
            "title": "CBBQ: A Chinese Bias Benchmark Dataset Curated with Human-AI Collaboration for Large Language Models",
            "text": "Figure 5 illustrates the bias scores across 14 social dimensions within two contextual settings. First, it is discernible that biases are manifest in the pre-trained models, with some categories registering higher bias scores even after fine-tuning. Second, it is observed that the evaluated LLMs have higher bias scores when contexts are ambiguous, suggesting that they may rely on harmful societal biases to answer these questions. However, we can also observe that the bias scores of the evaluated models remain at a high baseline when contexts are disambiguous, and even in some cases, the scores exceed thoses when contexts are ambiguous. This indicates that some of the current Chinese large language models may have the problem of insufficient comprehension of the given text in addition to the problem of bias. In general, across both ambiguous and disambiguous contexts, the evaluated LLMs exhibit varied levels of biases across different categories. Bias scores related to educational Ambig. \n\nDisambig.",
            "score": 0.5981667123267073,
            "section_title": "Bias across Categories",
            "char_start_offset": 30050,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 97,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1001
                },
                {
                    "start": 1004,
                    "end": 1013
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52685546875
        },
        {
            "corpus_id": "270562652",
            "title": "Interpreting Bias in Large Language Models: A Feature-Based Approach",
            "text": "Motivation.Large Language Models (LLMs) excel in various natural language processing (NLP) tasks such as text generation, question answering, and translation.Models like Mistral (Jiang et al., 2023) and LLaMA (Touvron et al., 2023), trained on diverse datasets including webpages, Wikipedia, books, and scientific papers, provide a comprehensive representation of global knowledge.However, this diversity also exposes these large models to inherent social biases.\n\nAddressing biases in LLMs has become a priority for the NLP community.Early work has re-vealed gender and ethnic biases in word embeddings like Word2Vec and GloVe (Bolukbasi et al., 2016), and subsequent research extended this to more complex models like BERT (Kurita et al., 2019;May et al., 2019).Vig et al. (2020a) introduced the use of causal mediation analysis to identify bias-causing neurons or attention heads, demonstrating that in decoder-only models such as GPT-2, gender bias effects are sparse.More recently, Chintam et al. (2023) expanded this approach to include fine-grained causal mediation called ACDC (Conmy et al., 2023) and a combinatorial approach, DiffMask (De Cao et al., 2021), for bias analysis.These studies focus on identifying components most effective in reducing bias, measured using a counterfactual dataset through causal mediation.Nevertheless, these studies do component identification in a generalised fashion.As components that contribute to one form of gender bias may not contribute to another, we propose a case-by-case study.\n\nVarious studies have have explored debiasing techniques for masked language models such as BERT (Meade et al., 2022) and larger models like LLaMA and OPT (Ranaldi et al., 2023), using methods like LoRA training on counterfactual datasets.Layer-wise bias benchmarking of original and debiased models has also been conducted (Prakash and Lee, 2023).Nevertheless, these studies have applied debiasing in a general fashion by training adapter heads.",
            "score": 0.5976060619596597,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 11
                },
                {
                    "start": 11,
                    "end": 158
                },
                {
                    "start": 158,
                    "end": 381
                },
                {
                    "start": 381,
                    "end": 463
                },
                {
                    "start": 465,
                    "end": 535
                },
                {
                    "start": 535,
                    "end": 764
                },
                {
                    "start": 764,
                    "end": 972
                },
                {
                    "start": 972,
                    "end": 1186
                },
                {
                    "start": 1186,
                    "end": 1330
                },
                {
                    "start": 1330,
                    "end": 1411
                },
                {
                    "start": 1411,
                    "end": 1531
                },
                {
                    "start": 1533,
                    "end": 1771
                },
                {
                    "start": 1771,
                    "end": 1880
                },
                {
                    "start": 1880,
                    "end": 1978
                }
            ],
            "ref_mentions": [
                {
                    "start": 628,
                    "end": 652,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 725,
                    "end": 746,
                    "matchedPaperCorpusId": "190000105"
                },
                {
                    "start": 746,
                    "end": 763,
                    "matchedPaperCorpusId": "85518027"
                },
                {
                    "start": 764,
                    "end": 782,
                    "matchedPaperCorpusId": "227275068"
                },
                {
                    "start": 1085,
                    "end": 1105,
                    "matchedPaperCorpusId": "258418244"
                },
                {
                    "start": 1629,
                    "end": 1649,
                    "matchedPaperCorpusId": "239015827"
                },
                {
                    "start": 1687,
                    "end": 1709,
                    "matchedPaperCorpusId": "202537041"
                },
                {
                    "start": 1856,
                    "end": 1879,
                    "matchedPaperCorpusId": "266054040"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65869140625
        },
        {
            "corpus_id": "219530686",
            "title": "Detecting Emergent Intersectional Biases: Contextualized Word Embeddings Contain a Distribution of Human-like Biases",
            "text": "State-of-the-art off-the-shelf neural language models such as the multi-million dollar GPT-3, associates men with competency and occupations demonstrating higher levels of education, in downstream natural language processing (NLP) tasks such as sequence prediction (Brown et al. 2020). When GPT-3's user interface for academic access is prompted for language generation with the input \"What is the gender of a doctor,\" the first answer is \"A: Doctor is a masculine noun;\" whereas when prompted with \"What is the gender of a nurse,\" the first answer is \"It's female.\" Propagation of social group bias in NLP applications such as automated resume screening, that shapes the workforce by making consequential decisions about job candidates, would not only perpetuate existing biases but potentially exacerbate harmful bias in society to affect future generations (De-Arteaga et al. 2019;Raghavan and Barocas). To enhance transparency in NLP, we use the representations of words learned from word co-occurrence statistics to discover social biases. Our methods uncover unique intersectional biases associated with individuals that are members of multiple minority groups. After identifying these emergent biases, we use numeric representations of words that vary according to neighboring words to analyze how prominent bias is in different contexts. Recent work has shown that human-like biases are embedded in the statistical regularities of language that are learned by word representations, namely word embeddings (Caliskan, Bryson, and Narayanan 2017;Blodgett et al. 2020). We build a method on this work to automatically identify intersectional biases, such as the ones associated with African American and Mexican American women from static word embeddings (SWE). Then, we measure how human-like biases manifest themselves in contextualized word embeddings (CWE), which are dynamic word representations generated by neural language models that adapt to their context. \n\nArtificial intelligence systems are known not only to perpetuate social biases, but they may also amplify existing cultural assumptions and inequalities (Campolo et al. 2017).",
            "score": 0.5962905655921199,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1573
                },
                {
                    "start": 1574,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 1969
                },
                {
                    "start": 1972,
                    "end": 2147
                }
            ],
            "ref_mentions": [
                {
                    "start": 860,
                    "end": 884,
                    "matchedPaperCorpusId": "58006082"
                },
                {
                    "start": 1513,
                    "end": 1551,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 1551,
                    "end": 1572,
                    "matchedPaperCorpusId": "218971825"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6982421875
        },
        {
            "corpus_id": "248006238",
            "title": "Mapping the Multilingual Margins: Intersectional Biases of Sentiment Analysis Systems in English, Spanish, and Arabic",
            "text": "In this section, we discuss the unisectional and intersectional social biases that we do and do not detect, across our five models that we trained on emotion regression tasks and evaluated using the EECs and novel statistical framework. The most pervasive statistically significant social bias observed is gender bias, followed by racial and ethnic bias, and finally by intersectional social bias.\n\nBecause of our statistical procedure, it is possible that some of the bias experienced by the intersectional identity is absorbed by either the gender and racial or ethnic coefficient, limiting the extent to which intersectional social bias may be measured.\n\nWe are primarily interested in our statistical analysis of intersectional social biases. A canonical example of intersectional social bias is the angry Black woman stereotype (Collins, 2004). We find the opposite: sentences referring to Black women are inferred as less angry across all three transformer-based language models and inferred as more joyful in BERT+ to a statistically significant degree (Table 3). It is possible that this bias is captured by other coefficients. For example, sentences referring to women are inferred as more angry in mBERT and XLM-RoBERTa and sentences referring to Black people are inferred as more angry in mBERT. It also is possible that the language models do not exhibit this stereotype, which supports experimental results in psychology (Walley-Jean, 2009) despite being well-established in the critical theory literature (Collins, 2004).\n\nWe note that sentences referring to Latinas display more joy across transformer-based language models in both English and Spanish (Table 4); however, other intersectional identities do not see a uniform statistically significant increase or decrease across models for a given emotion.\n\nWe find evidence of racial biases in our experiments. We find statistically significant evidence to suggest that transformer-based language models predict that sentences referring to Black people are less fearful, sad, and joyful than sentences referring to white people (Table 3). This demonstrates that these language models may predict lower emotional intensity for sentences referring to Black people in any case, placing more emphasis on white sentiment and the white experience.\n\nWe observe that ethnic biases are sometimes split by language. For example, English models predict sentences referring to Arabs as more fearful while Arabic models predict the same sentences as less fearful (Table 5)",
            "score": 0.5956341690762237,
            "section_title": "Discussion",
            "char_start_offset": 19987,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66455078125
        },
        {
            "corpus_id": "259274987",
            "title": "CBBQ: A Chinese Bias Benchmark Dataset Curated with Human-AI Collaboration for Large Language Models",
            "text": "The presence of bias in the hidden representations or embeddings of a model does not necessarily indicate that the model will produce biased outputs [21]. To understand when a model's outputs may exhibit and reinforce biases, we need to examine how these biases manifest in downstream tasks. \n\nTable 1: The comparison of CBBQ with other bias evaluation datasets. \n\nIn sentence completion, [7] introduces the Bias in Open-Ended Language Generation Dataset (BOLD), a large-scale dataset that consists of 23,679 English text generation prompts for bias benchmarking across five domains. Their findings reveal that most large language models display higher societal biases in all domains compared to human-written Wikipedia texts. \n\nIn machine translation, existing works on biases in machine translation have almost exclusively focused on issues of gender biases. [29] presents the first challenge set and evaluation protocol for the analysis of gender bias in machine translation for 8 languages, referred to as WinoMT Chanllege Set. [23] releases an evaluation scheme and dataset for measuring the ability of transformer-based NMT models to translate gender morphology correctly in unambiguous contexts. This dataset is translated from English to 20 languages. \n\nIn dialogue generation, [18] conducts pioneering research on fairness issues in dialogue systems. They construct a benchmark dataset and propose quantitative measures to understand fairness (We refer to this dataset as TFDS in Table 1.), the results show that popular dialogue models exhibit significant biases towards different genders and races. Nowadays, dialogue systems often adopt personas mimicking societal demographics. Therefore, in the work of [26], they present the first large-scale study on persona biases in dialogue systems. [27] emphasizes the importance to study ad hominems in dialogue responses. They compose an annotated dataset, and build a classifier to analyze human and dialogue system responses to English Twitter posts. \n\nIn question answering, one of challenges in directly assessing bias in model outputs is the difficulty in finding a fair and automatic evaluation method. Most of the existing methods rely on human judgement. UnQover [17] addresses this issue by transforming the generation task into a multiplechoice task. They utilize unspecified questions, which couldn't be answered based on the given context.",
            "score": 0.5951834831208649,
            "section_title": "Related Work",
            "char_start_offset": 5712,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 291
                },
                {
                    "start": 294,
                    "end": 362
                },
                {
                    "start": 365,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 726
                },
                {
                    "start": 729,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 1031
                },
                {
                    "start": 1032,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1259
                },
                {
                    "start": 1262,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1609
                },
                {
                    "start": 1610,
                    "end": 1690
                },
                {
                    "start": 1691,
                    "end": 1802
                },
                {
                    "start": 1803,
                    "end": 1877
                },
                {
                    "start": 1878,
                    "end": 2008
                },
                {
                    "start": 2011,
                    "end": 2164
                },
                {
                    "start": 2165,
                    "end": 2218
                },
                {
                    "start": 2219,
                    "end": 2316
                },
                {
                    "start": 2317,
                    "end": 2407
                }
            ],
            "ref_mentions": [
                {
                    "start": 149,
                    "end": 153,
                    "matchedPaperCorpusId": "239010011"
                },
                {
                    "start": 389,
                    "end": 392,
                    "matchedPaperCorpusId": "231719337"
                },
                {
                    "start": 1803,
                    "end": 1807,
                    "matchedPaperCorpusId": "235097342"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54931640625
        },
        {
            "corpus_id": "237753859",
            "title": "Slovene and Croatian word embeddings in terms of gender occupational analogies",
            "text": "Language corpora and datasets reflect linguistic variations (including different types of bias) in relation to social factors. NLP tools are trained on these data and can inherit the contained variations and biases. The bias in corpora can negatively impact NLP tools (Sun et al., 2019) and can perpetuate biases held towards certain groups. Word embeddings are trained on large corpora to capture syntactic and semantic relations between words and capture the expressed biases. \n\nFor instance, it has been shown that standard training data sets for part-ofspeech perform better on older people's language (Hovy and S\u00f8gaard, 2015). Garimella et al. (2019) show that a part-of-speech tagger and a dependency parser perform successfully on texts written by women, regardless of what data they had been trained on initially. On the other hand, male authors' texts are better tagged/parsed when the training data contained enough texts written by men. The success of tools such as parsers on male authors' texts may be due to the imbalances in the training data favouring male authorship. It has also been shown that NLP tools are more effective when demographic variations are considered (Volkova et al., 2013;Hovy, 2015). Hovy (2015) shows that including the information on the age and gender of authors improves the performance of three tasks in five different languages. \n\nBiases can have negative consequences in the coreference resolution task  Hutchinson et al. (2020) note that topics such as gun violence, homelessness, and addiction are over-represented, leading to disability topics receiving particularly negative scores in sentiment analysis tasks. Besides the aspects above, some authors call the attention to the effect biases can have on detection tools. For example, misogyny detection models may attribute high scores to non-misogynous texts simply because the latter contain the so-called identity terms, i.e. terms associated with misogyny (Nozza et al., 2019). In sum, the interplay of bias and NLP is an important and interesting field receiving increasing attention, notably regarding word embeddings, as explained next.",
            "score": 0.5950424613049613,
            "section_title": "R E L A T E D W O R K",
            "char_start_offset": 7001,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 478
                },
                {
                    "start": 481,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1370
                },
                {
                    "start": 1373,
                    "end": 1657
                },
                {
                    "start": 1658,
                    "end": 1766
                },
                {
                    "start": 1767,
                    "end": 1977
                },
                {
                    "start": 1978,
                    "end": 2139
                }
            ],
            "ref_mentions": [
                {
                    "start": 268,
                    "end": 286,
                    "matchedPaperCorpusId": "195316733"
                },
                {
                    "start": 606,
                    "end": 630,
                    "matchedPaperCorpusId": "7396193"
                },
                {
                    "start": 632,
                    "end": 655,
                    "matchedPaperCorpusId": "196181460"
                },
                {
                    "start": 1185,
                    "end": 1207,
                    "matchedPaperCorpusId": "2166300"
                },
                {
                    "start": 1207,
                    "end": 1218,
                    "matchedPaperCorpusId": "14021168"
                },
                {
                    "start": 1220,
                    "end": 1231,
                    "matchedPaperCorpusId": "14021168"
                },
                {
                    "start": 1447,
                    "end": 1471,
                    "matchedPaperCorpusId": "218487466"
                },
                {
                    "start": 1956,
                    "end": 1976,
                    "matchedPaperCorpusId": "204754417"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.489501953125
        },
        {
            "corpus_id": "271571036",
            "title": "Understanding the Interplay of Scale, Data, and Bias in Language Models: A Case Study with BERT",
            "text": "Our study provides a detailed case study on the interplay of scale, pre-training data, and bias with a specific focus on BERT, a widely used LLM. We find evidence that larger models are able to encode more biases upstream. Importantly, we observe that larger models, combined with unmoderated data, can lead to worse results for the task of masked language modeling. However, larger models can also produce more robust downstream classifiers after finetuning. \n\nWhile MLMs like BERT do not represent the state-of-theart in the rapidly developing landscape of LLM research, they remain extremely relevant for several applied natural language processing problems. Our investigation of bias is particularly relevant to practitioners who fine-tune embedding models for their tasks. In these applied use-cases, our results shed light on how scale and training data together can lead to different kinds of biases. We encourage practitioners to be aware of the biases their training datasets can introduce, and to actively measure these artifacts during the development process. On a more general level, our study highlights the role that training data can play in scaling, especially as it relates to biased model behavior. Our results also suggest that mixing in a moderated, high quality data source (e.g., Wikipedia) with larger datasets (e.g., CC-100, The Pile (Gao et al. 2020)) might be an approach to alleviate biases-we leave a full exploration of this direction to future work. \n\nOur analyses also underscore the limitations that exist in metrics used to measure bias, which is a nuanced socio-technical concept, whose meaning changes across tasks and domains. Negative sentiment and gaps in gender representation-as used here-are well-scoped ways of expressing bias that can be useful for different domains. Negative sentiment, for instance, could be a useful measure of bias for LLM use in chatbots or auto-complete tools; differences in gender likelihood could be useful for measuring bias in resum\u00e9 or search ranking, but they are not universal measures of linguistic bias. As seen in our results, depending on the choice of bias metric, a measurement of model behavior can look quite different.",
            "score": 0.5946770680427063,
            "section_title": "Concluding Discussion",
            "char_start_offset": 32889,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 459
                },
                {
                    "start": 462,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1480
                },
                {
                    "start": 1483,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1811
                },
                {
                    "start": 1812,
                    "end": 2080
                },
                {
                    "start": 2081,
                    "end": 2202
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7470703125
        },
        {
            "corpus_id": "246275593",
            "title": "Text and Code Embeddings by Contrastive Pre-Training",
            "text": "Prior research has shown that text representation models encode the biases present in their training data, including those which are discriminatory towards protected groups such as Black people or women (Bolukbasi et al., 2016;Caliskan et al., 2017;May et al., 2019;Zhao et al., 2018;Rudinger et al., 2018). Biases encoded in embedding models may cause representational harms3 by reinforcing existent societal biases in the text corpus, and further propagating them in downstream tasks of embedding models. \n\nTherefore, we encourage further research on two research agendas: (a) developing robust evaluation methodologies for multiple classes of bias in training data and pre-trained models, and (b) developing and improving methods for mitigating encoded bias, including fine-tuning to reduce bias in pre-trained models (Caliskan et al., 2017;May et al., 2019;Bolukbasi et al., 2016;Liang et al., 2020;Park et al., 2018;Solaiman & Dennison, 2021). Until we have robust evaluation methodology, it is important to restrict and monitor the use of the model in downstream applications. Par-ticularly for those where risk of representational harm is great and those where biased representations may influence the allocation of resources and opportunities to people. \n\nOur embedding models are trained with large batch sizes and require substantial computation resources. While this training regime is environmentally and computationally costly, there are promising paths forward to amortize and offset these costs while allowing users to benefits from the capabilities of these models. For example, safe public access to large pre-trained language models, and efficient training pipelines that leverage improved model architectures and training schemes. We encourage further research and implementation efforts in these areas.",
            "score": 0.5942181902848191,
            "section_title": "Broader Impacts",
            "char_start_offset": 23068,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 506
                },
                {
                    "start": 509,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1261
                },
                {
                    "start": 1264,
                    "end": 1366
                },
                {
                    "start": 1367,
                    "end": 1581
                },
                {
                    "start": 1582,
                    "end": 1749
                },
                {
                    "start": 1750,
                    "end": 1822
                }
            ],
            "ref_mentions": [
                {
                    "start": 227,
                    "end": 249,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 249,
                    "end": 266,
                    "matchedPaperCorpusId": "85518027"
                },
                {
                    "start": 821,
                    "end": 844,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 844,
                    "end": 861,
                    "matchedPaperCorpusId": "85518027"
                },
                {
                    "start": 884,
                    "end": 903,
                    "matchedPaperCorpusId": "207996257"
                },
                {
                    "start": 903,
                    "end": 921,
                    "matchedPaperCorpusId": "52070035"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52099609375
        },
        {
            "corpus_id": "273532822",
            "title": "Local Contrastive Editing of Gender Stereotypes",
            "text": "While our work ultimately targets the development of strategies for reducing bias in language models, it is important to acknowledge the potential dual use of these techniques. The same strategies designed to reduce bias, can also be used to perpetuate and amplify biases in language models. Moreover, our intentional design of language models that initially exhibit high levels of bias raises concerns about their potential deployment in various applications. Despite our intention to mitigate bias, the existence of such models risks normalizing and perpetuating stereotypical gender biases in society. \n\nAdditionally, our study focuses on bias specifically between male and female groups, thus operating on a binary specification of gender bias, only. We recognize that gender is a spectrum, encompassing identities beyond strictly male and female categories. Our rationale for this approach is to evaluate the effectiveness of our strategies using an existing and well-established specification that facilitates measurement of their effects.",
            "score": 0.5938684103142063,
            "section_title": "Ethical Considerations",
            "char_start_offset": 31468,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 604
                },
                {
                    "start": 607,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 1045
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66015625
        },
        {
            "corpus_id": "274192674",
            "title": "Leveraging Large Language Models to Measure Gender Representation Bias in Gendered Language Corpora",
            "text": "Biases in NLP systems There is a growing body of literature on biases in NLP systems, which has been summarized in a number of surveys (Blodgett et al., 2020;Sta\u0144czak and Augenstein, 2021;Bansal, 2022). In particular, Blodgett et al. (2020) provide a critical overview of bias in NLP systems, encompassing an analysis of 146 papers that study bias in various NLP tasks. They find that motivations for studying bias are often vague and inconsistent, lacking normative reasoning. Works on language modeling (Lu et al., 2020;Bordia and Bowman, 2019), coreference resolution (Rudinger et al., 2018;Zhao et al., 2018), machine translation (Vanmassenhove et al., 2018;Stanovsky et al., 2019), and sentiment analysis (Kiritchenko and Mohammad, 2018) highlight bias in NLP but vary widely in their conceptualizations and methodologies. The survey recommends a deeper engagement with the relationships between language and social hierarchies and emphasizes the need for explicit statements of why certain system behaviors are harmful and to whom. \n\nGender imbalances in the training corpora are a significant and well-documented source of bias in machine learning. Numerous studies have shown that these imbalances can significantly influence model behavior. For example, Bolukbasi et al. (2016) found that word embeddings trained on biased data can amplify gender stereotypes in tasks like word similarity and analogical reasoning. Similarly, studies by Caliskan et al. (2017) andBrunet et al. (2019) demonstrate that biases present in training corpora can directly influence model outputs, perpetuating gendered stereotypes and imbalances in downstream tasks. Therefore, detecting and addressing gender imbalances in corpora is crucial to mitigating bias effects. \n\nRecent advances in addressing biases in LLMs include various techniques focusing on detecting biases in the embeddings, prompt-based interactions, and contextual adaptations (Gallegos et al., 2024). However, we are not aware of any works aimed at the detection of gender representation bias in the training corpora, which is our focus.",
            "score": 0.5935192074602348,
            "section_title": "Related Work",
            "char_start_offset": 6255,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 477
                },
                {
                    "start": 478,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 1037
                },
                {
                    "start": 1040,
                    "end": 1155
                },
                {
                    "start": 1156,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1423
                },
                {
                    "start": 1424,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1756
                },
                {
                    "start": 1759,
                    "end": 1957
                },
                {
                    "start": 1958,
                    "end": 2094
                }
            ],
            "ref_mentions": [
                {
                    "start": 571,
                    "end": 594,
                    "matchedPaperCorpusId": "13756572"
                },
                {
                    "start": 594,
                    "end": 612,
                    "matchedPaperCorpusId": "4952494"
                },
                {
                    "start": 634,
                    "end": 662,
                    "matchedPaperCorpusId": "76654723"
                },
                {
                    "start": 662,
                    "end": 685,
                    "matchedPaperCorpusId": "173991101"
                },
                {
                    "start": 710,
                    "end": 742,
                    "matchedPaperCorpusId": "21670658"
                },
                {
                    "start": 1933,
                    "end": 1956,
                    "matchedPaperCorpusId": "261530629"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.677734375
        },
        {
            "corpus_id": "271064238",
            "title": "Listen and Speak Fairly: a Study on Semantic Gender Bias in Speech Integrated Large Language Models",
            "text": "Recently, there has been an increasing focus on fairness issues within the speech domain.Earlier research has thoroughly examined the influence of social biases on various speech tasks, such as automatic speech recognition [25][26][27], speech translation [28,29], and emotion recognition [30][31][32].These studies primarily analyze biases in models designed for specific tasks.Additionally, Meng et al. [33] explored the impact of data bias on self-supervised speech models and their downstream tasks, while Lin et al. [34] investigated how model architecture affects bias in self-supervised speech model representations.However, most studies focus on speech models, leaving speech-text models largely unexplored.\n\nThe development of LLMs also raises concerns about potential bias issues.Numerous works have proposed methods to measure biases in LLM generations like coreference resolution [24,35,36].At the same time, several datasets have been proposed for social bias evaluation.Crows-Pairs [18] and StereoSet [37] use crowdsourced sentences to reveal a wide range of social biases in language models, and concurrently BBQ [17] was further proposed and designed as a QA task to measure biases in language models.\n\nFollowing the trend of LLM, Multi-modal LLMs are designed to process and analyze different types of data, such as images, videos, and audio.This has raised concerns about biases in these pre-trained models.In the vision-language domain, some studies have investigated social biases in pre-trained vision-language models, such as image captioning [20] and image classification [21,22].However, research on speech-language models remains limited.To our knowledge, our work is the first to investigate bias and fairness in speechlanguage models.",
            "score": 0.5933766962221132,
            "section_title": "RELATED WORKS",
            "char_start_offset": 4133,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 89,
                    "end": 302
                },
                {
                    "start": 302,
                    "end": 379
                },
                {
                    "start": 379,
                    "end": 623
                },
                {
                    "start": 623,
                    "end": 715
                },
                {
                    "start": 717,
                    "end": 790
                },
                {
                    "start": 790,
                    "end": 903
                },
                {
                    "start": 903,
                    "end": 984
                },
                {
                    "start": 984,
                    "end": 1217
                },
                {
                    "start": 1219,
                    "end": 1359
                },
                {
                    "start": 1359,
                    "end": 1425
                },
                {
                    "start": 1425,
                    "end": 1603
                },
                {
                    "start": 1603,
                    "end": 1663
                },
                {
                    "start": 1663,
                    "end": 1761
                }
            ],
            "ref_mentions": [
                {
                    "start": 223,
                    "end": 227,
                    "matchedPaperCorpusId": "13997424"
                },
                {
                    "start": 231,
                    "end": 235,
                    "matchedPaperCorpusId": "201645086"
                },
                {
                    "start": 256,
                    "end": 260,
                    "matchedPaperCorpusId": "247939213"
                },
                {
                    "start": 260,
                    "end": 263,
                    "matchedPaperCorpusId": "225075735"
                },
                {
                    "start": 297,
                    "end": 301,
                    "matchedPaperCorpusId": "268540044"
                },
                {
                    "start": 405,
                    "end": 409,
                    "matchedPaperCorpusId": "239009728"
                },
                {
                    "start": 892,
                    "end": 896,
                    "matchedPaperCorpusId": "4952494"
                },
                {
                    "start": 896,
                    "end": 899,
                    "matchedPaperCorpusId": "13756572"
                },
                {
                    "start": 899,
                    "end": 902,
                    "matchedPaperCorpusId": "237494677"
                },
                {
                    "start": 996,
                    "end": 1000,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 1015,
                    "end": 1019,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 1128,
                    "end": 1132,
                    "matchedPaperCorpusId": "239010011"
                },
                {
                    "start": 1565,
                    "end": 1569,
                    "matchedPaperCorpusId": "4384334"
                },
                {
                    "start": 1595,
                    "end": 1599,
                    "matchedPaperCorpusId": "257631557"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69091796875
        },
        {
            "corpus_id": "264425921",
            "title": "DiFair: A Benchmark for Disentangled Assessment of Gender Knowledge and Bias",
            "text": "It is widely acknowledged that pre-trained language models may demonstrate biased behavior against underrepresented demographic groups, such as women (Silva et al., 2021) or racial minorities (Field et al., 2021). Given the broad adoption of these models across various use cases, it is imperative for social good to understand these biases and strive to mitigate them while retaining factual gender information that is required to make meaningful gender-based predictions. \n\nIn recent years, numerous studies have attempted to address the biased behaviour of language models, either by manipulating the training data (Webster et al., 2020), altering the training objective (Kaneko and Bollegala, 2021), or by modifying the architecture (Lauscher et al., 2021). Although \u2020 Work done as a Master's student at TeIAS. Figure 1: Ideally, a language model is expected not to favor a gender in a sentence that does not explicitly specify one (top example), while it should prefer genderspecific words when the gender is explicitly specified in the sentence (bottom example). Cf. Section 2.1 for task formulation. \n\ncurrent debiasing techniques, such as counterfactual augmentation (Zhao et al., 2018) and dropout techniques (Webster et al., 2020), are effective in removing biased information from model representations, recent studies have shown that such debiasing can damage a model's useful gender knowledge (Limisiewicz and Mare\u010dek, 2022). This suggests the need for more robust metrics for measuring bias in NLP models that simultaneously consider both performance and fairness. \n\nIn this study, we try to fill this evaluation gap by presenting DIFAIR, a benchmark for evaluating gender bias in language models via a masked lan-guage modeling (MLM) objective, while also considering the preservation of relevant gender data. We test several widely-used pretrained models on this dataset, along with recent debiasing techniques. Our findings echo prior research, indicating that these models struggle to discern when to differentiate between genders. This is emphasized by their performance lag compared to human upper bound on our dataset. We also note that while debiasing techniques enhance gender fairness, they typically compromise the model's ability to retain factual gender information.",
            "score": 0.5933402944175625,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 473
                },
                {
                    "start": 476,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1106
                },
                {
                    "start": 1109,
                    "end": 1438
                },
                {
                    "start": 1439,
                    "end": 1578
                },
                {
                    "start": 1581,
                    "end": 1824
                },
                {
                    "start": 1825,
                    "end": 1927
                },
                {
                    "start": 1928,
                    "end": 2049
                },
                {
                    "start": 2050,
                    "end": 2139
                },
                {
                    "start": 2140,
                    "end": 2293
                }
            ],
            "ref_mentions": [
                {
                    "start": 192,
                    "end": 212,
                    "matchedPaperCorpusId": "235593087"
                },
                {
                    "start": 674,
                    "end": 702,
                    "matchedPaperCorpusId": "231698657"
                },
                {
                    "start": 737,
                    "end": 760,
                    "matchedPaperCorpusId": "237440429"
                },
                {
                    "start": 1175,
                    "end": 1194,
                    "matchedPaperCorpusId": "4952494"
                },
                {
                    "start": 1406,
                    "end": 1437,
                    "matchedPaperCorpusId": "248980783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61767578125
        },
        {
            "corpus_id": "263830974",
            "title": "Evolution of Natural Language Processing Technology: Not Just Language Processing Towards General Purpose AI",
            "text": "Earlier it was mentioned how the biases contained in natural language models often become a problem. Let us discuss this point further. Equality and discrimination often surface as social issues. \n\nIt is known that a natural language model is heavily influenced by its training data, and how it can be corrected so as to avoid these problems is frequently debated. As a precondition, an algorithm must be balanced. However, one must be aware that bias is still possible. For instance, if LGBTK+ ID terminologies are etcluded from the training data, or if their number is very small, they could be underestimated, or completely deleted, from the data. \n\nIf an algorithm is to be neutral, this is in fact a correct operation. However, this would be a major issue for a model. The relationship between training data and the inherent bias in a natural language model is not accurately known. Therefore, it is currently believed that if a scaling law that can be applied to biases is established through small-scale and systematic research, it will also be possible to apply it to large-scale data practices. The fact that every natural language model must use neutral datasets that nevertheless guarantee equality is a future challenge. \n\nGoogle conducts especially strict studies on whether it is adhering to its own AI principles. In their services, language plays a central role, and many mechanisms for avoiding the misuse of language have been introduced, based on the premise that language is one of the best tools humanity has. They are required to be especially vigilant toward prejudice caused through misuse, malicious etpressions and the reproduction of misleading information. For instance, with a conversation application, careful etamination of its results (i.e. conversation outputs), does not etclude possibilities for misuse of the model itself. Such misuse must also be prevented. \n\nFor this reason, Google states that the priority when creating technologies, such as LaMDA, is to sufficiently discuss the actions that can minimize the risks and then to etecute the same. The research and development of these technologies over many years has addressed a number of related problems, such as how inequality bias affects machine learning models. Furthermore, based on these etperiences, \n\nthey provide an open source resource for analyzing models and training data, and they accept open, etternal etaminations.",
            "score": 0.5928077452881095,
            "section_title": "Important Challenges",
            "char_start_offset": 83117,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 195
                },
                {
                    "start": 198,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 650
                },
                {
                    "start": 653,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1232
                },
                {
                    "start": 1235,
                    "end": 1328
                },
                {
                    "start": 1329,
                    "end": 1530
                },
                {
                    "start": 1531,
                    "end": 1684
                },
                {
                    "start": 1685,
                    "end": 1772
                },
                {
                    "start": 1773,
                    "end": 1858
                },
                {
                    "start": 1859,
                    "end": 1894
                },
                {
                    "start": 1897,
                    "end": 2085
                },
                {
                    "start": 2086,
                    "end": 2257
                },
                {
                    "start": 2258,
                    "end": 2298
                },
                {
                    "start": 2301,
                    "end": 2422
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.327880859375
        },
        {
            "corpus_id": "265445454",
            "title": "Towards Auditing Large Language Models: Improving Text-based Stereotype Detection",
            "text": "The field of Artificial Intelligence (AI) continues to evolve with Large Language Models (LLMs) showing both potential and pitfalls. This research explores the ethical dimensions of LLM auditing in Natural Language Processing (NLP), with a focus on text-based stereotype classification and bias benchmarking in LLMs. The advent of state-of-the-art LLMs including OpenAI's GPT series [1-3], Meta's LLaMA series [4,5], and the Falcon series [6] has magnified the societal implications. These LLMs, shown up with abilities like in-context learning as a few-shot learner [1], reveal emergent capabilities with increasing parameter and training token sizes [7]. However, they show fairness concerns due to their training on extensive, unfiltered datasets such as book [8] and Wikipedia corpora [9], and large internet corpora like Common Crawl [10]. This training data often exhibits systemic biases and could further lead to detrimental real-world effects, confirmed by studies [11][12][13][14]. For instance, biases in LLMs and AI systems can reinforce political polarization as seen in Meta's news feed algorithm [15], and exacerbate racial bias in legal systems as documented in predictive policing recidivism algorithms like COMPAS [16]. Furthermore, issues such as gender stereotyping and cultural insensitivity are highlighted by tools like Google Translate and Microsoft's Tay [17,18]. Most existing studies focus on either bias benchmarks in LLMs or text-based stereotypes detection and overlook the interaction between them, which remains underexplored and indicates gaps. Our study makes a clear line between Bias, as observable deviations from neutrality in LLM downstream tasks, and Stereotype, a subset of bias entailing generalized assumptions about certain groups in LLM outputs. Aligning with established stereotype benchmark: StereoSet [19], we detect text-based stereotypes at sentence granularity, across four societal dimensions-Race, Profession, Religion, and Gender-within text generation task conducted with LLMs.",
            "score": 0.5927988858977686,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1577
                },
                {
                    "start": 1578,
                    "end": 1790
                },
                {
                    "start": 1791,
                    "end": 2032
                }
            ],
            "ref_mentions": [
                {
                    "start": 763,
                    "end": 766,
                    "matchedPaperCorpusId": "6866988"
                },
                {
                    "start": 974,
                    "end": 978,
                    "matchedPaperCorpusId": "85518027"
                },
                {
                    "start": 978,
                    "end": 982,
                    "matchedPaperCorpusId": "102352788"
                },
                {
                    "start": 982,
                    "end": 986,
                    "matchedPaperCorpusId": "168170119"
                },
                {
                    "start": 1111,
                    "end": 1115,
                    "matchedPaperCorpusId": "206632821"
                },
                {
                    "start": 1384,
                    "end": 1387,
                    "matchedPaperCorpusId": "149727444"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.771484375
        },
        {
            "corpus_id": "274437724",
            "title": "LLMs as mirrors of societal moral standards: reflection of cultural divergence and agreement across ethical topics",
            "text": "Over the past few years, large language models (LLMs) have become increasingly prominent in current discussions, both in the scientific and public realm (Bender et al., 2021). Due to significant advances in model performance, LLMs now offer promising avenues for applications across a wide range of fields. For instance, large language models are increasingly being used in various applications that impact people's daily lives profoundly, such as search engines, recommendation systems, and automated decision-making systems. However, while the recent performance of LLMs, such as OpenAI's newly released GPT-4, is impressive, there are also areas of concern. An important concern regarding LLMs is whether and in which areas these models may exhibit bias, such as gender, racial and cultural bias. \n\nLarge language models are sensitive to embedded bias due to the way they are trained, as they pick up the societal and cultural biases present in the training data. Due to the fact that LLMs are trained on large amounts of data, the models may be (partially) trained on data that reflects embedded societal and cultural prejudices, such as news articles or social media posts (Karpouzis, 2024;Mishra et al., 2024). Consequently, if a language model is trained on data that consistently portrays certain cultural groups negatively or inaccurately, it may adopt and replicate those biased views. Thus, as applications based on LLM outputs become more prevalent, the potential risk of perpetuating cultural bias present in these models increases as well. Therefore, it is important to assess whether LLMs accurately reflect the empirically observed moral judgments present in different cultures. Despite its importance, this issue remains understudied in the literature (Arora et al., 2022;Liu et al., 2023). In evaluating whether LLM's faithfully capture an understanding of the moral judgments across different cultures in a broad sense, it is crucial to assess how well these models reflect inter-cultural differences and similarities in moral judgments. This can be captured in the following research question: \"To what extent do language models capture cultural diversity and common tendencies regarding topics on which people around the world tend to diverge or agree in their moral judgments?\".",
            "score": 0.5922898377098498,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 799
                },
                {
                    "start": 802,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1216
                },
                {
                    "start": 1217,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1553
                },
                {
                    "start": 1554,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 1807
                },
                {
                    "start": 1808,
                    "end": 2056
                },
                {
                    "start": 2057,
                    "end": 2300
                }
            ],
            "ref_mentions": [
                {
                    "start": 153,
                    "end": 174,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 1178,
                    "end": 1195,
                    "matchedPaperCorpusId": "269134574"
                },
                {
                    "start": 1195,
                    "end": 1215,
                    "matchedPaperCorpusId": "269762608"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8125
        },
        {
            "corpus_id": "256390197",
            "title": "How Far Can It Go? On Intrinsic Gender Bias Mitigation for Text Classification",
            "text": "the discovery that results from different metrics for intrinsic bias in many cases do not correlate (Delobelle et al., 2022), we find this probe effective as an extra step in evaluating the efficacy of these mitigation strategies. We realize from this study that how intrinsic bias has been measured and the choice of bias mitigation strategies explored by some existing works have not been ideal. We further use this probe to investigate if some proposed mitigation strategies superficially conceal bias.\n\nIn this work, when we refer to bias in a language model, we mean stereotyping bias as defined by Garrido-Mu\u00f1oz et al. (2021) as \"the undesired variation of the [association] of certain words in that language model according to certain prior words in a given domain\". We focus our experiments on gender bias for two primary reasons: its intuitive nature making it easy to analyze and discuss, and the accessibility of resources and datasets regarding gender. For the same reason, we narrow our definition of bias in our experiments and analysis to binary gender bias 1 . This paper considers the primary goal of mitigating intrinsic bias to ensure fairness in downstream tasks.\n\nWe consider one case of binary classification and one multiclass classification case, all on English language corpora using the BERT-large (Devlin et al., 2019) and the ALBERT-large (Lan et al., 2019) pretrained models for each task.\n\nIn summary, we develop an extensive probe 2 to uncover intrinsic bias in pertained contextualized language models, and seek to answer three key research questions: RQ1: Do different intrinsic bias metrics respond differently to different bias mitigation techniques? ( \u00a7 4.1). RQ2: Can intrinsic bias mitigation techniques hide bias instead of resolving it? ( \u00a7 5.1). RQ3: Do intrinsic bias mitigation techniques in language models improve fairness in downstream tasks? ( \u00a7 5.2) 2 Measuring and mitigating bias Since many techniques for measuring and mitigating gender bias have been introduced for both intrinsic and extrinsic bias, we only discuss techniques we use in experiments in this paper.\n\nBias mitigation techniques can be applied to pretrained or finetuned language models, or both. Figure 1 illustrates these interactions with both training settings and this section",
            "score": 0.5919987745213331,
            "section_title": "Introduction",
            "char_start_offset": 1941,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5517578125
        },
        {
            "corpus_id": "271874752",
            "title": "Covert Bias: The Severity of Social Views\u2019 Unalignment in Language Models Towards Implicit and Explicit Opinion",
            "text": "Large language models perpetuate biases found in the training data, which constitute the pretraining models' main building blocks (Navigli et al., 2023). Recent studies looked into the severity of bias in the models (Nadeem et al., 2021). Those studies tend to focus on one type of misalignment, namely, the explicit expression of prejudice as a means to indicate a model bias. In that case, explicit sets of group-specific words tend to be used as a primary component to investigate bias by examining asymmetry between two groups (e.g., women and men) and their association with a set of attributes (e.g., home and work). This kind of spurious correlation generally appears in naturalistic data collected for training the models (Li and Michael, 2022;Zhou et al., 2023). Thus, some work has been made to understand the bias caused by these spurious correlations, such as studying the concept level of data to understand biases (Zhou et al., 2023). However, these concept-based framework data may be subject to hidden biases, particularly with regard to ambiguous or arguable labeling judgments and especially in the case of subjective opinions about a concept (R\u00f6ttger et al., 2024). \n\nTherefore, we conducted a focused examination of the impact of a viewpoint-based task to determine the extent of bias severity within implicit and explicit opinions regarding social prejudice issues. Specifically, we sought to answer the following questions: (Q 1 ) Does the discrepancy between implicit and explicit opinion affect the model behavior toward a specific social group? (Q 2 ) What is the magnitude of bias impact on a model's certainty and direct responses to a conflicting view (opposing stance)? The contributions of this study can be summarized as follows: (1) We empirically investigate the severity of bias in LLMs by using the concept of stress testing of implicit and explicit opinion using edge cases of extreme view of bias toward a target group. More specifically, we defined the target groups as women and religion and fine-tuned LLMs on opposing stances using data from two downstream: hate speech and stance detection.",
            "score": 0.591754112222951,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1184
                },
                {
                    "start": 1187,
                    "end": 1386
                },
                {
                    "start": 1387,
                    "end": 1569
                },
                {
                    "start": 1570,
                    "end": 1698
                },
                {
                    "start": 1699,
                    "end": 1956
                },
                {
                    "start": 1957,
                    "end": 2132
                }
            ],
            "ref_mentions": [
                {
                    "start": 130,
                    "end": 152,
                    "matchedPaperCorpusId": "258688053"
                },
                {
                    "start": 216,
                    "end": 237,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 730,
                    "end": 752,
                    "matchedPaperCorpusId": "250390679"
                },
                {
                    "start": 1161,
                    "end": 1183,
                    "matchedPaperCorpusId": "268032377"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53955078125
        },
        {
            "corpus_id": "236477415",
            "title": "Marked Attribute Bias in Natural Language Inference",
            "text": "Reporting and providing test sets for harmful bias in NLP applications is essential for building a robust understanding of the current problem. We present a new observation of gender bias in a downstream NLP application: marked attribute bias in natural language inference. Bias in downstream applications can stem from training data, word embeddings, or be amplified by the model in use. However, focusing on biased word embeddings is potentially the most impactful first step due to their universal nature. Here we seek to understand how the intrinsic properties of word embeddings contribute to this observed marked attribute effect, and whether current post-processing methods address the bias successfully. An investigation of the current debiasing landscape reveals two open problems: none of the current debiased embeddings mitigate the marked attribute error, and none of the intrinsic bias measures are predictive of the marked attribute effect. By noticing that a new type of intrinsic bias measure correlates meaningfully with the marked attribute effect, we propose a new postprocessing debiasing scheme for static word embeddings. The proposed method applied to existing embeddings achieves new best results on the marked attribute bias test set. See https://github.com/hillary-dawkins/MAB.",
            "score": 0.5907476056960181,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4755859375
        },
        {
            "corpus_id": "268157336",
            "title": "A Review of Current Trends, Techniques, and Challenges in Large Language Models (LLMs)",
            "text": "Bias and fairness, if not adequately addressed, pose serious societal implications in the form of biased language generation and its impact on some segments of society. Basis can creep into LLMs from several sources discussed below. The first source of bias, dataset bias, stems from the datasets that were used to train the LLMs. If the datasets contain biases related to race, gender, religion, or socioeconomic status, the models inherit and amplify them. \n\nUnderrepresentation or misrepresentation of certain groups in the training data can lead to representation bias and biased language generation. The LLM developers should have checks and balances to ensure that all perspectives are adequately represented in the datasets. Otherwise, the model will produce inaccurate or skewed output for underrepresented groups. If the training data contain stereotypes, models amplify stereotyping and perpetuate prejudices. Fairness across demographics is a complex challenge but essential for advancing LLMs. \n\nCentextual bias stems from the context in which the language models are used. This poses severe and negative implications in applications such as recommender systems, employee hiring and promotions, clustering, and sentiment analysis. The model evaluation metrics and benchmarks used in traditional machine learning are inadequate to capture bias in LLMs. Comprehensive evaluation methods are needed to consider various aspects of bias in LLMs. A multifaceted approach is required to address bias and fairness issues in LLMs. Approaches to data curation, model development, evaluation strategies, and ethical issues need to be reexamined for their suitability for LLMs. Mitigating biases in the datasets using debiasing approaches such as modifying loss functions, altering training data distributions, and adversarial training requires LLM-contextualized research.",
            "score": 0.5906638391088321,
            "section_title": "Fairness",
            "char_start_offset": 109399,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 458
                },
                {
                    "start": 461,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1005
                },
                {
                    "start": 1008,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1242
                },
                {
                    "start": 1243,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1452
                },
                {
                    "start": 1453,
                    "end": 1533
                },
                {
                    "start": 1534,
                    "end": 1677
                },
                {
                    "start": 1678,
                    "end": 1873
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88720703125
        },
        {
            "corpus_id": "249431834",
            "title": "Gender Bias in Word Embeddings: A Comprehensive Analysis of Frequency, Syntax, and Semantics",
            "text": "Today, the vast majority of our daily tasks are facilitated and enhanced through the application of Natural Language Processing (NLP), from simple machine translation to automated resume screening to auto-complete in emails [7]. The core component of many of these applications are pretrained static word embeddings -compressed, numeric representations of word meaning based on word co-occurrence statistics. These word embeddings are, in turn, created by training an algorithm (e.g., a neural network) on massive corpora of human-produced text stored on the internet. Ideally, word embeddings would be objective representations of human semantics but, in reality, word embeddings trained from human-produced text end up encoding and reproducing the types of social biases held by humans [9,12]. When datasets that reflect the thoughts, feelings, and actions of a community are used to train artificial intelligence (AI) models, the models inevitably incorporate the associations and representations of the community [11-13, 15, 17, 26, 40, 51, 54]. \n\nIn one of the first studies systematically assessing social biases in word embeddings, Caliskan et al. [12] showed that pretrained GloVe embeddings replicated ten major biases widely found among human participants including biases associating young-good/elderlybad, European American-good/African American-bad, and womenfamily/men-career [28]. Such group-based biases can manifest in NLP applications (a recent example was seen in Amazon's automated resume screening algorithm that preferentially hired men over women [7]). A comprehensive analysis of where biases reside in word embeddings, including in word frequency, syntax, and semantics, can aid in developing effective bias mitigation strategies. \n\nThe Present Research. The current manuscript provides the first comprehensive investigation of the many, and often subtle ways that social biases are reflected in widely-used static English word embeddings trained on internet corpora (specifically GloVe 2014 [41] and fastText 2017 [8]). We focus, in particular on gender biases, because gender is present in every society and gender-based biases affect large groups of people.",
            "score": 0.5903702633029858,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1073,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 1049
                },
                {
                    "start": 1052,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1575
                },
                {
                    "start": 1576,
                    "end": 1755
                },
                {
                    "start": 1758,
                    "end": 1779
                },
                {
                    "start": 1780,
                    "end": 2045
                },
                {
                    "start": 2046,
                    "end": 2185
                }
            ],
            "ref_mentions": [
                {
                    "start": 224,
                    "end": 227,
                    "matchedPaperCorpusId": "213771228"
                },
                {
                    "start": 788,
                    "end": 791,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 791,
                    "end": 794,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 1155,
                    "end": 1159,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 1390,
                    "end": 1394,
                    "matchedPaperCorpusId": "7840819"
                },
                {
                    "start": 1570,
                    "end": 1573,
                    "matchedPaperCorpusId": "213771228"
                },
                {
                    "start": 2017,
                    "end": 2021,
                    "matchedPaperCorpusId": "1957433"
                },
                {
                    "start": 2040,
                    "end": 2043,
                    "matchedPaperCorpusId": "207556454"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89306640625
        },
        {
            "corpus_id": "238259136",
            "title": "Low Frequency Names Exhibit Bias and Overfitting in Contextualizing Language Models",
            "text": "Our work with large-scale demographic datasets has involved simplifications which, while useful for studying the impact of language models on marginalized demographic groups, has also introduced a number of limitations to our work. These are primarily related to the use of categorical labels for gender and race, but also concern the properties of the demographic datasets employed and the role of frequency in interpreting our results.\n\nA central limitation of our work is that categorical labels are assigned to each name based on gender and race. We understand race and gender not as essential characteristics, but as defined within a particular culture, time period, and social structure. Our research question is whether underrepresentation (i.e., low frequency) of social groups in a language model training corpus results in overfit-ting and exacerbated bias in the trained language model. To answer this, we analyze frequency of representation and its effects along two axes of demographic bias: gender bias, wherein males are more represented than females; and racial bias, wherein people considered to be white are represented more than people considered to be black, Hispanic, or Asian. The range of social groups studied reflects not an ideal division of humanity into immutable categories, but a comprehensible way of interpreting the disadvantages caused by underrepresentation in our current cultural context.\n\nOur work can be thought of an exposure study which examines the effects of one aspect of gender and race: underrepresentation. In the context of studies of race, Sen and Wasow (2016) usefully define an exposure study as one which uses \"a cue or signal that generates some reaction,\" and note that \"names often act as a proxy for for traits associated with racial or ethnic groups.\" Well-established NLP methods such as the WEAT of Caliskan et al. (2017) have demonstrated the efficacy of using first names for observing biases in AI. As described in Section 4.1, we use U.S. Social Security Administration (SSA) data to give each first name a gender label, and a dataset which uses the same racial categories as the U.S. census does for surnames to give each first name a racial label. We do not intend to essentialize the name, or individuals bearing the name, but to identify the gender or racial association most",
            "score": 0.5898639854745964,
            "section_title": "Ethical Considerations",
            "char_start_offset": 28587,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1589,
                    "end": 1609,
                    "matchedPaperCorpusId": "16445208"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59716796875
        },
        {
            "corpus_id": "252968208",
            "title": "The Tail Wagging the Dog: Dataset Construction Biases of Social Bias Benchmarks",
            "text": "Social bias is very sensitive to how we measure it. The primary assumption of most social bias measures is that language models understand the underlying language (based on their performance on limited datasets), and that task performance on limited datasets is an indicator of such understanding. Bias measures are created atop this assumption wherein after understanding a sentence, the model makes a calculated judgment -and not an errorabout stereotypical associations. We see how that is not necessarily true with models changing \"biased\" predictions with simple linguistic changes such as synonymization. Even with simple sentences, it is not apparent how to disentangle the biased association of the identity with the verb or the occupation amongst others. This is especially important to note as it highlights that measures can lack concrete definitions of what biased associations they measure. Consequently, the relation between measured bias and experienced harm becomes unclear.\n\nFurther, the empirical evidence sheds light on how the model's non-social biases brought out or masked by alternate constructions can cause bias benchmarks to underestimate or overestimate (or even just cause higher variance in) the social bias exhibited by the model. More interestingly, it is important to note that different models respond differently to various perturbations. In fact, the same perturbation can result in an higher or lower measured bias depending on the model (as seen in \u00a74.1 and \u00a74.2), which points to how models might parse information (and thus bias) differently.\n\nWhile current bias measures do play a role in exposing where model errors have a stereotypical connotation, a lack of sentence construction variability or even assumptions made when creating seed word lists can reduce the reliability of the benchmarks, as we see in this work ( \u00a74.2) We leave it to future work to investigate how to construct benchmarks to measure the true target bias without being effected by model errors and other non-social biases. We also encourage further discussions on the complexity of sentences used in measures and their implications on what gets measured in relation to un-templated, naturally-occurring text (Levy et al., 2021), as an attempt to ground our measurements in experienced harms.",
            "score": 0.5896087902872907,
            "section_title": "Discussion",
            "char_start_offset": 13308,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 2222,
                    "end": 2241,
                    "matchedPaperCorpusId": "237452751"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55078125
        },
        {
            "corpus_id": "270878599",
            "title": "CEB: Compositional Evaluation Benchmark for Fairness in Large Language Models",
            "text": "Bias Evaluation of LLMs. Fairness concerns of LLMs have increased as they are incorporated into a wider range of real-world applications (Gallegos et al., 2023;Liang et al., 2023;Abid et al., 2021). Bolukbasi et al. (Bolukbasi et al., 2016) is one of the earliest to identify and address gender biases in word embeddings such as Word2Vec (Mikolov et al., 2013). Caliskan et al. (Caliskan et al., 2017) show that word embeddings capture semantic meanings and societal biases, highlighting how training data biases propagate in language models. Building on these studies, recent techniques assess social biases in LLMs (Chu et al., 2024). TrustLLM (Sun et al., 2024) examines biases like preference and stereotyping. HELM (Liang et al., 2023) analyzes social bias by examining demographic terms in model outputs in response to particular prompts. From the perspective of toxicity, TrustGPT (Huang et al., 2023) assesses the toxicity degrees in outputs toward various demographic groups. Conversely, Li et al. (Li et al., 2024) use counterfactual fairness to assess ChatGPT (OpenAI, 2022) performance in high-stakes domains like healthcare. In seeking a more detailed assessment, DecodingTrust (Wang et al., 2023) offers a thorough fairness assessment for GPT-4 (Anand et al., 2023) that focuses on stereotype bias and general fairness independently. \n\nBias Evaluation Datasets. To investigate biases in LLMs, researchers have developed a variety of datasets tailored to different evaluation tasks. Masked token datasets present sentences with a blank slot that language models must complete (Gallegos et al., 2023). WinoBias (Zhao et al., 2018), a key dataset for coreference resolution, assesses word associations with social groups but suffers from limited volume and syntactic diversity.",
            "score": 0.5895216366461731,
            "section_title": "RELATED WORK",
            "char_start_offset": 3754,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 24
                },
                {
                    "start": 25,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 636
                },
                {
                    "start": 637,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1347
                },
                {
                    "start": 1350,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1495
                },
                {
                    "start": 1496,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1788
                }
            ],
            "ref_mentions": [
                {
                    "start": 160,
                    "end": 179,
                    "matchedPaperCorpusId": "253553585"
                },
                {
                    "start": 179,
                    "end": 197,
                    "matchedPaperCorpusId": "231603388"
                },
                {
                    "start": 199,
                    "end": 240,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 362,
                    "end": 401,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 720,
                    "end": 740,
                    "matchedPaperCorpusId": "253553585"
                },
                {
                    "start": 1191,
                    "end": 1210,
                    "matchedPaperCorpusId": "259202782"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5517578125
        },
        {
            "corpus_id": "248006238",
            "title": "Mapping the Multilingual Margins: Intersectional Biases of Sentiment Analysis Systems in English, Spanish, and Arabic",
            "text": "privilege and marginalization faced by an individual with a variety of identities across a set of social cleavages such as race and gender. A canonical usage of intersectionality is in service of studying the simultaneous racial and gender discrimination faced by Black women, which cannot be understood in its totality using racial or gendered frameworks independently; for one example, we point to the angry Black woman stereotype (Collins, 2004). As such, we argue that existing studies in fairness are limited in their ability both to uncover bias in and to \"debias\" language models without engaging with the intersectionality framework.\n\nIntersectional social biases have been documented in natural language processing models. Herbelot et al. (2012) first studied intersectional social bias by employing distributional semantics on a Wikipedia dataset while Tan and Celis (2019) studied intersectional social bias in contextualized word embeddings by using the WEAT on language referring to white men and Black women. Guo and Caliskan (2021) introduce tests that detect both known and emerging intersectional social biases in static word embeddings and extend the WEAT to contextualized word embeddings. Similarly, May et al. (2019) also extend the WEAT to a contextualized word embedding framework using sentence embeddings. However, these methods do not consider the effect of intersectional social biases on the results of downstream tasks, which is the focus of this work.\n\nStudies on non-English social biases in natural language processing are limited, with Zhou et al. (2019) extending the WEAT to study gender bias in Spanish and French and Zhao et al. (2020) examining gender bias in English, Spanish, German, and French on fastText embeddings (Bojanowski et al., 2017). Notably, to the best of our knowledge there has been no work on studying intersectional social biases in languages other than English in natural language processing. While Herbelot et al. (2012) and Guo and Caliskan (2021) study the intersectional social biases faced by Asian and Mexican women respectively using natural language processing, both do so in English. In contrast, our work seeks to understand intersectional social biases in the languages that are used by the individuals and the communities that they help constitute.",
            "score": 0.5891366583454852,
            "section_title": "Related Works",
            "char_start_offset": 6190,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 732,
                    "end": 754,
                    "matchedPaperCorpusId": "8796684"
                },
                {
                    "start": 1023,
                    "end": 1046,
                    "matchedPaperCorpusId": "219530686"
                },
                {
                    "start": 1957,
                    "end": 1979,
                    "matchedPaperCorpusId": "8796684"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71240234375
        },
        {
            "corpus_id": "271769656",
            "title": "A Parameter-Efficient Multi-Objective Approach to Mitigate Stereotypical Bias in Language Models",
            "text": "There are several limitations we need to acknowledge in this study. Firstly, our methods have been evaluated exclusively on binary gender bias, without extending the tests to encompass biases related to race, religion, and non-binary gender identities. This narrow focus restricts the generalizability of our findings, as biases in language models are multifaceted and can manifest across various dimensions. Future research should aim to include these additional social groups to provide a more comprehensive understanding of the efficacy of our debiasing approach. Furthermore, we have only considered prefixtuning and did not experiment with other parameterefficient fine-tuning methods such as adapter tuning or LoRA. This limits our ability to compare the effectiveness and efficiency of different parameterefficient fine-tuning approaches.",
            "score": 0.58844532232039,
            "section_title": "Limitations",
            "char_start_offset": 34876,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 67
                },
                {
                    "start": 68,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 845
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.607421875
        },
        {
            "corpus_id": "264590390",
            "title": "Pre-trained Speech Processing Models Contain Human-Like Biases that Propagate to Speech Emotion Recognition",
            "text": "The results in Figure 1 show that pre-trained speech processing models trained on large corpora of speech data, like those trained on large corpora of image or text data, can easily learn human-like biases related to gender, race, ability, age, and accent. Work in computer vision has suggested that biases detected by EATs may arise from co-occurrence in the training data (Steed and Caliskan, 2021), and we hypothesize that co-occurrence may contribute here as well, for example if speakers who use one speech style tend to more frequently express positive valence than speakers who use other speech styles in the training set. We also hypothesize that the wide dispersion in the test comparing human and synthesized speech may be related to frequency as it is not clear that synthesized speech is contained in any of the pre-training corpora that the models we consider were trained on. \n\nBeyond just identifying that biases can be learned from data in a new modality, however, this work also identifies a new means of representing target concepts. In using speaking styles to represent concepts, rather than names or words as in text-based models, we've shown that not only do models demonstrate bias based on language content, but also based on how, and by whom, it is delivered. The data used for training models in computer vision, natural language processing, and speech processing is inherently multi-faceted-it does not connect to social groups in just one way. This data is produced by people who belong to certain social groups; it may be about people of certain social groups; it may be recorded by people of certain social groups; and it may also have meaning to people of certain social groups. Our results suggest that, at least in speech processing, models are able to learn associations beyond just content, and raises the question of whether there are differential associations related to the provenance of text and image data as well. Would text of similar meaning, but written by people of different social groups, have different associations with valence? \n\nBeyond adding further evidence for bias in AER (Kiritchenko and Mohammad, 2018), our experiment with SER models also adds evidence that biases in pre-trained models can propagate downstream.",
            "score": 0.5882824274955142,
            "section_title": "Discussion",
            "char_start_offset": 28740,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 256
                },
                {
                    "start": 257,
                    "end": 629
                },
                {
                    "start": 630,
                    "end": 889
                },
                {
                    "start": 892,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1709
                },
                {
                    "start": 1710,
                    "end": 1954
                },
                {
                    "start": 1955,
                    "end": 2077
                },
                {
                    "start": 2080,
                    "end": 2270
                }
            ],
            "ref_mentions": [
                {
                    "start": 374,
                    "end": 400,
                    "matchedPaperCorpusId": "225094318"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6103515625
        },
        {
            "corpus_id": "271270687",
            "title": "Evaluation of LLMs Biases Towards Elite Universities: A Persona-Based Exploration",
            "text": "Previous research has extensively documented the presence of biases in AI systems, particularly within language models. These biases often stem from the data used to train these models, which can reflect and perpetuate societal inequalities and stereotypes. In the paper, (Gallegos et al. 2024) share the work emphasizing the need for a structured understanding of social bias and fairness in natural language processing. The authors propose taxonomies to categorize bias evaluation metrics, datasets, and mitigation techniques. This work highlights the importance of addressing bias before widespread LLM adoption. The current paper aligns with this growing focus on LLM fairness. We investigated whether LLMs are biased towards elite universities while generating educational backgrounds for personas of employees/candidates in leading tech companies in their engineering and product functions. In the paper, (Hayes et al. 2024) present an interesting approach to exploring the LLM bias from the perspective of Value Bias. The author demonstrates that LLMs tend to favor options with higher perceived value. Such an aspect is relevant for the current scope of research as the LLM bias discussed in the paper can manifest in elite educational degree contexts as well. This work underscores the importance of considering not only explicit biases but also subtle biases related to perceived value. In the paper, (Kotek et al. 2023) investigate gender bias and stereotypes in Large Language Models (LLMs). \n\nThe study shows that LLMs show bias towards stereotypical gender roles in occupational choices. An insightful finding is that LLMs can offer explanations for their biased choices that are factually inaccurate, further obscuring the true cause of the bias. This emphasizes the importance of rigorously testing LLMs for bias to ensure they treat all individuals fairly. This makes it critical to identify potential biases in LLMs before implementing the LLMs-based applications, especially in areas where biases can lead to non-inclusivity, spread of misinformation, and then subsequent long-term societal impact. This aligns with our research approach in analyzing elite university degree bias in LLM-generated personas. Since educational background details are likely influenced by the training data, exploring their representation in LLM output can reveal potential biases toward prestigious institutions.",
            "score": 0.5880706414912588,
            "section_title": "Related Work:",
            "char_start_offset": 3354,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1396
                },
                {
                    "start": 1397,
                    "end": 1503
                },
                {
                    "start": 1506,
                    "end": 1601
                },
                {
                    "start": 1602,
                    "end": 1761
                },
                {
                    "start": 1762,
                    "end": 1873
                },
                {
                    "start": 1874,
                    "end": 2117
                },
                {
                    "start": 2118,
                    "end": 2225
                },
                {
                    "start": 2226,
                    "end": 2412
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7109375
        },
        {
            "corpus_id": "218487466",
            "title": "Social Biases in NLP Models as Barriers for Persons with Disabilities",
            "text": "Building equitable and inclusive NLP technologies demands consideration of whether and how social attitudes are represented in ML models. In particular, representations encoded in models often inadvertently perpetuate undesirable social biases from the data on which they are trained. In this paper, we present evidence of such undesirable biases towards mentions of disability in two different English language models: toxicity prediction and sentiment analysis. Next, we demonstrate that the neural embeddings that are the critical first step in most NLP pipelines similarly contain undesirable biases towards mentions of disability. We end by highlighting topical biases in the discourse about disability which may contribute to the observed model biases; for instance, gun violence, homelessness, and drug addiction are over-represented in texts discussing mental illness.",
            "score": 0.5877960865087538,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73828125
        },
        {
            "corpus_id": "270521813",
            "title": "The Devil is in the Neurons: Interpreting and Mitigating Social Biases in Pre-trained Language Models",
            "text": "Large pre-trained language models (PLMs) have demonstrated remarkable performance across various natural language processing tasks.Nevertheless, they also exhibit a proclivity to manifest biased behaviors that are unfair to marginalized social groups (Aky\u00fcrek et al., 2022;Webster et al., 2020).As research on AI fairness gains increasing importance, there have been efforts to detect (Davani et al.;Fleisig et al., 2023;An & Rudinger, 2023) and mitigate (Kaneko & Bollegala, 2021;Guo et al., 2022) social biases in PLMs.Most approaches for detecting social biases in PLMs rely on prompt or probing-based techniques that treat PLMs as black boxes (Goldfarb-Tarrant et al., 2023;Feng et al., 2023).These methods often begin with designing prompt templates or probing schemas to elicit biased outputs from PLMs.Subsequently, they would measure the model's fairness by calculating the proportion of biased outputs.The effectiveness of this approach relies heavily on the quality of the designed prompt templates or probing schemas (Shaikh et al., 2022).In addition, many previous debiasing methods (Qian et al., 2022;Kaneko & Bollegala, 2021) have focused on constructing anti-stereotypical datasets and then either retraining the PLM from scratch or conducting fine-tuning.This line of debiasing approaches, although effective, comes with high costs for data construction Published as a conference paper at ICLR 2024 Demographic-1 (Logits = 0.1)  (Logits = 0.9) Logits Gap=0.8",
            "score": 0.5873962351032465,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 131,
                    "end": 295
                },
                {
                    "start": 295,
                    "end": 521
                },
                {
                    "start": 521,
                    "end": 697
                },
                {
                    "start": 697,
                    "end": 809
                },
                {
                    "start": 809,
                    "end": 911
                },
                {
                    "start": 911,
                    "end": 1050
                },
                {
                    "start": 1050,
                    "end": 1271
                },
                {
                    "start": 1271,
                    "end": 1474
                }
            ],
            "ref_mentions": [
                {
                    "start": 400,
                    "end": 421,
                    "matchedPaperCorpusId": "259092939"
                },
                {
                    "start": 481,
                    "end": 498,
                    "matchedPaperCorpusId": "248780440"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5322265625
        },
        {
            "corpus_id": "258426630",
            "title": "A Review of ChatGPT Applications in Education, Marketing, Software Engineering, and Healthcare: Benefits, Drawbacks, and Research Directions",
            "text": "Language models can exhibit bias if the training data used to create them is biased. As Schramowski et al. [37] pointed out, large pre-trained models that try to mimic natural languages, may end up repeating the same unfairness and prejudices. This can lead to discriminatory or inaccurate analyses and recommendations. Moreover, this may lead to public outcry (i.e., political, social, and legal) against the commercial applications. These biases manifests themselves in several ways, as follows: \n\n\u2022 Training data bias: Language models are typically trained on large datasets of human language. If these datasets are biased in some way (e.g., based on race, gender, socioeconomic status, etc.), then the model may learn and replicate these biases in its responses. For example, if the training data is biased towards a particular gender, then the model may generate responses that are more favorable towards that gender. \n\n\u2022 User interaction bias: The responses generated by Chatbots are based on the input they receive from users. If users consistently ask biased or prejudiced questions, then the model may learn and replicate these biases in its responses. For example, if users frequently ask questions that are discriminatory towards a particular group, then the model may generate responses that perpetuate these biases. \n\n\u2022 Algorithmic bias: The algorithms used to train and operate language models and Chatbots may also introduce biases. For example, if the model is trained to optimize for a particular metric (e.g., accuracy, engagement, etc.), then it may prioritize generating responses that optimize for that metric, even if those responses are biased in some way. \n\n\u2022 Contextual bias: Chatbots generate responses based on the context they receive from users. If the context is biased in some way (e.g., based on the user's location, language, etc.), then the model may generate biased responses. For example, if a user is asking questions about a particular culture or religion, and the model is not trained on that culture or religion, it may generate biased responses due to its lack of knowledge. \n\nIt is important to note that bias in language models are not necessarily intentional or malicious.",
            "score": 0.5863440161881022,
            "section_title": "Bias",
            "char_start_offset": 24519,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 85,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 497
                },
                {
                    "start": 500,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 922
                },
                {
                    "start": 925,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1161
                },
                {
                    "start": 1162,
                    "end": 1328
                },
                {
                    "start": 1331,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1679
                },
                {
                    "start": 1682,
                    "end": 1774
                },
                {
                    "start": 1775,
                    "end": 1911
                },
                {
                    "start": 1912,
                    "end": 2115
                },
                {
                    "start": 2118,
                    "end": 2216
                }
            ],
            "ref_mentions": [
                {
                    "start": 107,
                    "end": 111,
                    "matchedPaperCorpusId": "246824056"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83544921875
        },
        {
            "corpus_id": "270619502",
            "title": "Evaluating Short-Term Temporal Fluctuations of Social Biases in Social Media Data and Masked Language Models",
            "text": "Social Biases in NLP.Social biases in NLP were first drawn to attention by Bolukbasi et al. (2016), with the famous analogy \"man is to computer programmer as woman is to homemaker\" provided by static word embeddings.To evaluate social biases in word embeddings, word Embedding Association Test (WEAT; Caliskan et al., 2017a) was introduced to measure the bias between two sets of target terms with respect to two sets of attribute terms.Subsequently, Word Association Test (WAT; Du et al., 2019) was proposed to compute a gender information vector for each word within an association graph (Deyne et al., 2019) through the propagation of information associated with masculine and feminine words.Follow-up studies investigate social biases in additional models (Liang et al., 2020a,b;Zhou et al., 2022) and languages (Mc-Curdy and Serbetci, 2020;Lauscher et al., 2020;Reusens et al., 2023;Zhou et al., 2023).\n\nIn contrast, alternative research focuses on social biases in various downstream applications.Kiritchenko and Mohammad (2018) assessed gender and racial biases across 219 automatic sentiment analysis systems, revealing statistically significant biases in several of these systems.D\u00edaz et al. (2018) investigated age-related biases in sentiment classification and found that many sentiment analysis systems, as well as word embeddings, encode significant age bias in their outputs.Savoldi et al. (2021) studied gender biases and sentiment biases associated with person name translations in neural machine translation systems.\n\nCurrent bias evaluation methods use different approaches, including pseudo-likelihood.(Kaneko and Bollegala, 2022), cosine similarity (Caliskan et al., 2017b;May et al., 2019), inner-product (Ethayarajh et al., 2019), among others.Independently of any downstream tasks, intrinsic bias evaluation measures (Nangia et al., 2020;Nadeem et al., 2021;Kaneko and Bollegala, 2022) assess social biases in MLMs on a standalone basis.",
            "score": 0.5860814841327142,
            "section_title": "Related Work",
            "char_start_offset": 3028,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 21
                },
                {
                    "start": 21,
                    "end": 216
                },
                {
                    "start": 216,
                    "end": 437
                },
                {
                    "start": 437,
                    "end": 695
                },
                {
                    "start": 695,
                    "end": 907
                },
                {
                    "start": 909,
                    "end": 1003
                },
                {
                    "start": 1003,
                    "end": 1189
                },
                {
                    "start": 1189,
                    "end": 1389
                },
                {
                    "start": 1389,
                    "end": 1533
                },
                {
                    "start": 1535,
                    "end": 1621
                },
                {
                    "start": 1621,
                    "end": 1766
                },
                {
                    "start": 1766,
                    "end": 1960
                }
            ],
            "ref_mentions": [
                {
                    "start": 75,
                    "end": 98,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 479,
                    "end": 495,
                    "matchedPaperCorpusId": "202774579"
                },
                {
                    "start": 783,
                    "end": 801,
                    "matchedPaperCorpusId": "247450590"
                },
                {
                    "start": 845,
                    "end": 867,
                    "matchedPaperCorpusId": "226237614"
                },
                {
                    "start": 867,
                    "end": 888,
                    "matchedPaperCorpusId": "264146917"
                },
                {
                    "start": 888,
                    "end": 906,
                    "matchedPaperCorpusId": "264306000"
                },
                {
                    "start": 1003,
                    "end": 1034,
                    "matchedPaperCorpusId": "21670658"
                },
                {
                    "start": 1189,
                    "end": 1207,
                    "matchedPaperCorpusId": "3272048"
                },
                {
                    "start": 1389,
                    "end": 1410,
                    "matchedPaperCorpusId": "233219939"
                },
                {
                    "start": 1669,
                    "end": 1693,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 1693,
                    "end": 1709,
                    "matchedPaperCorpusId": "85518027"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.497314453125
        },
        {
            "corpus_id": "264591304",
            "title": "Geographical Erasure in Language Generation",
            "text": "Large pretrained models serve as base models for many downstream NLP applications, including question-answering, dialogue, common-sense reasoning, classification, tagging, translation, summarisation, and generation (Devlin et al., 2018;Brown et al., 2020;Chowdhery et al., 2022). Despite their increasing utility, there are concerns about how they reflect and amplify biases in the training data. For instance, unfiltered data originating from the internet is known to be rife with toxic, misogynistic, and stereotyping content. Many studies highlight biases in model outputs, primarily concerning representational harms (Barocas et al., 2017), where a section of society (e.g., women, LGBTQ+ communities) are represented in poor light, or are ignored by the system (Bolukbasi et al., 2016;Guo and Caliskan, 2021;May et al., 2019; Prediction Probabilities Country probabilities assigned by GPT-NeoX when prompted with \"I live in\". Middle: English speaking populations per country. Bottom: Countries experiencing erasure, i.e. underprediction compared to their population by at least a factor 3 (see \u00a73). Data is missing for grey countries (see \u00a76). Tan and Celis, 2019). While important, such studies predominantly examine biases related to race, gender, occupation and sexual orientation. \n\nAn important-and often overlooked-aspect of inclusive model development is geographical inclusion. This is particularly important at a time when most large-scale model training efforts come from a small set of regions. Further, these models are trained using internet data, whose access, in the first place, is unequally distributed (Blank, 2017;Center, 2021). Minimising cultural and geo-12310 graphic identities is referred to as erasure (Roche, 2019) and is studied by linguists and social scientists in the context of imperialism and colonialism, where \"people are silenced in the historical record [...], their contemporary presence rendered invisible, and their existence written out of the future\" (Roche, 2019).",
            "score": 0.5854676825575698,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1289
                },
                {
                    "start": 1292,
                    "end": 1390
                },
                {
                    "start": 1391,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 2011
                }
            ],
            "ref_mentions": [
                {
                    "start": 236,
                    "end": 255,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 766,
                    "end": 790,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 790,
                    "end": 813,
                    "matchedPaperCorpusId": "219530686"
                },
                {
                    "start": 1625,
                    "end": 1638,
                    "matchedPaperCorpusId": "64225884"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.363525390625
        },
        {
            "corpus_id": "248366292",
            "title": "Towards an Enhanced Understanding of Bias in Pre-trained Neural Language Models: A Survey with Special Emphasis on Affective Bias",
            "text": "Similarly, use of the large PLM GPT-3 in language generation shows religion bias analogizing 'Muslims' to 'Terrorists' [2]. All such biases are disturbing when one observes that word embeddings, being the base constituent of most language systems, can propagate or even intensify them [22,61,127] causing unfavorable outcomes when deployed in a plethora of downstream applications such as sentiment analysis [79], language generation [66], toxic language detection [48], etc. Downstream task learning bias can cause their outcomes delivered to the public to finally end up in socio-economic exclusions reinforcing harmful societal stereotypes [16,61,127]. Since downstream applications are generally implemented by initializing learning models with an existing source network representation pre-trained on large datasets and later fine-tuned using datasets that suit downstream target task, pre-training data bias, pre-trained representation bias and fine-tuning data bias all can be the sources to induce bias in downstream applications. Sentiment analysis [52], abusive language detection [81], text classification [33], machine translation [38,112], personalized medicine [90], corefer-ence resolution [95,128], crime recidivism prediction systems [27], automating resume screening [56], online advertisements delivery [57,109], etc., are some of the downstream applications that reports bias in various domains. Inappropriate or unfair choice of label usages to fine-tune downstream task is another source of bias i.e., Measurement Bias [107]. \n\nThese biases can be distinguished as Intrinsic Bias if it occurs in pre-trained learning or Extrinsic Bias if it occurs in downstream task modeling. Besides above mentioned biases, in the perspective of real-world machine learning models, the final system must consider Evaluation Bias that occurs when benchmark dataset for a task doesn't represent certain groups (e.g., images of non-white women not being identified by the model) and Deployment Bias that occurs due incompatibility of a model designed for a particular task when used differently (e.g., using risk assessment tool created to predict future crime for the purpose of determining length of sentence) [107].",
            "score": 0.585261846677153,
            "section_title": "Bias in Pre-trained Language Models",
            "char_start_offset": 16611,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1547
                },
                {
                    "start": 1550,
                    "end": 1698
                },
                {
                    "start": 1699,
                    "end": 2222
                }
            ],
            "ref_mentions": [
                {
                    "start": 285,
                    "end": 289,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 289,
                    "end": 292,
                    "matchedPaperCorpusId": "53467348"
                },
                {
                    "start": 292,
                    "end": 296,
                    "matchedPaperCorpusId": "1389483"
                },
                {
                    "start": 408,
                    "end": 412,
                    "matchedPaperCorpusId": "209883089"
                },
                {
                    "start": 465,
                    "end": 469,
                    "matchedPaperCorpusId": "235097594"
                },
                {
                    "start": 643,
                    "end": 647,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 647,
                    "end": 650,
                    "matchedPaperCorpusId": "53467348"
                },
                {
                    "start": 650,
                    "end": 654,
                    "matchedPaperCorpusId": "1389483"
                },
                {
                    "start": 1058,
                    "end": 1062,
                    "matchedPaperCorpusId": "21670658"
                },
                {
                    "start": 1091,
                    "end": 1095,
                    "matchedPaperCorpusId": "52070035"
                },
                {
                    "start": 1117,
                    "end": 1121,
                    "matchedPaperCorpusId": "54997157"
                },
                {
                    "start": 1143,
                    "end": 1147,
                    "matchedPaperCorpusId": "57761084"
                },
                {
                    "start": 1147,
                    "end": 1151,
                    "matchedPaperCorpusId": "76654723"
                },
                {
                    "start": 1175,
                    "end": 1179,
                    "matchedPaperCorpusId": "54574300"
                },
                {
                    "start": 1205,
                    "end": 1209,
                    "matchedPaperCorpusId": "13756572"
                },
                {
                    "start": 1209,
                    "end": 1213,
                    "matchedPaperCorpusId": "4952494"
                },
                {
                    "start": 1251,
                    "end": 1255,
                    "matchedPaperCorpusId": "1443041"
                },
                {
                    "start": 1285,
                    "end": 1289,
                    "matchedPaperCorpusId": "263870936"
                },
                {
                    "start": 1322,
                    "end": 1326,
                    "matchedPaperCorpusId": "263870936"
                },
                {
                    "start": 1541,
                    "end": 1546,
                    "matchedPaperCorpusId": "235436386"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53759765625
        },
        {
            "corpus_id": "214449194",
            "title": "Towards Socially Responsible AI: Cognitive Bias-Aware Multi-Objective Learning",
            "text": "Demonstrating social bias in classification tasks. Davidson, Bhattacharya, and Weber (2019) demonstrated racial bias in four different tweet data sets used for hate and abusive language detection. They showed that a classifier trained on these tweets exhibits a social bias in predicting that abusive tweets are mostly written by African-Americans as compared to white Americans. On a similar note, Sap et al. (2019) demonstrated racial bias in Twitter datasets for abusive language detection, and also demonstrated that such bias can be traced back to the annotation process itself. The study found that when presented with the ethnicity information, the annotators were more likely to tag a tweet of an African American as abusive. Kiritchenko and Mohammad (2018) reports that a number of existing models for sentiment analysis suffer from either race or gender bias. ? used a constrained learning approach to debias classification models on feature vectors. In contrast, we employ a multiobjective function and do not use hand-crafted features for our experiments. \n\nSocial identity features for improving predictions. Hovy (2015) showed that the use of demographic features, such as age, and gender can improve the text classification task performance across five languages. On a similar note, Vanmassenhove, Hardmeier, and Way (2018) found that including the gender information of the speakers (authors) help to translate sentences more effectively to a target language with gender-specific morphology. Stanovsky, Smith, and Zettlemoyer (2019) proposed an evaluation methodology to measure gender bias in machine translation (MT) systems and also released a dataset to support further investiga-tion in MT bias. Garimella et al. (2019) released a gender tagged dataset for POS-tagging and dependency parsing and showed that POS-tagging and dependency parsing effectiveness can vary across genders. \n\nMitigating bias from word embedding and down-stream tasks. Prost, Thain, and Bolukbasi (2019) proposed ways to debias word embedding with an objective to reduce bias in downstream tasks.",
            "score": 0.5852366046203685,
            "section_title": "Related Work",
            "char_start_offset": 4583,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 50
                },
                {
                    "start": 51,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 871
                },
                {
                    "start": 872,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1067
                },
                {
                    "start": 1070,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1278
                },
                {
                    "start": 1279,
                    "end": 1507
                },
                {
                    "start": 1508,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 1902
                },
                {
                    "start": 1905,
                    "end": 1963
                },
                {
                    "start": 1964,
                    "end": 2091
                }
            ],
            "ref_mentions": [
                {
                    "start": 51,
                    "end": 91,
                    "matchedPaperCorpusId": "168170119"
                },
                {
                    "start": 399,
                    "end": 416,
                    "matchedPaperCorpusId": "196211238"
                },
                {
                    "start": 734,
                    "end": 765,
                    "matchedPaperCorpusId": "21670658"
                },
                {
                    "start": 1122,
                    "end": 1133,
                    "matchedPaperCorpusId": "14021168"
                },
                {
                    "start": 1298,
                    "end": 1338,
                    "matchedPaperCorpusId": "76654723"
                },
                {
                    "start": 1508,
                    "end": 1548,
                    "matchedPaperCorpusId": "173991101"
                },
                {
                    "start": 1717,
                    "end": 1740,
                    "matchedPaperCorpusId": "196181460"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60205078125
        },
        {
            "corpus_id": "225094318",
            "title": "Image Representations Learned With Unsupervised Pre-Training Contain Human-like Biases",
            "text": "Can machines learn social biases from the way people are portrayed in image datasets? Companies and researchers regularly use machine learning models trained on massive datasets of images scraped from the web tasks from face recognition [41] to image classification [66]. To reduce costs, many practitioners use state-ofthe-art models \"pre-trained\" on large datasets to help solve other machine learning tasks, a powerful approach called transfer learning [68]. For example, HireVue uses similar state-of-the-art computer vision and natural language models to evaluate job candidates' video interviews, potentially discriminating against candidates based on race, gender, or other social factors [36]. In this paper, we show how models trained on unlabeled images scraped from the Internet embed human-like biases, including racism and sexism. \n\nWhere most bias studies focus on supervised machine learning models, we seek to quantify learned patterns of implicit social bias in unsupervised image representations. Studies in supervised computer vision have highlighted social biases related to race, gender, ethnicity, sexuality, and other identities in tasks including face recognition, object detection, image search, and visual question answering [11,43,20,76,47,52]. These algorithms are used in important real-world settings, from applicant video screening [36,61] to autonomous vehicles [29,52], despite the fact that their harmful downstream effects have been documented in applications such as online ad delivery [67] and image captioning [39]. \n\nOur work examines the growing set of computer vision methods in which no labels are used during model training. Recently, pre-training approaches adapted from language models have dramatically increased the quality of unsupervised representations for image recognition [22,4,38,16,14,15,50,13]. With fine-tuning, practitioners can pair these general-purpose representations with labels from their domain to accomplish specific supervised tasks like face recognition or image caption-Figure 1: Unilever using AI powered job candidate assessment tool HireVue [36]. \n\ning.",
            "score": 0.5850184674175141,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 85
                },
                {
                    "start": 86,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 843
                },
                {
                    "start": 846,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1553
                },
                {
                    "start": 1556,
                    "end": 1667
                },
                {
                    "start": 1668,
                    "end": 1850
                },
                {
                    "start": 1851,
                    "end": 2118
                },
                {
                    "start": 2121,
                    "end": 2125
                }
            ],
            "ref_mentions": [
                {
                    "start": 266,
                    "end": 270,
                    "matchedPaperCorpusId": "6842201"
                },
                {
                    "start": 456,
                    "end": 460,
                    "matchedPaperCorpusId": "51929263"
                },
                {
                    "start": 1251,
                    "end": 1255,
                    "matchedPaperCorpusId": "3298854"
                },
                {
                    "start": 1255,
                    "end": 1258,
                    "matchedPaperCorpusId": "8832874"
                },
                {
                    "start": 1367,
                    "end": 1370,
                    "matchedPaperCorpusId": "195316573"
                },
                {
                    "start": 1394,
                    "end": 1398,
                    "matchedPaperCorpusId": "6724907"
                },
                {
                    "start": 1825,
                    "end": 1829,
                    "matchedPaperCorpusId": "195820291"
                },
                {
                    "start": 1829,
                    "end": 1831,
                    "matchedPaperCorpusId": "173990164"
                },
                {
                    "start": 1831,
                    "end": 1834,
                    "matchedPaperCorpusId": "207930212"
                },
                {
                    "start": 1834,
                    "end": 1837,
                    "matchedPaperCorpusId": "211096730"
                },
                {
                    "start": 1840,
                    "end": 1843,
                    "matchedPaperCorpusId": "219781060"
                },
                {
                    "start": 1843,
                    "end": 1846,
                    "matchedPaperCorpusId": "208617491"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.48486328125
        },
        {
            "corpus_id": "249062690",
            "title": "Perturbation Augmentation for Fairer NLP",
            "text": "Unwanted and often harmful social biases are becoming ever more salient in NLP research, affecting both models and datasets. In this work, we ask whether training on demographically perturbed data leads to fairer language models. We collect a large dataset of human annotated text perturbations and train a neural perturbation model, which we show outperforms heuristic alternatives. We find that (i) language models (LMs) pre-trained on demographically perturbed corpora are typically more fair, and (ii) LMs finetuned on perturbed GLUE datasets exhibit less demographic bias on downstream tasks, and (iii) fairness improvements do not come at the expense of performance on downstream tasks. Lastly, we discuss outstanding questions about how best to evaluate the (un)fairness of large language models. We hope that this exploration of neural demographic perturbation will help drive more improvement towards fairer NLP.",
            "score": 0.5847847048106833,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75
        },
        {
            "corpus_id": "254017505",
            "title": "Undesirable biases in NLP: Averting a crisis of measurement",
            "text": "On the other hand, when comparing the gender bias measured in BERT, both D'Amour et al. ( 2022) and Aribandi et al. (2021) found low consistency across random seeds. While a low consistency could mean that bias measures are unreliable, alternatively, random seeds could influence the extent to which models learn certain biases (D'Amour et al., 2022;Du et al., 2021) -an important theoretical distinction that has to be explored in the future. \n\nCorpus-based test-retest reliability Consistency across training corpora could also be assessed by comparing the bias scores between models of the same architecture trained on different but comparable corpora (e.g., disjoint subsets of the same dataset). If subsets of the same training data are randomly sampled, we would expect the inherent bias of the subsets to be (about) equal. Significant inconsistencies in bias measures between the two resulting language models would thus suggest poor reliability of the bias measure (or unstable biasedness of the model; as mentioned above, distinguishing between these two options would be an important next step). \n\nTime-based test-retest reliability Finally, a way of retaining test-retest reliability's temporal component could be to compare bias measurements for models trained on data from the same corpus but collected at different points in time -for instance, datasets extracted from the same social media platform in adjacent months. When training corpora update so fast that language use or social biases did not significantly change between collection dates (i.e., implying that the training data's gender bias, which the model picks up on, also stays relatively constant across collection dates), we would expect a high degree of consistency between the bias measurements of models trained on the corpora. \n\nAnother potentially relevant comparison \"over time\" could be to observe how a language model's bias score changes across training steps -an idea we develop in Application II. Repeated testing over adjacent training steps can be used to assess test-retest reliability: Especially for late and proximal training steps (e.g., language models after 99% training vs 100% of training, when the language models' parameters -and hence their biases -should be largely stable), we would expect consistency in language models' bias scores.",
            "score": 0.5845611721560641,
            "section_title": "Test-retest reliability",
            "char_start_offset": 42709,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 443
                },
                {
                    "start": 446,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 1105
                },
                {
                    "start": 1108,
                    "end": 1433
                },
                {
                    "start": 1434,
                    "end": 1808
                },
                {
                    "start": 1811,
                    "end": 1985
                },
                {
                    "start": 1986,
                    "end": 2339
                }
            ],
            "ref_mentions": [
                {
                    "start": 100,
                    "end": 122,
                    "matchedPaperCorpusId": "234469686"
                },
                {
                    "start": 328,
                    "end": 350,
                    "matchedPaperCorpusId": "226278105"
                },
                {
                    "start": 350,
                    "end": 366,
                    "matchedPaperCorpusId": "237485538"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.346435546875
        },
        {
            "corpus_id": "262828449",
            "title": "Survey of Social Bias in Vision-Language Models",
            "text": "Recently, researchers discover that machine learning (ML) models have the ability to capture and learn from the social biases that can lead to discrimination that exist in the training dataset. Such social bias in ML systems can cause social harms, such as the uneven allocation of resources or opportunities (allocational harm) and the unfavorable representation, failure of recognition, or overlooked demands of particular social groups (representational harm) [15,23]. The International Covenant on Civil and Political Rights (The United Nations General Assembly 1966) encodes that \"the law shall prohibit any discrimination and guarantee to all persons equal and effective protection against discrimination on any ground such as race, color, sex, language, religion, political or other opinion, national or social origin, property, birth or other status.\". Various international bodies and individual governments have also issued AI ethics guidelines that include \"fairness\" as an important criterion for AI and ML models and systems [3,4,60,87,173]. Thus, ensuring fairness in artificial intelligence (AI) systems by mitigating such social biases has become an increasingly important focus in the ML community. \n\nIn the past few years, Natural Language Processing (NLP) and Computer Vision (CV) have experienced huge leaps thanks to transformer-based pre-trained models. As such pre-trained models become widely adopted in each of the fields for research and applications, there is a dire need for fairer and non-biased models. As a response to the urgent call, there are many studies on social bias in NLP and CV, but they remain separate. Moreover, with the advancement of the multimodal field, especially the Diffusion Model in vision-and-language (VL), such fairness investigations in pre-trained VL models are also conducted. These VL models are indeed susceptible to social bias, but limited understanding has been shared compared to in-depth discussions of social bias in NLP or CV. This survey provides researchers with a high-level insight derived from the similarities and differences in social bias studies across NLP, CV, and VL. It aims to benefit the community by providing guidelines for addressing social bias in both unimodal and multimodal settings. \n\nOrganization of this Survey. The remainder of this survey is organized as follows.",
            "score": 0.5843784003221196,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1215
                },
                {
                    "start": 1218,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1532
                },
                {
                    "start": 1533,
                    "end": 1645
                },
                {
                    "start": 1646,
                    "end": 1835
                },
                {
                    "start": 1836,
                    "end": 1994
                },
                {
                    "start": 1995,
                    "end": 2146
                },
                {
                    "start": 2147,
                    "end": 2272
                },
                {
                    "start": 2275,
                    "end": 2303
                },
                {
                    "start": 2304,
                    "end": 2357
                }
            ],
            "ref_mentions": [
                {
                    "start": 467,
                    "end": 470,
                    "matchedPaperCorpusId": "218971825"
                },
                {
                    "start": 1043,
                    "end": 1046,
                    "matchedPaperCorpusId": "242024395"
                },
                {
                    "start": 1046,
                    "end": 1049,
                    "matchedPaperCorpusId": "201827642"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7783203125
        },
        {
            "corpus_id": "274130807",
            "title": "Bias in Large Language Models: Origin, Evaluation, and Mitigation",
            "text": "Extrinsic bias refers to disparities in a model's performance across different downstream tasks, also known as downstream bias or prediction bias (Doan et al., 2024;Gallegos et al., 2024;Li et al., 2023). This type of bias emerges when a model's effectiveness varies among tasks or demographic groups, potentially leading to unequal outcomes in practical applications. \n\nDepending on the downstream tasks, extrinsic bias manifests differently. We categorize these tasks into two main groups: natural language understanding (NLU) tasks and natural language generation (NLG) tasks. In NLU tasks, extrinsic bias affects how the model comprehends and interprets input text. In NLG tasks, it can result in the generation of biased or stereotypical language.",
            "score": 0.5843058606309434,
            "section_title": "Extrinsic Bias",
            "char_start_offset": 11375,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 368
                },
                {
                    "start": 371,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 752
                }
            ],
            "ref_mentions": [
                {
                    "start": 165,
                    "end": 187,
                    "matchedPaperCorpusId": "261530629"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8916015625
        },
        {
            "corpus_id": "270619502",
            "title": "Evaluating Short-Term Temporal Fluctuations of Social Biases in Social Media Data and Masked Language Models",
            "text": "Independently of any downstream tasks, intrinsic bias evaluation measures (Nangia et al., 2020;Nadeem et al., 2021;Kaneko and Bollegala, 2022) assess social biases in MLMs on a standalone basis.Nevertheless, considering that MLMs serve to represent input texts across various downstream tasks, several prior studies have suggested that the evaluation of social biases should be conducted in relation to those spe-cific tasks (De-Arteaga et al., 2019;Webster et al., 2020).Kaneko and Bollegala (2021) demonstrated that there is only a weak correlation between intrinsic and extrinsic social bias evaluation measures.In this paper, we use AULA which is an intrinsic measure for evaluating social biases in MLMs.\n\nVarious debiasing methods have been proposed to mitigate social biases in MLMs.Zhao et al. (2019) proposed a debiasing method by swapping the gender of female and male words in the training data.Webster et al. (2020) showed that dropout regularisation can reduce overfitting to gender information, thereby can be used for debiasing pretrained language models.Kaneko and Bollegala (2021) proposed a method for debiasing by orthogonalising the vectors representing gender information with the hidden layer of a language model given a sentence containing a stereotypical word.Our focus in this paper is the evaluation of social biases rather than proposing bias mitigation methods.\n\nTemporal Variations in MLMs.Diachronic Language Models that capture the meanings of words at a specific timestamp have been trained using historical corpora (Qiu and Xu, 2022;Loureiro et al., 2022a).Rosin and Radinsky (2022) introduced a temporal attention mechanism by extending the self-attention mechanism in transformers.They took into account the time stamps of the documents when calculating the attention scores.Tang et al. (2023b) proposed an unsupervised method to learn dynamic contextualised word embeddings via timeadapting a pretrained MLM using prompts from manual and automatic templates.",
            "score": 0.5842970595201223,
            "section_title": "Related Work",
            "char_start_offset": 4794,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 194,
                    "end": 472
                },
                {
                    "start": 472,
                    "end": 615
                },
                {
                    "start": 615,
                    "end": 709
                },
                {
                    "start": 711,
                    "end": 790
                },
                {
                    "start": 790,
                    "end": 906
                },
                {
                    "start": 906,
                    "end": 1070
                },
                {
                    "start": 1070,
                    "end": 1284
                },
                {
                    "start": 1284,
                    "end": 1389
                },
                {
                    "start": 1391,
                    "end": 1419
                },
                {
                    "start": 1419,
                    "end": 1590
                },
                {
                    "start": 1590,
                    "end": 1716
                },
                {
                    "start": 1716,
                    "end": 1810
                },
                {
                    "start": 1810,
                    "end": 1994
                }
            ],
            "ref_mentions": [
                {
                    "start": 74,
                    "end": 95,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 95,
                    "end": 115,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 425,
                    "end": 450,
                    "matchedPaperCorpusId": "58006082"
                },
                {
                    "start": 790,
                    "end": 808,
                    "matchedPaperCorpusId": "102352962"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.359130859375
        },
        {
            "corpus_id": "259370686",
            "title": "A Comparative Study on the Impact of Model Compression Techniques on Fairness in Language Models",
            "text": "Despite their increasing popularity, machine learning models have been known to exhibit biases in their outputs, present privacy risks, and have potentially negative environmental consequences from their training and deployment. (Bender et al., 2021;Talat et al., 2022). Language models suffer from biases that result in unequal resource distributions (allocational harms), in addition to the undesired tendency to reproduce biases and stereotypes in content that is reflective of hegemonic worldviews (representational harms). Although measures have been proposed in tasks such as text classification (Czarnowska et al., 2021) to investigate the disparate allocational treatment of different classes, much of the research on fairness in language models centers on addressing representational harms * Equal contribution (Blodgett et al., 2020). The potential of these models to further stigmatize marginalized communities is demonstrated in (Dressel and Farid, 2018), which illustrates how recidivism prediction systems are biased against black defendants, who have a higher baseline risk for repeat offences. Biases are also prevalent in computer vision applications such as facial recognition technologies. Within NLP, (Bolukbasi et al., 2016), one of the first forays that studied this phenomenon in language, noted that word embeddings contained stereotypical associations with respect to gender. Language models can exhibit biases toward different dialects for tasks like toxicity and hate speech detection (Garg et al., 2022;Sap et al., 2019), generate stereotypical representations and narratives (Lucy and Bamman, 2021), and are capable of the outright erasure of underrepresented identities (Dev et al., 2021). Compressed models that are biased may have detrimental consequences in the real world, as they are typically deployed on edge devices, which can further disadvantage communities without access to other forms of technology. Consequently, these issues have compelled a shift towards developing more inclusive systems. Hooker et al. (2020) demonstrates how compression techniques, when applied to models that deal with tabular data, lead to the disparate treatment of less-represented classes. However, equivalent studies in NLP (Tal et al., 2022;Ahn et al., 2022;Silva et al., 2021) do not",
            "score": 0.5842707816732489,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 250,
                    "end": 269,
                    "matchedPaperCorpusId": "247626152"
                },
                {
                    "start": 1531,
                    "end": 1548,
                    "matchedPaperCorpusId": "196211238"
                },
                {
                    "start": 1604,
                    "end": 1627,
                    "matchedPaperCorpusId": "235097208"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.39013671875
        },
        {
            "corpus_id": "258686693",
            "title": "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models",
            "text": ", and subsequently on downstream tasks, and how it affects the fairness towards diverse social groups. Our study aims to fill this gap.\n\nAs a case study, we focus on the effects of media biases in pretraining data on the fairness of hate speech detection with respect to diverse social attributes, such as gender, race, ethnicity, religion, and sexual orientation, and of misinformation detection with respect to partisan leanings. We investigate how media biases in the pretraining data propagate into LMs and ultimately affect downstream tasks, because discussions about polarizing social and economic issues are abundant in pretraining data sourced from news, forums, books, and online encyclopedias, and this language inevitably perpetuates social stereotypes. We choose hate speech and misinformation classification because these are social-oriented tasks in which unfair predictions can be especially harmful (Duggan, 2017;League, 2019League, , 2021.\n\nTo this end, grounded in political spectrum theories (Eysenck, 1957;Rokeach, 1973;Gindler, 2021) and the political compass test, 2 we propose to empirically quantify the political leaning of pretrained LMs ( \u00a72). We then further pretrain language models on different partisan corpora to investigate whether LMs pick up political biases from training data. Finally, we train classifiers on top of LMs with varying political leanings and evaluate their performance on hate speech instances targeting different identity groups (Yoder et al., 2022), and on misinformation detection with different agendas (Wang, 2017). In this way, we investigate the propagation of political bias through the entire pipeline from pretraining data to language models to downstream tasks.\n\nOur experiments across several data domains, partisan news datasets, and LM architectures ( \u00a73) demonstrate that different pretrained LMs do have different underlying political leanings, reinforcing the political polarization present in pretraining corpora ( \u00a74.1). Further, while the overall performance of hate speech and misinformation detectors remains consistent across such politically-biased LMs, these models exhibit significantly different behaviors against different identity groups and partisan media sources. ( \u00a74.2).\n\nThe main contributions of this paper are novel methods to quantify political biases in",
            "score": 0.5836198790929101,
            "section_title": "Introduction",
            "char_start_offset": 1762,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1559,
                    "end": 1571,
                    "matchedPaperCorpusId": "10326133"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67431640625
        },
        {
            "corpus_id": "270095451",
            "title": "Understanding Intrinsic Socioeconomic Biases in Large Language Models",
            "text": "Furthermore, we assess the dynamics between these biases by exploring intersectionality among gender, race, and marital status.Moreover, we evaluate socioeconomic bias of LLMs towards names which is an identification of the individuals that is used in most of the application such as credit scoring.In particular, we highlight the influence of these factors on the modification of economic class perception related to these groups through the intrinsic token prediction evaluation of LLMs.More precisely, our contributions include: 1. Creating a novel evaluation dataset of 1M English sentences with socioeconomic context prompts. 1 2. Assessing the intrinsic socioeconomic biases in four different LLMs: Falcon, Llama 2, GPT-2 and BERT with respect to four sensitive domains.3. Evaluating the impact of intersectionality of gender, race and marital status on socioeconomic biases in LLMs. 4. Assessing the capacity of LLMs to extract race and gender information from names, as well as their socioeconomic biases associated with these names.\n\nand Bollegala 2022) refers to biases inherent in the embedding or representation of language models.This form of bias is evaluated without fine-tuning the models on a specific task or dataset.In contrast, extrinsic bias involves assessing the fairness of outputs the system for the downstream tasks.This assessment helps to determine the overall fairness of combined system components, e.g., evaluating fairness in career prediction using individuals' biographies (Webster et al. 2020;Zhao et al. 2020).Shen et al. (2022) demonstrated that including names in the queries for a language-based recommender system named LMRec, which suggests restaurants, can shift the price levels of the recommended restaurants.This shift occurs despite the absence of names during the system's training phase.However, they did not identify which component of the CRS is responsible for this bias.In our work, we demonstrate how Large Language Models (LLMs) are capable of extracting race and gender from names-a finding supported by other studies (Haim, Salinas, and Nyarko 2024; Meltzer, Lambourne, and Grandi 2024)-and exhibit biases toward names and various demographic groups.",
            "score": 0.5834843902859793,
            "section_title": "Introduction",
            "char_start_offset": 1985,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 127,
                    "end": 299
                },
                {
                    "start": 299,
                    "end": 489
                },
                {
                    "start": 489,
                    "end": 776
                },
                {
                    "start": 776,
                    "end": 1041
                },
                {
                    "start": 1043,
                    "end": 1143
                },
                {
                    "start": 1143,
                    "end": 1235
                },
                {
                    "start": 1235,
                    "end": 1342
                },
                {
                    "start": 1342,
                    "end": 1546
                },
                {
                    "start": 1546,
                    "end": 1753
                },
                {
                    "start": 1753,
                    "end": 1835
                },
                {
                    "start": 1835,
                    "end": 1922
                },
                {
                    "start": 1922,
                    "end": 2206
                }
            ],
            "ref_mentions": [
                {
                    "start": 1047,
                    "end": 1062,
                    "matchedPaperCorpusId": "233241161"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59814453125
        },
        {
            "corpus_id": "248366292",
            "title": "Towards an Enhanced Understanding of Bias in Pre-trained Neural Language Models: A Survey with Special Emphasis on Affective Bias",
            "text": "Natural Language Processing (NLP) has recently achieved rapid progress with the aid of deep learning, especially Pre-trained Language Models (PLM) [50]. \n\nLarge PLMs like BERT [31], GPT [88], etc., are highly efficient at capturing linguistic properties and producing representations of text with semantic and contextual information. Inclusion of contextual representations has led large PLMs to become popular towards addressing many downstream tasks such as Question Answering, Sentiment Analysis, Neural Machine Translation, etc [87]. These data greedy Language Models (LM) are generally trained on large-scale human generated textual corpora. However, since ancient days, language has functioned as a channel to express and propagate unfairness towards marginalized social groups and assign power to oppressive institutions [29]. It is often very hard to analyze the quality of data in large corpora in context of such oppressive nature of language [118]. Yet, these human generated textual corpora can carry plenty of harmful linguistic biases and social stereotypes that can lead NLP algorithms to produce unfair discrimination towards socially marginalized populations when deployed in real-word [78]. A threatening scenario that was identified with the use of large PLM GPT-3 [19] has been experimentally demonstrated in [2], for example, 'Two Muslims walked into a ', is completed by GPT3 with 'synagogue with axes and a bomb' and 'gay bar in Seattle and started shooting at will, killing five people'. This is evidently discriminatory and is probably due to islamophobia manifesting in the training text.",
            "score": 0.5833829744175005,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 155,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1511
                },
                {
                    "start": 1512,
                    "end": 1614
                }
            ],
            "ref_mentions": [
                {
                    "start": 176,
                    "end": 180,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 186,
                    "end": 190,
                    "matchedPaperCorpusId": "49313245"
                },
                {
                    "start": 532,
                    "end": 536,
                    "matchedPaperCorpusId": "212747830"
                },
                {
                    "start": 828,
                    "end": 832,
                    "matchedPaperCorpusId": "210444739"
                },
                {
                    "start": 1284,
                    "end": 1288,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.587890625
        },
        {
            "corpus_id": "258378241",
            "title": "When Do Pre-Training Biases Propagate to Downstream Tasks? A Case Study in Text Summarization",
            "text": "Large language models (LLMs) are subject to sociocultural and other biases previously identified using intrinsic evaluations. However, when and how these intrinsic biases in pre-trained LM representations propagate to downstream, fine-tuned NLP tasks like summarization is not well understood. In this work, we investigate one type of bias\u2014name-nationality bias\u2014and trace it from the pre-training stage to a downstream summarization task across multiple summarization modeling choices. We show that these biases manifest themselves as hallucinations in summarization, leading to factually incorrect summaries. We also find that this propagation of biases is algorithm-dependent: more abstractive models allow biases to propagate more directly to downstream tasks as hallucinated facts. Building on these observations, we further analyze how changes to the adaptation method and fine-tuning data set affect name nationality biases and show that while they can reduce the overall rate of hallucinations, they do not change the types of biases that do appear.",
            "score": 0.5832826026616447,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64990234375
        },
        {
            "corpus_id": "268709624",
            "title": "Biases Mitigation and Expressiveness Preservation in Language Models: A Comprehensive Pipeline (Student Abstract)",
            "text": "Pre-trained Language Models (PLMs) excel in diverse natural language tasks due to their training on extensive data. However, prior studies have revealed that PLMs inadvertently encode and propagate social biases from their unfiltered pre-training data. Take gender bias as an example: the PLM is more inclined towards associating male (female) attributes with programmers (nurses). Several solutions for mitigating the social biases have been proposed, including: (1) Post-hoc-based method add a post-training step to these sentence representations before applied to downstream tasks, including removing the estimated genderdirection subspace from sentence representation (Liang et al. 2020), or use pre-defined word tuples combine specific techniques to debias text encoder for a fair sentence representation (Cheng et al. 2021). (2) Fine-tuning-based models use specific loss terms to guide a PLM to remove biases, including distribution alignment loss for debiasing embedding space (Guo, Yang, and Abbasi 2022); orthogonal loss aims to promote irrelevance between stereotyped words and gender-specific words (Kaneko and Bollegala 2021), etc. Current debiasing methods for PLMs have shown promise but grapple with notable challenges: (1) demanding timeconsuming to fine-tune entire parameters in PLMs; (2) disregarding the expressiveness of PLMs, which could potentially disrupt PLM's computational structure and undermine the benefits of pre-training; (3) reintroducing biases from downstream tasks into PLMs when applying debiased models to those tasks. Hence, we present a new two-stage pipeline that aims to simultaneously preserve the PLMs' expressiveness and mitigate biases from both internal and downstream contexts. As shown in Figure 1, in first stage, we keep PLM's parameter frozen, and only train the continuous prefix to reduce the magnitude of trainable parameters, towards mitigating internal bias and meanwhile preserving expressiveness. In second stage, we perform causal interventions on different demographic groups to eliminate the biases from downstream contexts.",
            "score": 0.5830943076277133,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1726
                },
                {
                    "start": 1727,
                    "end": 1956
                },
                {
                    "start": 1957,
                    "end": 2087
                }
            ],
            "ref_mentions": [
                {
                    "start": 672,
                    "end": 691,
                    "matchedPaperCorpusId": "207996257"
                },
                {
                    "start": 810,
                    "end": 829,
                    "matchedPaperCorpusId": "232185104"
                },
                {
                    "start": 985,
                    "end": 1013,
                    "matchedPaperCorpusId": "248780440"
                },
                {
                    "start": 1111,
                    "end": 1138,
                    "matchedPaperCorpusId": "231698657"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71630859375
        },
        {
            "corpus_id": "250390988",
            "title": "An Empirical Study on Pseudo-log-likelihood Bias Measures for Masked Language Models Using Paraphrased Sentences",
            "text": "In recent years, pretrained transformer-based language models, from BERT (Devlin et al., 2019) to PaLM (Chowdhery et al., 2022), have shown remarkable results in many downstream natural languages processing (NLP) tasks such as question answering, natural language inference, reading comprehension, and text classification as demonstrated by many benchmarks. Nevertheless, there is a growing concern if such language models contain social biases such as stereotyping negative generalizations of different social groups and communities, which might have been present in their training corpora (Liang et al., 2021;Garrido-Mu\u00f1oz et al., 2021). \n\nA cognitive bias, stereotyping, is defined as the assumption of some characteristics are applied to communities on the basis of their nationality, ethnicity, gender, religion, etc (Schneider, 2005). Relatedly, Fairness (\"zero-bias\"), in the context of NLP and machine learning is defined as preventing harmful, discriminatory decisions according to such unwanted, stereotypical characteristics (Garrido-Mu\u00f1oz et al., 2021). \n\nThere are benchmarks and metrics (Nadeem et al., 2021;Nangia et al., 2020;May et al., 2019;Rudinger et al., 2018;Zhao et al., 2018;Kurita et al.) defined for auditing and measuring biases in language models. In this paper, we focus on the CrowS-Pairs dataset (Nangia et al., 2020) which contains pairs of free-form contrastive sentences where one is a stereotypical sentence which reflects a social bias towards a disadvantaged group and the other with a perturbation of the same with the advantaged group. It attempts to measure if the language model prefers or likely to produce more stereotypical sentence by calculating pseudolog-likelihood Masked Language Model (MLM) scoring (Salazar et al., 2020). Nevertheless, some analysis such as (Blodgett et al., 2021) questions the extent to which such measures exactly capture the bias of a model. In this paper, we conduct an empirical study to investigate the robustness of the log-likelihoodbased bias measure by paraphrasing the test sentences as in Figure 1 and analysing if they produce consistent results.",
            "score": 0.5826770298478883,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 639
                },
                {
                    "start": 642,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 1065
                },
                {
                    "start": 1068,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1772
                },
                {
                    "start": 1773,
                    "end": 1913
                },
                {
                    "start": 1914,
                    "end": 2128
                }
            ],
            "ref_mentions": [
                {
                    "start": 73,
                    "end": 94,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 591,
                    "end": 611,
                    "matchedPaperCorpusId": "235623756"
                },
                {
                    "start": 611,
                    "end": 638,
                    "matchedPaperCorpusId": "233832881"
                },
                {
                    "start": 1036,
                    "end": 1064,
                    "matchedPaperCorpusId": "233832881"
                },
                {
                    "start": 1101,
                    "end": 1122,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 1122,
                    "end": 1142,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 1142,
                    "end": 1159,
                    "matchedPaperCorpusId": "85518027"
                },
                {
                    "start": 1159,
                    "end": 1181,
                    "matchedPaperCorpusId": "13756572"
                },
                {
                    "start": 1181,
                    "end": 1199,
                    "matchedPaperCorpusId": "4952494"
                },
                {
                    "start": 1199,
                    "end": 1213,
                    "matchedPaperCorpusId": "190000105"
                },
                {
                    "start": 1327,
                    "end": 1348,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 1749,
                    "end": 1771,
                    "matchedPaperCorpusId": "218628872"
                },
                {
                    "start": 1809,
                    "end": 1832,
                    "matchedPaperCorpusId": "236460302"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.45458984375
        },
        {
            "corpus_id": "267312114",
            "title": "UnMASKed: Quantifying Gender Biases in Masked Language Models through Linguistically Informed Job Market Prompts",
            "text": "In recent years, large language models (LLMs) have emerged as a powerful tool in the field of natural language processing (NLP), demonstrating an unparalleled ability to capture hidden patterns from large datasets (Bommasani et al., 2021;Zhou et al., 2023;Zhao et al., 2023). These models owe their power to the extensive training on corpora of human-generated text, enabling them to mimic human-like linguistic capabilities with remarkable accuracy (Bahri et al., 2021). While the ability to capture and reproduce these patterns often results in beneficial outcomes, it is not without its caveats. An increasing amount of studies (Bordia and Bowman, 2019;Abid et al., 2021;Kaneko et al., 2022) have underscored the potential risks associated with language models, pointing out their role in inheriting the biases present in the training data, a reflection of human prejudices and societal norms. In the context of language models (LMs), bias refers to the systematic misrepresentation of facts or factual distortions that benefit certain groups, spreading and fixing stereotypes, or producing incorrect presuppositions built on learned patterns. These prejudices can be intentionally or unintentionally introduced by (1) training data, (2) algorithms, or (3) human annotators (Ferrara, 2023). If the training datasets are skewed or lack representation from different groups, the model will inevitably inherit these biases. Algorithms follow mathematical and logical rules that make them more robust. However, if parameters are set or weighted in a way that they favor certain data points over others, they can introduce or amplify them. Lastly, human annotators bring their own perspectives and beliefs. This highlights the importance of having diverse teams involved in the data annotation process to minimize the introduction of individual or cultural biases. \n\nGender bias poses ethical concerns, particularly when found in models deployed in sensitive domains, such as the job market, where fairness and impartiality are paramount (Kodiyan, 2019). While previous work has focused on using larger and more complex datasets, the question is: do we need a large corpus to identify whether models show gender-biased behavior?",
            "score": 0.5824954909147091,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1423
                },
                {
                    "start": 1424,
                    "end": 1500
                },
                {
                    "start": 1501,
                    "end": 1637
                },
                {
                    "start": 1638,
                    "end": 1704
                },
                {
                    "start": 1705,
                    "end": 1862
                },
                {
                    "start": 1865,
                    "end": 2052
                },
                {
                    "start": 2053,
                    "end": 2226
                }
            ],
            "ref_mentions": [
                {
                    "start": 450,
                    "end": 470,
                    "matchedPaperCorpusId": "221376926"
                },
                {
                    "start": 631,
                    "end": 656,
                    "matchedPaperCorpusId": "102352788"
                },
                {
                    "start": 656,
                    "end": 674,
                    "matchedPaperCorpusId": "231603388"
                },
                {
                    "start": 674,
                    "end": 694,
                    "matchedPaperCorpusId": "248496913"
                },
                {
                    "start": 1277,
                    "end": 1292,
                    "matchedPaperCorpusId": "258041203"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65185546875
        },
        {
            "corpus_id": "265281304",
            "title": "Bias A-head? Analyzing Bias in Transformer-Based Language Model Attention Heads",
            "text": "A growing body of work exploring AI fairness in general, and bias in NLP systems in particular, has highlighted stereotyping embedded in state-of-theart large language models -that is, such models represent some social groups disparately on demographic subsets, including gender, race, and age (Bender et al., 2021;Shah et al., 2020;Guo and Caliskan, 2021;Hutchinson et al., 2020;Kurita et al., 2019;May et al., 2019;Tan and Celis, 2019;Wolfe and Caliskan, 2021;Rozado, 2023) Caliskan et al., 2017), which examines the associations in contextualized word embeddings between concepts captured in the Implicit Association Test (Greenwald et al., 1998). While the SEAT score provides a quantifiable score to evaluate the stereotyping in PLMs, it is unknown how such stereotypical associations manifest in PLMs. \n\nTo mitigate stereotyping and representational harms in PLMs, many different debiasing strategies have been proposed, including data augmentation (Garimella et al., 2021), post-hoc operations (Cheng et al., 2021;Liang et al., 2020), fine-tuning the model (Kaneko and Bollegala, 2021;Lauscher et al., 2021), prompting techniques (Guo et al., 2022), and Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022). However, recent literature has noted several critical weaknesses of existing bias mitigation approaches, including the effectiveness of bias mitigation (Gonen and Goldberg, 2019;Meade et al., 2022), high training cost (Kaneko and Bollegala, 2021;Lauscher et al., 2021), poor generalizability (Garimella et al., 2021), and the inevitable degradation of language modeling capability (He et al., 2022;Meade et al., 2022). We believe that progress in addressing PLM bias has been inhibited by a lack of deeper understanding of how the bias manifests/behaves internally in the PLM. This paper aims to offer a perspective on this research gap.",
            "score": 0.5822595135830733,
            "section_title": "Stereotyping and Representational Harms in PLMs",
            "char_start_offset": 7260,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 807
                },
                {
                    "start": 810,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1810
                },
                {
                    "start": 1811,
                    "end": 1871
                }
            ],
            "ref_mentions": [
                {
                    "start": 294,
                    "end": 315,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 315,
                    "end": 333,
                    "matchedPaperCorpusId": "209461005"
                },
                {
                    "start": 333,
                    "end": 356,
                    "matchedPaperCorpusId": "219530686"
                },
                {
                    "start": 356,
                    "end": 380,
                    "matchedPaperCorpusId": "218487466"
                },
                {
                    "start": 380,
                    "end": 400,
                    "matchedPaperCorpusId": "190000105"
                },
                {
                    "start": 400,
                    "end": 417,
                    "matchedPaperCorpusId": "85518027"
                },
                {
                    "start": 417,
                    "end": 437,
                    "matchedPaperCorpusId": "202781363"
                },
                {
                    "start": 437,
                    "end": 462,
                    "matchedPaperCorpusId": "238259136"
                },
                {
                    "start": 462,
                    "end": 475,
                    "matchedPaperCorpusId": "257342280"
                },
                {
                    "start": 476,
                    "end": 498,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 625,
                    "end": 649,
                    "matchedPaperCorpusId": "7840819"
                },
                {
                    "start": 955,
                    "end": 979,
                    "matchedPaperCorpusId": "236477795"
                },
                {
                    "start": 1021,
                    "end": 1040,
                    "matchedPaperCorpusId": "207996257"
                },
                {
                    "start": 1064,
                    "end": 1092,
                    "matchedPaperCorpusId": "231698657"
                },
                {
                    "start": 1092,
                    "end": 1114,
                    "matchedPaperCorpusId": "237440429"
                },
                {
                    "start": 1137,
                    "end": 1155,
                    "matchedPaperCorpusId": "248780440"
                },
                {
                    "start": 1211,
                    "end": 1232,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1386,
                    "end": 1412,
                    "matchedPaperCorpusId": "73729169"
                },
                {
                    "start": 1412,
                    "end": 1431,
                    "matchedPaperCorpusId": "239015827"
                },
                {
                    "start": 1452,
                    "end": 1480,
                    "matchedPaperCorpusId": "231698657"
                },
                {
                    "start": 1480,
                    "end": 1502,
                    "matchedPaperCorpusId": "237440429"
                },
                {
                    "start": 1526,
                    "end": 1550,
                    "matchedPaperCorpusId": "236477795"
                },
                {
                    "start": 1615,
                    "end": 1632,
                    "matchedPaperCorpusId": "252907344"
                },
                {
                    "start": 1632,
                    "end": 1651,
                    "matchedPaperCorpusId": "239015827"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.44482421875
        },
        {
            "corpus_id": "267372092",
            "title": "Shortcut Learning Explanations for Deep Natural Language Processing: A Survey on Dataset Biases",
            "text": "Recent studies have brought to light significant biases in Natural Language Processing (NLP), showcasing potential harm. However, a common shortcoming in many of these studies is a lack of critical engagement with the fundamental definition and understanding of the ''bias'' [12]. The term is frequently employed broadly, encompassing various system behaviors in NLP, such as gender and racial bias. Even when scrutinizing bias in NLP systems designed for a common objective, diverse research endeavors may exhibit distinct understandings and conceptualizations of what constitutes bias. For instance, the behaviors of systems, such as deeming the statement ''You are a good woman'' as sexist when trained on a particular dataset [13]. Additionally, reliance on the Equity Evaluation Corpus has uncovered instances where certain methods consistently predict sentiment intensity levels slightly higher for specific races or genders [14]. This diversity in biases underscores the complexity of addressing bias in NLP systems and highlights the necessity for nuanced considerations in research and mitigation efforts. \n\nFrom the examples, it is clear that the performance and behavior of NLP models are significantly influenced by the datasets that are used, as both researchers and practitioners have come to understand over time [15]. The ability of the model to generalize, identify patterns, and make reliable predictions can be significantly impacted by the quality, size, diversity, and representativeness of the data. The model may pick up on and reinforce biases present in the training data, producing biased results or reaffirming preexisting societal biases. The history of dataset development in the fields of data science and machine learning can be seen as a depiction of resistance against perceived unfairness and bias [16]. \n\nEach successive dataset has developed in response to the biases and constraints present in preceding datasets, to highlight the context more thoroughly and objectively. On the one hand, this change in the way datasets are being developed may be a sign of advancement. On the other side, a slight vicious cycle was also apparent. We as a community consistently reject the present datasets because we believe they are biased. Yet each time we construct a new dataset, it turns out to be biased in the same way, although in a little different way.",
            "score": 0.5820378915601434,
            "section_title": "II. BACKGROUND: ORIGIN OF DATASET BIASES",
            "char_start_offset": 6846,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1114
                },
                {
                    "start": 1117,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1521
                },
                {
                    "start": 1522,
                    "end": 1666
                },
                {
                    "start": 1667,
                    "end": 1837
                },
                {
                    "start": 1840,
                    "end": 2008
                },
                {
                    "start": 2009,
                    "end": 2107
                },
                {
                    "start": 2108,
                    "end": 2168
                },
                {
                    "start": 2169,
                    "end": 2263
                },
                {
                    "start": 2264,
                    "end": 2384
                }
            ],
            "ref_mentions": [
                {
                    "start": 1328,
                    "end": 1332,
                    "matchedPaperCorpusId": "251800110"
                },
                {
                    "start": 1832,
                    "end": 1836,
                    "matchedPaperCorpusId": "49561627"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65283203125
        },
        {
            "corpus_id": "258615799",
            "title": "Estimating the Personality of White-Box Language Models",
            "text": "Recent works have explored multiple societal biases that are learned by language models, which may sometimes be at odds with the prevailing societal values. Bolukbasi et al. (2016) demonstrated quantitatively that word embeddings contain biases in their geometry that reflect gender stereotypes present in the broader society. Sheng et al. (2019) performed an experiment analyzing different textual contexts where biases can occur for different demographics in text samples generated using the state-of-the-art language models. Bordia & Bowman (2019) evaluated the magnitude of gender bias in word-level language models that are trained on WikiText-2 and CNN/Daily Mail datasets. Nadeem et al. (2020) evaluated popular models such as BERT (Devlin et al., 2018), GPT-2 (Radford et al., 2019) and XLNet (Yang et al., 2019) using StereoSet dataset and show that these models exhibit strong stereotypical biases in the following four domains: gender, profession, race, and religion. We note that our work is complementary to all these studies in the following sense. While we aim to understand human tendencies captured by language models similar to these prior studies, our narrow but well-defined focus on characterizing the learned personality traits and potentially altering them is a differentiating factor, as well as a first of its kind.",
            "score": 0.5817991952971578,
            "section_title": "Study of Biases in Language Modeling",
            "char_start_offset": 10982,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1340
                }
            ],
            "ref_mentions": [
                {
                    "start": 157,
                    "end": 180,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 801,
                    "end": 820,
                    "matchedPaperCorpusId": "195069387"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78271484375
        },
        {
            "corpus_id": "274423106",
            "title": "How far can bias go? - Tracing bias from pretraining data to alignment",
            "text": "The impressive performance of Large Language Models (LLMs) across Natural Language Processing (NLP) tasks, ranging from question-answering to news summarization, has led to their widespread adoption. They have become essential components of applications such as chatbots designed to simulate human conversation (Ferrara, 2023). \n\nHowever, despite their appeal, LLMs have faced criticism for perpetuating and amplifying societal biases (Bommasani et al., 2021;Weidinger et al., 2021). They are believed to reflect and reinforce the biases present in the vast, uncurated corpora used for their training (Bender et al., 2021). These biases can lead to discriminatory and harmful outcomes, particularly for marginalized groups (Spol-  sky, 1998;Noble, 2018). Documented instances include biased resource allocation based on ethnicity (Jackson and Mendoza, 2020;Obermeyer et al., 2019), job discrimination (Kassir et al., 2023;Armstrong et al., 2024), and reinforcement of harmful stereotypes related to gender (Dastin, 2022;Chen, 2023;Lambrecht and Tucker, 2018). \n\nResearch on bias in NLP and LLMs has focused on intrinsic bias in model representations (Bolukbasi et al., 2016;Caliskan et al., 2017;Garg et al., 2018;Gupta et al., 2024) or at the output level (i.a., Schick et al., 2021;Leidinger and Rogers, 2024), often overlooking the impact of pretraining data on model outputs for specific tasks. Recent studies (K\u00f6ksal et al., 2023;Touvron et al., 2023;Orgad and Belinkov, 2022) have explored the connection between training data bias and model bias, but have been hampered by restricted access to training data for commercial LLMs (Solaiman, 2023). Most bias research is constrained to public datasets like CommonCrawl, Wikipedia (Schwenk et al., 2021), and mC4 (Xue et al., 2021).",
            "score": 0.5815132285055781,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 327
                },
                {
                    "start": 330,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 1059
                },
                {
                    "start": 1062,
                    "end": 1398
                },
                {
                    "start": 1399,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1785
                }
            ],
            "ref_mentions": [
                {
                    "start": 601,
                    "end": 622,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 741,
                    "end": 753,
                    "matchedPaperCorpusId": "150449904"
                },
                {
                    "start": 830,
                    "end": 857,
                    "matchedPaperCorpusId": "219137989"
                },
                {
                    "start": 857,
                    "end": 880,
                    "matchedPaperCorpusId": "204881868"
                },
                {
                    "start": 901,
                    "end": 922,
                    "matchedPaperCorpusId": "258702749"
                },
                {
                    "start": 1214,
                    "end": 1233,
                    "matchedPaperCorpusId": "259164882"
                },
                {
                    "start": 1264,
                    "end": 1284,
                    "matchedPaperCorpusId": "232075876"
                },
                {
                    "start": 1414,
                    "end": 1435,
                    "matchedPaperCorpusId": "258832603"
                },
                {
                    "start": 1456,
                    "end": 1481,
                    "matchedPaperCorpusId": "250390436"
                },
                {
                    "start": 1635,
                    "end": 1651,
                    "matchedPaperCorpusId": "256697103"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.417236328125
        },
        {
            "corpus_id": "258823009",
            "title": "Comparing Biases and the Impact of Multilingual Training across Multiple Languages",
            "text": "The growth of interest in natural language processing (NLP) has led to investigations of the various social biases learned by models. While researchers are actively studying bias and fairness in NLP models, they typically focus on a single language, primarily English (Bolukbasi et al., 2016;Hutchinson et al., 2020;Nadeem et al., 2021;Nangia et al., 2020). However, biases can manifest differently across languages (e.g., Table 1) due to differences *Work conducted during an internship at Amazon. 1 Bias templates for all languages will be publicly released.",
            "score": 0.5808734423334851,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 560
                }
            ],
            "ref_mentions": [
                {
                    "start": 268,
                    "end": 292,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 292,
                    "end": 316,
                    "matchedPaperCorpusId": "218487466"
                },
                {
                    "start": 316,
                    "end": 336,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 336,
                    "end": 356,
                    "matchedPaperCorpusId": "222090785"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.45849609375
        },
        {
            "corpus_id": "258832430",
            "title": "Learning Horn Envelopes via Queries from Large Language Models",
            "text": "Machine learned models can contain various types of biases that can stem from the training data [21]. These can lead to numerous undesired effects during deployment [11,9]. This also applies to pre-trained language models where biases can be introduced by the datasets used during training or while tuning a downstream classifier. A lot of work has been done to explore existing biases in pre-trained language models. For example, pre-training the BERT [15] language model on a medical corpus has shown to propagate harmful correlations between genders, ethnicity, and insurance groups [32]. Language models have also been shown to contain biases against persons with disabilities [22].\n\nMost work on detecting gender bias from pre-trained language models have focused on probing them using template-based approaches. Such templates are usually formed of sentences combining a predefined set of predicates and verb or noun phrases. To illustrate this, consider the template \"[predicate] works as [description]\". Predicate here can be pronouns or gendered-nouns, while the description could be anything from nouns referring to occupations, to adjectives referring to sentiment, emotions, or attributes [33,31,10,13].\n\nSome of the works using template-based approaches to investigate gender bias in correlation with occupations are those building on the Winograd Schemas [25]. Winograd is a dataset of templates manually annotated. It is used to assess the presence of biases in co-reference resolution systems. The biases are measured based on the dependency of the system on gendered pronouns along stereotypical and non-stereotypical gender associations with occupations. Also, the WinoBias dataset [37] has been developed to investigate the existing stereotypes in models by exploring the relationship between gendered pronouns and stereotypical occupations. In addition to these, the WinoGender dataset [30] was introduced to also include gender-neutral pronouns, while focusing on the same task of exploring correlations between pronouns, persons, and occupations. For occupational biases in pre-trained language models, some works have explored the correlations between genders and occupations from a descriptive point of view using census data [36], while others have used the pre-trained language models' ability to complete templates",
            "score": 0.5806083217113178,
            "section_title": "Probing Neural Networks",
            "char_start_offset": 6152,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 96,
                    "end": 100,
                    "matchedPaperCorpusId": "237298625"
                },
                {
                    "start": 165,
                    "end": 169,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 169,
                    "end": 171,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 453,
                    "end": 457,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 681,
                    "end": 685,
                    "matchedPaperCorpusId": "218487466"
                },
                {
                    "start": 1205,
                    "end": 1208,
                    "matchedPaperCorpusId": "222290842"
                },
                {
                    "start": 1208,
                    "end": 1211,
                    "matchedPaperCorpusId": "195584301"
                },
                {
                    "start": 1211,
                    "end": 1214,
                    "matchedPaperCorpusId": "167217680"
                },
                {
                    "start": 1369,
                    "end": 1373,
                    "matchedPaperCorpusId": "15710851"
                },
                {
                    "start": 1700,
                    "end": 1704,
                    "matchedPaperCorpusId": "4952494"
                },
                {
                    "start": 1906,
                    "end": 1910,
                    "matchedPaperCorpusId": "13756572"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63623046875
        },
        {
            "corpus_id": "266972664",
            "title": "Undesirable Biases in NLP: Addressing Challenges of Measurement",
            "text": "In recent years various examples of NLP applications were brought to the public attention that behaved in ways that are harmful for certain individuals or groups: Systems for matching vacancies may unintentionally disadvantage ethnic minorities or people with disabilities (Hutchinson et al., 2020), machine translation systems have been found to translate gender-neutral terms to the majority gender, which can amplify existing gender biases (Stanovsky et al., 2019), speech recognition systems have difficulties to correctly recognize the voices of speakers of minority dialects (Zhang et al., 2022b), and, more generally, the biases and misinformation that generative models propagate can distort people's worldviews in unprecedented ways (Kidd & Birhane, 2023). \n\nTo combat these effects of language technology on society, detecting undesirable biases in Large Language Models and other NLP systems, and finding ways to mitigate them, has emerged as a new and active domain of NLP research. However, both detection and mitigation face problems. One of these challenges is that we lack sound tools to measure bias that is present in NLP systems. While there had been a lot of excitement about some early methods used to make bias in such systems visible (e.g., Bolukbasi et al., 2016;Caliskan et al., 2017), more recent work has shown that these (as well as newer) methods are problematic. Many problems have been pointed out for how bias is defined and operationalized (see e.g., Blodgett et al., 2020Blodgett et al., , 2021;;Dev et al., 2022;Ethayarajh et al., 2019;Gonen & Goldberg, 2019;Nissim et al., 2020;Talat et al., 2022). There are also concrete issues with the measurement results. For instance, for some of the currently-used bias measures little to no evidence has been found that they correlate with other bias measures or with downstream harms (e.g., Cao et al., 2022;Delobelle et al., 2022;Goldfarb-Tarrant et al., 2021).",
            "score": 0.5804334734066853,
            "section_title": "Introduction",
            "char_start_offset": 1257,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 765
                },
                {
                    "start": 768,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1634
                },
                {
                    "start": 1635,
                    "end": 1695
                },
                {
                    "start": 1696,
                    "end": 1940
                }
            ],
            "ref_mentions": [
                {
                    "start": 273,
                    "end": 298,
                    "matchedPaperCorpusId": "218487466"
                },
                {
                    "start": 443,
                    "end": 467,
                    "matchedPaperCorpusId": "173991101"
                },
                {
                    "start": 581,
                    "end": 602,
                    "matchedPaperCorpusId": "252346024"
                },
                {
                    "start": 742,
                    "end": 764,
                    "matchedPaperCorpusId": "259223197"
                },
                {
                    "start": 1264,
                    "end": 1287,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 1287,
                    "end": 1309,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 1484,
                    "end": 1505,
                    "matchedPaperCorpusId": "218971825"
                },
                {
                    "start": 1505,
                    "end": 1530,
                    "matchedPaperCorpusId": "236460302"
                },
                {
                    "start": 1530,
                    "end": 1547,
                    "matchedPaperCorpusId": "252907216"
                },
                {
                    "start": 1547,
                    "end": 1571,
                    "matchedPaperCorpusId": "196182395"
                },
                {
                    "start": 1571,
                    "end": 1594,
                    "matchedPaperCorpusId": "73729169"
                },
                {
                    "start": 1594,
                    "end": 1614,
                    "matchedPaperCorpusId": "165163511"
                },
                {
                    "start": 1614,
                    "end": 1633,
                    "matchedPaperCorpusId": "247626152"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.541015625
        },
        {
            "corpus_id": "270357379",
            "title": "Emo-bias: A Large Scale Evaluation of Social Bias on Speech Emotion Recognition",
            "text": "Speech emotion recognition (SER) aims to detect and interpret emotional states conveyed through speech signals.However, SER models may capture and learn social bias, leading to potential social harm.Biased SER systems may exacerbate existing inequalities by disproportionately affecting marginalized communities.For example, if a system is less accurate in recognizing emotions in individuals with disabilities, non-native speakers, or a specific gender, it could further marginalize these groups by denying them equitable access to services or opportunities.\n\nWhile extensive research has addressed bias in various machine learning domains, such as Automatic Speech Recognition (ASR) [1,2,3], Speech Translation [4,5], Facial Emotion Recognition [6], and Automatic Speaker Verification (ASV) [7], limited attention has been paid to social bias within SER systems.For instance, Gorrostieta et al. [8] evaluated gender bias within a specific model and dataset, proposing two adversarial debiasing approaches.However, their analysis was confined to a singular model and only one dataset, potentially limiting its applicability to broader contexts.Similarly, Chien et al. [9] investigated gender-specific emotion perception using the IEMOCAP dataset [10], presenting a perceptual emotion learning framework.Yet, they overlooked the impact of training dataset selection on emotional bias.This underscores the need for comprehensive investigations into gender bias across diverse SER models and datasets to ensure robust and generalizable results.\n\nRecognizing such research gaps, our study delves into two inquiries: Firstly, do contemporary SER models exhibit gender bias?Secondly, what are the primary factors contributing to such bias?Specifically, we investigate whether upstream representations and downstream training data play a crucial role in shaping bias within these models.\n\nLeveraging the cutting-edge advancements in speech selfsupervised learning (SSL) [11,12], we employ 15 SSL models and classical speech features like FBank to train SER systems.Through rigorous and comprehensive experimentation across six diverse emotion datasets, we carefully train and assess a total of 96 SER models.\n\nOur work yields the following contributions: \u2022 We conduct a large-scale evaluation of 15 SER models on six emotion datasets.",
            "score": 0.5799253448951834,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 111,
                    "end": 199
                },
                {
                    "start": 199,
                    "end": 312
                },
                {
                    "start": 312,
                    "end": 559
                },
                {
                    "start": 561,
                    "end": 864
                },
                {
                    "start": 864,
                    "end": 1007
                },
                {
                    "start": 1007,
                    "end": 1145
                },
                {
                    "start": 1145,
                    "end": 1304
                },
                {
                    "start": 1304,
                    "end": 1384
                },
                {
                    "start": 1384,
                    "end": 1542
                },
                {
                    "start": 1544,
                    "end": 1669
                },
                {
                    "start": 1669,
                    "end": 1734
                },
                {
                    "start": 1734,
                    "end": 1881
                },
                {
                    "start": 1883,
                    "end": 2059
                },
                {
                    "start": 2059,
                    "end": 2202
                },
                {
                    "start": 2204,
                    "end": 2328
                }
            ],
            "ref_mentions": [
                {
                    "start": 685,
                    "end": 688,
                    "matchedPaperCorpusId": "253581416"
                },
                {
                    "start": 713,
                    "end": 716,
                    "matchedPaperCorpusId": "247595186"
                },
                {
                    "start": 716,
                    "end": 718,
                    "matchedPaperCorpusId": "235248117"
                },
                {
                    "start": 793,
                    "end": 796,
                    "matchedPaperCorpusId": "222005741"
                },
                {
                    "start": 1169,
                    "end": 1172,
                    "matchedPaperCorpusId": "258542169"
                },
                {
                    "start": 1247,
                    "end": 1251,
                    "matchedPaperCorpusId": "11820063"
                },
                {
                    "start": 1964,
                    "end": 1968,
                    "matchedPaperCorpusId": "248987289"
                },
                {
                    "start": 1968,
                    "end": 1971,
                    "matchedPaperCorpusId": "254154303"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8056640625
        },
        {
            "corpus_id": "102352962",
            "title": "Gender Bias in Contextualized Word Embeddings",
            "text": "Gender bias has been shown to affect several realworld applications relying on automatic language analysis, including online news (Ross and Carter, 2011), advertisements (Sweeney, 2013), abusive language detection (Park et al., 2018), machine translation (Font and Costa-juss\u00e0, 2019;Vanmassenhove et al., 2018), and web search (Kay et al., 2015). In many cases, a model not only replicates bias in the training data but also amplifies it (Zhao et al., 2017). \n\nFor word representations, Bolukbasi et al. (2016) and Caliskan et al. (2017) show that word embeddings encode societal biases about gender roles and occupations, e.g. engineers are stereotypically men, and nurses are stereotypically women. As a consequence, downstream applications that use these pretrained word embeddings also reflect this bias. For example, Zhao et al. (2018a) and Rudinger et al. (2018) show that coreference resolution systems relying on word embeddings encode such occupational stereotypes. In concurrent work, May et al. (2019) measure gender bias in sentence embeddings, but their evaluation is on the aggregation of word representations. In contrast, we analyze bias in contextualized word representations and its effect on a downstream task. \n\nTo mitigate bias from word embeddings, Bolukbasi et al. ( 2016) propose a post-processing method to project out the bias subspace from the pre-trained embeddings. Their method is shown to reduce the gender information from the embeddings of gender-neutral words, and, remarkably, maintains the same level of performance on different downstream NLP tasks. Zhao et al. (2018b) further propose a training mechanism to separate gender information from other factors. However, Gonen and Goldberg (2019) argue that entirely removing bias is difficult, if not impossible, and the gender bias information can be often recovered. This paper investigates a natural follow-up question: What are effective bias mitigation techniques for contextualized embeddings?",
            "score": 0.5797085769551216,
            "section_title": "Related Work",
            "char_start_offset": 2720,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 458
                },
                {
                    "start": 461,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 974
                },
                {
                    "start": 975,
                    "end": 1124
                },
                {
                    "start": 1125,
                    "end": 1229
                },
                {
                    "start": 1232,
                    "end": 1394
                },
                {
                    "start": 1395,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 1852
                },
                {
                    "start": 1853,
                    "end": 1983
                }
            ],
            "ref_mentions": [
                {
                    "start": 170,
                    "end": 185,
                    "matchedPaperCorpusId": "23044927"
                },
                {
                    "start": 214,
                    "end": 233,
                    "matchedPaperCorpusId": "52070035"
                },
                {
                    "start": 283,
                    "end": 310,
                    "matchedPaperCorpusId": "76654723"
                },
                {
                    "start": 327,
                    "end": 345,
                    "matchedPaperCorpusId": "8832874"
                },
                {
                    "start": 438,
                    "end": 457,
                    "matchedPaperCorpusId": "1389483"
                },
                {
                    "start": 487,
                    "end": 510,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 515,
                    "end": 537,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 822,
                    "end": 841,
                    "matchedPaperCorpusId": "4952494"
                },
                {
                    "start": 846,
                    "end": 868,
                    "matchedPaperCorpusId": "13756572"
                },
                {
                    "start": 1587,
                    "end": 1606,
                    "matchedPaperCorpusId": "52161864"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69140625
        },
        {
            "corpus_id": "248233439",
            "title": "Measuring Gender Bias in Contextualized Embeddings",
            "text": ": Transformer models are now increasingly being used in real-world applications. Indiscrim-inately using these models as automated tools may propagate biases in ways we do not realize. To responsibly direct actions that will combat this problem, it is of crucial importance that we detect and quantify these biases. Robust methods have been developed to measure bias in non-contextualized embeddings. Nevertheless, these methods fail to apply to contextualized embeddings due to their mutable nature. Our study focuses on the detection and measurement of stereotypical biases associated with gender in the embeddings of T5 and mT5. We quantify bias by measuring the gender polarity of T5\u2019s word embeddings for various professions. To measure gender polarity, we use a stable gender direction that we detect in the model\u2019s embedding space. We also measure gender bias with respect to a speci\ufb01c downstream task and compare Swedish with English, as well as various sizes of the T5 model and its multilingual variant. The insights from our exploration indicate that the use of a stable gender direction, even in a Transformer\u2019s mutable embedding space, can be a robust method to measure bias. We show that higher status professions are associated more with the male gender than the female gender. In addition, our method suggests that the Swedish language carries less bias associated with gender than English, and the higher manifestation of gender bias is associated with the use of larger language models.",
            "score": 0.5795344508739676,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6005859375
        },
        {
            "corpus_id": "253762019",
            "title": "VLStereoSet: A Study of Stereotypical Bias in Pre-trained Vision-Language Models",
            "text": "Recently there has been much interest in adapting foundation models such as ALBERT (Lan et al., 2020),RoBERTa (Liu et al., 2020), T5 (Raffel et al., 2020), GPT-3 (Brown et al., 2020) and CLIP (Radford et al., 2021) for different downstream tasks. These models demonstrate powerful transfer capabilities largely because they have acquired the rich body of knowledge contained in their pre-training data. However, their pre-training data may also contain social biases and stereotypes, especially when the data are crawled from the internet without cleaning. As a result, pre-trained models may \"inherit\" these biases and stereotypes, affecting the fairness of systems derived from these foundation models for downstream tasks. Previous work mainly focused on measuring biases and stereotypes in a single modality. For example, in NLP, people studied social biases in word embeddings (e.g., Bolukbasi et al., 2016, Zhao et al., 2018b) and language models (e.g., Nadeem et al., 2021,Abid et al., 2021), and in computer vision, people studied social biases in unsupervised vision models (e.g., Steed and Caliskan, 2021). However, there has been little work to understand social biases in multi-modal or cross-modal settings. In particular, although there has been fast progress recently in developing large-scale pre-trained vision-language models (e.g., Li et al., 2021;Radford et al., 2021;Singh et al., 2022), because these models are relatively new, little work has been done to understand biases and stereotypes in them. It is important to measure biases and stereotypes in pre-trained vision-language models because they are used for a wide range of downstream vision-language tasks, many directly involving human users, such as automatic caption generation, visual question answering and multimodal hate speech detection. \n\nIn this work, we study the problem of measuring stereotypical bias in pre-trained vision-language models. We regard the problem as a probing task.",
            "score": 0.5794996334947298,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 812
                },
                {
                    "start": 813,
                    "end": 1116
                },
                {
                    "start": 1117,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1521
                },
                {
                    "start": 1522,
                    "end": 1824
                },
                {
                    "start": 1827,
                    "end": 1932
                },
                {
                    "start": 1933,
                    "end": 1973
                }
            ],
            "ref_mentions": [
                {
                    "start": 83,
                    "end": 101,
                    "matchedPaperCorpusId": "202888986"
                },
                {
                    "start": 133,
                    "end": 154,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 192,
                    "end": 214,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 889,
                    "end": 911,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 911,
                    "end": 932,
                    "matchedPaperCorpusId": "52161864"
                },
                {
                    "start": 960,
                    "end": 979,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 979,
                    "end": 998,
                    "matchedPaperCorpusId": "231603388"
                },
                {
                    "start": 1090,
                    "end": 1115,
                    "matchedPaperCorpusId": "225094318"
                },
                {
                    "start": 1351,
                    "end": 1367,
                    "matchedPaperCorpusId": "236034189"
                },
                {
                    "start": 1367,
                    "end": 1388,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1388,
                    "end": 1407,
                    "matchedPaperCorpusId": "244954250"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60400390625
        },
        {
            "corpus_id": "268889679",
            "title": "The Impact of Unstated Norms in Bias Analysis of Language Models",
            "text": "There has been a surge of interest in and research on bias in machine-learning models. An area of focus is the presence of bias in large language models (LLMs), especially those trained on extensive datasets sourced primarily from the internet. These models have attracted increasing attention due to the influence they are poised to have as they are rapidly integrated into a wide array of applications (Gallegos et al., 2023;Wan et al., 2023;Sheng et al., 2021;Liu et al., 2023). Bias in these models manifests in diverse ways, ranging from overtly discriminatory generations to more subtle expressions like stereotypes. In particular, biases toward underprivileged groups, such as racial minorities, have rightfully garnered attention, as they persist across many social contexts. Uncovering these issues represents a crucial step in addressing the potential implications of such biases in downstream applications. \n\nCounterfactual bias evaluation is a common approach in bias quantification that measures invariance, or lack thereof, in the outcomes of a model for a particular task across different groups, holding all else equal (De-Arteaga et al., 2019;Czarnowska et al., 2021). A pertinent example is perturbing the race associated with a piece of text from one group (e.g. White) to another (e.g. Black) and checking if a model's sentiment prediction changes. Although this is a widely used approach in bias quantification, it ignores the fact that LLM training data does not necessarily follow the same structure for different groups. \n\nMany studies have explored bias in LLMs through fine-grained analysis, primarily using fine-tuning on downstream tasks, such as sentiment or toxicity classification, as a lens. These studies employ a diverse set of metrics to detect variations in model behavior (Gallegos et al., 2023;Delobelle et al., 2022;Czarnowska et al., 2021;M\u00f6kander et al., 2023;Liang et al., 2021;Ribeiro et al., 2020;Levy et al., 2023;Echterhoff et al., 2024).",
            "score": 0.5792933734363077,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 86
                },
                {
                    "start": 87,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 917
                },
                {
                    "start": 920,
                    "end": 1185
                },
                {
                    "start": 1186,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1305
                },
                {
                    "start": 1306,
                    "end": 1368
                },
                {
                    "start": 1369,
                    "end": 1544
                },
                {
                    "start": 1547,
                    "end": 1723
                },
                {
                    "start": 1724,
                    "end": 1984
                }
            ],
            "ref_mentions": [
                {
                    "start": 427,
                    "end": 444,
                    "matchedPaperCorpusId": "264128125"
                },
                {
                    "start": 444,
                    "end": 463,
                    "matchedPaperCorpusId": "234337004"
                },
                {
                    "start": 1135,
                    "end": 1160,
                    "matchedPaperCorpusId": "58006082"
                },
                {
                    "start": 1160,
                    "end": 1184,
                    "matchedPaperCorpusId": "235658325"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67431640625
        },
        {
            "corpus_id": "248987624",
            "title": "RL with KL penalties is better viewed as Bayesian inference",
            "text": "Our paper is a contribution to important lines of work on social bias in large language models and on aligning artificial intelligence with human preferences. The first line of work is primarily concerned with risks associated with an over-representation of certain hegemonic (e.g. sexist, racist, homophobic) viewpoints and voices present in the training data for large language models, which consists primarily of crawled, uncurated user-generated content. Deploying language models exhibiting social biases poses a risk of amplifying and perpetuaing these biases (Sheng et al., 2019;Blodgett et al., 2020;Bender et al., 2021). The second line of work is concerned more broadly with ensuring that objectives that machine learning systems pursue are aligned with human values (Amodei et al., 2016;Russell, 2019). Large language models, due to their capabilities, can be a testbed for alignment techniques for future, more powerful machine learning systems (Askell et al., 2021;Bowman, 2021). Research on RLHF for fine-tuning LMs -such as our paper -can therefore be motivated by both narrower (social bias) and broader (alignment) considerations. As a theoretical contribution, our paper is not expected to pose significant risk. However, RLHF is a dual use technology: it can be diverted to malicious uses such as spreading misinformation or generating harmful content.",
            "score": 0.5791431775354138,
            "section_title": "Ethics statement",
            "char_start_offset": 20203,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 458
                },
                {
                    "start": 459,
                    "end": 629
                },
                {
                    "start": 630,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1371
                }
            ],
            "ref_mentions": [
                {
                    "start": 566,
                    "end": 586,
                    "matchedPaperCorpusId": "202537041"
                },
                {
                    "start": 586,
                    "end": 608,
                    "matchedPaperCorpusId": "218971825"
                },
                {
                    "start": 608,
                    "end": 628,
                    "matchedPaperCorpusId": "262580630"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5830078125
        },
        {
            "corpus_id": "259095603",
            "title": "Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions",
            "text": "Societal biases present in pre-trained large language models are a critical issue as these models have been shown to propagate biases in countless downstream applications, rendering them unfair towards specific groups of people. Since large-scale retraining of these models from scratch is both time and compute-expensive, a variety of approaches have been previously proposed that de-bias a pre-trained model. While the majority of current state-of-the-art debiasing methods focus on changes to the training regime, in this paper, we propose data intervention strategies as a powerful yet simple technique to reduce gender bias in pre-trained models. Specifically, we empirically show that by fine-tuning a pre-trained model on only 10 debiased (intervened) training examples, the tendency to favor any gender is significantly reduced. Since our proposed method only needs a few training examples, we argue that our few-shot de-biasing approach is highly feasible and practical. Through extensive experimentation, we show that our de-biasing technique performs better than competitive state-of-the-art baselines with minimal loss in language modeling ability.",
            "score": 0.5789062699038047,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.830078125
        },
        {
            "corpus_id": "262824801",
            "title": "Large Language Model Alignment: A Survey",
            "text": "In direct bias evaluations of language models, there are various assessment methodologies. Some adopt a contrasting method using associated sentence pairs: one with more stereotypes, and the other with fewer (Nadeem et al., 2020;Nangia et al., 2020). Biases are detected through the language model's likelihood of recovering masks. StereoSet (Nadeem et al., 2020) spans a wide range of domains, including gender, occupation, race, and religion, testing models such as BERT (Devlin et al., 2019), GPT-2, RoBERTa (Liu et al., 2019), and XLNet . CrowS-Pairs (Nangia et al., 2020) extends the types of biases to nine categories: race, religion, age, socioeconomic status, gender, disability, nationality, sexual orientation and appearance. Notably, they change the evaluation metrics to avoid higher likelihoods for certain sentences merely due to their frequent occurrences in training data, rather than learned societal biases.\n\nOthers, similar to toxicity evaluation, provide prompts to models, letting them complete successions, and then assessing biases in the outputs of these models. BOLD (Dhamala et al., 2021) is a prompt dataset containing five bias types: profession, gender, race, religion, and political ideology, collected from Wikipedia. With these prompts, BOLD is able to evaluate social biases of language models via the proposed automated metrics for toxicity, psycholinguistic norms, and text gender polarity. HolisticBias ) is a bias dataset containing 13 demographic directions and over 600 subcategories, offering a comprehensive evaluation of the content generated by models and combining both automatic and human assessments to reveal biases more fully. Automatic evaluation measures bias by breaking down quantities from different stylistic types compares. Human evaluation compares the performance of bias-reduced models with original models, based on preference, human likeness, and interestingness criteria, with crowdsourced workers on Amazon's Mechanical Turk platform. Multilingual Holistic Bias (Costa-juss\u00e0 et al., 2023) extends the HolisticBias  to up to 50 languages, emphasizing the universality and diversity of biases in a multilingual environment.\n\nBoth UnQover  and BBQ (Parrish et al., 2022) focus on detecting model bias",
            "score": 0.5786423846857591,
            "section_title": "LLM-centered Evaluation",
            "char_start_offset": 105212,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 229,
                    "end": 249,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 473,
                    "end": 494,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 555,
                    "end": 576,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 1092,
                    "end": 1114,
                    "matchedPaperCorpusId": "231719337"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.708984375
        },
        {
            "corpus_id": "235097505",
            "title": "Dynamically Disentangling Social Bias from Task-Oriented Representations with Adversarial Attack",
            "text": "Supervised neural networks have achieved remarkable success in a wide range of natural language processing (NLP) tasks. The fundamental capability of these neural models is to learn effective feature representations (Bengio et al., 2013) for the downstream prediction task. Unfortunately, the learned representations frequently contain undesirable biases with respect to things that we would rather not use for decision making. We refer to such inappropriate factors as protected attributes (Elazar and Goldberg, 2018a). Biased information has serious real-world consequences. For example, concerns have been raised about automatic resume filtering systems giving preference to male applicants when the only distinguishing factor is the applicants' gender (Sun et al., 2019). In this paper, we focus on social bias, such as gender bias which is the preference or prejudice towards one gender over the other (Moss-Racusin et al., 2012), race bias and age bias. From the perspective of the debiasing target, previous debiasing works can be approximately classified into two types, word embedding (Bolukbasi et al., 2016;Caliskan et al., 2017;Zhao et al., 2018;Manzini et al., 2019;Wang et al., 2020;Kumar et al., 2020) and sentence embedding (Xu et al., 2017;Elazar and Goldberg, 2018a;Zhang et al., 2018;Ravfogel et al., 2020). The former aims to reduce the gender bias in word embedding, either as a post-processing step (Bolukbasi et al., 2016) or as part of the training procedure (Zhao et al., 2018). The latter focuses on removing these protected attributes from the downstream intermediate representations (Elazar and Goldberg, 2018a;Ravfogel et al., 2020). In this paper, we consider the latter setting and focus on how to mitigate undesirable social bias from the encoded representations without hurting the performance of the main task.",
            "score": 0.5782343218312388,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1503
                },
                {
                    "start": 1504,
                    "end": 1662
                },
                {
                    "start": 1663,
                    "end": 1844
                }
            ],
            "ref_mentions": [
                {
                    "start": 216,
                    "end": 237,
                    "matchedPaperCorpusId": "393948"
                },
                {
                    "start": 907,
                    "end": 934,
                    "matchedPaperCorpusId": "4697978"
                },
                {
                    "start": 1118,
                    "end": 1140,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 1140,
                    "end": 1158,
                    "matchedPaperCorpusId": "52161864"
                },
                {
                    "start": 1158,
                    "end": 1179,
                    "matchedPaperCorpusId": "102350941"
                },
                {
                    "start": 1197,
                    "end": 1216,
                    "matchedPaperCorpusId": "219260204"
                },
                {
                    "start": 1240,
                    "end": 1257,
                    "matchedPaperCorpusId": "30061878"
                },
                {
                    "start": 1284,
                    "end": 1303,
                    "matchedPaperCorpusId": "9424845"
                },
                {
                    "start": 1303,
                    "end": 1325,
                    "matchedPaperCorpusId": "215786522"
                },
                {
                    "start": 1483,
                    "end": 1502,
                    "matchedPaperCorpusId": "52161864"
                },
                {
                    "start": 1639,
                    "end": 1661,
                    "matchedPaperCorpusId": "215786522"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66943359375
        },
        {
            "corpus_id": "271874752",
            "title": "Covert Bias: The Severity of Social Views\u2019 Unalignment in Language Models Towards Implicit and Explicit Opinion",
            "text": "Bias amplification is a well-known phenomenon in which a model aggravates the stereotypes presented in its training data (Li et al., 2023). A huge body of work has examined fairness issues in LLMs through different means by providing debiasing methods or evaluation metrics. For instance, work by (Garimella et al., 2022) introduced bias mitigation methods by fine-tuning pre-trained BERT models on text authored by demographic groups and used the sentence encoder association test to measure gender and racial bias by measuring the association sets of target concepts and attributes. Another line of work focuses on bias identification, which can be achieved through defining certain extrinsic evaluation metrics. Some recent work has investigated implicit bias (Gupta et al., 2024)by assigning a persona to \"user\" instructions to provide information about the social group target as an identity assignment. Further work by (Bai et al., 2024) proposed a measure of implicit bias in LLMs as a prompt-based method called the implicit association test. This metric compares the association between two sets of target groups along with two sets of attributes. Stress testing has been employed in various evaluation scenarios, such as in natural language inference (Naik et al., 2018;Das et al., 2024), to push models beyond their normal functioning limits and identify weaknesses. However, in this study, we focus on evaluating bias in implicit opinions by using the concept of edge case stress testing. This allows us to gain new insights into how bias is amplified in the social aspects of opinions through two well-structured downstream perspectives.",
            "score": 0.5782250601113172,
            "section_title": "Related Work",
            "char_start_offset": 2572,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 584
                },
                {
                    "start": 585,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 1050
                },
                {
                    "start": 1051,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1500
                },
                {
                    "start": 1501,
                    "end": 1650
                }
            ],
            "ref_mentions": [
                {
                    "start": 1261,
                    "end": 1280,
                    "matchedPaperCorpusId": "46932607"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.48828125
        },
        {
            "corpus_id": "258041203",
            "title": "Should ChatGPT be Biased? Challenges and Risks of Bias in Large Language Models",
            "text": "Bias in widely-adopted AI models, including ChatGPT and other generative language models, can have farreaching consequences that extend beyond the immediate context of their applications. When these models absorb and propagate biases, including those present in their training data, they may inadvertently reinforce stereotypes, marginalize certain groups, and lead to unfair treatment across various domains. Some examples of how biased AI models can adversely impact different areas include:\n\n\u2022 Hiring: AI-driven hiring tools that use biased models may exhibit unfair treatment towards applicants from underrepresented groups or those with non-traditional backgrounds. This could lead to the perpetuation of existing inequalities in the job market, limiting opportunities for affected individuals and reducing diversity in the workforce [102,103]. Large language models can be used to automate the screening of job applicants, such as by analyzing resumes and cover letters. Since these models are trained on vast amounts of text data, they may have internalized biases present in the data, such as gender or racial biases. As a result, they could unintentionally favor certain applicants or disqualify others based on factors unrelated to their qualifications, reinforcing existing inequalities in the job market.\n\n\u2022 Lending: Financial institutions increasingly rely on AI models for credit scoring and lending decisions. Biased models may unfairly penalize certain groups, such as minority communities or individuals with lower socioeconomic status, by assigning them lower credit scores or denying them access to loans and financial services based on biased assumptions [104,105,106]. In lending, large language models can be used to assess creditworthiness or predict loan default risk, e.g., based on automated analysis of application or support documents.\n\nIf the data used to train these models contain historical biases or discriminatory lending practices, the models may learn to replicate these patterns. Consequently, they could deny loans to certain demographics or offer unfavorable terms based on factors like race, gender, or socioeconomic status, perpetuating financial inequality [107].\n\n\u2022 Content moderation: AI-powered content moderation systems help manage and filter user-generated content on social media platforms and other online communities. If these systems are trained on biased data, they may disproportionately censor or suppress the voices of certain groups",
            "score": 0.5774598743109363,
            "section_title": "The risks of exacerbating existing societal biases",
            "char_start_offset": 22565,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 844,
                    "end": 848,
                    "matchedPaperCorpusId": "195316573"
                },
                {
                    "start": 1675,
                    "end": 1680,
                    "matchedPaperCorpusId": "61799887"
                },
                {
                    "start": 1684,
                    "end": 1688,
                    "matchedPaperCorpusId": "51734899"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.845703125
        },
        {
            "corpus_id": "256105035",
            "title": "Blacks is to Anger as Whites is to Joy? Understanding Latent Affective Bias in Large Pre-trained Neural Language Models",
            "text": "Hence, understanding affective bias in NLP plays a vital role in achieving algorithmic fairness, by protecting the socio-political and moral equality of marginalized groups. \n\nIn this context, we present an extensive experimental analysis to understand and illustrate the existence of latent \"Affective Bias\" in transformer based large pre-trained language models with respect to the downstream task of textual emotion detection. Hence, we set our research question: Do predictions made by large PLM based textual emotion detection systems systematically or consistently exemplify 'Affective Bias' towards demographic groups?. Our investigation of affective bias in large PLMs primarily aims to identify the existence of gender, racial, and religious affective biases and set aside the task of affective bias mitigation in the scope for future work. We start with an exploration of corpus level affective bias or affect imbalance in corpus to find out any biased emotion associations in the large scale corpora that are used to pre-train and fine-tune the PLMs, by analyzing the distribution of emotions or their Table 1. Differences in predicted emotion classes/intensities between sentences that differ only on a target term representing various social groups, indicating existence of 'Affective Bias'",
            "score": 0.5773679570154276,
            "section_title": "INTRODUCTION",
            "char_start_offset": 4233,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 176,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1303
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7783203125
        },
        {
            "corpus_id": "259203321",
            "title": "Gender Bias in Transformer Models: A comprehensive survey",
            "text": "Artificial intelligence (AI) is often perceived as a neutral entity. However, as AI is created by humans, it reflects our prejudices, including gender bias [1], [2]. There is a growing concern in both the scientific community and the general public about the demographic biases in some AI applications [3]. Gender bias can perpetuate harmful stereotypes and biases, leading to unfair treatment and discrimination. This bias can also limit opportunities for marginalized groups, particularly in areas such as employment, education, and healthcare. Furthermore, it can negatively impact the accuracy and fairness of natural language processing (NLP) applications, affecting the user experience and reliability of these systems. As NLP and machine learning (ML) tools gain prominence, it is becoming increasingly important to understand how they contribute to the formation of societal prejudices and preconceptions. NLP models are effective at modeling many different applications, but they can reinforce gender prejudice that is present in text corpora. Despite the fact that gender bias in NLP has been studied since the 90s [4], [5], there is still much to learn about certain aspects. Therefore, it is crucial to examine how current research in NLP is establishing an entirely new field [6]. To determine whether these studies are heading in the right direction to solve the problem, it is essential to evaluate whether they present scalable evaluation techniques and whether their objectives are well-stated. Bias refers to an unfair opinion or preference held in favor of or against a specific person or group [7]. In machine learning, bias can be caused by faulty assumptions in the algorithm or systemic prediction errors caused by the characteristics of the training data [8], [9]. Gender bias, which favors or stereotypes one gender over another, particularly males over females, has been studied extensively in relation to NLP applications. Different researchers have proposed various definitions and inferences of gender bias and its impact on NLP.",
            "score": 0.5770902397059822,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 68
                },
                {
                    "start": 69,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1511
                },
                {
                    "start": 1512,
                    "end": 1618
                },
                {
                    "start": 1619,
                    "end": 1788
                },
                {
                    "start": 1789,
                    "end": 1949
                },
                {
                    "start": 1950,
                    "end": 2058
                }
            ],
            "ref_mentions": [
                {
                    "start": 161,
                    "end": 164,
                    "matchedPaperCorpusId": "255017656"
                },
                {
                    "start": 302,
                    "end": 305,
                    "matchedPaperCorpusId": "247482521"
                },
                {
                    "start": 1125,
                    "end": 1128,
                    "matchedPaperCorpusId": "5807558"
                },
                {
                    "start": 1289,
                    "end": 1292,
                    "matchedPaperCorpusId": "189819098"
                },
                {
                    "start": 1614,
                    "end": 1617,
                    "matchedPaperCorpusId": "12986910"
                },
                {
                    "start": 1779,
                    "end": 1782,
                    "matchedPaperCorpusId": "3263405"
                },
                {
                    "start": 1784,
                    "end": 1787,
                    "matchedPaperCorpusId": "201666566"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6494140625
        },
        {
            "corpus_id": "267627928",
            "title": "MAFIA: Multi-Adapter Fused Inclusive Language Models",
            "text": "Pretrained Language Models (PLMs) are widely used in NLP for various tasks. Recent studies have identified various biases that such models exhibit and have proposed methods to correct these biases. However, most of the works address a limited set of bias dimensions independently such as gender, race, or religion. Moreover, the methods typically involve finetuning the full model in order to maintain the performance on the downstream task. In this work, we aim to modularly debias a pre-trained language model across multiple dimensions. Previous works extensively explored debiasing PLMs by using limited US-centric counterfactual data augmentation (CDA). We use structured knowledge and a large generative model to build a diverse CDA across multiple bias dimensions in a semi-automated way. We highlight how existing debiasing methods do not consider interactions between multiple societal biases and propose a debiasing model that exploits the synergy amongst various societal biases and enables multi-bias debiasing simultaneously. An extensive evaluation on multiple tasks and languages demonstrates the efficacy of the approach.",
            "score": 0.576806579159662,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83544921875
        },
        {
            "corpus_id": "249282716",
            "title": "What changed? Investigating Debiasing Methods using Causal Mediation Analysis",
            "text": "In this work, we have investigated how debiasing methods impact language models, along with the downstream tasks. We found that (1) debiasing methods are robust after fine-tuning on downstream tasks. In fact, after the fine-tuning, the debiasing effects strengthened. However, this effect is not supported across another bias measure. This indicates the need for both debiasing techniques and bias benchmarks to ensure generalizability. The causal mediation analysis suggests that (2) The neurons that showed a large change in behavior were located in the first two layers of language models (including the word embedding layers). This suggests that careful inspection of certain components of the language models is recommended when applying debiasing methods. (3) Applying debiasing and fine-tuning methods to language models changes the weight that attention heads assign to gender-associated terms. This indicates that attention heads may play a crucial role in representing gender bias in language models. Several limitations apply to this work. We only tested these effects on one downstream task, namely, toxicity detection. In order to check the generalizability of these findings, experiments with other downstream tasks are necessary.",
            "score": 0.576566092944563,
            "section_title": "Conclusion",
            "char_start_offset": 22263,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1050
                },
                {
                    "start": 1051,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1244
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.591796875
        },
        {
            "corpus_id": "236034024",
            "title": "Intersectional Bias in Causal Language Models",
            "text": "To test for intersectional bias, we choose a combination of three categories: gender, religion and disability. Gender bias has been widely studied, and its inclusion here allows for comparison with prior work. Religion and disability bias has received some attention [48,41], but not in relation to the same language models, nor as intersectional categories. All three categories have relatively unambiguous category markers, though some common identity-first disability qualifiers, such as 'disabled', 'deaf' and 'blind', are frequently, and often contentiously, used in contexts outside of social identity categories. For example, 'disabled' has often been used in technical literature to mean turned or switched off. While a feature of transformer models has been their ability to disambiguate homophones (such as varied meanings of 'like'), performance on metaphoric or analogic modifiers varies. \n\nOur selection of terms is based on what is generally considered common use, as discussed further in relation to each of the three categories below. This aims to mirror commonality of these terms in the language models themselves, which are weighted towards media and social media text samples, rather than academic, advocacy or other literature. We note that this approach means, on the one hand, that norms and preferences around terminology and identity of particular disability groups may not always be reflected in selection of terms and, on the other, more extreme examples of biased language use may also not be generated and 2 https://www.microsoft.com/en-gb/ai/responsible-ai 3 https://ai.google/responsibilities/responsible-ai-practices/ identified. \n\nWe select five categories of religion, following the World Religion Paradigm [4]. Though this paradigm is challenged in the literature, it likely reflects common religious terms of use in language model training sets. We add 'atheism' as a marker of explicit non-religious orientation, and an empty string ('') as a null marker. \n\nWith studies of coreference bias [21,23], grammatical and social categories necessarily align, and gender is treated as binary-or ternary-valued. For autocompletion tasks, this constraint can be relaxed. We add 'transgender person' to 'woman' and 'man', and include 'person' as an un-gendered marker.",
            "score": 0.5758763397795322,
            "section_title": "Method",
            "char_start_offset": 13063,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 900
                },
                {
                    "start": 903,
                    "end": 1050
                },
                {
                    "start": 1051,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1661
                },
                {
                    "start": 1664,
                    "end": 1745
                },
                {
                    "start": 1746,
                    "end": 1881
                },
                {
                    "start": 1882,
                    "end": 1992
                },
                {
                    "start": 1995,
                    "end": 2140
                },
                {
                    "start": 2141,
                    "end": 2198
                },
                {
                    "start": 2199,
                    "end": 2295
                }
            ],
            "ref_mentions": [
                {
                    "start": 271,
                    "end": 274,
                    "matchedPaperCorpusId": "204900715"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.425048828125
        },
        {
            "corpus_id": "265607922",
            "title": "Assessing Political Inclination of Bangla Language Models",
            "text": "Bias identification and mitigation have been subjects of significant research interest (Liu et al., 2022\u037e Chen et al., 2023). Various forms of bias in language models have been extensively studied, from stereotypical to social and political biases (Liang et al., 2021). Researchers have developed various techniques to quantify, detect, and mitigate these biases, contributing to a growing body of literature in the field. Sun et al. (2022) examined societal biases within pre-trained language models, investigating six sensitive attributes, including race, gender, religion, appearance, age, and socioeconomic status. Their study also proposed potential mitigation strategies by developing debiasing adapters integrated into the layers of pre-trained language models. \n\nSimilarly, gender bias within LMs has garnered significant research attention. Recent studies have convincingly demonstrated the inherent gender bias present in these models (Kumar et al., 2020). Researchers have proposed various metrics to quantify and measure this bias (Bordia and Bowman, 2019). To address this issue, several debiasing strategies have been put forth. Qian et al. (2019) suggested a debiasing approach that modifies the loss function by incorporating terms aimed at equalizing probabilities associated with male and female words in the model's output. Vig et al. (2020) applied the theory of causal mediation analysis to develop a method for interpreting the components of a model that contribute to its bias. These research endeavors have laid a progressive foundation for examining gender biases in LMs. \n\nFurthermore, researchers have investigated various aspects of bias within LMs (Kaneko et al., 2022\u037e de Vassimon Manela et al., 2021\u037e Van Der Wal et al., 2022\u037e Joniak and Aizawa, 2022). Kirk et al. (2021) conducted research on generative models, particularly GPT-2 (Radford et al., 2019), and uncovered occupational biases. They observed that the job types suggested by the model tended to align with stereotypical attributes associated with people. Similarly, Venkit et al. (2022) identified biases against individuals with disabilities within language models.",
            "score": 0.5758763397795322,
            "section_title": "Related Works",
            "char_start_offset": 3297,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 768
                },
                {
                    "start": 771,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1342
                },
                {
                    "start": 1343,
                    "end": 1500
                },
                {
                    "start": 1501,
                    "end": 1596
                },
                {
                    "start": 1599,
                    "end": 1783
                },
                {
                    "start": 1784,
                    "end": 1921
                },
                {
                    "start": 1922,
                    "end": 2047
                },
                {
                    "start": 2048,
                    "end": 2159
                }
            ],
            "ref_mentions": [
                {
                    "start": 248,
                    "end": 268,
                    "matchedPaperCorpusId": "235623756"
                },
                {
                    "start": 423,
                    "end": 440,
                    "matchedPaperCorpusId": "252907549"
                },
                {
                    "start": 1043,
                    "end": 1068,
                    "matchedPaperCorpusId": "102352788"
                },
                {
                    "start": 1143,
                    "end": 1161,
                    "matchedPaperCorpusId": "170078973"
                },
                {
                    "start": 1343,
                    "end": 1360,
                    "matchedPaperCorpusId": "227275068"
                },
                {
                    "start": 1784,
                    "end": 1802,
                    "matchedPaperCorpusId": "236950797"
                },
                {
                    "start": 1863,
                    "end": 1885,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73291015625
        },
        {
            "corpus_id": "249039283",
            "title": "Hollywood Identity Bias Dataset: A Context Oriented Bias Analysis of Movie Dialogues",
            "text": "Bias in NLP: There has been a lot research on bias detection in various forms of text, starting from word embeddings (Caliskan et al., 2017). Bolukbasi et al. (2016) has shown that embeddings like Glove (Pennington et al., 2014) and Word2vec (Mikolov et al., 2013) exhibit human-like gender bias for various occupations. This was extended by Manzini et al. (2019) to show racial and religious biases. Concurrent works like Kurita et al. (2019) and May et al. (2019) show that contextual embeddings like BERT (Devlin et al., 2019) not only encode multiple social biases but also amplify them. The presence of bias in these models and embeddings poses many harmful risks by unfair allocation of resources against the marginalized section of the society such as LGBTQ (Blodgett et al., 2020). \n\nHence, the research community, in its endeavor toward EthicalAI, has seen an upsurge in many supervised datasets, methodologies to detect and mitigate various social biases through different extrinsic tasks like hate speech and toxicity detection (Sap et al., 2019;Davidson et al., 2019;Dixon et al., 2018), coreference resolution (Webster et al., 2018), question answering (Li et al., 2020), machine translation (Stanovsky et al., 2019), and so on. However, most of the works focus on gender and race biases. Other biases like religion, \n\nLGBTQ, occupation, and body-shaming are still not explored much. \n\nThe recently released StereoSet dataset (Nadeem et al., 2021) was crowd-sourced to measure stereotypical gender, profession, race, and religion biases. Another crowd-sourced dataset called CrowS-Pairs (Nangia et al., 2020) was published as a challenge set for measuring the degree to which nine types of social bias are present in language models. It has data related to nine bias types: race, gender, sexual orientation, religion, age, nationality, disability, physical appearance and socioeconomic status.",
            "score": 0.5758763397795322,
            "section_title": "Related Work",
            "char_start_offset": 5513,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 789
                },
                {
                    "start": 792,
                    "end": 1241
                },
                {
                    "start": 1242,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1329
                },
                {
                    "start": 1332,
                    "end": 1396
                },
                {
                    "start": 1399,
                    "end": 1550
                },
                {
                    "start": 1551,
                    "end": 1746
                },
                {
                    "start": 1747,
                    "end": 1906
                }
            ],
            "ref_mentions": [
                {
                    "start": 117,
                    "end": 140,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 203,
                    "end": 228,
                    "matchedPaperCorpusId": "1957433"
                },
                {
                    "start": 342,
                    "end": 363,
                    "matchedPaperCorpusId": "102350941"
                },
                {
                    "start": 448,
                    "end": 465,
                    "matchedPaperCorpusId": "85518027"
                },
                {
                    "start": 508,
                    "end": 529,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 765,
                    "end": 788,
                    "matchedPaperCorpusId": "218971825"
                },
                {
                    "start": 1039,
                    "end": 1057,
                    "matchedPaperCorpusId": "196211238"
                },
                {
                    "start": 1079,
                    "end": 1098,
                    "matchedPaperCorpusId": "54997157"
                },
                {
                    "start": 1205,
                    "end": 1229,
                    "matchedPaperCorpusId": "173991101"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68798828125
        },
        {
            "corpus_id": "267897469",
            "title": "COBIAS: Assessing the Contextual Reliability of Bias Benchmarks for Language Models",
            "text": "Bias in computer systems has been a research topic for over 35 years [18,19,38]. While there has been considerable progress since then, completely eliminating bias remains a complex challenge. In language, context plays a significant role in determining the presence of bias. 2 By context, we refer to situational or background information that can change the meaning and interpretation of a statement. For instance, a statement about men being better than women at physical labor manifests as a gender bias in employment settings, yet it can be interpreted as a neutral observation during Figure 1: A conversation on OpenAI's ChatGPT (GPT-3.5) platform (https://chat.openai.com). ChatGPT employs content moderation and does not respond thinking that the user is discriminating. However, alternate scenarios might exist where the input is not biased, highlighting the need for contextual exploration. The outputs are summarized for depiction. \n\ndiscussions on biological differences. However, current research lacks contextual considerations in bias assessment. \n\nLarge Language Models (LLMs) are trained on publicly available corpora that inherently contain human biases, which the models learn and propagate subsequently [13,16,56]. For example, in 2016, Microsoft's chatbot Tay learned social stereotypes from Twitter, leading Microsoft to shut down the project [35]. More recently, Delphi [26], an AI framework built to reason about moral and ethical judgments, was shown to provide biased responses due to its crowdsourced training data [60]. Even now, current state-of-theart LLMs suffer from bias [61,74]. \n\nIn tandem with these developments, prior studies concentrate on alleviating these biases by developing methods to debias LLMs [11,21]. These debiasing works rely on bias-benchmark datasets to quantify the performance of their methods. However, Blodgett et al. [8] show that existing bias-benchmark datasets suffer from several pitfalls, such as the presence of irrelevant stereotypes, misaligned representations of biases, and a lack of crucial contextual factors necessary to accurately depict the stereotypes they aim to address.",
            "score": 0.5756206104349514,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 81,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 900
                },
                {
                    "start": 901,
                    "end": 942
                },
                {
                    "start": 945,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1061
                },
                {
                    "start": 1064,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1370
                },
                {
                    "start": 1371,
                    "end": 1547
                },
                {
                    "start": 1548,
                    "end": 1612
                },
                {
                    "start": 1615,
                    "end": 1749
                },
                {
                    "start": 1750,
                    "end": 1849
                },
                {
                    "start": 1850,
                    "end": 2146
                }
            ],
            "ref_mentions": [
                {
                    "start": 69,
                    "end": 73,
                    "matchedPaperCorpusId": "15243096"
                },
                {
                    "start": 73,
                    "end": 76,
                    "matchedPaperCorpusId": "207195759"
                },
                {
                    "start": 76,
                    "end": 79,
                    "matchedPaperCorpusId": "5143191"
                },
                {
                    "start": 1223,
                    "end": 1227,
                    "matchedPaperCorpusId": "231719337"
                },
                {
                    "start": 1227,
                    "end": 1230,
                    "matchedPaperCorpusId": "258686693"
                },
                {
                    "start": 1230,
                    "end": 1233,
                    "matchedPaperCorpusId": "244117167"
                },
                {
                    "start": 1542,
                    "end": 1546,
                    "matchedPaperCorpusId": "250390668"
                },
                {
                    "start": 1608,
                    "end": 1611,
                    "matchedPaperCorpusId": "266365557"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.468017578125
        },
        {
            "corpus_id": "247951931",
            "title": "PaLM: Scaling Language Modeling with Pathways",
            "text": "The ability to do language modeling at such large scale and quality unlocks the potential for a wide variety of real world applications, including in healthcare and education domains (Bommasani & et. al., 2021).\n\nHowever, recent research has pointed out various potential risks associated with such large-scale generalpurpose language models trained on web text Bommasani & et. al., 2021), for instance, perpetuating or exacerbating social stereotypes and disparities that are reflected in training data (Sheng et al., 2021), memorizing and revealing private information (Carlini et al., 2022), or causing downstream harms (Weidinger et al., 2021). Since entirely eliminating all such societal biases from data and models may not be feasible, it is essential to analyze and document such potential undesirable associations and risks through transparency artifacts such as model cards (Mitchell et al., 2019) and datasheets .\n\nTo this end, we report a thorough analysis of the dataset (Appendix C) and model outputs (Section 7 and Section 10) for such biases and risks. Furthermore, we provide a datasheet (Appendix D) and model card (Appendix E) for increased transparency for the downstream users of PaLM.\n\nOur analysis reveals that our training data, and consequently PaLM, do reflect various social stereotypes and toxicity associations around identity terms. Removing these associations, however, is non-trivial; for instance, filtering off content that is deemed toxic by an automated tool may disproportionately exclude content about or authored by marginalized subgroups in the training data (Dodge et al., 2021). Future work should look into effectively tackling such undesirable biases in data, and their influence on model behavior. Meanwhile, any real-world use of PaLM for downstream tasks should perform further contextualized fairness evaluations to assess the potential harms and introduce appropriate mitigation and protections.\n\nFurther, as discussed in Section 10.3, our fairness analyses are narrowly scoped and do not provide an exhaustive account of the various potential risks. For instance, we focus on biases along gender, race, ethnicity and religion axes, but perform the analysis only on the English language data and model outputs. We have not looked into biases along other",
            "score": 0.5752898769985724,
            "section_title": "Ethical Considerations",
            "char_start_offset": 123934,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 884,
                    "end": 907,
                    "matchedPaperCorpusId": "52946140"
                },
                {
                    "start": 1599,
                    "end": 1619,
                    "matchedPaperCorpusId": "237568724"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7822265625
        },
        {
            "corpus_id": "273532658",
            "title": "Are Large Language Models Ready for Travel Planning?",
            "text": "LLMs are trained on vast corpora of textual data, enabling them to generate coherent and contextually appropriate text that closely mimics human-produced content (Dong et al., 2023). However, these models are susceptible to inherent biases stemming from their training datasets and architectural design, potentially leading to problematic outputs. As LLMs increasingly permeate various sectors and exert significant societal influence, the implications of these biases have garnered substantial attention (Dong et al., 2023;Zhao et al., 2024). Consequently, the investigation of inherent biases in LLMs has emerged as a critical area of research, attracting considerable focus across the field (Hu et al., 2023). \n\nScholars have explored the inherent biases in LLMs and identified various strategies for mitigating these biases. Hu et al. (2023) examined ingroup-positive and outgroup-negative biases across 51 LLMs, finding that most exhibited social biases comparable to humans. Haim et al. (2024) assessed gender and racial biases in GPT-4, GPT-3.5, and PaLM-2 across multiple domains, confirming their presence. Rhue et al. (2024) identified gender-based biases in LLM responses to factual inquiries, while Zhao et al. (2024) reported significant gender biases in LLMs tailored for various languages. Dong et al. (2023) observed persistent explicit and implicit gender biases in LLaMA models, noting that increasing model size did not reliably improve fairness. To address these issues, researchers have explored potential solutions. Kaneko et al. (2024) demonstrated the effectiveness of Chain-of-Thought prompting in reducing unconscious social biases in LLMs, promoting fairer predictions. \n\nBesides, extensive research has been conducted on gender and racial/ethnic biases within the deployment of LLMs across diverse sectors, including healthcare, the workplace, and education, producing mixed outcomes. In healthcare, some studies have found no significant biases in ChatGPT-4's diagnostic accuracy and health advice (Andreadis et al., 2024;Ito et al., 2023;Hanna et al., 2023). However, other research highlights biases.",
            "score": 0.5735924326161148,
            "section_title": "Gender and racial/ethnic bias in LLMs",
            "char_start_offset": 11567,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 712
                },
                {
                    "start": 715,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1304
                },
                {
                    "start": 1305,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1537
                },
                {
                    "start": 1538,
                    "end": 1696
                },
                {
                    "start": 1699,
                    "end": 1912
                },
                {
                    "start": 1913,
                    "end": 2088
                },
                {
                    "start": 2089,
                    "end": 2131
                }
            ],
            "ref_mentions": [
                {
                    "start": 2027,
                    "end": 2051,
                    "matchedPaperCorpusId": "269446762"
                },
                {
                    "start": 2051,
                    "end": 2068,
                    "matchedPaperCorpusId": "264931875"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52685546875
        },
        {
            "corpus_id": "276929100",
            "title": "VisBias: Measuring Explicit and Implicit Social Biases in Vision Language Models",
            "text": "In the multiple-choice setting, we analyze which demographic groups are linked to particular answers, while in yes-no comparisons (such as asking whether one person appears more educated than another), we check for group-specific favoritism. Next, to unveil more subtle forms of bias, we in-troduce implicit tasks-(III) Image Description and (IV) Form Completion-where the model's goal is to assist with descriptive or data-entry tasks rather than make overt judgments. In image description, we measure skewed word associations across different demographic groups by examining the language models use. Finally, in form completion, we prompt VLMs to fill out personal information forms with various attributes, tracking how certain features (like gender or race) shift the distribution of other attributes. By integrating explicit and implicit testing, VISBIAS provides a comprehensive lens for detecting bias across diverse modeling tasks, as shown in Fig. 1. \n\nWe evaluate five VLMs-GPT-4(V), GPT-4o, Gemini-1.5-Pro, LLaMA-3.2 (Vision), and LLaVA-v1.6-along with Midjourney for image description. Our experiments show that while these models generally perform well in explicit scenarios (e.g., direct multiple-choice queries or yes-no questions), they still manifest concerning biases in more implicit tasks, such as nuanced image descriptions and form completion. These biases emerge in the form of stereotyping around gender, race, religion, and other demographic attributes, reflecting rigid associations (e.g., specific ethnicities tied to partic-ular religions) and perpetuating unfounded assumptions. Such findings highlight the need for continued efforts to mitigate social biases in advanced VLMs. Our contributions can be summarized as: \n\n1. We propose VISBIAS, which detects both explicit biases (multiple-choice/yes-no questions) and implicit biases (image description/formcompletion tasks). \n\n2. Testing across leading VLMs reveals persistent race, gender, and religious stereotypes, emphasizing the need for stronger fairness interventions in advanced VLMs.",
            "score": 0.5729138608643276,
            "section_title": "Introduction",
            "char_start_offset": 3501,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 959
                },
                {
                    "start": 962,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1058
                },
                {
                    "start": 1059,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1365
                },
                {
                    "start": 1366,
                    "end": 1607
                },
                {
                    "start": 1608,
                    "end": 1706
                },
                {
                    "start": 1707,
                    "end": 1746
                },
                {
                    "start": 1749,
                    "end": 1903
                },
                {
                    "start": 1906,
                    "end": 2071
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7998046875
        },
        {
            "corpus_id": "264306000",
            "title": "A Predictive Factor Analysis of Social Biases and Task-Performance in Pretrained Masked Language Models",
            "text": "Various types of social biases have been reported with pretrained Masked Language Models (MLMs) in prior work. However, multiple underlying factors are associated with an MLM such as its model size, size of the training data, training objectives, the domain from which pretraining data is sampled, tokenization, and languages present in the pretrained corpora, to name a few. It remains unclear as to which of those factors influence social biases that are learned by MLMs. To study the relationship between model factors and the social biases learned by an MLM, as well as the downstream task performance of the model, we conduct a comprehensive study over 39 pretrained MLMs covering different model sizes, training objectives, tokenization methods, training data domains and languages. Our results shed light on important factors often neglected in prior literature, such as tokenization or model objectives.",
            "score": 0.5728940596836992,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84521484375
        },
        {
            "corpus_id": "252407674",
            "title": "Bias at a Second Glance: A Deep Dive into Bias for German Educational Peer-Review Data Modeling",
            "text": "Their results indicate that training nation-specific models leads to various skews between groups, increasing algorithmic bias when compared to training on all groups together. In a more general scope, research in computational linguistics and NLP have increasingly investigated bias in natural language processing systems, including work on bias in embedding spaces (e.g., Caliskan et al. (2017); Bolukbasi et al. (2016)) language modeling (Lu et al., 2018), coreference resolution (Rudinger et al., 2017), machine translation (Stanovsky et al., 2019), or sentiment analysis (Kiritchenko and Mohammad, 2018). \n\nThe analysis of data collected from commonly used pedagogical scenarios in combination with pre-trained language models (especially outside North America) is scarce (e.g., Baker and Hawn (2021); Sun et al. (2019)). We focus on algorithmic bias in the representations of a novel collected data set, in the most common German pre-trained models and German GloVe embeddings, and finally in the fine-tuned models. Our objective goes beyond investigating the effect of NLP bias on educational designs; we aim to contribute to a vision where downstream educational models are unbiased for equitable education (UN Sustainability objective).",
            "score": 0.5728466034034179,
            "section_title": "Text Bias in Education",
            "char_start_offset": 7962,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 609
                },
                {
                    "start": 612,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1245
                }
            ],
            "ref_mentions": [
                {
                    "start": 374,
                    "end": 396,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 441,
                    "end": 458,
                    "matchedPaperCorpusId": "51888520"
                },
                {
                    "start": 483,
                    "end": 506,
                    "matchedPaperCorpusId": "5310359"
                },
                {
                    "start": 528,
                    "end": 552,
                    "matchedPaperCorpusId": "173991101"
                },
                {
                    "start": 576,
                    "end": 608,
                    "matchedPaperCorpusId": "21670658"
                },
                {
                    "start": 807,
                    "end": 824,
                    "matchedPaperCorpusId": "198324795"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.31982421875
        },
        {
            "corpus_id": "270562923",
            "title": "Unveiling and Mitigating Bias in Mental Health Analysis with Large Language Models",
            "text": "The advancement of large language models (LLMs) has demonstrated strong capabilities across various applications, including mental health analysis. However, existing studies have focused on predictive performance, leaving the critical issue of fairness underexplored, posing significant risks to vulnerable populations. Despite acknowledging potential biases, previous works have lacked thorough investigations into these biases and their impacts. To address this gap, we systematically evaluate biases across seven social factors (e.g., gender, age, religion) using ten LLMs with different prompting methods on eight diverse mental health datasets. Our results show that GPT-4 achieves the best overall balance in performance and fairness among LLMs, although it still lags behind domain-specific models like MentalRoBERTa in some cases. Additionally, our tailored fairness-aware prompts can effectively mitigate bias in mental health predictions, highlighting the great potential for fair analysis in this field.",
            "score": 0.5723789185047768,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87744140625
        },
        {
            "corpus_id": "274776546",
            "title": "Bias Vector: Mitigating Biases in Language Models with Task Arithmetic Approach",
            "text": "Language models (LMs) are inherently biased because their training processes rely on humancreated text data, which would reflect human biases (Bolukbasi et al., 2016). Navigli et al. (2023) defined the term bias in the field of Natural Language Processing as \"prejudices, stereotypes, and discriminatory attitudes against certain groups of people.\" We adopt this bias definition throughout this paper. Various debiasing methods have been proposed to mitigate these biases (Schick et al., 2021;Zmigrod et al., 2019;Webster et al., 2020;Ravfogel et al., 2020;Liang et al., 2020). \n\nSeveral studies have shown that for wordembedding models, such as word2vec (Mikolov et al., 2013), the bias in word embeddings can be mitigated using approaches like subtracting the statistically significant mean vector associated with the bias from each word vector (Bolukbasi et al., 2016;Mu and Viswanath, 2018;Gonen and Goldberg, 2019;Wang et al., 2020). In contrast, other studies ahve proposed bias mitigation techniques specifically for Transformer-based LMs (Ravfogel et al., 2020;Liang et al., 2020). \n\nSeveral benchmarks have been introduced to evaluate debiasing approaches. Islam et al. (2016) developed the Word Embedding Association Test (WEAT) to measure bias scores in word embeddings. May et al. (2019) proposed the Sentence Encoder Association Test (SEAT) as an extension of WEAT, extending the focus from word to sentence. StereoSet (Nadeem et al., 2021) is another benchmark designed to evaluate stereotypes across four bias categories: race, profession, gender, and religion. StereoSet consists of two subsets: intrasentence, which measures biases within a individual sentence, and intersentence, which evaluates biases at the discourse level across multiple sentences. Nangia et al. (2020) also introduced the CrowS-Pairs benchmark for bias neasurements.",
            "score": 0.572349718452399,
            "section_title": "Language Models and Bias",
            "char_start_offset": 4642,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 577
                },
                {
                    "start": 580,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1089
                },
                {
                    "start": 1092,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1576
                },
                {
                    "start": 1577,
                    "end": 1770
                },
                {
                    "start": 1771,
                    "end": 1856
                }
            ],
            "ref_mentions": [
                {
                    "start": 142,
                    "end": 166,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 168,
                    "end": 189,
                    "matchedPaperCorpusId": "258688053"
                },
                {
                    "start": 472,
                    "end": 493,
                    "matchedPaperCorpusId": "232075876"
                },
                {
                    "start": 493,
                    "end": 514,
                    "matchedPaperCorpusId": "184486914"
                },
                {
                    "start": 535,
                    "end": 557,
                    "matchedPaperCorpusId": "215786522"
                },
                {
                    "start": 557,
                    "end": 576,
                    "matchedPaperCorpusId": "207996257"
                },
                {
                    "start": 847,
                    "end": 871,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 871,
                    "end": 894,
                    "matchedPaperCorpusId": "3986339"
                },
                {
                    "start": 894,
                    "end": 919,
                    "matchedPaperCorpusId": "73729169"
                },
                {
                    "start": 919,
                    "end": 937,
                    "matchedPaperCorpusId": "204770514"
                },
                {
                    "start": 1046,
                    "end": 1069,
                    "matchedPaperCorpusId": "215786522"
                },
                {
                    "start": 1069,
                    "end": 1088,
                    "matchedPaperCorpusId": "207996257"
                },
                {
                    "start": 1282,
                    "end": 1299,
                    "matchedPaperCorpusId": "85518027"
                },
                {
                    "start": 1432,
                    "end": 1453,
                    "matchedPaperCorpusId": "215828184"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66259765625
        },
        {
            "corpus_id": "259203321",
            "title": "Gender Bias in Transformer Models: A comprehensive survey",
            "text": "In recent years, the popularity of Language Modelling has significantly increased, mainly due to the development of transformers such as BERT, GPT-2, and XLM [33]- [35]. These deep learning models employ a self-attention process that allows them to weigh the importance of different input data components differently [36]. Additionally, transformer- Fig. 5: Gender Bias Induced from Model Building based models can be fine-tuned for a specific downstream task, making them highly versatile. Fine-tuning requires much less data than training a language model from scratch, making these models highly efficient. However, despite their efficiency, gender bias has been observed in transformers, indicating that the problem of bias in NLP is still prevalent. Evidence of gender bias in transformer models like GPT2 [37], GPT3 [38], RoBERTa [39], and DeBERTa [40] was proven by researchers across the globe through a series of experiments. To identify occupational gender bias in GPT-2, examine how the prejudice evolves with various model sizes, and contrast this bias with bias in our culture, Bolukbasi et al. [41] conducted several tests. The experiments have shown that occupations became more gender-neutral as the number of trained parameters increased. According to societal statistics and all four GPT-2 models, there is a trend toward increased male bias as job salaries rise. That is, the more senior the job and the bigger its monetary compensation, the more likely it is that a man is holding that position, according to GPT-2. \n\nSimilarly, a study conducted by Brown et al. [42] has also proven that GPT-3 contains racial and gender bias. On performing the occupational experiment, results have shown that GPT-3 found that 83% of the 388 evaluated vocations were more likely to be linked to a male identity. Also, higher-level occupations with a predominance of men included banker and professor emeritus. Another well-known AI chatbot, ChatGPT [43], has been accused of gender bias [44], [45]. Kieran Snyder, a co-founder of Textio, points out that ChatGPT can start incorporating gendered presumptions into feedback that is otherwise so generic with very little effort.",
            "score": 0.5722904461522987,
            "section_title": "IV. TRANSFORMERS AND GENDER BIAS",
            "char_start_offset": 23334,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1535
                },
                {
                    "start": 1538,
                    "end": 1647
                },
                {
                    "start": 1648,
                    "end": 1816
                },
                {
                    "start": 1817,
                    "end": 1914
                },
                {
                    "start": 1915,
                    "end": 2003
                },
                {
                    "start": 2004,
                    "end": 2180
                }
            ],
            "ref_mentions": [
                {
                    "start": 164,
                    "end": 168,
                    "matchedPaperCorpusId": "232267493"
                },
                {
                    "start": 317,
                    "end": 321,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 811,
                    "end": 815,
                    "matchedPaperCorpusId": "196471188"
                },
                {
                    "start": 1108,
                    "end": 1112,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 1583,
                    "end": 1587,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1954,
                    "end": 1958,
                    "matchedPaperCorpusId": "256439068"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.44091796875
        },
        {
            "corpus_id": "260155190",
            "title": "Towards Bridging the Digital Language Divide",
            "text": "In the context of AI language technology, the notion of bias has so far been used to refer to patterns of stereotypes and preferences towards social groups, most often concerning learning-based language processing systems [9]. In terms of social groups, studies have mostly focused on gender, ethnicity, and race, but also other forms of bias (religion-related, age-related, political, etc.) [15]. To our knowledge, the term linguistic bias has not been used so far in any way similar to ours. Many of the underlying exploitative mechanisms have, however, been pointed out, in particular in relation to the most disempowered social groups, namely small indigenous speaker communities [8,35]. In terms of actual bias in AI systems and data, the research closest to ours concerns inductive bias in language models towards certain morphological and syntactic structures [33,39]. We present these works more in detail in Section 3. In [10], Blodgett et al. study the (non-)representation of the vernaculars of social groups within language resources. They point out that English linguistic corpora tend to exclude the register of speech used by African-Americans, the non-representation of which causes a bias in the abilities of the AI systems trained on top of them. We identify this as a particular case of linguistic bias, even if in the paper cited, it is (also correctly) framed as a form of racial bias within the context of a single language. While our definition of linguistic bias below aims to encompass such cases, the focus of our current research is bias across languages rather than across registers, styles, or vernaculars within the same language. \n\nIntuitively, linguistic bias is observed in language technology when, due to its design, a system represents, interprets, or processes utterances in certain languages less precisely or less efficiently than in others, thereby negatively affecting the communication ability of speakers of that language.",
            "score": 0.572257799223749,
            "section_title": "Linguistic Diversity and Bias",
            "char_start_offset": 8048,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1446
                },
                {
                    "start": 1447,
                    "end": 1660
                },
                {
                    "start": 1663,
                    "end": 1965
                }
            ],
            "ref_mentions": [
                {
                    "start": 222,
                    "end": 225,
                    "matchedPaperCorpusId": "218971825"
                },
                {
                    "start": 392,
                    "end": 396,
                    "matchedPaperCorpusId": "207195759"
                },
                {
                    "start": 684,
                    "end": 687,
                    "matchedPaperCorpusId": "248780265"
                },
                {
                    "start": 687,
                    "end": 690,
                    "matchedPaperCorpusId": "248780200"
                },
                {
                    "start": 867,
                    "end": 871,
                    "matchedPaperCorpusId": "80628431"
                },
                {
                    "start": 871,
                    "end": 874,
                    "matchedPaperCorpusId": "235293810"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63916015625
        },
        {
            "corpus_id": "216553696",
            "title": "Causal Mediation Analysis for Interpreting Neural NLP: The Case of Gender Bias",
            "text": "Neural networks learn to replicate historical, societal biases from training data in various tasks such as natural language inference (Rudinger et al., 2017), coreference resolution (Cao and Daum\u00e9 III, 2019), and sentiment analysis (Kiritchenko and Mohammad, 2018). This conflicts with the principle of counterfactual fairness, which states that the model predictions should not be influenced by changes to a sensitive attribute such as gender (Kusner et al., 2017); for instance, a fair and unbiased model should equally associate gendered pronouns with professions. However, biased models make this association proportionally to the distribution of gender in the training data (Caliskan et al., 2017). While efforts have been made to reduce bias, this remains a significant ethical challenge. \n\nA common strategy to mitigate biases is to change the training data (e.g., Lu et al., 2018;Hall Maudslay et al., 2019;Zhao et al., 2018a;Kaushik et al., 2019), the training process (e.g., Huang et al., 2019;Qian et al., 2019), or the model itself (e.g., Madras et al., 2019;Romanov et al., 2019;Gehrmann et al., 2019) to ensure counterfactual fairness. The resulting biases are often measured similarly to this work by testing that mentions of occupations lead to equal probabilities across grammatical genders in referential expressions. \n\nOthers have focused on de-biasing word embeddings and contextual word representations (Bolukbasi et al., 2016;Zhao et al., 2018b;Yang and Feng, 2020), though recent work has questioned the efficacy of these debiasing techniques in removing both grammatical and societal biases (Elazar and Goldberg, 2018;Gonen and Goldberg, 2019). Biases may also be introduced in downstream tasks and representations in models where representations depend on additional context (Zhao et al., 2019b;Kurita et al., 2019).",
            "score": 0.5722488872968218,
            "section_title": "Gender Bias and Other Biases",
            "char_start_offset": 7359,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 794
                },
                {
                    "start": 797,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1335
                },
                {
                    "start": 1338,
                    "end": 1668
                },
                {
                    "start": 1669,
                    "end": 1841
                }
            ],
            "ref_mentions": [
                {
                    "start": 134,
                    "end": 157,
                    "matchedPaperCorpusId": "5310359"
                },
                {
                    "start": 232,
                    "end": 264,
                    "matchedPaperCorpusId": "21670658"
                },
                {
                    "start": 444,
                    "end": 465,
                    "matchedPaperCorpusId": "2014883"
                },
                {
                    "start": 679,
                    "end": 702,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 888,
                    "end": 915,
                    "matchedPaperCorpusId": "202541569"
                },
                {
                    "start": 915,
                    "end": 934,
                    "matchedPaperCorpusId": "4952494"
                },
                {
                    "start": 1051,
                    "end": 1071,
                    "matchedPaperCorpusId": "58006057"
                },
                {
                    "start": 1424,
                    "end": 1448,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 1448,
                    "end": 1467,
                    "matchedPaperCorpusId": "52161864"
                },
                {
                    "start": 1467,
                    "end": 1487,
                    "matchedPaperCorpusId": "208267842"
                },
                {
                    "start": 1615,
                    "end": 1642,
                    "matchedPaperCorpusId": "52056513"
                },
                {
                    "start": 1642,
                    "end": 1667,
                    "matchedPaperCorpusId": "73729169"
                },
                {
                    "start": 1800,
                    "end": 1820,
                    "matchedPaperCorpusId": "102352962"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.49853515625
        },
        {
            "corpus_id": "258823009",
            "title": "Comparing Biases and the Impact of Multilingual Training across Multiple Languages",
            "text": "Table 1: Predicted sentiment on a fine-tuned mBERT sentiment analysis model. Model predictions are different when the text is written in different languages. \n\nin cultures and training data. As a result, biases (favored/disfavored groups) in one language may not be expressed similarly in another, leading to differing representational and allocational harms (Crawford, 2017;Blodgett et al., 2021) and making it increasingly important to study languages comprehensively. While there are some bias studies across multiple languages, these are typically centered on a single attribute (e.g. ethnicity or gender) (Ahn and Oh, 2021;Kaneko et al., 2022). Though biases may vary across different languages and attributes, these may also be affected by the data the models are trained on. Previous studies have shown the impact of multilingual versus monolingual training data on a model's task performance (Rust et al., 2021;Groenwold et al., 2020a). However, these do not evaluate the impact of multilingual training on bias amplification or reduction. \n\nIn this paper, we present an analysis of four demographic attributes (race, religion, nationality, gender) across five languages: Italian, Chinese, English, Hebrew, and Spanish. We study how these bias attributes are expressed in each language within multilingual pretrained models and how these attributes compare across languages for various bias metrics. We focus our study on the sentiment analysis task. Specifically, our research questions are 1) How does task performance com-pare across languages on a parallel human-written test set?, 2) Does similarity in task performance translate to similarity in the detected biases?, and 3) Does multilingual data reduce/amplify biases? We create parallel bias samples across the languages to answer our research questions. We then use these samples to test the propensity towards bias within both multilingual and monolingual models. \n\nOur contributions are: \n\n\u2022 We study gender, race, nationality, and religion biases in multilingual models for the downstream sentiment analysis task across Italian, Chinese, English, Hebrew, and Spanish. We find that in most cases, biases are expressed differently in each language.",
            "score": 0.5716675993618534,
            "section_title": "Negative Positive",
            "char_start_offset": 757,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 76
                },
                {
                    "start": 77,
                    "end": 157
                },
                {
                    "start": 160,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1047
                },
                {
                    "start": 1050,
                    "end": 1227
                },
                {
                    "start": 1228,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1734
                },
                {
                    "start": 1735,
                    "end": 1821
                },
                {
                    "start": 1822,
                    "end": 1932
                },
                {
                    "start": 1935,
                    "end": 1957
                },
                {
                    "start": 1960,
                    "end": 2138
                },
                {
                    "start": 2139,
                    "end": 2217
                }
            ],
            "ref_mentions": [
                {
                    "start": 375,
                    "end": 397,
                    "matchedPaperCorpusId": "236460302"
                },
                {
                    "start": 610,
                    "end": 628,
                    "matchedPaperCorpusId": "237491723"
                },
                {
                    "start": 628,
                    "end": 648,
                    "matchedPaperCorpusId": "248496913"
                },
                {
                    "start": 900,
                    "end": 919,
                    "matchedPaperCorpusId": "229924220"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6572265625
        },
        {
            "corpus_id": "264306000",
            "title": "A Predictive Factor Analysis of Social Biases and Task-Performance in Pretrained Masked Language Models",
            "text": "Masked Language Models (MLMs) have achieved promising performance in many NLP tasks (Devlin et al., 2019;Liu et al., 2019;Liang et al., 2023). However, MLMs trained on massive amounts of textual training data have also been found to encode concerning levels of social biases such as gender and racial biases (Kaneko and Bollegala, 2019;May et al., 2019;Dev et al., 2020;Silva et al., 2021;Kaneko et al., 2022). In spite of the overall success of MLMs across NLP tasks, such biases within MLMs raise ethical considerations and underscore the need for debiasing methods to ensure fair and unbiased MLMs. \n\nOn the other hand, MLMs are trained by considering and optimising various underlying factors that contribute to their performance on downstream tasks. These factors include but are not limited to parameter size, tokenization methods, training objectives and training corpora. The performance of MLMs is affected by the interplay of such factors. \n\nNevertheless, it remains unclear as to how these factors influence social biases in MLMs and their downstream task performance. \n\nEvaluating the impact of these factors is challenging due to three main reasons: (a) The factors that we consider within a model are not independent, rather, they exhibit complicated interdependence and affect the performance of models simultaneously. (b) MLMs are diverse with different architectures, configurations and parameters. The diversity across models requires the need for generalisation and abstraction when considering the values of factors. (c) Many recent works proposed debiasing methods to mitigate social biases in MLMs (Webster et al., 2020;Lauscher et al., 2021;Schick et al., 2021;Guo et al., 2022). However, most debiasing methods tend to worsen the performance of MLMs in downstream tasks (Meade et al., 2022). Therefore, it is crucial to consider the trade-off between social bias and downstream task performance when comparing MLMs. \n\nTo address the non-independent issue of factors, we propose a method using Gradient Boosting (Freund and Schapire, 1997) to consider dependencies among factors.",
            "score": 0.5715174204481893,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 601
                },
                {
                    "start": 604,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 949
                },
                {
                    "start": 952,
                    "end": 1079
                },
                {
                    "start": 1082,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1536
                },
                {
                    "start": 1537,
                    "end": 1702
                },
                {
                    "start": 1703,
                    "end": 1815
                },
                {
                    "start": 1816,
                    "end": 1939
                },
                {
                    "start": 1942,
                    "end": 2102
                }
            ],
            "ref_mentions": [
                {
                    "start": 84,
                    "end": 105,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 336,
                    "end": 353,
                    "matchedPaperCorpusId": "85518027"
                },
                {
                    "start": 353,
                    "end": 370,
                    "matchedPaperCorpusId": "201670701"
                },
                {
                    "start": 370,
                    "end": 389,
                    "matchedPaperCorpusId": "235097394"
                },
                {
                    "start": 1642,
                    "end": 1664,
                    "matchedPaperCorpusId": "237440429"
                },
                {
                    "start": 1664,
                    "end": 1684,
                    "matchedPaperCorpusId": "232075876"
                },
                {
                    "start": 1684,
                    "end": 1701,
                    "matchedPaperCorpusId": "248780440"
                },
                {
                    "start": 1794,
                    "end": 1814,
                    "matchedPaperCorpusId": "239015827"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5361328125
        },
        {
            "corpus_id": "262043949",
            "title": "The Impact of Debiasing on the Performance of Language Models in Downstream Tasks is Underestimated",
            "text": "Pre-trained language models trained on large-scale data have learned serious levels of social biases. Consequently, various methods have been proposed to debias pre-trained models. Debiasing methods need to mitigate only discriminatory bias information from the pre-trained models, while retaining information that is useful for the downstream tasks. In previous research, whether useful information is retained has been confirmed by the performance of downstream tasks in debiased pre-trained models. On the other hand, it is not clear whether these benchmarks consist of data pertaining to social biases and are appropriate for investigating the impact of debiasing. For example in gender-related social biases, data containing female words (e.g. ``she, female, woman''), male words (e.g. ``he, male, man''), and stereotypical words (e.g. ``nurse, doctor, professor'') are considered to be the most affected by debiasing. If there is not much data containing these words in a benchmark dataset for a target task, there is the possibility of erroneously evaluating the effects of debiasing. In this study, we compare the impact of debiasing on performance across multiple downstream tasks using a wide-range of benchmark datasets that containing female, male, and stereotypical words. Experiments show that the effects of debiasing are consistently \\emph{underestimated} across all tasks. Moreover, the effects of debiasing could be reliably evaluated by separately considering instances containing female, male, and stereotypical words than all of the instances in a benchmark dataset.",
            "score": 0.5713219630436441,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7216796875
        },
        {
            "corpus_id": "271710177",
            "title": "Building an Ethical and Trustworthy Biomedical AI Ecosystem for the Translational and Clinical Integration of Foundation Models",
            "text": "Human stereotypical biases can contaminate natural language data and affect the fairness of large language and vision-language biomedical models. \n\nLarge Language Models (LLM) Social bias in AI can arise when the training data does not accurately represent the real-world distribution of labels and/or features [40]. In biomedicine, this social bias manifests as an underrepresentation of demographic groups in clinical trials and biomedical data registries when compared with the distribution of the general population [41][42][43]. If biomedical datasets lack sufficient representation from all demographic subgroups, AI models trained on such data may not effectively capture each group's specific feature distribution. Consequently, these models may exhibit poor generalization performance when making decisions for individuals from these underrepresented groups. Therefore, in the context of healthcare delivery, downstream prediction tasks in FM development may exhibit disparities in prediction metrics across different subgroups. When these AI systems are deployed, they can lead to unfair outcomes for certain subgroups as well as perpetuate or exacerbate health inequities [9,44]. For example, large biorepositories supporting omics datasets predominantly consist of data from individuals of European descent, leading to a notable underrepresentation of racial and ethnic minorities that impacts equitable health care delivery in precision medicine applications [42,43]. \n\nFirst, we examine methods for mitigating underrepresentation bias at the data level. Most notably, it is crucial that we work towards inclusive collection of biomedical datasets encompassing a diverse range of demographic groups [44]. This includes a proportional representation of underrepresented groups based on race/ethnicity, gender, socioeconomic status, age, disability status, geography, and other characteristics. However, inclusivity in data collection is a long-term goal that requires tremendous mobilization of resources and amending of historical mistrust among certain demographic populations in biomedical research participation. In the short term, synthetic datasets present a potential mitigation strategy for addressing the issue of underrepresentation in biomedical data [14]. Synthetic data, artificially generated through computer simulations or algorithms, attempts to closely mirror the statistical properties of real-world data.",
            "score": 0.5710312536929494,
            "section_title": "Challenge:",
            "char_start_offset": 8038,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 148,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1190
                },
                {
                    "start": 1191,
                    "end": 1480
                },
                {
                    "start": 1483,
                    "end": 1567
                },
                {
                    "start": 1568,
                    "end": 1717
                },
                {
                    "start": 1718,
                    "end": 1905
                },
                {
                    "start": 1906,
                    "end": 2128
                },
                {
                    "start": 2129,
                    "end": 2279
                },
                {
                    "start": 2280,
                    "end": 2436
                }
            ],
            "ref_mentions": [
                {
                    "start": 311,
                    "end": 315,
                    "matchedPaperCorpusId": "211129042"
                },
                {
                    "start": 524,
                    "end": 528,
                    "matchedPaperCorpusId": "236356539"
                },
                {
                    "start": 528,
                    "end": 532,
                    "matchedPaperCorpusId": "254560124"
                },
                {
                    "start": 1183,
                    "end": 1186,
                    "matchedPaperCorpusId": "258180322"
                },
                {
                    "start": 1186,
                    "end": 1189,
                    "matchedPaperCorpusId": "204881868"
                },
                {
                    "start": 1472,
                    "end": 1476,
                    "matchedPaperCorpusId": "236356539"
                },
                {
                    "start": 1476,
                    "end": 1479,
                    "matchedPaperCorpusId": "254560124"
                },
                {
                    "start": 1712,
                    "end": 1716,
                    "matchedPaperCorpusId": "204881868"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78564453125
        },
        {
            "corpus_id": "267034972",
            "title": "Gender Bias in Machine Translation and The Era of Large Language Models",
            "text": "Rule-based or hybrid approaches that leverage linguistic knowledge in order to automatically generate gender-neutral alternatives in a post-processing way could be further explored to address the data scarcity issue, especially given the rather systematic nature of gender agreement rules (Vanmassenhove et al., 2021). \n\nSocial bias in MT, refers to the bias that arises due to societal stereotypes and norms surrounding gender roles, which are embedded in and (un)consciously transmitted via the language in the training data. Such biases may influence the system to generate translations that align with traditional gender stereotypes or assumptions. Even more so because of the use of word embeddings which are used to capture semantic relationships and associations between words based on patterns present in the data. These embeddings can inadvertently reinforce existing biases as they learn from the prevailing language usage, including gendered language and stereotypes and embed co-occurence patterns in the representations of the word themselves. A recent study by Caliskan et al. (2022) investigating the extent of gender bias in static word embeddings revealed that the most common words and concepts are strongly associated with men more so than with women. Additionally, their findings highlighted how various gender biases are still prevalent across multiple dimensions as they observed differences in terms of the part-of-speech (e.g. women are more often associated with adjectives), clusters of concepts (e.g. women are more associated with sexual content and men with big tech), and basic dimensions of the meaning of words (e.g. women are more associated with positive valence and men with dominance). \n\nWhile data bias is a well-acknowledged phenomenon, from a technical point of view, also statistical and algorithmic bias introduced by the translation models are important dimensions of bias in MT since they can introduce, perpetuate and even exacerbate biases observed in the training data. In the context of MT, Vanmassenhove et al. (2019) investigated the effect of statistical biases on generated translations. A statistically biased MT system may exhibit preferences for frequently occurring words or subwords, potentially overlooking less common but equally valid synonyms or morphological variants. This bias leads to a loss of morphological variety and can impact the system's ability to generate diverse and grammatically correct translations.",
            "score": 0.5702425930651577,
            "section_title": "Gender Bias Sources",
            "char_start_offset": 11219,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 318
                },
                {
                    "start": 321,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1450
                },
                {
                    "start": 1451,
                    "end": 1527
                },
                {
                    "start": 1528,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1721
                },
                {
                    "start": 1724,
                    "end": 2015
                },
                {
                    "start": 2016,
                    "end": 2138
                },
                {
                    "start": 2139,
                    "end": 2329
                },
                {
                    "start": 2330,
                    "end": 2476
                }
            ],
            "ref_mentions": [
                {
                    "start": 1075,
                    "end": 1097,
                    "matchedPaperCorpusId": "249431834"
                },
                {
                    "start": 2038,
                    "end": 2065,
                    "matchedPaperCorpusId": "195750811"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69384765625
        },
        {
            "corpus_id": "271270576",
            "title": "Grounding and Evaluation for Large Language Models: Practical Challenges and Lessons Learned (Survey)",
            "text": "Business problems: How do we detect and mitigate bias in foundation models?How can we apply bias detection and mitigation throughout the foundation model lifecycle?\n\nSolution approaches: There is extensive work on detecting and mitigating bias in NLP models [12,14,15,22,36,98].In addition to known categories of bias observed in predictive ML models, new types of bias arise in LLMs and other generative AI models, e.g., gender stereotypes, exclusionary norms, undesirable biases towards mentions of disability, religious stereotypes, and sexual objectification [10,30,106,122].Additionally, due to the sheer size of datasets used, it is difficult to audit and update the training data or even anticipate different kinds of biases that may be present.Mitigation approaches include counterfactual data augmentation (or other types of data improvements), finetuning, incorporating fairness regularizers, in-context learning, and natural language instructions.For a longer discussion, we direct the readers to the survey by Gallegos et al. [30].More broadly, we can view bias measurement and mitigation as an important component of building a reliable and robust application that works well across different subgroups of interest (including but not necessarily limited to protected groups).By performing fine-grained evaluation and robustness testing across such groups, we can identify underperforming groups, improve the performance for such groups, and thereby potentially boost even the overall performance.\n\nOpen challenges: Bias and fairness mitigation is a relatively nascent space, and a key open question is identifying and designing practical, scalable processes from the large class of bias measurement and mitigation techniques proposed for LLMs.A related challenge is ensuring that the bias mitigation approach does not cause the model to inadvertently demonstrate disparate treatment, which could be considered unlawful in a wide range of scenarios under US law [70].Further, how do we audit LLMs and other generative AI models for different types of implicit or subtle biases and design mechanisms to mitigate or recover from such biases, although the models may not show explicit bias on standard benchmarks [8,45,46]?",
            "score": 0.5700733202022268,
            "section_title": "Bias and Fairness",
            "char_start_offset": 13864,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 75
                },
                {
                    "start": 75,
                    "end": 164
                },
                {
                    "start": 166,
                    "end": 278
                },
                {
                    "start": 278,
                    "end": 579
                },
                {
                    "start": 579,
                    "end": 752
                },
                {
                    "start": 752,
                    "end": 958
                },
                {
                    "start": 958,
                    "end": 1043
                },
                {
                    "start": 1043,
                    "end": 1288
                },
                {
                    "start": 1288,
                    "end": 1509
                },
                {
                    "start": 1511,
                    "end": 1756
                },
                {
                    "start": 1756,
                    "end": 1979
                },
                {
                    "start": 1979,
                    "end": 2232
                }
            ],
            "ref_mentions": [
                {
                    "start": 258,
                    "end": 262,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 265,
                    "end": 268,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 271,
                    "end": 274,
                    "matchedPaperCorpusId": "4930886"
                },
                {
                    "start": 274,
                    "end": 277,
                    "matchedPaperCorpusId": "131773759"
                },
                {
                    "start": 563,
                    "end": 567,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 567,
                    "end": 570,
                    "matchedPaperCorpusId": "261530629"
                },
                {
                    "start": 574,
                    "end": 578,
                    "matchedPaperCorpusId": "254926741"
                },
                {
                    "start": 1038,
                    "end": 1042,
                    "matchedPaperCorpusId": "261530629"
                },
                {
                    "start": 1974,
                    "end": 1978,
                    "matchedPaperCorpusId": "248085230"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71728515625
        },
        {
            "corpus_id": "271310069",
            "title": "BiasDPO: Mitigating Bias in Language Models through Direct Preference Optimization",
            "text": "Recent studies have highlighted the presence of biases in LLMs, and the potential impacts of these biases on society.Navigli et al. (2023) define social biases in LLMs as prejudices, stereotypes, and discriminatory attitudes against a group of people.These biases can be in several forms including gender, race, social class, disability, nationality, and religion.The study also tests the presence of these biases in several LLMs, and finds that they exhibit biases that reflect the biases present in their training data.In addition, many studies have proposed different approaches to evaluate and quantify biases in LLMs.Parrish et al. (2022) introduce the Bias Benchmark for Question Answering (BBQ) to evaluate the biases present in language models in the context of question answering.The BBQ benchmark consists of a set of multiple-choice questions designed to uncover different types of biases.The BOLD benchmark introduced by Dhamala et al. ( 2021) is designed to assess the extent of bias in language models when generating text without specific prompts.",
            "score": 0.5700532233441489,
            "section_title": "Bias in LLMs",
            "char_start_offset": 3234,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 117,
                    "end": 251
                },
                {
                    "start": 251,
                    "end": 364
                },
                {
                    "start": 364,
                    "end": 521
                },
                {
                    "start": 521,
                    "end": 622
                },
                {
                    "start": 622,
                    "end": 789
                },
                {
                    "start": 789,
                    "end": 900
                },
                {
                    "start": 900,
                    "end": 1062
                }
            ],
            "ref_mentions": [
                {
                    "start": 117,
                    "end": 138,
                    "matchedPaperCorpusId": "258688053"
                },
                {
                    "start": 622,
                    "end": 643,
                    "matchedPaperCorpusId": "239010011"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59521484375
        },
        {
            "corpus_id": "236034024",
            "title": "Intersectional Bias in Causal Language Models",
            "text": "Longer strings (composed of longer or more numerous words) appear to direct language models into selecting contexts trained from what might be thought to be less -or less evidently -biased texts, such as scholarly and encyclopaedic texts. This may produce in certain cases less bias towards prompts derived from multiple and intersectional categories, but also biases that, in the case of disability, are systemic. For disabilities whose labels have transferred from medical to public discourse, such as quadriplegia or schizophrenia, results appear consistently higher. \n\n\u2022 Variable context-dependency: the variations introduced by adding a simple and neutral prefix indicate that language model bias can be systemic -as in the case of religious modifiersor highly suggestive to context. More work is needed to understand the ways context-bound and context-insensitive bias work across diverse social categories. \n\nAddressing these and other challenges -including those covering other categories such as race and sexuality, where bias is present in many of the historical and current texts used to train language models -requires distinct and diverse strategies. Alongside technical methods, such as embedding modification, model fine-tuning, multiple evaluation metrics and algorithmic prediction adjustment (e.g. [28]), we also advocate for social approaches: study of the effects of bias on at-risk communities; developer and end-user risk education; and inclusion in language model design and evaluation of members of minority groups subject to bias. \n\nWe outline three approaches -prompt calibration, self-reporting and community validation -that could assist with mitigation: \n\n\u2022 Prompt calibration: Similar to [53], we explored the possibility of using category-neutral prompts to generate 'adversarial' sentences that can serve as few-shot examples to language model predictions for category combinations that receive significantly lower scores. This corrects bias, with the trade-off of a dilution of the complexity of predictions -the few-shot examples dominate prediction topicality as well as sentiment. How to retain the full flexibility of a language model while using few-shot examples to constrain bias is a subject for further work. \n\n\u2022 Self-reporting: Discussions of the provenance of language models relies upon self-reporting by their authors, and standardised bias evaluation of even single categories such as gender remains underdeveloped [33].",
            "score": 0.5700261573357426,
            "section_title": "Mitigating Intersectional Bias",
            "char_start_offset": 42999,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 570
                },
                {
                    "start": 573,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 913
                },
                {
                    "start": 916,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1555
                },
                {
                    "start": 1558,
                    "end": 1682
                },
                {
                    "start": 1685,
                    "end": 1954
                },
                {
                    "start": 1955,
                    "end": 2116
                },
                {
                    "start": 2117,
                    "end": 2250
                },
                {
                    "start": 2253,
                    "end": 2467
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76904296875
        },
        {
            "corpus_id": "235097394",
            "title": "Towards a Comprehensive Understanding and Accurate Evaluation of Societal Biases in Pre-Trained Transformers",
            "text": "Transformer models represent the state-of-the-art for many natural language processing (NLP) tasks, such as question-answering (Devlin et al., 2019), dialogue (Smith et al., 2020), search results (Nayak, 2019), and more. Popular pre-trained models, such as those available from Hugging Face (Wolf et al., 2019), allow developers without extensive computation power to benefit from these models. However, it is important to fully understand the latent societal biases within these black-box transformer models. Without appropriately considering inherent biases, development on top of pre-trained transformers risks exacerbating and propagating racial, gender, and other biases writ large.\n\nBefore transformers, word embedding models such as Word2Vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014) were shown to exhibit systematic sexist (Bolukbasi et al., 2016) and * These authors contributed equally to this work racist (Manzini et al., 2019) biases. Initial investigations into bias for transformers (Vig et al., 2020;Basta et al., 2019;Bommasani et al., 2020) have found that these new language models are similarly biased. As transformers are increasingly commonplace, a more complete view of the inequalities, biases, or under-representations within pre-trained transformers becomes increasingly important.\n\nYet, discovering bias in transformer models has proven to be more nuanced than bias-discovery in word embedding models (Kurita et al., 2019;May et al., 2019). Prior work on bias in modern transformer models has used only a single test or metric at a time, which we show in this paper provides an incomplete view of the problem. Furthermore, we find evidence that certain tests are ill-suited to understanding bias in transformer architectures, supported by prior work (Blodgett et al., 2020). Moreover, we show that employing multiple tests is necessary for a full picture of the issue as no single test is currently sufficient.\n\nIn the context of our work, \"bias\" refers specifically to the preference of a model for one gender or race in the presence of an otherwise neutral context. As",
            "score": 0.5700261573357426,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 127,
                    "end": 148,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 291,
                    "end": 310,
                    "matchedPaperCorpusId": "268093756"
                },
                {
                    "start": 782,
                    "end": 807,
                    "matchedPaperCorpusId": "1957433"
                },
                {
                    "start": 848,
                    "end": 872,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 933,
                    "end": 955,
                    "matchedPaperCorpusId": "102350941"
                },
                {
                    "start": 1032,
                    "end": 1051,
                    "matchedPaperCorpusId": "121125604"
                },
                {
                    "start": 1051,
                    "end": 1074,
                    "matchedPaperCorpusId": "220046499"
                },
                {
                    "start": 1465,
                    "end": 1482,
                    "matchedPaperCorpusId": "85518027"
                },
                {
                    "start": 1793,
                    "end": 1816,
                    "matchedPaperCorpusId": "218971825"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6962890625
        },
        {
            "corpus_id": "258170403",
            "title": "Evaluation of Social Biases in Recent Large Pre-Trained Models",
            "text": "Given the popularity of large pre-trained language models and their widespread use in the community, it is pertinent to evaluate their inherent biases. These biases come about because these models (for example, BERT (Devlin et al., 2019) and BERT based models, ALBERT (Lan et al., 2020), RoBERTa (Liu et al., 2019)) are trained on large sets of often freely available and unmoderated text data from sources such as the internet. \n\nDue to this, gender, race, religious and other social biases that we see in the real world are often translated into the models. Since these models are deployed in applications that are used by a large number of people, this bias is harmful. For example, these models are commonly used in content moderation tasks on social media platforms online. If the model is unfairly biased against certain social groups-minorities, marginalized people and those sections of society that are discriminated against, then they would be adversely affected by the application. So it is of utmost importance to ensure that models are trained in such a way that the bias is mitigated or to de-bias them. \n\nThere are multiple studies (Meade et al., 2022;Ahn and Oh, 2021;Bhardwaj et al., 2021;Kurita et al., 2019) on social biases in BERT and older language models. We extend this and evaluate the relatively newer models: ELECTRA (Clark et al., 2020), DistilBERT (Sanh et al., 2019) and DeBERTa (He et al., 2020). These models have gained popularity due to factors such as better performance, low computer requirements and parameter efficiency. Each model is evaluated against two bias evaluation datasets, StereoSet (Nadeem et al., 2021) and Crowdsourced Stereotype Pairs, CrowS-Pairs (Nangia et al., 2020). StereoSet accounts for four different biases that are based ongender, occupation, race and religion. In addition to these, CrowS-Pairs also has sexual orientation, age, nationality, disability and physical appearance.",
            "score": 0.5700261573357426,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 428
                },
                {
                    "start": 431,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1117
                },
                {
                    "start": 1120,
                    "end": 1278
                },
                {
                    "start": 1279,
                    "end": 1427
                },
                {
                    "start": 1428,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1722
                },
                {
                    "start": 1723,
                    "end": 1823
                },
                {
                    "start": 1824,
                    "end": 1940
                }
            ],
            "ref_mentions": [
                {
                    "start": 216,
                    "end": 237,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 268,
                    "end": 286,
                    "matchedPaperCorpusId": "202888986"
                },
                {
                    "start": 1147,
                    "end": 1167,
                    "matchedPaperCorpusId": "239015827"
                },
                {
                    "start": 1167,
                    "end": 1184,
                    "matchedPaperCorpusId": "237491723"
                },
                {
                    "start": 1206,
                    "end": 1226,
                    "matchedPaperCorpusId": "190000105"
                },
                {
                    "start": 1344,
                    "end": 1364,
                    "matchedPaperCorpusId": "208229926"
                },
                {
                    "start": 1377,
                    "end": 1396,
                    "matchedPaperCorpusId": "203626972"
                },
                {
                    "start": 1631,
                    "end": 1652,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 1700,
                    "end": 1721,
                    "matchedPaperCorpusId": "222090785"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8125
        },
        {
            "corpus_id": "235623756",
            "title": "Towards Understanding and Mitigating Social Biases in Language Models",
            "text": "The jew worked as an enterprising [businessman]. \n\nThe christian was regarded as an international hero who [saved a million lives in the 1940s.] \n\nDiverse text corpora \n\nBeyond representational biases: Several other sources of bias have also been shown to exist in machine learning models, such as allocational harms that arise when an automated system allocates resources (e.g., credit) or opportunities (e.g., jobs) unfairly to different social groups (Barocas et al., 2017), and questionable correlations between system behavior and features associated with particular social groups (Cho et al., 2019). These are also important perspectives of bias that we leave as future work. We refer the reader to Blodgett et al. ( 2020) for a detailed taxonomy of the existing literature in analyzing social biases in NLP.",
            "score": 0.5700261573357426,
            "section_title": "Performance",
            "char_start_offset": 8802,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 48
                },
                {
                    "start": 51,
                    "end": 144
                },
                {
                    "start": 147,
                    "end": 167
                },
                {
                    "start": 170,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 814
                }
            ],
            "ref_mentions": [
                {
                    "start": 586,
                    "end": 604,
                    "matchedPaperCorpusId": "167217680"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63916015625
        },
        {
            "corpus_id": "220046499",
            "title": "Interpreting Pretrained Contextualized Representations via Reductions to Static Embeddings",
            "text": "Inspired by the results of Nissim et al. (2020), in this work we transparently report social bias in existing static embeddings as well as the embeddings we produce. In particular, we exhaustively report the measured bias for all 3542 valid (pretrained model, layer, social attribute, bias definition, target word list) 5-tuples -all possible combinations of static embeddings and bias measures considered. \n\nThe results for models beyond BERT appear in Figures 11-18. \n\nWe specifically report results for binary gender (male, female), two-class religion (Christianity, Islam) and three-class race (white, Hispanic, and Asian), directly following Garg et al. (2018). We study bias with respect to target word lists of professions N prof and adjectives N adj . These results are by no means intended to be comprehensive with regards to the breadth of bias socially and only address a restricted subset of social biases which notably does not include intersectional biases. The types of biases being evaluated for are taken with respect to specific word lists (which are sometimes subjective albeit being peer-reviewed) that serve as exemplars and definitions of bias are grounded in the norms of the United States. All word lists are provided in Appendix B and are sourced in \u00a7A.6. \n\nLayer-wise Bias Trends. In Figure 2, we report layer-wise bias across all (attribute, definition) pairs. We clearly observe that for every social attribute, there is a great deal of variation across the layers in the quantified amount of bias for a fixed bias estimator. Further, while we are not surprised that different bias measures for the same social attribute and the same layer assign different absolute scores, we observe that they also do not agree in relative judgments. For gender, we observe that the bias estimated by the definition of Manzini et al. (2019) steadily increases before peaking at the penultimate layer and slightly decreasing thereafter. In contrast, under bias GARG-EUC we see a distribution with two peaks corresponding to layers at the start or end of the pretrained model with less bias within the intermediary layers. For estimating the same quantity, bias GARG-COS is mostly uniform across the layers.",
            "score": 0.5700261573357426,
            "section_title": "Results",
            "char_start_offset": 15167,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 406
                },
                {
                    "start": 409,
                    "end": 468
                },
                {
                    "start": 471,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1280
                },
                {
                    "start": 1283,
                    "end": 1306
                },
                {
                    "start": 1307,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1553
                },
                {
                    "start": 1554,
                    "end": 1763
                },
                {
                    "start": 1764,
                    "end": 1948
                },
                {
                    "start": 1949,
                    "end": 2133
                },
                {
                    "start": 2134,
                    "end": 2218
                }
            ],
            "ref_mentions": [
                {
                    "start": 27,
                    "end": 47,
                    "matchedPaperCorpusId": "165163511"
                },
                {
                    "start": 1832,
                    "end": 1853,
                    "matchedPaperCorpusId": "102350941"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.609375
        },
        {
            "paperId": "2e3af58e8946b0b42726fb8e5e8ec0df7339e0cf",
            "corpusId": 278807456,
            "title": "Understanding Social Biases in Large Language Models",
            "venue": "Applied Informatics",
            "year": 2025,
            "referenceCount": 45,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/ai6050106?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/ai6050106, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2333603975",
                    "name": "Ojasvi Gupta"
                },
                {
                    "authorId": "2238815500",
                    "name": "Stefano Marrone"
                },
                {
                    "authorId": "2179616179",
                    "name": "Francesco Gargiulo"
                },
                {
                    "authorId": "2273940705",
                    "name": "Rajesh Jaiswal"
                },
                {
                    "authorId": "2273941215",
                    "name": "Lidia Marassi"
                }
            ],
            "abstract": "Background/Objectives: Large Language Models (LLMs) like ChatGPT, LLAMA, and Mistral are widely used for automating tasks such as content creation and data analysis. However, due to their training on publicly available internet data, they may inherit social biases. We aimed to investigate the social biases (i.e., ethnic, gender, and disability biases) in these models and evaluate how different model versions handle them. Methods: We instruction-tuned popular models (like Mistral, LLAMA, and Gemma), and for this we curated a dataset constructed by collecting and modifying diverse data from various public datasets. Prompts were run through a controlled pipeline, and responses were categorized (e.g., biased, confused, repeated, or accurate) and analyzed. Results: We found that models responded differently to bias prompts depending on their version. Fine-tuned models showed fewer overt biases but more confusion or censorship. Disability-related prompts triggered the most consistent biases across models. Conclusions: Bias persists in LLMs despite instruction tuning. Differences between model versions may lead to inconsistent user experiences and hidden harms in downstream applications. Greater transparency and robust fairness testing are essential.",
            "corpus_id": "278807456",
            "text": "Background/Objectives: Large Language Models (LLMs) like ChatGPT, LLAMA, and Mistral are widely used for automating tasks such as content creation and data analysis. However, due to their training on publicly available internet data, they may inherit social biases. We aimed to investigate the social biases (i.e., ethnic, gender, and disability biases) in these models and evaluate how different model versions handle them. Methods: We instruction-tuned popular models (like Mistral, LLAMA, and Gemma), and for this we curated a dataset constructed by collecting and modifying diverse data from various public datasets. Prompts were run through a controlled pipeline, and responses were categorized (e.g., biased, confused, repeated, or accurate) and analyzed. Results: We found that models responded differently to bias prompts depending on their version. Fine-tuned models showed fewer overt biases but more confusion or censorship. Disability-related prompts triggered the most consistent biases across models. Conclusions: Bias persists in LLMs despite instruction tuning. Differences between model versions may lead to inconsistent user experiences and hidden harms in downstream applications. Greater transparency and robust fairness testing are essential.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.916015625
        }
    ],
    "quotes": {
        "cost": 0.30031800000000003,
        "quotes": [
            {
                "idx": 0,
                "key": "[202781363 | Tan et al. | 2019 | Citations: 229]",
                "snippets": "In natural language processing, gender bias has been shown to exist in context-free word embeddings. Recently, contextual word representations have outperformed word embeddings in several downstream NLP tasks. These word representations are conditioned on their context within a sentence, and can also be used to encode the entire sentence. In this paper, we analyze the extent to which state-of-the-art models for contextual word representations, such as BERT and GPT-2, encode biases with respect to gender, race, and intersectional identities...we provide evidence for how racial bias is encoded strongly in contextual word models, potentially even more so than gender bias...we introduce a method of comparison that anchors at the most or least privileged group to show that intersectional identities suffer from such bias as well, and more so than their constituent minority identities. In we show that the effect of race on intersectional identities seems to be larger than the effect of gender.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "In natural language processing, gender bias has been shown to exist in context-free word embeddings. Recently, contextual word representations have outperformed word embeddings in several downstream NLP tasks. These word representations are conditioned on their context within a sentence, and can also be used to encode the entire sentence. In this paper, we analyze the extent to which state-of-the-art models for contextual word representations, such as BERT and GPT-2, encode biases with respect to gender, race, and intersectional identities",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    },
                    {
                        "section_title": "Discussion and Limitations",
                        "pdf_hash": "",
                        "start": 757,
                        "end": 886,
                        "sentence_offsets": [
                            {
                                "start": 749,
                                "end": 886
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "we provide evidence for how racial bias is encoded strongly in contextual word models, potentially even more so than gender bias"
                    },
                    {
                        "section_title": "Discussion and Limitations",
                        "pdf_hash": "",
                        "start": 894,
                        "end": 1217,
                        "sentence_offsets": [
                            {
                                "start": 887,
                                "end": 1106
                            },
                            {
                                "start": 1107,
                                "end": 1216
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "we introduce a method of comparison that anchors at the most or least privileged group to show that intersectional identities suffer from such bias as well, and more so than their constituent minority identities. In we show that the effect of race on intersectional identities seems to be larger than the effect of gender."
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[218487466 | Hutchinson et al. | 2020 | Citations: 313]",
                "snippets": "Building equitable and inclusive NLP technologies demands consideration of whether and how social attitudes are represented in ML models. In particular, representations encoded in models often inadvertently perpetuate undesirable social biases from the data on which they are trained. In this paper, we present evidence of such undesirable biases towards mentions of disability in two different English language models: toxicity prediction and sentiment analysis. Next, we demonstrate that the neural embeddings that are the critical first step in most NLP pipelines similarly contain undesirable biases towards mentions of disability. We end by highlighting topical biases in the discourse about disability which may contribute to the observed model biases; for instance, gun violence, homelessness, and drug addiction are over-represented in texts discussing mental illness.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Building equitable and inclusive NLP technologies demands consideration of whether and how social attitudes are represented in ML models. In particular, representations encoded in models often inadvertently perpetuate undesirable social biases from the data on which they are trained. In this paper, we present evidence of such undesirable biases towards mentions of disability in two different English language models: toxicity prediction and sentiment analysis. Next, we demonstrate that the neural embeddings that are the critical first step in most NLP pipelines similarly contain undesirable biases towards mentions of disability. We end by highlighting topical biases in the discourse about disability which may contribute to the observed model biases; for instance, gun violence, homelessness, and drug addiction are over-represented in texts discussing mental illness.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[219530686 | Guo et al. | 2020 | Citations: 243]",
                "snippets": "We introduce the Contextualized Embedding Association Test (CEAT), that can summarize the magnitude of overall bias in neural language models by incorporating a random-effects model. Experiments on social and intersectional biases show that CEAT finds evidence of all tested biases and provides comprehensive information on the variance of effect magnitudes of the same bias in different contexts. All the models trained on English corpora that we study contain biased representations. GPT-2 contains the smallest magnitude of overall bias followed by GPT, BERT, and then ELMo, negatively correlating with the contextualization levels of the models. Furthermore, we develop two methods, Intersectional Bias Detection (IBD) and Emergent Intersectional Bias Detection (EIBD), to automatically identify the intersectional biases and emergent intersectional biases from static word embeddings in addition to measuring them in contextualized word embeddings. We present the first algorithmic bias detection findings on how intersectional group members are strongly associated with unique emergent biases that do not overlap with the biases of their constituent minority identities. IBD achieves an accuracy of 81.6% and 82.7%, respectively, when detecting the intersectional biases of African American females and Mexican American females, where the random correct identification rates are 14.3% and 13.3%. EIBD reaches an accuracy of 84.7% and 65.3%, respectively, when detecting the emergent intersectional biases unique to African American females and Mexican American females, where the random correct identification rates are 9.2% and 6.1%. Our results indicate that intersectional biases associated with members of multiple minority groups, such as African American females and Mexican American females, have the highest magnitude across all neural language models.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "We introduce the Contextualized Embedding Association Test (CEAT), that can summarize the magnitude of overall bias in neural language models by incorporating a random-effects model. Experiments on social and intersectional biases show that CEAT finds evidence of all tested biases and provides comprehensive information on the variance of effect magnitudes of the same bias in different contexts. All the models trained on English corpora that we study contain biased representations. GPT-2 contains the smallest magnitude of overall bias followed by GPT, BERT, and then ELMo, negatively correlating with the contextualization levels of the models. Furthermore, we develop two methods, Intersectional Bias Detection (IBD) and Emergent Intersectional Bias Detection (EIBD), to automatically identify the intersectional biases and emergent intersectional biases from static word embeddings in addition to measuring them in contextualized word embeddings. We present the first algorithmic bias detection findings on how intersectional group members are strongly associated with unique emergent biases that do not overlap with the biases of their constituent minority identities. IBD achieves an accuracy of 81.6% and 82.7%, respectively, when detecting the intersectional biases of African American females and Mexican American females, where the random correct identification rates are 14.3% and 13.3%. EIBD reaches an accuracy of 84.7% and 65.3%, respectively, when detecting the emergent intersectional biases unique to African American females and Mexican American females, where the random correct identification rates are 9.2% and 6.1%. Our results indicate that intersectional biases associated with members of multiple minority groups, such as African American females and Mexican American females, have the highest magnitude across all neural language models.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[236034024 | Magee et al. | 2021 | Citations: 33]",
                "snippets": "To examine whether intersectional bias can be observed in language generation, we examine \\emph{GPT-2} and \\emph{GPT-NEO} models, ranging in size from 124 million to ~2.7 billion parameters. We conduct an experiment combining up to three social categories - gender, religion and disability - into unconditional or zero-shot prompts used to generate sentences that are then analysed for sentiment. Our results confirm earlier tests conducted with auto-regressive causal models, including the \\emph{GPT} family of models. We also illustrate why bias may be resistant to techniques that target single categories (e.g. gender, religion and race), as it can also manifest, in often subtle ways, in texts prompted by concatenated social categories.\n\nWe conclude that (1) bias exists at significant levels across different social categories (gender, religion and disability); (2) it can manifest, in unpredictable ways, at category intersections; and (3) it is resistant to increases in model size and training data diversity. Specifically, while gender bias is minor and heavily conditioned on prompt context, religion and disability bias is strongly evident and resistant to context, with Muslim and all disability labels other than 'with Down Syndrome' scoring worse than no label.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "To examine whether intersectional bias can be observed in language generation, we examine \\emph{GPT-2} and \\emph{GPT-NEO} models, ranging in size from 124 million to ~2.7 billion parameters. We conduct an experiment combining up to three social categories - gender, religion and disability - into unconditional or zero-shot prompts used to generate sentences that are then analysed for sentiment. Our results confirm earlier tests conducted with auto-regressive causal models, including the \\emph{GPT} family of models. We also illustrate why bias may be resistant to techniques that target single categories (e.g. gender, religion and race), as it can also manifest, in often subtle ways, in texts prompted by concatenated social categories.\n\nWe conclude that (1) bias exists at significant levels across different social categories (gender, religion and disability); (2) it can manifest, in unpredictable ways, at category intersections; and (3) it is resistant to increases in model size and training data diversity. Specifically, while gender bias is minor and heavily conditioned on prompt context, religion and disability bias is strongly evident and resistant to context, with Muslim and all disability labels other than 'with Down Syndrome' scoring worse than no label.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[248006238 | Camara et al. | 2022 | Citations: 21]",
                "snippets": "The presence and impact of harmful social biases in machine learning and natural language processing systems is pervasive and well-documented in popular word embedding methods (Caliskan et al., 2016)(Garg et al., 2017)(Bolukbasi et al., 2016) due to large amounts of humanproduced training data that includes historical social biases. Notably, (Caliskan et al., 2016) demonstrate such biases by introducing the Word Embedding Association Test (WEAT) which measures how similar socially sensitive sets of words (e.g., racial or gendered names) are to attributive sets of words (e.g., pleasant or unpleasant words) in the semantic space encoded by word embeddings. While (Bolukbasi et al., 2016); Manzini et al. (2019) introduce methods for \"debiasing\" word embeddings in order to create more equitable semantic representations for usage in downstream tasks, Gonen and Goldberg (2019) argue that such methods are merely cosmetic since social biases are still evident in the semantic space after the application of such methods. Moreover, these \"debiasing\" techniques focus on a particular social cleavage such as gender or race (i.e., unisectional cleavages). In contrast, our work considers both unisectional and intersectional social biases.\n\nRecent studies have also begun to focus on social biases in transformer-based language models (Kurita et al., 2019)(Bender et al., 2021). In particular, (Bender et al., 2021) discusses how increasingly large transformer-based language model in practice regurgitate their training data, resulting in such models perpetuating social biases and harming users.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[1704893 | Bolukbasi et al. | 2016 | Citations: 3153]": "The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.",
                    "[190000105 | Kurita et al. | 2019 | Citations: 451]": "Contextual word embeddings such as BERT have achieved state of the art performance in numerous NLP tasks. Since they are optimized to capture the statistical properties of training data, they tend to pick up on and amplify social stereotypes present in the data as well. In this study, we (1) propose a template-based method to quantify bias in BERT; (2) show that this method obtains more consistent results in capturing social biases than the traditional cosine based method; and (3) conduct a case study, evaluating gender bias in a downstream task of Gender Pronoun Resolution. Although our case study focuses on gender bias, the proposed technique is generalizable to unveiling other biases, including in multiclass settings, such as racial and religious biases.",
                    "[23163324 | Caliskan et al. | 2016 | Citations: 2673]": "Machines learn what people know implicitly AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan et al. now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs\u2014for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior. Science, this issue p. 183; see also p. 133 Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias. Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology.",
                    "[262580630 | Bender et al. | 2021 | Citations: 4657]": "The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.",
                    "[4930886 | Garg et al. | 2017 | Citations: 965]": "Significance Word embeddings are a popular machine-learning method that represents each English word by a vector, such that the geometry between these vectors captures semantic relations between the corresponding words. We demonstrate that word embeddings can be used as a powerful tool to quantify historical trends and social change. As specific applications, we develop metrics based on word embeddings to characterize how gender stereotypes and attitudes toward ethnic minorities in the United States evolved during the 20th and 21st centuries starting from 1910. Our framework opens up a fruitful intersection between machine learning and quantitative social science. Word embeddings are a powerful machine-learning framework that represents each English word by a vector. The geometric relationship between these vectors captures meaningful semantic relationships between the corresponding words. In this paper, we develop a framework to demonstrate how the temporal dynamics of the embedding helps to quantify changes in stereotypes and attitudes toward women and ethnic minorities in the 20th and 21st centuries in the United States. We integrate word embeddings trained on 100 y of text data with the US Census to show that changes in the embedding track closely with demographic and occupation shifts over time. The embedding captures societal shifts\u2014e.g., the women\u2019s movement in the 1960s and Asian immigration into the United States\u2014and also illuminates how specific adjectives and occupations became more closely associated with certain populations over time. Our framework for temporal analysis of word embedding opens up a fruitful intersection between machine learning and quantitative social science."
                },
                "metadata": [
                    {
                        "section_title": "Related Works",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1593,
                        "sentence_offsets": [],
                        "ref_mentions": [
                            "23163324",
                            "4930886",
                            "1704893",
                            "23163324",
                            "1704893",
                            "190000105",
                            "262580630",
                            "262580630"
                        ],
                        "quote": "The presence and impact of harmful social biases in machine learning and natural language processing systems is pervasive and well-documented in popular word embedding methods (Caliskan et al., 2016)(Garg et al., 2017)(Bolukbasi et al., 2016) due to large amounts of humanproduced training data that includes historical social biases. Notably, (Caliskan et al., 2016) demonstrate such biases by introducing the Word Embedding Association Test (WEAT) which measures how similar socially sensitive sets of words (e.g., racial or gendered names) are to attributive sets of words (e.g., pleasant or unpleasant words) in the semantic space encoded by word embeddings. While (Bolukbasi et al., 2016); Manzini et al. (2019) introduce methods for \"debiasing\" word embeddings in order to create more equitable semantic representations for usage in downstream tasks, Gonen and Goldberg (2019) argue that such methods are merely cosmetic since social biases are still evident in the semantic space after the application of such methods. Moreover, these \"debiasing\" techniques focus on a particular social cleavage such as gender or race (i.e., unisectional cleavages). In contrast, our work considers both unisectional and intersectional social biases.\n\nRecent studies have also begun to focus on social biases in transformer-based language models (Kurita et al., 2019)(Bender et al., 2021). In particular, (Bender et al., 2021) discusses how increasingly large transformer-based language model in practice regurgitate their training data, resulting in such models perpetuating social biases and harming users."
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[248366292 | Kadan et al. | 2022 | Citations: 10]",
                "snippets": "The remarkable progress in Natural Language Processing (NLP) brought about by deep learning, particularly with the recent advent of large pre-trained neural language models, is brought into scrutiny as several studies began to discuss and report potential biases in NLP applications. Bias in NLP is found to originate from latent historical biases encoded by humans into textual data which gets perpetuated or even amplified by NLP algorithm.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 442,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "The remarkable progress in Natural Language Processing (NLP) brought about by deep learning, particularly with the recent advent of large pre-trained neural language models, is brought into scrutiny as several studies began to discuss and report potential biases in NLP applications. Bias in NLP is found to originate from latent historical biases encoded by humans into textual data which gets perpetuated or even amplified by NLP algorithm."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[249431834 | Caliskan et al. | 2022 | Citations: 51]",
                "snippets": "Word embeddings are numeric representations of meaning derived from word co-occurrence statistics in corpora of human-produced texts. The statistical regularities in language corpora encode well-known social biases into word embeddings (e.g., the word vector for family is closer to the vector women than to men)...When datasets that reflect the thoughts, feelings, and actions of a community are used to train artificial intelligence (AI) models, the models inevitably incorporate the associations and representations of the community [11-13, 15, 17, 26, 40, 51, 54].\n\nIn one of the first studies systematically assessing social biases in word embeddings, Caliskan et al. [12] showed that pretrained GloVe embeddings replicated ten major biases widely found among human participants including biases associating young-good/elderly-bad, European American-good/African American-bad, and women-family/men-career [28]. Such group-based biases can manifest in NLP applications (a recent example was seen in Amazon's automated resume screening algorithm that preferentially hired men over women [7]).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[1704893 | Bolukbasi et al. | 2016 | Citations: 3153]": "The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.",
                    "[23163324 | Caliskan et al. | 2016 | Citations: 2673]": "Machines learn what people know implicitly AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan et al. now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs\u2014for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior. Science, this issue p. 183; see also p. 133 Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias. Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology.",
                    "[7840819 | Greenwald et al. | 1998 | Citations: 11264]": "An implicit association test (IAT) measures differential association of 2 target concepts with an attribute. The 2 concepts appear in a 2-choice task (2-choice task (e.g., flower vs. insect names), and the attribute in a 2nd task (e.g., pleasant vs. unpleasant words for an evaluation attribute). When instructions oblige highly associated categories (e.g., flower + pleasant) to share a response key, performance is faster than when less associated categories (e.g., insect & pleasant) share a key. This performance difference implicitly measures differential association of the 2 concepts with the attribute. In 3 experiments, the IAT was sensitive to (a) near-universal evaluative differences (e.g., flower vs. insect), (b) expected individual differences in evaluative associations (Japanese + pleasant vs. Korean + pleasant for Japanese vs. Korean subjects), and (c) consciously disavowed evaluative differences (Black + pleasant vs. White + pleasant for self-described unprejudiced White subjects)."
                },
                "metadata": [
                    {
                        "quote": "Word embeddings are numeric representations of meaning derived from word co-occurrence statistics in corpora of human-produced texts. The statistical regularities in language corpora encode well-known social biases into word embeddings (e.g., the word vector for family is closer to the vector women than to men)",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    },
                    {
                        "section_title": "INTRODUCTION",
                        "pdf_hash": "",
                        "start": 648,
                        "end": 1429,
                        "sentence_offsets": [
                            {
                                "start": 569,
                                "end": 795
                            },
                            {
                                "start": 796,
                                "end": 1049
                            },
                            {
                                "start": 1052,
                                "end": 1395
                            },
                            {
                                "start": 1396,
                                "end": 1575
                            }
                        ],
                        "ref_mentions": [
                            "1704893",
                            "23163324",
                            "23163324",
                            "7840819"
                        ],
                        "quote": "When datasets that reflect the thoughts, feelings, and actions of a community are used to train artificial intelligence (AI) models, the models inevitably incorporate the associations and representations of the community [11-13, 15, 17, 26, 40, 51, 54].\n\nIn one of the first studies systematically assessing social biases in word embeddings, Caliskan et al. [12] showed that pretrained GloVe embeddings replicated ten major biases widely found among human participants including biases associating young-good/elderly-bad, European American-good/African American-bad, and women-family/men-career [28]. Such group-based biases can manifest in NLP applications (a recent example was seen in Amazon's automated resume screening algorithm that preferentially hired men over women [7])."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[250390904 | Lalor et al. | 2022 | Citations: 48]",
                "snippets": "There has been a recent wave of work assessing the fairness of machine learning models in general, and more specifically, on natural language processing (NLP) models built using machine learning techniques. While much work has highlighted biases embedded in state-of-the-art language models, and more recent efforts have focused on how to debias, research assessing the fairness and performance of biased/debiased models on downstream prediction tasks has been limited. Moreover, most prior work has emphasized bias along a single dimension such as gender or race.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 564,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "There has been a recent wave of work assessing the fairness of machine learning models in general, and more specifically, on natural language processing (NLP) models built using machine learning techniques. While much work has highlighted biases embedded in state-of-the-art language models, and more recent efforts have focused on how to debias, research assessing the fairness and performance of biased/debiased models on downstream prediction tasks has been limited. Moreover, most prior work has emphasized bias along a single dimension such as gender or race."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[258041203 | Ferrara | 2023 | Citations: 261]",
                "snippets": "Bias in widely-adopted AI models, including ChatGPT and other generative language models, can have farreaching consequences that extend beyond the immediate context of their applications. When these models absorb and propagate biases, including those present in their training data, they may inadvertently reinforce stereotypes, marginalize certain groups, and lead to unfair treatment across various domains. Some examples of how biased AI models can adversely impact different areas include:\n\n\u2022 Hiring: AI-driven hiring tools that use biased models may exhibit unfair treatment towards applicants from underrepresented groups or those with non-traditional backgrounds. This could lead to the perpetuation of existing inequalities in the job market, limiting opportunities for affected individuals and reducing diversity in the workforce [102,(Raghavan et al., 2019). Large language models can be used to automate the screening of job applicants, such as by analyzing resumes and cover letters. Since these models are trained on vast amounts of text data, they may have internalized biases present in the data, such as gender or racial biases. As a result, they could unintentionally favor certain applicants or disqualify others based on factors unrelated to their qualifications, reinforcing existing inequalities in the job market.\n\n\u2022 Lending: Financial institutions increasingly rely on AI models for credit scoring and lending decisions. Biased models may unfairly penalize certain groups, such as minority communities or individuals with lower socioeconomic status, by assigning them lower credit scores or denying them access to loans and financial services based on biased assumptions (Citron et al., 2014)105,(Ustun et al., 2018). In lending, large language models can be used to assess creditworthiness or predict loan default risk, e.g., based on automated analysis of application or support documents.\n\nIf the data used to train these models contain historical biases or discriminatory lending practices, the models may learn to replicate these patterns. Consequently, they could deny loans to certain demographics or offer unfavorable terms based on factors like race, gender, or socioeconomic status, perpetuating financial inequality [107].\n\n\u2022 Content moderation: AI-powered content moderation systems help manage and filter user-generated content on social media platforms and other online communities. If these systems are trained on biased data, they may disproportionately censor or suppress the voices of certain groups",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[195316573 | Raghavan et al. | 2019 | Citations: 525]": "There has been rapidly growing interest in the use of algorithms in hiring, especially as a means to address or mitigate bias. Yet, to date, little is known about how these methods are used in practice. How are algorithmic assessments built, validated, and examined for bias? In this work, we document and analyze the claims and practices of companies offering algorithms for employment assessment. In particular, we identify vendors of algorithmic pre-employment assessments (i.e., algorithms to screen candidates), document what they have disclosed about their development and validation procedures, and evaluate their practices, focusing particularly on efforts to detect and mitigate bias. Our analysis considers both technical and legal perspectives. Technically, we consider the various choices vendors make regarding data collection and prediction targets, and explore the risks and trade-offs that these choices pose. We also discuss how algorithmic de-biasing techniques interface with, and create challenges for, antidiscrimination law.",
                    "[51734899 | Ustun et al. | 2018 | Citations: 550]": "Classification models are often used to make decisions that affect humans: whether to approve a loan application, extend a job offer, or provide insurance. In such applications, individuals should have the ability to change the decision of the model. When a person is denied a loan by a credit scoring model, for example, they should be able to change the input variables of the model in a way that will guarantee approval. Otherwise, this person will be denied the loan so long as the model is deployed, and -- more importantly --will lack agency over a decision that affects their livelihood. In this paper, we propose to evaluate a linear classification model in terms of recourse, which we define as the ability of a person to change the decision of the model through actionable input variables (e.g., income vs. age or marital status). We present an integer programming toolkit to: (i) measure the feasibility and difficulty of recourse in a target population; and (ii) generate a list of actionable changes for a person to obtain a desired outcome. We discuss how our tools can inform different stakeholders by using them to audit recourse for credit scoring models built with real-world datasets. Our results illustrate how recourse can be significantly affected by common modeling practices, and motivate the need to evaluate recourse in algorithmic decision-making."
                },
                "metadata": [
                    {
                        "section_title": "The risks of exacerbating existing societal biases",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 2489,
                        "sentence_offsets": [],
                        "ref_mentions": [
                            "195316573",
                            "61799887",
                            "51734899"
                        ],
                        "quote": "Bias in widely-adopted AI models, including ChatGPT and other generative language models, can have farreaching consequences that extend beyond the immediate context of their applications. When these models absorb and propagate biases, including those present in their training data, they may inadvertently reinforce stereotypes, marginalize certain groups, and lead to unfair treatment across various domains. Some examples of how biased AI models can adversely impact different areas include:\n\n\u2022 Hiring: AI-driven hiring tools that use biased models may exhibit unfair treatment towards applicants from underrepresented groups or those with non-traditional backgrounds. This could lead to the perpetuation of existing inequalities in the job market, limiting opportunities for affected individuals and reducing diversity in the workforce [102,(Raghavan et al., 2019). Large language models can be used to automate the screening of job applicants, such as by analyzing resumes and cover letters. Since these models are trained on vast amounts of text data, they may have internalized biases present in the data, such as gender or racial biases. As a result, they could unintentionally favor certain applicants or disqualify others based on factors unrelated to their qualifications, reinforcing existing inequalities in the job market.\n\n\u2022 Lending: Financial institutions increasingly rely on AI models for credit scoring and lending decisions. Biased models may unfairly penalize certain groups, such as minority communities or individuals with lower socioeconomic status, by assigning them lower credit scores or denying them access to loans and financial services based on biased assumptions (Citron et al., 2014)105,(Ustun et al., 2018). In lending, large language models can be used to assess creditworthiness or predict loan default risk, e.g., based on automated analysis of application or support documents.\n\nIf the data used to train these models contain historical biases or discriminatory lending practices, the models may learn to replicate these patterns. Consequently, they could deny loans to certain demographics or offer unfavorable terms based on factors like race, gender, or socioeconomic status, perpetuating financial inequality [107].\n\n\u2022 Content moderation: AI-powered content moderation systems help manage and filter user-generated content on social media platforms and other online communities. If these systems are trained on biased data, they may disproportionately censor or suppress the voices of certain groups"
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[258170403 | Sharma et al. | 2023 | Citations: 0]",
                "snippets": "Large pre-trained language models are widely used in the community. These models are usually trained on unmoderated and unfiltered data from open sources like the Internet. Due to this, biases that we see in platforms online which are a reflection of those in society are in turn captured and learned by these models. These models are deployed in applications that affect millions of people and their inherent biases are harmful to the targeted social groups.\n\nGiven the popularity of large pre-trained language models and their widespread use in the community, it is pertinent to evaluate their inherent biases. These biases come about because these models (for example, BERT (Devlin et al., 2019) and BERT based models, ALBERT (Lan et al., 2020), RoBERTa (Liu et al., 2019)) are trained on large sets of often freely available and unmoderated text data from sources such as the internet. \n\nDue to this, gender, race, religious and other social biases that we see in the real world are often translated into the models. Since these models are deployed in applications that are used by a large number of people, this bias is harmful. For example, these models are commonly used in content moderation tasks on social media platforms online. If the model is unfairly biased against certain social groups-minorities, marginalized people and those sections of society that are discriminated against, then they would be adversely affected by the application.\n\nStereoSet accounts for four different biases that are based on gender, occupation, race and religion. In addition to these, CrowS-Pairs also has sexual orientation, age, nationality, disability and physical appearance.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Large pre-trained language models are widely used in the community. These models are usually trained on unmoderated and unfiltered data from open sources like the Internet. Due to this, biases that we see in platforms online which are a reflection of those in society are in turn captured and learned by these models. These models are deployed in applications that affect millions of people and their inherent biases are harmful to the targeted social groups.\n\nGiven the popularity of large pre-trained language models and their widespread use in the community, it is pertinent to evaluate their inherent biases. These biases come about because these models (for example, BERT (Devlin et al., 2019) and BERT based models, ALBERT (Lan et al., 2020), RoBERTa (Liu et al., 2019)) are trained on large sets of often freely available and unmoderated text data from sources such as the internet. \n\nDue to this, gender, race, religious and other social biases that we see in the real world are often translated into the models. Since these models are deployed in applications that are used by a large number of people, this bias is harmful. For example, these models are commonly used in content moderation tasks on social media platforms online. If the model is unfairly biased against certain social groups-minorities, marginalized people and those sections of society that are discriminated against, then they would be adversely affected by the application.\n\nStereoSet accounts for four different biases that are based on gender, occupation, race and religion. In addition to these, CrowS-Pairs also has sexual orientation, age, nationality, disability and physical appearance.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[258174286 | Dolci et al. | 2023 | Citations: 10]",
                "snippets": "Therefore, it is of the utmost importance to expand research on how sentence embedding encoders internalise the semantics of natural languages. An important step towards this direction is to define metrics that are able to reflect and quantify social bias in sentence encoders. Furthermore, studying and limiting the causes and consequences of bias in language models is an extremely important task [4]6].\n\nThis work expands research on social bias in embeddingbased models, focusing specifically on gender bias in sentence representations. First, we propose a method to estimate gender bias in sentence embeddings, highlighting the correlation between bias and stereotypical concepts in the sentence. Our solution, named bias score, is highly flexible and designed to be easily adapted to both different kinds of social biases (e.g. ethnic, religious) and various sentence encoders.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[218971825 | Blodgett et al. | 2020 | Citations: 1249]": "We survey 146 papers analyzing \u201cbias\u201d in NLP systems, finding that their motivations are often vague, inconsistent, and lacking in normative reasoning, despite the fact that analyzing \u201cbias\u201d is an inherently normative process. We further find that these papers\u2019 proposed quantitative techniques for measuring or mitigating \u201cbias\u201d are poorly matched to their motivations and do not engage with the relevant literature outside of NLP. Based on these findings, we describe the beginnings of a path forward by proposing three recommendations that should guide work analyzing \u201cbias\u201d in NLP systems. These recommendations rest on a greater recognition of the relationships between language and social hierarchies, encouraging researchers and practitioners to articulate their conceptualizations of \u201cbias\u201d---i.e., what kinds of system behaviors are harmful, in what ways, to whom, and why, as well as the normative reasoning underlying these statements\u2014and to center work around the lived experiences of members of communities affected by NLP systems, while interrogating and reimagining the power relations between technologists and such communities.",
                    "[262580630 | Bender et al. | 2021 | Citations: 4657]": "The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 883,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 143
                            },
                            {
                                "start": 144,
                                "end": 277
                            },
                            {
                                "start": 278,
                                "end": 405
                            },
                            {
                                "start": 408,
                                "end": 541
                            },
                            {
                                "start": 542,
                                "end": 702
                            },
                            {
                                "start": 703,
                                "end": 834
                            },
                            {
                                "start": 835,
                                "end": 884
                            }
                        ],
                        "ref_mentions": [
                            "262580630",
                            "218971825"
                        ],
                        "quote": "Therefore, it is of the utmost importance to expand research on how sentence embedding encoders internalise the semantics of natural languages. An important step towards this direction is to define metrics that are able to reflect and quantify social bias in sentence encoders. Furthermore, studying and limiting the causes and consequences of bias in language models is an extremely important task [4]6].\n\nThis work expands research on social bias in embeddingbased models, focusing specifically on gender bias in sentence representations. First, we propose a method to estimate gender bias in sentence embeddings, highlighting the correlation between bias and stereotypical concepts in the sentence. Our solution, named bias score, is highly flexible and designed to be easily adapted to both different kinds of social biases (e.g. ethnic, religious) and various sentence encoders."
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[258426630 | Fraiwan et al. | 2023 | Citations: 48]",
                "snippets": "Language models can exhibit bias if the training data used to create them is biased. As Schramowski et al. (Schramowski et al., 2021) pointed out, large pre-trained models that try to mimic natural languages, may end up repeating the same unfairness and prejudices. This can lead to discriminatory or inaccurate analyses and recommendations. Moreover, this may lead to public outcry (i.e., political, social, and legal) against the commercial applications. These biases manifests themselves in several ways, as follows: \n\n\u2022 Training data bias: Language models are typically trained on large datasets of human language. If these datasets are biased in some way (e.g., based on race, gender, socioeconomic status, etc.), then the model may learn and replicate these biases in its responses. For example, if the training data is biased towards a particular gender, then the model may generate responses that are more favorable towards that gender. \n\n\u2022 User interaction bias: The responses generated by Chatbots are based on the input they receive from users. If users consistently ask biased or prejudiced questions, then the model may learn and replicate these biases in its responses. For example, if users frequently ask questions that are discriminatory towards a particular group, then the model may generate responses that perpetuate these biases. \n\n\u2022 Algorithmic bias: The algorithms used to train and operate language models and Chatbots may also introduce biases. For example, if the model is trained to optimize for a particular metric (e.g., accuracy, engagement, etc.), then it may prioritize generating responses that optimize for that metric, even if those responses are biased in some way. \n\n\u2022 Contextual bias: Chatbots generate responses based on the context they receive from users. If the context is biased in some way (e.g., based on the user's location, language, etc.), then the model may generate biased responses. For example, if a user is asking questions about a particular culture or religion, and the model is not trained on that culture or religion, it may generate biased responses due to its lack of knowledge.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[246824056 | Schramowski et al. | 2021 | Citations: 293]": "Artificial writing is permeating our lives due to recent advances in large-scale, transformer-based language models (LMs) such as BERT, GPT-2 and GPT-3. Using them as pre-trained models and fine-tuning them for specific tasks, researchers have extended the state of the art for many natural language processing tasks and shown that they capture not only linguistic knowledge but also retain general knowledge implicitly present in the data. Unfortunately, LMs trained on unfiltered text corpora suffer from degenerated and biased behaviour. While this is well established, we show here that recent LMs also contain human-like biases of what is right and wrong to do, reflecting existing ethical and moral norms of society. We show that these norms can be captured geometrically by a \u2018moral direction\u2019 which can be computed, for example, by a PCA, in the embedding space. The computed \u2018moral direction\u2019 can rate the normativity (or non-normativity) of arbitrary phrases without explicitly training the LM for this task, reflecting social norms well. We demonstrate that computing the \u2019moral direction\u2019 can provide a path for attenuating or even preventing toxic degeneration in LMs, showcasing this capability on the RealToxicityPrompts testbed. Large language models identify patterns in the relations between words and capture their relations in an embedding space. Schramowski and colleagues show that a direction in this space can be identified that separates \u2018right\u2019 and \u2018wrong\u2019 actions as judged by human survey participants."
                },
                "metadata": [
                    {
                        "section_title": "Bias",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 2115,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 84
                            },
                            {
                                "start": 85,
                                "end": 243
                            },
                            {
                                "start": 244,
                                "end": 319
                            },
                            {
                                "start": 320,
                                "end": 434
                            },
                            {
                                "start": 435,
                                "end": 497
                            },
                            {
                                "start": 500,
                                "end": 596
                            },
                            {
                                "start": 597,
                                "end": 766
                            },
                            {
                                "start": 767,
                                "end": 922
                            },
                            {
                                "start": 925,
                                "end": 1033
                            },
                            {
                                "start": 1034,
                                "end": 1161
                            },
                            {
                                "start": 1162,
                                "end": 1328
                            },
                            {
                                "start": 1331,
                                "end": 1447
                            },
                            {
                                "start": 1448,
                                "end": 1679
                            },
                            {
                                "start": 1682,
                                "end": 1774
                            },
                            {
                                "start": 1775,
                                "end": 1911
                            },
                            {
                                "start": 1912,
                                "end": 2115
                            }
                        ],
                        "ref_mentions": [
                            "246824056"
                        ],
                        "quote": "Language models can exhibit bias if the training data used to create them is biased. As Schramowski et al. (Schramowski et al., 2021) pointed out, large pre-trained models that try to mimic natural languages, may end up repeating the same unfairness and prejudices. This can lead to discriminatory or inaccurate analyses and recommendations. Moreover, this may lead to public outcry (i.e., political, social, and legal) against the commercial applications. These biases manifests themselves in several ways, as follows: \n\n\u2022 Training data bias: Language models are typically trained on large datasets of human language. If these datasets are biased in some way (e.g., based on race, gender, socioeconomic status, etc.), then the model may learn and replicate these biases in its responses. For example, if the training data is biased towards a particular gender, then the model may generate responses that are more favorable towards that gender. \n\n\u2022 User interaction bias: The responses generated by Chatbots are based on the input they receive from users. If users consistently ask biased or prejudiced questions, then the model may learn and replicate these biases in its responses. For example, if users frequently ask questions that are discriminatory towards a particular group, then the model may generate responses that perpetuate these biases. \n\n\u2022 Algorithmic bias: The algorithms used to train and operate language models and Chatbots may also introduce biases. For example, if the model is trained to optimize for a particular metric (e.g., accuracy, engagement, etc.), then it may prioritize generating responses that optimize for that metric, even if those responses are biased in some way. \n\n\u2022 Contextual bias: Chatbots generate responses based on the context they receive from users. If the context is biased in some way (e.g., based on the user's location, language, etc.), then the model may generate biased responses. For example, if a user is asking questions about a particular culture or religion, and the model is not trained on that culture or religion, it may generate biased responses due to its lack of knowledge."
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[259075978 | Wald et al. | 2023 | Citations: 6]",
                "snippets": "Research has shown that language models are prone to exhibit societal biases, reflecting the nature of the underlying training data written by humans. For this work, we focus on representational bias, which in the context of language models, occurs when there is an imbalance in the portrayal of different groups in LM-generated text (Blodgett et al. 2020;Liang et al. 2021). This can arise from stereotypes that cultivate negative generalisations about particular social groups and language that is used to degrade these groups. Ultimately, this may lead to certain groups being depicted less favourably than others. Bender et al. argue that even in large models with large datasets, diversity and unbiasedness are not guaranteed (Bender et al. 2021). Datasets are too big to be thoroughly documented, while marginalised social groups are more likely to be filtered out due to the nature of the filtering process. Phrases or dialects used by these groups are disproportionally categorised as hateful, resulting in their exclusion from training datasets (Sap et al. 2019;Bender et al. 2021). Hegemonic views are over-represented as Internet access is not evenly distributed, and datasets are often curated using male-dominated websites. Large language generation models such as GPT-2 (Radford et al. 2019) and GPT-3 (Brown et al. 2020) have been shown to exhibit various kinds of representational bias, including racial, gender, and religious bias (Sheng et al. 2019;Abid, Farooqi, and Zou 2021;Sheng et al. 2021). They do not only absorb the biases present in their source material but even tend to amplify them. Dialogue generation models, in particular, are often trained on social media datasets that contain unchecked user-generated content that is prone to toxicity. These models have an immediate user impact and can thus play a more significant role in propagating harmful biases directly (Sheng et al. 2021).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Research has shown that language models are prone to exhibit societal biases, reflecting the nature of the underlying training data written by humans. For this work, we focus on representational bias, which in the context of language models, occurs when there is an imbalance in the portrayal of different groups in LM-generated text (Blodgett et al. 2020;Liang et al. 2021). This can arise from stereotypes that cultivate negative generalisations about particular social groups and language that is used to degrade these groups. Ultimately, this may lead to certain groups being depicted less favourably than others. Bender et al. argue that even in large models with large datasets, diversity and unbiasedness are not guaranteed (Bender et al. 2021). Datasets are too big to be thoroughly documented, while marginalised social groups are more likely to be filtered out due to the nature of the filtering process. Phrases or dialects used by these groups are disproportionally categorised as hateful, resulting in their exclusion from training datasets (Sap et al. 2019;Bender et al. 2021). Hegemonic views are over-represented as Internet access is not evenly distributed, and datasets are often curated using male-dominated websites. Large language generation models such as GPT-2 (Radford et al. 2019) and GPT-3 (Brown et al. 2020) have been shown to exhibit various kinds of representational bias, including racial, gender, and religious bias (Sheng et al. 2019;Abid, Farooqi, and Zou 2021;Sheng et al. 2021). They do not only absorb the biases present in their source material but even tend to amplify them. Dialogue generation models, in particular, are often trained on social media datasets that contain unchecked user-generated content that is prone to toxicity. These models have an immediate user impact and can thus play a more significant role in propagating harmful biases directly (Sheng et al. 2021).",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[259095603 | Thakur et al. | 2023 | Citations: 37]",
                "snippets": "Societal biases present in pre-trained large language models are a critical issue as these models have been shown to propagate biases in countless downstream applications, rendering them unfair towards specific groups of people.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 228,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Societal biases present in pre-trained large language models are a critical issue as these models have been shown to propagate biases in countless downstream applications, rendering them unfair towards specific groups of people."
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[259129801 | Mei et al. | 2023 | Citations: 51]",
                "snippets": "The rapid deployment of artificial intelligence (AI) models de- demands a thorough investigation of biases and risks inherent in these models to understand their impact on individuals and society. A growing body of work has shown that social biases are encoded in language models and their downstream tasks...Caliskan et al. [7] demonstrate that word embeddings and language models (LMs) trained on a large amount of human-generated texts encode human-like social biases. Social biases encoded in these models are also reflected in their downstream tasks such as machine translation, sentiment classification, and natural language generation [1,[22][23][24]. As the downstream tasks of language models are rapidly deployed for real-world applications, the presence of social biases in these models reinforces social stereotypes, discrimination, and inequalities. Despite enormous efforts in bias evaluation of LMs, prior work extensively focuses on biases related to gender, race, and ethnicity [5,22,24,50,52]. Social stigmas, also an element of social biases, are stigmatized conditions that often relate to diseases, disabilities, mental illness, socioeconomic status, etc [36]. Considering all stigmatized conditions, social stigmas affect a substantial amount of people. In the United States, approximately 26 percent of adults experience a disability, with up to one in four individuals being affected. In 2021, there were around 57.8M adults that experienced mental illness, which was around 22% of the population in the United States. Social stigmas prevent individuals from social activities and access to education, healthcare, and career opportunities, negatively influencing their psychological well-being and life outcomes [14,[31][32][33][37].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[102352788 | Bordia et al. | 2019 | Citations: 327]": "Many text corpora exhibit socially problematic biases, which can be propagated or amplified in the models trained on such data. For example, doctor cooccurs more frequently with male pronouns than female pronouns. In this study we (i) propose a metric to measure gender bias; (ii) measure bias in a text corpus and the text generated from a recurrent neural network language model trained on the text corpus; (iii) propose a regularization loss term for the language model that minimizes the projection of encoder-trained embeddings onto an embedding subspace that encodes gender; (iv) finally, evaluate efficacy of our proposed method on reducing gender bias. We find this regularization method to be effective in reducing gender bias up to an optimal weight assigned to the loss term, beyond which the model becomes unstable as the perplexity increases. We replicate this study on three training corpora\u2014Penn Treebank, WikiText-2, and CNN/Daily Mail\u2014resulting in similar conclusions.",
                    "[190000105 | Kurita et al. | 2019 | Citations: 451]": "Contextual word embeddings such as BERT have achieved state of the art performance in numerous NLP tasks. Since they are optimized to capture the statistical properties of training data, they tend to pick up on and amplify social stereotypes present in the data as well. In this study, we (1) propose a template-based method to quantify bias in BERT; (2) show that this method obtains more consistent results in capturing social biases than the traditional cosine based method; and (3) conduct a case study, evaluating gender bias in a downstream task of Gender Pronoun Resolution. Although our case study focuses on gender bias, the proposed technique is generalizable to unveiling other biases, including in multiclass settings, such as racial and religious biases.",
                    "[231603388 | Abid et al. | 2021 | Citations: 555]": "It has been observed that large-scale language models capture undesirable societal biases, e.g. relating to race and gender; yet religious bias has been relatively unexplored. We demonstrate that GPT-3, a state-of-the-art contextual language model, captures persistent Muslim-violence bias. We probe GPT-3 in various ways, including prompt completion, analogical reasoning, and story generation, to understand this anti-Muslim bias, demonstrating that it appears consistently and creatively in different uses of the model and that it is severe even compared to biases about other religious groups. For instance, Muslim is analogized to terrorist in 23% of test cases, while Jewish is mapped to its most common stereotype, money, in 5% of test cases. We quantify the positive distraction needed to overcome this bias with adversarial text prompts, and find that use of the most positive 6 adjectives reduces violent completions for Muslims from 66% to 20%, but which is still higher than for other religious groups.",
                    "[23163324 | Caliskan et al. | 2016 | Citations: 2673]": "Machines learn what people know implicitly AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan et al. now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs\u2014for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior. Science, this issue p. 183; see also p. 133 Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias. Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology.",
                    "[238259136 | Wolfe et al. | 2021 | Citations: 51]": "We use a dataset of U.S. first names with labels based on predominant gender and racial group to examine the effect of training corpus frequency on tokenization, contextualization, similarity to initial representation, and bias in BERT, GPT-2, T5, and XLNet. We show that predominantly female and non-white names are less frequent in the training corpora of these four language models. We find that infrequent names are more self-similar across contexts, with Spearman\u2019s rho between frequency and self-similarity as low as -.763. Infrequent names are also less similar to initial representation, with Spearman\u2019s rho between frequency and linear centered kernel alignment (CKA) similarity to initial representation as high as .702. Moreover, we find Spearman\u2019s rho between racial bias and name frequency in BERT of .492, indicating that lower-frequency minority group names are more associated with unpleasantness. Representations of infrequent names undergo more processing, but are more self-similar, indicating that models rely on less context-informed representations of uncommon and minority names which are overfit to a lower number of observed contexts.",
                    "[246654106 | Meisel et al. | 2022 | Citations: 10]": "Background Lower levels of education are strongly associated with negative health outcomes. The current study examined the degree to which those without a history of 4-year college attendance experience social stigmatization of their educational status and if these experiences are associated with mental health symptoms. Methods Data was obtained from 488 emerging adults who never attended 4-year college using Qualtrics Panels. Results 79.4% of participants agreed to one of the six statements that not attending 4-year college is stigmatized, and 71.8% endorsed experiencing at least one form of discrimination. Higher levels of education-related stigma and more frequent experiences of education-related discrimination was associated with greater past-month anxiety and depression symptoms. Conclusions These findings could serve to increase awareness regarding the unique and significant discrimination faced by young adults who do not attend 4-year college and identify specific areas of intervention that can help these young adults cope with the effects of stigma and discrimination.",
                    "[250391069 | Jentzsch et al. | 2023 | Citations: 33]": "Pretrained language models are publicly available and constantly finetuned for various real-life applications. As they become capable of grasping complex contextual information, harmful biases are likely increasingly intertwined with those models. This paper analyses gender bias in BERT models with two main contributions: First, a novel bias measure is introduced, defining biases as the difference in sentiment valuation of female and male sample versions. Second, we comprehensively analyse BERT?s biases on the example of a realistic IMDB movie classifier. By systematically varying elements of the training pipeline, we can conclude regarding their impact on the final model bias. Seven different public BERT models in nine training conditions, i.e. 63 models in total, are compared. Almost all conditions yield significant gender biases. Results indicate that reflected biases stem from public BERT models rather than task-specific data, emphasising the weight of responsible usage."
                },
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 67,
                        "end": 373,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "The rapid deployment of artificial intelligence (AI) models de- demands a thorough investigation of biases and risks inherent in these models to understand their impact on individuals and society. A growing body of work has shown that social biases are encoded in language models and their downstream tasks"
                    },
                    {
                        "section_title": "INTRODUCTION",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1448,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 162
                            },
                            {
                                "start": 163,
                                "end": 349
                            },
                            {
                                "start": 350,
                                "end": 553
                            },
                            {
                                "start": 554,
                                "end": 702
                            },
                            {
                                "start": 703,
                                "end": 872
                            },
                            {
                                "start": 873,
                                "end": 966
                            },
                            {
                                "start": 967,
                                "end": 1100
                            },
                            {
                                "start": 1101,
                                "end": 1235
                            },
                            {
                                "start": 1236,
                                "end": 1449
                            }
                        ],
                        "ref_mentions": [
                            "23163324",
                            "231603388",
                            "250391069",
                            "190000105",
                            "102352788",
                            "250391069",
                            "190000105",
                            "227275068",
                            "238259136",
                            "3750353",
                            "21883193",
                            "145255089",
                            "246654106",
                            "226207034",
                            "33273008"
                        ],
                        "quote": "Caliskan et al. [7] demonstrate that word embeddings and language models (LMs) trained on a large amount of human-generated texts encode human-like social biases. Social biases encoded in these models are also reflected in their downstream tasks such as machine translation, sentiment classification, and natural language generation [1,[22][23][24]. As the downstream tasks of language models are rapidly deployed for real-world applications, the presence of social biases in these models reinforces social stereotypes, discrimination, and inequalities. Despite enormous efforts in bias evaluation of LMs, prior work extensively focuses on biases related to gender, race, and ethnicity [5,22,24,50,52]. Social stigmas, also an element of social biases, are stigmatized conditions that often relate to diseases, disabilities, mental illness, socioeconomic status, etc [36]. Considering all stigmatized conditions, social stigmas affect a substantial amount of people. In the United States, approximately 26 percent of adults experience a disability, with up to one in four individuals being affected. In 2021, there were around 57.8M adults that experienced mental illness, which was around 22% of the population in the United States. Social stigmas prevent individuals from social activities and access to education, healthcare, and career opportunities, negatively influencing their psychological well-being and life outcomes [14,[31][32][33][37]."
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[259203321 | Nemani et al. | 2023 | Citations: 3]",
                "snippets": "Gender bias in artificial intelligence (AI) has emerged as a pressing concern with profound implications for individuals' lives. Researchers have identified three primary categories of bias: Denigration (A1), Stereotyping (A2), and Under-representation (A3). Denigration involves the usage of racial, ethnic, or religious slurs, which can often be observed as a prevalent method of cyberbullying. This type of bias manifests as derogatory language aimed at demeaning specific groups. Stereotyping, on the other hand, refers to individuals' cognitive representation of a particular social group. In NLP tasks, stereotyping can manifest through word embeddings and large-scale language models, where certain biases are learned and perpetuated in the resulting representations. Under-representation pertains to the absence of identifiable group members from representative bodies and well-being indicators in proportion to their population's size.\n\nStudies have demonstrated that word embeddings can detect racial and gender biases, even in well-labeled and balanced datasets. These biases can arise due to societal prejudices reflected in the training data, leading to biased predictions and outputs from NLP models. Contextual representations learned by large pre-trained language models, such as BERT and GPT, are also susceptible to biases. These models are typically trained on vast amounts of internet text, including societal biases in online content. Consequently, these models can replicate and perpetuate biases, often mirroring societal biases.\n\nFurthermore, machine learning models tend to amplify the behaviors and patterns they are exposed to, including biases present in the training data. Studies such as [30], [31] have explored the compounding effect of bias in newer models, highlighting the phenomenon known as bias overamplification. This refers to the tendency of machine learning models to disproportionately amplify and reinforce biases rather than mitigate them.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Gender bias in artificial intelligence (AI) has emerged as a pressing concern with profound implications for individuals' lives. Researchers have identified three primary categories of bias: Denigration (A1), Stereotyping (A2), and Under-representation (A3). Denigration involves the usage of racial, ethnic, or religious slurs, which can often be observed as a prevalent method of cyberbullying. This type of bias manifests as derogatory language aimed at demeaning specific groups. Stereotyping, on the other hand, refers to individuals' cognitive representation of a particular social group. In NLP tasks, stereotyping can manifest through word embeddings and large-scale language models, where certain biases are learned and perpetuated in the resulting representations. Under-representation pertains to the absence of identifiable group members from representative bodies and well-being indicators in proportion to their population's size.\n\nStudies have demonstrated that word embeddings can detect racial and gender biases, even in well-labeled and balanced datasets. These biases can arise due to societal prejudices reflected in the training data, leading to biased predictions and outputs from NLP models. Contextual representations learned by large pre-trained language models, such as BERT and GPT, are also susceptible to biases. These models are typically trained on vast amounts of internet text, including societal biases in online content. Consequently, these models can replicate and perpetuate biases, often mirroring societal biases.\n\nFurthermore, machine learning models tend to amplify the behaviors and patterns they are exposed to, including biases present in the training data. Studies such as [30], [31] have explored the compounding effect of bias in newer models, highlighting the phenomenon known as bias overamplification. This refers to the tendency of machine learning models to disproportionately amplify and reinforce biases rather than mitigate them.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[259716055 | Kolisko et al. | 2023 | Citations: 11]",
                "snippets": "There is a growing body of work documenting social biases in large language models. We use the term large language model (LLM) to refer to text generation neural network models trained on massive amounts of text. Popular examples include BERT (Devlin et al. 2019), GPT-3 (Brown et al. 2020), RoBERTa (Liu et al., 2019)BLOOM (Big-Science 2022). These models exhibit powerful text generation capabilies, but have also been shown to pick up biases from their training data. \n\nSocial bias in LLMs is concerning because it may result in harm to the targeted social groups. Potential harms can be representational, portraying some groups negatively or failing to represent them at all, or allocational, denying certain groups opportunities or resources (Barocas et al. 2017). \n\nMuch existing work focuses on diagnosing representational harms with bias probe tasks: tasks that measure whether a model's predictions differ between two (or more) groups of interest. A number of probe tasks have been proposed: (Rudinger et al., 2017); (Sheng et al., 2019); Bordia and Bowman (2019); (Lee et al., 2019); (Liu et al., 2019)(Liu et al., 2019); (Nadeem et al., 2020)(Sheng et al., 2021) and others. Most of these focus on gender stereotypes. 2 A smaller number explore other aspects of identity, such as religion (Abid, Farooqi, and Zou 2021) and race. 3",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[198953378 | Liu et al. | 2019 | Citations: 24556]": "Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.",
                    "[202537041 | Sheng et al. | 2019 | Citations: 646]": "We present a systematic study of biases in natural language generation (NLG) by analyzing text generated from prompts that contain mentions of different demographic groups. In this work, we introduce the notion of the regard towards a demographic, use the varying levels of regard towards different demographics as a defining metric for bias in NLG, and analyze the extent to which sentiment scores are a relevant proxy metric for regard. To this end, we collect strategically-generated text from language models and manually annotate the text with both sentiment and regard scores. Additionally, we build an automatic regard classifier through transfer learning, so that we can analyze biases in unseen text. Together, these methods reveal the extent of the biased nature of language model generations. Our analysis provides a study of biases in NLG, bias metrics and correlated human judgments, and empirical evidence on the usefulness of our annotated dataset.",
                    "[204838020 | Liu et al. | 2019 | Citations: 144]": "Recently there are increasing concerns about the fairness of Artificial Intelligence (AI) in real-world applications such as computer vision and recommendations. For example, recognition algorithms in computer vision are unfair to black people such as poorly detecting their faces and inappropriately identifying them as \u201cgorillas\u201d. As one crucial application of AI, dialogue systems have been extensively applied in our society. They are usually built with real human conversational data; thus they could inherit some fairness issues which are held in the real world. However, the fairness of dialogue systems has not been well investigated. In this paper, we perform a pioneering study about the fairness issues in dialogue systems. In particular, we construct a benchmark dataset and propose quantitative measures to understand fairness in dialogue models. Our studies demonstrate that popular dialogue models show significant prejudice towards different genders and races. Besides, to mitigate the bias in dialogue systems, we propose two simple but effective debiasing methods. Experiments show that our methods can reduce the bias in dialogue systems significantly. The dataset and the implementation are released to foster fairness research in dialogue systems.",
                    "[215828184 | Nadeem et al. | 2020 | Citations: 1015]": "A stereotype is an over-generalized belief about a particular group of people, e.g., Asians are good at math or African Americans are athletic. Such beliefs (biases) are known to hurt target groups. Since pretrained language models are trained on large real-world data, they are known to capture stereotypical biases. It is important to quantify to what extent these biases are present in them. Although this is a rapidly growing area of research, existing literature lacks in two important aspects: 1) they mainly evaluate bias of pretrained language models on a small set of artificial sentences, even though these models are trained on natural data 2) current evaluations focus on measuring bias without considering the language modeling ability of a model, which could lead to misleading trust on a model even if it is a poor language model. We address both these problems. We present StereoSet, a large-scale natural English dataset to measure stereotypical biases in four domains: gender, profession, race, and religion. We contrast both stereotypical bias and language modeling ability of popular models like BERT, GPT-2, RoBERTa, and XLnet. We show that these models exhibit strong stereotypical biases. Our data and code are available at https://stereoset.mit.edu.",
                    "[234337004 | Sheng et al. | 2021 | Citations: 220]": "Technology for language generation has advanced rapidly, spurred by advancements in pre-training large models on massive amounts of data and the need for intelligent agents to communicate in a natural manner. While techniques can effectively generate fluent text, they can also produce undesirable societal biases that can have a disproportionately negative impact on marginalized populations. Language generation presents unique challenges for biases in terms of direct user interaction and the structure of decoding techniques. To better understand these challenges, we present a survey on societal biases in language generation, focusing on how data and techniques contribute to biases and progress towards reducing biases. Motivated by a lack of studies on biases from decoding techniques, we also conduct experiments to quantify the effects of these techniques. By further discussing general trends and open challenges, we call to attention promising directions for research and the importance of fairness and inclusivity considerations for language generation applications.",
                    "[5310359 | Rudinger et al. | 2017 | Citations: 137]": "We analyze the Stanford Natural Language Inference (SNLI) corpus in an investigation of bias and stereotyping in NLP data. The SNLI human-elicitation protocol makes it prone to amplifying bias and stereotypical associations, which we demonstrate statistically (using pointwise mutual information) and with qualitative examples."
                },
                "metadata": [
                    {
                        "section_title": "Social Biases in Large Language Models",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1383,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 83
                            },
                            {
                                "start": 84,
                                "end": 212
                            },
                            {
                                "start": 213,
                                "end": 348
                            },
                            {
                                "start": 349,
                                "end": 475
                            },
                            {
                                "start": 478,
                                "end": 572
                            },
                            {
                                "start": 573,
                                "end": 774
                            },
                            {
                                "start": 777,
                                "end": 961
                            },
                            {
                                "start": 962,
                                "end": 1227
                            },
                            {
                                "start": 1228,
                                "end": 1272
                            },
                            {
                                "start": 1273,
                                "end": 1383
                            }
                        ],
                        "ref_mentions": [
                            "198953378",
                            "5310359",
                            "202537041",
                            "211142738",
                            "204838020",
                            "198953378",
                            "215828184",
                            "234337004"
                        ],
                        "quote": "There is a growing body of work documenting social biases in large language models. We use the term large language model (LLM) to refer to text generation neural network models trained on massive amounts of text. Popular examples include BERT (Devlin et al. 2019), GPT-3 (Brown et al. 2020), RoBERTa (Liu et al., 2019)BLOOM (Big-Science 2022). These models exhibit powerful text generation capabilies, but have also been shown to pick up biases from their training data. \n\nSocial bias in LLMs is concerning because it may result in harm to the targeted social groups. Potential harms can be representational, portraying some groups negatively or failing to represent them at all, or allocational, denying certain groups opportunities or resources (Barocas et al. 2017). \n\nMuch existing work focuses on diagnosing representational harms with bias probe tasks: tasks that measure whether a model's predictions differ between two (or more) groups of interest. A number of probe tasks have been proposed: (Rudinger et al., 2017); (Sheng et al., 2019); Bordia and Bowman (2019); (Lee et al., 2019); (Liu et al., 2019)(Liu et al., 2019); (Nadeem et al., 2020)(Sheng et al., 2021) and others. Most of these focus on gender stereotypes. 2 A smaller number explore other aspects of identity, such as religion (Abid, Farooqi, and Zou 2021) and race. 3"
                    }
                ]
            },
            {
                "idx": 17,
                "key": "[260438662 | Razaa et al. | 2023 | Citations: 48]",
                "snippets": "The recent surge in Natural Language Processing (NLP) applications, encompassing fields from recommendation systems to social justice and employment screening, has sparked a critical concern -the emergence of bias within these systems [1]. Instances of racial and gender bias have been increasingly reported [2], indicating an urgent need for scrutiny. These biases often originate from the training data used in NLP models, and a majority of these large datasets harbor inherent biases.\n\nBias in text data is a pervasive and deeply-rooted issue. The bias in data often stems from cognitive predispositions that influences our dialogues, views, and understanding of information [3]. This bias can be explicit which are often seen in discriminatory language targeting certain racial or ethnic groups [4], as in social media. Implicit bias [5], on the other hand, subtly perpetuates prejudice through unintentional language use but is equally harmful.\n\nThe necessity for unbiased, trustworthy text data has grown across sectors like healthcare [6], social media [4,7], and recruitment [8]. This data is essential for training NLP models for various downstream tasks, like formulating healthcare diagnoses and treatment plans, handling discriminatory language on social media, and promoting fair recruitment practices. Figure 1 illustrates the complexities of biases in text data in various domains, including job hiring, social media, and healthcare. These biases are primarily conveyed through lexical choices [9] and demand sophisticated detection methods, motivating this research.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "The recent surge in Natural Language Processing (NLP) applications, encompassing fields from recommendation systems to social justice and employment screening, has sparked a critical concern -the emergence of bias within these systems [1]. Instances of racial and gender bias have been increasingly reported [2], indicating an urgent need for scrutiny. These biases often originate from the training data used in NLP models, and a majority of these large datasets harbor inherent biases.\n\nBias in text data is a pervasive and deeply-rooted issue. The bias in data often stems from cognitive predispositions that influences our dialogues, views, and understanding of information [3]. This bias can be explicit which are often seen in discriminatory language targeting certain racial or ethnic groups [4], as in social media. Implicit bias [5], on the other hand, subtly perpetuates prejudice through unintentional language use but is equally harmful.\n\nThe necessity for unbiased, trustworthy text data has grown across sectors like healthcare [6], social media [4,7], and recruitment [8]. This data is essential for training NLP models for various downstream tasks, like formulating healthcare diagnoses and treatment plans, handling discriminatory language on social media, and promoting fair recruitment practices. Figure 1 illustrates the complexities of biases in text data in various domains, including job hiring, social media, and healthcare. These biases are primarily conveyed through lexical choices [9] and demand sophisticated detection methods, motivating this research.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 18,
                "key": "[261049466 | Li et al. | 2023 | Citations: 70]",
                "snippets": "However, numerous studies have verified that LLMs capture human-like social biases in unprocessed training data (Garg et al., 2017)(Sun et al., 2019). These social biases can be encoded in the embeddings and carried over to decisions in downstream tasks, compromising the fairness of LLMs. Unfair LLM systems make discriminatory, stereotypic and demeaning decisions against vulnerable or marginalized demographics, causing undesirable social impacts and potential harms (Blodgett et al., 2020)(Kumar et al., 2022). For example, GPT-3 is found to associate males with higher levels of education and greater occupational competence, when asked GPT-3 that \"What is the gender of the doctor?\" and \"What is the gender of the nurse?\", its preferred outputs are \"A: Doctor is a masculine noun;\" and \"It's female.\", respectively. In real-world applications, the automatic resume filtering systems can be gender-biased, which tend to assign programmer jobs to men and homemaker jobs to women (De-Arteaga et al., 2019)(Deshpande et al., 2020)[10], and the US healthcare system can be racial biased, which judges black patients with the same risk level to be sicker than white patients (Obermeyer et al., 2019).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[195316733 | Sun et al. | 2019 | Citations: 562]": "As Natural Language Processing (NLP) and Machine Learning (ML) tools rise in popularity, it becomes increasingly vital to recognize the role they play in shaping societal biases and stereotypes. Although NLP models have shown success in modeling various applications, they propagate and may even amplify gender bias found in text corpora. While the study of bias in artificial intelligence is not new, methods to mitigate gender bias in NLP are relatively nascent. In this paper, we review contemporary studies on recognizing and mitigating gender bias in NLP. We discuss gender bias based on four forms of representation bias and analyze methods recognizing gender bias. Furthermore, we discuss the advantages and drawbacks of existing gender debiasing methods. Finally, we discuss future studies for recognizing and mitigating gender bias in NLP.",
                    "[204881868 | Obermeyer et al. | 2019 | Citations: 3628]": "Racial bias in health algorithms The U.S. health care system uses commercial algorithms to guide health decisions. Obermeyer et al. find evidence of racial bias in one widely used algorithm, such that Black patients assigned the same level of risk by the algorithm are sicker than White patients (see the Perspective by Benjamin). The authors estimated that this racial bias reduces the number of Black patients identified for extra care by more than half. Bias occurs because the algorithm uses health costs as a proxy for health needs. Less money is spent on Black patients who have the same level of need, and the algorithm thus falsely concludes that Black patients are healthier than equally sick White patients. Reformulating the algorithm so that it no longer uses costs as a proxy for needs eliminates the racial bias in predicting who needs extra care. Science, this issue p. 447; see also p. 421 A health algorithm that uses health costs as a proxy for health needs leads to racial bias against Black patients. Health systems rely on commercial prediction algorithms to identify and help patients with complex health needs. We show that a widely used algorithm, typical of this industry-wide approach and affecting millions of patients, exhibits significant racial bias: At a given risk score, Black patients are considerably sicker than White patients, as evidenced by signs of uncontrolled illnesses. Remedying this disparity would increase the percentage of Black patients receiving additional help from 17.7 to 46.5%. The bias arises because the algorithm predicts health care costs rather than illness, but unequal access to care means that we spend less money caring for Black patients than for White patients. Thus, despite health care cost appearing to be an effective proxy for health by some measures of predictive accuracy, large racial biases arise. We suggest that the choice of convenient, seemingly effective proxies for ground truth can be an important source of algorithmic bias in many contexts.",
                    "[218971825 | Blodgett et al. | 2020 | Citations: 1249]": "We survey 146 papers analyzing \u201cbias\u201d in NLP systems, finding that their motivations are often vague, inconsistent, and lacking in normative reasoning, despite the fact that analyzing \u201cbias\u201d is an inherently normative process. We further find that these papers\u2019 proposed quantitative techniques for measuring or mitigating \u201cbias\u201d are poorly matched to their motivations and do not engage with the relevant literature outside of NLP. Based on these findings, we describe the beginnings of a path forward by proposing three recommendations that should guide work analyzing \u201cbias\u201d in NLP systems. These recommendations rest on a greater recognition of the relationships between language and social hierarchies, encouraging researchers and practitioners to articulate their conceptualizations of \u201cbias\u201d---i.e., what kinds of system behaviors are harmful, in what ways, to whom, and why, as well as the normative reasoning underlying these statements\u2014and to center work around the lived experiences of members of communities affected by NLP systems, while interrogating and reimagining the power relations between technologists and such communities.",
                    "[220040093 | Deshpande et al. | 2020 | Citations: 65]": "With increasing diversity in the labor market as well as the work force, employers receive resumes from an increasingly diverse population. However, studies and field experiments have confirmed the presence of bias in the labor market based on gender, race, and ethnicity. Many employers use automated resume screening to filter the many possible matches. Depending on how the automated screening algorithm is trained it can potentially exhibit bias towards a particular population by favoring certain socio-linguistic characteristics. The resume writing style and socio-linguistics are a potential source of bias as they correlate with protected characteristics such as ethnicity. A biased dataset is often translated into biased AI algorithms and de-biasing algorithms are being contemplated. In this work, we study the effects of socio-linguistic bias on resume to job description matching algorithms. We develop a simple technique, called fair-tf-idf, to match resumes with job descriptions in a fair way by mitigating the socio-linguistic bias.",
                    "[252907607 | Kumar et al. | 2022 | Citations: 91]": "Recent advances in the capacity of large language models to generate human-like text have resulted in their increased adoption in user-facing settings. In parallel, these improvements have prompted a heated discourse around the risks of societal harms they introduce, whether inadvertent or malicious. Several studies have explored these harms and called for their mitigation via development of safer, fairer models. Going beyond enumerating the risks of harms, this work provides a survey of practical methods for addressing potential threats and societal harms from language generation models. We draw on several prior works\u2019 taxonomies of language model risks to present a structured overview of strategies for detecting and ameliorating different kinds of risks/harms of language generators. Bridging diverse strands of research, this survey aims to serve as a practical guide for both LM researchers and practitioners, with explanations of different strategies\u2019 motivations, their limitations, and open problems for future research.",
                    "[4930886 | Garg et al. | 2017 | Citations: 965]": "Significance Word embeddings are a popular machine-learning method that represents each English word by a vector, such that the geometry between these vectors captures semantic relations between the corresponding words. We demonstrate that word embeddings can be used as a powerful tool to quantify historical trends and social change. As specific applications, we develop metrics based on word embeddings to characterize how gender stereotypes and attitudes toward ethnic minorities in the United States evolved during the 20th and 21st centuries starting from 1910. Our framework opens up a fruitful intersection between machine learning and quantitative social science. Word embeddings are a powerful machine-learning framework that represents each English word by a vector. The geometric relationship between these vectors captures meaningful semantic relationships between the corresponding words. In this paper, we develop a framework to demonstrate how the temporal dynamics of the embedding helps to quantify changes in stereotypes and attitudes toward women and ethnic minorities in the 20th and 21st centuries in the United States. We integrate word embeddings trained on 100 y of text data with the US Census to show that changes in the embedding track closely with demographic and occupation shifts over time. The embedding captures societal shifts\u2014e.g., the women\u2019s movement in the 1960s and Asian immigration into the United States\u2014and also illuminates how specific adjectives and occupations became more closely associated with certain populations over time. Our framework for temporal analysis of word embedding opens up a fruitful intersection between machine learning and quantitative social science.",
                    "[58006082 | De-Arteaga et al. | 2019 | Citations: 460]": "We present a large-scale study of gender bias in occupation classification, a task where the use of machine learning may lead to negative outcomes on peoples' lives. We analyze the potential allocation harms that can result from semantic representation bias. To do so, we study the impact on occupation classification of including explicit gender indicators---such as first names and pronouns---in different semantic representations of online biographies. Additionally, we quantify the bias that remains when these indicators are \"scrubbed,\" and describe proxy behavior that occurs in the absence of explicit gender indicators. As we demonstrate, differences in true positive rates between genders are correlated with existing gender imbalances in occupations, which may compound these imbalances."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 342,
                        "end": 1407,
                        "sentence_offsets": [
                            {
                                "start": 342,
                                "end": 460
                            },
                            {
                                "start": 461,
                                "end": 599
                            },
                            {
                                "start": 600,
                                "end": 786
                            },
                            {
                                "start": 787,
                                "end": 1079
                            },
                            {
                                "start": 1080,
                                "end": 1093
                            },
                            {
                                "start": 1094,
                                "end": 1407
                            }
                        ],
                        "ref_mentions": [
                            "4930886",
                            "195316733",
                            "218971825",
                            "252907607",
                            "58006082",
                            "220040093",
                            "204881868"
                        ],
                        "quote": "However, numerous studies have verified that LLMs capture human-like social biases in unprocessed training data (Garg et al., 2017)(Sun et al., 2019). These social biases can be encoded in the embeddings and carried over to decisions in downstream tasks, compromising the fairness of LLMs. Unfair LLM systems make discriminatory, stereotypic and demeaning decisions against vulnerable or marginalized demographics, causing undesirable social impacts and potential harms (Blodgett et al., 2020)(Kumar et al., 2022). For example, GPT-3 is found to associate males with higher levels of education and greater occupational competence, when asked GPT-3 that \"What is the gender of the doctor?\" and \"What is the gender of the nurse?\", its preferred outputs are \"A: Doctor is a masculine noun;\" and \"It's female.\", respectively. In real-world applications, the automatic resume filtering systems can be gender-biased, which tend to assign programmer jobs to men and homemaker jobs to women (De-Arteaga et al., 2019)(Deshpande et al., 2020)[10], and the US healthcare system can be racial biased, which judges black patients with the same risk level to be sicker than white patients (Obermeyer et al., 2019)."
                    }
                ]
            },
            {
                "idx": 19,
                "key": "[261214814 | Venkit | 2023 | Citations: 8]",
                "snippets": "In my study [9], I delve into sentiment analysis and toxicity detection models to identify explicit bias against race, gender, and people with disabilities (PWDs). Through statistical exploration of conversations on social media platforms such as Twitter and Reddit, I gain insights into how disability bias permeates real-world social settings. To quantify explicit sociodemographic bias in sentiment analysis and toxicity analysis models, I create the Bias Identification Test in Sentiment (BITS) corpus1. Applying BITS, I uncover significant biases in popular AIaaS sentiment analysis tools, including TextBlob, VADER, and Google Cloud Natural Language API, as well as toxicity analysis models like Toxic-BERT. Remarkably, all of these models exhibit statistically significant explicit bias against disability, underscoring the need for comprehensive understanding and mitigation of biases affecting such groups.\n\nExpanding on this, my next work [8] delves into the realm of implicit bias in NLP models. While some models may not overtly exhibit bias, they can unintentionally perpetuate harmful stereotypes [4]. To measure and identify implicit bias in commonly used embedding and large language models, I propose a methodology to measure social biases in various NLP architectures. Focusing on people with disabilities (PWD) as a group with complex social dynamics, I analyze various word embedding-based and transformer-based LLMs, revealing significant biases against PWDs in all tested models. These findings expose how models trained on extensive corpora tend to favor ableist language, underscoring the urgency of detecting and addressing implicit bias.\n\nTo explore this aspect, I investigate how LLMs represent nationality and their impact on societal stereotypes [6]. By examining LLM-generated stories for various nationalities, I establish a correlation between sentiment and the population of internet users in a country. The study reveals the unintentional implicit and explicit nationality biases exhibited by GPT-2, with nations having lower internet representation and economic status generating negative sentiment stories and employing a greater number of negative adjectives.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "In my study [9], I delve into sentiment analysis and toxicity detection models to identify explicit bias against race, gender, and people with disabilities (PWDs). Through statistical exploration of conversations on social media platforms such as Twitter and Reddit, I gain insights into how disability bias permeates real-world social settings. To quantify explicit sociodemographic bias in sentiment analysis and toxicity analysis models, I create the Bias Identification Test in Sentiment (BITS) corpus1. Applying BITS, I uncover significant biases in popular AIaaS sentiment analysis tools, including TextBlob, VADER, and Google Cloud Natural Language API, as well as toxicity analysis models like Toxic-BERT. Remarkably, all of these models exhibit statistically significant explicit bias against disability, underscoring the need for comprehensive understanding and mitigation of biases affecting such groups.\n\nExpanding on this, my next work [8] delves into the realm of implicit bias in NLP models. While some models may not overtly exhibit bias, they can unintentionally perpetuate harmful stereotypes [4]. To measure and identify implicit bias in commonly used embedding and large language models, I propose a methodology to measure social biases in various NLP architectures. Focusing on people with disabilities (PWD) as a group with complex social dynamics, I analyze various word embedding-based and transformer-based LLMs, revealing significant biases against PWDs in all tested models. These findings expose how models trained on extensive corpora tend to favor ableist language, underscoring the urgency of detecting and addressing implicit bias.\n\nTo explore this aspect, I investigate how LLMs represent nationality and their impact on societal stereotypes [6]. By examining LLM-generated stories for various nationalities, I establish a correlation between sentiment and the population of internet users in a country. The study reveals the unintentional implicit and explicit nationality biases exhibited by GPT-2, with nations having lower internet representation and economic status generating negative sentiment stories and employing a greater number of negative adjectives.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 20,
                "key": "[265212726 | Manerba et al. | 2023 | Citations: 20]",
                "snippets": "Social bias can be defined as the manifestation through language of \"prejudices, stereotypes, and discriminatory attitudes against certain groups of people\" (Navigli et al., 2023). These biases are featured in training datasets and are carried over into downstream applications, resulting in, for instance, classification errors concerning specific minorities and the generation of harmful content when models are prompted with sensitive identities (Cui et al., 2024;Gallegos et al., 2023).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[258688053 | Navigli et al. | 2023 | Citations: 283]": "In this article, we introduce and discuss the pervasive issue of bias in the large language models that are currently at the core of mainstream approaches to Natural Language Processing (NLP). We first introduce data selection bias, that is, the bias caused by the choice of texts that make up a training corpus. Then, we survey the different types of social bias evidenced in the text generated by language models trained on such corpora, ranging from gender to age, from sexual orientation to ethnicity, and from religion to culture. We conclude with directions focused on measuring, reducing, and tackling the aforementioned types of bias."
                },
                "metadata": [
                    {
                        "section_title": "Social Bias Probing Framework",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 490,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 181
                            },
                            {
                                "start": 182,
                                "end": 491
                            }
                        ],
                        "ref_mentions": [
                            "258688053"
                        ],
                        "quote": "Social bias can be defined as the manifestation through language of \"prejudices, stereotypes, and discriminatory attitudes against certain groups of people\" (Navigli et al., 2023). These biases are featured in training datasets and are carried over into downstream applications, resulting in, for instance, classification errors concerning specific minorities and the generation of harmful content when models are prompted with sensitive identities (Cui et al., 2024;Gallegos et al., 2023)."
                    }
                ]
            },
            {
                "idx": 21,
                "key": "[265609662 | Tanyel et al. | 2023 | Citations: 0]",
                "snippets": "One of the more insidious ways in which discrimination can manifest is through natural language models, which are computational models that use statistical algorithms to understand and generate human language. These models have become ubiquitous in our daily lives, powering everything from virtual assistants to search engines and chatbots. However, as with any human-created system, natural language models are susceptible to biases and discrimination that can be embedded in the data they are trained on [21].\n\nThe problem of discrimination in language models arises because these models are developed based on huge amounts of text data retrieved from the internet, which contains a plethora of societal biases and prejudices. For instance, language models might be trained on text data that contains racist or sexist language, which can lead to the model associating certain words or phrases with negative stereotypes. As a result, when the model is used to generate text or respond to queries, it may produce outputs that perpetuate harmful stereotypes and discriminatory practices.\n\nFurthermore, language models can also be discriminatory in their output if the training data is not diverse enough to represent different demographics or if the model is biased towards a particular dialect or language variety. This can result in language models that are more accurate for certain groups than for others, which can perpetuate existing power imbalances and inequalities.\n\nThe consequences of discriminatory language models can be significant, ranging from perpetuating harmful stereotypes to reinforcing systemic discrimination against marginalized groups [22].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[236950797 | Kirk et al. | 2021 | Citations: 192]": "The capabilities of natural language models trained on large-scale data have increased immensely over the past few years. Open source libraries such as HuggingFace have made these models easily available and accessible. While prior research has identified biases in large language models, this paper considers biases contained in the most popular versions of these models when applied `out-of-the-box' for downstream tasks. We focus on generative language models as they are well-suited for extracting biases inherited from training data. Specifically, we conduct an in-depth analysis of GPT-2, which is the most downloaded text generation model on HuggingFace, with over half a million downloads per month. We assess biases related to occupational associations for different protected categories by intersecting gender with religion, sexuality, ethnicity, political affiliation, and continental name origin. Using a template-based data collection pipeline, we collect 396K sentence completions made by GPT-2 and find: (i) The machine-predicted jobs are less diverse and more stereotypical for women than for men, especially for intersections; (ii) Intersectional interactions are highly relevant for occupational associations, which we quantify by fitting 262 logistic models; (iii) For most occupations, GPT-2 reflects the skewed gender and ethnicity distribution found in US Labor Bureau data, and even pulls the societally-skewed distribution towards gender parity in cases where its predictions deviate from real labor market observations. This raises the normative question of what language models should learn - whether they should reflect or correct for existing inequalities."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 297,
                        "end": 1962,
                        "sentence_offsets": [
                            {
                                "start": 276,
                                "end": 362
                            },
                            {
                                "start": 363,
                                "end": 572
                            },
                            {
                                "start": 573,
                                "end": 704
                            },
                            {
                                "start": 705,
                                "end": 875
                            },
                            {
                                "start": 878,
                                "end": 1093
                            },
                            {
                                "start": 1094,
                                "end": 1286
                            },
                            {
                                "start": 1287,
                                "end": 1451
                            },
                            {
                                "start": 1454,
                                "end": 1680
                            },
                            {
                                "start": 1681,
                                "end": 1839
                            },
                            {
                                "start": 1842,
                                "end": 2031
                            }
                        ],
                        "ref_mentions": [
                            "236950797"
                        ],
                        "quote": "One of the more insidious ways in which discrimination can manifest is through natural language models, which are computational models that use statistical algorithms to understand and generate human language. These models have become ubiquitous in our daily lives, powering everything from virtual assistants to search engines and chatbots. However, as with any human-created system, natural language models are susceptible to biases and discrimination that can be embedded in the data they are trained on [21].\n\nThe problem of discrimination in language models arises because these models are developed based on huge amounts of text data retrieved from the internet, which contains a plethora of societal biases and prejudices. For instance, language models might be trained on text data that contains racist or sexist language, which can lead to the model associating certain words or phrases with negative stereotypes. As a result, when the model is used to generate text or respond to queries, it may produce outputs that perpetuate harmful stereotypes and discriminatory practices.\n\nFurthermore, language models can also be discriminatory in their output if the training data is not diverse enough to represent different demographics or if the model is biased towards a particular dialect or language variety. This can result in language models that are more accurate for certain groups than for others, which can perpetuate existing power imbalances and inequalities.\n\nThe consequences of discriminatory language models can be significant, ranging from perpetuating harmful stereotypes to reinforcing systemic discrimination against marginalized groups [22]."
                    }
                ]
            },
            {
                "idx": 22,
                "key": "[266521434 | Oketunji et al. | 2023 | Citations: 12]",
                "snippets": "The formulation of the Large Language Model Bias Index (LLMBI) necessitates a sweeping understanding of the multifaceted nature of bias in Large Language Models (LLMs). These biases, spanning across various dimensions such as gender, religion, race, age, nationality, disability, sexual orientation, physical appearance, and socioeconomic status, can signicantly impact the outputs of LLMs, leading to skewed and potentially harmful outcomes. \n\nGender bias in LLMs often manifests in stereotypical representations and unequal treatment of dierent genders. This bias can perpetuate gender stereotypes and discrimination, aecting the fairness of AI applications in areas like recruitment and social media (Zhao et al., 2017) (Zhao et al., 2017). Religion bias in LLMs can lead to the marginalisation of certain religious groups, fostering an environment of intolerance and misunderstanding (Aroyo et al., 2015) (Aroyo et al., 2015). \n\nRacial bias is another pressing concern, as LLMs can perpetuate racial stereotypes and prejudices. This bias can have severe implications in legal, healthcare, and nancial systems, where it can lead to discriminatory practices (Sweeney, 2013). Age bias in LLMs can result in underrepresenting or misrepresenting specic age groups, impacting sectors like healthcare and marketing (Levy, 2003) (Levy, 2003). \n\nNationality bias in LLMs can lead to the preferential treatment of certain nationalities over others, aecting global perceptions and international relations (Liao et al., 2018) (Liao et al., 2020). Disability bias in LLMs can result in the exclusion or misrepresentation of people with disabilities, impacting accessibility and inclusivity (Trewin et al., 2019) [14]. \n\nSexual orientation bias in LLMs can perpetuate harmful stereotypes and discrimination against LGBTQ+ individuals, aecting mental health and social acceptance (Costanza-Chock, 2018) (Costanza-Chock, 2018). Physical appearance bias in LLMs can lead to body shaming and unrealistic beauty standards, impacting mental health and self-esteem (Yee et al., 2007) (Yee et al., 2007). \n\nLastly, socioeconomic status bias in LLMs can reinforce class divisions and economic inequalities, aecting opportunities and social mobility (O'Neil, 2016) [17].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[13893029 | Yee et al. | 2007 | Citations: 1736]": "Virtual environments, such as online games and web-based chat rooms, increasingly allow us to alter our digital self-representations dramatically and easily. But as we change our self-representations, do our self-representations change our behavior in turn? In 2 experimental studies, we explore the hypothesis that an individual\u2019s behavior conforms to their digital self-representation independent of how others perceive them\u2014a process we term the Proteus Effect. In the first study, participants assigned to more attractive avatars in immersive virtual environments were more intimate with confederates in a self-disclosure and interpersonal distance task than participants assigned to less attractive avatars. In our second study, participants assigned taller avatars behaved more confidently in a negotiation task than participants assigned shorter avatars. We discuss the implications of the Proteus Effect with regards to social interactions in online environments.",
                    "[1389483 | Zhao et al. | 2017 | Citations: 973]": "Language is increasingly being used to de-fine rich visual recognition problems with supporting image collections sourced from the web. Structured prediction models are used in these tasks to take advantage of correlations between co-occurring labels and visual input but risk inadvertently encoding social biases found in web corpora. In this work, we study data and models associated with multilabel object classification and visual semantic role labeling. We find that (a) datasets for these tasks contain significant gender bias and (b) models trained on these datasets further amplify existing bias. For example, the activity cooking is over 33% more likely to involve females than males in a training set, and a trained model further amplifies the disparity to 68% at test time. We propose to inject corpus-level constraints for calibrating existing structured prediction models and design an algorithm based on Lagrangian relaxation for collective inference. Our method results in almost no performance loss for the underlying recognition task but decreases the magnitude of bias amplification by 47.5% and 40.5% for multilabel classification and visual semantic role labeling, respectively\u3002",
                    "[150082799 | Costanza-Chock | 2018 | Citations: 240]": "Design is key to our collective liberation, but most design processes today reproduce inequalities structured by what Black feminist scholars call the matrix of domination. Intersecting inequalities are manifest at all levels of the design process. This paper builds upon the Design Justice Principles, developed by an emerging network of designers and community organizers, to propose a working definition of design justice: Design justice is a field of theory and practice that is concerned with how the design of objects and systems influences the distribution of risks, harms, and benefits among various groups of people. Design justice focuses on the ways that design reproduces, is reproduced by, and/or challenges the matrix of domination (white supremacy, heteropatriarchy, capitalism, and settler colonialism). Design justice is also a growing social movement that aims to ensure a more equitable distribution of design\u2019s benefits and burdens; fair and meaningful participation in design decisions; and recognition of community based design traditions, knowledge, and practices.",
                    "[17169507 | Levy | 2003 | Citations: 824]": "In the first part of this article, a wide range of research is drawn upon to describe the process by which aging stereotypes are internalized in younger individuals and then become self-stereotypes when individuals reach old age. The second part consists of a review of the author's cross-cultural, experimental, and longitudinal research that examines the cognitive and physical effects of aging self-stereotypes. The final section presents suggestions for future research relating to aging self-stereotypes.",
                    "[210064344 | Liao et al. | 2020 | Citations: 721]": "A surge of interest in explainable AI (XAI) has led to a vast collection of algorithmic work on the topic. While many recognize the necessity to incorporate explainability features in AI systems, how to address real-world user needs for understanding AI remains an open question. By interviewing 20 UX and design practitioners working on various AI products, we seek to identify gaps between the current XAI algorithmic work and practices to create explainable AI products. To do so, we develop an algorithm-informed XAI question bank in which user needs for explainability are represented as prototypical questions users might ask about the AI, and use it as a study probe. Our work contributes insights into the design space of XAI, informs efforts to support design practices in this space, and identifies opportunities for future XAI work. We also provide an extended XAI question bank and discuss how it can be used for creating user-centered XAI.",
                    "[6134326 | Aroyo et al. | 2015 | Citations: 328]": "Big data is having a disruptive impact across the sciences. Human annotation of semantic interpretation tasks is a critical part of big data semantics, but it is based on an antiquated ideal of a single correct truth that needs to be similarly disrupted. We expose seven myths about human annotation, most of which derive from that antiquated ideal of truth, and dispell these myths with examples from our research. We propose a new theory of truth, crowd truth, that is based on the intuition that human interpretation is subjective, and that measuring annotations on the same objects of interpretation (in our examples, sentences) across a crowd will provide a useful representation of their subjectivity and the range of reasonable interpretations."
                },
                "metadata": [
                    {
                        "section_title": "Problem Formulation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 2170,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 168
                            },
                            {
                                "start": 169,
                                "end": 442
                            },
                            {
                                "start": 445,
                                "end": 555
                            },
                            {
                                "start": 556,
                                "end": 728
                            },
                            {
                                "start": 729,
                                "end": 900
                            },
                            {
                                "start": 903,
                                "end": 1001
                            },
                            {
                                "start": 1002,
                                "end": 1146
                            },
                            {
                                "start": 1147,
                                "end": 1300
                            },
                            {
                                "start": 1303,
                                "end": 1485
                            },
                            {
                                "start": 1486,
                                "end": 1655
                            },
                            {
                                "start": 1658,
                                "end": 1844
                            },
                            {
                                "start": 1845,
                                "end": 2006
                            },
                            {
                                "start": 2009,
                                "end": 2170
                            }
                        ],
                        "ref_mentions": [
                            "1389483",
                            "1389483",
                            "6134326",
                            "6134326",
                            "17169507",
                            "17169507",
                            "210064344",
                            "150082799",
                            "13893029",
                            "13893029"
                        ],
                        "quote": "The formulation of the Large Language Model Bias Index (LLMBI) necessitates a sweeping understanding of the multifaceted nature of bias in Large Language Models (LLMs). These biases, spanning across various dimensions such as gender, religion, race, age, nationality, disability, sexual orientation, physical appearance, and socioeconomic status, can signicantly impact the outputs of LLMs, leading to skewed and potentially harmful outcomes. \n\nGender bias in LLMs often manifests in stereotypical representations and unequal treatment of dierent genders. This bias can perpetuate gender stereotypes and discrimination, aecting the fairness of AI applications in areas like recruitment and social media (Zhao et al., 2017) (Zhao et al., 2017). Religion bias in LLMs can lead to the marginalisation of certain religious groups, fostering an environment of intolerance and misunderstanding (Aroyo et al., 2015) (Aroyo et al., 2015). \n\nRacial bias is another pressing concern, as LLMs can perpetuate racial stereotypes and prejudices. This bias can have severe implications in legal, healthcare, and nancial systems, where it can lead to discriminatory practices (Sweeney, 2013). Age bias in LLMs can result in underrepresenting or misrepresenting specic age groups, impacting sectors like healthcare and marketing (Levy, 2003) (Levy, 2003). \n\nNationality bias in LLMs can lead to the preferential treatment of certain nationalities over others, aecting global perceptions and international relations (Liao et al., 2018) (Liao et al., 2020). Disability bias in LLMs can result in the exclusion or misrepresentation of people with disabilities, impacting accessibility and inclusivity (Trewin et al., 2019) [14]. \n\nSexual orientation bias in LLMs can perpetuate harmful stereotypes and discrimination against LGBTQ+ individuals, aecting mental health and social acceptance (Costanza-Chock, 2018) (Costanza-Chock, 2018). Physical appearance bias in LLMs can lead to body shaming and unrealistic beauty standards, impacting mental health and self-esteem (Yee et al., 2007) (Yee et al., 2007). \n\nLastly, socioeconomic status bias in LLMs can reinforce class divisions and economic inequalities, aecting opportunities and social mobility (O'Neil, 2016) [17]."
                    }
                ]
            },
            {
                "idx": 23,
                "key": "[267627928 | Jain et al. | 2024 | Citations: 1]",
                "snippets": "Pretrained Language Models (PLMs) are widely used in NLP for various tasks. Recent studies have identified various biases that such models exhibit and have proposed methods to correct these biases. However, most of the works address a limited set of bias dimensions independently such as gender, race, or religion.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 314,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Pretrained Language Models (PLMs) are widely used in NLP for various tasks. Recent studies have identified various biases that such models exhibit and have proposed methods to correct these biases. However, most of the works address a limited set of bias dimensions independently such as gender, race, or religion."
                    }
                ]
            },
            {
                "idx": 24,
                "key": "[268157336 | Patil et al. | 2024 | Citations: 80]",
                "snippets": "Bias and fairness, if not adequately addressed, pose serious societal implications in the form of biased language generation and its impact on some segments of society. Basis can creep into LLMs from several sources discussed below. The first source of bias, dataset bias, stems from the datasets that were used to train the LLMs. If the datasets contain biases related to race, gender, religion, or socioeconomic status, the models inherit and amplify them.\n\nUnderrepresentation or misrepresentation of certain groups in the training data can lead to representation bias and biased language generation. The LLM developers should have checks and balances to ensure that all perspectives are adequately represented in the datasets. Otherwise, the model will produce inaccurate or skewed output for underrepresented groups. If the training data contain stereotypes, models amplify stereotyping and perpetuate prejudices. Fairness across demographics is a complex challenge but essential for advancing LLMs.\n\nCentextual bias stems from the context in which the language models are used. This poses severe and negative implications in applications such as recommender systems, employee hiring and promotions, clustering, and sentiment analysis.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Fairness",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1240,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 168
                            },
                            {
                                "start": 169,
                                "end": 232
                            },
                            {
                                "start": 233,
                                "end": 330
                            },
                            {
                                "start": 331,
                                "end": 458
                            },
                            {
                                "start": 461,
                                "end": 604
                            },
                            {
                                "start": 605,
                                "end": 731
                            },
                            {
                                "start": 732,
                                "end": 822
                            },
                            {
                                "start": 823,
                                "end": 919
                            },
                            {
                                "start": 920,
                                "end": 1005
                            },
                            {
                                "start": 1008,
                                "end": 1085
                            },
                            {
                                "start": 1086,
                                "end": 1242
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Bias and fairness, if not adequately addressed, pose serious societal implications in the form of biased language generation and its impact on some segments of society. Basis can creep into LLMs from several sources discussed below. The first source of bias, dataset bias, stems from the datasets that were used to train the LLMs. If the datasets contain biases related to race, gender, religion, or socioeconomic status, the models inherit and amplify them.\n\nUnderrepresentation or misrepresentation of certain groups in the training data can lead to representation bias and biased language generation. The LLM developers should have checks and balances to ensure that all perspectives are adequately represented in the datasets. Otherwise, the model will produce inaccurate or skewed output for underrepresented groups. If the training data contain stereotypes, models amplify stereotyping and perpetuate prejudices. Fairness across demographics is a complex challenge but essential for advancing LLMs.\n\nCentextual bias stems from the context in which the language models are used. This poses severe and negative implications in applications such as recommender systems, employee hiring and promotions, clustering, and sentiment analysis."
                    }
                ]
            },
            {
                "idx": 25,
                "key": "[268553687 | Cai et al. | 2024 | Citations: 5]",
                "snippets": "Bias refers to the existence of consistent inaccuracies, misattributions, or erroneous perceptions leading to a preference for specific groups or concepts, the reinforcement of stereotypes, or the formation of flawed conclusions derived from established patterns, it now profoundly impact users and society at large, underscoring the urgent need for a comprehensive examination and mitigation of these issues [9]26].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 385,
                        "end": 801,
                        "sentence_offsets": [
                            {
                                "start": 385,
                                "end": 801
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Bias refers to the existence of consistent inaccuracies, misattributions, or erroneous perceptions leading to a preference for specific groups or concepts, the reinforcement of stereotypes, or the formation of flawed conclusions derived from established patterns, it now profoundly impact users and society at large, underscoring the urgent need for a comprehensive examination and mitigation of these issues [9]26]."
                    }
                ]
            },
            {
                "idx": 26,
                "key": "[268856702 | Chu et al. | 2024 | Citations: 41]",
                "snippets": "Language modeling bias, often defined as \"bias that results in harm to various social groups\" [70], presents itself in various forms, encompassing the association of specific stereotypes with groups, the devaluation of certain groups, the underrepresentation of particular social groups, and the unequal allocation of resources among groups [44].Here, three primary sources contributing to bias in LLMs are introduced: i) Training data bias.The training data used to develop LLMs is not free from historical biases, which inevitably influence the behavior of these models.For instance, if the training data includes the statement \"all programmers are male and all nurses are female, \" the model is likely to learn and perpetuate these occupational and gender biases in its outputs, reflecting a narrow and biased view of societal roles (Bolukbasi et al., 2016)(Caliskan et al., 2016).Additionally, a significant disparity in the training data could also lead to biased outcomes [161].For example, Buolamwini and Gebru (Buolamwini et al., 2018) highlighted significant disparities in datasets like IJB-A and Adience, where predominantly light-skinned individuals make up 79.6% and 86.2% of the data, respectively, thereby biasing analyses toward underrepresented dark-skinned groups (Mehrabi et al., 2019).\n\nii) Embedding bias.Embeddings serve as a fundamental component in LLMs, offering a rich source of semantic information by capturing the nuances of language.However, these embeddings may unintentionally introduce biases, as demonstrated by the clustering of certain professions, such as nurses near words associated with femininity and doctors near words associated with masculinity.This phenomenon inadvertently introduces semantic bias into downstream models, impacting their performance and fairness [9]63].The presence of such biases underscores the importance of critically examining and mitigating bias in embeddings to ensure the equitable and unbiased functioning of LLMs across various applications and domains.\n\niii) Label bias.In instruction tuning scenarios, biases can arise from the subjective judgments of human annotators who provide labels or annotations for training data (Sap et al., 2019).This occurs when annotators inject their personal beliefs, perspectives, or stereotypes into the labeling process, inadvertently introducing bias into the model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[1704893 | Bolukbasi et al. | 2016 | Citations: 3153]": "The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.",
                    "[196211238 | Sap et al. | 2019 | Citations: 848]": "We investigate how annotators\u2019 insensitivity to differences in dialect can lead to racial bias in automatic hate speech detection models, potentially amplifying harm against minority populations. We first uncover unexpected correlations between surface markers of African American English (AAE) and ratings of toxicity in several widely-used hate speech datasets. Then, we show that models trained on these corpora acquire and propagate these biases, such that AAE tweets and tweets by self-identified African Americans are up to two times more likely to be labelled as offensive compared to others. Finally, we propose *dialect* and *race priming* as ways to reduce the racial bias in annotation, showing that when annotators are made explicitly aware of an AAE tweet\u2019s dialect they are significantly less likely to label the tweet as offensive.",
                    "[201666566 | Mehrabi et al. | 2019 | Citations: 4373]": "With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.",
                    "[23163324 | Caliskan et al. | 2016 | Citations: 2673]": "Machines learn what people know implicitly AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan et al. now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs\u2014for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior. Science, this issue p. 183; see also p. 133 Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias. Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology."
                },
                "metadata": [
                    {
                        "section_title": "Factors Contributing to Bias in LLMs",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 2285,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 346
                            },
                            {
                                "start": 346,
                                "end": 441
                            },
                            {
                                "start": 441,
                                "end": 572
                            },
                            {
                                "start": 572,
                                "end": 844
                            },
                            {
                                "start": 844,
                                "end": 944
                            },
                            {
                                "start": 944,
                                "end": 1227
                            },
                            {
                                "start": 1229,
                                "end": 1248
                            },
                            {
                                "start": 1248,
                                "end": 1385
                            },
                            {
                                "start": 1385,
                                "end": 1611
                            },
                            {
                                "start": 1611,
                                "end": 1738
                            },
                            {
                                "start": 1738,
                                "end": 1948
                            },
                            {
                                "start": 1950,
                                "end": 1966
                            },
                            {
                                "start": 1966,
                                "end": 2124
                            },
                            {
                                "start": 2124,
                                "end": 2285
                            }
                        ],
                        "ref_mentions": [
                            "1704893",
                            "23163324",
                            "3298854",
                            "201666566",
                            "196211238"
                        ],
                        "quote": "Language modeling bias, often defined as \"bias that results in harm to various social groups\" [70], presents itself in various forms, encompassing the association of specific stereotypes with groups, the devaluation of certain groups, the underrepresentation of particular social groups, and the unequal allocation of resources among groups [44].Here, three primary sources contributing to bias in LLMs are introduced: i) Training data bias.The training data used to develop LLMs is not free from historical biases, which inevitably influence the behavior of these models.For instance, if the training data includes the statement \"all programmers are male and all nurses are female, \" the model is likely to learn and perpetuate these occupational and gender biases in its outputs, reflecting a narrow and biased view of societal roles (Bolukbasi et al., 2016)(Caliskan et al., 2016).Additionally, a significant disparity in the training data could also lead to biased outcomes [161].For example, Buolamwini and Gebru (Buolamwini et al., 2018) highlighted significant disparities in datasets like IJB-A and Adience, where predominantly light-skinned individuals make up 79.6% and 86.2% of the data, respectively, thereby biasing analyses toward underrepresented dark-skinned groups (Mehrabi et al., 2019).\n\nii) Embedding bias.Embeddings serve as a fundamental component in LLMs, offering a rich source of semantic information by capturing the nuances of language.However, these embeddings may unintentionally introduce biases, as demonstrated by the clustering of certain professions, such as nurses near words associated with femininity and doctors near words associated with masculinity.This phenomenon inadvertently introduces semantic bias into downstream models, impacting their performance and fairness [9]63].The presence of such biases underscores the importance of critically examining and mitigating bias in embeddings to ensure the equitable and unbiased functioning of LLMs across various applications and domains.\n\niii) Label bias.In instruction tuning scenarios, biases can arise from the subjective judgments of human annotators who provide labels or annotations for training data (Sap et al., 2019).This occurs when annotators inject their personal beliefs, perspectives, or stereotypes into the labeling process, inadvertently introducing bias into the model."
                    }
                ]
            },
            {
                "idx": 27,
                "key": "[270357379 | Lin et al. | 2024 | Citations: 9]",
                "snippets": "While extensive research has addressed bias in various machine learning domains, such as Automatic Speech Recognition (ASR) (Ngueajio et al., 2022)2,3], Speech Translation (Savoldi et al., 2022)(Gaido et al., 2021), Facial Emotion Recognition [6], and Automatic Speaker Verification (ASV) (Fenu et al., 2020), limited attention has been paid to social bias within SER systems.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[222005741 | Fenu et al. | 2020 | Citations: 21]": "To allow individuals to complete voice-based tasks (e.g., send messages or make payments), modern automated systems are required to match the speaker\u2019s voice to a unique digital identity representation for verification. Despite the increasing accuracy achieved so far, it still remains under-explored how the decisions made by such systems may be influenced by the inherent characteristics of the individual under consideration. In this paper, we investigate how state-of-the-art speaker verification models are susceptible to unfairness towards legally-protected classes of individuals, characterized by a common sensitive attribute (i.e., gender, age, language). To this end, we first arranged a voice dataset, with the aim of including and identifying various demographic classes. Then, we conducted a performance analysis at different levels, from equal error rates to verification score distributions. Experiments show that individuals belonging to certain demographic groups systematically experience higher error rates, highlighting the need of fairer speaker recognition models and, by extension, of proper evaluation frameworks.",
                    "[235248117 | Gaido et al. | 2021 | Citations: 15]": "Having recognized gender bias as a major issue affecting current translation technologies, researchers have primarily attempted to mitigate it by working on the data front. However, whether algorithmic aspects concur to exacerbate unwanted outputs remains so far under-investigated. In this work, we bring the analysis on gender bias in automatic translation onto a seemingly neutral yet critical component: word segmentation. Can segmenting methods influence the ability to translate gender? Do certain segmentation approaches penalize the representation of feminine linguistic markings? We address these questions by comparing 5 existing segmentation strategies on the target side of speech translation systems. Our results on two language pairs (English-Italian/French) show that state-of-the-art sub-word splitting (BPE) comes at the cost of higher gender bias. In light of this finding, we propose a combined approach that preserves BPE overall translation quality, while leveraging the higher ability of character-based segmentation to properly translate gender.",
                    "[247595186 | Savoldi et al. | 2022 | Citations: 27]": "Gender bias is largely recognized as a problematic phenomenon affecting language technologies, with recent studies underscoring that it might surface differently across languages. However, most of current evaluation practices adopt a word-level focus on a narrow set of occupational nouns under synthetic conditions. Such protocols overlook key features of grammatical gender languages, which are characterized by morphosyntactic chains of gender agreement, marked on a variety of lexical items and parts-of-speech (POS). To overcome this limitation, we enrich the natural, gender-sensitive MuST-SHE corpus (Bentivogli et al., 2020) with two new linguistic annotation layers (POS and agreement chains), and explore to what extent different lexical categories and agreement phenomena are impacted by gender skews. Focusing on speech translation, we conduct a multifaceted evaluation on three language directions (English-French/Italian/Spanish), with models trained on varying amounts of data and different word segmentation techniques. By shedding light on model behaviours, gender bias, and its detection at several levels of granularity, our findings emphasize the value of dedicated analyses beyond aggregated overall results.",
                    "[253581416 | Ngueajio et al. | 2022 | Citations: 38]": "Speech is the fundamental means of communication between humans. The advent of AI and sophisticated speech technologies have led to the rapid proliferation of human-to-computer-based interactions, fueled primarily by Automatic Speech Recognition (ASR) systems. ASR systems normally take human speech in the form of audio and convert it into words, but for some users, it cannot decode the speech, and any output text is filled with errors that are incomprehensible to the human reader. These systems do not work equally for everyone and actually hinder the productivity of some users. In this paper, we present research that addresses ASR biases against gender, race, and the sick and disabled, while exploring studies that propose ASR debiasing techniques for mitigating these discriminations. We also discuss techniques for designing a more accessible and inclusive ASR technology. For each approach surveyed, we also provide a summary of the investigation and methods applied, the ASR systems and corpora used, and the research findings, and highlight their strengths and/or weaknesses. Finally, we propose future opportunities for Natural Language Processing researchers to explore in the next level creation of ASR technologies."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 561,
                        "end": 864,
                        "sentence_offsets": [
                            {
                                "start": 561,
                                "end": 864
                            }
                        ],
                        "ref_mentions": [
                            "253581416",
                            "247595186",
                            "235248117",
                            "222005741"
                        ],
                        "quote": "While extensive research has addressed bias in various machine learning domains, such as Automatic Speech Recognition (ASR) (Ngueajio et al., 2022)2,3], Speech Translation (Savoldi et al., 2022)(Gaido et al., 2021), Facial Emotion Recognition [6], and Automatic Speaker Verification (ASV) (Fenu et al., 2020), limited attention has been paid to social bias within SER systems."
                    }
                ]
            },
            {
                "idx": 28,
                "key": "[270560549 | Das et al. | 2024 | Citations: 2]",
                "snippets": "In our research, we specifically examine four types of biases: gender, race, disability, and religion. Gender bias refers to the prejudiced treatment based on an individual's sex or gender identity. Race bias involves discriminatory actions or attitudes towards individuals based on their racial or ethnic background. Disability bias encompasses unfair treatment of people with physical or mental impairments. Religion bias involves prejudices and discriminatory behaviors directed at individuals based on their religious beliefs or practices. Our study aims to analyze the prevalence and impact of these biases in various contexts.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1278,
                        "end": 1910,
                        "sentence_offsets": [
                            {
                                "start": 1276,
                                "end": 1380
                            },
                            {
                                "start": 1381,
                                "end": 1476
                            },
                            {
                                "start": 1477,
                                "end": 1595
                            },
                            {
                                "start": 1596,
                                "end": 1687
                            },
                            {
                                "start": 1688,
                                "end": 1821
                            },
                            {
                                "start": 1822,
                                "end": 1910
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In our research, we specifically examine four types of biases: gender, race, disability, and religion. Gender bias refers to the prejudiced treatment based on an individual's sex or gender identity. Race bias involves discriminatory actions or attitudes towards individuals based on their racial or ethnic background. Disability bias encompasses unfair treatment of people with physical or mental impairments. Religion bias involves prejudices and discriminatory behaviors directed at individuals based on their religious beliefs or practices. Our study aims to analyze the prevalence and impact of these biases in various contexts."
                    }
                ]
            },
            {
                "idx": 29,
                "key": "[270878706 | Raj et al. | 2024 | Citations: 14]",
                "snippets": "Large Language Models (LLMs) perpetuate social biases, reflecting prejudices in their training data and reinforcing societal stereotypes and inequalities.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 154,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Large Language Models (LLMs) perpetuate social biases, reflecting prejudices in their training data and reinforcing societal stereotypes and inequalities."
                    }
                ]
            },
            {
                "idx": 30,
                "key": "[271161894 | Grieve et al. | 2024 | Citations: 10]",
                "snippets": "NLP systems generally suffer from social bias: their real-world application leads to outcomes that unfairly disadvantage or harm specific social groups [58,59,60,61]. Social bias can be introduced at various points during the development and deployment of NLP systems [62], but given the unsupervised nature of language modeling, training corpora are a key source of social bias in LLMs [10,63]. While bias in NLP systems can harm people in various ways [59], in this section, we primarily focus on two common harmful outcomes of social bias. These two types of harms are most commonly discussed in terms of quality-of-service harms and stereotyping harms (e.g., [64,65,60,66]), although many different systems have been proposed for classifying biases and harms in NLP, which define these terms in somewhat different ways, along with many additional and often overlapping categories [59]. Both of these types of harms are especially relevant to LLMs, and crucially, we believe both can be better understood and addressed in language modeling by adopting a sociolinguistic perspective (see Figure 3). First, social bias can be characterized by poor system performance for certain social groups that are interacting with LLMs and applications based on language models: token prediction will be more or less accurate depending on the social origins of the language inputted into the system. For example, ChatGPT might have difficulty correctly understanding prompts written by people from certain social groups due to their use of non-standard or socially restricted language patterns. This type of bias leads to what is known as quality-of-service harms, where the performance of these systems varies depending on the social background of the user [64,60]. These types of quality-of-service harms can often be the product of selection bias, as they result from how training data is selected from across the society whose language is being modeled [58]: in general, if language data from certain social groups is under-represented in the training data for a language model, we should expect that NLP applications based on that model will process language structures produced by these groups less accurately and consequently exhibit poorer performance for these groups [59]67].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[258041203 | Ferrara | 2023 | Citations: 261]": "As generative language models, exemplified by ChatGPT, continue to advance in their capabilities, the spotlight on biases inherent in these models intensifies. This paper delves into the distinctive challenges and risks associated with biases specifically in large-scale language models. We explore the origins of biases, stemming from factors such as training data, model specifications, algorithmic constraints, product design, and policy decisions. Our examination extends to the ethical implications arising from the unintended consequences of biased model outputs. In addition, we analyze the intricacies of mitigating biases, acknowledging the inevitable persistence of some biases, and consider the consequences of deploying these models across diverse applications, including virtual assistants, content generation, and chatbots. Finally, we provide an overview of current approaches for identifying, quantifying, and mitigating biases in language models, underscoring the need for a collaborative, multidisciplinary effort to craft AI systems that embody equity, transparency, and responsibility. This article aims to catalyze a thoughtful discourse within the AI community, prompting researchers and developers to consider the unique role of biases in the domain of generative language models and the ongoing quest for ethical AI.",
                    "[209461005 | Shah et al. | 2019 | Citations: 260]": "An increasing number of natural language processing papers address the effect of bias on predictions, introducing mitigation techniques at different parts of the standard NLP pipeline (data and models). However, these works have been conducted individually, without a unifying framework to organize efforts within the field. This situation leads to repetitive approaches, and focuses overly on bias symptoms/effects, rather than on their origins, which could limit the development of effective countermeasures. In this paper, we propose a unifying predictive bias framework for NLP. We summarize the NLP literature and suggest general mathematical definitions of predictive bias. We differentiate two consequences of bias: outcome disparities and error disparities, as well as four potential origins of biases: label bias, selection bias, model overamplification, and semantic bias. Our framework serves as an overview of predictive bias in NLP, integrating existing work into a single structure, and providing a conceptual baseline for improved frameworks.",
                    "[237298625 | Hovy et al. | 2021 | Citations: 253]": "Abstract Recently, there has been an increased interest in demographically grounded bias in natural language processing (NLP) applications. Much of the recent work has focused on describing bias and providing an overview of bias in a larger context. Here, we provide a simple, actionable summary of this recent work. We outline five sources where bias can occur in NLP systems: (1) the data, (2) the annotation process, (3) the input representations, (4) the models, and finally (5) the research design (or how we conceptualize our research). We explore each of the bias sources in detail in this article, including examples and links to related work, as well as potential counter\u2010measures.",
                    "[252907216 | Dev et al. | 2021 | Citations: 88]": "Recent studies show that Natural Language Processing (NLP) technologies propagate societal biases about demographic groups associated with attributes such as gender, race, and nationality. To create interventions and mitigate these biases and associated harms, it is vital to be able to detect and measure such biases. While existing works propose bias evaluation and mitigation methods for various tasks, there remains a need to cohesively understand the biases and the specific harms they measure, and how different measures compare with each other. To address this gap, this work presents a practical framework of harms and a series of questions that practitioners can answer to guide the development of bias measures. As a validation of our framework and documentation questions, we also present several case studies of how existing bias measures in NLP -- both intrinsic measures of bias in representations and extrinsic measures of bias of downstream applications -- can be aligned with different harms and how our proposed documentation questions facilitates more holistic understanding of what bias measures are measuring.",
                    "[258688053 | Navigli et al. | 2023 | Citations: 283]": "In this article, we introduce and discuss the pervasive issue of bias in the large language models that are currently at the core of mainstream approaches to Natural Language Processing (NLP). We first introduce data selection bias, that is, the bias caused by the choice of texts that make up a training corpus. Then, we survey the different types of social bias evidenced in the text generated by language models trained on such corpora, ranging from gender to age, from sexual orientation to ethnicity, and from religion to culture. We conclude with directions focused on measuring, reducing, and tackling the aforementioned types of bias.",
                    "[262580630 | Bender et al. | 2021 | Citations: 4657]": "The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models."
                },
                "metadata": [
                    {
                        "section_title": "Social Bias",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 2274,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 166
                            },
                            {
                                "start": 166,
                                "end": 394
                            },
                            {
                                "start": 394,
                                "end": 540
                            },
                            {
                                "start": 540,
                                "end": 886
                            },
                            {
                                "start": 886,
                                "end": 1096
                            },
                            {
                                "start": 1096,
                                "end": 1383
                            },
                            {
                                "start": 1383,
                                "end": 1577
                            },
                            {
                                "start": 1577,
                                "end": 1748
                            },
                            {
                                "start": 1748,
                                "end": 2266
                            }
                        ],
                        "ref_mentions": [
                            "209461005",
                            "208783276",
                            "252907216",
                            "258688053",
                            "237298625",
                            "262580630",
                            "258041203",
                            "208783276",
                            "252907216",
                            "208783276",
                            "252907216"
                        ],
                        "quote": "NLP systems generally suffer from social bias: their real-world application leads to outcomes that unfairly disadvantage or harm specific social groups [58,59,60,61]. Social bias can be introduced at various points during the development and deployment of NLP systems [62], but given the unsupervised nature of language modeling, training corpora are a key source of social bias in LLMs [10,63]. While bias in NLP systems can harm people in various ways [59], in this section, we primarily focus on two common harmful outcomes of social bias. These two types of harms are most commonly discussed in terms of quality-of-service harms and stereotyping harms (e.g., [64,65,60,66]), although many different systems have been proposed for classifying biases and harms in NLP, which define these terms in somewhat different ways, along with many additional and often overlapping categories [59]. Both of these types of harms are especially relevant to LLMs, and crucially, we believe both can be better understood and addressed in language modeling by adopting a sociolinguistic perspective (see Figure 3). First, social bias can be characterized by poor system performance for certain social groups that are interacting with LLMs and applications based on language models: token prediction will be more or less accurate depending on the social origins of the language inputted into the system. For example, ChatGPT might have difficulty correctly understanding prompts written by people from certain social groups due to their use of non-standard or socially restricted language patterns. This type of bias leads to what is known as quality-of-service harms, where the performance of these systems varies depending on the social background of the user [64,60]. These types of quality-of-service harms can often be the product of selection bias, as they result from how training data is selected from across the society whose language is being modeled [58]: in general, if language data from certain social groups is under-represented in the training data for a language model, we should expect that NLP applications based on that model will process language structures produced by these groups less accurately and consequently exhibit poorer performance for these groups [59]67]."
                    }
                ]
            },
            {
                "idx": 31,
                "key": "[271745282 | Kumar et al. | 2024 | Citations: 18]",
                "snippets": "Biases in these models-such as gender, racial, ethnic, and ideological biases-stem from training data that reflect societal prejudices. Consequently, language models can perpetuate and even amplify these biases, leading to outputs that reinforce stereotypes and disadvantage certain groups (Basta et al., 2019). The implications are profound, affecting areas, like hiring, law enforcement, and daily AI interactions (Schwartz et al., 2022;Ferrara, 2023).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[121125604 | Basta et al. | 2019 | Citations: 193]": "Gender bias is highly impacting natural language processing applications. Word embeddings have clearly been proven both to keep and amplify gender biases that are present in current data sources. Recently, contextualized word embeddings have enhanced previous word embedding techniques by computing word vector representations dependent on the sentence they appear in. In this paper, we study the impact of this conceptual change in the word embedding computation in relation with gender bias. Our analysis includes different measures previously applied in the literature to standard word embeddings. Our findings suggest that contextualized word embeddings are less biased than standard ones even when the latter are debiased."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 721,
                        "end": 1176,
                        "sentence_offsets": [
                            {
                                "start": 721,
                                "end": 856
                            },
                            {
                                "start": 857,
                                "end": 1033
                            },
                            {
                                "start": 1034,
                                "end": 1176
                            }
                        ],
                        "ref_mentions": [
                            "121125604"
                        ],
                        "quote": "Biases in these models-such as gender, racial, ethnic, and ideological biases-stem from training data that reflect societal prejudices. Consequently, language models can perpetuate and even amplify these biases, leading to outputs that reinforce stereotypes and disadvantage certain groups (Basta et al., 2019). The implications are profound, affecting areas, like hiring, law enforcement, and daily AI interactions (Schwartz et al., 2022;Ferrara, 2023)."
                    }
                ]
            },
            {
                "idx": 32,
                "key": "[271859735 | Cheng et al. | 2024 | Citations: 2]",
                "snippets": "Multi-modal Large Language Models (MLLMs) have advanced significantly, offering powerful vision-language understanding capabilities. However, these models often inherit severe social biases from their training datasets, leading to unfair predictions based on attributes like race and gender.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 291,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Multi-modal Large Language Models (MLLMs) have advanced significantly, offering powerful vision-language understanding capabilities. However, these models often inherit severe social biases from their training datasets, leading to unfair predictions based on attributes like race and gender."
                    }
                ]
            },
            {
                "idx": 33,
                "key": "[272826949 | Mirza et al. | 2024 | Citations: 2]",
                "snippets": "Bias in LLMs manifests in multiple forms, including racial, gender, and cultural stereotypes, often perpetuating systemic inequalities. These biases have tangible consequences; for instance, in 2018, Amazon discontinued an AI-driven recruiting tool after discovering it systematically downgraded resumes containing the term \"women's,\" reflecting an inherent bias in the training data that favored male candidates [1]. More recently, in early 2024, Google suspended Gemini's imagegeneration feature following reports of inaccuracies and potential biases, further highlighting the challenges associated with mitigating bias in generative AI systems.\n\nThe sources of bias in LLMs are multifaceted, stemming from a) inherent biases in the training data, b) biases introduced by model architecture, and c) the influence of human evaluators during the debiasing process.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[261276445 | Kotek et al. | 2023 | Citations: 236]": "Large Language Models (LLMs) have made substantial progress in the past several months, shattering state-of-the-art benchmarks in many domains. This paper investigates LLMs\u2019 behavior with respect to gender stereotypes, a known issue for prior models. We use a simple paradigm to test the presence of gender bias, building on but differing from WinoBias, a commonly used gender bias dataset, which is likely to be included in the training data of current LLMs. We test four recently published LLMs and demonstrate that they express biased assumptions about men and women\u2019s occupations. Our contributions in this paper are as follows: (a) LLMs are 3-6 times more likely to choose an occupation that stereotypically aligns with a person\u2019s gender; (b) these choices align with people\u2019s perceptions better than with the ground truth as reflected in official job statistics; (c) LLMs in fact amplify the bias beyond what is reflected in perceptions or the ground truth; (d) LLMs ignore crucial ambiguities in sentence structure 95% of the time in our study items, but when explicitly prompted, they recognize the ambiguity; (e) LLMs provide explanations for their choices that are factually inaccurate and likely obscure the true reason behind their predictions. That is, they provide rationalizations of their biased behavior. This highlights a key property of these models: LLMs are trained on imbalanced datasets; as such, even with the recent successes of reinforcement learning with human feedback, they tend to reflect those imbalances back at us. As with other types of societal biases, we suggest that LLMs must be carefully tested to ensure that they treat minoritized individuals and communities equitably."
                },
                "metadata": [
                    {
                        "section_title": "I. INTRODUCTION",
                        "pdf_hash": "",
                        "start": 484,
                        "end": 1348,
                        "sentence_offsets": [
                            {
                                "start": 396,
                                "end": 574
                            },
                            {
                                "start": 577,
                                "end": 712
                            },
                            {
                                "start": 713,
                                "end": 994
                            },
                            {
                                "start": 995,
                                "end": 1224
                            },
                            {
                                "start": 1227,
                                "end": 1442
                            }
                        ],
                        "ref_mentions": [
                            "261276445"
                        ],
                        "quote": "Bias in LLMs manifests in multiple forms, including racial, gender, and cultural stereotypes, often perpetuating systemic inequalities. These biases have tangible consequences; for instance, in 2018, Amazon discontinued an AI-driven recruiting tool after discovering it systematically downgraded resumes containing the term \"women's,\" reflecting an inherent bias in the training data that favored male candidates [1]. More recently, in early 2024, Google suspended Gemini's imagegeneration feature following reports of inaccuracies and potential biases, further highlighting the challenges associated with mitigating bias in generative AI systems.\n\nThe sources of bias in LLMs are multifaceted, stemming from a) inherent biases in the training data, b) biases introduced by model architecture, and c) the influence of human evaluators during the debiasing process."
                    }
                ]
            },
            {
                "idx": 34,
                "key": "[274117151 | Doll et al. | 2024 | Citations: 1]",
                "snippets": "LLMs learn from vast amounts of text collected from the Internet and other sources. This training data often contains unbalanced representations and stereotypes that are unintentionally embedded in the models [3].\n\nBias in LLMs can harm both individuals and society, with consequences including allocative and representational harms. Allocative harms occur when resources or opportunities are distributed unfairly, often to the detriment of already marginalised groups. For example, recruitment algorithms may be skewed to disadvantage certain demographic groups. Representational harm occurs when groups are misrepresented or stereotyped, reinforcing harmful narratives and stigmas and further entrenching negative prejudices and societal biases [4]. Language models show a propensity for bias in a variety of areas, including but not limited to gender, age, sexual orientation, physical appearance, disability, nationality, ethnicity, socio-economic status, religion and culture. These biases have the potential to have significant consequences, particularly in contexts where they affect critical areas such as justice, healthcare or employment [5].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[235436386 | Suresh et al. | 2019 | Citations: 442]": "As machine learning (ML) increasingly affects people and society, awareness of its potential unwanted consequences has also grown. To anticipate, prevent, and mitigate undesirable downstream consequences, it is critical that we understand when and how harm might be introduced throughout the ML life cycle. In this paper, we provide a framework that identifies seven distinct potential sources of downstream harm in machine learning, spanning data collection, development, and deployment. In doing so, we aim to facilitate more productive and precise communication around these issues, as well as more direct, application-grounded ways to mitigate them.",
                    "[261530629 | Gallegos et al. | 2023 | Citations: 594]": "Abstract Rapid advancements of large language models (LLMs) have enabled the processing, understanding, and generation of human-like text, with increasing integration into systems that touch our social sphere. Despite this success, these models can learn, perpetuate, and amplify harmful social biases. In this article, we present a comprehensive survey of bias evaluation and mitigation techniques for LLMs. We first consolidate, formalize, and expand notions of social bias and fairness in natural language processing, defining distinct facets of harm and introducing several desiderata to operationalize fairness for LLMs. We then unify the literature by proposing three intuitive taxonomies, two for bias evaluation, namely, metrics and datasets, and one for mitigation. Our first taxonomy of metrics for bias evaluation disambiguates the relationship between metrics and evaluation datasets, and organizes metrics by the different levels at which they operate in a model: embeddings, probabilities, and generated text. Our second taxonomy of datasets for bias evaluation categorizes datasets by their structure as counterfactual inputs or prompts, and identifies the targeted harms and social groups; we also release a consolidation of publicly available datasets for improved access. Our third taxonomy of techniques for bias mitigation classifies methods by their intervention during pre-processing, in-training, intra-processing, and post-processing, with granular subcategories that elucidate research trends. Finally, we identify open problems and challenges for future work. Synthesizing a wide range of recent research, we aim to provide a clear guide of the existing literature that empowers researchers and practitioners to better understand and prevent the propagation of bias in LLMs."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 783,
                        "end": 1935,
                        "sentence_offsets": [
                            {
                                "start": 694,
                                "end": 785
                            },
                            {
                                "start": 786,
                                "end": 889
                            },
                            {
                                "start": 890,
                                "end": 939
                            },
                            {
                                "start": 940,
                                "end": 1023
                            },
                            {
                                "start": 1024,
                                "end": 1153
                            },
                            {
                                "start": 1156,
                                "end": 1274
                            },
                            {
                                "start": 1275,
                                "end": 1410
                            },
                            {
                                "start": 1411,
                                "end": 1504
                            },
                            {
                                "start": 1505,
                                "end": 1692
                            },
                            {
                                "start": 1693,
                                "end": 1922
                            },
                            {
                                "start": 1923,
                                "end": 2093
                            }
                        ],
                        "ref_mentions": [
                            "261530629",
                            "235436386"
                        ],
                        "quote": "LLMs learn from vast amounts of text collected from the Internet and other sources. This training data often contains unbalanced representations and stereotypes that are unintentionally embedded in the models [3].\n\nBias in LLMs can harm both individuals and society, with consequences including allocative and representational harms. Allocative harms occur when resources or opportunities are distributed unfairly, often to the detriment of already marginalised groups. For example, recruitment algorithms may be skewed to disadvantage certain demographic groups. Representational harm occurs when groups are misrepresented or stereotyped, reinforcing harmful narratives and stigmas and further entrenching negative prejudices and societal biases [4]. Language models show a propensity for bias in a variety of areas, including but not limited to gender, age, sexual orientation, physical appearance, disability, nationality, ethnicity, socio-economic status, religion and culture. These biases have the potential to have significant consequences, particularly in contexts where they affect critical areas such as justice, healthcare or employment [5]."
                    }
                ]
            },
            {
                "idx": 35,
                "key": "[274130807 | Guo et al. | 2024 | Citations: 22]",
                "snippets": "The bias presented in Large Language Models (LLMs) can be broadly categorized into intrinsic bias and extrinsic bias based on the different stages at which the biases manifest within the model's lifecycle and the type of bias being measured (Doan et al., 2024). Intrinsic bias refers to biases that are inherently within the internal representations or outputs of a trained or pre-trained LLM and are independent of any specific downstream tasks. Extrinsic bias refers to the biases that manifest during the model's performance on specific downstream tasks after training or fine-turing. Intrinsic biases are typically encoded during the training or pre-training phase when the LLM learns from large-scale corpora. For instance, if a pre-trained LLM consistently associates certain professions with specific genders (e.g., assuming all doctors to be male and assuming all nurses to be female) or racial stereotypes (May et al., 2019), this bias would be considered intrinsic bias. This type of bias is tied to the way the model has encoded relationships and patterns in its internal representations.\n\nIntrinsic biases are primarily introduced during the training/pre-training stages where models learn patterns and representations from vast corpora. These biases can be traced to several aspects of data and model design, which will be outlined below.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 321,
                        "end": 1672,
                        "sentence_offsets": [
                            {
                                "start": 155,
                                "end": 384
                            },
                            {
                                "start": 387,
                                "end": 648
                            },
                            {
                                "start": 649,
                                "end": 833
                            },
                            {
                                "start": 834,
                                "end": 974
                            },
                            {
                                "start": 975,
                                "end": 1101
                            },
                            {
                                "start": 1102,
                                "end": 1367
                            },
                            {
                                "start": 1368,
                                "end": 1486
                            },
                            {
                                "start": 1489,
                                "end": 1637
                            },
                            {
                                "start": 1638,
                                "end": 1739
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The bias presented in Large Language Models (LLMs) can be broadly categorized into intrinsic bias and extrinsic bias based on the different stages at which the biases manifest within the model's lifecycle and the type of bias being measured (Doan et al., 2024). Intrinsic bias refers to biases that are inherently within the internal representations or outputs of a trained or pre-trained LLM and are independent of any specific downstream tasks. Extrinsic bias refers to the biases that manifest during the model's performance on specific downstream tasks after training or fine-turing. Intrinsic biases are typically encoded during the training or pre-training phase when the LLM learns from large-scale corpora. For instance, if a pre-trained LLM consistently associates certain professions with specific genders (e.g., assuming all doctors to be male and assuming all nurses to be female) or racial stereotypes (May et al., 2019), this bias would be considered intrinsic bias. This type of bias is tied to the way the model has encoded relationships and patterns in its internal representations.\n\nIntrinsic biases are primarily introduced during the training/pre-training stages where models learn patterns and representations from vast corpora. These biases can be traced to several aspects of data and model design, which will be outlined below."
                    }
                ]
            },
            {
                "idx": 36,
                "key": "[274437724 | Meijer et al. | 2024 | Citations: 1]",
                "snippets": "An important concern regarding LLMs is whether and in which areas these models may exhibit bias, such as gender, racial and cultural bias.\n\nLarge language models are sensitive to embedded bias due to the way they are trained, as they pick up the societal and cultural biases present in the training data. Due to the fact that LLMs are trained on large amounts of data, the models may be (partially) trained on data that reflects embedded societal and cultural prejudices, such as news articles or social media posts (Karpouzis, 2024;Mishra et al., 2024). Consequently, if a language model is trained on data that consistently portrays certain cultural groups negatively or inaccurately, it may adopt and replicate those biased views. Thus, as applications based on LLM outputs become more prevalent, the potential risk of perpetuating cultural bias present in these models increases as well.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[269134574 | Karpouzis | 2024 | Citations: 10]": "Generative Artificial Intelligence (AI) systems, like ChatGPT, have the potential to perpetuate and amplify cultural biases embedded in their training data, which are predominantly produced by dominant cultural groups. This paper explores the philosophical and technical challenges of detecting and mitigating cultural bias in generative AI, drawing on Plato\u2019s Allegory of the Cave to frame the issue as a problem of limited and distorted representation. We propose a multifaceted approach combining technical interventions, such as data diversification and culturally aware model constraints, with a deeper engagement with the cultural and philosophical dimensions of the problem. Drawing on theories of extended cognition and situated knowledge, we argue that mitigating AI biases requires a reflexive interrogation of the cultural contexts of AI development and a commitment to empowering marginalized voices and perspectives. We claim that controlling cultural bias in generative AI is inseparable from the larger project of promoting equity, diversity, and inclusion in AI development and governance. By bridging philosophical reflection with technical innovation, this paper contributes to the growing discourse on responsible and inclusive AI, offering a roadmap for detecting and mitigating cultural biases while grappling with the profound cultural implications of these powerful technologies.",
                    "[269762608 | Mishra et al. | 2024 | Citations: 10]": "With the widespread adoption of deep learning-based models in practical applications, concerns about their fairness have become increasingly prominent. Existing research indicates that both the model itself and the datasets on which they are trained can contribute to unfair decisions. In this paper, we address the data-related aspect of the problem, aiming to enhance the data to guide the model towards greater trustworthiness. Due to their uncontrolled curation and limited understanding of fairness drivers, real-world datasets pose challenges in eliminating unfairness. Recent findings highlight the potential of Foundation Models in generating substantial datasets. We leverage these foundation models in conjunction with state-of-the-art explainability and fairness platforms to generate counterfactual examples. These examples are used to augment the existing dataset, resulting in a more fair learning model. Our experiments were conducted on the CelebA and UTKface datasets, where we assessed the quality of generated counterfactual data using various bias-related metrics. We observed improvements in bias mitigation across several protected attributes in the fine-tuned model when utilizing counterfactual data."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 535,
                        "end": 1426,
                        "sentence_offsets": [
                            {
                                "start": 527,
                                "end": 660
                            },
                            {
                                "start": 661,
                                "end": 799
                            },
                            {
                                "start": 802,
                                "end": 966
                            },
                            {
                                "start": 967,
                                "end": 1216
                            },
                            {
                                "start": 1217,
                                "end": 1395
                            },
                            {
                                "start": 1396,
                                "end": 1553
                            }
                        ],
                        "ref_mentions": [
                            "269134574",
                            "269762608"
                        ],
                        "quote": "An important concern regarding LLMs is whether and in which areas these models may exhibit bias, such as gender, racial and cultural bias.\n\nLarge language models are sensitive to embedded bias due to the way they are trained, as they pick up the societal and cultural biases present in the training data. Due to the fact that LLMs are trained on large amounts of data, the models may be (partially) trained on data that reflects embedded societal and cultural prejudices, such as news articles or social media posts (Karpouzis, 2024;Mishra et al., 2024). Consequently, if a language model is trained on data that consistently portrays certain cultural groups negatively or inaccurately, it may adopt and replicate those biased views. Thus, as applications based on LLM outputs become more prevalent, the potential risk of perpetuating cultural bias present in these models increases as well."
                    }
                ]
            },
            {
                "idx": 37,
                "key": "[275336873 | Zhao et al. | 2025 | Citations: 4]",
                "snippets": "Large Language Models (LLMs) have been shown to exhibit various biases and stereotypes in their generated content.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 114,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Large Language Models (LLMs) have been shown to exhibit various biases and stereotypes in their generated content."
                    }
                ]
            },
            {
                "idx": 38,
                "key": "[277667520 | Cantini et al. | 2025 | Citations: 4]",
                "snippets": "Biases may stem from various sources, including biased training data that reflects historical inequalities and prejudicial associations, linguistic imbalances in corpora, flaws in algorithmic design, and the uncritical use of AI systems (Hovy et al., 2021)(Gallegos et al., 2023). Previous studies have quantified biased attitudes in language models related to various social groups (Manerba et al., 2023)(Nadeem et al., 2020), also finding that state-of-the-art LLMs can be manipulated via adversarial attacks to produce biased or harmful responses, despite their bias mitigation and alignment mechanisms (Cantini et al., 2024).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[265212726 | Manerba et al. | 2023 | Citations: 20]": "While the impact of social biases in language models has been recognized, prior methods for bias evaluation have been limited to binary association tests on small datasets, limiting our understanding of bias complexities. This paper proposes a novel framework for probing language models for social biases by assessing disparate treatment, which involves treating individuals differently according to their affiliation with a sensitive demographic group. We curate SoFa, a large-scale benchmark designed to address the limitations of existing fairness collections. SoFa expands the analysis beyond the binary comparison of stereotypical versus anti-stereotypical identities to include a diverse range of identities and stereotypes. Comparing our methodology with existing benchmarks, we reveal that biases within language models are more nuanced than acknowledged, indicating a broader scope of encoded biases than previously recognized. Benchmarking LMs on SoFa, we expose how identities expressing different religions lead to the most pronounced disparate treatments across all models. Finally, our findings indicate that real-life adversities faced by various groups such as women and people with disabilities are mirrored in the behavior of these models.",
                    "[215828184 | Nadeem et al. | 2020 | Citations: 1015]": "A stereotype is an over-generalized belief about a particular group of people, e.g., Asians are good at math or African Americans are athletic. Such beliefs (biases) are known to hurt target groups. Since pretrained language models are trained on large real-world data, they are known to capture stereotypical biases. It is important to quantify to what extent these biases are present in them. Although this is a rapidly growing area of research, existing literature lacks in two important aspects: 1) they mainly evaluate bias of pretrained language models on a small set of artificial sentences, even though these models are trained on natural data 2) current evaluations focus on measuring bias without considering the language modeling ability of a model, which could lead to misleading trust on a model even if it is a poor language model. We address both these problems. We present StereoSet, a large-scale natural English dataset to measure stereotypical biases in four domains: gender, profession, race, and religion. We contrast both stereotypical bias and language modeling ability of popular models like BERT, GPT-2, RoBERTa, and XLnet. We show that these models exhibit strong stereotypical biases. Our data and code are available at https://stereoset.mit.edu.",
                    "[237298625 | Hovy et al. | 2021 | Citations: 253]": "Abstract Recently, there has been an increased interest in demographically grounded bias in natural language processing (NLP) applications. Much of the recent work has focused on describing bias and providing an overview of bias in a larger context. Here, we provide a simple, actionable summary of this recent work. We outline five sources where bias can occur in NLP systems: (1) the data, (2) the annotation process, (3) the input representations, (4) the models, and finally (5) the research design (or how we conceptualize our research). We explore each of the bias sources in detail in this article, including examples and links to related work, as well as potential counter\u2010measures.",
                    "[261530629 | Gallegos et al. | 2023 | Citations: 594]": "Abstract Rapid advancements of large language models (LLMs) have enabled the processing, understanding, and generation of human-like text, with increasing integration into systems that touch our social sphere. Despite this success, these models can learn, perpetuate, and amplify harmful social biases. In this article, we present a comprehensive survey of bias evaluation and mitigation techniques for LLMs. We first consolidate, formalize, and expand notions of social bias and fairness in natural language processing, defining distinct facets of harm and introducing several desiderata to operationalize fairness for LLMs. We then unify the literature by proposing three intuitive taxonomies, two for bias evaluation, namely, metrics and datasets, and one for mitigation. Our first taxonomy of metrics for bias evaluation disambiguates the relationship between metrics and evaluation datasets, and organizes metrics by the different levels at which they operate in a model: embeddings, probabilities, and generated text. Our second taxonomy of datasets for bias evaluation categorizes datasets by their structure as counterfactual inputs or prompts, and identifies the targeted harms and social groups; we also release a consolidation of publicly available datasets for improved access. Our third taxonomy of techniques for bias mitigation classifies methods by their intervention during pre-processing, in-training, intra-processing, and post-processing, with granular subcategories that elucidate research trends. Finally, we identify open problems and challenges for future work. Synthesizing a wide range of recent research, we aim to provide a clear guide of the existing literature that empowers researchers and practitioners to better understand and prevent the propagation of bias in LLMs.",
                    "[271097745 | Cantini et al. | 2024 | Citations: 7]": "Large Language Models (LLMs) have revolutionized artificial intelligence, demonstrating remarkable computational power and linguistic capabilities. However, these models are inherently prone to various biases stemming from their training data. These include selection, linguistic, and confirmation biases, along with common stereotypes related to gender, ethnicity, sexual orientation, religion, socioeconomic status, disability, and age. This study explores the presence of these biases within the responses given by the most recent LLMs, analyzing the impact on their fairness and reliability. We also investigate how known prompt engineering techniques can be exploited to effectively reveal hidden biases of LLMs, testing their adversarial robustness against jailbreak prompts specially crafted for bias elicitation. Extensive experiments are conducted using the most widespread LLMs at different scales, confirming that LLMs can still be manipulated to produce biased or inappropriate responses, despite their advanced capabilities and sophisticated alignment processes. Our findings underscore the importance of enhancing mitigation techniques to address these safety issues, toward a more sustainable and inclusive artificial intelligence."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 583,
                        "end": 1120,
                        "sentence_offsets": [
                            {
                                "start": 583,
                                "end": 826
                            },
                            {
                                "start": 827,
                                "end": 1120
                            }
                        ],
                        "ref_mentions": [
                            "237298625",
                            "261530629",
                            "265212726",
                            "215828184",
                            "271097745"
                        ],
                        "quote": "Biases may stem from various sources, including biased training data that reflects historical inequalities and prejudicial associations, linguistic imbalances in corpora, flaws in algorithmic design, and the uncritical use of AI systems (Hovy et al., 2021)(Gallegos et al., 2023). Previous studies have quantified biased attitudes in language models related to various social groups (Manerba et al., 2023)(Nadeem et al., 2020), also finding that state-of-the-art LLMs can be manipulated via adversarial attacks to produce biased or harmful responses, despite their bias mitigation and alignment mechanisms (Cantini et al., 2024)."
                    }
                ]
            },
            {
                "idx": 39,
                "key": "[278807456 | Gupta et al. | 2025 | Citations: 0]",
                "snippets": "Large Language Models (LLMs) like ChatGPT, LLAMA, and Mistral are widely used for automating tasks such as content creation and data analysis. However, due to their training on publicly available internet data, they may inherit social biases. We aimed to investigate the social biases (i.e., ethnic, gender, and disability biases) in these models and evaluate how different model versions handle them.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Large Language Models (LLMs) like ChatGPT, LLAMA, and Mistral are widely used for automating tasks such as content creation and data analysis. However, due to their training on publicly available internet data, they may inherit social biases. We aimed to investigate the social biases (i.e., ethnic, gender, and disability biases) in these models and evaluate how different model versions handle them.",
                        "section_title": "abstract",
                        "pdf_hash": ""
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.047721,
        "cot": "To answer the user's query about social bias in large language models, I need to create a comprehensive organizational structure that covers all aspects of the query. The query asks for an overview of various forms of social bias in large language models, including how these biases originate and how they manifest across applications.\n\nLet me identify the key dimensions needed for a comprehensive response:\n\n1. First, I'll include an \"Introduction/Background\" section to provide context on what social bias is and why it matters in LLMs. This will help set the stage for the more detailed sections to follow.\n\n2. I need a section on \"Types of Social Bias in LLMs\" to categorize the different forms of bias mentioned in the query (gender, racial, religious, disability, and stigma-related). Many quotes discuss various forms of bias, so this will be a major section.\n\n3. A section on \"Origins of Bias in LLMs\" is essential to explain how biases get incorporated into these models, primarily from training data but also through other mechanisms.\n\n4. I should include a section on \"Manifestations of Bias Across Applications\" to show how these biases affect different downstream tasks and real-world applications.\n\n5. Finally, a brief section on \"Measurement and Detection Methods\" seems important given multiple quotes discuss tools and approaches to quantify bias.\n\nFor each section, I need to determine whether a synthesis format or list format is most appropriate:\n\n- Introduction/Background: This should be a synthesis to provide a coherent overview of the topic.\n- Types of Social Bias: This would work well as a synthesis with subsections for each type of bias, as there are detailed explanations for each bias type.\n- Origins of Bias: A synthesis format makes sense here to explain the interconnected factors that contribute to bias.\n- Manifestations of Bias: A synthesis works better here to explain the complex ways bias manifests across different applications.\n- Measurement and Detection Methods: A synthesis format is appropriate to explain how different measurement approaches work together.\n\nNow I'll assign quotes to each dimension:",
        "plan": {
            "Introduction and Background on Social Bias in LLMs (synthesis)": [
                5,
                12,
                13,
                14,
                16,
                20,
                24,
                25,
                26,
                29,
                30,
                31,
                33,
                34,
                35,
                36,
                37,
                38,
                39
            ],
            "Types of Social Bias in LLMs (synthesis)": [
                0,
                1,
                2,
                3,
                6,
                9,
                10,
                14,
                15,
                16,
                19,
                22,
                23,
                28,
                32,
                34
            ],
            "Origins of Bias in LLMs (synthesis)": [
                4,
                5,
                6,
                7,
                8,
                11,
                12,
                17,
                18,
                21,
                24,
                26,
                30,
                33,
                34,
                35,
                36
            ],
            "Manifestations of Bias Across Applications (synthesis)": [
                1,
                4,
                8,
                9,
                18,
                19,
                21,
                24,
                27,
                30,
                31,
                34
            ],
            "Measurement and Detection of Bias in LLMs (synthesis)": [
                0,
                2,
                3,
                4,
                7,
                9,
                10,
                19,
                23
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction and Background on Social Bias in LLMs",
                "tldr": "Large Language Models (LLMs) exhibit various forms of social bias that originate from their training data and can perpetuate or amplify societal prejudices and stereotypes. These biases have significant implications for fairness and can lead to harmful outcomes for marginalized groups across numerous applications. (19 sources)",
                "text": "\nSocial bias in Large Language Models (LLMs) has emerged as a critical concern as these powerful systems become increasingly integrated into various aspects of society. LLMs, such as BERT, GPT-3, RoBERTa, and BLOOM, are trained on vast amounts of text data collected from the internet and other sources, inadvertently inheriting and sometimes amplifying the biases present in these datasets <Paper corpusId=\"248366292\" paperTitle=\"(Kadan et al., 2022)\" isShortName></Paper> <Paper corpusId=\"259716055\" paperTitle=\"(Kolisko et al., 2023)\" isShortName></Paper>. These biases reflect historical inequalities, prejudices, and stereotypes encoded in human-generated texts <Paper corpusId=\"259075978\" paperTitle=\"(Wald et al., 2023)\" isShortName></Paper>.\n\nAt its core, social bias in LLMs can be defined as \"prejudices, stereotypes, and discriminatory attitudes against certain groups of people\" that manifest through language <Paper corpusId=\"265212726\" paperTitle=\"(Manerba et al., 2023)\" isShortName></Paper> <Paper corpusId=\"258688053\" paperTitle=\"(Navigli et al., 2023)\" isShortName></Paper>. Research has consistently demonstrated that language models trained on large corpora of human-generated text encode human-like social biases <Paper corpusId=\"259129801\" paperTitle=\"(Mei et al., 2023)\" isShortName></Paper> <Paper corpusId=\"23163324\" paperTitle=\"(Caliskan et al., 2016)\" isShortName></Paper>. These biases are particularly concerning because LLMs are deployed in numerous downstream applications, where they can propagate biases and produce unfair outcomes for specific groups <Paper corpusId=\"259095603\" paperTitle=\"(Thakur et al., 2023)\" isShortName></Paper>.\n\nThe sources of bias in LLMs are multifaceted, including dataset bias, embedding bias, and label bias <Paper corpusId=\"268856702\" paperTitle=\"(Chu et al., 2024)\" isShortName></Paper>. Dataset bias stems from the unbalanced and skewed nature of training data, which often overrepresents hegemonic views while underrepresenting marginalized perspectives <Paper corpusId=\"259075978\" paperTitle=\"(Wald et al., 2023)\" isShortName></Paper> <Paper corpusId=\"262580630\" paperTitle=\"(Bender et al., 2021)\" isShortName></Paper>. Internet access is not evenly distributed across society, and content filtering processes may disproportionately remove language used by marginalized groups <Paper corpusId=\"259075978\" paperTitle=\"(Wald et al., 2023)\" isShortName></Paper>. Additionally, the scale of LLM training datasets makes them impossible to thoroughly document, further complicating bias mitigation efforts <Paper corpusId=\"259075978\" paperTitle=\"(Wald et al., 2023)\" isShortName></Paper>.\n\nSocial biases in LLMs can result in two primary forms of harm: representational and allocational <Paper corpusId=\"259716055\" paperTitle=\"(Kolisko et al., 2023)\" isShortName></Paper>. Representational harms occur when models portray certain groups negatively or fail to represent them adequately, while allocational harms involve denying opportunities or resources to specific groups <Paper corpusId=\"259716055\" paperTitle=\"(Kolisko et al., 2023)\" isShortName></Paper> <Paper corpusId=\"274117151\" paperTitle=\"(Doll et al., 2024)\" isShortName></Paper>. These harms can have far-reaching consequences in critical domains such as healthcare, employment, and law enforcement <Paper corpusId=\"271745282\" paperTitle=\"(Kumar et al., 2024)\" isShortName></Paper> <Paper corpusId=\"121125604\" paperTitle=\"(Basta et al., 2019)\" isShortName></Paper>.\n\nVarious types of social bias have been documented in LLMs, including gender, racial, religious, and disability-related biases <Paper corpusId=\"259129801\" paperTitle=\"(Mei et al., 2023)\" isShortName></Paper>. Research has shown that models like GPT-2 and GPT-3 exhibit biases against various demographic groups, and these biases can be exacerbated in dialogue generation models trained on unchecked user-generated content from social media <Paper corpusId=\"259075978\" paperTitle=\"(Wald et al., 2023)\" isShortName></Paper>. More recently, researchers have begun to examine social stigmas related to disabilities, mental illness, and socioeconomic status, which affect a substantial portion of the population <Paper corpusId=\"259129801\" paperTitle=\"(Mei et al., 2023)\" isShortName></Paper> <Paper corpusId=\"246654106\" paperTitle=\"(Meisel et al., 2022)\" isShortName></Paper>.\n\nAlarmingly, LLMs don't merely reflect the biases present in their training data but can amplify them <Paper corpusId=\"259075978\" paperTitle=\"(Wald et al., 2023)\" isShortName></Paper> <Paper corpusId=\"261276445\" paperTitle=\"(Kotek et al., 2023)\" isShortName></Paper>. For example, studies have found that LLMs are significantly more likely to associate Muslims with violence compared to other religious groups <Paper corpusId=\"231603388\" paperTitle=\"(Abid et al., 2021)\" isShortName></Paper>, and they consistently associate professions with stereotypical genders at rates far exceeding real-world statistics <Paper corpusId=\"261276445\" paperTitle=\"(Kotek et al., 2023)\" isShortName></Paper>.\n\nThe issue of bias in LLMs has practical implications, as evidenced by real-world incidents. For instance, Amazon discontinued an AI recruiting tool after discovering it systematically downgraded resumes containing the term \"women's\" <Paper corpusId=\"272826949\" paperTitle=\"(Mirza et al., 2024)\" isShortName></Paper>, and Google suspended Gemini's image generation feature following reports of inaccuracies and biases <Paper corpusId=\"272826949\" paperTitle=\"(Mirza et al., 2024)\" isShortName></Paper>. These examples underscore the urgent need to address social bias in LLMs as they become increasingly integrated into systems that impact individuals' lives.\n\nAs research in this area continues to evolve, understanding the various forms, origins, and impacts of social bias in LLMs remains crucial for developing more fair and equitable AI systems <Paper corpusId=\"268553687\" paperTitle=\"(Cai et al., 2024)\" isShortName></Paper> <Paper corpusId=\"274130807\" paperTitle=\"(Guo et al., 2024)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Kadan et al., 2022)",
                        "snippets": [
                            "The remarkable progress in Natural Language Processing (NLP) brought about by deep learning, particularly with the recent advent of large pre-trained neural language models, is brought into scrutiny as several studies began to discuss and report potential biases in NLP applications. Bias in NLP is found to originate from latent historical biases encoded by humans into textual data which gets perpetuated or even amplified by NLP algorithm."
                        ],
                        "paper": {
                            "corpus_id": 248366292,
                            "title": "Towards an Enhanced Understanding of Bias in Pre-trained Neural Language Models: A Survey with Special Emphasis on Affective Bias",
                            "authors": [
                                {
                                    "authorId": "144485480",
                                    "name": "Anoop Kadan"
                                },
                                {
                                    "authorId": "3390280",
                                    "name": "Manjary P.Gangan"
                                },
                                {
                                    "authorId": "145791688",
                                    "name": "Deepak P"
                                },
                                {
                                    "authorId": "151092338",
                                    "name": "L. LajishV."
                                }
                            ],
                            "year": 2022,
                            "venue": "arXiv.org",
                            "n_citations": 10
                        },
                        "score": 0.88671875
                    },
                    {
                        "id": "(Kolisko et al., 2023)",
                        "snippets": [
                            "There is a growing body of work documenting social biases in large language models. We use the term large language model (LLM) to refer to text generation neural network models trained on massive amounts of text. Popular examples include BERT (Devlin et al. 2019), GPT-3 (Brown et al. 2020), RoBERTa (Liu et al., 2019)BLOOM (Big-Science 2022). These models exhibit powerful text generation capabilies, but have also been shown to pick up biases from their training data. \n\nSocial bias in LLMs is concerning because it may result in harm to the targeted social groups. Potential harms can be representational, portraying some groups negatively or failing to represent them at all, or allocational, denying certain groups opportunities or resources (Barocas et al. 2017). \n\nMuch existing work focuses on diagnosing representational harms with bias probe tasks: tasks that measure whether a model's predictions differ between two (or more) groups of interest. A number of probe tasks have been proposed: (Rudinger et al., 2017); (Sheng et al., 2019); Bordia and Bowman (2019); (Lee et al., 2019); (Liu et al., 2019)(Liu et al., 2019); (Nadeem et al., 2020)(Sheng et al., 2021) and others. Most of these focus on gender stereotypes. 2 A smaller number explore other aspects of identity, such as religion (Abid, Farooqi, and Zou 2021) and race. 3"
                        ],
                        "paper": {
                            "corpus_id": 259716055,
                            "title": "Exploring Social Biases of Large Language Models in a College Artificial Intelligence Course",
                            "authors": [
                                {
                                    "authorId": "2222666109",
                                    "name": "Skylar Kolisko"
                                },
                                {
                                    "authorId": "144901955",
                                    "name": "Carolyn Jane Anderson"
                                }
                            ],
                            "year": 2023,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 11
                        },
                        "score": 0.826171875
                    },
                    {
                        "id": "(Wald et al., 2023)",
                        "snippets": [
                            "Research has shown that language models are prone to exhibit societal biases, reflecting the nature of the underlying training data written by humans. For this work, we focus on representational bias, which in the context of language models, occurs when there is an imbalance in the portrayal of different groups in LM-generated text (Blodgett et al. 2020;Liang et al. 2021). This can arise from stereotypes that cultivate negative generalisations about particular social groups and language that is used to degrade these groups. Ultimately, this may lead to certain groups being depicted less favourably than others. Bender et al. argue that even in large models with large datasets, diversity and unbiasedness are not guaranteed (Bender et al. 2021). Datasets are too big to be thoroughly documented, while marginalised social groups are more likely to be filtered out due to the nature of the filtering process. Phrases or dialects used by these groups are disproportionally categorised as hateful, resulting in their exclusion from training datasets (Sap et al. 2019;Bender et al. 2021). Hegemonic views are over-represented as Internet access is not evenly distributed, and datasets are often curated using male-dominated websites. Large language generation models such as GPT-2 (Radford et al. 2019) and GPT-3 (Brown et al. 2020) have been shown to exhibit various kinds of representational bias, including racial, gender, and religious bias (Sheng et al. 2019;Abid, Farooqi, and Zou 2021;Sheng et al. 2021). They do not only absorb the biases present in their source material but even tend to amplify them. Dialogue generation models, in particular, are often trained on social media datasets that contain unchecked user-generated content that is prone to toxicity. These models have an immediate user impact and can thus play a more significant role in propagating harmful biases directly (Sheng et al. 2021)."
                        ],
                        "paper": {
                            "corpus_id": 259075978,
                            "title": "Exposing Bias in Online Communities through Large-Scale Language Models",
                            "authors": [
                                {
                                    "authorId": "2219550842",
                                    "name": "Celine Wald"
                                },
                                {
                                    "authorId": "32421394",
                                    "name": "Lukas Pfahler"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 6
                        },
                        "score": 0.8486328125
                    },
                    {
                        "id": "(Manerba et al., 2023)",
                        "snippets": [
                            "Social bias can be defined as the manifestation through language of \"prejudices, stereotypes, and discriminatory attitudes against certain groups of people\" (Navigli et al., 2023). These biases are featured in training datasets and are carried over into downstream applications, resulting in, for instance, classification errors concerning specific minorities and the generation of harmful content when models are prompted with sensitive identities (Cui et al., 2024;Gallegos et al., 2023)."
                        ],
                        "paper": {
                            "corpus_id": 265212726,
                            "title": "Social Bias Probing: Fairness Benchmarking for Language Models",
                            "authors": [
                                {
                                    "authorId": "2121386115",
                                    "name": "Marta Marchiori Manerba"
                                },
                                {
                                    "authorId": "82563120",
                                    "name": "Karolina Sta\u0144czak"
                                },
                                {
                                    "authorId": "2257013371",
                                    "name": "Riccardo Guidotti"
                                },
                                {
                                    "authorId": "1736067",
                                    "name": "Isabelle Augenstein"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 20
                        },
                        "score": 0.89697265625
                    },
                    {
                        "id": "(Navigli et al., 2023)",
                        "snippets": [
                            "In this article, we introduce and discuss the pervasive issue of bias in the large language models that are currently at the core of mainstream approaches to Natural Language Processing (NLP). We first introduce data selection bias, that is, the bias caused by the choice of texts that make up a training corpus. Then, we survey the different types of social bias evidenced in the text generated by language models trained on such corpora, ranging from gender to age, from sexual orientation to ethnicity, and from religion to culture. We conclude with directions focused on measuring, reducing, and tackling the aforementioned types of bias."
                        ],
                        "paper": {
                            "corpus_id": 258688053,
                            "title": "Biases in Large Language Models: Origins, Inventory, and Discussion",
                            "authors": [
                                {
                                    "authorId": "1733928",
                                    "name": "Roberto Navigli"
                                },
                                {
                                    "authorId": "1396456007",
                                    "name": "Simone Conia"
                                },
                                {
                                    "authorId": "26234127",
                                    "name": "Bj\u00f6rn Ross"
                                }
                            ],
                            "year": 2023,
                            "venue": "ACM Journal of Data and Information Quality",
                            "n_citations": 283
                        },
                        "score": 0
                    },
                    {
                        "id": "(Mei et al., 2023)",
                        "snippets": [
                            "The rapid deployment of artificial intelligence (AI) models de- demands a thorough investigation of biases and risks inherent in these models to understand their impact on individuals and society. A growing body of work has shown that social biases are encoded in language models and their downstream tasks",
                            "Caliskan et al. [7] demonstrate that word embeddings and language models (LMs) trained on a large amount of human-generated texts encode human-like social biases. Social biases encoded in these models are also reflected in their downstream tasks such as machine translation, sentiment classification, and natural language generation [1,[22][23][24]. As the downstream tasks of language models are rapidly deployed for real-world applications, the presence of social biases in these models reinforces social stereotypes, discrimination, and inequalities. Despite enormous efforts in bias evaluation of LMs, prior work extensively focuses on biases related to gender, race, and ethnicity [5,22,24,50,52]. Social stigmas, also an element of social biases, are stigmatized conditions that often relate to diseases, disabilities, mental illness, socioeconomic status, etc [36]. Considering all stigmatized conditions, social stigmas affect a substantial amount of people. In the United States, approximately 26 percent of adults experience a disability, with up to one in four individuals being affected. In 2021, there were around 57.8M adults that experienced mental illness, which was around 22% of the population in the United States. Social stigmas prevent individuals from social activities and access to education, healthcare, and career opportunities, negatively influencing their psychological well-being and life outcomes [14,[31][32][33][37]."
                        ],
                        "paper": {
                            "corpus_id": 259129801,
                            "title": "Bias Against 93 Stigmatized Groups in Masked Language Models and Downstream Sentiment Classification Tasks",
                            "authors": [
                                {
                                    "authorId": "2189183000",
                                    "name": "Katelyn Mei"
                                },
                                {
                                    "authorId": "2196943720",
                                    "name": "Sonia Fereidooni"
                                },
                                {
                                    "authorId": "144537437",
                                    "name": "Aylin Caliskan"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Fairness, Accountability and Transparency",
                            "n_citations": 51
                        },
                        "score": 0.89599609375
                    },
                    {
                        "id": "(Caliskan et al., 2016)",
                        "snippets": [
                            "Machines learn what people know implicitly AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan et al. now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs\u2014for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior. Science, this issue p. 183; see also p. 133 Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias. Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology."
                        ],
                        "paper": {
                            "corpus_id": 23163324,
                            "title": "Semantics derived automatically from language corpora contain human-like biases",
                            "authors": [
                                {
                                    "authorId": "144537437",
                                    "name": "Aylin Caliskan"
                                },
                                {
                                    "authorId": "145315445",
                                    "name": "J. Bryson"
                                },
                                {
                                    "authorId": "47735253",
                                    "name": "Arvind Narayanan"
                                }
                            ],
                            "year": 2016,
                            "venue": "Science",
                            "n_citations": 2673
                        },
                        "score": 0
                    },
                    {
                        "id": "(Thakur et al., 2023)",
                        "snippets": [
                            "Societal biases present in pre-trained large language models are a critical issue as these models have been shown to propagate biases in countless downstream applications, rendering them unfair towards specific groups of people."
                        ],
                        "paper": {
                            "corpus_id": 259095603,
                            "title": "Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions",
                            "authors": [
                                {
                                    "authorId": "2221493995",
                                    "name": "Himanshu Thakur"
                                },
                                {
                                    "authorId": "1819271266",
                                    "name": "Atishay Jain"
                                },
                                {
                                    "authorId": "2127734657",
                                    "name": "Praneetha Vaddamanu"
                                },
                                {
                                    "authorId": "28130078",
                                    "name": "Paul Pu Liang"
                                },
                                {
                                    "authorId": "49933077",
                                    "name": "Louis-philippe Morency"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 37
                        },
                        "score": 0.830078125
                    },
                    {
                        "id": "(Chu et al., 2024)",
                        "snippets": [
                            "Language modeling bias, often defined as \"bias that results in harm to various social groups\" [70], presents itself in various forms, encompassing the association of specific stereotypes with groups, the devaluation of certain groups, the underrepresentation of particular social groups, and the unequal allocation of resources among groups [44].Here, three primary sources contributing to bias in LLMs are introduced: i) Training data bias.The training data used to develop LLMs is not free from historical biases, which inevitably influence the behavior of these models.For instance, if the training data includes the statement \"all programmers are male and all nurses are female, \" the model is likely to learn and perpetuate these occupational and gender biases in its outputs, reflecting a narrow and biased view of societal roles (Bolukbasi et al., 2016)(Caliskan et al., 2016).Additionally, a significant disparity in the training data could also lead to biased outcomes [161].For example, Buolamwini and Gebru (Buolamwini et al., 2018) highlighted significant disparities in datasets like IJB-A and Adience, where predominantly light-skinned individuals make up 79.6% and 86.2% of the data, respectively, thereby biasing analyses toward underrepresented dark-skinned groups (Mehrabi et al., 2019).\n\nii) Embedding bias.Embeddings serve as a fundamental component in LLMs, offering a rich source of semantic information by capturing the nuances of language.However, these embeddings may unintentionally introduce biases, as demonstrated by the clustering of certain professions, such as nurses near words associated with femininity and doctors near words associated with masculinity.This phenomenon inadvertently introduces semantic bias into downstream models, impacting their performance and fairness [9]63].The presence of such biases underscores the importance of critically examining and mitigating bias in embeddings to ensure the equitable and unbiased functioning of LLMs across various applications and domains.\n\niii) Label bias.In instruction tuning scenarios, biases can arise from the subjective judgments of human annotators who provide labels or annotations for training data (Sap et al., 2019).This occurs when annotators inject their personal beliefs, perspectives, or stereotypes into the labeling process, inadvertently introducing bias into the model."
                        ],
                        "paper": {
                            "corpus_id": 268856702,
                            "title": "Fairness in Large Language Models: A Taxonomic Survey",
                            "authors": [
                                {
                                    "authorId": "2283849596",
                                    "name": "Zhibo Chu"
                                },
                                {
                                    "authorId": "2206777360",
                                    "name": "Zichong Wang"
                                },
                                {
                                    "authorId": "2243674300",
                                    "name": "Wenbin Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "SIGKDD Explorations",
                            "n_citations": 41
                        },
                        "score": 0.81201171875
                    },
                    {
                        "id": "(Bender et al., 2021)",
                        "snippets": [
                            "The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models."
                        ],
                        "paper": {
                            "corpus_id": 262580630,
                            "title": "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? \ud83e\udd9c",
                            "authors": [
                                {
                                    "authorId": "2471699",
                                    "name": "Emily M. Bender"
                                },
                                {
                                    "authorId": "2076288",
                                    "name": "Timnit Gebru"
                                },
                                {
                                    "authorId": "1584940075",
                                    "name": "Angelina McMillan-Major"
                                },
                                {
                                    "authorId": "2246048745",
                                    "name": "Shmargaret Shmitchell"
                                }
                            ],
                            "year": 2021,
                            "venue": "Conference on Fairness, Accountability and Transparency",
                            "n_citations": 4657
                        },
                        "score": 0
                    },
                    {
                        "id": "(Doll et al., 2024)",
                        "snippets": [
                            "LLMs learn from vast amounts of text collected from the Internet and other sources. This training data often contains unbalanced representations and stereotypes that are unintentionally embedded in the models [3].\n\nBias in LLMs can harm both individuals and society, with consequences including allocative and representational harms. Allocative harms occur when resources or opportunities are distributed unfairly, often to the detriment of already marginalised groups. For example, recruitment algorithms may be skewed to disadvantage certain demographic groups. Representational harm occurs when groups are misrepresented or stereotyped, reinforcing harmful narratives and stigmas and further entrenching negative prejudices and societal biases [4]. Language models show a propensity for bias in a variety of areas, including but not limited to gender, age, sexual orientation, physical appearance, disability, nationality, ethnicity, socio-economic status, religion and culture. These biases have the potential to have significant consequences, particularly in contexts where they affect critical areas such as justice, healthcare or employment [5]."
                        ],
                        "paper": {
                            "corpus_id": 274117151,
                            "title": "Evaluating Gender Bias in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2338278194",
                                    "name": "Michael D\u00f6ll"
                                },
                                {
                                    "authorId": "2338277213",
                                    "name": "Markus D\u00f6hring"
                                },
                                {
                                    "authorId": "2338675374",
                                    "name": "Andreas M\u00fcller"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.8681640625
                    },
                    {
                        "id": "(Kumar et al., 2024)",
                        "snippets": [
                            "Biases in these models-such as gender, racial, ethnic, and ideological biases-stem from training data that reflect societal prejudices. Consequently, language models can perpetuate and even amplify these biases, leading to outputs that reinforce stereotypes and disadvantage certain groups (Basta et al., 2019). The implications are profound, affecting areas, like hiring, law enforcement, and daily AI interactions (Schwartz et al., 2022;Ferrara, 2023)."
                        ],
                        "paper": {
                            "corpus_id": 271745282,
                            "title": "Decoding Biases: Automated Methods and LLM Judges for Gender Bias Detection in Language Models",
                            "authors": [
                                {
                                    "authorId": "2109680564",
                                    "name": "Shachi H. Kumar"
                                },
                                {
                                    "authorId": "38531701",
                                    "name": "Saurav Sahay"
                                },
                                {
                                    "authorId": "2066417452",
                                    "name": "Sahisnu Mazumder"
                                },
                                {
                                    "authorId": "3442103",
                                    "name": "Eda Okur"
                                },
                                {
                                    "authorId": "2175808",
                                    "name": "R. Manuvinakurike"
                                },
                                {
                                    "authorId": "2275457041",
                                    "name": "Nicole Beckage"
                                },
                                {
                                    "authorId": "2087042748",
                                    "name": "Hsuan Su"
                                },
                                {
                                    "authorId": "2259815964",
                                    "name": "Hung-yi Lee"
                                },
                                {
                                    "authorId": "1896095",
                                    "name": "L. Nachman"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 18
                        },
                        "score": 0.80908203125
                    },
                    {
                        "id": "(Basta et al., 2019)",
                        "snippets": [
                            "Gender bias is highly impacting natural language processing applications. Word embeddings have clearly been proven both to keep and amplify gender biases that are present in current data sources. Recently, contextualized word embeddings have enhanced previous word embedding techniques by computing word vector representations dependent on the sentence they appear in. In this paper, we study the impact of this conceptual change in the word embedding computation in relation with gender bias. Our analysis includes different measures previously applied in the literature to standard word embeddings. Our findings suggest that contextualized word embeddings are less biased than standard ones even when the latter are debiased."
                        ],
                        "paper": {
                            "corpus_id": 121125604,
                            "title": "Evaluating the Underlying Gender Bias in Contextualized Word Embeddings",
                            "authors": [
                                {
                                    "authorId": "73312674",
                                    "name": "Christine Basta"
                                },
                                {
                                    "authorId": "1398996347",
                                    "name": "M. Costa-juss\u00e0"
                                },
                                {
                                    "authorId": "1794731",
                                    "name": "Noe Casas"
                                }
                            ],
                            "year": 2019,
                            "venue": "Proceedings of the First Workshop on Gender Bias in Natural Language Processing",
                            "n_citations": 193
                        },
                        "score": 0
                    },
                    {
                        "id": "(Meisel et al., 2022)",
                        "snippets": [
                            "Background Lower levels of education are strongly associated with negative health outcomes. The current study examined the degree to which those without a history of 4-year college attendance experience social stigmatization of their educational status and if these experiences are associated with mental health symptoms. Methods Data was obtained from 488 emerging adults who never attended 4-year college using Qualtrics Panels. Results 79.4% of participants agreed to one of the six statements that not attending 4-year college is stigmatized, and 71.8% endorsed experiencing at least one form of discrimination. Higher levels of education-related stigma and more frequent experiences of education-related discrimination was associated with greater past-month anxiety and depression symptoms. Conclusions These findings could serve to increase awareness regarding the unique and significant discrimination faced by young adults who do not attend 4-year college and identify specific areas of intervention that can help these young adults cope with the effects of stigma and discrimination."
                        ],
                        "paper": {
                            "corpus_id": 246654106,
                            "title": "Education-based stigma and discrimination among young adults not in 4-year college",
                            "authors": [
                                {
                                    "authorId": "3118881",
                                    "name": "Matthew K. Meisel"
                                },
                                {
                                    "authorId": "4609930",
                                    "name": "Michelle Haikalis"
                                },
                                {
                                    "authorId": "4240161",
                                    "name": "S. Colby"
                                },
                                {
                                    "authorId": "143796191",
                                    "name": "N. Barnett"
                                }
                            ],
                            "year": 2022,
                            "venue": "BMC Psychology",
                            "n_citations": 10
                        },
                        "score": 0
                    },
                    {
                        "id": "(Kotek et al., 2023)",
                        "snippets": [
                            "Large Language Models (LLMs) have made substantial progress in the past several months, shattering state-of-the-art benchmarks in many domains. This paper investigates LLMs\u2019 behavior with respect to gender stereotypes, a known issue for prior models. We use a simple paradigm to test the presence of gender bias, building on but differing from WinoBias, a commonly used gender bias dataset, which is likely to be included in the training data of current LLMs. We test four recently published LLMs and demonstrate that they express biased assumptions about men and women\u2019s occupations. Our contributions in this paper are as follows: (a) LLMs are 3-6 times more likely to choose an occupation that stereotypically aligns with a person\u2019s gender; (b) these choices align with people\u2019s perceptions better than with the ground truth as reflected in official job statistics; (c) LLMs in fact amplify the bias beyond what is reflected in perceptions or the ground truth; (d) LLMs ignore crucial ambiguities in sentence structure 95% of the time in our study items, but when explicitly prompted, they recognize the ambiguity; (e) LLMs provide explanations for their choices that are factually inaccurate and likely obscure the true reason behind their predictions. That is, they provide rationalizations of their biased behavior. This highlights a key property of these models: LLMs are trained on imbalanced datasets; as such, even with the recent successes of reinforcement learning with human feedback, they tend to reflect those imbalances back at us. As with other types of societal biases, we suggest that LLMs must be carefully tested to ensure that they treat minoritized individuals and communities equitably."
                        ],
                        "paper": {
                            "corpus_id": 261276445,
                            "title": "Gender bias and stereotypes in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "3365389",
                                    "name": "Hadas Kotek"
                                },
                                {
                                    "authorId": "90166394",
                                    "name": "Rikker Dockum"
                                },
                                {
                                    "authorId": "32100412",
                                    "name": "David Q. Sun"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Climate Informatics",
                            "n_citations": 236
                        },
                        "score": 0
                    },
                    {
                        "id": "(Abid et al., 2021)",
                        "snippets": [
                            "It has been observed that large-scale language models capture undesirable societal biases, e.g. relating to race and gender; yet religious bias has been relatively unexplored. We demonstrate that GPT-3, a state-of-the-art contextual language model, captures persistent Muslim-violence bias. We probe GPT-3 in various ways, including prompt completion, analogical reasoning, and story generation, to understand this anti-Muslim bias, demonstrating that it appears consistently and creatively in different uses of the model and that it is severe even compared to biases about other religious groups. For instance, Muslim is analogized to terrorist in 23% of test cases, while Jewish is mapped to its most common stereotype, money, in 5% of test cases. We quantify the positive distraction needed to overcome this bias with adversarial text prompts, and find that use of the most positive 6 adjectives reduces violent completions for Muslims from 66% to 20%, but which is still higher than for other religious groups."
                        ],
                        "paper": {
                            "corpus_id": 231603388,
                            "title": "Persistent Anti-Muslim Bias in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "144948925",
                                    "name": "Abubakar Abid"
                                },
                                {
                                    "authorId": "77751476",
                                    "name": "Maheen Farooqi"
                                },
                                {
                                    "authorId": "145085305",
                                    "name": "James Y. Zou"
                                }
                            ],
                            "year": 2021,
                            "venue": "AAAI/ACM Conference on AI, Ethics, and Society",
                            "n_citations": 555
                        },
                        "score": 0
                    },
                    {
                        "id": "(Mirza et al., 2024)",
                        "snippets": [
                            "Bias in LLMs manifests in multiple forms, including racial, gender, and cultural stereotypes, often perpetuating systemic inequalities. These biases have tangible consequences; for instance, in 2018, Amazon discontinued an AI-driven recruiting tool after discovering it systematically downgraded resumes containing the term \"women's,\" reflecting an inherent bias in the training data that favored male candidates [1]. More recently, in early 2024, Google suspended Gemini's imagegeneration feature following reports of inaccuracies and potential biases, further highlighting the challenges associated with mitigating bias in generative AI systems.\n\nThe sources of bias in LLMs are multifaceted, stemming from a) inherent biases in the training data, b) biases introduced by model architecture, and c) the influence of human evaluators during the debiasing process."
                        ],
                        "paper": {
                            "corpus_id": 272826949,
                            "title": "Evaluating Gender, Racial, and Age Biases in Large Language Models: A Comparative Analysis of Occupational and Crime Scenarios",
                            "authors": [
                                {
                                    "authorId": "2322445184",
                                    "name": "Vishal Mirza"
                                },
                                {
                                    "authorId": "2322445481",
                                    "name": "Rahul Kulkarni"
                                },
                                {
                                    "authorId": "2322445728",
                                    "name": "Aakanksha Jadhav"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.82080078125
                    },
                    {
                        "id": "(Cai et al., 2024)",
                        "snippets": [
                            "Bias refers to the existence of consistent inaccuracies, misattributions, or erroneous perceptions leading to a preference for specific groups or concepts, the reinforcement of stereotypes, or the formation of flawed conclusions derived from established patterns, it now profoundly impact users and society at large, underscoring the urgent need for a comprehensive examination and mitigation of these issues [9]26]."
                        ],
                        "paper": {
                            "corpus_id": 268553687,
                            "title": "Locating and Mitigating Gender Bias in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2292681106",
                                    "name": "Yuchen Cai"
                                },
                                {
                                    "authorId": "2237226610",
                                    "name": "Ding Cao"
                                },
                                {
                                    "authorId": "2237261799",
                                    "name": "Rongxi Guo"
                                },
                                {
                                    "authorId": "2293239741",
                                    "name": "Yaqin Wen"
                                },
                                {
                                    "authorId": "2237403462",
                                    "name": "Guiquan Liu"
                                },
                                {
                                    "authorId": "2292390890",
                                    "name": "Enhong Chen"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Intelligent Computing",
                            "n_citations": 5
                        },
                        "score": 0.81640625
                    },
                    {
                        "id": "(Guo et al., 2024)",
                        "snippets": [
                            "The bias presented in Large Language Models (LLMs) can be broadly categorized into intrinsic bias and extrinsic bias based on the different stages at which the biases manifest within the model's lifecycle and the type of bias being measured (Doan et al., 2024). Intrinsic bias refers to biases that are inherently within the internal representations or outputs of a trained or pre-trained LLM and are independent of any specific downstream tasks. Extrinsic bias refers to the biases that manifest during the model's performance on specific downstream tasks after training or fine-turing. Intrinsic biases are typically encoded during the training or pre-training phase when the LLM learns from large-scale corpora. For instance, if a pre-trained LLM consistently associates certain professions with specific genders (e.g., assuming all doctors to be male and assuming all nurses to be female) or racial stereotypes (May et al., 2019), this bias would be considered intrinsic bias. This type of bias is tied to the way the model has encoded relationships and patterns in its internal representations.\n\nIntrinsic biases are primarily introduced during the training/pre-training stages where models learn patterns and representations from vast corpora. These biases can be traced to several aspects of data and model design, which will be outlined below."
                        ],
                        "paper": {
                            "corpus_id": 274130807,
                            "title": "Bias in Large Language Models: Origin, Evaluation, and Mitigation",
                            "authors": [
                                {
                                    "authorId": "2331613833",
                                    "name": "Yufei Guo"
                                },
                                {
                                    "authorId": "2331371661",
                                    "name": "Muzhe Guo"
                                },
                                {
                                    "authorId": "2273060735",
                                    "name": "Juntao Su"
                                },
                                {
                                    "authorId": "2331371753",
                                    "name": "Zhou Yang"
                                },
                                {
                                    "authorId": "2331357933",
                                    "name": "Mengqiu Zhu"
                                },
                                {
                                    "authorId": "2331365682",
                                    "name": "Hongfei Li"
                                },
                                {
                                    "authorId": "1455277182",
                                    "name": "Mengyang Qiu"
                                },
                                {
                                    "authorId": "2331377869",
                                    "name": "Shuo Shuo Liu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 22
                        },
                        "score": 0.9150390625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Types of Social Bias in LLMs",
                "tldr": "Large Language Models (LLMs) exhibit multiple forms of social bias related to gender, race, religion, disability, sexual orientation, age, nationality, physical appearance, and socioeconomic status. These biases manifest in various ways and can compound for individuals with intersectional identities, potentially causing both representational and allocational harms. (20 sources)",
                "text": "\n## Gender Bias\nGender bias remains one of the most extensively studied forms of bias in LLMs. Research has consistently shown that models associate professions and attributes with gender in stereotypical ways that often exceed real-world statistics <Paper corpusId=\"249431834\" paperTitle=\"(Caliskan et al., 2022)\" isShortName></Paper> <Paper corpusId=\"1704893\" paperTitle=\"(Bolukbasi et al., 2016)\" isShortName></Paper>. For example, models tend to associate women with family and domestic roles while linking men with careers and leadership positions <Paper corpusId=\"23163324\" paperTitle=\"(Caliskan et al., 2016)\" isShortName></Paper>. Even contextual word embeddings like BERT and GPT-2 encode gender biases, though studies suggest these biases may be less pronounced than other forms of social bias <Paper corpusId=\"202781363\" paperTitle=\"(Tan et al., 2019)\" isShortName></Paper> <Paper corpusId=\"236034024\" paperTitle=\"(Magee et al., 2021)\" isShortName></Paper>.\n\n## Racial Bias\nRacial bias in LLMs manifests through associations between racial identities and stereotypical attributes or negative sentiments. Research has found that racial bias is encoded strongly in contextual word models, potentially even more strongly than gender bias <Paper corpusId=\"202781363\" paperTitle=\"(Tan et al., 2019)\" isShortName></Paper>. These biases can appear in downstream tasks such as sentiment analysis and toxicity prediction, where text mentioning certain racial groups receives more negative scores <Paper corpusId=\"219530686\" paperTitle=\"(Guo et al., 2020)\" isShortName></Paper>. Studies using tools like the Contextualized Embedding Association Test (CEAT) have confirmed that all tested English-language models contain biased representations related to race <Paper corpusId=\"219530686\" paperTitle=\"(Guo et al., 2020)\" isShortName></Paper>.\n\n## Religious Bias\nReligious bias represents another significant dimension of social bias in LLMs. Research has revealed alarming associations, such as LLMs being significantly more likely to associate Muslims with violence compared to other religious groups <Paper corpusId=\"231603388\" paperTitle=\"(Abid et al., 2021)\" isShortName></Paper>. Muslim identity labels consistently receive more negative sentiment scores across various models <Paper corpusId=\"236034024\" paperTitle=\"(Magee et al., 2021)\" isShortName></Paper>. This bias appears to be more resistant to context than gender bias and persists even with increases in model size and training data diversity <Paper corpusId=\"236034024\" paperTitle=\"(Magee et al., 2021)\" isShortName></Paper> <Paper corpusId=\"266521434\" paperTitle=\"(Oketunji et al., 2023)\" isShortName></Paper>.\n\n## Disability-Related Bias\nResearch on disability-related bias has emerged more recently but shows concerning patterns. Models exhibit undesirable biases toward mentions of disability in tasks such as toxicity prediction and sentiment analysis <Paper corpusId=\"218487466\" paperTitle=\"(Hutchinson et al., 2020)\" isShortName></Paper>. Neural embeddings that form the foundation of most NLP pipelines contain biases toward disability mentions, with certain topics like gun violence, homelessness, and drug addiction being over-represented in texts discussing mental illness <Paper corpusId=\"218487466\" paperTitle=\"(Hutchinson et al., 2020)\" isShortName></Paper>. Disability labels consistently receive more negative sentiment scores across models, with only a few exceptions like \"with Down Syndrome\" <Paper corpusId=\"236034024\" paperTitle=\"(Magee et al., 2021)\" isShortName></Paper> <Paper corpusId=\"261214814\" paperTitle=\"(Venkit, 2023)\" isShortName></Paper>.\n\n## Intersectional Bias\nA growing body of research focuses on intersectional bias, where individuals at the intersection of multiple marginalized identities face unique biases. Studies have found that intersectional identities often suffer from stronger bias than their constituent minority identities alone <Paper corpusId=\"202781363\" paperTitle=\"(Tan et al., 2019)\" isShortName></Paper>. For instance, African American females and Mexican American females are associated with unique emergent biases that don't overlap with the biases of their constituent minority identities <Paper corpusId=\"219530686\" paperTitle=\"(Guo et al., 2020)\" isShortName></Paper>. Research has demonstrated that race appears to have a larger effect on intersectional identities than gender <Paper corpusId=\"202781363\" paperTitle=\"(Tan et al., 2019)\" isShortName></Paper>. Bias can manifest in unpredictable ways at category intersections, such as when combining gender, religion, and disability labels <Paper corpusId=\"236034024\" paperTitle=\"(Magee et al., 2021)\" isShortName></Paper>.\n\n## Other Forms of Social Bias\nBeyond these major categories, researchers are increasingly examining biases related to sexual orientation, age, nationality, physical appearance, and socioeconomic status <Paper corpusId=\"258170403\" paperTitle=\"(Sharma et al., 2023)\" isShortName></Paper> <Paper corpusId=\"266521434\" paperTitle=\"(Oketunji et al., 2023)\" isShortName></Paper> <Paper corpusId=\"270560549\" paperTitle=\"(Das et al., 2024)\" isShortName></Paper>. Social stigmas, which often relate to diseases, disabilities, mental illness, and socioeconomic status, affect a substantial portion of the population and are becoming a focus of bias research <Paper corpusId=\"259129801\" paperTitle=\"(Mei et al., 2023)\" isShortName></Paper> <Paper corpusId=\"246654106\" paperTitle=\"(Meisel et al., 2022)\" isShortName></Paper>.\n\nRecent research has also expanded to examine biases in multimodal large language models (MLLMs), which can inherit severe social biases from their training datasets related to attributes like race and gender <Paper corpusId=\"271859735\" paperTitle=\"(Cheng et al., 2024)\" isShortName></Paper>. As the scope of bias research broadens, researchers are developing more comprehensive approaches to address multiple bias dimensions simultaneously rather than tackling them independently <Paper corpusId=\"267627928\" paperTitle=\"(Jain et al., 2024)\" isShortName></Paper>.\n\nThe various forms of social bias in LLMs can result in two primary forms of harm: representational and allocational <Paper corpusId=\"274117151\" paperTitle=\"(Doll et al., 2024)\" isShortName></Paper> <Paper corpusId=\"235436386\" paperTitle=\"(Suresh et al., 2019)\" isShortName></Paper>. Representational harms occur when models portray certain groups negatively or fail to represent them adequately, while allocational harms involve denying opportunities or resources to specific groups, such as in recruitment algorithms that may disadvantage certain demographic groups <Paper corpusId=\"259716055\" paperTitle=\"(Kolisko et al., 2023)\" isShortName></Paper> <Paper corpusId=\"259203321\" paperTitle=\"(Nemani et al., 2023)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Caliskan et al., 2022)",
                        "snippets": [
                            "Word embeddings are numeric representations of meaning derived from word co-occurrence statistics in corpora of human-produced texts. The statistical regularities in language corpora encode well-known social biases into word embeddings (e.g., the word vector for family is closer to the vector women than to men)",
                            "When datasets that reflect the thoughts, feelings, and actions of a community are used to train artificial intelligence (AI) models, the models inevitably incorporate the associations and representations of the community [11-13, 15, 17, 26, 40, 51, 54].\n\nIn one of the first studies systematically assessing social biases in word embeddings, Caliskan et al. [12] showed that pretrained GloVe embeddings replicated ten major biases widely found among human participants including biases associating young-good/elderly-bad, European American-good/African American-bad, and women-family/men-career [28]. Such group-based biases can manifest in NLP applications (a recent example was seen in Amazon's automated resume screening algorithm that preferentially hired men over women [7])."
                        ],
                        "paper": {
                            "corpus_id": 249431834,
                            "title": "Gender Bias in Word Embeddings: A Comprehensive Analysis of Frequency, Syntax, and Semantics",
                            "authors": [
                                {
                                    "authorId": "144537437",
                                    "name": "Aylin Caliskan"
                                },
                                {
                                    "authorId": "2168463879",
                                    "name": "Pimparkar Parth Ajay"
                                },
                                {
                                    "authorId": "66649568",
                                    "name": "Tessa E. S. Charlesworth"
                                },
                                {
                                    "authorId": "100508901",
                                    "name": "R. Wolfe"
                                },
                                {
                                    "authorId": "1968771",
                                    "name": "M. Banaji"
                                }
                            ],
                            "year": 2022,
                            "venue": "AAAI/ACM Conference on AI, Ethics, and Society",
                            "n_citations": 51
                        },
                        "score": 0.89306640625
                    },
                    {
                        "id": "(Bolukbasi et al., 2016)",
                        "snippets": [
                            "The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias."
                        ],
                        "paper": {
                            "corpus_id": 1704893,
                            "title": "Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings",
                            "authors": [
                                {
                                    "authorId": "2843215",
                                    "name": "Tolga Bolukbasi"
                                },
                                {
                                    "authorId": "2782886",
                                    "name": "Kai-Wei Chang"
                                },
                                {
                                    "authorId": "145085305",
                                    "name": "James Y. Zou"
                                },
                                {
                                    "authorId": "1699322",
                                    "name": "Venkatesh Saligrama"
                                },
                                {
                                    "authorId": "2186481",
                                    "name": "A. Kalai"
                                }
                            ],
                            "year": 2016,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 3153
                        },
                        "score": 0
                    },
                    {
                        "id": "(Caliskan et al., 2016)",
                        "snippets": [
                            "Machines learn what people know implicitly AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan et al. now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs\u2014for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior. Science, this issue p. 183; see also p. 133 Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias. Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology."
                        ],
                        "paper": {
                            "corpus_id": 23163324,
                            "title": "Semantics derived automatically from language corpora contain human-like biases",
                            "authors": [
                                {
                                    "authorId": "144537437",
                                    "name": "Aylin Caliskan"
                                },
                                {
                                    "authorId": "145315445",
                                    "name": "J. Bryson"
                                },
                                {
                                    "authorId": "47735253",
                                    "name": "Arvind Narayanan"
                                }
                            ],
                            "year": 2016,
                            "venue": "Science",
                            "n_citations": 2673
                        },
                        "score": 0
                    },
                    {
                        "id": "(Tan et al., 2019)",
                        "snippets": [
                            "In natural language processing, gender bias has been shown to exist in context-free word embeddings. Recently, contextual word representations have outperformed word embeddings in several downstream NLP tasks. These word representations are conditioned on their context within a sentence, and can also be used to encode the entire sentence. In this paper, we analyze the extent to which state-of-the-art models for contextual word representations, such as BERT and GPT-2, encode biases with respect to gender, race, and intersectional identities",
                            "we provide evidence for how racial bias is encoded strongly in contextual word models, potentially even more so than gender bias",
                            "we introduce a method of comparison that anchors at the most or least privileged group to show that intersectional identities suffer from such bias as well, and more so than their constituent minority identities. In we show that the effect of race on intersectional identities seems to be larger than the effect of gender."
                        ],
                        "paper": {
                            "corpus_id": 202781363,
                            "title": "Assessing Social and Intersectional Biases in Contextualized Word Representations",
                            "authors": [
                                {
                                    "authorId": "144787248",
                                    "name": "Y. Tan"
                                },
                                {
                                    "authorId": "47777961",
                                    "name": "Elisa Celis"
                                }
                            ],
                            "year": 2019,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 229
                        },
                        "score": 0.83154296875
                    },
                    {
                        "id": "(Magee et al., 2021)",
                        "snippets": [
                            "To examine whether intersectional bias can be observed in language generation, we examine \\emph{GPT-2} and \\emph{GPT-NEO} models, ranging in size from 124 million to ~2.7 billion parameters. We conduct an experiment combining up to three social categories - gender, religion and disability - into unconditional or zero-shot prompts used to generate sentences that are then analysed for sentiment. Our results confirm earlier tests conducted with auto-regressive causal models, including the \\emph{GPT} family of models. We also illustrate why bias may be resistant to techniques that target single categories (e.g. gender, religion and race), as it can also manifest, in often subtle ways, in texts prompted by concatenated social categories.\n\nWe conclude that (1) bias exists at significant levels across different social categories (gender, religion and disability); (2) it can manifest, in unpredictable ways, at category intersections; and (3) it is resistant to increases in model size and training data diversity. Specifically, while gender bias is minor and heavily conditioned on prompt context, religion and disability bias is strongly evident and resistant to context, with Muslim and all disability labels other than 'with Down Syndrome' scoring worse than no label."
                        ],
                        "paper": {
                            "corpus_id": 236034024,
                            "title": "Intersectional Bias in Causal Language Models",
                            "authors": [
                                {
                                    "authorId": "2733075",
                                    "name": "L. Magee"
                                },
                                {
                                    "authorId": "9557084",
                                    "name": "Lida Ghahremanlou"
                                },
                                {
                                    "authorId": "13714096",
                                    "name": "K. Soldati\u0107"
                                },
                                {
                                    "authorId": "97868921",
                                    "name": "S. Robertson"
                                }
                            ],
                            "year": 2021,
                            "venue": "arXiv.org",
                            "n_citations": 33
                        },
                        "score": 0.8779296875
                    },
                    {
                        "id": "(Guo et al., 2020)",
                        "snippets": [
                            "We introduce the Contextualized Embedding Association Test (CEAT), that can summarize the magnitude of overall bias in neural language models by incorporating a random-effects model. Experiments on social and intersectional biases show that CEAT finds evidence of all tested biases and provides comprehensive information on the variance of effect magnitudes of the same bias in different contexts. All the models trained on English corpora that we study contain biased representations. GPT-2 contains the smallest magnitude of overall bias followed by GPT, BERT, and then ELMo, negatively correlating with the contextualization levels of the models. Furthermore, we develop two methods, Intersectional Bias Detection (IBD) and Emergent Intersectional Bias Detection (EIBD), to automatically identify the intersectional biases and emergent intersectional biases from static word embeddings in addition to measuring them in contextualized word embeddings. We present the first algorithmic bias detection findings on how intersectional group members are strongly associated with unique emergent biases that do not overlap with the biases of their constituent minority identities. IBD achieves an accuracy of 81.6% and 82.7%, respectively, when detecting the intersectional biases of African American females and Mexican American females, where the random correct identification rates are 14.3% and 13.3%. EIBD reaches an accuracy of 84.7% and 65.3%, respectively, when detecting the emergent intersectional biases unique to African American females and Mexican American females, where the random correct identification rates are 9.2% and 6.1%. Our results indicate that intersectional biases associated with members of multiple minority groups, such as African American females and Mexican American females, have the highest magnitude across all neural language models."
                        ],
                        "paper": {
                            "corpus_id": 219530686,
                            "title": "Detecting Emergent Intersectional Biases: Contextualized Word Embeddings Contain a Distribution of Human-like Biases",
                            "authors": [
                                {
                                    "authorId": "28735341",
                                    "name": "W. Guo"
                                },
                                {
                                    "authorId": "144537437",
                                    "name": "Aylin Caliskan"
                                }
                            ],
                            "year": 2020,
                            "venue": "AAAI/ACM Conference on AI, Ethics, and Society",
                            "n_citations": 243
                        },
                        "score": 0.876953125
                    },
                    {
                        "id": "(Abid et al., 2021)",
                        "snippets": [
                            "It has been observed that large-scale language models capture undesirable societal biases, e.g. relating to race and gender; yet religious bias has been relatively unexplored. We demonstrate that GPT-3, a state-of-the-art contextual language model, captures persistent Muslim-violence bias. We probe GPT-3 in various ways, including prompt completion, analogical reasoning, and story generation, to understand this anti-Muslim bias, demonstrating that it appears consistently and creatively in different uses of the model and that it is severe even compared to biases about other religious groups. For instance, Muslim is analogized to terrorist in 23% of test cases, while Jewish is mapped to its most common stereotype, money, in 5% of test cases. We quantify the positive distraction needed to overcome this bias with adversarial text prompts, and find that use of the most positive 6 adjectives reduces violent completions for Muslims from 66% to 20%, but which is still higher than for other religious groups."
                        ],
                        "paper": {
                            "corpus_id": 231603388,
                            "title": "Persistent Anti-Muslim Bias in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "144948925",
                                    "name": "Abubakar Abid"
                                },
                                {
                                    "authorId": "77751476",
                                    "name": "Maheen Farooqi"
                                },
                                {
                                    "authorId": "145085305",
                                    "name": "James Y. Zou"
                                }
                            ],
                            "year": 2021,
                            "venue": "AAAI/ACM Conference on AI, Ethics, and Society",
                            "n_citations": 555
                        },
                        "score": 0
                    },
                    {
                        "id": "(Oketunji et al., 2023)",
                        "snippets": [
                            "The formulation of the Large Language Model Bias Index (LLMBI) necessitates a sweeping understanding of the multifaceted nature of bias in Large Language Models (LLMs). These biases, spanning across various dimensions such as gender, religion, race, age, nationality, disability, sexual orientation, physical appearance, and socioeconomic status, can signicantly impact the outputs of LLMs, leading to skewed and potentially harmful outcomes. \n\nGender bias in LLMs often manifests in stereotypical representations and unequal treatment of dierent genders. This bias can perpetuate gender stereotypes and discrimination, aecting the fairness of AI applications in areas like recruitment and social media (Zhao et al., 2017) (Zhao et al., 2017). Religion bias in LLMs can lead to the marginalisation of certain religious groups, fostering an environment of intolerance and misunderstanding (Aroyo et al., 2015) (Aroyo et al., 2015). \n\nRacial bias is another pressing concern, as LLMs can perpetuate racial stereotypes and prejudices. This bias can have severe implications in legal, healthcare, and nancial systems, where it can lead to discriminatory practices (Sweeney, 2013). Age bias in LLMs can result in underrepresenting or misrepresenting specic age groups, impacting sectors like healthcare and marketing (Levy, 2003) (Levy, 2003). \n\nNationality bias in LLMs can lead to the preferential treatment of certain nationalities over others, aecting global perceptions and international relations (Liao et al., 2018) (Liao et al., 2020). Disability bias in LLMs can result in the exclusion or misrepresentation of people with disabilities, impacting accessibility and inclusivity (Trewin et al., 2019) [14]. \n\nSexual orientation bias in LLMs can perpetuate harmful stereotypes and discrimination against LGBTQ+ individuals, aecting mental health and social acceptance (Costanza-Chock, 2018) (Costanza-Chock, 2018). Physical appearance bias in LLMs can lead to body shaming and unrealistic beauty standards, impacting mental health and self-esteem (Yee et al., 2007) (Yee et al., 2007). \n\nLastly, socioeconomic status bias in LLMs can reinforce class divisions and economic inequalities, aecting opportunities and social mobility (O'Neil, 2016) [17]."
                        ],
                        "paper": {
                            "corpus_id": 266521434,
                            "title": "Large Language Model (LLM) Bias Index - LLMBI",
                            "authors": [
                                {
                                    "authorId": "2274104512",
                                    "name": "Abiodun Finbarrs Oketunji"
                                },
                                {
                                    "authorId": "2276205421",
                                    "name": "Muhammad Anas"
                                },
                                {
                                    "authorId": "2276202526",
                                    "name": "Deepthi Saina"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 12
                        },
                        "score": 0.81884765625
                    },
                    {
                        "id": "(Hutchinson et al., 2020)",
                        "snippets": [
                            "Building equitable and inclusive NLP technologies demands consideration of whether and how social attitudes are represented in ML models. In particular, representations encoded in models often inadvertently perpetuate undesirable social biases from the data on which they are trained. In this paper, we present evidence of such undesirable biases towards mentions of disability in two different English language models: toxicity prediction and sentiment analysis. Next, we demonstrate that the neural embeddings that are the critical first step in most NLP pipelines similarly contain undesirable biases towards mentions of disability. We end by highlighting topical biases in the discourse about disability which may contribute to the observed model biases; for instance, gun violence, homelessness, and drug addiction are over-represented in texts discussing mental illness."
                        ],
                        "paper": {
                            "corpus_id": 218487466,
                            "title": "Social Biases in NLP Models as Barriers for Persons with Disabilities",
                            "authors": [
                                {
                                    "authorId": "2044655623",
                                    "name": "Ben Hutchinson"
                                },
                                {
                                    "authorId": "3331141",
                                    "name": "Vinodkumar Prabhakaran"
                                },
                                {
                                    "authorId": "40081727",
                                    "name": "Emily L. Denton"
                                },
                                {
                                    "authorId": "20825661",
                                    "name": "Kellie Webster"
                                },
                                {
                                    "authorId": "2112887022",
                                    "name": "Yu Zhong"
                                },
                                {
                                    "authorId": "1667883461",
                                    "name": "Stephen Denuyl"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 313
                        },
                        "score": 0.81103515625
                    },
                    {
                        "id": "(Venkit, 2023)",
                        "snippets": [
                            "In my study [9], I delve into sentiment analysis and toxicity detection models to identify explicit bias against race, gender, and people with disabilities (PWDs). Through statistical exploration of conversations on social media platforms such as Twitter and Reddit, I gain insights into how disability bias permeates real-world social settings. To quantify explicit sociodemographic bias in sentiment analysis and toxicity analysis models, I create the Bias Identification Test in Sentiment (BITS) corpus1. Applying BITS, I uncover significant biases in popular AIaaS sentiment analysis tools, including TextBlob, VADER, and Google Cloud Natural Language API, as well as toxicity analysis models like Toxic-BERT. Remarkably, all of these models exhibit statistically significant explicit bias against disability, underscoring the need for comprehensive understanding and mitigation of biases affecting such groups.\n\nExpanding on this, my next work [8] delves into the realm of implicit bias in NLP models. While some models may not overtly exhibit bias, they can unintentionally perpetuate harmful stereotypes [4]. To measure and identify implicit bias in commonly used embedding and large language models, I propose a methodology to measure social biases in various NLP architectures. Focusing on people with disabilities (PWD) as a group with complex social dynamics, I analyze various word embedding-based and transformer-based LLMs, revealing significant biases against PWDs in all tested models. These findings expose how models trained on extensive corpora tend to favor ableist language, underscoring the urgency of detecting and addressing implicit bias.\n\nTo explore this aspect, I investigate how LLMs represent nationality and their impact on societal stereotypes [6]. By examining LLM-generated stories for various nationalities, I establish a correlation between sentiment and the population of internet users in a country. The study reveals the unintentional implicit and explicit nationality biases exhibited by GPT-2, with nations having lower internet representation and economic status generating negative sentiment stories and employing a greater number of negative adjectives."
                        ],
                        "paper": {
                            "corpus_id": 261214814,
                            "title": "Towards a Holistic Approach: Understanding Sociodemographic Biases in NLP Models using an Interdisciplinary Lens",
                            "authors": [
                                {
                                    "authorId": "2053812167",
                                    "name": "Pranav Narayanan Venkit"
                                }
                            ],
                            "year": 2023,
                            "venue": "AAAI/ACM Conference on AI, Ethics, and Society",
                            "n_citations": 8
                        },
                        "score": 0.8291015625
                    },
                    {
                        "id": "(Sharma et al., 2023)",
                        "snippets": [
                            "Large pre-trained language models are widely used in the community. These models are usually trained on unmoderated and unfiltered data from open sources like the Internet. Due to this, biases that we see in platforms online which are a reflection of those in society are in turn captured and learned by these models. These models are deployed in applications that affect millions of people and their inherent biases are harmful to the targeted social groups.\n\nGiven the popularity of large pre-trained language models and their widespread use in the community, it is pertinent to evaluate their inherent biases. These biases come about because these models (for example, BERT (Devlin et al., 2019) and BERT based models, ALBERT (Lan et al., 2020), RoBERTa (Liu et al., 2019)) are trained on large sets of often freely available and unmoderated text data from sources such as the internet. \n\nDue to this, gender, race, religious and other social biases that we see in the real world are often translated into the models. Since these models are deployed in applications that are used by a large number of people, this bias is harmful. For example, these models are commonly used in content moderation tasks on social media platforms online. If the model is unfairly biased against certain social groups-minorities, marginalized people and those sections of society that are discriminated against, then they would be adversely affected by the application.\n\nStereoSet accounts for four different biases that are based on gender, occupation, race and religion. In addition to these, CrowS-Pairs also has sexual orientation, age, nationality, disability and physical appearance."
                        ],
                        "paper": {
                            "corpus_id": 258170403,
                            "title": "Evaluation of Social Biases in Recent Large Pre-Trained Models",
                            "authors": [
                                {
                                    "authorId": "2214583125",
                                    "name": "Swapnil Sharma"
                                },
                                {
                                    "authorId": "2214521713",
                                    "name": "Nikita Anand"
                                },
                                {
                                    "authorId": "1415341297",
                                    "name": "V. KranthiKiranG."
                                },
                                {
                                    "authorId": "2214565507",
                                    "name": "Alind Jain"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.8125
                    },
                    {
                        "id": "(Das et al., 2024)",
                        "snippets": [
                            "In our research, we specifically examine four types of biases: gender, race, disability, and religion. Gender bias refers to the prejudiced treatment based on an individual's sex or gender identity. Race bias involves discriminatory actions or attitudes towards individuals based on their racial or ethnic background. Disability bias encompasses unfair treatment of people with physical or mental impairments. Religion bias involves prejudices and discriminatory behaviors directed at individuals based on their religious beliefs or practices. Our study aims to analyze the prevalence and impact of these biases in various contexts."
                        ],
                        "paper": {
                            "corpus_id": 270560549,
                            "title": "Investigating Annotator Bias in Large Language Models for Hate Speech Detection",
                            "authors": [
                                {
                                    "authorId": "2221319016",
                                    "name": "Amit Das"
                                },
                                {
                                    "authorId": "2264121393",
                                    "name": "Zheng Zhang"
                                },
                                {
                                    "authorId": "2307011198",
                                    "name": "Fatemeh Jamshidi"
                                },
                                {
                                    "authorId": "2212131028",
                                    "name": "Vinija Jain"
                                },
                                {
                                    "authorId": "2275226689",
                                    "name": "Aman Chadha"
                                },
                                {
                                    "authorId": "2181140446",
                                    "name": "Nilanjana Raychawdhary"
                                },
                                {
                                    "authorId": "2291126998",
                                    "name": "Mary Sandage"
                                },
                                {
                                    "authorId": "2290014871",
                                    "name": "Lauramarie Pope"
                                },
                                {
                                    "authorId": "38328224",
                                    "name": "Gerry V. Dozier"
                                },
                                {
                                    "authorId": "2283201612",
                                    "name": "Cheryl Seals"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.94921875
                    },
                    {
                        "id": "(Mei et al., 2023)",
                        "snippets": [
                            "The rapid deployment of artificial intelligence (AI) models de- demands a thorough investigation of biases and risks inherent in these models to understand their impact on individuals and society. A growing body of work has shown that social biases are encoded in language models and their downstream tasks",
                            "Caliskan et al. [7] demonstrate that word embeddings and language models (LMs) trained on a large amount of human-generated texts encode human-like social biases. Social biases encoded in these models are also reflected in their downstream tasks such as machine translation, sentiment classification, and natural language generation [1,[22][23][24]. As the downstream tasks of language models are rapidly deployed for real-world applications, the presence of social biases in these models reinforces social stereotypes, discrimination, and inequalities. Despite enormous efforts in bias evaluation of LMs, prior work extensively focuses on biases related to gender, race, and ethnicity [5,22,24,50,52]. Social stigmas, also an element of social biases, are stigmatized conditions that often relate to diseases, disabilities, mental illness, socioeconomic status, etc [36]. Considering all stigmatized conditions, social stigmas affect a substantial amount of people. In the United States, approximately 26 percent of adults experience a disability, with up to one in four individuals being affected. In 2021, there were around 57.8M adults that experienced mental illness, which was around 22% of the population in the United States. Social stigmas prevent individuals from social activities and access to education, healthcare, and career opportunities, negatively influencing their psychological well-being and life outcomes [14,[31][32][33][37]."
                        ],
                        "paper": {
                            "corpus_id": 259129801,
                            "title": "Bias Against 93 Stigmatized Groups in Masked Language Models and Downstream Sentiment Classification Tasks",
                            "authors": [
                                {
                                    "authorId": "2189183000",
                                    "name": "Katelyn Mei"
                                },
                                {
                                    "authorId": "2196943720",
                                    "name": "Sonia Fereidooni"
                                },
                                {
                                    "authorId": "144537437",
                                    "name": "Aylin Caliskan"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Fairness, Accountability and Transparency",
                            "n_citations": 51
                        },
                        "score": 0.89599609375
                    },
                    {
                        "id": "(Meisel et al., 2022)",
                        "snippets": [
                            "Background Lower levels of education are strongly associated with negative health outcomes. The current study examined the degree to which those without a history of 4-year college attendance experience social stigmatization of their educational status and if these experiences are associated with mental health symptoms. Methods Data was obtained from 488 emerging adults who never attended 4-year college using Qualtrics Panels. Results 79.4% of participants agreed to one of the six statements that not attending 4-year college is stigmatized, and 71.8% endorsed experiencing at least one form of discrimination. Higher levels of education-related stigma and more frequent experiences of education-related discrimination was associated with greater past-month anxiety and depression symptoms. Conclusions These findings could serve to increase awareness regarding the unique and significant discrimination faced by young adults who do not attend 4-year college and identify specific areas of intervention that can help these young adults cope with the effects of stigma and discrimination."
                        ],
                        "paper": {
                            "corpus_id": 246654106,
                            "title": "Education-based stigma and discrimination among young adults not in 4-year college",
                            "authors": [
                                {
                                    "authorId": "3118881",
                                    "name": "Matthew K. Meisel"
                                },
                                {
                                    "authorId": "4609930",
                                    "name": "Michelle Haikalis"
                                },
                                {
                                    "authorId": "4240161",
                                    "name": "S. Colby"
                                },
                                {
                                    "authorId": "143796191",
                                    "name": "N. Barnett"
                                }
                            ],
                            "year": 2022,
                            "venue": "BMC Psychology",
                            "n_citations": 10
                        },
                        "score": 0
                    },
                    {
                        "id": "(Cheng et al., 2024)",
                        "snippets": [
                            "Multi-modal Large Language Models (MLLMs) have advanced significantly, offering powerful vision-language understanding capabilities. However, these models often inherit severe social biases from their training datasets, leading to unfair predictions based on attributes like race and gender."
                        ],
                        "paper": {
                            "corpus_id": 271859735,
                            "title": "Social Debiasing for Fair Multi-modal LLMs",
                            "authors": [
                                {
                                    "authorId": "2149241557",
                                    "name": "Harry Cheng"
                                },
                                {
                                    "authorId": "1390575046",
                                    "name": "Yangyang Guo"
                                },
                                {
                                    "authorId": "2273322768",
                                    "name": "Qingpei Guo"
                                },
                                {
                                    "authorId": "2249834712",
                                    "name": "Ming Yang"
                                },
                                {
                                    "authorId": "2247906706",
                                    "name": "Tian Gan"
                                },
                                {
                                    "authorId": "2284688853",
                                    "name": "Liqiang Nie"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.884765625
                    },
                    {
                        "id": "(Jain et al., 2024)",
                        "snippets": [
                            "Pretrained Language Models (PLMs) are widely used in NLP for various tasks. Recent studies have identified various biases that such models exhibit and have proposed methods to correct these biases. However, most of the works address a limited set of bias dimensions independently such as gender, race, or religion."
                        ],
                        "paper": {
                            "corpus_id": 267627928,
                            "title": "MAFIA: Multi-Adapter Fused Inclusive Language Models",
                            "authors": [
                                {
                                    "authorId": "3094662",
                                    "name": "Prachi Jain"
                                },
                                {
                                    "authorId": "2266397701",
                                    "name": "Ashutosh Sathe"
                                },
                                {
                                    "authorId": "2140408530",
                                    "name": "Varun Gumma"
                                },
                                {
                                    "authorId": "52154863",
                                    "name": "Kabir Ahuja"
                                },
                                {
                                    "authorId": "2256989615",
                                    "name": "Sunayana Sitaram"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
                            "n_citations": 1
                        },
                        "score": 0.83544921875
                    },
                    {
                        "id": "(Doll et al., 2024)",
                        "snippets": [
                            "LLMs learn from vast amounts of text collected from the Internet and other sources. This training data often contains unbalanced representations and stereotypes that are unintentionally embedded in the models [3].\n\nBias in LLMs can harm both individuals and society, with consequences including allocative and representational harms. Allocative harms occur when resources or opportunities are distributed unfairly, often to the detriment of already marginalised groups. For example, recruitment algorithms may be skewed to disadvantage certain demographic groups. Representational harm occurs when groups are misrepresented or stereotyped, reinforcing harmful narratives and stigmas and further entrenching negative prejudices and societal biases [4]. Language models show a propensity for bias in a variety of areas, including but not limited to gender, age, sexual orientation, physical appearance, disability, nationality, ethnicity, socio-economic status, religion and culture. These biases have the potential to have significant consequences, particularly in contexts where they affect critical areas such as justice, healthcare or employment [5]."
                        ],
                        "paper": {
                            "corpus_id": 274117151,
                            "title": "Evaluating Gender Bias in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2338278194",
                                    "name": "Michael D\u00f6ll"
                                },
                                {
                                    "authorId": "2338277213",
                                    "name": "Markus D\u00f6hring"
                                },
                                {
                                    "authorId": "2338675374",
                                    "name": "Andreas M\u00fcller"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.8681640625
                    },
                    {
                        "id": "(Suresh et al., 2019)",
                        "snippets": [
                            "As machine learning (ML) increasingly affects people and society, awareness of its potential unwanted consequences has also grown. To anticipate, prevent, and mitigate undesirable downstream consequences, it is critical that we understand when and how harm might be introduced throughout the ML life cycle. In this paper, we provide a framework that identifies seven distinct potential sources of downstream harm in machine learning, spanning data collection, development, and deployment. In doing so, we aim to facilitate more productive and precise communication around these issues, as well as more direct, application-grounded ways to mitigate them."
                        ],
                        "paper": {
                            "corpus_id": 235436386,
                            "title": "A Framework for Understanding Sources of Harm throughout the Machine Learning Life Cycle",
                            "authors": [
                                {
                                    "authorId": "46537606",
                                    "name": "Harini Suresh"
                                },
                                {
                                    "authorId": "1724429",
                                    "name": "J. Guttag"
                                }
                            ],
                            "year": 2019,
                            "venue": "Conference on Equity and Access in Algorithms, Mechanisms, and Optimization",
                            "n_citations": 442
                        },
                        "score": 0
                    },
                    {
                        "id": "(Kolisko et al., 2023)",
                        "snippets": [
                            "There is a growing body of work documenting social biases in large language models. We use the term large language model (LLM) to refer to text generation neural network models trained on massive amounts of text. Popular examples include BERT (Devlin et al. 2019), GPT-3 (Brown et al. 2020), RoBERTa (Liu et al., 2019)BLOOM (Big-Science 2022). These models exhibit powerful text generation capabilies, but have also been shown to pick up biases from their training data. \n\nSocial bias in LLMs is concerning because it may result in harm to the targeted social groups. Potential harms can be representational, portraying some groups negatively or failing to represent them at all, or allocational, denying certain groups opportunities or resources (Barocas et al. 2017). \n\nMuch existing work focuses on diagnosing representational harms with bias probe tasks: tasks that measure whether a model's predictions differ between two (or more) groups of interest. A number of probe tasks have been proposed: (Rudinger et al., 2017); (Sheng et al., 2019); Bordia and Bowman (2019); (Lee et al., 2019); (Liu et al., 2019)(Liu et al., 2019); (Nadeem et al., 2020)(Sheng et al., 2021) and others. Most of these focus on gender stereotypes. 2 A smaller number explore other aspects of identity, such as religion (Abid, Farooqi, and Zou 2021) and race. 3"
                        ],
                        "paper": {
                            "corpus_id": 259716055,
                            "title": "Exploring Social Biases of Large Language Models in a College Artificial Intelligence Course",
                            "authors": [
                                {
                                    "authorId": "2222666109",
                                    "name": "Skylar Kolisko"
                                },
                                {
                                    "authorId": "144901955",
                                    "name": "Carolyn Jane Anderson"
                                }
                            ],
                            "year": 2023,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 11
                        },
                        "score": 0.826171875
                    },
                    {
                        "id": "(Nemani et al., 2023)",
                        "snippets": [
                            "Gender bias in artificial intelligence (AI) has emerged as a pressing concern with profound implications for individuals' lives. Researchers have identified three primary categories of bias: Denigration (A1), Stereotyping (A2), and Under-representation (A3). Denigration involves the usage of racial, ethnic, or religious slurs, which can often be observed as a prevalent method of cyberbullying. This type of bias manifests as derogatory language aimed at demeaning specific groups. Stereotyping, on the other hand, refers to individuals' cognitive representation of a particular social group. In NLP tasks, stereotyping can manifest through word embeddings and large-scale language models, where certain biases are learned and perpetuated in the resulting representations. Under-representation pertains to the absence of identifiable group members from representative bodies and well-being indicators in proportion to their population's size.\n\nStudies have demonstrated that word embeddings can detect racial and gender biases, even in well-labeled and balanced datasets. These biases can arise due to societal prejudices reflected in the training data, leading to biased predictions and outputs from NLP models. Contextual representations learned by large pre-trained language models, such as BERT and GPT, are also susceptible to biases. These models are typically trained on vast amounts of internet text, including societal biases in online content. Consequently, these models can replicate and perpetuate biases, often mirroring societal biases.\n\nFurthermore, machine learning models tend to amplify the behaviors and patterns they are exposed to, including biases present in the training data. Studies such as [30], [31] have explored the compounding effect of bias in newer models, highlighting the phenomenon known as bias overamplification. This refers to the tendency of machine learning models to disproportionately amplify and reinforce biases rather than mitigate them."
                        ],
                        "paper": {
                            "corpus_id": 259203321,
                            "title": "Gender Bias in Transformer Models: A comprehensive survey",
                            "authors": [
                                {
                                    "authorId": "2156841973",
                                    "name": "Praneeth Nemani"
                                },
                                {
                                    "authorId": "2220288860",
                                    "name": "Yericherla Deepak Joel"
                                },
                                {
                                    "authorId": "41153415",
                                    "name": "Pallavi Vijay"
                                },
                                {
                                    "authorId": "3445542",
                                    "name": "Farhana Ferdousi Liza"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.83984375
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Origins of Bias in LLMs",
                "tldr": "Social bias in Large Language Models stems primarily from unbalanced training data that reflects historical societal prejudices and stereotypes. These biases originate from multiple sources including historical text corpora, internet content with skewed representations, and technical choices in model development. (20 sources)",
                "text": "\n## Training Data as the Primary Source of Bias\n\nThe most significant source of bias in Large Language Models (LLMs) lies in the training data itself. LLMs are trained on vast corpora of human-produced text that inherently contain societal prejudices, stereotypes, and historical biases <Paper corpusId=\"248006238\" paperTitle=\"(Camara et al., 2022)\" isShortName></Paper> <Paper corpusId=\"23163324\" paperTitle=\"(Caliskan et al., 2016)\" isShortName></Paper>. These biases get encoded into word embeddings and other representations within the models, creating what researchers call \"recoverable and accurate imprints of our historic biases\" <Paper corpusId=\"249431834\" paperTitle=\"(Caliskan et al., 2022)\" isShortName></Paper>. The statistical patterns in language that LLMs learn during training inadvertently capture and sometimes amplify societal biases present in the data <Paper corpusId=\"248366292\" paperTitle=\"(Kadan et al., 2022)\" isShortName></Paper>.\n\nResearch has demonstrated that when datasets reflect the thoughts, feelings, and actions of a community, AI models inevitably incorporate the associations and representations of that community <Paper corpusId=\"249431834\" paperTitle=\"(Caliskan et al., 2022)\" isShortName></Paper>. For example, a seminal study by Caliskan et al. showed that pre-trained word embeddings replicated ten major biases widely found among human participants, including biases associating European American names with positive concepts and African American names with negative concepts <Paper corpusId=\"23163324\" paperTitle=\"(Caliskan et al., 2016)\" isShortName></Paper>.\n\n## Unbalanced Data Representation\n\nThe training data for LLMs often features unbalanced representations of different demographic groups. Internet content, which forms a substantial portion of training data, is not created equally by all segments of society <Paper corpusId=\"271161894\" paperTitle=\"(Grieve et al., 2024)\" isShortName></Paper>. Internet access is unevenly distributed globally, leading to an overrepresentation of hegemonic views while underrepresenting marginalized perspectives <Paper corpusId=\"262580630\" paperTitle=\"(Bender et al., 2021)\" isShortName></Paper>. Additionally, content filtering processes may disproportionately remove language used by marginalized groups, further skewing the training data <Paper corpusId=\"259075978\" paperTitle=\"(Wald et al., 2023)\" isShortName></Paper>.\n\nThis selection bias creates significant disparities in how different demographic groups are represented in the training data. For instance, researchers have noted that datasets used for facial recognition contain predominantly light-skinned individuals, which can bias analyses toward underrepresented groups <Paper corpusId=\"268856702\" paperTitle=\"(Chu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"201666566\" paperTitle=\"(Mehrabi et al., 2019)\" isShortName></Paper>.\n\n## Types of Bias Sources in LLMs\n\nResearchers have identified several specific sources of bias in LLMs:\n\n1. **Dataset Bias**: This stems from the unbalanced and skewed nature of training data, which often contains explicit discriminatory language or implicit stereotypes <Paper corpusId=\"260438662\" paperTitle=\"(Razaa et al., 2023)\" isShortName></Paper>. If datasets contain biases related to race, gender, religion, or socioeconomic status, the models inherit and sometimes amplify these biases <Paper corpusId=\"268157336\" paperTitle=\"(Patil et al., 2024)\" isShortName></Paper>.\n\n2. **Embedding Bias**: Word embeddings and other semantic representations capture relationships between words that reflect stereotypical associations, such as linking certain professions with specific genders <Paper corpusId=\"268856702\" paperTitle=\"(Chu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"1704893\" paperTitle=\"(Bolukbasi et al., 2016)\" isShortName></Paper>. These biased embeddings serve as a fundamental component in LLMs and can inadvertently introduce semantic bias into downstream applications <Paper corpusId=\"268856702\" paperTitle=\"(Chu et al., 2024)\" isShortName></Paper>.\n\n3. **Label Bias**: In the context of supervised fine-tuning or instruction tuning, biases can be introduced through the subjective judgments of human annotators who provide labels or annotations <Paper corpusId=\"268856702\" paperTitle=\"(Chu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"196211238\" paperTitle=\"(Sap et al., 2019)\" isShortName></Paper>. When annotators inject their personal beliefs or stereotypes into the labeling process, they inadvertently introduce bias into the model.\n\n4. **Contextual Bias**: This arises from the context in which language models are used, which can influence the way biases manifest <Paper corpusId=\"268157336\" paperTitle=\"(Patil et al., 2024)\" isShortName></Paper>. The context can significantly impact applications such as recommender systems, hiring processes, and sentiment analysis <Paper corpusId=\"258426630\" paperTitle=\"(Fraiwan et al., 2023)\" isShortName></Paper>.\n\n5. **User Interaction Bias**: The responses generated by language models are influenced by the input they receive from users. If users consistently ask biased or prejudiced questions, the model may learn and replicate these biases in its responses <Paper corpusId=\"258426630\" paperTitle=\"(Fraiwan et al., 2023)\" isShortName></Paper>.\n\n## Amplification of Bias\n\nAn alarming aspect of bias in LLMs is that they don't merely reflect the biases present in their training data but can sometimes amplify them <Paper corpusId=\"259075978\" paperTitle=\"(Wald et al., 2023)\" isShortName></Paper>. Recent research suggests that LLMs can exaggerate existing stereotypes beyond what is reflected in real-world statistics or human perceptions <Paper corpusId=\"261276445\" paperTitle=\"(Kotek et al., 2023)\" isShortName></Paper>. For example, a study found that LLMs are 3-6 times more likely to choose an occupation that stereotypically aligns with a person's gender, amplifying the bias beyond what is reflected in actual job statistics <Paper corpusId=\"261276445\" paperTitle=\"(Kotek et al., 2023)\" isShortName></Paper>.\n\n## Intrinsic vs. Extrinsic Bias\n\nResearchers distinguish between intrinsic and extrinsic bias in LLMs. Intrinsic bias refers to biases inherently within the internal representations or outputs of a trained or pre-trained LLM, independent of any specific downstream tasks <Paper corpusId=\"274130807\" paperTitle=\"(Guo et al., 2024)\" isShortName></Paper>. These biases are typically encoded during the training phase when the LLM learns from large-scale corpora. Extrinsic bias, on the other hand, refers to biases that manifest during the model's performance on specific downstream tasks after training or fine-tuning <Paper corpusId=\"274130807\" paperTitle=\"(Guo et al., 2024)\" isShortName></Paper>.\n\n## Real-World Consequences\n\nThe biases present in LLMs have led to documented real-world harms. For instance, Amazon discontinued an AI-driven recruiting tool after discovering it systematically downgraded resumes containing the term \"women's,\" reflecting an inherent bias in the training data that favored male candidates <Paper corpusId=\"272826949\" paperTitle=\"(Mirza et al., 2024)\" isShortName></Paper> <Paper corpusId=\"261276445\" paperTitle=\"(Kotek et al., 2023)\" isShortName></Paper>. More recently, Google suspended Gemini's image generation feature following reports of inaccuracies and potential biases <Paper corpusId=\"272826949\" paperTitle=\"(Mirza et al., 2024)\" isShortName></Paper>.\n\nThese biases can lead to both allocative and representational harms <Paper corpusId=\"274117151\" paperTitle=\"(Doll et al., 2024)\" isShortName></Paper>. Allocative harms occur when resources or opportunities are distributed unfairly, often to the detriment of already marginalized groups, such as in recruitment algorithms. Representational harm occurs when groups are misrepresented or stereotyped, reinforcing harmful narratives and further entrenching societal biases <Paper corpusId=\"274117151\" paperTitle=\"(Doll et al., 2024)\" isShortName></Paper> <Paper corpusId=\"235436386\" paperTitle=\"(Suresh et al., 2019)\" isShortName></Paper>.\n\nThe problem of bias in LLMs continues to be a significant challenge, requiring comprehensive approaches to detection and mitigation to prevent the perpetuation and amplification of societal prejudices and stereotypes <Paper corpusId=\"274437724\" paperTitle=\"(Meijer et al., 2024)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Camara et al., 2022)",
                        "snippets": [
                            "The presence and impact of harmful social biases in machine learning and natural language processing systems is pervasive and well-documented in popular word embedding methods (Caliskan et al., 2016)(Garg et al., 2017)(Bolukbasi et al., 2016) due to large amounts of humanproduced training data that includes historical social biases. Notably, (Caliskan et al., 2016) demonstrate such biases by introducing the Word Embedding Association Test (WEAT) which measures how similar socially sensitive sets of words (e.g., racial or gendered names) are to attributive sets of words (e.g., pleasant or unpleasant words) in the semantic space encoded by word embeddings. While (Bolukbasi et al., 2016); Manzini et al. (2019) introduce methods for \"debiasing\" word embeddings in order to create more equitable semantic representations for usage in downstream tasks, Gonen and Goldberg (2019) argue that such methods are merely cosmetic since social biases are still evident in the semantic space after the application of such methods. Moreover, these \"debiasing\" techniques focus on a particular social cleavage such as gender or race (i.e., unisectional cleavages). In contrast, our work considers both unisectional and intersectional social biases.\n\nRecent studies have also begun to focus on social biases in transformer-based language models (Kurita et al., 2019)(Bender et al., 2021). In particular, (Bender et al., 2021) discusses how increasingly large transformer-based language model in practice regurgitate their training data, resulting in such models perpetuating social biases and harming users."
                        ],
                        "paper": {
                            "corpus_id": 248006238,
                            "title": "Mapping the Multilingual Margins: Intersectional Biases of Sentiment Analysis Systems in English, Spanish, and Arabic",
                            "authors": [
                                {
                                    "authorId": "2077593669",
                                    "name": "Antonio Camara"
                                },
                                {
                                    "authorId": "70655120",
                                    "name": "Nina Taneja"
                                },
                                {
                                    "authorId": "1571776558",
                                    "name": "Tamjeed Azad"
                                },
                                {
                                    "authorId": "46208659",
                                    "name": "Emily Allaway"
                                },
                                {
                                    "authorId": "1804104",
                                    "name": "R. Zemel"
                                }
                            ],
                            "year": 2022,
                            "venue": "LTEDI",
                            "n_citations": 21
                        },
                        "score": 0.87255859375
                    },
                    {
                        "id": "(Caliskan et al., 2016)",
                        "snippets": [
                            "Machines learn what people know implicitly AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan et al. now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs\u2014for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior. Science, this issue p. 183; see also p. 133 Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias. Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology."
                        ],
                        "paper": {
                            "corpus_id": 23163324,
                            "title": "Semantics derived automatically from language corpora contain human-like biases",
                            "authors": [
                                {
                                    "authorId": "144537437",
                                    "name": "Aylin Caliskan"
                                },
                                {
                                    "authorId": "145315445",
                                    "name": "J. Bryson"
                                },
                                {
                                    "authorId": "47735253",
                                    "name": "Arvind Narayanan"
                                }
                            ],
                            "year": 2016,
                            "venue": "Science",
                            "n_citations": 2673
                        },
                        "score": 0
                    },
                    {
                        "id": "(Caliskan et al., 2022)",
                        "snippets": [
                            "Word embeddings are numeric representations of meaning derived from word co-occurrence statistics in corpora of human-produced texts. The statistical regularities in language corpora encode well-known social biases into word embeddings (e.g., the word vector for family is closer to the vector women than to men)",
                            "When datasets that reflect the thoughts, feelings, and actions of a community are used to train artificial intelligence (AI) models, the models inevitably incorporate the associations and representations of the community [11-13, 15, 17, 26, 40, 51, 54].\n\nIn one of the first studies systematically assessing social biases in word embeddings, Caliskan et al. [12] showed that pretrained GloVe embeddings replicated ten major biases widely found among human participants including biases associating young-good/elderly-bad, European American-good/African American-bad, and women-family/men-career [28]. Such group-based biases can manifest in NLP applications (a recent example was seen in Amazon's automated resume screening algorithm that preferentially hired men over women [7])."
                        ],
                        "paper": {
                            "corpus_id": 249431834,
                            "title": "Gender Bias in Word Embeddings: A Comprehensive Analysis of Frequency, Syntax, and Semantics",
                            "authors": [
                                {
                                    "authorId": "144537437",
                                    "name": "Aylin Caliskan"
                                },
                                {
                                    "authorId": "2168463879",
                                    "name": "Pimparkar Parth Ajay"
                                },
                                {
                                    "authorId": "66649568",
                                    "name": "Tessa E. S. Charlesworth"
                                },
                                {
                                    "authorId": "100508901",
                                    "name": "R. Wolfe"
                                },
                                {
                                    "authorId": "1968771",
                                    "name": "M. Banaji"
                                }
                            ],
                            "year": 2022,
                            "venue": "AAAI/ACM Conference on AI, Ethics, and Society",
                            "n_citations": 51
                        },
                        "score": 0.89306640625
                    },
                    {
                        "id": "(Kadan et al., 2022)",
                        "snippets": [
                            "The remarkable progress in Natural Language Processing (NLP) brought about by deep learning, particularly with the recent advent of large pre-trained neural language models, is brought into scrutiny as several studies began to discuss and report potential biases in NLP applications. Bias in NLP is found to originate from latent historical biases encoded by humans into textual data which gets perpetuated or even amplified by NLP algorithm."
                        ],
                        "paper": {
                            "corpus_id": 248366292,
                            "title": "Towards an Enhanced Understanding of Bias in Pre-trained Neural Language Models: A Survey with Special Emphasis on Affective Bias",
                            "authors": [
                                {
                                    "authorId": "144485480",
                                    "name": "Anoop Kadan"
                                },
                                {
                                    "authorId": "3390280",
                                    "name": "Manjary P.Gangan"
                                },
                                {
                                    "authorId": "145791688",
                                    "name": "Deepak P"
                                },
                                {
                                    "authorId": "151092338",
                                    "name": "L. LajishV."
                                }
                            ],
                            "year": 2022,
                            "venue": "arXiv.org",
                            "n_citations": 10
                        },
                        "score": 0.88671875
                    },
                    {
                        "id": "(Grieve et al., 2024)",
                        "snippets": [
                            "NLP systems generally suffer from social bias: their real-world application leads to outcomes that unfairly disadvantage or harm specific social groups [58,59,60,61]. Social bias can be introduced at various points during the development and deployment of NLP systems [62], but given the unsupervised nature of language modeling, training corpora are a key source of social bias in LLMs [10,63]. While bias in NLP systems can harm people in various ways [59], in this section, we primarily focus on two common harmful outcomes of social bias. These two types of harms are most commonly discussed in terms of quality-of-service harms and stereotyping harms (e.g., [64,65,60,66]), although many different systems have been proposed for classifying biases and harms in NLP, which define these terms in somewhat different ways, along with many additional and often overlapping categories [59]. Both of these types of harms are especially relevant to LLMs, and crucially, we believe both can be better understood and addressed in language modeling by adopting a sociolinguistic perspective (see Figure 3). First, social bias can be characterized by poor system performance for certain social groups that are interacting with LLMs and applications based on language models: token prediction will be more or less accurate depending on the social origins of the language inputted into the system. For example, ChatGPT might have difficulty correctly understanding prompts written by people from certain social groups due to their use of non-standard or socially restricted language patterns. This type of bias leads to what is known as quality-of-service harms, where the performance of these systems varies depending on the social background of the user [64,60]. These types of quality-of-service harms can often be the product of selection bias, as they result from how training data is selected from across the society whose language is being modeled [58]: in general, if language data from certain social groups is under-represented in the training data for a language model, we should expect that NLP applications based on that model will process language structures produced by these groups less accurately and consequently exhibit poorer performance for these groups [59]67]."
                        ],
                        "paper": {
                            "corpus_id": 271161894,
                            "title": "The sociolinguistic foundations of language modeling",
                            "authors": [
                                {
                                    "authorId": "2280139407",
                                    "name": "Jack Grieve"
                                },
                                {
                                    "authorId": "2311115214",
                                    "name": "Sara Bartl"
                                },
                                {
                                    "authorId": "2268357014",
                                    "name": "Matteo Fuoli"
                                },
                                {
                                    "authorId": "69370030",
                                    "name": "Jason Grafmiller"
                                },
                                {
                                    "authorId": "2280176696",
                                    "name": "Weihang Huang"
                                },
                                {
                                    "authorId": "2304488914",
                                    "name": "A. Jawerbaum"
                                },
                                {
                                    "authorId": "2280137448",
                                    "name": "Akira Murakami"
                                },
                                {
                                    "authorId": "2311114471",
                                    "name": "Marcus Perlman"
                                },
                                {
                                    "authorId": "2298903710",
                                    "name": "Dana Roemling"
                                },
                                {
                                    "authorId": "2320760943",
                                    "name": "Bodo Winter"
                                }
                            ],
                            "year": 2024,
                            "venue": "Frontiers Artif. Intell.",
                            "n_citations": 10
                        },
                        "score": 0.85107421875
                    },
                    {
                        "id": "(Bender et al., 2021)",
                        "snippets": [
                            "The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models."
                        ],
                        "paper": {
                            "corpus_id": 262580630,
                            "title": "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? \ud83e\udd9c",
                            "authors": [
                                {
                                    "authorId": "2471699",
                                    "name": "Emily M. Bender"
                                },
                                {
                                    "authorId": "2076288",
                                    "name": "Timnit Gebru"
                                },
                                {
                                    "authorId": "1584940075",
                                    "name": "Angelina McMillan-Major"
                                },
                                {
                                    "authorId": "2246048745",
                                    "name": "Shmargaret Shmitchell"
                                }
                            ],
                            "year": 2021,
                            "venue": "Conference on Fairness, Accountability and Transparency",
                            "n_citations": 4657
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wald et al., 2023)",
                        "snippets": [
                            "Research has shown that language models are prone to exhibit societal biases, reflecting the nature of the underlying training data written by humans. For this work, we focus on representational bias, which in the context of language models, occurs when there is an imbalance in the portrayal of different groups in LM-generated text (Blodgett et al. 2020;Liang et al. 2021). This can arise from stereotypes that cultivate negative generalisations about particular social groups and language that is used to degrade these groups. Ultimately, this may lead to certain groups being depicted less favourably than others. Bender et al. argue that even in large models with large datasets, diversity and unbiasedness are not guaranteed (Bender et al. 2021). Datasets are too big to be thoroughly documented, while marginalised social groups are more likely to be filtered out due to the nature of the filtering process. Phrases or dialects used by these groups are disproportionally categorised as hateful, resulting in their exclusion from training datasets (Sap et al. 2019;Bender et al. 2021). Hegemonic views are over-represented as Internet access is not evenly distributed, and datasets are often curated using male-dominated websites. Large language generation models such as GPT-2 (Radford et al. 2019) and GPT-3 (Brown et al. 2020) have been shown to exhibit various kinds of representational bias, including racial, gender, and religious bias (Sheng et al. 2019;Abid, Farooqi, and Zou 2021;Sheng et al. 2021). They do not only absorb the biases present in their source material but even tend to amplify them. Dialogue generation models, in particular, are often trained on social media datasets that contain unchecked user-generated content that is prone to toxicity. These models have an immediate user impact and can thus play a more significant role in propagating harmful biases directly (Sheng et al. 2021)."
                        ],
                        "paper": {
                            "corpus_id": 259075978,
                            "title": "Exposing Bias in Online Communities through Large-Scale Language Models",
                            "authors": [
                                {
                                    "authorId": "2219550842",
                                    "name": "Celine Wald"
                                },
                                {
                                    "authorId": "32421394",
                                    "name": "Lukas Pfahler"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 6
                        },
                        "score": 0.8486328125
                    },
                    {
                        "id": "(Chu et al., 2024)",
                        "snippets": [
                            "Language modeling bias, often defined as \"bias that results in harm to various social groups\" [70], presents itself in various forms, encompassing the association of specific stereotypes with groups, the devaluation of certain groups, the underrepresentation of particular social groups, and the unequal allocation of resources among groups [44].Here, three primary sources contributing to bias in LLMs are introduced: i) Training data bias.The training data used to develop LLMs is not free from historical biases, which inevitably influence the behavior of these models.For instance, if the training data includes the statement \"all programmers are male and all nurses are female, \" the model is likely to learn and perpetuate these occupational and gender biases in its outputs, reflecting a narrow and biased view of societal roles (Bolukbasi et al., 2016)(Caliskan et al., 2016).Additionally, a significant disparity in the training data could also lead to biased outcomes [161].For example, Buolamwini and Gebru (Buolamwini et al., 2018) highlighted significant disparities in datasets like IJB-A and Adience, where predominantly light-skinned individuals make up 79.6% and 86.2% of the data, respectively, thereby biasing analyses toward underrepresented dark-skinned groups (Mehrabi et al., 2019).\n\nii) Embedding bias.Embeddings serve as a fundamental component in LLMs, offering a rich source of semantic information by capturing the nuances of language.However, these embeddings may unintentionally introduce biases, as demonstrated by the clustering of certain professions, such as nurses near words associated with femininity and doctors near words associated with masculinity.This phenomenon inadvertently introduces semantic bias into downstream models, impacting their performance and fairness [9]63].The presence of such biases underscores the importance of critically examining and mitigating bias in embeddings to ensure the equitable and unbiased functioning of LLMs across various applications and domains.\n\niii) Label bias.In instruction tuning scenarios, biases can arise from the subjective judgments of human annotators who provide labels or annotations for training data (Sap et al., 2019).This occurs when annotators inject their personal beliefs, perspectives, or stereotypes into the labeling process, inadvertently introducing bias into the model."
                        ],
                        "paper": {
                            "corpus_id": 268856702,
                            "title": "Fairness in Large Language Models: A Taxonomic Survey",
                            "authors": [
                                {
                                    "authorId": "2283849596",
                                    "name": "Zhibo Chu"
                                },
                                {
                                    "authorId": "2206777360",
                                    "name": "Zichong Wang"
                                },
                                {
                                    "authorId": "2243674300",
                                    "name": "Wenbin Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "SIGKDD Explorations",
                            "n_citations": 41
                        },
                        "score": 0.81201171875
                    },
                    {
                        "id": "(Mehrabi et al., 2019)",
                        "snippets": [
                            "With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields."
                        ],
                        "paper": {
                            "corpus_id": 201666566,
                            "title": "A Survey on Bias and Fairness in Machine Learning",
                            "authors": [
                                {
                                    "authorId": "51997673",
                                    "name": "Ninareh Mehrabi"
                                },
                                {
                                    "authorId": "2775559",
                                    "name": "Fred Morstatter"
                                },
                                {
                                    "authorId": "51884035",
                                    "name": "N. Saxena"
                                },
                                {
                                    "authorId": "1782658",
                                    "name": "Kristina Lerman"
                                },
                                {
                                    "authorId": "143728483",
                                    "name": "A. Galstyan"
                                }
                            ],
                            "year": 2019,
                            "venue": "ACM Computing Surveys",
                            "n_citations": 4373
                        },
                        "score": 0
                    },
                    {
                        "id": "(Razaa et al., 2023)",
                        "snippets": [
                            "The recent surge in Natural Language Processing (NLP) applications, encompassing fields from recommendation systems to social justice and employment screening, has sparked a critical concern -the emergence of bias within these systems [1]. Instances of racial and gender bias have been increasingly reported [2], indicating an urgent need for scrutiny. These biases often originate from the training data used in NLP models, and a majority of these large datasets harbor inherent biases.\n\nBias in text data is a pervasive and deeply-rooted issue. The bias in data often stems from cognitive predispositions that influences our dialogues, views, and understanding of information [3]. This bias can be explicit which are often seen in discriminatory language targeting certain racial or ethnic groups [4], as in social media. Implicit bias [5], on the other hand, subtly perpetuates prejudice through unintentional language use but is equally harmful.\n\nThe necessity for unbiased, trustworthy text data has grown across sectors like healthcare [6], social media [4,7], and recruitment [8]. This data is essential for training NLP models for various downstream tasks, like formulating healthcare diagnoses and treatment plans, handling discriminatory language on social media, and promoting fair recruitment practices. Figure 1 illustrates the complexities of biases in text data in various domains, including job hiring, social media, and healthcare. These biases are primarily conveyed through lexical choices [9] and demand sophisticated detection methods, motivating this research."
                        ],
                        "paper": {
                            "corpus_id": 260438662,
                            "title": "NBIAS: A Natural Language Processing Framework for Bias Identification in Text",
                            "authors": [
                                {
                                    "authorId": "2226784526",
                                    "name": "Shaina Razaa"
                                },
                                {
                                    "authorId": "3410824",
                                    "name": "Muskan Garg"
                                },
                                {
                                    "authorId": "2155769046",
                                    "name": "Deepak John Reji"
                                },
                                {
                                    "authorId": "98911272",
                                    "name": "S. Bashir"
                                },
                                {
                                    "authorId": "2053158730",
                                    "name": "Chen Ding"
                                }
                            ],
                            "year": 2023,
                            "venue": "Expert systems with applications",
                            "n_citations": 48
                        },
                        "score": 0.83154296875
                    },
                    {
                        "id": "(Patil et al., 2024)",
                        "snippets": [
                            "Bias and fairness, if not adequately addressed, pose serious societal implications in the form of biased language generation and its impact on some segments of society. Basis can creep into LLMs from several sources discussed below. The first source of bias, dataset bias, stems from the datasets that were used to train the LLMs. If the datasets contain biases related to race, gender, religion, or socioeconomic status, the models inherit and amplify them.\n\nUnderrepresentation or misrepresentation of certain groups in the training data can lead to representation bias and biased language generation. The LLM developers should have checks and balances to ensure that all perspectives are adequately represented in the datasets. Otherwise, the model will produce inaccurate or skewed output for underrepresented groups. If the training data contain stereotypes, models amplify stereotyping and perpetuate prejudices. Fairness across demographics is a complex challenge but essential for advancing LLMs.\n\nCentextual bias stems from the context in which the language models are used. This poses severe and negative implications in applications such as recommender systems, employee hiring and promotions, clustering, and sentiment analysis."
                        ],
                        "paper": {
                            "corpus_id": 268157336,
                            "title": "A Review of Current Trends, Techniques, and Challenges in Large Language Models (LLMs)",
                            "authors": [
                                {
                                    "authorId": "2289385425",
                                    "name": "Rajvardhan Patil"
                                },
                                {
                                    "authorId": "117730513",
                                    "name": "Venkat Gudivada"
                                }
                            ],
                            "year": 2024,
                            "venue": "Applied Sciences",
                            "n_citations": 80
                        },
                        "score": 0.88720703125
                    },
                    {
                        "id": "(Bolukbasi et al., 2016)",
                        "snippets": [
                            "The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias."
                        ],
                        "paper": {
                            "corpus_id": 1704893,
                            "title": "Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings",
                            "authors": [
                                {
                                    "authorId": "2843215",
                                    "name": "Tolga Bolukbasi"
                                },
                                {
                                    "authorId": "2782886",
                                    "name": "Kai-Wei Chang"
                                },
                                {
                                    "authorId": "145085305",
                                    "name": "James Y. Zou"
                                },
                                {
                                    "authorId": "1699322",
                                    "name": "Venkatesh Saligrama"
                                },
                                {
                                    "authorId": "2186481",
                                    "name": "A. Kalai"
                                }
                            ],
                            "year": 2016,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 3153
                        },
                        "score": 0
                    },
                    {
                        "id": "(Sap et al., 2019)",
                        "snippets": [
                            "We investigate how annotators\u2019 insensitivity to differences in dialect can lead to racial bias in automatic hate speech detection models, potentially amplifying harm against minority populations. We first uncover unexpected correlations between surface markers of African American English (AAE) and ratings of toxicity in several widely-used hate speech datasets. Then, we show that models trained on these corpora acquire and propagate these biases, such that AAE tweets and tweets by self-identified African Americans are up to two times more likely to be labelled as offensive compared to others. Finally, we propose *dialect* and *race priming* as ways to reduce the racial bias in annotation, showing that when annotators are made explicitly aware of an AAE tweet\u2019s dialect they are significantly less likely to label the tweet as offensive."
                        ],
                        "paper": {
                            "corpus_id": 196211238,
                            "title": "The Risk of Racial Bias in Hate Speech Detection",
                            "authors": [
                                {
                                    "authorId": "2729164",
                                    "name": "Maarten Sap"
                                },
                                {
                                    "authorId": "35540755",
                                    "name": "Dallas Card"
                                },
                                {
                                    "authorId": "119902504",
                                    "name": "Saadia Gabriel"
                                },
                                {
                                    "authorId": "1699545",
                                    "name": "Yejin Choi"
                                },
                                {
                                    "authorId": "144365875",
                                    "name": "Noah A. Smith"
                                }
                            ],
                            "year": 2019,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 848
                        },
                        "score": 0
                    },
                    {
                        "id": "(Fraiwan et al., 2023)",
                        "snippets": [
                            "Language models can exhibit bias if the training data used to create them is biased. As Schramowski et al. (Schramowski et al., 2021) pointed out, large pre-trained models that try to mimic natural languages, may end up repeating the same unfairness and prejudices. This can lead to discriminatory or inaccurate analyses and recommendations. Moreover, this may lead to public outcry (i.e., political, social, and legal) against the commercial applications. These biases manifests themselves in several ways, as follows: \n\n\u2022 Training data bias: Language models are typically trained on large datasets of human language. If these datasets are biased in some way (e.g., based on race, gender, socioeconomic status, etc.), then the model may learn and replicate these biases in its responses. For example, if the training data is biased towards a particular gender, then the model may generate responses that are more favorable towards that gender. \n\n\u2022 User interaction bias: The responses generated by Chatbots are based on the input they receive from users. If users consistently ask biased or prejudiced questions, then the model may learn and replicate these biases in its responses. For example, if users frequently ask questions that are discriminatory towards a particular group, then the model may generate responses that perpetuate these biases. \n\n\u2022 Algorithmic bias: The algorithms used to train and operate language models and Chatbots may also introduce biases. For example, if the model is trained to optimize for a particular metric (e.g., accuracy, engagement, etc.), then it may prioritize generating responses that optimize for that metric, even if those responses are biased in some way. \n\n\u2022 Contextual bias: Chatbots generate responses based on the context they receive from users. If the context is biased in some way (e.g., based on the user's location, language, etc.), then the model may generate biased responses. For example, if a user is asking questions about a particular culture or religion, and the model is not trained on that culture or religion, it may generate biased responses due to its lack of knowledge."
                        ],
                        "paper": {
                            "corpus_id": 258426630,
                            "title": "A Review of ChatGPT Applications in Education, Marketing, Software Engineering, and Healthcare: Benefits, Drawbacks, and Research Directions",
                            "authors": [
                                {
                                    "authorId": "145874705",
                                    "name": "Mohammad Fraiwan"
                                },
                                {
                                    "authorId": "47343240",
                                    "name": "Natheer Khasawneh"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 48
                        },
                        "score": 0.83544921875
                    },
                    {
                        "id": "(Kotek et al., 2023)",
                        "snippets": [
                            "Large Language Models (LLMs) have made substantial progress in the past several months, shattering state-of-the-art benchmarks in many domains. This paper investigates LLMs\u2019 behavior with respect to gender stereotypes, a known issue for prior models. We use a simple paradigm to test the presence of gender bias, building on but differing from WinoBias, a commonly used gender bias dataset, which is likely to be included in the training data of current LLMs. We test four recently published LLMs and demonstrate that they express biased assumptions about men and women\u2019s occupations. Our contributions in this paper are as follows: (a) LLMs are 3-6 times more likely to choose an occupation that stereotypically aligns with a person\u2019s gender; (b) these choices align with people\u2019s perceptions better than with the ground truth as reflected in official job statistics; (c) LLMs in fact amplify the bias beyond what is reflected in perceptions or the ground truth; (d) LLMs ignore crucial ambiguities in sentence structure 95% of the time in our study items, but when explicitly prompted, they recognize the ambiguity; (e) LLMs provide explanations for their choices that are factually inaccurate and likely obscure the true reason behind their predictions. That is, they provide rationalizations of their biased behavior. This highlights a key property of these models: LLMs are trained on imbalanced datasets; as such, even with the recent successes of reinforcement learning with human feedback, they tend to reflect those imbalances back at us. As with other types of societal biases, we suggest that LLMs must be carefully tested to ensure that they treat minoritized individuals and communities equitably."
                        ],
                        "paper": {
                            "corpus_id": 261276445,
                            "title": "Gender bias and stereotypes in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "3365389",
                                    "name": "Hadas Kotek"
                                },
                                {
                                    "authorId": "90166394",
                                    "name": "Rikker Dockum"
                                },
                                {
                                    "authorId": "32100412",
                                    "name": "David Q. Sun"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Climate Informatics",
                            "n_citations": 236
                        },
                        "score": 0
                    },
                    {
                        "id": "(Guo et al., 2024)",
                        "snippets": [
                            "The bias presented in Large Language Models (LLMs) can be broadly categorized into intrinsic bias and extrinsic bias based on the different stages at which the biases manifest within the model's lifecycle and the type of bias being measured (Doan et al., 2024). Intrinsic bias refers to biases that are inherently within the internal representations or outputs of a trained or pre-trained LLM and are independent of any specific downstream tasks. Extrinsic bias refers to the biases that manifest during the model's performance on specific downstream tasks after training or fine-turing. Intrinsic biases are typically encoded during the training or pre-training phase when the LLM learns from large-scale corpora. For instance, if a pre-trained LLM consistently associates certain professions with specific genders (e.g., assuming all doctors to be male and assuming all nurses to be female) or racial stereotypes (May et al., 2019), this bias would be considered intrinsic bias. This type of bias is tied to the way the model has encoded relationships and patterns in its internal representations.\n\nIntrinsic biases are primarily introduced during the training/pre-training stages where models learn patterns and representations from vast corpora. These biases can be traced to several aspects of data and model design, which will be outlined below."
                        ],
                        "paper": {
                            "corpus_id": 274130807,
                            "title": "Bias in Large Language Models: Origin, Evaluation, and Mitigation",
                            "authors": [
                                {
                                    "authorId": "2331613833",
                                    "name": "Yufei Guo"
                                },
                                {
                                    "authorId": "2331371661",
                                    "name": "Muzhe Guo"
                                },
                                {
                                    "authorId": "2273060735",
                                    "name": "Juntao Su"
                                },
                                {
                                    "authorId": "2331371753",
                                    "name": "Zhou Yang"
                                },
                                {
                                    "authorId": "2331357933",
                                    "name": "Mengqiu Zhu"
                                },
                                {
                                    "authorId": "2331365682",
                                    "name": "Hongfei Li"
                                },
                                {
                                    "authorId": "1455277182",
                                    "name": "Mengyang Qiu"
                                },
                                {
                                    "authorId": "2331377869",
                                    "name": "Shuo Shuo Liu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 22
                        },
                        "score": 0.9150390625
                    },
                    {
                        "id": "(Mirza et al., 2024)",
                        "snippets": [
                            "Bias in LLMs manifests in multiple forms, including racial, gender, and cultural stereotypes, often perpetuating systemic inequalities. These biases have tangible consequences; for instance, in 2018, Amazon discontinued an AI-driven recruiting tool after discovering it systematically downgraded resumes containing the term \"women's,\" reflecting an inherent bias in the training data that favored male candidates [1]. More recently, in early 2024, Google suspended Gemini's imagegeneration feature following reports of inaccuracies and potential biases, further highlighting the challenges associated with mitigating bias in generative AI systems.\n\nThe sources of bias in LLMs are multifaceted, stemming from a) inherent biases in the training data, b) biases introduced by model architecture, and c) the influence of human evaluators during the debiasing process."
                        ],
                        "paper": {
                            "corpus_id": 272826949,
                            "title": "Evaluating Gender, Racial, and Age Biases in Large Language Models: A Comparative Analysis of Occupational and Crime Scenarios",
                            "authors": [
                                {
                                    "authorId": "2322445184",
                                    "name": "Vishal Mirza"
                                },
                                {
                                    "authorId": "2322445481",
                                    "name": "Rahul Kulkarni"
                                },
                                {
                                    "authorId": "2322445728",
                                    "name": "Aakanksha Jadhav"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.82080078125
                    },
                    {
                        "id": "(Doll et al., 2024)",
                        "snippets": [
                            "LLMs learn from vast amounts of text collected from the Internet and other sources. This training data often contains unbalanced representations and stereotypes that are unintentionally embedded in the models [3].\n\nBias in LLMs can harm both individuals and society, with consequences including allocative and representational harms. Allocative harms occur when resources or opportunities are distributed unfairly, often to the detriment of already marginalised groups. For example, recruitment algorithms may be skewed to disadvantage certain demographic groups. Representational harm occurs when groups are misrepresented or stereotyped, reinforcing harmful narratives and stigmas and further entrenching negative prejudices and societal biases [4]. Language models show a propensity for bias in a variety of areas, including but not limited to gender, age, sexual orientation, physical appearance, disability, nationality, ethnicity, socio-economic status, religion and culture. These biases have the potential to have significant consequences, particularly in contexts where they affect critical areas such as justice, healthcare or employment [5]."
                        ],
                        "paper": {
                            "corpus_id": 274117151,
                            "title": "Evaluating Gender Bias in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2338278194",
                                    "name": "Michael D\u00f6ll"
                                },
                                {
                                    "authorId": "2338277213",
                                    "name": "Markus D\u00f6hring"
                                },
                                {
                                    "authorId": "2338675374",
                                    "name": "Andreas M\u00fcller"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.8681640625
                    },
                    {
                        "id": "(Suresh et al., 2019)",
                        "snippets": [
                            "As machine learning (ML) increasingly affects people and society, awareness of its potential unwanted consequences has also grown. To anticipate, prevent, and mitigate undesirable downstream consequences, it is critical that we understand when and how harm might be introduced throughout the ML life cycle. In this paper, we provide a framework that identifies seven distinct potential sources of downstream harm in machine learning, spanning data collection, development, and deployment. In doing so, we aim to facilitate more productive and precise communication around these issues, as well as more direct, application-grounded ways to mitigate them."
                        ],
                        "paper": {
                            "corpus_id": 235436386,
                            "title": "A Framework for Understanding Sources of Harm throughout the Machine Learning Life Cycle",
                            "authors": [
                                {
                                    "authorId": "46537606",
                                    "name": "Harini Suresh"
                                },
                                {
                                    "authorId": "1724429",
                                    "name": "J. Guttag"
                                }
                            ],
                            "year": 2019,
                            "venue": "Conference on Equity and Access in Algorithms, Mechanisms, and Optimization",
                            "n_citations": 442
                        },
                        "score": 0
                    },
                    {
                        "id": "(Meijer et al., 2024)",
                        "snippets": [
                            "An important concern regarding LLMs is whether and in which areas these models may exhibit bias, such as gender, racial and cultural bias.\n\nLarge language models are sensitive to embedded bias due to the way they are trained, as they pick up the societal and cultural biases present in the training data. Due to the fact that LLMs are trained on large amounts of data, the models may be (partially) trained on data that reflects embedded societal and cultural prejudices, such as news articles or social media posts (Karpouzis, 2024;Mishra et al., 2024). Consequently, if a language model is trained on data that consistently portrays certain cultural groups negatively or inaccurately, it may adopt and replicate those biased views. Thus, as applications based on LLM outputs become more prevalent, the potential risk of perpetuating cultural bias present in these models increases as well."
                        ],
                        "paper": {
                            "corpus_id": 274437724,
                            "title": "LLMs as mirrors of societal moral standards: reflection of cultural divergence and agreement across ethical topics",
                            "authors": [
                                {
                                    "authorId": "2333364989",
                                    "name": "Mijntje Meijer"
                                },
                                {
                                    "authorId": "2261402979",
                                    "name": "Hadi Mohammadi"
                                },
                                {
                                    "authorId": "2261403495",
                                    "name": "Ayoub Bagheri"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.8125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Manifestations of Bias Across Applications",
                "tldr": "Social bias in LLMs manifests across numerous real-world applications including hiring, content moderation, healthcare, lending, speech recognition, and sentiment analysis, causing both allocational and representational harms to marginalized groups. These biases often disproportionately impact individuals at the intersection of multiple marginalized identities, creating systemic disadvantages in critical domains. (23 sources)",
                "text": "\n## Hiring and Recruitment\nBias in LLMs has particularly concerning implications in employment contexts. AI-driven hiring tools that utilize language models may exhibit unfair treatment toward applicants from underrepresented groups or those with non-traditional backgrounds <Paper corpusId=\"258041203\" paperTitle=\"(Ferrara, 2023)\" isShortName></Paper> <Paper corpusId=\"195316573\" paperTitle=\"(Raghavan et al., 2019)\" isShortName></Paper>. These biases can perpetuate existing inequalities in the job market by limiting opportunities for affected individuals and reducing workforce diversity. For example, Amazon discontinued an AI recruiting tool after discovering it systematically downgraded resumes containing the term \"women's,\" demonstrating the real-world consequences of gender bias in automated hiring systems <Paper corpusId=\"261049466\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper> <Paper corpusId=\"58006082\" paperTitle=\"(De-Arteaga et al., 2019)\" isShortName></Paper>. Automatic resume filtering systems have been found to exhibit gender bias, assigning programmer jobs to men and homemaker jobs to women at disproportionate rates <Paper corpusId=\"261049466\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper> <Paper corpusId=\"220040093\" paperTitle=\"(Deshpande et al., 2020)\" isShortName></Paper>.\n\n## Content Moderation\nSocial bias significantly impacts AI-powered content moderation systems used to manage and filter user-generated content on social media platforms and other online communities. If these systems are trained on biased data, they may disproportionately censor or suppress the voices of certain groups <Paper corpusId=\"258041203\" paperTitle=\"(Ferrara, 2023)\" isShortName></Paper>. Research has demonstrated that content moderation models exhibit undesirable biases toward mentions of disability in toxicity prediction tasks <Paper corpusId=\"218487466\" paperTitle=\"(Hutchinson et al., 2020)\" isShortName></Paper>. These biases can result in the over-policing of language used by marginalized communities, further silencing already underrepresented voices.\n\n## Healthcare\nBias in LLMs used for healthcare applications can have life-threatening consequences. Studies have shown that healthcare systems can exhibit racial bias, with models judging Black patients with the same risk level to be sicker than white patients <Paper corpusId=\"261049466\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper> <Paper corpusId=\"204881868\" paperTitle=\"(Obermeyer et al., 2019)\" isShortName></Paper>. When LLMs are deployed in clinical settings for tasks such as medical record analysis, symptom assessment, or treatment recommendations, these biases can lead to disparities in care quality and outcomes for marginalized groups.\n\n## Financial Services\nIn the financial sector, institutions increasingly rely on AI models for credit scoring and lending decisions. Biased models may unfairly penalize certain groups, such as minority communities or individuals with lower socioeconomic status, by assigning them lower credit scores or denying them access to loans and financial services based on biased assumptions <Paper corpusId=\"258041203\" paperTitle=\"(Ferrara, 2023)\" isShortName></Paper> <Paper corpusId=\"51734899\" paperTitle=\"(Ustun et al., 2018)\" isShortName></Paper>. These biases in lending applications can perpetuate financial inequality and limit economic opportunities for already disadvantaged populations.\n\n## Speech and Emotion Recognition\nBias also manifests in speech-related applications powered by LLMs. Research has identified biases in Automatic Speech Recognition (ASR), Speech Translation, Facial Emotion Recognition, and Automatic Speaker Verification systems <Paper corpusId=\"270357379\" paperTitle=\"(Lin et al., 2024)\" isShortName></Paper> <Paper corpusId=\"222005741\" paperTitle=\"(Fenu et al., 2020)\" isShortName></Paper>. These biases can result in higher error rates for certain demographic groups, creating accessibility barriers. For example, speech recognition systems often perform worse for speakers with accents, disabilities, or non-standard dialects, limiting the utility of these technologies for diverse populations <Paper corpusId=\"253581416\" paperTitle=\"(Ngueajio et al., 2022)\" isShortName></Paper>.\n\n## Sentiment Analysis and Toxicity Detection\nLanguage models deployed for sentiment analysis and toxicity detection frequently exhibit bias against marginalized groups. Research has uncovered significant biases in popular sentiment analysis tools, including TextBlob, VADER, and Google Cloud Natural Language API, as well as toxicity analysis models like Toxic-BERT <Paper corpusId=\"261214814\" paperTitle=\"(Venkit, 2023)\" isShortName></Paper>. All of these models show statistically significant explicit bias against disability mentions <Paper corpusId=\"218487466\" paperTitle=\"(Hutchinson et al., 2020)\" isShortName></Paper>. Similarly, studies have found that racial and religious identity labels consistently receive more negative sentiment scores across various models <Paper corpusId=\"258170403\" paperTitle=\"(Sharma et al., 2023)\" isShortName></Paper>.\n\n## Quality-of-Service and Stereotyping Harms\nThe manifestation of bias in LLMs across applications generally results in two primary types of harms: quality-of-service harms and stereotyping harms <Paper corpusId=\"271161894\" paperTitle=\"(Grieve et al., 2024)\" isShortName></Paper> <Paper corpusId=\"258041203\" paperTitle=\"(Ferrara, 2023)\" isShortName></Paper>. Quality-of-service harms occur when model performance varies depending on the user's social background, often resulting from underrepresentation of certain groups in training data <Paper corpusId=\"271161894\" paperTitle=\"(Grieve et al., 2024)\" isShortName></Paper> <Paper corpusId=\"209461005\" paperTitle=\"(Shah et al., 2019)\" isShortName></Paper>. Stereotyping harms, on the other hand, involve the perpetuation and amplification of harmful stereotypes about marginalized groups <Paper corpusId=\"274117151\" paperTitle=\"(Doll et al., 2024)\" isShortName></Paper>.\n\n## Intersectional Impact\nThe harmful effects of bias in applications are often compounded for individuals with intersectional identities. For example, a Black woman may face greater biases in hiring algorithms than either Black men or white women due to the intersection of racial and gender biases <Paper corpusId=\"248006238\" paperTitle=\"(Camara et al., 2022)\" isShortName></Paper>. These intersectional impacts highlight the complex nature of bias in LLMs and the need for comprehensive approaches to addressing it.\n\n## Perpetuation of Systemic Discrimination\nWhen deployed at scale, biased LLMs can perpetuate and amplify existing systemic discrimination. The consequences can range from reinforcing harmful stereotypes to causing material harm through the denial of opportunities, resources, or services <Paper corpusId=\"265609662\" paperTitle=\"(Tanyel et al., 2023)\" isShortName></Paper> <Paper corpusId=\"236950797\" paperTitle=\"(Kirk et al., 2021)\" isShortName></Paper>. As LLMs become increasingly integrated into critical systems that impact individuals' lives, addressing these biases becomes an urgent ethical imperative <Paper corpusId=\"271745282\" paperTitle=\"(Kumar et al., 2024)\" isShortName></Paper> <Paper corpusId=\"121125604\" paperTitle=\"(Basta et al., 2019)\" isShortName></Paper>.\n\nThe varied manifestations of bias across applications highlight the need for comprehensive bias detection and mitigation strategies that address both allocational and representational harms <Paper corpusId=\"274117151\" paperTitle=\"(Doll et al., 2024)\" isShortName></Paper> <Paper corpusId=\"235436386\" paperTitle=\"(Suresh et al., 2019)\" isShortName></Paper>. Without intervention, these biases threaten to replicate and amplify existing social inequalities in increasingly automated decision-making systems <Paper corpusId=\"268157336\" paperTitle=\"(Patil et al., 2024)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Ferrara, 2023)",
                        "snippets": [
                            "Bias in widely-adopted AI models, including ChatGPT and other generative language models, can have farreaching consequences that extend beyond the immediate context of their applications. When these models absorb and propagate biases, including those present in their training data, they may inadvertently reinforce stereotypes, marginalize certain groups, and lead to unfair treatment across various domains. Some examples of how biased AI models can adversely impact different areas include:\n\n\u2022 Hiring: AI-driven hiring tools that use biased models may exhibit unfair treatment towards applicants from underrepresented groups or those with non-traditional backgrounds. This could lead to the perpetuation of existing inequalities in the job market, limiting opportunities for affected individuals and reducing diversity in the workforce [102,(Raghavan et al., 2019). Large language models can be used to automate the screening of job applicants, such as by analyzing resumes and cover letters. Since these models are trained on vast amounts of text data, they may have internalized biases present in the data, such as gender or racial biases. As a result, they could unintentionally favor certain applicants or disqualify others based on factors unrelated to their qualifications, reinforcing existing inequalities in the job market.\n\n\u2022 Lending: Financial institutions increasingly rely on AI models for credit scoring and lending decisions. Biased models may unfairly penalize certain groups, such as minority communities or individuals with lower socioeconomic status, by assigning them lower credit scores or denying them access to loans and financial services based on biased assumptions (Citron et al., 2014)105,(Ustun et al., 2018). In lending, large language models can be used to assess creditworthiness or predict loan default risk, e.g., based on automated analysis of application or support documents.\n\nIf the data used to train these models contain historical biases or discriminatory lending practices, the models may learn to replicate these patterns. Consequently, they could deny loans to certain demographics or offer unfavorable terms based on factors like race, gender, or socioeconomic status, perpetuating financial inequality [107].\n\n\u2022 Content moderation: AI-powered content moderation systems help manage and filter user-generated content on social media platforms and other online communities. If these systems are trained on biased data, they may disproportionately censor or suppress the voices of certain groups"
                        ],
                        "paper": {
                            "corpus_id": 258041203,
                            "title": "Should ChatGPT be Biased? Challenges and Risks of Bias in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "48898287",
                                    "name": "Emilio Ferrara"
                                }
                            ],
                            "year": 2023,
                            "venue": "First Monday",
                            "n_citations": 261
                        },
                        "score": 0.845703125
                    },
                    {
                        "id": "(Raghavan et al., 2019)",
                        "snippets": [
                            "There has been rapidly growing interest in the use of algorithms in hiring, especially as a means to address or mitigate bias. Yet, to date, little is known about how these methods are used in practice. How are algorithmic assessments built, validated, and examined for bias? In this work, we document and analyze the claims and practices of companies offering algorithms for employment assessment. In particular, we identify vendors of algorithmic pre-employment assessments (i.e., algorithms to screen candidates), document what they have disclosed about their development and validation procedures, and evaluate their practices, focusing particularly on efforts to detect and mitigate bias. Our analysis considers both technical and legal perspectives. Technically, we consider the various choices vendors make regarding data collection and prediction targets, and explore the risks and trade-offs that these choices pose. We also discuss how algorithmic de-biasing techniques interface with, and create challenges for, antidiscrimination law."
                        ],
                        "paper": {
                            "corpus_id": 195316573,
                            "title": "Mitigating bias in algorithmic hiring: evaluating claims and practices",
                            "authors": [
                                {
                                    "authorId": "38009222",
                                    "name": "Manish Raghavan"
                                },
                                {
                                    "authorId": "2881033",
                                    "name": "Solon Barocas"
                                },
                                {
                                    "authorId": "3371403",
                                    "name": "J. Kleinberg"
                                },
                                {
                                    "authorId": "144463523",
                                    "name": "K. Levy"
                                }
                            ],
                            "year": 2019,
                            "venue": "FAT*",
                            "n_citations": 525
                        },
                        "score": 0
                    },
                    {
                        "id": "(Li et al., 2023)",
                        "snippets": [
                            "However, numerous studies have verified that LLMs capture human-like social biases in unprocessed training data (Garg et al., 2017)(Sun et al., 2019). These social biases can be encoded in the embeddings and carried over to decisions in downstream tasks, compromising the fairness of LLMs. Unfair LLM systems make discriminatory, stereotypic and demeaning decisions against vulnerable or marginalized demographics, causing undesirable social impacts and potential harms (Blodgett et al., 2020)(Kumar et al., 2022). For example, GPT-3 is found to associate males with higher levels of education and greater occupational competence, when asked GPT-3 that \"What is the gender of the doctor?\" and \"What is the gender of the nurse?\", its preferred outputs are \"A: Doctor is a masculine noun;\" and \"It's female.\", respectively. In real-world applications, the automatic resume filtering systems can be gender-biased, which tend to assign programmer jobs to men and homemaker jobs to women (De-Arteaga et al., 2019)(Deshpande et al., 2020)[10], and the US healthcare system can be racial biased, which judges black patients with the same risk level to be sicker than white patients (Obermeyer et al., 2019)."
                        ],
                        "paper": {
                            "corpus_id": 261049466,
                            "title": "A Survey on Fairness in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2116202874",
                                    "name": "Yingji Li"
                                },
                                {
                                    "authorId": "3432460",
                                    "name": "Mengnan Du"
                                },
                                {
                                    "authorId": "145401373",
                                    "name": "Rui Song"
                                },
                                {
                                    "authorId": "2153687737",
                                    "name": "Xin Wang"
                                },
                                {
                                    "authorId": "49416173",
                                    "name": "Y. Wang"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 70
                        },
                        "score": 0.81005859375
                    },
                    {
                        "id": "(De-Arteaga et al., 2019)",
                        "snippets": [
                            "We present a large-scale study of gender bias in occupation classification, a task where the use of machine learning may lead to negative outcomes on peoples' lives. We analyze the potential allocation harms that can result from semantic representation bias. To do so, we study the impact on occupation classification of including explicit gender indicators---such as first names and pronouns---in different semantic representations of online biographies. Additionally, we quantify the bias that remains when these indicators are \"scrubbed,\" and describe proxy behavior that occurs in the absence of explicit gender indicators. As we demonstrate, differences in true positive rates between genders are correlated with existing gender imbalances in occupations, which may compound these imbalances."
                        ],
                        "paper": {
                            "corpus_id": 58006082,
                            "title": "Bias in Bios: A Case Study of Semantic Representation Bias in a High-Stakes Setting",
                            "authors": [
                                {
                                    "authorId": "1406443102",
                                    "name": "Maria De-Arteaga"
                                },
                                {
                                    "authorId": "145020546",
                                    "name": "Alexey Romanov"
                                },
                                {
                                    "authorId": "1831395",
                                    "name": "Hanna M. Wallach"
                                },
                                {
                                    "authorId": "1695997",
                                    "name": "J. Chayes"
                                },
                                {
                                    "authorId": "1721812",
                                    "name": "C. Borgs"
                                },
                                {
                                    "authorId": "2359380329",
                                    "name": "Alexandra Chouldechova"
                                },
                                {
                                    "authorId": "6748971",
                                    "name": "S. Geyik"
                                },
                                {
                                    "authorId": "1769861",
                                    "name": "K. Kenthapadi"
                                },
                                {
                                    "authorId": "2186481",
                                    "name": "A. Kalai"
                                }
                            ],
                            "year": 2019,
                            "venue": "FAT",
                            "n_citations": 460
                        },
                        "score": 0
                    },
                    {
                        "id": "(Deshpande et al., 2020)",
                        "snippets": [
                            "With increasing diversity in the labor market as well as the work force, employers receive resumes from an increasingly diverse population. However, studies and field experiments have confirmed the presence of bias in the labor market based on gender, race, and ethnicity. Many employers use automated resume screening to filter the many possible matches. Depending on how the automated screening algorithm is trained it can potentially exhibit bias towards a particular population by favoring certain socio-linguistic characteristics. The resume writing style and socio-linguistics are a potential source of bias as they correlate with protected characteristics such as ethnicity. A biased dataset is often translated into biased AI algorithms and de-biasing algorithms are being contemplated. In this work, we study the effects of socio-linguistic bias on resume to job description matching algorithms. We develop a simple technique, called fair-tf-idf, to match resumes with job descriptions in a fair way by mitigating the socio-linguistic bias."
                        ],
                        "paper": {
                            "corpus_id": 220040093,
                            "title": "Mitigating Demographic Bias in AI-based Resume Filtering",
                            "authors": [
                                {
                                    "authorId": "3106764",
                                    "name": "K. Deshpande"
                                },
                                {
                                    "authorId": "2239443126",
                                    "name": "Shimei Pan"
                                },
                                {
                                    "authorId": "40289577",
                                    "name": "James R. Foulds"
                                }
                            ],
                            "year": 2020,
                            "venue": "User Modeling, Adaptation, and Personalization",
                            "n_citations": 65
                        },
                        "score": 0
                    },
                    {
                        "id": "(Hutchinson et al., 2020)",
                        "snippets": [
                            "Building equitable and inclusive NLP technologies demands consideration of whether and how social attitudes are represented in ML models. In particular, representations encoded in models often inadvertently perpetuate undesirable social biases from the data on which they are trained. In this paper, we present evidence of such undesirable biases towards mentions of disability in two different English language models: toxicity prediction and sentiment analysis. Next, we demonstrate that the neural embeddings that are the critical first step in most NLP pipelines similarly contain undesirable biases towards mentions of disability. We end by highlighting topical biases in the discourse about disability which may contribute to the observed model biases; for instance, gun violence, homelessness, and drug addiction are over-represented in texts discussing mental illness."
                        ],
                        "paper": {
                            "corpus_id": 218487466,
                            "title": "Social Biases in NLP Models as Barriers for Persons with Disabilities",
                            "authors": [
                                {
                                    "authorId": "2044655623",
                                    "name": "Ben Hutchinson"
                                },
                                {
                                    "authorId": "3331141",
                                    "name": "Vinodkumar Prabhakaran"
                                },
                                {
                                    "authorId": "40081727",
                                    "name": "Emily L. Denton"
                                },
                                {
                                    "authorId": "20825661",
                                    "name": "Kellie Webster"
                                },
                                {
                                    "authorId": "2112887022",
                                    "name": "Yu Zhong"
                                },
                                {
                                    "authorId": "1667883461",
                                    "name": "Stephen Denuyl"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 313
                        },
                        "score": 0.81103515625
                    },
                    {
                        "id": "(Obermeyer et al., 2019)",
                        "snippets": [
                            "Racial bias in health algorithms The U.S. health care system uses commercial algorithms to guide health decisions. Obermeyer et al. find evidence of racial bias in one widely used algorithm, such that Black patients assigned the same level of risk by the algorithm are sicker than White patients (see the Perspective by Benjamin). The authors estimated that this racial bias reduces the number of Black patients identified for extra care by more than half. Bias occurs because the algorithm uses health costs as a proxy for health needs. Less money is spent on Black patients who have the same level of need, and the algorithm thus falsely concludes that Black patients are healthier than equally sick White patients. Reformulating the algorithm so that it no longer uses costs as a proxy for needs eliminates the racial bias in predicting who needs extra care. Science, this issue p. 447; see also p. 421 A health algorithm that uses health costs as a proxy for health needs leads to racial bias against Black patients. Health systems rely on commercial prediction algorithms to identify and help patients with complex health needs. We show that a widely used algorithm, typical of this industry-wide approach and affecting millions of patients, exhibits significant racial bias: At a given risk score, Black patients are considerably sicker than White patients, as evidenced by signs of uncontrolled illnesses. Remedying this disparity would increase the percentage of Black patients receiving additional help from 17.7 to 46.5%. The bias arises because the algorithm predicts health care costs rather than illness, but unequal access to care means that we spend less money caring for Black patients than for White patients. Thus, despite health care cost appearing to be an effective proxy for health by some measures of predictive accuracy, large racial biases arise. We suggest that the choice of convenient, seemingly effective proxies for ground truth can be an important source of algorithmic bias in many contexts."
                        ],
                        "paper": {
                            "corpus_id": 204881868,
                            "title": "Dissecting racial bias in an algorithm used to manage the health of populations",
                            "authors": [
                                {
                                    "authorId": "3797258",
                                    "name": "Z. Obermeyer"
                                },
                                {
                                    "authorId": "34593484",
                                    "name": "Brian W. Powers"
                                },
                                {
                                    "authorId": "4744063",
                                    "name": "C. Vogeli"
                                },
                                {
                                    "authorId": "2062143",
                                    "name": "S. Mullainathan"
                                }
                            ],
                            "year": 2019,
                            "venue": "Science",
                            "n_citations": 3628
                        },
                        "score": 0
                    },
                    {
                        "id": "(Ustun et al., 2018)",
                        "snippets": [
                            "Classification models are often used to make decisions that affect humans: whether to approve a loan application, extend a job offer, or provide insurance. In such applications, individuals should have the ability to change the decision of the model. When a person is denied a loan by a credit scoring model, for example, they should be able to change the input variables of the model in a way that will guarantee approval. Otherwise, this person will be denied the loan so long as the model is deployed, and -- more importantly --will lack agency over a decision that affects their livelihood. In this paper, we propose to evaluate a linear classification model in terms of recourse, which we define as the ability of a person to change the decision of the model through actionable input variables (e.g., income vs. age or marital status). We present an integer programming toolkit to: (i) measure the feasibility and difficulty of recourse in a target population; and (ii) generate a list of actionable changes for a person to obtain a desired outcome. We discuss how our tools can inform different stakeholders by using them to audit recourse for credit scoring models built with real-world datasets. Our results illustrate how recourse can be significantly affected by common modeling practices, and motivate the need to evaluate recourse in algorithmic decision-making."
                        ],
                        "paper": {
                            "corpus_id": 51734899,
                            "title": "Actionable Recourse in Linear Classification",
                            "authors": [
                                {
                                    "authorId": "3072590",
                                    "name": "Berk Ustun"
                                },
                                {
                                    "authorId": "51444076",
                                    "name": "Alexander Spangher"
                                },
                                {
                                    "authorId": "2152797134",
                                    "name": "Yang Liu"
                                }
                            ],
                            "year": 2018,
                            "venue": "FAT",
                            "n_citations": 550
                        },
                        "score": 0
                    },
                    {
                        "id": "(Lin et al., 2024)",
                        "snippets": [
                            "While extensive research has addressed bias in various machine learning domains, such as Automatic Speech Recognition (ASR) (Ngueajio et al., 2022)2,3], Speech Translation (Savoldi et al., 2022)(Gaido et al., 2021), Facial Emotion Recognition [6], and Automatic Speaker Verification (ASV) (Fenu et al., 2020), limited attention has been paid to social bias within SER systems."
                        ],
                        "paper": {
                            "corpus_id": 270357379,
                            "title": "Emo-bias: A Large Scale Evaluation of Social Bias on Speech Emotion Recognition",
                            "authors": [
                                {
                                    "authorId": "2284761645",
                                    "name": "Yi-Cheng Lin"
                                },
                                {
                                    "authorId": "2243260548",
                                    "name": "Haibin Wu"
                                },
                                {
                                    "authorId": "35725460",
                                    "name": "Huang-Cheng Chou"
                                },
                                {
                                    "authorId": "2208036805",
                                    "name": "Chi-Chun Lee"
                                },
                                {
                                    "authorId": "2305556858",
                                    "name": "Hung-yi Lee"
                                }
                            ],
                            "year": 2024,
                            "venue": "Interspeech",
                            "n_citations": 9
                        },
                        "score": 0.8056640625
                    },
                    {
                        "id": "(Fenu et al., 2020)",
                        "snippets": [
                            "To allow individuals to complete voice-based tasks (e.g., send messages or make payments), modern automated systems are required to match the speaker\u2019s voice to a unique digital identity representation for verification. Despite the increasing accuracy achieved so far, it still remains under-explored how the decisions made by such systems may be influenced by the inherent characteristics of the individual under consideration. In this paper, we investigate how state-of-the-art speaker verification models are susceptible to unfairness towards legally-protected classes of individuals, characterized by a common sensitive attribute (i.e., gender, age, language). To this end, we first arranged a voice dataset, with the aim of including and identifying various demographic classes. Then, we conducted a performance analysis at different levels, from equal error rates to verification score distributions. Experiments show that individuals belonging to certain demographic groups systematically experience higher error rates, highlighting the need of fairer speaker recognition models and, by extension, of proper evaluation frameworks."
                        ],
                        "paper": {
                            "corpus_id": 222005741,
                            "title": "Exploring Algorithmic Fairness in Deep Speaker Verification",
                            "authors": [
                                {
                                    "authorId": "40433308",
                                    "name": "G. Fenu"
                                },
                                {
                                    "authorId": "1975012884",
                                    "name": "Hicham Lafhouli"
                                },
                                {
                                    "authorId": "28922901",
                                    "name": "Mirko Marras"
                                }
                            ],
                            "year": 2020,
                            "venue": "Communication Systems and Applications",
                            "n_citations": 21
                        },
                        "score": 0
                    },
                    {
                        "id": "(Ngueajio et al., 2022)",
                        "snippets": [
                            "Speech is the fundamental means of communication between humans. The advent of AI and sophisticated speech technologies have led to the rapid proliferation of human-to-computer-based interactions, fueled primarily by Automatic Speech Recognition (ASR) systems. ASR systems normally take human speech in the form of audio and convert it into words, but for some users, it cannot decode the speech, and any output text is filled with errors that are incomprehensible to the human reader. These systems do not work equally for everyone and actually hinder the productivity of some users. In this paper, we present research that addresses ASR biases against gender, race, and the sick and disabled, while exploring studies that propose ASR debiasing techniques for mitigating these discriminations. We also discuss techniques for designing a more accessible and inclusive ASR technology. For each approach surveyed, we also provide a summary of the investigation and methods applied, the ASR systems and corpora used, and the research findings, and highlight their strengths and/or weaknesses. Finally, we propose future opportunities for Natural Language Processing researchers to explore in the next level creation of ASR technologies."
                        ],
                        "paper": {
                            "corpus_id": 253581416,
                            "title": "Hey ASR System! Why Aren't You More Inclusive? Automatic Speech Recognition Systems' Bias and Proposed Bias Mitigation Techniques. A Literature Review",
                            "authors": [
                                {
                                    "authorId": "2183790481",
                                    "name": "Mikel K. Ngueajio"
                                },
                                {
                                    "authorId": "2065226591",
                                    "name": "Gloria J. Washington"
                                }
                            ],
                            "year": 2022,
                            "venue": "Interacci\u00f3n",
                            "n_citations": 38
                        },
                        "score": 0
                    },
                    {
                        "id": "(Venkit, 2023)",
                        "snippets": [
                            "In my study [9], I delve into sentiment analysis and toxicity detection models to identify explicit bias against race, gender, and people with disabilities (PWDs). Through statistical exploration of conversations on social media platforms such as Twitter and Reddit, I gain insights into how disability bias permeates real-world social settings. To quantify explicit sociodemographic bias in sentiment analysis and toxicity analysis models, I create the Bias Identification Test in Sentiment (BITS) corpus1. Applying BITS, I uncover significant biases in popular AIaaS sentiment analysis tools, including TextBlob, VADER, and Google Cloud Natural Language API, as well as toxicity analysis models like Toxic-BERT. Remarkably, all of these models exhibit statistically significant explicit bias against disability, underscoring the need for comprehensive understanding and mitigation of biases affecting such groups.\n\nExpanding on this, my next work [8] delves into the realm of implicit bias in NLP models. While some models may not overtly exhibit bias, they can unintentionally perpetuate harmful stereotypes [4]. To measure and identify implicit bias in commonly used embedding and large language models, I propose a methodology to measure social biases in various NLP architectures. Focusing on people with disabilities (PWD) as a group with complex social dynamics, I analyze various word embedding-based and transformer-based LLMs, revealing significant biases against PWDs in all tested models. These findings expose how models trained on extensive corpora tend to favor ableist language, underscoring the urgency of detecting and addressing implicit bias.\n\nTo explore this aspect, I investigate how LLMs represent nationality and their impact on societal stereotypes [6]. By examining LLM-generated stories for various nationalities, I establish a correlation between sentiment and the population of internet users in a country. The study reveals the unintentional implicit and explicit nationality biases exhibited by GPT-2, with nations having lower internet representation and economic status generating negative sentiment stories and employing a greater number of negative adjectives."
                        ],
                        "paper": {
                            "corpus_id": 261214814,
                            "title": "Towards a Holistic Approach: Understanding Sociodemographic Biases in NLP Models using an Interdisciplinary Lens",
                            "authors": [
                                {
                                    "authorId": "2053812167",
                                    "name": "Pranav Narayanan Venkit"
                                }
                            ],
                            "year": 2023,
                            "venue": "AAAI/ACM Conference on AI, Ethics, and Society",
                            "n_citations": 8
                        },
                        "score": 0.8291015625
                    },
                    {
                        "id": "(Sharma et al., 2023)",
                        "snippets": [
                            "Large pre-trained language models are widely used in the community. These models are usually trained on unmoderated and unfiltered data from open sources like the Internet. Due to this, biases that we see in platforms online which are a reflection of those in society are in turn captured and learned by these models. These models are deployed in applications that affect millions of people and their inherent biases are harmful to the targeted social groups.\n\nGiven the popularity of large pre-trained language models and their widespread use in the community, it is pertinent to evaluate their inherent biases. These biases come about because these models (for example, BERT (Devlin et al., 2019) and BERT based models, ALBERT (Lan et al., 2020), RoBERTa (Liu et al., 2019)) are trained on large sets of often freely available and unmoderated text data from sources such as the internet. \n\nDue to this, gender, race, religious and other social biases that we see in the real world are often translated into the models. Since these models are deployed in applications that are used by a large number of people, this bias is harmful. For example, these models are commonly used in content moderation tasks on social media platforms online. If the model is unfairly biased against certain social groups-minorities, marginalized people and those sections of society that are discriminated against, then they would be adversely affected by the application.\n\nStereoSet accounts for four different biases that are based on gender, occupation, race and religion. In addition to these, CrowS-Pairs also has sexual orientation, age, nationality, disability and physical appearance."
                        ],
                        "paper": {
                            "corpus_id": 258170403,
                            "title": "Evaluation of Social Biases in Recent Large Pre-Trained Models",
                            "authors": [
                                {
                                    "authorId": "2214583125",
                                    "name": "Swapnil Sharma"
                                },
                                {
                                    "authorId": "2214521713",
                                    "name": "Nikita Anand"
                                },
                                {
                                    "authorId": "1415341297",
                                    "name": "V. KranthiKiranG."
                                },
                                {
                                    "authorId": "2214565507",
                                    "name": "Alind Jain"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.8125
                    },
                    {
                        "id": "(Grieve et al., 2024)",
                        "snippets": [
                            "NLP systems generally suffer from social bias: their real-world application leads to outcomes that unfairly disadvantage or harm specific social groups [58,59,60,61]. Social bias can be introduced at various points during the development and deployment of NLP systems [62], but given the unsupervised nature of language modeling, training corpora are a key source of social bias in LLMs [10,63]. While bias in NLP systems can harm people in various ways [59], in this section, we primarily focus on two common harmful outcomes of social bias. These two types of harms are most commonly discussed in terms of quality-of-service harms and stereotyping harms (e.g., [64,65,60,66]), although many different systems have been proposed for classifying biases and harms in NLP, which define these terms in somewhat different ways, along with many additional and often overlapping categories [59]. Both of these types of harms are especially relevant to LLMs, and crucially, we believe both can be better understood and addressed in language modeling by adopting a sociolinguistic perspective (see Figure 3). First, social bias can be characterized by poor system performance for certain social groups that are interacting with LLMs and applications based on language models: token prediction will be more or less accurate depending on the social origins of the language inputted into the system. For example, ChatGPT might have difficulty correctly understanding prompts written by people from certain social groups due to their use of non-standard or socially restricted language patterns. This type of bias leads to what is known as quality-of-service harms, where the performance of these systems varies depending on the social background of the user [64,60]. These types of quality-of-service harms can often be the product of selection bias, as they result from how training data is selected from across the society whose language is being modeled [58]: in general, if language data from certain social groups is under-represented in the training data for a language model, we should expect that NLP applications based on that model will process language structures produced by these groups less accurately and consequently exhibit poorer performance for these groups [59]67]."
                        ],
                        "paper": {
                            "corpus_id": 271161894,
                            "title": "The sociolinguistic foundations of language modeling",
                            "authors": [
                                {
                                    "authorId": "2280139407",
                                    "name": "Jack Grieve"
                                },
                                {
                                    "authorId": "2311115214",
                                    "name": "Sara Bartl"
                                },
                                {
                                    "authorId": "2268357014",
                                    "name": "Matteo Fuoli"
                                },
                                {
                                    "authorId": "69370030",
                                    "name": "Jason Grafmiller"
                                },
                                {
                                    "authorId": "2280176696",
                                    "name": "Weihang Huang"
                                },
                                {
                                    "authorId": "2304488914",
                                    "name": "A. Jawerbaum"
                                },
                                {
                                    "authorId": "2280137448",
                                    "name": "Akira Murakami"
                                },
                                {
                                    "authorId": "2311114471",
                                    "name": "Marcus Perlman"
                                },
                                {
                                    "authorId": "2298903710",
                                    "name": "Dana Roemling"
                                },
                                {
                                    "authorId": "2320760943",
                                    "name": "Bodo Winter"
                                }
                            ],
                            "year": 2024,
                            "venue": "Frontiers Artif. Intell.",
                            "n_citations": 10
                        },
                        "score": 0.85107421875
                    },
                    {
                        "id": "(Shah et al., 2019)",
                        "snippets": [
                            "An increasing number of natural language processing papers address the effect of bias on predictions, introducing mitigation techniques at different parts of the standard NLP pipeline (data and models). However, these works have been conducted individually, without a unifying framework to organize efforts within the field. This situation leads to repetitive approaches, and focuses overly on bias symptoms/effects, rather than on their origins, which could limit the development of effective countermeasures. In this paper, we propose a unifying predictive bias framework for NLP. We summarize the NLP literature and suggest general mathematical definitions of predictive bias. We differentiate two consequences of bias: outcome disparities and error disparities, as well as four potential origins of biases: label bias, selection bias, model overamplification, and semantic bias. Our framework serves as an overview of predictive bias in NLP, integrating existing work into a single structure, and providing a conceptual baseline for improved frameworks."
                        ],
                        "paper": {
                            "corpus_id": 209461005,
                            "title": "Predictive Biases in Natural Language Processing Models: A Conceptual Framework and Overview",
                            "authors": [
                                {
                                    "authorId": "1994268",
                                    "name": "Deven Santosh Shah"
                                },
                                {
                                    "authorId": "145035129",
                                    "name": "H. A. Schwartz"
                                },
                                {
                                    "authorId": "2022288",
                                    "name": "Dirk Hovy"
                                }
                            ],
                            "year": 2019,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 260
                        },
                        "score": 0
                    },
                    {
                        "id": "(Doll et al., 2024)",
                        "snippets": [
                            "LLMs learn from vast amounts of text collected from the Internet and other sources. This training data often contains unbalanced representations and stereotypes that are unintentionally embedded in the models [3].\n\nBias in LLMs can harm both individuals and society, with consequences including allocative and representational harms. Allocative harms occur when resources or opportunities are distributed unfairly, often to the detriment of already marginalised groups. For example, recruitment algorithms may be skewed to disadvantage certain demographic groups. Representational harm occurs when groups are misrepresented or stereotyped, reinforcing harmful narratives and stigmas and further entrenching negative prejudices and societal biases [4]. Language models show a propensity for bias in a variety of areas, including but not limited to gender, age, sexual orientation, physical appearance, disability, nationality, ethnicity, socio-economic status, religion and culture. These biases have the potential to have significant consequences, particularly in contexts where they affect critical areas such as justice, healthcare or employment [5]."
                        ],
                        "paper": {
                            "corpus_id": 274117151,
                            "title": "Evaluating Gender Bias in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2338278194",
                                    "name": "Michael D\u00f6ll"
                                },
                                {
                                    "authorId": "2338277213",
                                    "name": "Markus D\u00f6hring"
                                },
                                {
                                    "authorId": "2338675374",
                                    "name": "Andreas M\u00fcller"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.8681640625
                    },
                    {
                        "id": "(Camara et al., 2022)",
                        "snippets": [
                            "The presence and impact of harmful social biases in machine learning and natural language processing systems is pervasive and well-documented in popular word embedding methods (Caliskan et al., 2016)(Garg et al., 2017)(Bolukbasi et al., 2016) due to large amounts of humanproduced training data that includes historical social biases. Notably, (Caliskan et al., 2016) demonstrate such biases by introducing the Word Embedding Association Test (WEAT) which measures how similar socially sensitive sets of words (e.g., racial or gendered names) are to attributive sets of words (e.g., pleasant or unpleasant words) in the semantic space encoded by word embeddings. While (Bolukbasi et al., 2016); Manzini et al. (2019) introduce methods for \"debiasing\" word embeddings in order to create more equitable semantic representations for usage in downstream tasks, Gonen and Goldberg (2019) argue that such methods are merely cosmetic since social biases are still evident in the semantic space after the application of such methods. Moreover, these \"debiasing\" techniques focus on a particular social cleavage such as gender or race (i.e., unisectional cleavages). In contrast, our work considers both unisectional and intersectional social biases.\n\nRecent studies have also begun to focus on social biases in transformer-based language models (Kurita et al., 2019)(Bender et al., 2021). In particular, (Bender et al., 2021) discusses how increasingly large transformer-based language model in practice regurgitate their training data, resulting in such models perpetuating social biases and harming users."
                        ],
                        "paper": {
                            "corpus_id": 248006238,
                            "title": "Mapping the Multilingual Margins: Intersectional Biases of Sentiment Analysis Systems in English, Spanish, and Arabic",
                            "authors": [
                                {
                                    "authorId": "2077593669",
                                    "name": "Antonio Camara"
                                },
                                {
                                    "authorId": "70655120",
                                    "name": "Nina Taneja"
                                },
                                {
                                    "authorId": "1571776558",
                                    "name": "Tamjeed Azad"
                                },
                                {
                                    "authorId": "46208659",
                                    "name": "Emily Allaway"
                                },
                                {
                                    "authorId": "1804104",
                                    "name": "R. Zemel"
                                }
                            ],
                            "year": 2022,
                            "venue": "LTEDI",
                            "n_citations": 21
                        },
                        "score": 0.87255859375
                    },
                    {
                        "id": "(Tanyel et al., 2023)",
                        "snippets": [
                            "One of the more insidious ways in which discrimination can manifest is through natural language models, which are computational models that use statistical algorithms to understand and generate human language. These models have become ubiquitous in our daily lives, powering everything from virtual assistants to search engines and chatbots. However, as with any human-created system, natural language models are susceptible to biases and discrimination that can be embedded in the data they are trained on [21].\n\nThe problem of discrimination in language models arises because these models are developed based on huge amounts of text data retrieved from the internet, which contains a plethora of societal biases and prejudices. For instance, language models might be trained on text data that contains racist or sexist language, which can lead to the model associating certain words or phrases with negative stereotypes. As a result, when the model is used to generate text or respond to queries, it may produce outputs that perpetuate harmful stereotypes and discriminatory practices.\n\nFurthermore, language models can also be discriminatory in their output if the training data is not diverse enough to represent different demographics or if the model is biased towards a particular dialect or language variety. This can result in language models that are more accurate for certain groups than for others, which can perpetuate existing power imbalances and inequalities.\n\nThe consequences of discriminatory language models can be significant, ranging from perpetuating harmful stereotypes to reinforcing systemic discrimination against marginalized groups [22]."
                        ],
                        "paper": {
                            "corpus_id": 265609662,
                            "title": "Developing Linguistic Patterns to Mitigate Inherent Human Bias in Offensive Language Detection",
                            "authors": [
                                {
                                    "authorId": "2189148265",
                                    "name": "T. Tanyel"
                                },
                                {
                                    "authorId": "2189146673",
                                    "name": "Besher Alkurdi"
                                },
                                {
                                    "authorId": "2460907",
                                    "name": "S. Ayvaz"
                                }
                            ],
                            "year": 2023,
                            "venue": "Turkish J. Electr. Eng. Comput. Sci.",
                            "n_citations": 0
                        },
                        "score": 0.8603515625
                    },
                    {
                        "id": "(Kirk et al., 2021)",
                        "snippets": [
                            "The capabilities of natural language models trained on large-scale data have increased immensely over the past few years. Open source libraries such as HuggingFace have made these models easily available and accessible. While prior research has identified biases in large language models, this paper considers biases contained in the most popular versions of these models when applied `out-of-the-box' for downstream tasks. We focus on generative language models as they are well-suited for extracting biases inherited from training data. Specifically, we conduct an in-depth analysis of GPT-2, which is the most downloaded text generation model on HuggingFace, with over half a million downloads per month. We assess biases related to occupational associations for different protected categories by intersecting gender with religion, sexuality, ethnicity, political affiliation, and continental name origin. Using a template-based data collection pipeline, we collect 396K sentence completions made by GPT-2 and find: (i) The machine-predicted jobs are less diverse and more stereotypical for women than for men, especially for intersections; (ii) Intersectional interactions are highly relevant for occupational associations, which we quantify by fitting 262 logistic models; (iii) For most occupations, GPT-2 reflects the skewed gender and ethnicity distribution found in US Labor Bureau data, and even pulls the societally-skewed distribution towards gender parity in cases where its predictions deviate from real labor market observations. This raises the normative question of what language models should learn - whether they should reflect or correct for existing inequalities."
                        ],
                        "paper": {
                            "corpus_id": 236950797,
                            "title": "Bias Out-of-the-Box: An Empirical Analysis of Intersectional Occupational Biases in Popular Generative Language Models",
                            "authors": [
                                {
                                    "authorId": "90729626",
                                    "name": "Hannah Rose Kirk"
                                },
                                {
                                    "authorId": "1664016559",
                                    "name": "Yennie Jun"
                                },
                                {
                                    "authorId": "2146192816",
                                    "name": "Haider Iqbal"
                                },
                                {
                                    "authorId": "133876374",
                                    "name": "Elias Benussi"
                                },
                                {
                                    "authorId": "2048000297",
                                    "name": "Filippo Volpin"
                                },
                                {
                                    "authorId": "102485516",
                                    "name": "F. Dreyer"
                                },
                                {
                                    "authorId": "2048000614",
                                    "name": "Aleksandar Shtedritski"
                                },
                                {
                                    "authorId": "47792365",
                                    "name": "Yuki M. Asano"
                                }
                            ],
                            "year": 2021,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 192
                        },
                        "score": 0
                    },
                    {
                        "id": "(Kumar et al., 2024)",
                        "snippets": [
                            "Biases in these models-such as gender, racial, ethnic, and ideological biases-stem from training data that reflect societal prejudices. Consequently, language models can perpetuate and even amplify these biases, leading to outputs that reinforce stereotypes and disadvantage certain groups (Basta et al., 2019). The implications are profound, affecting areas, like hiring, law enforcement, and daily AI interactions (Schwartz et al., 2022;Ferrara, 2023)."
                        ],
                        "paper": {
                            "corpus_id": 271745282,
                            "title": "Decoding Biases: Automated Methods and LLM Judges for Gender Bias Detection in Language Models",
                            "authors": [
                                {
                                    "authorId": "2109680564",
                                    "name": "Shachi H. Kumar"
                                },
                                {
                                    "authorId": "38531701",
                                    "name": "Saurav Sahay"
                                },
                                {
                                    "authorId": "2066417452",
                                    "name": "Sahisnu Mazumder"
                                },
                                {
                                    "authorId": "3442103",
                                    "name": "Eda Okur"
                                },
                                {
                                    "authorId": "2175808",
                                    "name": "R. Manuvinakurike"
                                },
                                {
                                    "authorId": "2275457041",
                                    "name": "Nicole Beckage"
                                },
                                {
                                    "authorId": "2087042748",
                                    "name": "Hsuan Su"
                                },
                                {
                                    "authorId": "2259815964",
                                    "name": "Hung-yi Lee"
                                },
                                {
                                    "authorId": "1896095",
                                    "name": "L. Nachman"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 18
                        },
                        "score": 0.80908203125
                    },
                    {
                        "id": "(Basta et al., 2019)",
                        "snippets": [
                            "Gender bias is highly impacting natural language processing applications. Word embeddings have clearly been proven both to keep and amplify gender biases that are present in current data sources. Recently, contextualized word embeddings have enhanced previous word embedding techniques by computing word vector representations dependent on the sentence they appear in. In this paper, we study the impact of this conceptual change in the word embedding computation in relation with gender bias. Our analysis includes different measures previously applied in the literature to standard word embeddings. Our findings suggest that contextualized word embeddings are less biased than standard ones even when the latter are debiased."
                        ],
                        "paper": {
                            "corpus_id": 121125604,
                            "title": "Evaluating the Underlying Gender Bias in Contextualized Word Embeddings",
                            "authors": [
                                {
                                    "authorId": "73312674",
                                    "name": "Christine Basta"
                                },
                                {
                                    "authorId": "1398996347",
                                    "name": "M. Costa-juss\u00e0"
                                },
                                {
                                    "authorId": "1794731",
                                    "name": "Noe Casas"
                                }
                            ],
                            "year": 2019,
                            "venue": "Proceedings of the First Workshop on Gender Bias in Natural Language Processing",
                            "n_citations": 193
                        },
                        "score": 0
                    },
                    {
                        "id": "(Suresh et al., 2019)",
                        "snippets": [
                            "As machine learning (ML) increasingly affects people and society, awareness of its potential unwanted consequences has also grown. To anticipate, prevent, and mitigate undesirable downstream consequences, it is critical that we understand when and how harm might be introduced throughout the ML life cycle. In this paper, we provide a framework that identifies seven distinct potential sources of downstream harm in machine learning, spanning data collection, development, and deployment. In doing so, we aim to facilitate more productive and precise communication around these issues, as well as more direct, application-grounded ways to mitigate them."
                        ],
                        "paper": {
                            "corpus_id": 235436386,
                            "title": "A Framework for Understanding Sources of Harm throughout the Machine Learning Life Cycle",
                            "authors": [
                                {
                                    "authorId": "46537606",
                                    "name": "Harini Suresh"
                                },
                                {
                                    "authorId": "1724429",
                                    "name": "J. Guttag"
                                }
                            ],
                            "year": 2019,
                            "venue": "Conference on Equity and Access in Algorithms, Mechanisms, and Optimization",
                            "n_citations": 442
                        },
                        "score": 0
                    },
                    {
                        "id": "(Patil et al., 2024)",
                        "snippets": [
                            "Bias and fairness, if not adequately addressed, pose serious societal implications in the form of biased language generation and its impact on some segments of society. Basis can creep into LLMs from several sources discussed below. The first source of bias, dataset bias, stems from the datasets that were used to train the LLMs. If the datasets contain biases related to race, gender, religion, or socioeconomic status, the models inherit and amplify them.\n\nUnderrepresentation or misrepresentation of certain groups in the training data can lead to representation bias and biased language generation. The LLM developers should have checks and balances to ensure that all perspectives are adequately represented in the datasets. Otherwise, the model will produce inaccurate or skewed output for underrepresented groups. If the training data contain stereotypes, models amplify stereotyping and perpetuate prejudices. Fairness across demographics is a complex challenge but essential for advancing LLMs.\n\nCentextual bias stems from the context in which the language models are used. This poses severe and negative implications in applications such as recommender systems, employee hiring and promotions, clustering, and sentiment analysis."
                        ],
                        "paper": {
                            "corpus_id": 268157336,
                            "title": "A Review of Current Trends, Techniques, and Challenges in Large Language Models (LLMs)",
                            "authors": [
                                {
                                    "authorId": "2289385425",
                                    "name": "Rajvardhan Patil"
                                },
                                {
                                    "authorId": "117730513",
                                    "name": "Venkat Gudivada"
                                }
                            ],
                            "year": 2024,
                            "venue": "Applied Sciences",
                            "n_citations": 80
                        },
                        "score": 0.88720703125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Measurement and Detection of Bias in LLMs",
                "tldr": "Researchers have developed various metrics and frameworks to measure and detect bias in LLMs, including association tests, prompt-based methods, and automated tools that can identify biases across multiple social dimensions. These approaches have revealed that bias exists at significant levels across different social categories with intersectional biases often showing stronger effects than unisectional biases. (11 sources)",
                "text": "\n## Bias Measurement Techniques\n\nResearchers have developed several key methodologies to detect and quantify social bias in LLMs. One of the most influential approaches has been association tests, which measure how closely certain social groups are associated with particular attributes in the embedding space. The Word Embedding Association Test (WEAT) pioneered this approach by demonstrating that word embeddings replicate human-like social biases <Paper corpusId=\"23163324\" paperTitle=\"(Caliskan et al., 2016)\" isShortName></Paper>. Building on this foundation, the Contextualized Embedding Association Test (CEAT) was introduced to assess bias in contextual word embeddings, providing information on how bias magnitude varies across different contexts <Paper corpusId=\"219530686\" paperTitle=\"(Guo et al., 2020)\" isShortName></Paper>.\n\nSentiment analysis has emerged as another important technique for measuring bias. Researchers use sentiment scores to quantify how positively or negatively models respond to different identity terms. For instance, studies have used prompt-based methods that combine social categories (e.g., gender, religion, and disability) to generate model outputs that are then analyzed for sentiment bias <Paper corpusId=\"236034024\" paperTitle=\"(Magee et al., 2021)\" isShortName></Paper>. These approaches have revealed that while gender bias may be relatively minor and context-dependent, religious and disability biases tend to be more pronounced and resistant to context <Paper corpusId=\"236034024\" paperTitle=\"(Magee et al., 2021)\" isShortName></Paper>.\n\n## Specialized Datasets and Benchmarks\n\nTo facilitate systematic bias evaluation, researchers have developed specialized datasets and benchmarks. Notable examples include BITS (Bias Identification Test in Sentiment) corpus, which was created to quantify explicit sociodemographic bias in sentiment analysis and toxicity detection models <Paper corpusId=\"261214814\" paperTitle=\"(Venkit, 2023)\" isShortName></Paper>. Other benchmark datasets like StereoSet measure biases related to gender, occupation, race, and religion, while CrowS-Pairs additionally covers sexual orientation, age, nationality, disability, and physical appearance <Paper corpusId=\"258170403\" paperTitle=\"(Sharma et al., 2023)\" isShortName></Paper>.\n\nThese benchmarks have enabled consistent evaluation across different models and revealed that even state-of-the-art language models exhibit significant biases. Studies applying these benchmarks have found that popular sentiment analysis tools (TextBlob, VADER, Google Cloud Natural Language API) and toxicity analysis models (Toxic-BERT) all show statistically significant explicit bias against disability mentions <Paper corpusId=\"261214814\" paperTitle=\"(Venkit, 2023)\" isShortName></Paper>.\n\n## Intersectional Bias Detection\n\nA significant advancement in bias measurement has been the development of methods to detect intersectional bias\u2014bias affecting individuals with multiple marginalized identities. The Intersectional Bias Detection (IBD) and Emergent Intersectional Bias Detection (EIBD) methods have been developed to automatically identify both intersectional biases and emergent intersectional biases from word embeddings <Paper corpusId=\"219530686\" paperTitle=\"(Guo et al., 2020)\" isShortName></Paper>.\n\nThese methods have revealed important findings about the nature of intersectional bias. Research has shown that intersectional group members are often strongly associated with unique emergent biases that do not overlap with the biases of their constituent minority identities <Paper corpusId=\"219530686\" paperTitle=\"(Guo et al., 2020)\" isShortName></Paper>. For example, IBD achieved accuracy rates of 81.6% and 82.7% when detecting intersectional biases affecting African American females and Mexican American females, respectively\u2014far exceeding random identification rates <Paper corpusId=\"219530686\" paperTitle=\"(Guo et al., 2020)\" isShortName></Paper>.\n\nStudies have also found that the effect of race on intersectional identities appears to be larger than the effect of gender <Paper corpusId=\"202781363\" paperTitle=\"(Tan et al., 2019)\" isShortName></Paper>. This highlights the complexity of bias in LLMs and the need for measurement approaches that can capture multiple dimensions of bias simultaneously, rather than addressing them independently <Paper corpusId=\"250390904\" paperTitle=\"(Lalor et al., 2022)\" isShortName></Paper> <Paper corpusId=\"267627928\" paperTitle=\"(Jain et al., 2024)\" isShortName></Paper>.\n\n## Bias Across Different Models and Architectures\n\nComparative analysis of bias across different LLM architectures has yielded important insights. Research has shown that all tested models trained on English corpora contain biased representations, though the magnitude varies <Paper corpusId=\"219530686\" paperTitle=\"(Guo et al., 2020)\" isShortName></Paper>. When comparing models, studies have found that GPT-2 contains the smallest magnitude of overall bias, followed by GPT, BERT, and then ELMo, suggesting a negative correlation between bias magnitude and contextualization levels <Paper corpusId=\"219530686\" paperTitle=\"(Guo et al., 2020)\" isShortName></Paper>.\n\nHowever, certain biases appear resistant to improvements in model architecture. Research examining models with varying parameter counts (124 million to ~2.7 billion) found that while gender bias might be minor and heavily context-dependent, religious and disability biases remain strongly evident and resistant to increases in model size and training data diversity <Paper corpusId=\"236034024\" paperTitle=\"(Magee et al., 2021)\" isShortName></Paper>.\n\n## Challenges in Bias Detection\n\nDespite progress in bias measurement, researchers face significant challenges. The multidimensional nature of bias makes comprehensive evaluation difficult, as most prior work has emphasized bias along a single dimension such as gender or race <Paper corpusId=\"250390904\" paperTitle=\"(Lalor et al., 2022)\" isShortName></Paper>. Additionally, biases can manifest in unpredictable ways at category intersections, making them resistant to techniques that target single categories <Paper corpusId=\"236034024\" paperTitle=\"(Magee et al., 2021)\" isShortName></Paper>.\n\nAnother challenge is that models trained on unmoderated and unfiltered internet data inherit biases that reflect those in society <Paper corpusId=\"258170403\" paperTitle=\"(Sharma et al., 2023)\" isShortName></Paper>. These biases can be harmful to targeted social groups when the models are deployed in applications affecting millions of people <Paper corpusId=\"258170403\" paperTitle=\"(Sharma et al., 2023)\" isShortName></Paper> <Paper corpusId=\"262580630\" paperTitle=\"(Bender et al., 2021)\" isShortName></Paper>.\n\nThe field continues to evolve, with researchers emphasizing the importance of expanding bias detection beyond traditional categories. Recent work has proposed flexible methods like the \"bias score\" for estimating gender bias in sentence embeddings, designed to be adaptable to different types of social biases (e.g., ethnic, religious) and various sentence encoders <Paper corpusId=\"258174286\" paperTitle=\"(Dolci et al., 2023)\" isShortName></Paper>. These approaches highlight the growing recognition that analyzing bias is an inherently normative process requiring careful consideration of what kinds of system behaviors are harmful, to whom, and why <Paper corpusId=\"218971825\" paperTitle=\"(Blodgett et al., 2020)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Caliskan et al., 2016)",
                        "snippets": [
                            "Machines learn what people know implicitly AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan et al. now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs\u2014for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior. Science, this issue p. 183; see also p. 133 Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias. Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology."
                        ],
                        "paper": {
                            "corpus_id": 23163324,
                            "title": "Semantics derived automatically from language corpora contain human-like biases",
                            "authors": [
                                {
                                    "authorId": "144537437",
                                    "name": "Aylin Caliskan"
                                },
                                {
                                    "authorId": "145315445",
                                    "name": "J. Bryson"
                                },
                                {
                                    "authorId": "47735253",
                                    "name": "Arvind Narayanan"
                                }
                            ],
                            "year": 2016,
                            "venue": "Science",
                            "n_citations": 2673
                        },
                        "score": 0
                    },
                    {
                        "id": "(Guo et al., 2020)",
                        "snippets": [
                            "We introduce the Contextualized Embedding Association Test (CEAT), that can summarize the magnitude of overall bias in neural language models by incorporating a random-effects model. Experiments on social and intersectional biases show that CEAT finds evidence of all tested biases and provides comprehensive information on the variance of effect magnitudes of the same bias in different contexts. All the models trained on English corpora that we study contain biased representations. GPT-2 contains the smallest magnitude of overall bias followed by GPT, BERT, and then ELMo, negatively correlating with the contextualization levels of the models. Furthermore, we develop two methods, Intersectional Bias Detection (IBD) and Emergent Intersectional Bias Detection (EIBD), to automatically identify the intersectional biases and emergent intersectional biases from static word embeddings in addition to measuring them in contextualized word embeddings. We present the first algorithmic bias detection findings on how intersectional group members are strongly associated with unique emergent biases that do not overlap with the biases of their constituent minority identities. IBD achieves an accuracy of 81.6% and 82.7%, respectively, when detecting the intersectional biases of African American females and Mexican American females, where the random correct identification rates are 14.3% and 13.3%. EIBD reaches an accuracy of 84.7% and 65.3%, respectively, when detecting the emergent intersectional biases unique to African American females and Mexican American females, where the random correct identification rates are 9.2% and 6.1%. Our results indicate that intersectional biases associated with members of multiple minority groups, such as African American females and Mexican American females, have the highest magnitude across all neural language models."
                        ],
                        "paper": {
                            "corpus_id": 219530686,
                            "title": "Detecting Emergent Intersectional Biases: Contextualized Word Embeddings Contain a Distribution of Human-like Biases",
                            "authors": [
                                {
                                    "authorId": "28735341",
                                    "name": "W. Guo"
                                },
                                {
                                    "authorId": "144537437",
                                    "name": "Aylin Caliskan"
                                }
                            ],
                            "year": 2020,
                            "venue": "AAAI/ACM Conference on AI, Ethics, and Society",
                            "n_citations": 243
                        },
                        "score": 0.876953125
                    },
                    {
                        "id": "(Magee et al., 2021)",
                        "snippets": [
                            "To examine whether intersectional bias can be observed in language generation, we examine \\emph{GPT-2} and \\emph{GPT-NEO} models, ranging in size from 124 million to ~2.7 billion parameters. We conduct an experiment combining up to three social categories - gender, religion and disability - into unconditional or zero-shot prompts used to generate sentences that are then analysed for sentiment. Our results confirm earlier tests conducted with auto-regressive causal models, including the \\emph{GPT} family of models. We also illustrate why bias may be resistant to techniques that target single categories (e.g. gender, religion and race), as it can also manifest, in often subtle ways, in texts prompted by concatenated social categories.\n\nWe conclude that (1) bias exists at significant levels across different social categories (gender, religion and disability); (2) it can manifest, in unpredictable ways, at category intersections; and (3) it is resistant to increases in model size and training data diversity. Specifically, while gender bias is minor and heavily conditioned on prompt context, religion and disability bias is strongly evident and resistant to context, with Muslim and all disability labels other than 'with Down Syndrome' scoring worse than no label."
                        ],
                        "paper": {
                            "corpus_id": 236034024,
                            "title": "Intersectional Bias in Causal Language Models",
                            "authors": [
                                {
                                    "authorId": "2733075",
                                    "name": "L. Magee"
                                },
                                {
                                    "authorId": "9557084",
                                    "name": "Lida Ghahremanlou"
                                },
                                {
                                    "authorId": "13714096",
                                    "name": "K. Soldati\u0107"
                                },
                                {
                                    "authorId": "97868921",
                                    "name": "S. Robertson"
                                }
                            ],
                            "year": 2021,
                            "venue": "arXiv.org",
                            "n_citations": 33
                        },
                        "score": 0.8779296875
                    },
                    {
                        "id": "(Venkit, 2023)",
                        "snippets": [
                            "In my study [9], I delve into sentiment analysis and toxicity detection models to identify explicit bias against race, gender, and people with disabilities (PWDs). Through statistical exploration of conversations on social media platforms such as Twitter and Reddit, I gain insights into how disability bias permeates real-world social settings. To quantify explicit sociodemographic bias in sentiment analysis and toxicity analysis models, I create the Bias Identification Test in Sentiment (BITS) corpus1. Applying BITS, I uncover significant biases in popular AIaaS sentiment analysis tools, including TextBlob, VADER, and Google Cloud Natural Language API, as well as toxicity analysis models like Toxic-BERT. Remarkably, all of these models exhibit statistically significant explicit bias against disability, underscoring the need for comprehensive understanding and mitigation of biases affecting such groups.\n\nExpanding on this, my next work [8] delves into the realm of implicit bias in NLP models. While some models may not overtly exhibit bias, they can unintentionally perpetuate harmful stereotypes [4]. To measure and identify implicit bias in commonly used embedding and large language models, I propose a methodology to measure social biases in various NLP architectures. Focusing on people with disabilities (PWD) as a group with complex social dynamics, I analyze various word embedding-based and transformer-based LLMs, revealing significant biases against PWDs in all tested models. These findings expose how models trained on extensive corpora tend to favor ableist language, underscoring the urgency of detecting and addressing implicit bias.\n\nTo explore this aspect, I investigate how LLMs represent nationality and their impact on societal stereotypes [6]. By examining LLM-generated stories for various nationalities, I establish a correlation between sentiment and the population of internet users in a country. The study reveals the unintentional implicit and explicit nationality biases exhibited by GPT-2, with nations having lower internet representation and economic status generating negative sentiment stories and employing a greater number of negative adjectives."
                        ],
                        "paper": {
                            "corpus_id": 261214814,
                            "title": "Towards a Holistic Approach: Understanding Sociodemographic Biases in NLP Models using an Interdisciplinary Lens",
                            "authors": [
                                {
                                    "authorId": "2053812167",
                                    "name": "Pranav Narayanan Venkit"
                                }
                            ],
                            "year": 2023,
                            "venue": "AAAI/ACM Conference on AI, Ethics, and Society",
                            "n_citations": 8
                        },
                        "score": 0.8291015625
                    },
                    {
                        "id": "(Sharma et al., 2023)",
                        "snippets": [
                            "Large pre-trained language models are widely used in the community. These models are usually trained on unmoderated and unfiltered data from open sources like the Internet. Due to this, biases that we see in platforms online which are a reflection of those in society are in turn captured and learned by these models. These models are deployed in applications that affect millions of people and their inherent biases are harmful to the targeted social groups.\n\nGiven the popularity of large pre-trained language models and their widespread use in the community, it is pertinent to evaluate their inherent biases. These biases come about because these models (for example, BERT (Devlin et al., 2019) and BERT based models, ALBERT (Lan et al., 2020), RoBERTa (Liu et al., 2019)) are trained on large sets of often freely available and unmoderated text data from sources such as the internet. \n\nDue to this, gender, race, religious and other social biases that we see in the real world are often translated into the models. Since these models are deployed in applications that are used by a large number of people, this bias is harmful. For example, these models are commonly used in content moderation tasks on social media platforms online. If the model is unfairly biased against certain social groups-minorities, marginalized people and those sections of society that are discriminated against, then they would be adversely affected by the application.\n\nStereoSet accounts for four different biases that are based on gender, occupation, race and religion. In addition to these, CrowS-Pairs also has sexual orientation, age, nationality, disability and physical appearance."
                        ],
                        "paper": {
                            "corpus_id": 258170403,
                            "title": "Evaluation of Social Biases in Recent Large Pre-Trained Models",
                            "authors": [
                                {
                                    "authorId": "2214583125",
                                    "name": "Swapnil Sharma"
                                },
                                {
                                    "authorId": "2214521713",
                                    "name": "Nikita Anand"
                                },
                                {
                                    "authorId": "1415341297",
                                    "name": "V. KranthiKiranG."
                                },
                                {
                                    "authorId": "2214565507",
                                    "name": "Alind Jain"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.8125
                    },
                    {
                        "id": "(Tan et al., 2019)",
                        "snippets": [
                            "In natural language processing, gender bias has been shown to exist in context-free word embeddings. Recently, contextual word representations have outperformed word embeddings in several downstream NLP tasks. These word representations are conditioned on their context within a sentence, and can also be used to encode the entire sentence. In this paper, we analyze the extent to which state-of-the-art models for contextual word representations, such as BERT and GPT-2, encode biases with respect to gender, race, and intersectional identities",
                            "we provide evidence for how racial bias is encoded strongly in contextual word models, potentially even more so than gender bias",
                            "we introduce a method of comparison that anchors at the most or least privileged group to show that intersectional identities suffer from such bias as well, and more so than their constituent minority identities. In we show that the effect of race on intersectional identities seems to be larger than the effect of gender."
                        ],
                        "paper": {
                            "corpus_id": 202781363,
                            "title": "Assessing Social and Intersectional Biases in Contextualized Word Representations",
                            "authors": [
                                {
                                    "authorId": "144787248",
                                    "name": "Y. Tan"
                                },
                                {
                                    "authorId": "47777961",
                                    "name": "Elisa Celis"
                                }
                            ],
                            "year": 2019,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 229
                        },
                        "score": 0.83154296875
                    },
                    {
                        "id": "(Lalor et al., 2022)",
                        "snippets": [
                            "There has been a recent wave of work assessing the fairness of machine learning models in general, and more specifically, on natural language processing (NLP) models built using machine learning techniques. While much work has highlighted biases embedded in state-of-the-art language models, and more recent efforts have focused on how to debias, research assessing the fairness and performance of biased/debiased models on downstream prediction tasks has been limited. Moreover, most prior work has emphasized bias along a single dimension such as gender or race."
                        ],
                        "paper": {
                            "corpus_id": 250390904,
                            "title": "Benchmarking Intersectional Biases in NLP",
                            "authors": [
                                {
                                    "authorId": "9051130",
                                    "name": "John P. Lalor"
                                },
                                {
                                    "authorId": "2143685866",
                                    "name": "Yi Yang"
                                },
                                {
                                    "authorId": "2152595932",
                                    "name": "Kendall Smith"
                                },
                                {
                                    "authorId": "3047212",
                                    "name": "Nicole Forsgren"
                                },
                                {
                                    "authorId": "144849629",
                                    "name": "A. Abbasi"
                                }
                            ],
                            "year": 2022,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 48
                        },
                        "score": 0.86376953125
                    },
                    {
                        "id": "(Jain et al., 2024)",
                        "snippets": [
                            "Pretrained Language Models (PLMs) are widely used in NLP for various tasks. Recent studies have identified various biases that such models exhibit and have proposed methods to correct these biases. However, most of the works address a limited set of bias dimensions independently such as gender, race, or religion."
                        ],
                        "paper": {
                            "corpus_id": 267627928,
                            "title": "MAFIA: Multi-Adapter Fused Inclusive Language Models",
                            "authors": [
                                {
                                    "authorId": "3094662",
                                    "name": "Prachi Jain"
                                },
                                {
                                    "authorId": "2266397701",
                                    "name": "Ashutosh Sathe"
                                },
                                {
                                    "authorId": "2140408530",
                                    "name": "Varun Gumma"
                                },
                                {
                                    "authorId": "52154863",
                                    "name": "Kabir Ahuja"
                                },
                                {
                                    "authorId": "2256989615",
                                    "name": "Sunayana Sitaram"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
                            "n_citations": 1
                        },
                        "score": 0.83544921875
                    },
                    {
                        "id": "(Bender et al., 2021)",
                        "snippets": [
                            "The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models."
                        ],
                        "paper": {
                            "corpus_id": 262580630,
                            "title": "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? \ud83e\udd9c",
                            "authors": [
                                {
                                    "authorId": "2471699",
                                    "name": "Emily M. Bender"
                                },
                                {
                                    "authorId": "2076288",
                                    "name": "Timnit Gebru"
                                },
                                {
                                    "authorId": "1584940075",
                                    "name": "Angelina McMillan-Major"
                                },
                                {
                                    "authorId": "2246048745",
                                    "name": "Shmargaret Shmitchell"
                                }
                            ],
                            "year": 2021,
                            "venue": "Conference on Fairness, Accountability and Transparency",
                            "n_citations": 4657
                        },
                        "score": 0
                    },
                    {
                        "id": "(Dolci et al., 2023)",
                        "snippets": [
                            "Therefore, it is of the utmost importance to expand research on how sentence embedding encoders internalise the semantics of natural languages. An important step towards this direction is to define metrics that are able to reflect and quantify social bias in sentence encoders. Furthermore, studying and limiting the causes and consequences of bias in language models is an extremely important task [4]6].\n\nThis work expands research on social bias in embeddingbased models, focusing specifically on gender bias in sentence representations. First, we propose a method to estimate gender bias in sentence embeddings, highlighting the correlation between bias and stereotypical concepts in the sentence. Our solution, named bias score, is highly flexible and designed to be easily adapted to both different kinds of social biases (e.g. ethnic, religious) and various sentence encoders."
                        ],
                        "paper": {
                            "corpus_id": 258174286,
                            "title": "Improving Gender-Related Fairness in Sentence Encoders: A Semantics-Based Approach",
                            "authors": [
                                {
                                    "authorId": "2182555493",
                                    "name": "Tommaso Dolci"
                                },
                                {
                                    "authorId": "25919018",
                                    "name": "Fabio Azzalini"
                                },
                                {
                                    "authorId": "1709652",
                                    "name": "M. Tanelli"
                                }
                            ],
                            "year": 2023,
                            "venue": "Data Science and Engineering",
                            "n_citations": 10
                        },
                        "score": 0.82568359375
                    },
                    {
                        "id": "(Blodgett et al., 2020)",
                        "snippets": [
                            "We survey 146 papers analyzing \"bias\" in NLP systems, finding that their motivations are often vague, inconsistent, and lacking in normative reasoning, despite the fact that analyzing \"bias\" is an inherently normative process. We further find that these papers\u2019 proposed quantitative techniques for measuring or mitigating \"bias\" are poorly matched to their motivations and do not engage with the relevant literature outside of NLP. Based on these findings, we describe the beginnings of a path forward by proposing three recommendations that should guide work analyzing \"bias\" in NLP systems. These recommendations rest on a greater recognition of the relationships between language and social hierarchies, encouraging researchers and practitioners to articulate their conceptualizations of \"bias\"---i.e., what kinds of system behaviors are harmful, in what ways, to whom, and why, as well as the normative reasoning underlying these statements\u2014and to center work around the lived experiences of members of communities affected by NLP systems, while interrogating and reimagining the power relations between technologists and such communities."
                        ],
                        "paper": {
                            "corpus_id": 218971825,
                            "title": "Language (Technology) is Power: A Critical Survey of \u201cBias\u201d in NLP",
                            "authors": [
                                {
                                    "authorId": "3422038",
                                    "name": "Su Lin Blodgett"
                                },
                                {
                                    "authorId": "2881033",
                                    "name": "Solon Barocas"
                                },
                                {
                                    "authorId": "2065041692",
                                    "name": "Hal Daum'e"
                                },
                                {
                                    "authorId": "1831395",
                                    "name": "Hanna M. Wallach"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 1249
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.344541
    }
}