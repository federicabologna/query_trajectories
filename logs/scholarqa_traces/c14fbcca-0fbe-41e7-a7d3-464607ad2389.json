{
    "query": "What are recent advances in synthetic data generation techniques for solving hard reasoning tasks?",
    "user_id": "lib_user",
    "task_id": "c14fbcca-0fbe-41e7-a7d3-464607ad2389",
    "timestamp": "2025-06-23T22:00:59.767696",
    "n_retrieval": 256,
    "n_retrieved": 264,
    "n_candidates": 46,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.6010229999999999,
    "decomposed_query": {
        "rewritten_query": "Advances in synthetic data generation techniques for solving hard reasoning tasks.",
        "keyword_query": "synthetic data generation techniques hard reasoning tasks",
        "search_filters": {
            "year": "2022-2025",
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.009576,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Neuro-Symbolic Scene Graph Conditioning for Synthetic Image Dataset Generation",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 30,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.17224, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2351599231",
                    "name": "Giacomo Savazzi"
                },
                {
                    "authorId": "2221012084",
                    "name": "Eugenio Lomurno"
                },
                {
                    "authorId": "2172115265",
                    "name": "Cristian Sbrolli"
                },
                {
                    "authorId": "2607532",
                    "name": "Agnese Chiatti"
                },
                {
                    "authorId": "2304551810",
                    "name": "Matteo Matteucci"
                }
            ],
            "abstract": "As machine learning models increase in scale and complexity, obtaining sufficient training data has become a critical bottleneck due to acquisition costs, privacy constraints, and data scarcity in specialised domains. While synthetic data generation has emerged as a promising alternative, a notable performance gap remains compared to models trained on real data, particularly as task complexity grows. Concurrently, Neuro-Symbolic methods, which combine neural networks' learning strengths with symbolic reasoning's structured representations, have demonstrated significant potential across various cognitive tasks. This paper explores the utility of Neuro-Symbolic conditioning for synthetic image dataset generation, focusing specifically on improving the performance of Scene Graph Generation models. The research investigates whether structured symbolic representations in the form of scene graphs can enhance synthetic data quality through explicit encoding of relational constraints. The results demonstrate that Neuro-Symbolic conditioning yields significant improvements of up to +2.59% in standard Recall metrics and +2.83% in No Graph Constraint Recall metrics when used for dataset augmentation. These findings establish that merging Neuro-Symbolic and generative approaches produces synthetic data with complementary structural information that enhances model performance when combined with real data, providing a novel approach to overcome data scarcity limitations even for complex visual reasoning tasks.",
            "corpus_id": 277244110,
            "sentences": [
                {
                    "corpus_id": "277244110",
                    "title": "Neuro-Symbolic Scene Graph Conditioning for Synthetic Image Dataset Generation",
                    "text": "Recently, research on synthetic dataset generation has emerged as a critical direction for advancing deep learning architectures. As models continue to scale in complexity and parameter count, acquiring and annotating large-scale real-world datasets remains prohibitively resource-intensive due to costs, privacy regulations, and domain-specific data scarcity. Synthetic datasets offer a scalable alternative with reduced annotation overhead and controllable attribute distribution. Despite these advantages, models trained exclusively on synthetic data consistently underperform compared to those trained on real datasets, particularly on high-level reasoning tasks. This performance gap widens further as task complexity increases, even when scaling dataset size. To address these limitations, various conditioning techniques for generative models have been explored. \n\nSimultaneously, Neuro-Symbolic (NeSy) approaches have gained traction in the machine learning community [1]. These methods integrate neural representation learning with symbolic reasoning frameworks, combining data-driven optimization with structured knowledge. While NeSy architectures have demonstrated enhanced generalization in reasoning tasks, their potential for improving synthetic data generation remains largely unexplored. Scene graphs-structured representations where nodes represent objects and edges represent relations-offer a promising symbolic framework for conditioning image generation. This work explores the integration of such NeSy approaches into dataset generation to improve performance on complex tasks, particularly Scene Graph Generation (SGG) from images. The hypothesis posits that scene graphs encode useful background knowledge that can guide the generation process, ensuring synthetic data adhere to structural and semantic constraints. To the best of current knowledge, this represents the first systematic exploration of NeSy conditioning for synthetic dataset generation in the context of SGG tasks. The experimental framework employs SGAdapter [2] for Scene Graph-to-Image generation, with Stable Diffusion 2.0 [3] as the baseline model. CausalTDE [4] serves as the SGG evaluation model. \n\nThe key contributions of this work are threefold: (1) proposing a novel framework that integrates scene graph-based symbolic knowledge into synthetic dataset generation; (2) demonstrating that structurally-guided generation produces complementary training signals that enhance augmentation efficacy despite lower perceptual fidelity; and (3) providing empirical evidence that NeSy conditioning addresses specific limitations in conventional synthetic data generation for complex visual reasoning tasks.",
                    "score": 0.4568283775548592,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 129
                        },
                        {
                            "start": 130,
                            "end": 360
                        },
                        {
                            "start": 361,
                            "end": 482
                        },
                        {
                            "start": 483,
                            "end": 667
                        },
                        {
                            "start": 668,
                            "end": 765
                        },
                        {
                            "start": 766,
                            "end": 869
                        },
                        {
                            "start": 872,
                            "end": 980
                        },
                        {
                            "start": 981,
                            "end": 1133
                        },
                        {
                            "start": 1134,
                            "end": 1304
                        },
                        {
                            "start": 1305,
                            "end": 1476
                        },
                        {
                            "start": 1477,
                            "end": 1655
                        },
                        {
                            "start": 1656,
                            "end": 1840
                        },
                        {
                            "start": 1841,
                            "end": 2006
                        },
                        {
                            "start": 2007,
                            "end": 2145
                        },
                        {
                            "start": 2146,
                            "end": 2195
                        },
                        {
                            "start": 2198,
                            "end": 2700
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 976,
                            "end": 979,
                            "matchedPaperCorpusId": "234469907"
                        },
                        {
                            "start": 2119,
                            "end": 2122,
                            "matchedPaperCorpusId": "219955663"
                        },
                        {
                            "start": 2156,
                            "end": 2159,
                            "matchedPaperCorpusId": "211532586"
                        },
                        {
                            "start": 2248,
                            "end": 2251,
                            "matchedPaperCorpusId": "234469907"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.984375
                }
            ],
            "relevance_judgement": 0.984375,
            "relevance_judgment_input_expanded": "# Title: Neuro-Symbolic Scene Graph Conditioning for Synthetic Image Dataset Generation\n# Venue: arXiv.org\n# Authors: Giacomo Savazzi, Eugenio Lomurno, Cristian Sbrolli, Agnese Chiatti, Matteo Matteucci\n## Abstract\nAs machine learning models increase in scale and complexity, obtaining sufficient training data has become a critical bottleneck due to acquisition costs, privacy constraints, and data scarcity in specialised domains. While synthetic data generation has emerged as a promising alternative, a notable performance gap remains compared to models trained on real data, particularly as task complexity grows. Concurrently, Neuro-Symbolic methods, which combine neural networks' learning strengths with symbolic reasoning's structured representations, have demonstrated significant potential across various cognitive tasks. This paper explores the utility of Neuro-Symbolic conditioning for synthetic image dataset generation, focusing specifically on improving the performance of Scene Graph Generation models. The research investigates whether structured symbolic representations in the form of scene graphs can enhance synthetic data quality through explicit encoding of relational constraints. The results demonstrate that Neuro-Symbolic conditioning yields significant improvements of up to +2.59% in standard Recall metrics and +2.83% in No Graph Constraint Recall metrics when used for dataset augmentation. These findings establish that merging Neuro-Symbolic and generative approaches produces synthetic data with complementary structural information that enhances model performance when combined with real data, providing a novel approach to overcome data scarcity limitations even for complex visual reasoning tasks.\n## Introduction\nRecently, research on synthetic dataset generation has emerged as a critical direction for advancing deep learning architectures. As models continue to scale in complexity and parameter count, acquiring and annotating large-scale real-world datasets remains prohibitively resource-intensive due to costs, privacy regulations, and domain-specific data scarcity. Synthetic datasets offer a scalable alternative with reduced annotation overhead and controllable attribute distribution. Despite these advantages, models trained exclusively on synthetic data consistently underperform compared to those trained on real datasets, particularly on high-level reasoning tasks. This performance gap widens further as task complexity increases, even when scaling dataset size. To address these limitations, various conditioning techniques for generative models have been explored. \n\nSimultaneously, Neuro-Symbolic (NeSy) approaches have gained traction in the machine learning community [1]. These methods integrate neural representation learning with symbolic reasoning frameworks, combining data-driven optimization with structured knowledge. While NeSy architectures have demonstrated enhanced generalization in reasoning tasks, their potential for improving synthetic data generation remains largely unexplored. Scene graphs-structured representations where nodes represent objects and edges represent relations-offer a promising symbolic framework for conditioning image generation. This work explores the integration of such NeSy approaches into dataset generation to improve performance on complex tasks, particularly Scene Graph Generation (SGG) from images. The hypothesis posits that scene graphs encode useful background knowledge that can guide the generation process, ensuring synthetic data adhere to structural and semantic constraints. To the best of current knowledge, this represents the first systematic exploration of NeSy conditioning for synthetic dataset generation in the context of SGG tasks. The experimental framework employs SGAdapter [2] for Scene Graph-to-Image generation, with Stable Diffusion 2.0 [3] as the baseline model. CausalTDE [4] serves as the SGG evaluation model. \n\nThe key contributions of this work are threefold: (1) proposing a novel framework that integrates scene graph-based symbolic knowledge into synthetic dataset generation; (2) demonstrating that structurally-guided generation produces complementary training signals that enhance augmentation efficacy despite lower perceptual fidelity; and (3) providing empirical evidence that NeSy conditioning addresses specific limitations in conventional synthetic data generation for complex visual reasoning tasks.",
            "reference_string": "[277244110 | Savazzi et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Synthesize-on-Graph: Knowledgeable Synthetic Data Generation for Continue Pre-training of Large Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 22,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.00979, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "144267788",
                    "name": "Xuhui Jiang"
                },
                {
                    "authorId": "2311556497",
                    "name": "Shengjie Ma"
                },
                {
                    "authorId": "2250617116",
                    "name": "Chengjin Xu"
                },
                {
                    "authorId": "2307038391",
                    "name": "Cehao Yang"
                },
                {
                    "authorId": "2359053210",
                    "name": "Liyu Zhang"
                },
                {
                    "authorId": "2284217200",
                    "name": "Jian Guo"
                }
            ],
            "abstract": "Large Language Models (LLMs) have achieved remarkable success but remain data-inefficient, especially when learning from small, specialized corpora with limited and proprietary data. Existing synthetic data generation methods for continue pre-training focus on intra-document content and overlook cross-document knowledge associations, limiting content diversity and depth. We propose Synthetic-on-Graph (SoG), a synthetic data generation framework that incorporates cross-document knowledge associations for efficient corpus expansion. SoG constructs a context graph by extracting entities and concepts from the original corpus, representing cross-document associations, and employing a graph walk strategy for knowledge-associated sampling. This enhances synthetic data diversity and coherence, enabling models to learn complex knowledge structures and handle rare knowledge. To further improve synthetic data quality, we integrate Chain-of-Thought (CoT) and Contrastive Clarifying (CC) synthetic, enhancing reasoning processes and discriminative power. Experiments show that SoG outperforms the state-of-the-art (SOTA) method in a multi-hop document Q&A dataset while performing comparably to the SOTA method on the reading comprehension task datasets, which also underscores the better generalization capability of SoG. Our work advances synthetic data generation and provides practical solutions for efficient knowledge acquisition in LLMs, especially in domains with limited data availability.",
            "corpus_id": 278310934,
            "sentences": [
                {
                    "corpus_id": "278310934",
                    "title": "Synthesize-on-Graph: Knowledgeable Synthetic Data Generation for Continue Pre-training of Large Language Models",
                    "text": "Large Language Models (LLMs) have achieved remarkable success but remain data-inefficient, especially when learning from small, specialized corpora with limited and proprietary data. Existing synthetic data generation methods for continue pre-training focus on intra-document content and overlook cross-document knowledge associations, limiting content diversity and depth. We propose Synthetic-on-Graph (SoG), a synthetic data generation framework that incorporates cross-document knowledge associations for efficient corpus expansion. SoG constructs a context graph by extracting entities and concepts from the original corpus, representing cross-document associations, and employing a graph walk strategy for knowledge-associated sampling. This enhances synthetic data diversity and coherence, enabling models to learn complex knowledge structures and handle rare knowledge. To further improve synthetic data quality, we integrate Chain-of-Thought (CoT) and Contrastive Clarifying (CC) synthetic, enhancing reasoning processes and discriminative power. Experiments show that SoG outperforms the state-of-the-art (SOTA) method in a multi-hop document Q&A dataset while performing comparably to the SOTA method on the reading comprehension task datasets, which also underscores the better generalization capability of SoG. Our work advances synthetic data generation and provides practical solutions for efficient knowledge acquisition in LLMs, especially in domains with limited data availability.",
                    "score": 0.34991793658522774,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98193359375
                }
            ],
            "relevance_judgement": 0.98193359375,
            "relevance_judgment_input_expanded": "# Title: Synthesize-on-Graph: Knowledgeable Synthetic Data Generation for Continue Pre-training of Large Language Models\n# Venue: arXiv.org\n# Authors: Xuhui Jiang, Shengjie Ma, Chengjin Xu, Cehao Yang, Liyu Zhang, Jian Guo\n## Abstract\nLarge Language Models (LLMs) have achieved remarkable success but remain data-inefficient, especially when learning from small, specialized corpora with limited and proprietary data. Existing synthetic data generation methods for continue pre-training focus on intra-document content and overlook cross-document knowledge associations, limiting content diversity and depth. We propose Synthetic-on-Graph (SoG), a synthetic data generation framework that incorporates cross-document knowledge associations for efficient corpus expansion. SoG constructs a context graph by extracting entities and concepts from the original corpus, representing cross-document associations, and employing a graph walk strategy for knowledge-associated sampling. This enhances synthetic data diversity and coherence, enabling models to learn complex knowledge structures and handle rare knowledge. To further improve synthetic data quality, we integrate Chain-of-Thought (CoT) and Contrastive Clarifying (CC) synthetic, enhancing reasoning processes and discriminative power. Experiments show that SoG outperforms the state-of-the-art (SOTA) method in a multi-hop document Q&A dataset while performing comparably to the SOTA method on the reading comprehension task datasets, which also underscores the better generalization capability of SoG. Our work advances synthetic data generation and provides practical solutions for efficient knowledge acquisition in LLMs, especially in domains with limited data availability.\n",
            "reference_string": "[278310934 | Jiang et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Synthetic Data Generation Using Large Language Models: Advances in Text and Code",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 62,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.14023, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2350754914",
                    "name": "Mihai Nadas"
                },
                {
                    "authorId": "2306585020",
                    "name": "Laura Dio\u015fan"
                },
                {
                    "authorId": "2350756350",
                    "name": "Andreea Tomescu"
                }
            ],
            "abstract": "Large language models (LLMs) have unlocked new possibilities for generating synthetic training data in both natural language and code. By producing artificial but task-relevant examples, these models can significantly augment or even replace real-world datasets, especially when labeled data is scarce or sensitive. This paper surveys recent advances in using LLMs to create synthetic text and code, emphasizing prompt-based generation, retrieval-augmented pipelines, and iterative self-refinement. We show how these methods enrich low-resource tasks such as classification and question answering, as well as code-centric applications such as instruction tuning, code translation, and bug repair, by enabling automated verification of functional correctness. Alongside potential benefits like cost-effectiveness, broad coverage, and controllable diversity, we address challenges such as factual inaccuracies in generated text, lack of stylistic realism, and the risk of bias amplification. Proposed mitigations include filtering and weighting outputs and reinforcement learning with execution feedback for code. We conclude with open research directions like automated prompt engineering, cross-modal data synthesis, and robust evaluation frameworks, highlighting the importance of LLM-generated synthetic data in advancing AI while emphasizing ethical and quality safeguards.",
            "corpus_id": 277104955,
            "sentences": [
                {
                    "corpus_id": "277104955",
                    "title": "Synthetic Data Generation Using Large Language Models: Advances in Text and Code",
                    "text": "Large Language Models (LLMs) have achieved remarkable success across natural language and code generation tasks, owing in part to training on massive datasets. However, acquiring sufficient high-quality training data remains a bottleneck in many domains [60]. Data scarcity, high annotation costs, and privacy constraints often limit the availability of large supervised corpora. These challenges have spurred growing interest in synthetic data generation, where additional training examples are produced artificially rather than collected from the real world. Recent advances in generative AI -particularly LLMs like Anthropic's Claude 3.7 Sonnet, DeepSeek's R1, Meta's Llama 3, and OpenAI's GPT-o3 -provide powerful new tools to generate synthetic text and code that mimic real data distributions. This paper surveys and analyzes the latest developments in LLM-driven synthetic data generation for both natural language text and programming code, highlighting techniques, applications, challenges, and future directions. \n\nLLMs can produce human-like text and code, making them attractive data generators for tasks where obtaining real data is costly or infeasible. For example, instead of manually labeling thousands of sentences for a classifier, one can prompt an LLM to create diverse labeled examples [33], or have it generate code snippets to augment code model training. Synthetic data generated by LLMs has shown promise in boosting model performance in low-resource settings, reducing annotation costs, and enabling data augmentation for improved robustness [11]. At the same time, using LLMs as data generators raises questions about quality control, realism, and biases in the generated data. \n\nA comprehensive review of LLM-based approaches for text and code generation is currently lacking, despite their increasing adoption [52]. This paper aims to fill that gap by providing a structured survey of existing techniques and challenges. The focus is specifically on text and code generation, as opposed to other forms of data, due to several key reasons. First, visual data synthesis has been extensively studied, making an additional review redundant [10,8]. Second, while synthetic signal data (e.g., audio) is a developing field, it remains outside the scope of this study, though future work could extend these analyses [24].",
                    "score": 0.4267670029697321,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 159
                        },
                        {
                            "start": 160,
                            "end": 259
                        },
                        {
                            "start": 260,
                            "end": 379
                        },
                        {
                            "start": 380,
                            "end": 560
                        },
                        {
                            "start": 561,
                            "end": 799
                        },
                        {
                            "start": 800,
                            "end": 1022
                        },
                        {
                            "start": 1025,
                            "end": 1167
                        },
                        {
                            "start": 1168,
                            "end": 1379
                        },
                        {
                            "start": 1380,
                            "end": 1574
                        },
                        {
                            "start": 1575,
                            "end": 1705
                        },
                        {
                            "start": 1708,
                            "end": 1845
                        },
                        {
                            "start": 1846,
                            "end": 1950
                        },
                        {
                            "start": 1951,
                            "end": 2068
                        },
                        {
                            "start": 2069,
                            "end": 2173
                        },
                        {
                            "start": 2174,
                            "end": 2343
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 254,
                            "end": 258,
                            "matchedPaperCorpusId": "273458929"
                        },
                        {
                            "start": 1569,
                            "end": 1573,
                            "matchedPaperCorpusId": "254877171"
                        },
                        {
                            "start": 2166,
                            "end": 2170,
                            "matchedPaperCorpusId": "234357997"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97607421875
                },
                {
                    "corpus_id": "277104955",
                    "title": "Synthetic Data Generation Using Large Language Models: Advances in Text and Code",
                    "text": "Synthetic data generation using large language models has emerged as a powerful technique to address data scarcity and enhance model training in both natural language and programming language domains. We have reviewed the state-of-the-art advances in this area, highlighting how LLMs can generate high-quality text and code data that, when used judiciously, lead to significant performance gains on downstream tasks. \n\nIn the text domain, LLM-generated data has proven especially useful for low-resource scenarios, delivering substantial improvements in tasks like classification and QA when human-labeled data is limited [33]. Techniques such as promptbased augmentation (zero-shot, few-shot, etc.), retrieval augmentation for grounding facts [3], and iterative refinement have pushed synthetic text data closer in effectiveness to real data. \n\nIn the code domain, LLMs have unlocked new possibilities by generating code snippets, programming instructions, and even whole problem solutions, facilitating better training of code models. The ability to verify code correctness via execution [37] provides a strong advantage, allowing the curation of large-scale, correct-by-construction synthetic code datasets (e.g., Code Alpaca, WizardCoder) that have propelled open-source code models to approach the competency of their proprietary counterparts. \n\nThroughout this survey, we have also underscored the practical considerations: synthetic data must be used with care to ensure quality and avoid pitfalls like distribution shift, bias amplification, or model collapse from feedback loops. Encouraging findings, such as the avoidance of model collapse by combining synthetic with real data [18], give confidence that these pitfalls can be managed with thoughtful strategies. The community has developed numerous methods (from filtering heuristics to advanced RL techniques) to maximize the signal and minimize the noise in synthetic datasets [33]. \n\nOur discussion on challenges reflects that this field is still evolving. Issues of factual accuracy, diversity, and evaluation metrics remain open for continued research. However, the rapid progress in just the last two years -with surveys cataloging dozens of new works [6] -indicates a vibrant research momentum. We are likely to see even more innovative solutions, like integrated human-in-the-loop generation and domain-specific synthetic data models, being realized soon.",
                    "score": 0.3572887998312396,
                    "section_title": "Conclusion",
                    "char_start_offset": 64076,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 200
                        },
                        {
                            "start": 201,
                            "end": 416
                        },
                        {
                            "start": 419,
                            "end": 627
                        },
                        {
                            "start": 628,
                            "end": 843
                        },
                        {
                            "start": 846,
                            "end": 1036
                        },
                        {
                            "start": 1037,
                            "end": 1348
                        },
                        {
                            "start": 1351,
                            "end": 1588
                        },
                        {
                            "start": 1589,
                            "end": 1773
                        },
                        {
                            "start": 1774,
                            "end": 1946
                        },
                        {
                            "start": 1949,
                            "end": 2021
                        },
                        {
                            "start": 2022,
                            "end": 2119
                        },
                        {
                            "start": 2120,
                            "end": 2263
                        },
                        {
                            "start": 2264,
                            "end": 2425
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97412109375
                },
                {
                    "corpus_id": "277104955",
                    "title": "Synthetic Data Generation Using Large Language Models: Advances in Text and Code",
                    "text": "Second, while synthetic signal data (e.g., audio) is a developing field, it remains outside the scope of this study, though future work could extend these analyses [24]. Lastly, text and code share a common modality, as both are purely symbolic representations that can be generated and processed using the same fundamental methods-unlike images, sounds, or videos, which require distinct encoding formats and specialized equipment [29]. \n\nThis paper provides a comprehensive analysis of LLM-based synthetic data generation, with contributions summarized as follows: \n\n\u2022 Survey of Techniques: We review major approaches for LLM-driven data generation in text and code, including prompt-based augmentation, retrieval-augmented generation, self-instruct methods, and reinforcement learning with feedback. We categorize methods by their strategies (e.g., zero-shot vs. few-shot prompting, knowledge integration, iterative refinement) and discuss representative examples from recent literature [3] \u2022 Advances in Text Data Generation: We explore how LLMs are used to create synthetic text datasets for tasks such as classification, question answering, and instruction-following. Key studies are examined to understand the impact of synthetic data on model performance, data diversity, and efficiency. We highlight empirical findings (e.g., improvements of 3-26% with synthetic augmentation in low-data regimes) and analyze how prompt design and data curation affect quality [33]. \n\n\u2022 Advances in Code Data Generation: We survey the parallel emergence of LLM-generated code data for training code intelligence models. We discuss techniques for generating code through LLM prompts, executing code to validate correctness, and synthesizing coding instruction data at scale [37]. Examples include synthetic programming problems and solutions, code edits for diversity, and instruction-tuning datasets (e.g., Code Alpaca, WizardCoder) generated by LLMs. \n\n\u2022 Challenges and Mitigations: We outline the main challenges in using LLMs for synthetic data, such as ensuring data fidelity and factuality, avoiding modeling biases, maintaining diversity without drifting from real data distribution, and preventing model collapse from iterative self-training on AI-generated data.",
                    "score": 0.37474108971800546,
                    "section_title": "Introduction",
                    "char_start_offset": 2189,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 169
                        },
                        {
                            "start": 170,
                            "end": 437
                        },
                        {
                            "start": 440,
                            "end": 566
                        },
                        {
                            "start": 569,
                            "end": 802
                        },
                        {
                            "start": 803,
                            "end": 1173
                        },
                        {
                            "start": 1174,
                            "end": 1295
                        },
                        {
                            "start": 1296,
                            "end": 1474
                        },
                        {
                            "start": 1477,
                            "end": 1611
                        },
                        {
                            "start": 1612,
                            "end": 1770
                        },
                        {
                            "start": 1771,
                            "end": 1943
                        },
                        {
                            "start": 1946,
                            "end": 2262
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94580078125
                },
                {
                    "corpus_id": "277104955",
                    "title": "Synthetic Data Generation Using Large Language Models: Advances in Text and Code",
                    "text": "We are likely to see even more innovative solutions, like integrated human-in-the-loop generation and domain-specific synthetic data models, being realized soon. \n\nIn conclusion, LLM-based synthetic data generation stands as a promising paradigm shift in how we obtain training data for AI. By leveraging the generative power of advanced models, we reduce our reliance on large hand-curated datasets and open up opportunities to develop AI systems in domains and languages that suffer from data scarcity. This democratizes the development of AI, enabling customization and improvement even when real data is lacking. \n\nAs we refine these techniques, backed by rigorous evaluations and ethical safeguards, synthetic data generation will become an increasingly standard part of the machine learning toolkit-complementing real data, accelerating development, and perhaps one day even supplanting the need for certain kinds of manual data collection altogether. \n\nThe synergy between learning from data and generating data to learn from encapsulates a fascinating frontier for AI research, one that blurs the line between model and data and pushes us toward more autonomous, self-improving AI systems.",
                    "score": 0.4024982055363106,
                    "section_title": "Conclusion",
                    "char_start_offset": 66340,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 161
                        },
                        {
                            "start": 164,
                            "end": 290
                        },
                        {
                            "start": 291,
                            "end": 504
                        },
                        {
                            "start": 505,
                            "end": 616
                        },
                        {
                            "start": 619,
                            "end": 957
                        },
                        {
                            "start": 960,
                            "end": 1197
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93896484375
                }
            ],
            "relevance_judgement": 0.97607421875,
            "relevance_judgment_input_expanded": "# Title: Synthetic Data Generation Using Large Language Models: Advances in Text and Code\n# Venue: arXiv.org\n# Authors: Mihai Nadas, Laura Dio\u015fan, Andreea Tomescu\n## Abstract\nLarge language models (LLMs) have unlocked new possibilities for generating synthetic training data in both natural language and code. By producing artificial but task-relevant examples, these models can significantly augment or even replace real-world datasets, especially when labeled data is scarce or sensitive. This paper surveys recent advances in using LLMs to create synthetic text and code, emphasizing prompt-based generation, retrieval-augmented pipelines, and iterative self-refinement. We show how these methods enrich low-resource tasks such as classification and question answering, as well as code-centric applications such as instruction tuning, code translation, and bug repair, by enabling automated verification of functional correctness. Alongside potential benefits like cost-effectiveness, broad coverage, and controllable diversity, we address challenges such as factual inaccuracies in generated text, lack of stylistic realism, and the risk of bias amplification. Proposed mitigations include filtering and weighting outputs and reinforcement learning with execution feedback for code. We conclude with open research directions like automated prompt engineering, cross-modal data synthesis, and robust evaluation frameworks, highlighting the importance of LLM-generated synthetic data in advancing AI while emphasizing ethical and quality safeguards.\n## Introduction\nLarge Language Models (LLMs) have achieved remarkable success across natural language and code generation tasks, owing in part to training on massive datasets. However, acquiring sufficient high-quality training data remains a bottleneck in many domains [60]. Data scarcity, high annotation costs, and privacy constraints often limit the availability of large supervised corpora. These challenges have spurred growing interest in synthetic data generation, where additional training examples are produced artificially rather than collected from the real world. Recent advances in generative AI -particularly LLMs like Anthropic's Claude 3.7 Sonnet, DeepSeek's R1, Meta's Llama 3, and OpenAI's GPT-o3 -provide powerful new tools to generate synthetic text and code that mimic real data distributions. This paper surveys and analyzes the latest developments in LLM-driven synthetic data generation for both natural language text and programming code, highlighting techniques, applications, challenges, and future directions. \n\nLLMs can produce human-like text and code, making them attractive data generators for tasks where obtaining real data is costly or infeasible. For example, instead of manually labeling thousands of sentences for a classifier, one can prompt an LLM to create diverse labeled examples [33], or have it generate code snippets to augment code model training. Synthetic data generated by LLMs has shown promise in boosting model performance in low-resource settings, reducing annotation costs, and enabling data augmentation for improved robustness [11]. At the same time, using LLMs as data generators raises questions about quality control, realism, and biases in the generated data. \n\nA comprehensive review of LLM-based approaches for text and code generation is currently lacking, despite their increasing adoption [52]. This paper aims to fill that gap by providing a structured survey of existing techniques and challenges. The focus is specifically on text and code generation, as opposed to other forms of data, due to several key reasons. First, visual data synthesis has been extensively studied, making an additional review redundant [10,8]. Second, while synthetic signal data (e.g., audio) is a developing field, it remains outside the scope of this study, though future work could extend these analyses [24].\n...\nSecond, while synthetic signal data (e.g., audio) is a developing field, it remains outside the scope of this study, though future work could extend these analyses [24]. Lastly, text and code share a common modality, as both are purely symbolic representations that can be generated and processed using the same fundamental methods-unlike images, sounds, or videos, which require distinct encoding formats and specialized equipment [29]. \n\nThis paper provides a comprehensive analysis of LLM-based synthetic data generation, with contributions summarized as follows: \n\n\u2022 Survey of Techniques: We review major approaches for LLM-driven data generation in text and code, including prompt-based augmentation, retrieval-augmented generation, self-instruct methods, and reinforcement learning with feedback. We categorize methods by their strategies (e.g., zero-shot vs. few-shot prompting, knowledge integration, iterative refinement) and discuss representative examples from recent literature [3] \u2022 Advances in Text Data Generation: We explore how LLMs are used to create synthetic text datasets for tasks such as classification, question answering, and instruction-following. Key studies are examined to understand the impact of synthetic data on model performance, data diversity, and efficiency. We highlight empirical findings (e.g., improvements of 3-26% with synthetic augmentation in low-data regimes) and analyze how prompt design and data curation affect quality [33]. \n\n\u2022 Advances in Code Data Generation: We survey the parallel emergence of LLM-generated code data for training code intelligence models. We discuss techniques for generating code through LLM prompts, executing code to validate correctness, and synthesizing coding instruction data at scale [37]. Examples include synthetic programming problems and solutions, code edits for diversity, and instruction-tuning datasets (e.g., Code Alpaca, WizardCoder) generated by LLMs. \n\n\u2022 Challenges and Mitigations: We outline the main challenges in using LLMs for synthetic data, such as ensuring data fidelity and factuality, avoiding modeling biases, maintaining diversity without drifting from real data distribution, and preventing model collapse from iterative self-training on AI-generated data.\n\n## Conclusion\nSynthetic data generation using large language models has emerged as a powerful technique to address data scarcity and enhance model training in both natural language and programming language domains. We have reviewed the state-of-the-art advances in this area, highlighting how LLMs can generate high-quality text and code data that, when used judiciously, lead to significant performance gains on downstream tasks. \n\nIn the text domain, LLM-generated data has proven especially useful for low-resource scenarios, delivering substantial improvements in tasks like classification and QA when human-labeled data is limited [33]. Techniques such as promptbased augmentation (zero-shot, few-shot, etc.), retrieval augmentation for grounding facts [3], and iterative refinement have pushed synthetic text data closer in effectiveness to real data. \n\nIn the code domain, LLMs have unlocked new possibilities by generating code snippets, programming instructions, and even whole problem solutions, facilitating better training of code models. The ability to verify code correctness via execution [37] provides a strong advantage, allowing the curation of large-scale, correct-by-construction synthetic code datasets (e.g., Code Alpaca, WizardCoder) that have propelled open-source code models to approach the competency of their proprietary counterparts. \n\nThroughout this survey, we have also underscored the practical considerations: synthetic data must be used with care to ensure quality and avoid pitfalls like distribution shift, bias amplification, or model collapse from feedback loops. Encouraging findings, such as the avoidance of model collapse by combining synthetic with real data [18], give confidence that these pitfalls can be managed with thoughtful strategies. The community has developed numerous methods (from filtering heuristics to advanced RL techniques) to maximize the signal and minimize the noise in synthetic datasets [33]. \n\nOur discussion on challenges reflects that this field is still evolving. Issues of factual accuracy, diversity, and evaluation metrics remain open for continued research. However, the rapid progress in just the last two years -with surveys cataloging dozens of new works [6] -indicates a vibrant research momentum. We are likely to see even more innovative solutions, like integrated human-in-the-loop generation and domain-specific synthetic data models, being realized soon.\n...\nWe are likely to see even more innovative solutions, like integrated human-in-the-loop generation and domain-specific synthetic data models, being realized soon. \n\nIn conclusion, LLM-based synthetic data generation stands as a promising paradigm shift in how we obtain training data for AI. By leveraging the generative power of advanced models, we reduce our reliance on large hand-curated datasets and open up opportunities to develop AI systems in domains and languages that suffer from data scarcity. This democratizes the development of AI, enabling customization and improvement even when real data is lacking. \n\nAs we refine these techniques, backed by rigorous evaluations and ethical safeguards, synthetic data generation will become an increasingly standard part of the machine learning toolkit-complementing real data, accelerating development, and perhaps one day even supplanting the need for certain kinds of manual data collection altogether. \n\nThe synergy between learning from data and generating data to learn from encapsulates a fascinating frontier for AI research, one that blurs the line between model and data and pushes us toward more autonomous, self-improving AI systems.",
            "reference_string": "[277104955 | Nadas et al. | 2025 | Citations: 3]"
        },
        {
            "title": "Unleashing LLM Reasoning Capability via Scalable Question Synthesis from Scratch",
            "venue": "",
            "year": 2024,
            "reference_count": 57,
            "citation_count": 8,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.18693, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2243263723",
                    "name": "Yuyang Ding"
                },
                {
                    "authorId": "2327658780",
                    "name": "Xinyu Shi"
                },
                {
                    "authorId": "48083523",
                    "name": "Xiaobo Liang"
                },
                {
                    "authorId": "2257093356",
                    "name": "Juntao Li"
                },
                {
                    "authorId": "2363558591",
                    "name": "Zhaopeng Tu"
                },
                {
                    "authorId": "2243490219",
                    "name": "Qiaoming Zhu"
                },
                {
                    "authorId": "2258690233",
                    "name": "Min Zhang"
                }
            ],
            "abstract": "Improving the mathematical reasoning capabilities of Large Language Models (LLMs) is critical for advancing artificial intelligence. However, access to extensive, diverse, and high-quality reasoning datasets remains a significant challenge, particularly for the open-source community. In this paper, we propose ScaleQuest, a novel, scalable, and cost-effective data synthesis method that enables the generation of large-scale mathematical reasoning datasets using lightweight 7B-scale models. ScaleQuest introduces a two-stage question-tuning process comprising Question Fine-Tuning (QFT) and Question Preference Optimization (QPO) to unlock the question generation capabilities of problem-solving models. By generating diverse questions from scratch -- without relying on powerful proprietary models or seed data -- we produce a dataset of 1 million problem-solution pairs. Our experiments demonstrate that models trained on our data outperform existing open-source datasets in both in-domain and out-of-domain evaluations. Furthermore, our approach shows continued performance improvement as the volume of training data increases, highlighting its potential for ongoing data scaling. The extensive improvements observed in code reasoning tasks demonstrate the generalization capabilities of our proposed method. Our work provides the open-source community with a practical solution to enhance the mathematical reasoning abilities of LLMs.",
            "corpus_id": 273549775,
            "sentences": [
                {
                    "corpus_id": "273549775",
                    "title": "Unleashing LLM Reasoning Capability via Scalable Question Synthesis from Scratch",
                    "text": "In this work, we propose ScaleQuest, a novel data synthesis framework that unlocks the ability of open-source smaller models to independently generate large-scale, high-quality reasoning data from scratch, at a low cost. By training the problem-solving models on a small subset of questions, we effectively activate their question-generation capabilities. We also introduce a response enhancement method. With these techniques, we successfully developed a fully synthetic math reasoning dataset consisting of 1 million question-answer pairs. Using this dataset, we fine-tuned the model and achieved remarkable improvements, with gains ranging from 29.2% to 46.4% compared to the base model. The fine-tuned 7B model, Qwen2-Math-7B-ScaleQuest, outperforms all competitors in the 7B-70B range and even surpasses proprietary models like GPT-4-Turbo and Claude-3.5-Sonnet. \n\nDue to time and cost constraints, there are several areas where our approach can be further optimized. \n\nFor instance, leveraging more powerful, larger problem-solving models like Qwen2.5-Math-72B-Instruct (Yang et al., 2024b) for question and response generation, using advanced models such as GPT-4o for constructing preference data for Question Preference Optimization, and further scaling up the generation of synthetic data. Each stage of our process has significant room for improvement. \n\nIn this paper, we have demonstrated the potential of this framework, laying the groundwork for future enhancements. \n\nFurthermore, despite the progress made in this work, there are still several limitations that need to be addressed. In our future research, we will concentrate on the following areas: \n\n\u2022 Large-scale and diverse high-quality data: This work chooses mathematical reasoning as a case study to demonstrate the effectiveness of our method. In the future, we will focus on broader and more complex tasks such as science and competitive programming. Additionally, future research will aim to continuously scale data synthesis to explore the scaling laws for synthetic data and seek a more efficient approach to scaling data generation. \u2022 Self-improvement capability: Our experiments demonstrate the model's self-improvement capability, meaning that it can generate data of higher quality than its original training set.",
                    "score": 0.41141244983564634,
                    "section_title": "CONCLUSION",
                    "char_start_offset": 27670,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 220
                        },
                        {
                            "start": 221,
                            "end": 355
                        },
                        {
                            "start": 356,
                            "end": 404
                        },
                        {
                            "start": 405,
                            "end": 541
                        },
                        {
                            "start": 542,
                            "end": 690
                        },
                        {
                            "start": 691,
                            "end": 867
                        },
                        {
                            "start": 870,
                            "end": 972
                        },
                        {
                            "start": 975,
                            "end": 1299
                        },
                        {
                            "start": 1300,
                            "end": 1363
                        },
                        {
                            "start": 1366,
                            "end": 1481
                        },
                        {
                            "start": 1484,
                            "end": 1599
                        },
                        {
                            "start": 1600,
                            "end": 1667
                        },
                        {
                            "start": 1670,
                            "end": 1819
                        },
                        {
                            "start": 1820,
                            "end": 1927
                        },
                        {
                            "start": 1928,
                            "end": 2113
                        },
                        {
                            "start": 2114,
                            "end": 2297
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.966796875
                }
            ],
            "relevance_judgement": 0.966796875,
            "relevance_judgment_input_expanded": "# Title: Unleashing LLM Reasoning Capability via Scalable Question Synthesis from Scratch\n# Venue: \n# Authors: Yuyang Ding, Xinyu Shi, Xiaobo Liang, Juntao Li, Zhaopeng Tu, Qiaoming Zhu, Min Zhang\n## Abstract\nImproving the mathematical reasoning capabilities of Large Language Models (LLMs) is critical for advancing artificial intelligence. However, access to extensive, diverse, and high-quality reasoning datasets remains a significant challenge, particularly for the open-source community. In this paper, we propose ScaleQuest, a novel, scalable, and cost-effective data synthesis method that enables the generation of large-scale mathematical reasoning datasets using lightweight 7B-scale models. ScaleQuest introduces a two-stage question-tuning process comprising Question Fine-Tuning (QFT) and Question Preference Optimization (QPO) to unlock the question generation capabilities of problem-solving models. By generating diverse questions from scratch -- without relying on powerful proprietary models or seed data -- we produce a dataset of 1 million problem-solution pairs. Our experiments demonstrate that models trained on our data outperform existing open-source datasets in both in-domain and out-of-domain evaluations. Furthermore, our approach shows continued performance improvement as the volume of training data increases, highlighting its potential for ongoing data scaling. The extensive improvements observed in code reasoning tasks demonstrate the generalization capabilities of our proposed method. Our work provides the open-source community with a practical solution to enhance the mathematical reasoning abilities of LLMs.\n## CONCLUSION\nIn this work, we propose ScaleQuest, a novel data synthesis framework that unlocks the ability of open-source smaller models to independently generate large-scale, high-quality reasoning data from scratch, at a low cost. By training the problem-solving models on a small subset of questions, we effectively activate their question-generation capabilities. We also introduce a response enhancement method. With these techniques, we successfully developed a fully synthetic math reasoning dataset consisting of 1 million question-answer pairs. Using this dataset, we fine-tuned the model and achieved remarkable improvements, with gains ranging from 29.2% to 46.4% compared to the base model. The fine-tuned 7B model, Qwen2-Math-7B-ScaleQuest, outperforms all competitors in the 7B-70B range and even surpasses proprietary models like GPT-4-Turbo and Claude-3.5-Sonnet. \n\nDue to time and cost constraints, there are several areas where our approach can be further optimized. \n\nFor instance, leveraging more powerful, larger problem-solving models like Qwen2.5-Math-72B-Instruct (Yang et al., 2024b) for question and response generation, using advanced models such as GPT-4o for constructing preference data for Question Preference Optimization, and further scaling up the generation of synthetic data. Each stage of our process has significant room for improvement. \n\nIn this paper, we have demonstrated the potential of this framework, laying the groundwork for future enhancements. \n\nFurthermore, despite the progress made in this work, there are still several limitations that need to be addressed. In our future research, we will concentrate on the following areas: \n\n\u2022 Large-scale and diverse high-quality data: This work chooses mathematical reasoning as a case study to demonstrate the effectiveness of our method. In the future, we will focus on broader and more complex tasks such as science and competitive programming. Additionally, future research will aim to continuously scale data synthesis to explore the scaling laws for synthetic data and seek a more efficient approach to scaling data generation. \u2022 Self-improvement capability: Our experiments demonstrate the model's self-improvement capability, meaning that it can generate data of higher quality than its original training set.",
            "reference_string": "[273549775 | Ding et al. | 2024 | Citations: 8]"
        },
        {
            "title": "Scaling Laws of Synthetic Data for Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 51,
            "citation_count": 7,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.19551, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2118242824",
                    "name": "Zeyu Qin"
                },
                {
                    "authorId": "2287927238",
                    "name": "Qingxiu Dong"
                },
                {
                    "authorId": "2284863493",
                    "name": "Xingxing Zhang"
                },
                {
                    "authorId": "2294850817",
                    "name": "Li Dong"
                },
                {
                    "authorId": "2116768132",
                    "name": "Xiaolong Huang"
                },
                {
                    "authorId": "2291073936",
                    "name": "Ziyi Yang"
                },
                {
                    "authorId": "2268760479",
                    "name": "Mahmoud Khademi"
                },
                {
                    "authorId": "2273919921",
                    "name": "Dongdong Zhang"
                },
                {
                    "authorId": "3032929",
                    "name": "H. Awadalla"
                },
                {
                    "authorId": "2352012716",
                    "name": "Yi R. Fung"
                },
                {
                    "authorId": "2347682196",
                    "name": "Weizhu Chen"
                },
                {
                    "authorId": "2258337019",
                    "name": "Minhao Cheng"
                },
                {
                    "authorId": "2323870436",
                    "name": "Furu Wei"
                }
            ],
            "abstract": "Large language models (LLMs) achieve strong performance across diverse tasks, largely driven by high-quality web data used in pre-training. However, recent studies indicate this data source is rapidly depleting. Synthetic data emerges as a promising alternative, but it remains unclear whether synthetic datasets exhibit predictable scalability comparable to raw pre-training data. In this work, we systematically investigate the scaling laws of synthetic data by introducing SynthLLM, a scalable framework that transforms pre-training corpora into diverse, high-quality synthetic datasets. Our approach achieves this by automatically extracting and recombining high-level concepts across multiple documents using a graph algorithm. Key findings from our extensive mathematical experiments on SynthLLM include: (1) SynthLLM generates synthetic data that reliably adheres to the rectified scaling law across various model sizes; (2) Performance improvements plateau near 300B tokens; and (3) Larger models approach optimal performance with fewer training tokens. For instance, an 8B model peaks at 1T tokens, while a 3B model requires 4T. Moreover, comparisons with existing synthetic data generation and augmentation methods demonstrate that SynthLLM achieves superior performance and scalability. Our findings highlight synthetic data as a scalable and reliable alternative to organic pre-training corpora, offering a viable path toward continued improvement in model performance.",
            "corpus_id": 277313659,
            "sentences": [
                {
                    "corpus_id": "277313659",
                    "title": "Scaling Laws of Synthetic Data for Language Models",
                    "text": "Synthetic Data Generation. Synthetic data has emerged as a promising approach to augment real data, addressing challenges related to data scarcity and the high cost of data collection and annotation. It has been widely applied in pre-training, instruction following, and reasoning tasks, demonstrating notable improvements in model performance. In this work, we focus on synthetic data generation for the post-training phase. Existing synthetic data generation methods typically rely on a small set of high-quality human-annotated seed samples, leveraging LLMs to generate diverse augmentations through various techniques: 1) sampling seed instructions as few-shot examples to elicit LLMs to generate new instructions [19,23,47,50]; 2) leveraging LLMs to rephrase seed samples [12,21,34,35,53]. However, the scarcity of high-quality seed examples limits the scalability and diversity of generated data. To address this, Xu et al. [51] and Li et al. [24] propose generating synthetic data from scratch, leveraging either the inherent uncertainty of LLMs or a predefined knowledge taxonomy to guide the generation process. In contrast, pre-training data-being both vast and highly diverse-still remains an underutilized resource for scalable post-training data generation. Recent methods have explored extracting high-quality samples directly from web documents (similar to our Level-1 method) [54,55,56] or employing document backtranslation to generate questions [26]. Our SYNTHLLM aligns with this line of research and offers a more efficient approach to leveraging web documents for scaling diverse sample generation compared to aforementioned methods. \n\nScaling Law of LLMs. Scaling laws provide a predictive framework for estimating model performance based on key factors such as model size and pre-training data size, and have been extensively studied [5,17,18,20,42]. These laws offer valuable insights into how performance scales with computational resources, enabling more informed decisions on the optimal allocation of compute for pre-training LLMs [18,20]. Recently, more fine-grained scaling laws have been proposed, including data-constrained scaling [37], hyperparameter scaling [6], and model distillation [8].",
                    "score": 0.39369950346573385,
                    "section_title": "Related Work",
                    "char_start_offset": 4862,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 26
                        },
                        {
                            "start": 27,
                            "end": 199
                        },
                        {
                            "start": 200,
                            "end": 344
                        },
                        {
                            "start": 345,
                            "end": 425
                        },
                        {
                            "start": 426,
                            "end": 794
                        },
                        {
                            "start": 795,
                            "end": 902
                        },
                        {
                            "start": 903,
                            "end": 1120
                        },
                        {
                            "start": 1121,
                            "end": 1270
                        },
                        {
                            "start": 1271,
                            "end": 1468
                        },
                        {
                            "start": 1469,
                            "end": 1654
                        },
                        {
                            "start": 1657,
                            "end": 1677
                        },
                        {
                            "start": 1678,
                            "end": 1873
                        },
                        {
                            "start": 1874,
                            "end": 2067
                        },
                        {
                            "start": 2068,
                            "end": 2225
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 725,
                            "end": 728,
                            "matchedPaperCorpusId": "273025760"
                        },
                        {
                            "start": 790,
                            "end": 793,
                            "matchedPaperCorpusId": "262084051"
                        },
                        {
                            "start": 1396,
                            "end": 1399,
                            "matchedPaperCorpusId": "269605607"
                        },
                        {
                            "start": 1399,
                            "end": 1402,
                            "matchedPaperCorpusId": "269981934"
                        },
                        {
                            "start": 1463,
                            "end": 1467,
                            "matchedPaperCorpusId": "260866107"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96337890625
                }
            ],
            "relevance_judgement": 0.96337890625,
            "relevance_judgment_input_expanded": "# Title: Scaling Laws of Synthetic Data for Language Models\n# Venue: arXiv.org\n# Authors: Zeyu Qin, Qingxiu Dong, Xingxing Zhang, Li Dong, Xiaolong Huang, Ziyi Yang, Mahmoud Khademi, Dongdong Zhang, H. Awadalla, Yi R. Fung, Weizhu Chen, Minhao Cheng, Furu Wei\n## Abstract\nLarge language models (LLMs) achieve strong performance across diverse tasks, largely driven by high-quality web data used in pre-training. However, recent studies indicate this data source is rapidly depleting. Synthetic data emerges as a promising alternative, but it remains unclear whether synthetic datasets exhibit predictable scalability comparable to raw pre-training data. In this work, we systematically investigate the scaling laws of synthetic data by introducing SynthLLM, a scalable framework that transforms pre-training corpora into diverse, high-quality synthetic datasets. Our approach achieves this by automatically extracting and recombining high-level concepts across multiple documents using a graph algorithm. Key findings from our extensive mathematical experiments on SynthLLM include: (1) SynthLLM generates synthetic data that reliably adheres to the rectified scaling law across various model sizes; (2) Performance improvements plateau near 300B tokens; and (3) Larger models approach optimal performance with fewer training tokens. For instance, an 8B model peaks at 1T tokens, while a 3B model requires 4T. Moreover, comparisons with existing synthetic data generation and augmentation methods demonstrate that SynthLLM achieves superior performance and scalability. Our findings highlight synthetic data as a scalable and reliable alternative to organic pre-training corpora, offering a viable path toward continued improvement in model performance.\n## Related Work\nSynthetic Data Generation. Synthetic data has emerged as a promising approach to augment real data, addressing challenges related to data scarcity and the high cost of data collection and annotation. It has been widely applied in pre-training, instruction following, and reasoning tasks, demonstrating notable improvements in model performance. In this work, we focus on synthetic data generation for the post-training phase. Existing synthetic data generation methods typically rely on a small set of high-quality human-annotated seed samples, leveraging LLMs to generate diverse augmentations through various techniques: 1) sampling seed instructions as few-shot examples to elicit LLMs to generate new instructions [19,23,47,50]; 2) leveraging LLMs to rephrase seed samples [12,21,34,35,53]. However, the scarcity of high-quality seed examples limits the scalability and diversity of generated data. To address this, Xu et al. [51] and Li et al. [24] propose generating synthetic data from scratch, leveraging either the inherent uncertainty of LLMs or a predefined knowledge taxonomy to guide the generation process. In contrast, pre-training data-being both vast and highly diverse-still remains an underutilized resource for scalable post-training data generation. Recent methods have explored extracting high-quality samples directly from web documents (similar to our Level-1 method) [54,55,56] or employing document backtranslation to generate questions [26]. Our SYNTHLLM aligns with this line of research and offers a more efficient approach to leveraging web documents for scaling diverse sample generation compared to aforementioned methods. \n\nScaling Law of LLMs. Scaling laws provide a predictive framework for estimating model performance based on key factors such as model size and pre-training data size, and have been extensively studied [5,17,18,20,42]. These laws offer valuable insights into how performance scales with computational resources, enabling more informed decisions on the optimal allocation of compute for pre-training LLMs [18,20]. Recently, more fine-grained scaling laws have been proposed, including data-constrained scaling [37], hyperparameter scaling [6], and model distillation [8].",
            "reference_string": "[277313659 | Qin et al. | 2025 | Citations: 7]"
        },
        {
            "title": "CLIPPER: Compression enables long-context synthetic data generation",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 59,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.14854, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2264486147",
                    "name": "Chau Minh Pham"
                },
                {
                    "authorId": "144455052",
                    "name": "Yapei Chang"
                },
                {
                    "authorId": "2136562",
                    "name": "Mohit Iyyer"
                }
            ],
            "abstract": "LLM developers are increasingly reliant on synthetic data, but generating high-quality data for complex long-context reasoning tasks remains challenging. We introduce CLIPPER, a compression-based approach for generating synthetic data tailored to narrative claim verification - a task that requires reasoning over a book to verify a given claim. Instead of generating claims directly from the raw text of the book, which results in artifact-riddled claims, CLIPPER first compresses the book into chapter outlines and book summaries and then uses these intermediate representations to generate complex claims and corresponding chain-of-thoughts. Compared to naive approaches, CLIPPER produces claims that are more valid, grounded, and complex. Using CLIPPER, we construct a dataset of 19K synthetic book claims paired with their source texts and chain-of-thought reasoning, and use it to fine-tune three open-weight models. Our best model achieves breakthrough results on narrative claim verification (from 28% to 76% accuracy on our test set) and sets a new state-of-the-art for sub-10B models on the NoCha leaderboard. Further analysis shows that our models generate more detailed and grounded chain-of-thought reasoning while also improving performance on other narrative understanding tasks (e.g., NarrativeQA).",
            "corpus_id": 276482747,
            "sentences": [],
            "relevance_judgement": 0.958984375,
            "relevance_judgment_input_expanded": "# Title: CLIPPER: Compression enables long-context synthetic data generation\n# Venue: arXiv.org\n# Authors: Chau Minh Pham, Yapei Chang, Mohit Iyyer\n## Abstract\nLLM developers are increasingly reliant on synthetic data, but generating high-quality data for complex long-context reasoning tasks remains challenging. We introduce CLIPPER, a compression-based approach for generating synthetic data tailored to narrative claim verification - a task that requires reasoning over a book to verify a given claim. Instead of generating claims directly from the raw text of the book, which results in artifact-riddled claims, CLIPPER first compresses the book into chapter outlines and book summaries and then uses these intermediate representations to generate complex claims and corresponding chain-of-thoughts. Compared to naive approaches, CLIPPER produces claims that are more valid, grounded, and complex. Using CLIPPER, we construct a dataset of 19K synthetic book claims paired with their source texts and chain-of-thought reasoning, and use it to fine-tune three open-weight models. Our best model achieves breakthrough results on narrative claim verification (from 28% to 76% accuracy on our test set) and sets a new state-of-the-art for sub-10B models on the NoCha leaderboard. Further analysis shows that our models generate more detailed and grounded chain-of-thought reasoning while also improving performance on other narrative understanding tasks (e.g., NarrativeQA).\n",
            "reference_string": "[276482747 | Pham et al. | 2025 | Citations: 1]"
        },
        {
            "title": "LLM-Assisted Code Cleaning For Training Accurate Code Generators",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 55,
            "citation_count": 32,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.14904, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1646458461",
                    "name": "Naman Jain"
                },
                {
                    "authorId": "1993655237",
                    "name": "Tianjun Zhang"
                },
                {
                    "authorId": "2537924",
                    "name": "Wei-Lin Chiang"
                },
                {
                    "authorId": "2254681613",
                    "name": "Joseph Gonzalez"
                },
                {
                    "authorId": "2268398121",
                    "name": "Koushik Sen"
                },
                {
                    "authorId": "2055174324",
                    "name": "Ion Stoica"
                }
            ],
            "abstract": "Natural language to code generation is an important application area of LLMs and has received wide attention from the community. The majority of relevant studies have exclusively concentrated on increasing the quantity and functional correctness of training sets while disregarding other stylistic elements of programs. More recently, data quality has garnered a lot of interest and multiple works have showcased its importance for improving performance. In this work, we investigate data quality for code and find that making the code more structured and readable leads to improved code generation performance of the system. We build a novel data-cleaning pipeline that uses these principles to transform existing programs by 1.) renaming variables, 2.) modularizing and decomposing complex code into smaller helper sub-functions, and 3.) inserting natural-language based plans via LLM based transformations. We evaluate our approach on two challenging algorithmic code generation benchmarks and find that fine-tuning CodeLLaMa-7B on our transformed modularized programs improves the performance by up to 30% compared to fine-tuning on the original dataset. Additionally, we demonstrate improved performance from using a smaller amount of higher-quality data, finding that a model fine-tuned on the entire original dataset is outperformed by a model trained on 15% of our cleaned dataset. Even in comparison to closed-source models, our models outperform the much larger AlphaCoder models.",
            "corpus_id": 265456227,
            "sentences": [
                {
                    "corpus_id": "265456227",
                    "title": "LLM-Assisted Code Cleaning For Training Accurate Code Generators",
                    "text": "Synthetic data for LLMS. Recent works have explored using synthetic datasets for generalpurpose or task-specific finetuning of LLMS. These approaches work by generating synthetic datasets from a strong LLM (like GPT-3.5-TURBO or GPT-4) using a set of existing tasks (Taori et al., 2023;Chiang et al., 2023) or generating new tasks using self-instruct (Wang et al., 2022) or evol-instruct (Xu et al., 2023) approaches. This has been also applied for task-specific finetuningin common-sense reasoning (West et al., 2022), text-summarization (Sclar et al., 2022), mathematical reasoning (Luo et al., 2023a;Yue et al., 2023), tool use (Patil et al., 2023), coding (Luo et al., 2023b), and general-purpose reasoning Li et al. (2023b); Zelikman et al. (2022). \n\nMore specifically, Yue et al. (2023) curates diverse corpus of mathematics problems with chain-ofthought or program-of-thought (Chen et al., 2022b) annotations for mathematical reasoning analogous to our plans. Gunasekar et al. (2023) proposed pre-training models on programming \"textbooks\" generated synthetically from GPT-3.5-TURBO. Haluptzok et al. (2023) similarly generates programming puzzles and corresponding solutions from language models. Our work also studies curating synthetic data for code-generation space. However, instead of directly generating data using LLMS, we identify good programming patterns and clean existing datasets using them. \n\nAlgorithmic Code Generation. Code generation is a broad domain and is covered in Appendix D. We only discuss pertinent algorithmic code generation works here. Hendrycks et al. (2021) released the APPS dataset while Li et al. (2022) released the CODE-CONTESTS dataset with the ALPHACODE models.",
                    "score": 0.4251144303761493,
                    "section_title": "RELATED WORK",
                    "char_start_offset": 24209,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 24
                        },
                        {
                            "start": 25,
                            "end": 132
                        },
                        {
                            "start": 133,
                            "end": 417
                        },
                        {
                            "start": 418,
                            "end": 753
                        },
                        {
                            "start": 756,
                            "end": 966
                        },
                        {
                            "start": 967,
                            "end": 1090
                        },
                        {
                            "start": 1091,
                            "end": 1204
                        },
                        {
                            "start": 1205,
                            "end": 1277
                        },
                        {
                            "start": 1278,
                            "end": 1412
                        },
                        {
                            "start": 1415,
                            "end": 1443
                        },
                        {
                            "start": 1444,
                            "end": 1573
                        },
                        {
                            "start": 1574,
                            "end": 1708
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 499,
                            "end": 517,
                            "matchedPaperCorpusId": "238857304"
                        },
                        {
                            "start": 711,
                            "end": 728,
                            "matchedPaperCorpusId": "259251773"
                        },
                        {
                            "start": 730,
                            "end": 752,
                            "matchedPaperCorpusId": "247762790"
                        },
                        {
                            "start": 1091,
                            "end": 1114,
                            "matchedPaperCorpusId": "251197051"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95654296875
                }
            ],
            "relevance_judgement": 0.95654296875,
            "relevance_judgment_input_expanded": "# Title: LLM-Assisted Code Cleaning For Training Accurate Code Generators\n# Venue: arXiv.org\n# Authors: Naman Jain, Tianjun Zhang, Wei-Lin Chiang, Joseph Gonzalez, Koushik Sen, Ion Stoica\n## Abstract\nNatural language to code generation is an important application area of LLMs and has received wide attention from the community. The majority of relevant studies have exclusively concentrated on increasing the quantity and functional correctness of training sets while disregarding other stylistic elements of programs. More recently, data quality has garnered a lot of interest and multiple works have showcased its importance for improving performance. In this work, we investigate data quality for code and find that making the code more structured and readable leads to improved code generation performance of the system. We build a novel data-cleaning pipeline that uses these principles to transform existing programs by 1.) renaming variables, 2.) modularizing and decomposing complex code into smaller helper sub-functions, and 3.) inserting natural-language based plans via LLM based transformations. We evaluate our approach on two challenging algorithmic code generation benchmarks and find that fine-tuning CodeLLaMa-7B on our transformed modularized programs improves the performance by up to 30% compared to fine-tuning on the original dataset. Additionally, we demonstrate improved performance from using a smaller amount of higher-quality data, finding that a model fine-tuned on the entire original dataset is outperformed by a model trained on 15% of our cleaned dataset. Even in comparison to closed-source models, our models outperform the much larger AlphaCoder models.\n## RELATED WORK\nSynthetic data for LLMS. Recent works have explored using synthetic datasets for generalpurpose or task-specific finetuning of LLMS. These approaches work by generating synthetic datasets from a strong LLM (like GPT-3.5-TURBO or GPT-4) using a set of existing tasks (Taori et al., 2023;Chiang et al., 2023) or generating new tasks using self-instruct (Wang et al., 2022) or evol-instruct (Xu et al., 2023) approaches. This has been also applied for task-specific finetuningin common-sense reasoning (West et al., 2022), text-summarization (Sclar et al., 2022), mathematical reasoning (Luo et al., 2023a;Yue et al., 2023), tool use (Patil et al., 2023), coding (Luo et al., 2023b), and general-purpose reasoning Li et al. (2023b); Zelikman et al. (2022). \n\nMore specifically, Yue et al. (2023) curates diverse corpus of mathematics problems with chain-ofthought or program-of-thought (Chen et al., 2022b) annotations for mathematical reasoning analogous to our plans. Gunasekar et al. (2023) proposed pre-training models on programming \"textbooks\" generated synthetically from GPT-3.5-TURBO. Haluptzok et al. (2023) similarly generates programming puzzles and corresponding solutions from language models. Our work also studies curating synthetic data for code-generation space. However, instead of directly generating data using LLMS, we identify good programming patterns and clean existing datasets using them. \n\nAlgorithmic Code Generation. Code generation is a broad domain and is covered in Appendix D. We only discuss pertinent algorithmic code generation works here. Hendrycks et al. (2021) released the APPS dataset while Li et al. (2022) released the CODE-CONTESTS dataset with the ALPHACODE models.",
            "reference_string": "[265456227 | Jain et al. | 2023 | Citations: 32]"
        },
        {
            "title": "VersaPRM: Multi-Domain Process Reward Model via Synthetic Reasoning Data",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 43,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.06737, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2323820473",
                    "name": "Thomas Zeng"
                },
                {
                    "authorId": "2190935184",
                    "name": "Shuibai Zhang"
                },
                {
                    "authorId": "2344794414",
                    "name": "Shutong Wu"
                },
                {
                    "authorId": "2344760610",
                    "name": "Christian Classen"
                },
                {
                    "authorId": "2253659910",
                    "name": "Daewon Chae"
                },
                {
                    "authorId": "2323781863",
                    "name": "Ethan Ewer"
                },
                {
                    "authorId": "2345324748",
                    "name": "Minjae Lee"
                },
                {
                    "authorId": "2344831673",
                    "name": "Heeju Kim"
                },
                {
                    "authorId": "2282472476",
                    "name": "Wonjun Kang"
                },
                {
                    "authorId": "2334358539",
                    "name": "Jackson Kunde"
                },
                {
                    "authorId": "2344807639",
                    "name": "Ying Fan"
                },
                {
                    "authorId": "2344788904",
                    "name": "Jungtaek Kim"
                },
                {
                    "authorId": "2282471620",
                    "name": "Hyung Il Koo"
                },
                {
                    "authorId": "144161012",
                    "name": "K. Ramchandran"
                },
                {
                    "authorId": "107579958",
                    "name": "Dimitris Papailiopoulos"
                },
                {
                    "authorId": "2323790154",
                    "name": "Kangwook Lee"
                }
            ],
            "abstract": "Process Reward Models (PRMs) have proven effective at enhancing mathematical reasoning for Large Language Models (LLMs) by leveraging increased inference-time computation. However, they are predominantly trained on mathematical data and their generalizability to non-mathematical domains has not been rigorously studied. In response, this work first shows that current PRMs have poor performance in other domains. To address this limitation, we introduce VersaPRM, a multi-domain PRM trained on synthetic reasoning data generated using our novel data generation and annotation method. VersaPRM achieves consistent performance gains across diverse domains. For instance, in the MMLU-Pro category of Law, VersaPRM via weighted majority voting, achieves a 7.9% performance gain over the majority voting baseline -- surpassing Qwen2.5-Math-PRM's gain of 1.3%. We further contribute to the community by open-sourcing all data, code and models for VersaPRM.",
            "corpus_id": 276250232,
            "sentences": [
                {
                    "corpus_id": "276250232",
                    "title": "VersaPRM: Multi-Domain Process Reward Model via Synthetic Reasoning Data",
                    "text": "We proposed VersaPRM trained using synthetic reasoning data to address the limitations of existing math PRMs. By leveraging a cost-efficient synthetic data generation pipeline, we enabled production of high-quality step-wise reasoning data and demonstrate that PRMs can effectively scale reasoning ability at inference time in diverse domains. \n\nFuture Work. Several directions remain for advancing multi-domain PRMs. Can VersaPRM be effectively used as a reward model for RL training? Can it improves RL training beyond math PRMs? Could more sophisticated counterfactual augmentation enhance PRM effectiveness? Also, evaluating PRMs on harder, open-ended problems would better assess their generalization. Lastly, more thorough exploration of PRMs with large models (e.g., GPT-4 (Achiam et al., 2023) and DeepSeek-R1) could clarify their scalability and role in state-of-the-art reasoning systems.",
                    "score": 0.41728281853229787,
                    "section_title": "Discussion and Future Directions",
                    "char_start_offset": 26556,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 109
                        },
                        {
                            "start": 110,
                            "end": 343
                        },
                        {
                            "start": 346,
                            "end": 358
                        },
                        {
                            "start": 359,
                            "end": 417
                        },
                        {
                            "start": 418,
                            "end": 485
                        },
                        {
                            "start": 486,
                            "end": 531
                        },
                        {
                            "start": 532,
                            "end": 611
                        },
                        {
                            "start": 612,
                            "end": 706
                        },
                        {
                            "start": 707,
                            "end": 898
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95654296875
                }
            ],
            "relevance_judgement": 0.95654296875,
            "relevance_judgment_input_expanded": "# Title: VersaPRM: Multi-Domain Process Reward Model via Synthetic Reasoning Data\n# Venue: arXiv.org\n# Authors: Thomas Zeng, Shuibai Zhang, Shutong Wu, Christian Classen, Daewon Chae, Ethan Ewer, Minjae Lee, Heeju Kim, Wonjun Kang, Jackson Kunde, Ying Fan, Jungtaek Kim, Hyung Il Koo, K. Ramchandran, Dimitris Papailiopoulos, Kangwook Lee\n## Abstract\nProcess Reward Models (PRMs) have proven effective at enhancing mathematical reasoning for Large Language Models (LLMs) by leveraging increased inference-time computation. However, they are predominantly trained on mathematical data and their generalizability to non-mathematical domains has not been rigorously studied. In response, this work first shows that current PRMs have poor performance in other domains. To address this limitation, we introduce VersaPRM, a multi-domain PRM trained on synthetic reasoning data generated using our novel data generation and annotation method. VersaPRM achieves consistent performance gains across diverse domains. For instance, in the MMLU-Pro category of Law, VersaPRM via weighted majority voting, achieves a 7.9% performance gain over the majority voting baseline -- surpassing Qwen2.5-Math-PRM's gain of 1.3%. We further contribute to the community by open-sourcing all data, code and models for VersaPRM.\n## Discussion and Future Directions\nWe proposed VersaPRM trained using synthetic reasoning data to address the limitations of existing math PRMs. By leveraging a cost-efficient synthetic data generation pipeline, we enabled production of high-quality step-wise reasoning data and demonstrate that PRMs can effectively scale reasoning ability at inference time in diverse domains. \n\nFuture Work. Several directions remain for advancing multi-domain PRMs. Can VersaPRM be effectively used as a reward model for RL training? Can it improves RL training beyond math PRMs? Could more sophisticated counterfactual augmentation enhance PRM effectiveness? Also, evaluating PRMs on harder, open-ended problems would better assess their generalization. Lastly, more thorough exploration of PRMs with large models (e.g., GPT-4 (Achiam et al., 2023) and DeepSeek-R1) could clarify their scalability and role in state-of-the-art reasoning systems.",
            "reference_string": "[276250232 | Zeng et al. | 2025 | Citations: 4]"
        },
        {
            "title": "Generative AI for Synthetic Data Generation: Methods, Challenges and the Future",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 93,
            "citation_count": 35,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.04190, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2290239663",
                    "name": "Xu Guo"
                },
                {
                    "authorId": "2290210689",
                    "name": "Yiqiang Chen"
                }
            ],
            "abstract": "The recent surge in research focused on generating synthetic data from large language models (LLMs), especially for scenarios with limited data availability, marks a notable shift in Generative Artificial Intelligence (AI). Their ability to perform comparably to real-world data positions this approach as a compelling solution to low-resource challenges. This paper delves into advanced technologies that leverage these gigantic LLMs for the generation of task-specific training data. We outline methodologies, evaluation techniques, and practical applications, discuss the current limitations, and suggest potential pathways for future research.",
            "corpus_id": 268264094,
            "sentences": [],
            "relevance_judgement": 0.95556640625,
            "relevance_judgment_input_expanded": "# Title: Generative AI for Synthetic Data Generation: Methods, Challenges and the Future\n# Venue: arXiv.org\n# Authors: Xu Guo, Yiqiang Chen\n## Abstract\nThe recent surge in research focused on generating synthetic data from large language models (LLMs), especially for scenarios with limited data availability, marks a notable shift in Generative Artificial Intelligence (AI). Their ability to perform comparably to real-world data positions this approach as a compelling solution to low-resource challenges. This paper delves into advanced technologies that leverage these gigantic LLMs for the generation of task-specific training data. We outline methodologies, evaluation techniques, and practical applications, discuss the current limitations, and suggest potential pathways for future research.\n",
            "reference_string": "[268264094 | Guo et al. | 2024 | Citations: 35]"
        },
        {
            "title": "CraftRTL: High-quality Synthetic Data Generation for Verilog Code Models with Correct-by-Construction Non-Textual Representations and Targeted Code Repair",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 52,
            "citation_count": 17,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.12993, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2264029479",
                    "name": "Mingjie Liu"
                },
                {
                    "authorId": "3328096",
                    "name": "Yun-Da Tsai"
                },
                {
                    "authorId": "2322144595",
                    "name": "Wenfei Zhou"
                },
                {
                    "authorId": "2268825069",
                    "name": "Haoxing Ren"
                }
            ],
            "abstract": "Despite the significant progress made in code generation with large language models, challenges persist, especially with hardware description languages such as Verilog. This paper first presents an analysis of fine-tuned LLMs on Verilog coding, with synthetic data from prior methods. We identify two main issues: difficulties in handling non-textual representations (Karnaugh maps, state-transition diagrams and waveforms) and significant variability during training with models randomly making\"minor\"mistakes. To address these limitations, we enhance data curation by creating correct-by-construction data targeting non-textual representations. Additionally, we introduce an automated framework that generates error reports from various model checkpoints and injects these errors into open-source code to create targeted code repair data. Our fine-tuned Starcoder2-15B outperforms prior state-of-the-art results by 3.8%, 10.9%, 6.6% for pass@1 on VerilogEval-Machine, VerilogEval-Human, and RTLLM.",
            "corpus_id": 272770433,
            "sentences": [
                {
                    "corpus_id": "272770433",
                    "title": "CraftRTL: High-quality Synthetic Data Generation for Verilog Code Models with Correct-by-Construction Non-Textual Representations and Targeted Code Repair",
                    "text": "Synthetic Data Generation for Model Fine-tuning. The performance of large language models (LLMs) hinge on the quality and diversity of their training data. To address the limitations of manual datasets, synthetic data generation methods (Wang et al., 2022;Xu et al., 2023) have been developed \n\nPreprint to automatically create instruction-following examples from LLMs, reducing reliance on human annotations. Various techniques enhance data quality: Wang et al. (2022) generates multiple reasoning traces and selects the most frequent output to improve robustness, while other approaches (Lightman et al., 2023;Zhang et al., 2024b) assess response quality based on these traces. Self-training methods utilize synthetic data for iterative fine-tuning, boosting reasoning capabilities (Singh et al., 2023;Feng et al., 2023). These advancements show how synthetic data can effectively scale and optimize models through iterative feedback. \n\nLarge Language Models for Code Generation. Recent breakthroughs in large language models (LLMs) have greatly enhanced their capability to tackle complex code generation tasks. Much of the research focuses on developing LLMs specialized for code by continuing their pretraining on code data (Guo et al., 2024;Bai et al., 2023;Roziere et al., 2023;DeepSeek-AI et al., 2024) from open-source repositories like GitHub (Kocetkov et al., 2022;Lozhkov et al., 2024) and commit histories (Muennighoff et al., 2023). Further improvements to these models come from reinforcement learning (Le et al., 2022) and more often instruction fine-tuning, which involves techniques to address more complex coding problems (Luo et al., 2024b), increasing diversity with unlabeled open-source code (Wei et al., 2023;Yu et al., 2024;Wu et al., 2024), ensuring solution correctness through self-written tests (Chen et al., 2022), and validating and debugging code execution through interactions with LLM agents (Lei et al., 2024).",
                    "score": 0.3799239128319761,
                    "section_title": "RELATED WORK",
                    "char_start_offset": 24430,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 48
                        },
                        {
                            "start": 49,
                            "end": 155
                        },
                        {
                            "start": 156,
                            "end": 292
                        },
                        {
                            "start": 295,
                            "end": 409
                        },
                        {
                            "start": 410,
                            "end": 679
                        },
                        {
                            "start": 680,
                            "end": 823
                        },
                        {
                            "start": 824,
                            "end": 936
                        },
                        {
                            "start": 939,
                            "end": 981
                        },
                        {
                            "start": 982,
                            "end": 1114
                        },
                        {
                            "start": 1115,
                            "end": 1446
                        },
                        {
                            "start": 1447,
                            "end": 1945
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95361328125
                }
            ],
            "relevance_judgement": 0.95361328125,
            "relevance_judgment_input_expanded": "# Title: CraftRTL: High-quality Synthetic Data Generation for Verilog Code Models with Correct-by-Construction Non-Textual Representations and Targeted Code Repair\n# Venue: arXiv.org\n# Authors: Mingjie Liu, Yun-Da Tsai, Wenfei Zhou, Haoxing Ren\n## Abstract\nDespite the significant progress made in code generation with large language models, challenges persist, especially with hardware description languages such as Verilog. This paper first presents an analysis of fine-tuned LLMs on Verilog coding, with synthetic data from prior methods. We identify two main issues: difficulties in handling non-textual representations (Karnaugh maps, state-transition diagrams and waveforms) and significant variability during training with models randomly making\"minor\"mistakes. To address these limitations, we enhance data curation by creating correct-by-construction data targeting non-textual representations. Additionally, we introduce an automated framework that generates error reports from various model checkpoints and injects these errors into open-source code to create targeted code repair data. Our fine-tuned Starcoder2-15B outperforms prior state-of-the-art results by 3.8%, 10.9%, 6.6% for pass@1 on VerilogEval-Machine, VerilogEval-Human, and RTLLM.\n## RELATED WORK\nSynthetic Data Generation for Model Fine-tuning. The performance of large language models (LLMs) hinge on the quality and diversity of their training data. To address the limitations of manual datasets, synthetic data generation methods (Wang et al., 2022;Xu et al., 2023) have been developed \n\nPreprint to automatically create instruction-following examples from LLMs, reducing reliance on human annotations. Various techniques enhance data quality: Wang et al. (2022) generates multiple reasoning traces and selects the most frequent output to improve robustness, while other approaches (Lightman et al., 2023;Zhang et al., 2024b) assess response quality based on these traces. Self-training methods utilize synthetic data for iterative fine-tuning, boosting reasoning capabilities (Singh et al., 2023;Feng et al., 2023). These advancements show how synthetic data can effectively scale and optimize models through iterative feedback. \n\nLarge Language Models for Code Generation. Recent breakthroughs in large language models (LLMs) have greatly enhanced their capability to tackle complex code generation tasks. Much of the research focuses on developing LLMs specialized for code by continuing their pretraining on code data (Guo et al., 2024;Bai et al., 2023;Roziere et al., 2023;DeepSeek-AI et al., 2024) from open-source repositories like GitHub (Kocetkov et al., 2022;Lozhkov et al., 2024) and commit histories (Muennighoff et al., 2023). Further improvements to these models come from reinforcement learning (Le et al., 2022) and more often instruction fine-tuning, which involves techniques to address more complex coding problems (Luo et al., 2024b), increasing diversity with unlabeled open-source code (Wei et al., 2023;Yu et al., 2024;Wu et al., 2024), ensuring solution correctness through self-written tests (Chen et al., 2022), and validating and debugging code execution through interactions with LLM agents (Lei et al., 2024).",
            "reference_string": "[272770433 | Liu et al. | 2024 | Citations: 17]"
        },
        {
            "title": "A Graph-Based Synthetic Data Pipeline for Scaling High-Quality Reasoning Instructions",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 46,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.08864, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2335083125",
                    "name": "Jiankang Wang"
                },
                {
                    "authorId": "2187441024",
                    "name": "Jianjun Xu"
                },
                {
                    "authorId": "2334884780",
                    "name": "Xiaorui Wang"
                },
                {
                    "authorId": "2143529316",
                    "name": "Yuxin Wang"
                },
                {
                    "authorId": "2067080682",
                    "name": "Mengting Xing"
                },
                {
                    "authorId": "13808397",
                    "name": "Shancheng Fang"
                },
                {
                    "authorId": "2334916976",
                    "name": "Zhineng Chen"
                },
                {
                    "authorId": "2323899630",
                    "name": "Hongtao Xie"
                },
                {
                    "authorId": "2290965957",
                    "name": "Yongdong Zhang"
                }
            ],
            "abstract": "Synthesizing high-quality reasoning data for continual training has been proven to be effective in enhancing the performance of Large Language Models (LLMs). However, previous synthetic approaches struggle to easily scale up data and incur high costs in the pursuit of high quality. In this paper, we propose the Graph-based Synthetic Data Pipeline (GSDP), an economical and scalable framework for high-quality reasoning data synthesis. Inspired by knowledge graphs, we extracted knowledge points from seed data and constructed a knowledge point relationships graph to explore their interconnections. By exploring the implicit relationships among knowledge, our method achieves $\\times$255 data expansion. Furthermore, GSDP led by open-source models, achieves synthesis quality comparable to GPT-4-0613 while maintaining $\\times$100 lower costs. To tackle the most challenging mathematical reasoning task, we present the GSDP-MATH dataset comprising over 1.91 million pairs of math problems and answers. After fine-tuning on GSDP-MATH, GSDP-7B based on Mistral-7B achieves 37.7% accuracy on MATH and 78.4% on GSM8K, demonstrating the effectiveness of our method. The dataset and models will be released at https://github.com/Jayce1kk/GSDP.",
            "corpus_id": 274656618,
            "sentences": [
                {
                    "corpus_id": "274656618",
                    "title": "A Graph-Based Synthetic Data Pipeline for Scaling High-Quality Reasoning Instructions",
                    "text": "Despite the remarkable capabilities large language models (LLMs) have demonstrated in various linguistic tasks, significant gaps remain in their ability to comprehend and solve intricate reasoning tasks (e.g., mathematics, coding, physics, and chemistry). One effective approach to bridging these gaps is using large-scale, high-quality synthetic data. However, it is still a challenge to develop a low-cost and effective synthesis pipeline. Take mathematics as an example. The two main approaches for building high-quality mathematics reasoning datasets are data filtering and data synthesis. Data filtering (Yue et al., 2024b;Shao et al., 2024;Ying et al., 2024) involves extracting data from pre-training corpora such as Common Crawl, and rewriting it using advanced commercial models or human annotation. However, the vast scale and inherent noise of these corpora result in high post-processing costs and inconsistent data quality. Data synthesis (Yu et al., 2023;Luo et al., 2023;Yue et al., 2024a;Tang et al., 2024;Huang et al., 2024a;Li et al., 2024a;Toshniwal et al., 2024;Li et al., 2024b) leverages frontier large language models, such as GPT-3.5 (Floridi & Chiriatti, 2020) and GPT-4 (Achiam et al., 2023), to augment or regenerate high-quality mathematical reasoning datasets. One approach entails rewriting or regenerating similar problems based on seed data for data augmentation. Another approach entails generating new problems using knowledge points. The \"knowledge points\" refers to fine-grained math concepts (e.g., the Pythagorean theorem, polynomial factorization skills) in problem solving, and they can be generated freshly via LLMs or extracted from existing seed data. Although data synthesis is straightforward, it still suffers from three significant drawbacks: (1) Limited scalability: Existing methods have poor scalability, making it difficult to synthesize larger-scale data from smaller seed data.",
                    "score": 0.3544289123930477,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 255
                        },
                        {
                            "start": 256,
                            "end": 352
                        },
                        {
                            "start": 353,
                            "end": 441
                        },
                        {
                            "start": 442,
                            "end": 473
                        },
                        {
                            "start": 474,
                            "end": 593
                        },
                        {
                            "start": 594,
                            "end": 808
                        },
                        {
                            "start": 809,
                            "end": 936
                        },
                        {
                            "start": 937,
                            "end": 1289
                        },
                        {
                            "start": 1290,
                            "end": 1395
                        },
                        {
                            "start": 1396,
                            "end": 1468
                        },
                        {
                            "start": 1469,
                            "end": 1694
                        },
                        {
                            "start": 1695,
                            "end": 1930
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 986,
                            "end": 1004,
                            "matchedPaperCorpusId": "261696697"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95361328125
                }
            ],
            "relevance_judgement": 0.95361328125,
            "relevance_judgment_input_expanded": "# Title: A Graph-Based Synthetic Data Pipeline for Scaling High-Quality Reasoning Instructions\n# Venue: arXiv.org\n# Authors: Jiankang Wang, Jianjun Xu, Xiaorui Wang, Yuxin Wang, Mengting Xing, Shancheng Fang, Zhineng Chen, Hongtao Xie, Yongdong Zhang\n## Abstract\nSynthesizing high-quality reasoning data for continual training has been proven to be effective in enhancing the performance of Large Language Models (LLMs). However, previous synthetic approaches struggle to easily scale up data and incur high costs in the pursuit of high quality. In this paper, we propose the Graph-based Synthetic Data Pipeline (GSDP), an economical and scalable framework for high-quality reasoning data synthesis. Inspired by knowledge graphs, we extracted knowledge points from seed data and constructed a knowledge point relationships graph to explore their interconnections. By exploring the implicit relationships among knowledge, our method achieves $\\times$255 data expansion. Furthermore, GSDP led by open-source models, achieves synthesis quality comparable to GPT-4-0613 while maintaining $\\times$100 lower costs. To tackle the most challenging mathematical reasoning task, we present the GSDP-MATH dataset comprising over 1.91 million pairs of math problems and answers. After fine-tuning on GSDP-MATH, GSDP-7B based on Mistral-7B achieves 37.7% accuracy on MATH and 78.4% on GSM8K, demonstrating the effectiveness of our method. The dataset and models will be released at https://github.com/Jayce1kk/GSDP.\n## INTRODUCTION\nDespite the remarkable capabilities large language models (LLMs) have demonstrated in various linguistic tasks, significant gaps remain in their ability to comprehend and solve intricate reasoning tasks (e.g., mathematics, coding, physics, and chemistry). One effective approach to bridging these gaps is using large-scale, high-quality synthetic data. However, it is still a challenge to develop a low-cost and effective synthesis pipeline. Take mathematics as an example. The two main approaches for building high-quality mathematics reasoning datasets are data filtering and data synthesis. Data filtering (Yue et al., 2024b;Shao et al., 2024;Ying et al., 2024) involves extracting data from pre-training corpora such as Common Crawl, and rewriting it using advanced commercial models or human annotation. However, the vast scale and inherent noise of these corpora result in high post-processing costs and inconsistent data quality. Data synthesis (Yu et al., 2023;Luo et al., 2023;Yue et al., 2024a;Tang et al., 2024;Huang et al., 2024a;Li et al., 2024a;Toshniwal et al., 2024;Li et al., 2024b) leverages frontier large language models, such as GPT-3.5 (Floridi & Chiriatti, 2020) and GPT-4 (Achiam et al., 2023), to augment or regenerate high-quality mathematical reasoning datasets. One approach entails rewriting or regenerating similar problems based on seed data for data augmentation. Another approach entails generating new problems using knowledge points. The \"knowledge points\" refers to fine-grained math concepts (e.g., the Pythagorean theorem, polynomial factorization skills) in problem solving, and they can be generated freshly via LLMs or extracted from existing seed data. Although data synthesis is straightforward, it still suffers from three significant drawbacks: (1) Limited scalability: Existing methods have poor scalability, making it difficult to synthesize larger-scale data from smaller seed data.",
            "reference_string": "[274656618 | Wang et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Diffusion Models for Tabular Data Imputation and Synthetic Data Generation",
            "venue": "ACM Transactions on Knowledge Discovery from Data",
            "year": 2024,
            "reference_count": 67,
            "citation_count": 8,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.02549, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2309480102",
                    "name": "Mario Villaiz'an-Vallelado"
                },
                {
                    "authorId": "2309482633",
                    "name": "Matteo Salvatori"
                },
                {
                    "authorId": "2366108687",
                    "name": "Carlos Segura Perales"
                },
                {
                    "authorId": "2309480030",
                    "name": "Ioannis Arapakis"
                }
            ],
            "abstract": "Data imputation and data generation have important applications across many domains where incomplete or missing data can hinder accurate analysis and decision-making. Diffusion models have emerged as powerful generative models capable of capturing complex data distributions across various data modalities such as image, audio, and time series. Recently, they have been also adapted to generate tabular data. In this paper, we propose a diffusion model for tabular data that introduces three key enhancements: (1) a conditioning attention mechanism, (2) an encoder-decoder transformer as the denoising network, and (3) dynamic masking. The conditioning attention mechanism is designed to improve the model's ability to capture the relationship between the condition and synthetic data. The transformer layers help model interactions within the condition (encoder) or synthetic data (decoder), while dynamic masking enables our model to efficiently handle both missing data imputation and synthetic data generation tasks within a unified framework. We conduct a comprehensive evaluation by comparing the performance of diffusion models with transformer conditioning against state-of-the-art techniques such as Variational Autoencoders, Generative Adversarial Networks and Diffusion Models, on benchmark datasets. Our evaluation focuses on the assessment of the generated samples with respect to three important criteria, namely: (1) Machine Learning efficiency, (2) statistical similarity, and (3) privacy risk mitigation. For the task of data imputation, we consider the efficiency of the generated samples across different levels of missing features. The results demonstrates average superior machine learning efficiency and statistical accuracy compared to the baselines, while maintaining privacy risks at a comparable level, particularly showing increased performance in datasets with a large number of features. By conditioning the data generation on a desired target variable, the model can mitigate systemic biases, generate augmented datasets to address data imbalance issues, and improve data quality for subsequent analysis. This has significant implications for domains such as healthcare and finance, where accurate, unbiased, and privacy-preserving data are critical for informed decision-making and fair model outcomes.",
            "corpus_id": 270924192,
            "sentences": [],
            "relevance_judgement": 0.95263671875,
            "relevance_judgment_input_expanded": "# Title: Diffusion Models for Tabular Data Imputation and Synthetic Data Generation\n# Venue: ACM Transactions on Knowledge Discovery from Data\n# Authors: Mario Villaiz'an-Vallelado, Matteo Salvatori, Carlos Segura Perales, Ioannis Arapakis\n## Abstract\nData imputation and data generation have important applications across many domains where incomplete or missing data can hinder accurate analysis and decision-making. Diffusion models have emerged as powerful generative models capable of capturing complex data distributions across various data modalities such as image, audio, and time series. Recently, they have been also adapted to generate tabular data. In this paper, we propose a diffusion model for tabular data that introduces three key enhancements: (1) a conditioning attention mechanism, (2) an encoder-decoder transformer as the denoising network, and (3) dynamic masking. The conditioning attention mechanism is designed to improve the model's ability to capture the relationship between the condition and synthetic data. The transformer layers help model interactions within the condition (encoder) or synthetic data (decoder), while dynamic masking enables our model to efficiently handle both missing data imputation and synthetic data generation tasks within a unified framework. We conduct a comprehensive evaluation by comparing the performance of diffusion models with transformer conditioning against state-of-the-art techniques such as Variational Autoencoders, Generative Adversarial Networks and Diffusion Models, on benchmark datasets. Our evaluation focuses on the assessment of the generated samples with respect to three important criteria, namely: (1) Machine Learning efficiency, (2) statistical similarity, and (3) privacy risk mitigation. For the task of data imputation, we consider the efficiency of the generated samples across different levels of missing features. The results demonstrates average superior machine learning efficiency and statistical accuracy compared to the baselines, while maintaining privacy risks at a comparable level, particularly showing increased performance in datasets with a large number of features. By conditioning the data generation on a desired target variable, the model can mitigate systemic biases, generate augmented datasets to address data imbalance issues, and improve data quality for subsequent analysis. This has significant implications for domains such as healthcare and finance, where accurate, unbiased, and privacy-preserving data are critical for informed decision-making and fair model outcomes.\n",
            "reference_string": "[270924192 | Villaiz'an-Vallelado et al. | 2024 | Citations: 8]"
        },
        {
            "title": "Agentic AI: The Era of Semantic Decoding",
            "venue": "",
            "year": 2024,
            "reference_count": 120,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.14562, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "35512303",
                    "name": "Maxime Peyrard"
                },
                {
                    "authorId": "65826567",
                    "name": "Martin Josifoski"
                },
                {
                    "authorId": "2269473532",
                    "name": "Robert West"
                }
            ],
            "abstract": "Recent work demonstrated great promise in the idea of orchestrating collaborations between LLMs, human input, and various tools to address the inherent limitations of LLMs. We propose a novel perspective called semantic decoding, which frames these collaborative processes as optimization procedures in semantic space. Specifically, we conceptualize LLMs as semantic processors that manipulate meaningful pieces of information that we call semantic tokens (known thoughts). LLMs are among a large pool of other semantic processors, including humans and tools, such as search engines or code executors. Collectively, semantic processors engage in dynamic exchanges of semantic tokens to progressively construct high-utility outputs. We refer to these orchestrated interactions among semantic processors, optimizing and searching in semantic space, as semantic decoding algorithms. This concept draws a direct parallel to the well-studied problem of syntactic decoding, which involves crafting algorithms to best exploit auto-regressive language models for extracting high-utility sequences of syntactic tokens. By focusing on the semantic level and disregarding syntactic details, we gain a fresh perspective on the engineering of AI systems, enabling us to imagine systems with much greater complexity and capabilities. In this position paper, we formalize the transition from syntactic to semantic tokens as well as the analogy between syntactic and semantic decoding. Subsequently, we explore the possibilities of optimizing within the space of semantic tokens via semantic decoding algorithms. We conclude with a list of research opportunities and questions arising from this fresh perspective. The semantic decoding perspective offers a powerful abstraction for search and optimization directly in the space of meaningful concepts, with semantic tokens as the fundamental units of a new type of computation.",
            "corpus_id": 268553593,
            "sentences": [
                {
                    "corpus_id": "268553593",
                    "title": "Agentic AI: The Era of Semantic Decoding",
                    "text": "Data, itself composed of semantic tokens, can become the focal point for Flows dedicated to manipulating or synthesizing data. Similarly, specialized trainer Flows can be aimed at training semantic processors (e.g., models) or even entire (sub-)Flows. By combining trainer Flows with synthetic data generation Flows, opportunities emerge for creating sophisticated self-training loops. The synthetic data generation Flows can leverage domain knowledge (Tang et al., 2023), task properties (Lu et al., 2024;Veselovsky et al., 2023;Josifoski et al., 2023c), or collaboration (Abdullin et al., 2024), and synthesize data of notably higher quality than what a single model or simple heuristics can achieve. This sets the stage for effective self-improvement loops where a language model participates in a semantic decoding algorithm producing high-quality synthetic data. Then, the language model improves itself through fine-tuning, thereby improving the Flow's capacity to generate even better synthetic data in a virtuous cycle (Silver et al., 2017;Burns et al., 2023;Singh et al., 2023;Chen et al., 2024b). An example of such a Flow is MAGDi (Chen et al., 2024a), a framework designed to distill reasoning interactions among multiple LLMs into smaller ones. This approach surpasses single-teacher distillation (Li et al., 2023c;Magister et al., 2023) and finetuning based on reasoning trajectories sampled from GPT-4 (Chen et al., 2023a).",
                    "score": 0.40463605752393145,
                    "section_title": "Synthetic Data Flow",
                    "char_start_offset": 45140,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 126
                        },
                        {
                            "start": 127,
                            "end": 251
                        },
                        {
                            "start": 252,
                            "end": 385
                        },
                        {
                            "start": 386,
                            "end": 702
                        },
                        {
                            "start": 703,
                            "end": 867
                        },
                        {
                            "start": 868,
                            "end": 1106
                        },
                        {
                            "start": 1107,
                            "end": 1257
                        },
                        {
                            "start": 1258,
                            "end": 1438
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 530,
                            "end": 554,
                            "matchedPaperCorpusId": "257378179"
                        },
                        {
                            "start": 1310,
                            "end": 1328,
                            "matchedPaperCorpusId": "259251773"
                        },
                        {
                            "start": 1328,
                            "end": 1350,
                            "matchedPaperCorpusId": "254823156"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95166015625
                }
            ],
            "relevance_judgement": 0.95166015625,
            "relevance_judgment_input_expanded": "# Title: Agentic AI: The Era of Semantic Decoding\n# Venue: \n# Authors: Maxime Peyrard, Martin Josifoski, Robert West\n## Abstract\nRecent work demonstrated great promise in the idea of orchestrating collaborations between LLMs, human input, and various tools to address the inherent limitations of LLMs. We propose a novel perspective called semantic decoding, which frames these collaborative processes as optimization procedures in semantic space. Specifically, we conceptualize LLMs as semantic processors that manipulate meaningful pieces of information that we call semantic tokens (known thoughts). LLMs are among a large pool of other semantic processors, including humans and tools, such as search engines or code executors. Collectively, semantic processors engage in dynamic exchanges of semantic tokens to progressively construct high-utility outputs. We refer to these orchestrated interactions among semantic processors, optimizing and searching in semantic space, as semantic decoding algorithms. This concept draws a direct parallel to the well-studied problem of syntactic decoding, which involves crafting algorithms to best exploit auto-regressive language models for extracting high-utility sequences of syntactic tokens. By focusing on the semantic level and disregarding syntactic details, we gain a fresh perspective on the engineering of AI systems, enabling us to imagine systems with much greater complexity and capabilities. In this position paper, we formalize the transition from syntactic to semantic tokens as well as the analogy between syntactic and semantic decoding. Subsequently, we explore the possibilities of optimizing within the space of semantic tokens via semantic decoding algorithms. We conclude with a list of research opportunities and questions arising from this fresh perspective. The semantic decoding perspective offers a powerful abstraction for search and optimization directly in the space of meaningful concepts, with semantic tokens as the fundamental units of a new type of computation.\n## Synthetic Data Flow\nData, itself composed of semantic tokens, can become the focal point for Flows dedicated to manipulating or synthesizing data. Similarly, specialized trainer Flows can be aimed at training semantic processors (e.g., models) or even entire (sub-)Flows. By combining trainer Flows with synthetic data generation Flows, opportunities emerge for creating sophisticated self-training loops. The synthetic data generation Flows can leverage domain knowledge (Tang et al., 2023), task properties (Lu et al., 2024;Veselovsky et al., 2023;Josifoski et al., 2023c), or collaboration (Abdullin et al., 2024), and synthesize data of notably higher quality than what a single model or simple heuristics can achieve. This sets the stage for effective self-improvement loops where a language model participates in a semantic decoding algorithm producing high-quality synthetic data. Then, the language model improves itself through fine-tuning, thereby improving the Flow's capacity to generate even better synthetic data in a virtuous cycle (Silver et al., 2017;Burns et al., 2023;Singh et al., 2023;Chen et al., 2024b). An example of such a Flow is MAGDi (Chen et al., 2024a), a framework designed to distill reasoning interactions among multiple LLMs into smaller ones. This approach surpasses single-teacher distillation (Li et al., 2023c;Magister et al., 2023) and finetuning based on reasoning trajectories sampled from GPT-4 (Chen et al., 2023a).",
            "reference_string": "[268553593 | Peyrard et al. | 2024 | Citations: 0]"
        },
        {
            "title": "LAB: Large-Scale Alignment for ChatBots",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 22,
            "citation_count": 34,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.01081, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2114819100",
                    "name": "Shivchander Sudalairaj"
                },
                {
                    "authorId": "150894502",
                    "name": "Abhishek Bhandwaldar"
                },
                {
                    "authorId": "2288530668",
                    "name": "Aldo Pareja"
                },
                {
                    "authorId": "2261102120",
                    "name": "Kai Xu"
                },
                {
                    "authorId": "2289845472",
                    "name": "David D. Cox"
                },
                {
                    "authorId": "2243025154",
                    "name": "Akash Srivastava"
                }
            ],
            "abstract": "This work introduces LAB (Large-scale Alignment for chatBots), a novel methodology designed to overcome the scalability challenges in the instruction-tuning phase of large language model (LLM) training. Leveraging a taxonomy-guided synthetic data generation process and a multi-phase tuning framework, LAB significantly reduces reliance on expensive human annotations and proprietary models like GPT-4. We demonstrate that LAB-trained models can achieve competitive performance across several benchmarks compared to models trained with traditional human-annotated or GPT-4 generated synthetic data. Thus offering a scalable, cost-effective solution for enhancing LLM capabilities and instruction-following behaviors without the drawbacks of catastrophic forgetting, marking a step forward in the efficient training of LLMs for a wide range of applications.",
            "corpus_id": 268230871,
            "sentences": [
                {
                    "corpus_id": "268230871",
                    "title": "LAB: Large-Scale Alignment for ChatBots",
                    "text": "Xu et al. (2023) introduces Evol-Instruct, another variant of Self-Instruct, that synthesizes iteratively more complex instruction to overcome shortcomings of previous methods. Mukherjee et al. (2023), Mitra et al. (2023) present a synthetic data generation approach to enhance task diversity and scalability, alongside a progressive training framework aimed at improving the model's reasoning ability and response style to match teacher models. This is achieved by generating rich reasoning signals in the generated answer and progressively training on datasets of varying difficulty in incremental phases. \n\nSimilar to LAB, concurrent work, GLAN (Li et al., 2024), employs a semi-automatic approach to synthetic data generation that uses a human-curated taxonomy to generate instruction tuning data from a teacher model. However, as explained in section 3.2.2, unlike LAB, GLAN cannot be used to generate synthetic data from domains that are not captured in the teacher model's support. As such, while LAB uses the open-source Mixtral model as the teacher, like many other synthetic data generation approaches, GLAN has to rely on a large proprietary model (GPT-4). This poses complicated questions about the usability of generated data (especially for commercial purposes) since the terms of use of proprietary models typically forbid using the model to improve other models.",
                    "score": 0.39711861865809817,
                    "section_title": "RELATED WORK",
                    "char_start_offset": 6021,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 176
                        },
                        {
                            "start": 177,
                            "end": 445
                        },
                        {
                            "start": 446,
                            "end": 607
                        },
                        {
                            "start": 610,
                            "end": 822
                        },
                        {
                            "start": 823,
                            "end": 862
                        },
                        {
                            "start": 863,
                            "end": 988
                        },
                        {
                            "start": 989,
                            "end": 1167
                        },
                        {
                            "start": 1168,
                            "end": 1378
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 648,
                            "end": 665,
                            "matchedPaperCorpusId": "259129398"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.951171875
                }
            ],
            "relevance_judgement": 0.951171875,
            "relevance_judgment_input_expanded": "# Title: LAB: Large-Scale Alignment for ChatBots\n# Venue: arXiv.org\n# Authors: Shivchander Sudalairaj, Abhishek Bhandwaldar, Aldo Pareja, Kai Xu, David D. Cox, Akash Srivastava\n## Abstract\nThis work introduces LAB (Large-scale Alignment for chatBots), a novel methodology designed to overcome the scalability challenges in the instruction-tuning phase of large language model (LLM) training. Leveraging a taxonomy-guided synthetic data generation process and a multi-phase tuning framework, LAB significantly reduces reliance on expensive human annotations and proprietary models like GPT-4. We demonstrate that LAB-trained models can achieve competitive performance across several benchmarks compared to models trained with traditional human-annotated or GPT-4 generated synthetic data. Thus offering a scalable, cost-effective solution for enhancing LLM capabilities and instruction-following behaviors without the drawbacks of catastrophic forgetting, marking a step forward in the efficient training of LLMs for a wide range of applications.\n## RELATED WORK\nXu et al. (2023) introduces Evol-Instruct, another variant of Self-Instruct, that synthesizes iteratively more complex instruction to overcome shortcomings of previous methods. Mukherjee et al. (2023), Mitra et al. (2023) present a synthetic data generation approach to enhance task diversity and scalability, alongside a progressive training framework aimed at improving the model's reasoning ability and response style to match teacher models. This is achieved by generating rich reasoning signals in the generated answer and progressively training on datasets of varying difficulty in incremental phases. \n\nSimilar to LAB, concurrent work, GLAN (Li et al., 2024), employs a semi-automatic approach to synthetic data generation that uses a human-curated taxonomy to generate instruction tuning data from a teacher model. However, as explained in section 3.2.2, unlike LAB, GLAN cannot be used to generate synthetic data from domains that are not captured in the teacher model's support. As such, while LAB uses the open-source Mixtral model as the teacher, like many other synthetic data generation approaches, GLAN has to rely on a large proprietary model (GPT-4). This poses complicated questions about the usability of generated data (especially for commercial purposes) since the terms of use of proprietary models typically forbid using the model to improve other models.",
            "reference_string": "[268230871 | Sudalairaj et al. | 2024 | Citations: 34]"
        },
        {
            "title": "Enhancing Logical Reasoning in Large Language Models through Graph-based Synthetic Data",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 36,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.12437, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2297828274",
                    "name": "Jiaming Zhou"
                },
                {
                    "authorId": "2321867953",
                    "name": "Abbas Ghaddar"
                },
                {
                    "authorId": "2321875142",
                    "name": "Ge Zhang"
                },
                {
                    "authorId": "1892081076",
                    "name": "Liheng Ma"
                },
                {
                    "authorId": "2288403553",
                    "name": "Yaochen Hu"
                },
                {
                    "authorId": "38939190",
                    "name": "Soumyasundar Pal"
                },
                {
                    "authorId": "2287938359",
                    "name": "Mark Coates"
                },
                {
                    "authorId": "2321911323",
                    "name": "Bin Wang"
                },
                {
                    "authorId": "2275529643",
                    "name": "Yingxue Zhang"
                },
                {
                    "authorId": "2307072418",
                    "name": "Jianye Hao"
                }
            ],
            "abstract": "Despite recent advances in training and prompting strategies for Large Language Models (LLMs), these models continue to face challenges with complex logical reasoning tasks that involve long reasoning chains. In this work, we explore the potential and limitations of using graph-based synthetic reasoning data as training signals to enhance LLMs' reasoning capabilities. Our extensive experiments, conducted on two established natural language reasoning tasks -- inductive reasoning and spatial reasoning -- demonstrate that supervised fine-tuning (SFT) with synthetic graph-based reasoning data effectively enhances LLMs' reasoning performance without compromising their effectiveness on other standard evaluation benchmarks.",
            "corpus_id": 272753147,
            "sentences": [
                {
                    "corpus_id": "272753147",
                    "title": "Enhancing Logical Reasoning in Large Language Models through Graph-based Synthetic Data",
                    "text": "The reasoning capabilities of Large Language Models (LLMs) (Touvron et al., 2023;Jiang et al., 2023;Dubey et al., 2024) can be greatly enhanced by post-training techniques (Ouyang et al., 2022;Zhang et al., 2023) and prompting strategies (Wei et al., 2022b;Yao et al., 2023;Madaan et al., 2023). However, even with the aforementioned techniques, the multi-hop reasoning tasks remain challenging (Touvron et al., 2023;Jiang et al., 2023;Dubey et al., 2024): LLMs struggle to reason over steps (Agrawal et al., 2024;Zhao and Zhang, 2024), and are fragile to minor perturbations (Ullman, 2023;Chen et al., 2024) in the input prompt. \n\nRecently, several works (Xu et al., 2024;Abdin et al., 2024;Anil et al., 2023) have demonstrated the efficacy of boosting the LLMs' reasoning capacity via fine-tuning on synthetic data generated by stronger LLMs. However, how to make such synthetic data generation effective and controllable for specific applications remains an open question. \n\nExtensive prompt engineering and quality filtering are required to guide LLMs' generation, yet the quality of generated reasoning questions and their labels remains uncertain (Gudibande et al., 2023;Wang et al., 2023;Tan et al., 2024). Motivated by the fact that natural language reasoning tasks can be represented as structured data with finite nodes and edges (Jin et al., 2024), and inspired by existing works on constructing reasoning benchmarks (Fatemi et al., 2024;Kazemi et al., 2023;Agrawal et al., 2024), we propose to leverage synthetic graph-based data for task-specific posttraining adaptation to improve the correctness of the generated reasoning questions and labels. \n\nIn this paper, we carefully design a random walk sampling algorithm on graphs and introduce a new prompting strategy that first extracts a reasoning chain and then derives the answer.",
                    "score": 0.3810596343791894,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 295
                        },
                        {
                            "start": 296,
                            "end": 629
                        },
                        {
                            "start": 632,
                            "end": 844
                        },
                        {
                            "start": 845,
                            "end": 975
                        },
                        {
                            "start": 978,
                            "end": 1213
                        },
                        {
                            "start": 1214,
                            "end": 1659
                        },
                        {
                            "start": 1662,
                            "end": 1845
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 590,
                            "end": 608,
                            "matchedPaperCorpusId": "267657940"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.951171875
                },
                {
                    "corpus_id": "272753147",
                    "title": "Enhancing Logical Reasoning in Large Language Models through Graph-based Synthetic Data",
                    "text": "Despite recent advances in training and prompting strategies for Large Language Models (LLMs), these models continue to face challenges with complex logical reasoning tasks that involve long reasoning chains. In this work, we explore the potential and limitations of using graph-based synthetic reasoning data as training signals to enhance LLMs' reasoning capabilities. Our extensive experiments, conducted on two established natural language reasoning tasks -- inductive reasoning and spatial reasoning -- demonstrate that supervised fine-tuning (SFT) with synthetic graph-based reasoning data effectively enhances LLMs' reasoning performance without compromising their effectiveness on other standard evaluation benchmarks.",
                    "score": 0.38998210231146757,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.943359375
                }
            ],
            "relevance_judgement": 0.951171875,
            "relevance_judgment_input_expanded": "# Title: Enhancing Logical Reasoning in Large Language Models through Graph-based Synthetic Data\n# Venue: arXiv.org\n# Authors: Jiaming Zhou, Abbas Ghaddar, Ge Zhang, Liheng Ma, Yaochen Hu, Soumyasundar Pal, Mark Coates, Bin Wang, Yingxue Zhang, Jianye Hao\n## Abstract\nDespite recent advances in training and prompting strategies for Large Language Models (LLMs), these models continue to face challenges with complex logical reasoning tasks that involve long reasoning chains. In this work, we explore the potential and limitations of using graph-based synthetic reasoning data as training signals to enhance LLMs' reasoning capabilities. Our extensive experiments, conducted on two established natural language reasoning tasks -- inductive reasoning and spatial reasoning -- demonstrate that supervised fine-tuning (SFT) with synthetic graph-based reasoning data effectively enhances LLMs' reasoning performance without compromising their effectiveness on other standard evaluation benchmarks.\n## Introduction\nThe reasoning capabilities of Large Language Models (LLMs) (Touvron et al., 2023;Jiang et al., 2023;Dubey et al., 2024) can be greatly enhanced by post-training techniques (Ouyang et al., 2022;Zhang et al., 2023) and prompting strategies (Wei et al., 2022b;Yao et al., 2023;Madaan et al., 2023). However, even with the aforementioned techniques, the multi-hop reasoning tasks remain challenging (Touvron et al., 2023;Jiang et al., 2023;Dubey et al., 2024): LLMs struggle to reason over steps (Agrawal et al., 2024;Zhao and Zhang, 2024), and are fragile to minor perturbations (Ullman, 2023;Chen et al., 2024) in the input prompt. \n\nRecently, several works (Xu et al., 2024;Abdin et al., 2024;Anil et al., 2023) have demonstrated the efficacy of boosting the LLMs' reasoning capacity via fine-tuning on synthetic data generated by stronger LLMs. However, how to make such synthetic data generation effective and controllable for specific applications remains an open question. \n\nExtensive prompt engineering and quality filtering are required to guide LLMs' generation, yet the quality of generated reasoning questions and their labels remains uncertain (Gudibande et al., 2023;Wang et al., 2023;Tan et al., 2024). Motivated by the fact that natural language reasoning tasks can be represented as structured data with finite nodes and edges (Jin et al., 2024), and inspired by existing works on constructing reasoning benchmarks (Fatemi et al., 2024;Kazemi et al., 2023;Agrawal et al., 2024), we propose to leverage synthetic graph-based data for task-specific posttraining adaptation to improve the correctness of the generated reasoning questions and labels. \n\nIn this paper, we carefully design a random walk sampling algorithm on graphs and introduce a new prompting strategy that first extracts a reasoning chain and then derives the answer.",
            "reference_string": "[272753147 | Zhou et al. | 2024 | Citations: 4]"
        },
        {
            "title": "SoftSRV: Learn to Generate Targeted Synthetic Data",
            "venue": "",
            "year": 2024,
            "reference_count": 35,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.16534, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2280911353",
                    "name": "Giulia DeSalvo"
                },
                {
                    "authorId": "2327046826",
                    "name": "Jean-Fracois Kagy"
                },
                {
                    "authorId": "2283008",
                    "name": "Lazaros Karydas"
                },
                {
                    "authorId": "2435268",
                    "name": "Afshin Rostamizadeh"
                },
                {
                    "authorId": "2275226495",
                    "name": "Sanjiv Kumar"
                }
            ],
            "abstract": "We present a novel framework, SoftSRV, that is used to generate targeted synthetic fine-tuning data for improving task-specific model performance. Given a sample from a target distribution, our proposed framework uses a data-driven loss minimization approach to steer a frozen large language model (LLM) to generate synthetic sequences that are similar to those from the target distribution. SoftSRV provides a practical improvement over common prompt engineering approaches that rely on human-engineered prompt-templates, which can be idiosyncratic, labor-intensive to craft, and may need to be specialized per domain. We empirically evaluate our method against standard baselines guiding a large LLM to generate synthetic data to fine-tune a smaller language model on three different domains (coding, math, reasoning). We perform these evaluations without any particular specialization of the framework to each domain, emphasizing the generality of our approach. We find that SoftSRV improves upon typical prompt engineering approaches, generating targeted data that leads to fine-tuned models with significantly better task-specific performance. In addition, SoftSRV-generated data better matches the target distribution according to the MAUVE similarity metric.",
            "corpus_id": 273507197,
            "sentences": [
                {
                    "corpus_id": "273507197",
                    "title": "SoftSRV: Learn to Generate Targeted Synthetic Data",
                    "text": "In this work, we established the effectiveness of the SoftSRV framework for generating targeted synthetic fine-tuning data. We deploy the same SoftSRV pipeline across math, coding, and reasoning tasks, finding in each case that SoftSRV generates fine-tuning data that provides strong downstream performance with no manual prompt engineering or per-domain specialization needed. A potentially fruitful direction for future work is to adaptively select or generate context vectors z, in order to generate even more impactful synthetic data.",
                    "score": 0.34875255336180516,
                    "section_title": "CONCLUSION",
                    "char_start_offset": 30640,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 123
                        },
                        {
                            "start": 124,
                            "end": 377
                        },
                        {
                            "start": 378,
                            "end": 538
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94921875
                }
            ],
            "relevance_judgement": 0.94921875,
            "relevance_judgment_input_expanded": "# Title: SoftSRV: Learn to Generate Targeted Synthetic Data\n# Venue: \n# Authors: Giulia DeSalvo, Jean-Fracois Kagy, Lazaros Karydas, Afshin Rostamizadeh, Sanjiv Kumar\n## Abstract\nWe present a novel framework, SoftSRV, that is used to generate targeted synthetic fine-tuning data for improving task-specific model performance. Given a sample from a target distribution, our proposed framework uses a data-driven loss minimization approach to steer a frozen large language model (LLM) to generate synthetic sequences that are similar to those from the target distribution. SoftSRV provides a practical improvement over common prompt engineering approaches that rely on human-engineered prompt-templates, which can be idiosyncratic, labor-intensive to craft, and may need to be specialized per domain. We empirically evaluate our method against standard baselines guiding a large LLM to generate synthetic data to fine-tune a smaller language model on three different domains (coding, math, reasoning). We perform these evaluations without any particular specialization of the framework to each domain, emphasizing the generality of our approach. We find that SoftSRV improves upon typical prompt engineering approaches, generating targeted data that leads to fine-tuned models with significantly better task-specific performance. In addition, SoftSRV-generated data better matches the target distribution according to the MAUVE similarity metric.\n## CONCLUSION\nIn this work, we established the effectiveness of the SoftSRV framework for generating targeted synthetic fine-tuning data. We deploy the same SoftSRV pipeline across math, coding, and reasoning tasks, finding in each case that SoftSRV generates fine-tuning data that provides strong downstream performance with no manual prompt engineering or per-domain specialization needed. A potentially fruitful direction for future work is to adaptively select or generate context vectors z, in order to generate even more impactful synthetic data.",
            "reference_string": "[273507197 | DeSalvo et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Exploring the Landscape for Generative Sequence Models for Specialized Data Synthesis",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 56,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.01929, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2329103285",
                    "name": "Mohammad Zbeeb"
                },
                {
                    "authorId": "2329102879",
                    "name": "Mohammad Ghorayeb"
                },
                {
                    "authorId": "2329103319",
                    "name": "Mariam Salman"
                }
            ],
            "abstract": "Artificial Intelligence (AI) research often aims to develop models that can generalize reliably across complex datasets, yet this remains challenging in fields where data is scarce, intricate, or inaccessible. This paper introduces a novel approach that leverages three generative models of varying complexity to synthesize one of the most demanding structured datasets: Malicious Network Traffic. Our approach uniquely transforms numerical data into text, re-framing data generation as a language modeling task, which not only enhances data regularization but also significantly improves generalization and the quality of the synthetic data. Extensive statistical analyses demonstrate that our method surpasses state-of-the-art generative models in producing high-fidelity synthetic data. Additionally, we conduct a comprehensive study on synthetic data applications, effectiveness, and evaluation strategies, offering valuable insights into its role across various domains. Our code and pre-trained models are openly accessible at Github, enabling further exploration and application of our methodology. Index Terms: Data synthesis, machine learning, traffic generation, privacy preserving data, generative models.",
            "corpus_id": 273811804,
            "sentences": [
                {
                    "corpus_id": "273811804",
                    "title": "Exploring the Landscape for Generative Sequence Models for Specialized Data Synthesis",
                    "text": "To further advance the field of synthetic data generation, several key areas warrant additional exploration and development. One significant avenue is the capability to generate larger and more diverse datasets. Expanding the capacity to synthesize extensive datasets with high variability would greatly enhance the applicability of synthetic data in machine learning tasks, especially in domains where data scarcity remains a challenge. \n\nMoreover, exploring innovative architectures beyond the current models can lead to substantial advancements. Investigating new generative models or enhancing existing ones could improve the quality and diversity of synthetic data. Importantly, demonstrating that these advancements can be achieved using accessible computational resources, such as a personal computer with a well-coded pipeline, would underscore the feasibility of cutting-edge AI developments without the need for extensive infrastructure. This democratization of technology could encourage broader participation in the field and accelerate innovation. \n\nAdditionally, integrating more robust privacy-preserving techniques into the data generation process remains a critical area for future work. As privacy concerns continue to grow, developing methods that ensure data utility while rigorously protecting sensitive information is essential. Combining differential privacy mechanisms with generative models could provide stronger guarantees and expand the adoption of synthetic data in sensitive domains. \n\nFinally, applying synthetic data generation techniques to a wider range of applications, including those with complex data types such as time-series, graphs, and multimodal data, would significantly broaden the impact of this research. Tailoring generative models to handle these complex data structures effectively could open new opportunities in various fields, from healthcare to finance, where such data types are prevalent.",
                    "score": 0.3614349632155474,
                    "section_title": "X Future Work",
                    "char_start_offset": 38230,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 124
                        },
                        {
                            "start": 125,
                            "end": 211
                        },
                        {
                            "start": 212,
                            "end": 437
                        },
                        {
                            "start": 440,
                            "end": 548
                        },
                        {
                            "start": 549,
                            "end": 670
                        },
                        {
                            "start": 671,
                            "end": 947
                        },
                        {
                            "start": 948,
                            "end": 1060
                        },
                        {
                            "start": 1063,
                            "end": 1204
                        },
                        {
                            "start": 1205,
                            "end": 1350
                        },
                        {
                            "start": 1351,
                            "end": 1513
                        },
                        {
                            "start": 1516,
                            "end": 1751
                        },
                        {
                            "start": 1752,
                            "end": 1944
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.943359375
                }
            ],
            "relevance_judgement": 0.943359375,
            "relevance_judgment_input_expanded": "# Title: Exploring the Landscape for Generative Sequence Models for Specialized Data Synthesis\n# Venue: arXiv.org\n# Authors: Mohammad Zbeeb, Mohammad Ghorayeb, Mariam Salman\n## Abstract\nArtificial Intelligence (AI) research often aims to develop models that can generalize reliably across complex datasets, yet this remains challenging in fields where data is scarce, intricate, or inaccessible. This paper introduces a novel approach that leverages three generative models of varying complexity to synthesize one of the most demanding structured datasets: Malicious Network Traffic. Our approach uniquely transforms numerical data into text, re-framing data generation as a language modeling task, which not only enhances data regularization but also significantly improves generalization and the quality of the synthetic data. Extensive statistical analyses demonstrate that our method surpasses state-of-the-art generative models in producing high-fidelity synthetic data. Additionally, we conduct a comprehensive study on synthetic data applications, effectiveness, and evaluation strategies, offering valuable insights into its role across various domains. Our code and pre-trained models are openly accessible at Github, enabling further exploration and application of our methodology. Index Terms: Data synthesis, machine learning, traffic generation, privacy preserving data, generative models.\n## X Future Work\nTo further advance the field of synthetic data generation, several key areas warrant additional exploration and development. One significant avenue is the capability to generate larger and more diverse datasets. Expanding the capacity to synthesize extensive datasets with high variability would greatly enhance the applicability of synthetic data in machine learning tasks, especially in domains where data scarcity remains a challenge. \n\nMoreover, exploring innovative architectures beyond the current models can lead to substantial advancements. Investigating new generative models or enhancing existing ones could improve the quality and diversity of synthetic data. Importantly, demonstrating that these advancements can be achieved using accessible computational resources, such as a personal computer with a well-coded pipeline, would underscore the feasibility of cutting-edge AI developments without the need for extensive infrastructure. This democratization of technology could encourage broader participation in the field and accelerate innovation. \n\nAdditionally, integrating more robust privacy-preserving techniques into the data generation process remains a critical area for future work. As privacy concerns continue to grow, developing methods that ensure data utility while rigorously protecting sensitive information is essential. Combining differential privacy mechanisms with generative models could provide stronger guarantees and expand the adoption of synthetic data in sensitive domains. \n\nFinally, applying synthetic data generation techniques to a wider range of applications, including those with complex data types such as time-series, graphs, and multimodal data, would significantly broaden the impact of this research. Tailoring generative models to handle these complex data structures effectively could open new opportunities in various fields, from healthcare to finance, where such data types are prevalent.",
            "reference_string": "[273811804 | Zbeeb et al. | 2024 | Citations: 0]"
        },
        {
            "title": "LogicPro: Improving Complex Logical Reasoning via Program-Guided Learning",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 78,
            "citation_count": 7,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.12929, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2319385003",
                    "name": "Jin Jiang"
                },
                {
                    "authorId": "2284984220",
                    "name": "Yuchen Yan"
                },
                {
                    "authorId": "2316670312",
                    "name": "Yang Liu"
                },
                {
                    "authorId": "2321879179",
                    "name": "Yonggang Jin"
                },
                {
                    "authorId": "2072715089",
                    "name": "Shuai Peng"
                },
                {
                    "authorId": "2284132141",
                    "name": "Mengdi Zhang"
                },
                {
                    "authorId": "2317059694",
                    "name": "Xunliang Cai"
                },
                {
                    "authorId": "2319459835",
                    "name": "Yixin Cao"
                },
                {
                    "authorId": "2165982472",
                    "name": "Liangcai Gao"
                },
                {
                    "authorId": "2268046262",
                    "name": "Zhi Tang"
                }
            ],
            "abstract": "In this paper, we propose a new data synthesis method called \\textbf{LogicPro}, which leverages LeetCode-style algorithm \\underline{Pro}blems and their corresponding \\underline{Pro}gram solutions to synthesize Complex \\underline{Logic}al Reasoning data in text format. First, we synthesize complex reasoning problems through source algorithm problems and test cases. Then, standard answers and intermediate variable outputs are obtained for each problem based on standard python solutions and test cases. Finally, with the guidance of code intermediate variables, we synthesize the text reasoning process for each reasoning problems. Through this method, we can synthesize data that is difficult, scalable, effective, and comes with golden standard answers and high-quality reasoning processes. As a result, with our 540K synthesized dataset constructed solely from 2,360 algorithm problems, our approach Code and data are publicly available at https://github.com/jiangjin1999/LogicPro achieves significant improvements in multiple models for the datasets \\textit{BBH$^{27}$}, \\textit{LogicBench}, \\textit{DROP}, \\textit{AR-LSAT}, and \\textit{GSM8K}, etc. outperforming a wide range of existing reasoning datasets.",
            "corpus_id": 272753174,
            "sentences": [
                {
                    "corpus_id": "272753174",
                    "title": "LogicPro: Improving Complex Logical Reasoning via Program-Guided Learning",
                    "text": "Synthetic data has played an important role in enhancing LLMs' reasoning capabilities (Dubey et al., 2024), especially in mathematics and coding domains. \n\nFor mathematical reasoning, synthetic data generation includes problem-driven methods such as evol-instruct (Luo et al., 2023a;Zeng et al., 2024), problem restatement (Yu et al., 2023), backtranslation (Lu et al., 2024), and few-shot examples (Dalvi et al., 2021), as well as knowledge-driven methods that rely on knowledge bases (Dalvi et al., 2021) or concept graphs (Tang et al., 2024) to generate new problems by sampling key reasoning points (Huang et al., 2024). \n\nIn terms of code reasoning, from Code Alpaca's (Wang et al., 2023) use of self-instruct to generate 20K instruction data based on seed tasks, to Wiz-ardCoder's (Luo et al., 2023b) use of Code evolinstruct to generate more complex tasks, to Magicoder's use of oss-instruct (Wei et al., 2024) to extract 75K instructions from open source code. Synthesizing data continuously improves the model's code reasoning capabilities. \n\nIn contrast, there is less research on synthetic data for complex logical reasoning. Learning combines formal logic theory (Morishita et al., 2023) to synthesize data using basic reasoning rules to train language models' multi-step reasoning abilities. LogicBench (Parmar et al., 2024) not only constructs logical reasoning benchmark datasets but also provides synthetic data based on formal logic. \n\nLeetCode-style algorithm problems contain rich reasoning processes. This paper synthesizes highquality complex logical sreasoning data based on the formal logic of programming languages to enhance models' reasoning capabilities.",
                    "score": 0.46012733454595806,
                    "section_title": "A Related work A.1 Synthetic Reasoning Data",
                    "char_start_offset": 25934,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 153
                        },
                        {
                            "start": 156,
                            "end": 624
                        },
                        {
                            "start": 627,
                            "end": 968
                        },
                        {
                            "start": 969,
                            "end": 1049
                        },
                        {
                            "start": 1052,
                            "end": 1136
                        },
                        {
                            "start": 1137,
                            "end": 1304
                        },
                        {
                            "start": 1305,
                            "end": 1450
                        },
                        {
                            "start": 1453,
                            "end": 1520
                        },
                        {
                            "start": 1521,
                            "end": 1681
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 399,
                            "end": 419,
                            "matchedPaperCorpusId": "233297051"
                        },
                        {
                            "start": 486,
                            "end": 506,
                            "matchedPaperCorpusId": "233297051"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93896484375
                }
            ],
            "relevance_judgement": 0.93896484375,
            "relevance_judgment_input_expanded": "# Title: LogicPro: Improving Complex Logical Reasoning via Program-Guided Learning\n# Venue: arXiv.org\n# Authors: Jin Jiang, Yuchen Yan, Yang Liu, Yonggang Jin, Shuai Peng, Mengdi Zhang, Xunliang Cai, Yixin Cao, Liangcai Gao, Zhi Tang\n## Abstract\nIn this paper, we propose a new data synthesis method called \\textbf{LogicPro}, which leverages LeetCode-style algorithm \\underline{Pro}blems and their corresponding \\underline{Pro}gram solutions to synthesize Complex \\underline{Logic}al Reasoning data in text format. First, we synthesize complex reasoning problems through source algorithm problems and test cases. Then, standard answers and intermediate variable outputs are obtained for each problem based on standard python solutions and test cases. Finally, with the guidance of code intermediate variables, we synthesize the text reasoning process for each reasoning problems. Through this method, we can synthesize data that is difficult, scalable, effective, and comes with golden standard answers and high-quality reasoning processes. As a result, with our 540K synthesized dataset constructed solely from 2,360 algorithm problems, our approach Code and data are publicly available at https://github.com/jiangjin1999/LogicPro achieves significant improvements in multiple models for the datasets \\textit{BBH$^{27}$}, \\textit{LogicBench}, \\textit{DROP}, \\textit{AR-LSAT}, and \\textit{GSM8K}, etc. outperforming a wide range of existing reasoning datasets.\n## A Related work A.1 Synthetic Reasoning Data\nSynthetic data has played an important role in enhancing LLMs' reasoning capabilities (Dubey et al., 2024), especially in mathematics and coding domains. \n\nFor mathematical reasoning, synthetic data generation includes problem-driven methods such as evol-instruct (Luo et al., 2023a;Zeng et al., 2024), problem restatement (Yu et al., 2023), backtranslation (Lu et al., 2024), and few-shot examples (Dalvi et al., 2021), as well as knowledge-driven methods that rely on knowledge bases (Dalvi et al., 2021) or concept graphs (Tang et al., 2024) to generate new problems by sampling key reasoning points (Huang et al., 2024). \n\nIn terms of code reasoning, from Code Alpaca's (Wang et al., 2023) use of self-instruct to generate 20K instruction data based on seed tasks, to Wiz-ardCoder's (Luo et al., 2023b) use of Code evolinstruct to generate more complex tasks, to Magicoder's use of oss-instruct (Wei et al., 2024) to extract 75K instructions from open source code. Synthesizing data continuously improves the model's code reasoning capabilities. \n\nIn contrast, there is less research on synthetic data for complex logical reasoning. Learning combines formal logic theory (Morishita et al., 2023) to synthesize data using basic reasoning rules to train language models' multi-step reasoning abilities. LogicBench (Parmar et al., 2024) not only constructs logical reasoning benchmark datasets but also provides synthetic data based on formal logic. \n\nLeetCode-style algorithm problems contain rich reasoning processes. This paper synthesizes highquality complex logical sreasoning data based on the formal logic of programming languages to enhance models' reasoning capabilities.",
            "reference_string": "[272753174 | Jiang et al. | 2024 | Citations: 7]"
        },
        {
            "title": "RV-Syn: Rational and Verifiable Mathematical Reasoning Data Synthesis based on Structured Function Library",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 38,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.20426, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2302813110",
                    "name": "Jiapeng Wang"
                },
                {
                    "authorId": "2118240359",
                    "name": "Jinhao Jiang"
                },
                {
                    "authorId": "2358106383",
                    "name": "Zhiqiang Zhang"
                },
                {
                    "authorId": "2279870653",
                    "name": "Jun Zhou"
                },
                {
                    "authorId": "2294811281",
                    "name": "Wayne Xin Zhao"
                }
            ],
            "abstract": "The advancement of reasoning capabilities in Large Language Models (LLMs) requires substantial amounts of high-quality reasoning data, particularly in mathematics. Existing data synthesis methods, such as data augmentation from annotated training sets or direct question generation based on relevant knowledge points and documents, have expanded datasets but face challenges in mastering the inner logic of the problem during generation and ensuring the verifiability of the solutions. To address these issues, we propose RV-Syn, a novel Rational and Verifiable mathematical Synthesis approach. RV-Syn constructs a structured mathematical operation function library based on initial seed problems and generates computational graphs as solutions by combining Python-formatted functions from this library. These graphs are then back-translated into complex problems. Based on the constructed computation graph, we achieve solution-guided logic-aware problem generation. Furthermore, the executability of the computational graph ensures the verifiability of the solving process. Experimental results show that RV-Syn surpasses existing synthesis methods, including those involving human-generated problems, achieving greater efficient data scaling. This approach provides a scalable framework for generating high-quality reasoning datasets.",
            "corpus_id": 278171321,
            "sentences": [
                {
                    "corpus_id": "278171321",
                    "title": "RV-Syn: Rational and Verifiable Mathematical Reasoning Data Synthesis based on Structured Function Library",
                    "text": "The development of advanced reasoning Large Language Models (LLMs) (Zhao et al., 2023;OpenAI, 2024) has markedly improved their ability to address complex tasks across domains such as mathematics, science, and coding. This highlights the importance of synthesizing complex reasoning data to drive further advancements, given the limited availability of high-quality annotated instructions (Shah et al., 2024;Yuan et al., 2023). \n\nTo address this scarcity, researchers have explored various synthesizing methods, particularly in the mathematics domain. The mainstream methods involve data augmentation based on existing annotated training sets, such as GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021), ranging from self-evolving instructions (Xu et al., 2024;Zeng et al., 2024) and question paraphrasing (Yu et al., 2024), to solution augmentation (Lu et al., 2024). However, these methods are limited by the available training data, constraining the synthesis diversity (Li et al., 2024c(Li et al., , 2023)). To enhance diversity, recent approaches enable LLMs to generate a large scale of questions from various mathematicsrelated sources, including web pages (Yue et al., 2024) and knowledge points (Tang et al., 2024) from web corpora or textbooks. \n\nHowever, as shown in Figure 1, these direct problem generation methods encounter challenges in lacking a deep understanding and simulation of the underlying reasoning processes. As a result, the synthesized problems may appear superficially plausible but actually suffer from insufficient reasoning depth and logical inconsistencies (such as internal contradictions), especially when generating complex multi-step reasoning problems. Furthermore, a long-standing challenge in synthetic data generation is the difficulty in supporting the validation of solution correctness during the synthesis process, ultimately compromising training efficiency. \n\nTo address the aforementioned challenges, we draw inspiration from how human educators create problems. Just as one cannot directly write down an Olympic-level math problem without deep consideration and careful curation of the underlying problem-solving process, the same holds true for models.",
                    "score": 0.4142472820137965,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 217
                        },
                        {
                            "start": 218,
                            "end": 427
                        },
                        {
                            "start": 430,
                            "end": 551
                        },
                        {
                            "start": 552,
                            "end": 878
                        },
                        {
                            "start": 879,
                            "end": 1021
                        },
                        {
                            "start": 1022,
                            "end": 1264
                        },
                        {
                            "start": 1267,
                            "end": 1444
                        },
                        {
                            "start": 1445,
                            "end": 1700
                        },
                        {
                            "start": 1701,
                            "end": 1914
                        },
                        {
                            "start": 1917,
                            "end": 2020
                        },
                        {
                            "start": 2021,
                            "end": 2212
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 688,
                            "end": 712,
                            "matchedPaperCorpusId": "268247488"
                        },
                        {
                            "start": 771,
                            "end": 789,
                            "matchedPaperCorpusId": "270213007"
                        },
                        {
                            "start": 1214,
                            "end": 1233,
                            "matchedPaperCorpusId": "268247902"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93798828125
                }
            ],
            "relevance_judgement": 0.93798828125,
            "relevance_judgment_input_expanded": "# Title: RV-Syn: Rational and Verifiable Mathematical Reasoning Data Synthesis based on Structured Function Library\n# Venue: arXiv.org\n# Authors: Jiapeng Wang, Jinhao Jiang, Zhiqiang Zhang, Jun Zhou, Wayne Xin Zhao\n## Abstract\nThe advancement of reasoning capabilities in Large Language Models (LLMs) requires substantial amounts of high-quality reasoning data, particularly in mathematics. Existing data synthesis methods, such as data augmentation from annotated training sets or direct question generation based on relevant knowledge points and documents, have expanded datasets but face challenges in mastering the inner logic of the problem during generation and ensuring the verifiability of the solutions. To address these issues, we propose RV-Syn, a novel Rational and Verifiable mathematical Synthesis approach. RV-Syn constructs a structured mathematical operation function library based on initial seed problems and generates computational graphs as solutions by combining Python-formatted functions from this library. These graphs are then back-translated into complex problems. Based on the constructed computation graph, we achieve solution-guided logic-aware problem generation. Furthermore, the executability of the computational graph ensures the verifiability of the solving process. Experimental results show that RV-Syn surpasses existing synthesis methods, including those involving human-generated problems, achieving greater efficient data scaling. This approach provides a scalable framework for generating high-quality reasoning datasets.\n## Introduction\nThe development of advanced reasoning Large Language Models (LLMs) (Zhao et al., 2023;OpenAI, 2024) has markedly improved their ability to address complex tasks across domains such as mathematics, science, and coding. This highlights the importance of synthesizing complex reasoning data to drive further advancements, given the limited availability of high-quality annotated instructions (Shah et al., 2024;Yuan et al., 2023). \n\nTo address this scarcity, researchers have explored various synthesizing methods, particularly in the mathematics domain. The mainstream methods involve data augmentation based on existing annotated training sets, such as GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021), ranging from self-evolving instructions (Xu et al., 2024;Zeng et al., 2024) and question paraphrasing (Yu et al., 2024), to solution augmentation (Lu et al., 2024). However, these methods are limited by the available training data, constraining the synthesis diversity (Li et al., 2024c(Li et al., , 2023)). To enhance diversity, recent approaches enable LLMs to generate a large scale of questions from various mathematicsrelated sources, including web pages (Yue et al., 2024) and knowledge points (Tang et al., 2024) from web corpora or textbooks. \n\nHowever, as shown in Figure 1, these direct problem generation methods encounter challenges in lacking a deep understanding and simulation of the underlying reasoning processes. As a result, the synthesized problems may appear superficially plausible but actually suffer from insufficient reasoning depth and logical inconsistencies (such as internal contradictions), especially when generating complex multi-step reasoning problems. Furthermore, a long-standing challenge in synthetic data generation is the difficulty in supporting the validation of solution correctness during the synthesis process, ultimately compromising training efficiency. \n\nTo address the aforementioned challenges, we draw inspiration from how human educators create problems. Just as one cannot directly write down an Olympic-level math problem without deep consideration and careful curation of the underlying problem-solving process, the same holds true for models.",
            "reference_string": "[278171321 | Wang et al. | 2025 | Citations: 0]"
        },
        {
            "title": "SynC2S: An Efficient Method for Synthesizing Tabular Data With a Learnable Pre-Processing",
            "venue": "IEEE Access",
            "year": 2025,
            "reference_count": 53,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1109/access.2024.3472706",
                "status": "GOLD",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2024.3472706?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2024.3472706, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2214924470",
                    "name": "Jiwoo Kim"
                },
                {
                    "authorId": "2324215497",
                    "name": "Seri Park"
                },
                {
                    "authorId": "2324137155",
                    "name": "Junsung Koh"
                },
                {
                    "authorId": "2215102498",
                    "name": "Dongha Kim"
                }
            ],
            "abstract": "There has been a growing demand to access large public datasets to extract valuable insights or enhance their services. However, this also involves risks, such as privacy breaches and unauthorized data exposure. Data synthesis has emerged as a popular technique to address privacy preservation and data usability simultaneously. Recently, numerous methods based on deep learning have been developed, while a clear understanding of their effectiveness is still insufficient, and the necessity for more efficient frameworks persists. In this study, we propose an efficient and theoretically principled method based on a deep generative model to effectively generate high-quality synthetic tabular data. First, we introduce a novel technique called C2Smap\u2013a learnable pre-processing method that automatically transforms continuous distributions into simpler and easily generatable forms. We then develop a conditional generative model with a hierarchical structure and its corresponding learning framework, called HCIWAE, to successfully capture imbalanced categorical distributions. Combining these two components, we coin our method Synthetic data generation with C2Smap (SynC2S). Through comprehensive experimental analyses, we demonstrate the superiority and efficiency of SynC2S in generating synthetic data compared to other recent competitors. Furthermore, as a by-product, we claim that SynC2S could be a favorable option to solve over-sampling tasks, constructing high-performance prediction models by generating synthetic data for the minority class.",
            "corpus_id": 273115023,
            "sentences": [
                {
                    "corpus_id": "273115023",
                    "title": "SynC2S: An Efficient Method for Synthesizing Tabular Data With a Learnable Pre-Processing",
                    "text": "There has been a growing demand to access large public datasets to extract valuable insights or enhance their services. However, this also involves risks, such as privacy breaches and unauthorized data exposure. Data synthesis has emerged as a popular technique to address privacy preservation and data usability simultaneously. Recently, numerous methods based on deep learning have been developed, while a clear understanding of their effectiveness is still insufficient, and the necessity for more efficient frameworks persists. In this study, we propose an efficient and theoretically principled method based on a deep generative model to effectively generate high-quality synthetic tabular data. First, we introduce a novel technique called C2Smap\u2013a learnable pre-processing method that automatically transforms continuous distributions into simpler and easily generatable forms. We then develop a conditional generative model with a hierarchical structure and its corresponding learning framework, called HCIWAE, to successfully capture imbalanced categorical distributions. Combining these two components, we coin our method Synthetic data generation with C2Smap (SynC2S). Through comprehensive experimental analyses, we demonstrate the superiority and efficiency of SynC2S in generating synthetic data compared to other recent competitors. Furthermore, as a by-product, we claim that SynC2S could be a favorable option to solve over-sampling tasks, constructing high-performance prediction models by generating synthetic data for the minority class.",
                    "score": 0.3493079417352395,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93603515625
                }
            ],
            "relevance_judgement": 0.93603515625,
            "relevance_judgment_input_expanded": "# Title: SynC2S: An Efficient Method for Synthesizing Tabular Data With a Learnable Pre-Processing\n# Venue: IEEE Access\n# Authors: Jiwoo Kim, Seri Park, Junsung Koh, Dongha Kim\n## Abstract\nThere has been a growing demand to access large public datasets to extract valuable insights or enhance their services. However, this also involves risks, such as privacy breaches and unauthorized data exposure. Data synthesis has emerged as a popular technique to address privacy preservation and data usability simultaneously. Recently, numerous methods based on deep learning have been developed, while a clear understanding of their effectiveness is still insufficient, and the necessity for more efficient frameworks persists. In this study, we propose an efficient and theoretically principled method based on a deep generative model to effectively generate high-quality synthetic tabular data. First, we introduce a novel technique called C2Smap\u2013a learnable pre-processing method that automatically transforms continuous distributions into simpler and easily generatable forms. We then develop a conditional generative model with a hierarchical structure and its corresponding learning framework, called HCIWAE, to successfully capture imbalanced categorical distributions. Combining these two components, we coin our method Synthetic data generation with C2Smap (SynC2S). Through comprehensive experimental analyses, we demonstrate the superiority and efficiency of SynC2S in generating synthetic data compared to other recent competitors. Furthermore, as a by-product, we claim that SynC2S could be a favorable option to solve over-sampling tasks, constructing high-performance prediction models by generating synthetic data for the minority class.\n",
            "reference_string": "[273115023 | Kim et al. | 2025 | Citations: 0]"
        },
        {
            "title": "CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 68,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.11963, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2297867187",
                    "name": "Qingqing Cao"
                },
                {
                    "authorId": "40465379",
                    "name": "Mahyar Najibi"
                },
                {
                    "authorId": "2326117224",
                    "name": "Sachin Mehta"
                }
            ],
            "abstract": "Pretraining robust vision or multimodal foundation models (e.g., CLIP) relies on large-scale datasets that may be noisy, potentially misaligned, and have long-tail distributions. Previous works have shown promising results in augmenting datasets by generating synthetic samples. However, they only support domain-specific ad hoc use cases (e.g., either image or text only, but not both), and are limited in data diversity due to a lack of fine-grained control over the synthesis process. In this paper, we design a \\emph{controllable} image-text synthesis pipeline, CtrlSynth, for data-efficient and robust multimodal learning. The key idea is to decompose the visual semantics of an image into basic elements, apply user-specified control policies (e.g., remove, add, or replace operations), and recompose them to synthesize images or texts. The decompose and recompose feature in CtrlSynth allows users to control data synthesis in a fine-grained manner by defining customized control policies to manipulate the basic elements. CtrlSynth leverages the capabilities of pretrained foundation models such as large language models or diffusion models to reason and recompose basic elements such that synthetic samples are natural and composed in diverse ways. CtrlSynth is a closed-loop, training-free, and modular framework, making it easy to support different pretrained models. With extensive experiments on 31 datasets spanning different vision and vision-language tasks, we show that CtrlSynth substantially improves zero-shot classification, image-text retrieval, and compositional reasoning performance of CLIP models.",
            "corpus_id": 273374871,
            "sentences": [
                {
                    "corpus_id": "273374871",
                    "title": "CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning",
                    "text": "High-quality large-scale datasets have driven the success of large foundational AI models (Radford et al., 2021;Rombach et al., 2022;Touvron et al., 2023). Collecting and annotating datasets at large-scale is challenging and costly. One solution is to crawl data from the web; however, web data is noisy (Lai et al., 2024;Kang et al., 2023), has long-tail distributions (Udandarao et al., 2024), and often causes privacy or copyright issues (Schuhmann et al., 2022). Synthetic data presents a viable and complementary alternative to overcome these challenges, as it allows for precise control over data generation and customization to meet specific requirements. A large body of work has focused on improving the quality of synthetic data for image and text data, from the generation of high-quality images (Dunlap et al., 2023;Islam et al., 2024) to the improvement of synthetic captions (Lai et al., 2024;Fan et al., 2023). While these works have shown that synthetic data successfully improves model performance for various vision or vision-language tasks, their synthetic pipeline is often ad hoc and tailored to specific purposes such as training better CLIP models or improving domain-specific vision models (e.g., DiffuseMix uses diffusion models to augment images and improves accuracy on image classification tasks Islam et al., 2024). These data synthesis works also lack explicit fine-grained control over the generated texts or images, which are important for tasks with long-tail distribution (e.g., augmenting tail class samples) or enforcing safety requirements (e.g., mitigating biased or sensitive content generation Schramowski et al., 2023). \n\nIn this work, we aim to systematically control the synthetic pipeline for generating image-text data while accommodating different use cases (e.g., improving long-tail task performance, enhancing compositional reasoning of CLIP models, etc.).",
                    "score": 0.35978191581424696,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 155
                        },
                        {
                            "start": 156,
                            "end": 232
                        },
                        {
                            "start": 233,
                            "end": 466
                        },
                        {
                            "start": 467,
                            "end": 662
                        },
                        {
                            "start": 663,
                            "end": 925
                        },
                        {
                            "start": 926,
                            "end": 1344
                        },
                        {
                            "start": 1345,
                            "end": 1660
                        },
                        {
                            "start": 1663,
                            "end": 1905
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 90,
                            "end": 112,
                            "matchedPaperCorpusId": "231591445"
                        },
                        {
                            "start": 112,
                            "end": 133,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 322,
                            "end": 340,
                            "matchedPaperCorpusId": "255186162"
                        },
                        {
                            "start": 441,
                            "end": 465,
                            "matchedPaperCorpusId": "252917726"
                        },
                        {
                            "start": 807,
                            "end": 828,
                            "matchedPaperCorpusId": "258887976"
                        },
                        {
                            "start": 828,
                            "end": 847,
                            "matchedPaperCorpusId": "270045834"
                        },
                        {
                            "start": 1324,
                            "end": 1343,
                            "matchedPaperCorpusId": "270045834"
                        },
                        {
                            "start": 1634,
                            "end": 1659,
                            "matchedPaperCorpusId": "253420366"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93359375
                }
            ],
            "relevance_judgement": 0.93359375,
            "relevance_judgment_input_expanded": "# Title: CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning\n# Venue: arXiv.org\n# Authors: Qingqing Cao, Mahyar Najibi, Sachin Mehta\n## Abstract\nPretraining robust vision or multimodal foundation models (e.g., CLIP) relies on large-scale datasets that may be noisy, potentially misaligned, and have long-tail distributions. Previous works have shown promising results in augmenting datasets by generating synthetic samples. However, they only support domain-specific ad hoc use cases (e.g., either image or text only, but not both), and are limited in data diversity due to a lack of fine-grained control over the synthesis process. In this paper, we design a \\emph{controllable} image-text synthesis pipeline, CtrlSynth, for data-efficient and robust multimodal learning. The key idea is to decompose the visual semantics of an image into basic elements, apply user-specified control policies (e.g., remove, add, or replace operations), and recompose them to synthesize images or texts. The decompose and recompose feature in CtrlSynth allows users to control data synthesis in a fine-grained manner by defining customized control policies to manipulate the basic elements. CtrlSynth leverages the capabilities of pretrained foundation models such as large language models or diffusion models to reason and recompose basic elements such that synthetic samples are natural and composed in diverse ways. CtrlSynth is a closed-loop, training-free, and modular framework, making it easy to support different pretrained models. With extensive experiments on 31 datasets spanning different vision and vision-language tasks, we show that CtrlSynth substantially improves zero-shot classification, image-text retrieval, and compositional reasoning performance of CLIP models.\n## INTRODUCTION\nHigh-quality large-scale datasets have driven the success of large foundational AI models (Radford et al., 2021;Rombach et al., 2022;Touvron et al., 2023). Collecting and annotating datasets at large-scale is challenging and costly. One solution is to crawl data from the web; however, web data is noisy (Lai et al., 2024;Kang et al., 2023), has long-tail distributions (Udandarao et al., 2024), and often causes privacy or copyright issues (Schuhmann et al., 2022). Synthetic data presents a viable and complementary alternative to overcome these challenges, as it allows for precise control over data generation and customization to meet specific requirements. A large body of work has focused on improving the quality of synthetic data for image and text data, from the generation of high-quality images (Dunlap et al., 2023;Islam et al., 2024) to the improvement of synthetic captions (Lai et al., 2024;Fan et al., 2023). While these works have shown that synthetic data successfully improves model performance for various vision or vision-language tasks, their synthetic pipeline is often ad hoc and tailored to specific purposes such as training better CLIP models or improving domain-specific vision models (e.g., DiffuseMix uses diffusion models to augment images and improves accuracy on image classification tasks Islam et al., 2024). These data synthesis works also lack explicit fine-grained control over the generated texts or images, which are important for tasks with long-tail distribution (e.g., augmenting tail class samples) or enforcing safety requirements (e.g., mitigating biased or sensitive content generation Schramowski et al., 2023). \n\nIn this work, we aim to systematically control the synthetic pipeline for generating image-text data while accommodating different use cases (e.g., improving long-tail task performance, enhancing compositional reasoning of CLIP models, etc.).",
            "reference_string": "[273374871 | Cao et al. | 2024 | Citations: 1]"
        },
        {
            "title": "MIND: Math Informed syNthetic Dialogues for Pretraining LLMs",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "reference_count": 65,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.12881, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1900302322",
                    "name": "Syeda Nahida Akter"
                },
                {
                    "authorId": "9358910",
                    "name": "Shrimai Prabhumoye"
                },
                {
                    "authorId": "51028721",
                    "name": "John Kamalu"
                },
                {
                    "authorId": "145031342",
                    "name": "S. Satheesh"
                },
                {
                    "authorId": "2279547669",
                    "name": "Eric Nyberg"
                },
                {
                    "authorId": "66870756",
                    "name": "M. Patwary"
                },
                {
                    "authorId": "1911755",
                    "name": "M. Shoeybi"
                },
                {
                    "authorId": "2264406909",
                    "name": "Bryan Catanzaro"
                }
            ],
            "abstract": "The utility of synthetic data to enhance pretraining data quality and hence to improve downstream task accuracy has been widely explored in recent large language models (LLMs). Yet, these approaches fall inadequate in complex, multi-hop and mathematical reasoning tasks as the synthetic data typically fails to add complementary knowledge to the existing raw corpus. In this work, we propose a novel large-scale and diverse Math Informed syNthetic Dialogue (MIND) generation method that improves the mathematical reasoning ability of LLMs. Specifically, using MIND, we generate synthetic conversations based on OpenWebMath (OWM), resulting in a new math corpus, MIND-OWM. Our experiments with different conversational settings reveal that incorporating knowledge gaps between dialog participants is essential for generating high-quality math data. We further identify an effective way to format and integrate synthetic and raw data during pretraining to maximize the gain in mathematical reasoning, emphasizing the need to restructure raw data rather than use it as-is. Compared to pretraining just on raw data, a model pretrained on MIND-OWM shows significant boost in mathematical reasoning (GSM8K: +13.42%, MATH: +2.30%), including superior performance in specialized knowledge (MMLU: +4.55%, MMLU-STEM: +4.28%) and general purpose reasoning tasks (GENERAL REASONING: +2.51%).",
            "corpus_id": 273403575,
            "sentences": [
                {
                    "corpus_id": "273403575",
                    "title": "MIND: Math Informed syNthetic Dialogues for Pretraining LLMs",
                    "text": "Additionally, our experiment with entire OpenWebMath (OWM-14B) and its corresponding synthetic conversa-tions shows a consistent trend, indicating that the benefits of conversational data continue to hold as the data scales. In fact, with all conversations generated from OWM-4B, we can outperform model trained with OWM-14B, a 3.6\u00d7 larger data-2.94% average improvement across GSM8K and MATH tasks, 1.56% across all benchmarks (Figure 1). This underlines the value of synthetic conversations, particularly when high-quality in-domain data is limited. Moreover, our analysis with other datasets reveals that conversational data further amplifies reasoning capabilities in models even when the raw data originates from high-quality sources. We hope that MIND will pave a way to improve complex reasoning ability of smaller models with limited training data and accelerate further innovation towards building strong reasoning ability with structured high-quality data. To generate high-quality data at scale, current synthetic data generation approach explores rephrasing texts using LLMs in varied syntax while preserving the core content (Maini et al., 2024). However, their proposed approach limits up-sampling high-quality data in a way that does not go beyond grammatical styles or surface form transformations-leading little to no improvement when it comes to performance across complex and logical reasoning tasks. We hypothesize that simple rephrasing does not leverage the full potential of the synthetic data to improve the mathematical and complex multi-hop reasoning ability of LLM. Therefore, we propose, MIND, a conversational synthetic data generation approach that adds semantic variations and structured complexity to the raw text which is required to improve complex reasoning ability of the LLMs. In addition, multi-turn conversations can break down the original context step-by-step while each step addresses a sub-context at a time by often injecting complimentary reasoning or explanations. This resonates with how human solves a complex problem using consecutive chain-of-thought reasoning.",
                    "score": 0.37877369099434166,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 5946,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 224
                        },
                        {
                            "start": 225,
                            "end": 350
                        },
                        {
                            "start": 351,
                            "end": 439
                        },
                        {
                            "start": 440,
                            "end": 551
                        },
                        {
                            "start": 552,
                            "end": 739
                        },
                        {
                            "start": 740,
                            "end": 966
                        },
                        {
                            "start": 967,
                            "end": 1159
                        },
                        {
                            "start": 1160,
                            "end": 1419
                        },
                        {
                            "start": 1420,
                            "end": 1592
                        },
                        {
                            "start": 1593,
                            "end": 1813
                        },
                        {
                            "start": 1814,
                            "end": 2010
                        },
                        {
                            "start": 2011,
                            "end": 2111
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1138,
                            "end": 1158,
                            "matchedPaperCorpusId": "267312030"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9326171875
                }
            ],
            "relevance_judgement": 0.9326171875,
            "relevance_judgment_input_expanded": "# Title: MIND: Math Informed syNthetic Dialogues for Pretraining LLMs\n# Venue: International Conference on Learning Representations\n# Authors: Syeda Nahida Akter, Shrimai Prabhumoye, John Kamalu, S. Satheesh, Eric Nyberg, M. Patwary, M. Shoeybi, Bryan Catanzaro\n## Abstract\nThe utility of synthetic data to enhance pretraining data quality and hence to improve downstream task accuracy has been widely explored in recent large language models (LLMs). Yet, these approaches fall inadequate in complex, multi-hop and mathematical reasoning tasks as the synthetic data typically fails to add complementary knowledge to the existing raw corpus. In this work, we propose a novel large-scale and diverse Math Informed syNthetic Dialogue (MIND) generation method that improves the mathematical reasoning ability of LLMs. Specifically, using MIND, we generate synthetic conversations based on OpenWebMath (OWM), resulting in a new math corpus, MIND-OWM. Our experiments with different conversational settings reveal that incorporating knowledge gaps between dialog participants is essential for generating high-quality math data. We further identify an effective way to format and integrate synthetic and raw data during pretraining to maximize the gain in mathematical reasoning, emphasizing the need to restructure raw data rather than use it as-is. Compared to pretraining just on raw data, a model pretrained on MIND-OWM shows significant boost in mathematical reasoning (GSM8K: +13.42%, MATH: +2.30%), including superior performance in specialized knowledge (MMLU: +4.55%, MMLU-STEM: +4.28%) and general purpose reasoning tasks (GENERAL REASONING: +2.51%).\n## INTRODUCTION\nAdditionally, our experiment with entire OpenWebMath (OWM-14B) and its corresponding synthetic conversa-tions shows a consistent trend, indicating that the benefits of conversational data continue to hold as the data scales. In fact, with all conversations generated from OWM-4B, we can outperform model trained with OWM-14B, a 3.6\u00d7 larger data-2.94% average improvement across GSM8K and MATH tasks, 1.56% across all benchmarks (Figure 1). This underlines the value of synthetic conversations, particularly when high-quality in-domain data is limited. Moreover, our analysis with other datasets reveals that conversational data further amplifies reasoning capabilities in models even when the raw data originates from high-quality sources. We hope that MIND will pave a way to improve complex reasoning ability of smaller models with limited training data and accelerate further innovation towards building strong reasoning ability with structured high-quality data. To generate high-quality data at scale, current synthetic data generation approach explores rephrasing texts using LLMs in varied syntax while preserving the core content (Maini et al., 2024). However, their proposed approach limits up-sampling high-quality data in a way that does not go beyond grammatical styles or surface form transformations-leading little to no improvement when it comes to performance across complex and logical reasoning tasks. We hypothesize that simple rephrasing does not leverage the full potential of the synthetic data to improve the mathematical and complex multi-hop reasoning ability of LLM. Therefore, we propose, MIND, a conversational synthetic data generation approach that adds semantic variations and structured complexity to the raw text which is required to improve complex reasoning ability of the LLMs. In addition, multi-turn conversations can break down the original context step-by-step while each step addresses a sub-context at a time by often injecting complimentary reasoning or explanations. This resonates with how human solves a complex problem using consecutive chain-of-thought reasoning.",
            "reference_string": "[273403575 | Akter et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Did You Hear That? Introducing AADG: A Framework for Generating Benchmark Data in Audio Anomaly Detection",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 56,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.03904, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2093263031",
                    "name": "Ksheeraja Raghavan"
                },
                {
                    "authorId": "2199184565",
                    "name": "Samiran Gode"
                },
                {
                    "authorId": "2257395734",
                    "name": "Ankit Shah"
                },
                {
                    "authorId": "2324783136",
                    "name": "Surabhi Raghavan"
                },
                {
                    "authorId": "2322439428",
                    "name": "Wolfram Burgard"
                },
                {
                    "authorId": "2288787089",
                    "name": "Bhiksha Raj"
                },
                {
                    "authorId": "2289375291",
                    "name": "Rita Singh"
                }
            ],
            "abstract": "We introduce a novel, general-purpose audio generation framework specifically designed for anomaly detection and localization. Unlike existing datasets that predominantly focus on industrial and machine-related sounds, our framework focuses a broader range of environments, particularly useful in real-world scenarios where only audio data are available, such as in video-derived or telephonic audio. To generate such data, we propose a new method inspired by the LLM-Modulo framework, which leverages large language models(LLMs) as world models to simulate such real-world scenarios. This tool is modular allowing a plug-and-play approach. It operates by first using LLMs to predict plausible real-world scenarios. An LLM further extracts the constituent sounds, the order and the way in which these should be merged to create coherent wholes. Much like the LLM-Modulo framework, we include rigorous verification of each output stage, ensuring the reliability of the generated data. The data produced using the framework serves as a benchmark for anomaly detection applications, potentially enhancing the performance of models trained on audio data, particularly in handling out-of-distribution cases. Our contributions thus fill a critical void in audio anomaly detection resources and provide a scalable tool for generating diverse, realistic audio data.",
            "corpus_id": 273185896,
            "sentences": [
                {
                    "corpus_id": "273185896",
                    "title": "Did You Hear That? Introducing AADG: A Framework for Generating Benchmark Data in Audio Anomaly Detection",
                    "text": "Recently, (Evans et al. 2024) introduced a diffusion transformer. \n\nLLMs for synthetic data generation -Recent works have shown significant progress in synthetic data generation in the image space in (Liang et al. 2024) (Ramesh et al. 2021) (Sun et al. 2024) (Bae et al. 2023) etc. However, similar progress has not been seen in the audio space. Zeroshot text-to-image generation approaches have expanded the scope of synthetic data applications by enabling the generation of novel image data from unseen textual prompts, highlighting the model's ability to generalize from limited examples (Ramesh et al. 2021). Digiface-1m (Bae et al. 2023) dataset exemplifies the practical applications of these technologies, providing a robust framework for testing and improving face recognition algorithms through access to one million digital face images. (Ye et al. 2022) outlines a method to leverage LLMs to create synthetic datasets produced entirely using pre-trained language models (PLMs) without human interference while emphasizing the efficiency and flexibility of using synthetic datasets to train task-specific models. (Yu et al. 2024) explores generation of training data that not only focuses on diversity, but also addresses inherent biases within the data generated by LLMs. It highlights the critical role of using diversely attributed prompts that enhance quality and utility of synthetic datasets improving model performance across NLP tasks. (Patel, Raffel, and Callison-Burch 2024) presents a tool designed to streamline synthetic data generation using LLMs providing a platform to generate, train, and share data sets and models. \n\nTraining on synthetic data can improve the performance of the model (Nvidia et al. 2024). In (Dubey et al. 2024) it has been used to generate training data for text quality classifiers. \n\nLLMs for planning Chain-of-thought (CoT) prompting has emerged as a powerful technique to enhance the reasoning capabilities of LLMs by generating intermediate reasoning steps, thereby improving performance on complex tasks such as arithmetic and common sense reasoning.",
                    "score": 0.409616034664173,
                    "section_title": "Related Work",
                    "char_start_offset": 9094,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 65
                        },
                        {
                            "start": 68,
                            "end": 281
                        },
                        {
                            "start": 282,
                            "end": 345
                        },
                        {
                            "start": 346,
                            "end": 612
                        },
                        {
                            "start": 613,
                            "end": 846
                        },
                        {
                            "start": 847,
                            "end": 1121
                        },
                        {
                            "start": 1122,
                            "end": 1281
                        },
                        {
                            "start": 1282,
                            "end": 1452
                        },
                        {
                            "start": 1453,
                            "end": 1642
                        },
                        {
                            "start": 1645,
                            "end": 1734
                        },
                        {
                            "start": 1735,
                            "end": 1830
                        },
                        {
                            "start": 1833,
                            "end": 2103
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 200,
                            "end": 219,
                            "matchedPaperCorpusId": "266348349"
                        },
                        {
                            "start": 220,
                            "end": 239,
                            "matchedPaperCorpusId": "232035663"
                        },
                        {
                            "start": 259,
                            "end": 276,
                            "matchedPaperCorpusId": "252735071"
                        },
                        {
                            "start": 591,
                            "end": 611,
                            "matchedPaperCorpusId": "232035663"
                        },
                        {
                            "start": 625,
                            "end": 641,
                            "matchedPaperCorpusId": "252735071"
                        },
                        {
                            "start": 1122,
                            "end": 1137,
                            "matchedPaperCorpusId": "259275123"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.931640625
                }
            ],
            "relevance_judgement": 0.931640625,
            "relevance_judgment_input_expanded": "# Title: Did You Hear That? Introducing AADG: A Framework for Generating Benchmark Data in Audio Anomaly Detection\n# Venue: arXiv.org\n# Authors: Ksheeraja Raghavan, Samiran Gode, Ankit Shah, Surabhi Raghavan, Wolfram Burgard, Bhiksha Raj, Rita Singh\n## Abstract\nWe introduce a novel, general-purpose audio generation framework specifically designed for anomaly detection and localization. Unlike existing datasets that predominantly focus on industrial and machine-related sounds, our framework focuses a broader range of environments, particularly useful in real-world scenarios where only audio data are available, such as in video-derived or telephonic audio. To generate such data, we propose a new method inspired by the LLM-Modulo framework, which leverages large language models(LLMs) as world models to simulate such real-world scenarios. This tool is modular allowing a plug-and-play approach. It operates by first using LLMs to predict plausible real-world scenarios. An LLM further extracts the constituent sounds, the order and the way in which these should be merged to create coherent wholes. Much like the LLM-Modulo framework, we include rigorous verification of each output stage, ensuring the reliability of the generated data. The data produced using the framework serves as a benchmark for anomaly detection applications, potentially enhancing the performance of models trained on audio data, particularly in handling out-of-distribution cases. Our contributions thus fill a critical void in audio anomaly detection resources and provide a scalable tool for generating diverse, realistic audio data.\n## Related Work\nRecently, (Evans et al. 2024) introduced a diffusion transformer. \n\nLLMs for synthetic data generation -Recent works have shown significant progress in synthetic data generation in the image space in (Liang et al. 2024) (Ramesh et al. 2021) (Sun et al. 2024) (Bae et al. 2023) etc. However, similar progress has not been seen in the audio space. Zeroshot text-to-image generation approaches have expanded the scope of synthetic data applications by enabling the generation of novel image data from unseen textual prompts, highlighting the model's ability to generalize from limited examples (Ramesh et al. 2021). Digiface-1m (Bae et al. 2023) dataset exemplifies the practical applications of these technologies, providing a robust framework for testing and improving face recognition algorithms through access to one million digital face images. (Ye et al. 2022) outlines a method to leverage LLMs to create synthetic datasets produced entirely using pre-trained language models (PLMs) without human interference while emphasizing the efficiency and flexibility of using synthetic datasets to train task-specific models. (Yu et al. 2024) explores generation of training data that not only focuses on diversity, but also addresses inherent biases within the data generated by LLMs. It highlights the critical role of using diversely attributed prompts that enhance quality and utility of synthetic datasets improving model performance across NLP tasks. (Patel, Raffel, and Callison-Burch 2024) presents a tool designed to streamline synthetic data generation using LLMs providing a platform to generate, train, and share data sets and models. \n\nTraining on synthetic data can improve the performance of the model (Nvidia et al. 2024). In (Dubey et al. 2024) it has been used to generate training data for text quality classifiers. \n\nLLMs for planning Chain-of-thought (CoT) prompting has emerged as a powerful technique to enhance the reasoning capabilities of LLMs by generating intermediate reasoning steps, thereby improving performance on complex tasks such as arithmetic and common sense reasoning.",
            "reference_string": "[273185896 | Raghavan et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Spend Wisely: Maximizing Post-Training Gains in Iterative Synthetic Data Boostrapping",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 38,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.18962, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2284644270",
                    "name": "Pu Yang"
                },
                {
                    "authorId": "2343647731",
                    "name": "Yunzhen Feng"
                },
                {
                    "authorId": "2343512547",
                    "name": "Ziyuan Chen"
                },
                {
                    "authorId": "2343592601",
                    "name": "Yuhang Wu"
                },
                {
                    "authorId": "2344832360",
                    "name": "Zhuoyuan Li"
                }
            ],
            "abstract": "Modern foundation models often undergo iterative ``bootstrapping'' in their post-training phase: a model generates synthetic data, an external verifier filters out low-quality samples, and the high-quality subset is used for further fine-tuning. Over multiple iterations, the model's performance improves--raising a crucial question: how should the total budget on generation and training be allocated across iterations to maximize final performance? In this work, we develop a theoretical framework to analyze budget allocation strategies. Specifically, we show that constant policies fail to converge with high probability, while increasing policies--particularly exponential growth policies--exhibit significant theoretical advantages. Experiments on image denoising with diffusion probabilistic models and math reasoning with large language models show that both exponential and polynomial growth policies consistently outperform constant policies, with exponential policies often providing more stable performance.",
            "corpus_id": 276079713,
            "sentences": [
                {
                    "corpus_id": "276079713",
                    "title": "Spend Wisely: Maximizing Post-Training Gains in Iterative Synthetic Data Boostrapping",
                    "text": "A growing focus in the community is on post-training regimes, where models first acquire general knowledge during pretraining but require further refinement to excel in complex problem-solving tasks, such as agentic behavior (Shao et al., 2023), math reasoning (Poesia et al., 2024), and advanced code generation (Guo et al., 2024a). In these settings, the availability of high-quality human-annotated data often becomes a significant bottleneck due to the demand for expert input and substantial resources. \n\nTo address this limitation, synthetic data have emerged as a promising alternative, offering scalability and costeffectiveness. Despite concerns about potential risks of model collapse (Shumailov et al., 2024;Dohmatob et al., Figure 1. Iterative learning with synthetic data. In this framework, synthetic data is generated, filtered using a reward model, and the selected data is used to further train the generator. The budget policy is defined as the quantity of data retained after selection, nt. Our goal is to identify the optimal policy across iterations to achieve the best final performance, given a fixed budget. 2024c), synthetic data is typically selected and verified before use in post-training (Feng et al., 2024;Setlur et al., 2024), ensuring its quality. A common paradigm for leveraging synthetic data employs an iterative bootstrapping process (Zelikman et al., 2022;Trung et al., 2024): the model generates synthetic data, rewards or verifiers are used to filter and select high-quality data, and the model is then fine-tuned on selected data. This process is repeated iteratively to fully improve performance. An illustration of this approach is provided in Figure 1. \n\nHowever, for practitioners implementing this approach, an important question arises: how should a fixed computational budget be allocated to decide the amount of synthetic data to generate and select at each iteration to maximize performance? \n\nIn this paper, we establish foundational principles for crafting optimal strategies for synthetic data generation across iterations. To the best of our knowledge, this is the first attempt to address this problem.",
                    "score": 0.40049478399282545,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 333
                        },
                        {
                            "start": 334,
                            "end": 507
                        },
                        {
                            "start": 510,
                            "end": 637
                        },
                        {
                            "start": 638,
                            "end": 745
                        },
                        {
                            "start": 746,
                            "end": 785
                        },
                        {
                            "start": 786,
                            "end": 926
                        },
                        {
                            "start": 927,
                            "end": 1009
                        },
                        {
                            "start": 1010,
                            "end": 1131
                        },
                        {
                            "start": 1132,
                            "end": 1280
                        },
                        {
                            "start": 1281,
                            "end": 1572
                        },
                        {
                            "start": 1573,
                            "end": 1639
                        },
                        {
                            "start": 1640,
                            "end": 1697
                        },
                        {
                            "start": 1700,
                            "end": 1942
                        },
                        {
                            "start": 1945,
                            "end": 2077
                        },
                        {
                            "start": 2078,
                            "end": 2158
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 225,
                            "end": 244,
                            "matchedPaperCorpusId": "264145862"
                        },
                        {
                            "start": 261,
                            "end": 282,
                            "matchedPaperCorpusId": "270870392"
                        },
                        {
                            "start": 695,
                            "end": 719,
                            "matchedPaperCorpusId": "271448069"
                        },
                        {
                            "start": 1218,
                            "end": 1237,
                            "matchedPaperCorpusId": "271213167"
                        },
                        {
                            "start": 1237,
                            "end": 1257,
                            "matchedPaperCorpusId": "270620658"
                        },
                        {
                            "start": 1372,
                            "end": 1395,
                            "matchedPaperCorpusId": "247762790"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93017578125
                }
            ],
            "relevance_judgement": 0.93017578125,
            "relevance_judgment_input_expanded": "# Title: Spend Wisely: Maximizing Post-Training Gains in Iterative Synthetic Data Boostrapping\n# Venue: arXiv.org\n# Authors: Pu Yang, Yunzhen Feng, Ziyuan Chen, Yuhang Wu, Zhuoyuan Li\n## Abstract\nModern foundation models often undergo iterative ``bootstrapping'' in their post-training phase: a model generates synthetic data, an external verifier filters out low-quality samples, and the high-quality subset is used for further fine-tuning. Over multiple iterations, the model's performance improves--raising a crucial question: how should the total budget on generation and training be allocated across iterations to maximize final performance? In this work, we develop a theoretical framework to analyze budget allocation strategies. Specifically, we show that constant policies fail to converge with high probability, while increasing policies--particularly exponential growth policies--exhibit significant theoretical advantages. Experiments on image denoising with diffusion probabilistic models and math reasoning with large language models show that both exponential and polynomial growth policies consistently outperform constant policies, with exponential policies often providing more stable performance.\n## Introduction\nA growing focus in the community is on post-training regimes, where models first acquire general knowledge during pretraining but require further refinement to excel in complex problem-solving tasks, such as agentic behavior (Shao et al., 2023), math reasoning (Poesia et al., 2024), and advanced code generation (Guo et al., 2024a). In these settings, the availability of high-quality human-annotated data often becomes a significant bottleneck due to the demand for expert input and substantial resources. \n\nTo address this limitation, synthetic data have emerged as a promising alternative, offering scalability and costeffectiveness. Despite concerns about potential risks of model collapse (Shumailov et al., 2024;Dohmatob et al., Figure 1. Iterative learning with synthetic data. In this framework, synthetic data is generated, filtered using a reward model, and the selected data is used to further train the generator. The budget policy is defined as the quantity of data retained after selection, nt. Our goal is to identify the optimal policy across iterations to achieve the best final performance, given a fixed budget. 2024c), synthetic data is typically selected and verified before use in post-training (Feng et al., 2024;Setlur et al., 2024), ensuring its quality. A common paradigm for leveraging synthetic data employs an iterative bootstrapping process (Zelikman et al., 2022;Trung et al., 2024): the model generates synthetic data, rewards or verifiers are used to filter and select high-quality data, and the model is then fine-tuned on selected data. This process is repeated iteratively to fully improve performance. An illustration of this approach is provided in Figure 1. \n\nHowever, for practitioners implementing this approach, an important question arises: how should a fixed computational budget be allocated to decide the amount of synthetic data to generate and select at each iteration to maximize performance? \n\nIn this paper, we establish foundational principles for crafting optimal strategies for synthetic data generation across iterations. To the best of our knowledge, this is the first attempt to address this problem.",
            "reference_string": "[276079713 | Yang et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Large Language Models(LLMs) on Tabular Data: Prediction, Generation, and Understanding - A Survey",
            "venue": "Trans. Mach. Learn. Res.",
            "year": 2024,
            "reference_count": 222,
            "citation_count": 81,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.17944, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2288381545",
                    "name": "Xi Fang"
                },
                {
                    "authorId": "2110546424",
                    "name": "Weijie Xu"
                },
                {
                    "authorId": "2287992265",
                    "name": "F. Tan"
                },
                {
                    "authorId": "2258747730",
                    "name": "Jiani Zhang"
                },
                {
                    "authorId": "2287978009",
                    "name": "Ziqing Hu"
                },
                {
                    "authorId": "2288038766",
                    "name": "Yanjun Qi"
                },
                {
                    "authorId": "2287927391",
                    "name": "Scott Nickleach"
                },
                {
                    "authorId": "2287926360",
                    "name": "Diego Socolinsky"
                },
                {
                    "authorId": "2287927370",
                    "name": "Srinivasan H. Sengamedu"
                },
                {
                    "authorId": "2263543517",
                    "name": "Christos Faloutsos"
                }
            ],
            "abstract": "Recent breakthroughs in large language modeling have facilitated rigorous exploration of their application in diverse tasks related to tabular data modeling, such as prediction, tabular data synthesis, question answering, and table understanding. Each task presents unique challenges and opportunities. However, there is currently a lack of comprehensive review that summarizes and compares the key techniques, metrics, datasets, models, and optimization approaches in this research domain. This survey aims to address this gap by consolidating recent progress in these areas, offering a thorough survey and taxonomy of the datasets, metrics, and methodologies utilized. It identifies strengths, limitations, unexplored territories, and gaps in the existing literature, while providing some insights for future research directions in this vital and rapidly evolving field. It also provides relevant code and datasets references. Through this comprehensive review, we hope to provide interested readers with pertinent references and insightful perspectives, empowering them with the necessary tools and knowledge to effectively navigate and address the prevailing challenges in the field.",
            "corpus_id": 268041519,
            "sentences": [
                {
                    "corpus_id": "268041519",
                    "title": "Large Language Models(LLMs) on Tabular Data: Prediction, Generation, and Understanding - A Survey",
                    "text": "The ability to synthesize real and highquality data is essential for model development. Data generation is used for augmentation when the data is sparse (Onishi & Meguro, 2023), imputing missing values (Jolicoeur-Martineau et al., 2023), and class rebalancing in imbalanced data (Sauber-Cole & Khoshgoftaar, 2022). Traditional methods for synthetic data generation are mostly based on Copulas (Patki et al., 2016;Li et al., 2020b) and Bayesian networks (Zhang et al., 2017;Madl et al., 2023) while recent advancement in generative models such as Variational Autoencoders (VAEs) (Ma et al., 2020;Darabi & Elor, 2021;Vardhan & Kok, 2020;Liu et al., 2023d;Xu et al., 2023b)), generative adversarial networks (GANs) (Park et al., 2018;Choi et al., 2018;Baowaly et al., 2019;Xu et al., 2019), diffusion models (Kotelnikov et al., 2022;Xu et al., 2023a;Kim et al., 2022b;a;Lee et al., 2023;Zhang et al., 2023c), and LLMs, opened up many new opportunities. These deep learning approaches have demonstrated superior performance over classical methods such as Bayesian networks ( (Xu et al., 2019)). A comprehensive understanding of the strengths and weaknesses of different tabular data synthesis methods can be found in Du & Li (2024). \n\nTable understanding is a broad field, covering various tasks like question answering (QA), natural language inference (NLI), Text2SQL tasks, and more.",
                    "score": 0.375857175152596,
                    "section_title": "Traditional and deep learning in tabular data",
                    "char_start_offset": 9697,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 87
                        },
                        {
                            "start": 88,
                            "end": 314
                        },
                        {
                            "start": 315,
                            "end": 949
                        },
                        {
                            "start": 950,
                            "end": 1090
                        },
                        {
                            "start": 1091,
                            "end": 1228
                        },
                        {
                            "start": 1231,
                            "end": 1381
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 279,
                            "end": 313,
                            "matchedPaperCorpusId": "251725737"
                        },
                        {
                            "start": 393,
                            "end": 413,
                            "matchedPaperCorpusId": "9027681"
                        },
                        {
                            "start": 453,
                            "end": 473,
                            "matchedPaperCorpusId": "13095103"
                        },
                        {
                            "start": 615,
                            "end": 635,
                            "matchedPaperCorpusId": "229131432"
                        },
                        {
                            "start": 635,
                            "end": 653,
                            "matchedPaperCorpusId": "259298730"
                        },
                        {
                            "start": 712,
                            "end": 731,
                            "matchedPaperCorpusId": "47017667"
                        },
                        {
                            "start": 749,
                            "end": 770,
                            "matchedPaperCorpusId": "54479855"
                        },
                        {
                            "start": 830,
                            "end": 847,
                            "matchedPaperCorpusId": "264439324"
                        },
                        {
                            "start": 847,
                            "end": 865,
                            "matchedPaperCorpusId": "249847841"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93017578125
                }
            ],
            "relevance_judgement": 0.93017578125,
            "relevance_judgment_input_expanded": "# Title: Large Language Models(LLMs) on Tabular Data: Prediction, Generation, and Understanding - A Survey\n# Venue: Trans. Mach. Learn. Res.\n# Authors: Xi Fang, Weijie Xu, F. Tan, Jiani Zhang, Ziqing Hu, Yanjun Qi, Scott Nickleach, Diego Socolinsky, Srinivasan H. Sengamedu, Christos Faloutsos\n## Abstract\nRecent breakthroughs in large language modeling have facilitated rigorous exploration of their application in diverse tasks related to tabular data modeling, such as prediction, tabular data synthesis, question answering, and table understanding. Each task presents unique challenges and opportunities. However, there is currently a lack of comprehensive review that summarizes and compares the key techniques, metrics, datasets, models, and optimization approaches in this research domain. This survey aims to address this gap by consolidating recent progress in these areas, offering a thorough survey and taxonomy of the datasets, metrics, and methodologies utilized. It identifies strengths, limitations, unexplored territories, and gaps in the existing literature, while providing some insights for future research directions in this vital and rapidly evolving field. It also provides relevant code and datasets references. Through this comprehensive review, we hope to provide interested readers with pertinent references and insightful perspectives, empowering them with the necessary tools and knowledge to effectively navigate and address the prevailing challenges in the field.\n## Traditional and deep learning in tabular data\nThe ability to synthesize real and highquality data is essential for model development. Data generation is used for augmentation when the data is sparse (Onishi & Meguro, 2023), imputing missing values (Jolicoeur-Martineau et al., 2023), and class rebalancing in imbalanced data (Sauber-Cole & Khoshgoftaar, 2022). Traditional methods for synthetic data generation are mostly based on Copulas (Patki et al., 2016;Li et al., 2020b) and Bayesian networks (Zhang et al., 2017;Madl et al., 2023) while recent advancement in generative models such as Variational Autoencoders (VAEs) (Ma et al., 2020;Darabi & Elor, 2021;Vardhan & Kok, 2020;Liu et al., 2023d;Xu et al., 2023b)), generative adversarial networks (GANs) (Park et al., 2018;Choi et al., 2018;Baowaly et al., 2019;Xu et al., 2019), diffusion models (Kotelnikov et al., 2022;Xu et al., 2023a;Kim et al., 2022b;a;Lee et al., 2023;Zhang et al., 2023c), and LLMs, opened up many new opportunities. These deep learning approaches have demonstrated superior performance over classical methods such as Bayesian networks ( (Xu et al., 2019)). A comprehensive understanding of the strengths and weaknesses of different tabular data synthesis methods can be found in Du & Li (2024). \n\nTable understanding is a broad field, covering various tasks like question answering (QA), natural language inference (NLI), Text2SQL tasks, and more.",
            "reference_string": "[268041519 | Fang et al. | 2024 | Citations: 81]"
        },
        {
            "title": "Teaching Transformers Causal Reasoning through Axiomatic Training",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 33,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.07612, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2063799722",
                    "name": "Aniket Vashishtha"
                },
                {
                    "authorId": "2262494154",
                    "name": "Abhinav Kumar"
                },
                {
                    "authorId": "2110653264",
                    "name": "Abbavaram Gowtham Reddy"
                },
                {
                    "authorId": "2227135768",
                    "name": "Vineeth N. Balasubramanian"
                },
                {
                    "authorId": "2262512588",
                    "name": "Amit Sharma"
                }
            ],
            "abstract": "For text-based AI systems to interact in the real world, causal reasoning is an essential skill. Since active interventions are costly, we study to what extent a system can learn causal reasoning from symbolic demonstrations of causal axioms. Specifically, we present an axiomatic training method where the system learns from multiple demonstrations of a causal axiom (or rule), rather than incorporating the axiom as an inductive bias or inferring it from data values. A key question is whether the system would learn to generalize from the axiom demonstrations to more complex scenarios. Our results, based on applying axiomatic training to learn the transitivity axiom and d-separation rule, indicate that such generalization is possible. To avoid data contamination issues, we start with a 67 million parameter transformer model and train it from scratch. On both tasks, we find that a model trained on linear causal chains (along with some noisy variations) can generalize well to complex graphs, including longer causal chains, causal chains with reversed order, and graphs with branching.To handle diverse text inputs, the same method is extended to finetune language models. Finetuning Llama-3.1 8B model on our axiomatic data leads to significant gains on causal benchmarks such as Corr2Cause and CLEAR, in some cases providing state-of-the-art performance surpassing GPT-4.",
            "corpus_id": 271088512,
            "sentences": [
                {
                    "corpus_id": "271088512",
                    "title": "Teaching Transformers Causal Reasoning through Axiomatic Training",
                    "text": "To understand how transformers can be optimized for learning through axiomatic training and generalizing to unseen larger causal structures, we also examine different types of positional encoding such as no positional encoding (PE), Learnable PEs (Radford et al., 2018) and Sinusoidal PEs (Vaswani et al., 2023). \n\nSynthetic data generation for teaching transformers reasoning: Synthetic data generation has been explored for optimising model training for reasoning. For example, (Li et al., 2023;Gunasekar et al., 2023) use LLM-generated synthetic text for training Phi-1 and Phi-1.5 models and show impressive performance for reasoning-based tasks. (Trinh et al., 2024) introduce a novel neuro-symbolic framework to pre-train a transformer model for Olympiad-level math problems. (Morishita et al., 2024) construct synthetic training data to improve language models' performance on logical reasoning tasks. Building on this stream of work, we apply synthetic data generation for teaching causal reasoning.",
                    "score": 0.4628996966311564,
                    "section_title": "Related Work",
                    "char_start_offset": 7807,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 312
                        },
                        {
                            "start": 315,
                            "end": 466
                        },
                        {
                            "start": 467,
                            "end": 650
                        },
                        {
                            "start": 651,
                            "end": 781
                        },
                        {
                            "start": 782,
                            "end": 908
                        },
                        {
                            "start": 909,
                            "end": 1007
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 651,
                            "end": 671,
                            "matchedPaperCorpusId": "267032902"
                        },
                        {
                            "start": 782,
                            "end": 806,
                            "matchedPaperCorpusId": "274141027"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9267578125
                }
            ],
            "relevance_judgement": 0.9267578125,
            "relevance_judgment_input_expanded": "# Title: Teaching Transformers Causal Reasoning through Axiomatic Training\n# Venue: arXiv.org\n# Authors: Aniket Vashishtha, Abhinav Kumar, Abbavaram Gowtham Reddy, Vineeth N. Balasubramanian, Amit Sharma\n## Abstract\nFor text-based AI systems to interact in the real world, causal reasoning is an essential skill. Since active interventions are costly, we study to what extent a system can learn causal reasoning from symbolic demonstrations of causal axioms. Specifically, we present an axiomatic training method where the system learns from multiple demonstrations of a causal axiom (or rule), rather than incorporating the axiom as an inductive bias or inferring it from data values. A key question is whether the system would learn to generalize from the axiom demonstrations to more complex scenarios. Our results, based on applying axiomatic training to learn the transitivity axiom and d-separation rule, indicate that such generalization is possible. To avoid data contamination issues, we start with a 67 million parameter transformer model and train it from scratch. On both tasks, we find that a model trained on linear causal chains (along with some noisy variations) can generalize well to complex graphs, including longer causal chains, causal chains with reversed order, and graphs with branching.To handle diverse text inputs, the same method is extended to finetune language models. Finetuning Llama-3.1 8B model on our axiomatic data leads to significant gains on causal benchmarks such as Corr2Cause and CLEAR, in some cases providing state-of-the-art performance surpassing GPT-4.\n## Related Work\nTo understand how transformers can be optimized for learning through axiomatic training and generalizing to unseen larger causal structures, we also examine different types of positional encoding such as no positional encoding (PE), Learnable PEs (Radford et al., 2018) and Sinusoidal PEs (Vaswani et al., 2023). \n\nSynthetic data generation for teaching transformers reasoning: Synthetic data generation has been explored for optimising model training for reasoning. For example, (Li et al., 2023;Gunasekar et al., 2023) use LLM-generated synthetic text for training Phi-1 and Phi-1.5 models and show impressive performance for reasoning-based tasks. (Trinh et al., 2024) introduce a novel neuro-symbolic framework to pre-train a transformer model for Olympiad-level math problems. (Morishita et al., 2024) construct synthetic training data to improve language models' performance on logical reasoning tasks. Building on this stream of work, we apply synthetic data generation for teaching causal reasoning.",
            "reference_string": "[271088512 | Vashishtha et al. | 2024 | Citations: 4]"
        },
        {
            "title": "Learning to Infer from Unlabeled Data: A Semi-supervised Learning Approach for Robust Natural Language Inference",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2022,
            "reference_count": 50,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2211.02971",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2211.02971, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2140791766",
                    "name": "Mobashir Sadat"
                },
                {
                    "authorId": "2140493460",
                    "name": "Cornelia Caragea"
                }
            ],
            "abstract": "Natural Language Inference (NLI) or Recognizing Textual Entailment (RTE) aims at predicting the relation between a pair of sentences (premise and hypothesis) as entailment, contradiction or semantic independence. Although deep learning models have shown promising performance for NLI in recent years, they rely on large scale expensive human-annotated datasets. Semi-supervised learning (SSL) is a popular technique for reducing the reliance on human annotation by leveraging unlabeled data for training. However, despite its substantial success on single sentence classification tasks where the challenge in making use of unlabeled data is to assign\"good enough\"pseudo-labels, for NLI tasks, the nature of unlabeled data is more complex: one of the sentences in the pair (usually the hypothesis) along with the class label are missing from the data and require human annotations, which makes SSL for NLI more challenging. In this paper, we propose a novel way to incorporate unlabeled data in SSL for NLI where we use a conditional language model, BART to generate the hypotheses for the unlabeled sentences (used as premises). Our experiments show that our SSL framework successfully exploits unlabeled data and substantially improves the performance of four NLI datasets in low-resource settings. We release our code at: https://github.com/msadat3/SSL_for_NLI.",
            "corpus_id": 253384598,
            "sentences": [
                {
                    "corpus_id": "253384598",
                    "title": "Learning to Infer from Unlabeled Data: A Semi-supervised Learning Approach for Robust Natural Language Inference",
                    "text": "With the advancement of text generation models in recent years (Radford et al., 2019;Brown et al., 2020;Lewis et al., 2020), researchers have started to adopt synthetic text generation as a data augmentation strategy for various tasks such as commonsense reasoning (Yang et al., 2020), sentence classification (Anaby-Tavor et al., 2020) and question answering (Puri et al., 2020). More recently, synthetic text generation methods have also been employed to remove spurious correlations in human annotated data for fact verification (Lee et al., 2021) and NLI (Wu et al., 2022). However, to our knowledge, no prior work aims at tackling low-resource NLI scenarios nor do they address the unavailability of hypotheses for unlabeled premises by using synthetic data generation methods.",
                    "score": 0.4483924175418692,
                    "section_title": "Synthetic Data Generation",
                    "char_start_offset": 28074,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 63,
                            "end": 85,
                            "matchedPaperCorpusId": "160025533"
                        },
                        {
                            "start": 104,
                            "end": 123,
                            "matchedPaperCorpusId": "204960716"
                        },
                        {
                            "start": 265,
                            "end": 284,
                            "matchedPaperCorpusId": "250390686"
                        },
                        {
                            "start": 360,
                            "end": 379,
                            "matchedPaperCorpusId": "211258652"
                        },
                        {
                            "start": 532,
                            "end": 550,
                            "matchedPaperCorpusId": "238227205"
                        },
                        {
                            "start": 559,
                            "end": 576,
                            "matchedPaperCorpusId": "247628095"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92626953125
                }
            ],
            "relevance_judgement": 0.92626953125,
            "relevance_judgment_input_expanded": "# Title: Learning to Infer from Unlabeled Data: A Semi-supervised Learning Approach for Robust Natural Language Inference\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Mobashir Sadat, Cornelia Caragea\n## Abstract\nNatural Language Inference (NLI) or Recognizing Textual Entailment (RTE) aims at predicting the relation between a pair of sentences (premise and hypothesis) as entailment, contradiction or semantic independence. Although deep learning models have shown promising performance for NLI in recent years, they rely on large scale expensive human-annotated datasets. Semi-supervised learning (SSL) is a popular technique for reducing the reliance on human annotation by leveraging unlabeled data for training. However, despite its substantial success on single sentence classification tasks where the challenge in making use of unlabeled data is to assign\"good enough\"pseudo-labels, for NLI tasks, the nature of unlabeled data is more complex: one of the sentences in the pair (usually the hypothesis) along with the class label are missing from the data and require human annotations, which makes SSL for NLI more challenging. In this paper, we propose a novel way to incorporate unlabeled data in SSL for NLI where we use a conditional language model, BART to generate the hypotheses for the unlabeled sentences (used as premises). Our experiments show that our SSL framework successfully exploits unlabeled data and substantially improves the performance of four NLI datasets in low-resource settings. We release our code at: https://github.com/msadat3/SSL_for_NLI.\n## Synthetic Data Generation\nWith the advancement of text generation models in recent years (Radford et al., 2019;Brown et al., 2020;Lewis et al., 2020), researchers have started to adopt synthetic text generation as a data augmentation strategy for various tasks such as commonsense reasoning (Yang et al., 2020), sentence classification (Anaby-Tavor et al., 2020) and question answering (Puri et al., 2020). More recently, synthetic text generation methods have also been employed to remove spurious correlations in human annotated data for fact verification (Lee et al., 2021) and NLI (Wu et al., 2022). However, to our knowledge, no prior work aims at tackling low-resource NLI scenarios nor do they address the unavailability of hypotheses for unlabeled premises by using synthetic data generation methods.",
            "reference_string": "[253384598 | Sadat et al. | 2022 | Citations: 2]"
        },
        {
            "title": "Intellecta Cognitiva: A Comprehensive Dataset for Advancing Academic Knowledge and Machine Reasoning",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 10,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.13065, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2297772548",
                    "name": "PS Ajmal"
                },
                {
                    "authorId": "2297772495",
                    "name": "PS Ditto"
                },
                {
                    "authorId": "2297773046",
                    "name": "VG Jithin"
                }
            ],
            "abstract": "Intellecta dataset emerges as an innovative synthetic dataset, engineered to enhance the cognitive processing capabilities of contemporary language models. With a composition of 11.53 billion tokens, integrating 8.01 billion tokens of synthetic data with 3.52 billion tokens of rich textbook data, Intellecta is crafted to foster advanced reasoning and comprehensive educational narrative generation. Leveraging the Mixtral-8x7B-Instruct-v0.1 model, the dataset facilitates the generation of complex thought processes and detailed, textbook-style explanations, thus enabling language models to engage in both critical thinking and profound educational discourse. This hybrid dataset stands as a testament to the potential of synthetic data in pushing the boundaries of AI, offering a repository that is not only vast and varied but also refined to align with ethical standards and intellectual rigor.",
            "corpus_id": 269294098,
            "sentences": [
                {
                    "corpus_id": "269294098",
                    "title": "Intellecta Cognitiva: A Comprehensive Dataset for Advancing Academic Knowledge and Machine Reasoning",
                    "text": "Intellecta arises as a strategic innovation in the realm of linguistic data resources, targeting the deficit in datasets that effectively sharpen the reasoning prowess of language models.Conventional datasets often fall short in nurturing the complex cognitive functions Rane [2023] that advanced language processing demands.To bridge this chasm, Intellecta offers an expansive and multifaceted dataset, curated to foster advanced reasoning capabilities in language models.Mirroring the success of the most accomplished large language models (LLMs) which thrive on synthetic data Li et al. [2022],Intellecta provides a breadth of highquality, synthetic datasets previously inaccessible to the wider research community.\n\nWith an eye on the triumphs of prominent LLMs, Intellecta aims for extensive model generalization, aspiring to surpass existing synthetic data paradigms by controlling the entirety of the data generation process.This initiative yields a reservoir of millions of diverse data samples that span across multiple domains, thus broadening the horizon for LLMs and strengthening their capacity for diverse problem-solving.Integral to Intellecta is the incorporation of advanced synthetic generation techniques, which fabricate a dual-composed content: one that simulates complex thought processes and another that yields textbookstyle elucidations laden with core concepts and pragmatic examples.We took available open source instruction data as the seed data to generate the synthetic data.From the instructions, we first create a textbook style Gunasekar et al. [2023] text explaining the concept required for answering the instruction.The second step is to enrich the response from the seed data with thought process how the model arrive at that result.Combining these 2 will provide a textbook style concept explanation followed by exercises and the thought process to resolve the same.This correlate to the same a student learn a chapter starting with textbook followed by exercises and thought process behind it.This synthesis forges a dataset that not only offers educational profundity but also incisive analytical clarity, catalyzing a leap forward in the domain of language modeling.",
                    "score": 0.36357988730454766,
                    "section_title": "Introduction",
                    "char_start_offset": 1028,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 187
                        },
                        {
                            "start": 187,
                            "end": 325
                        },
                        {
                            "start": 325,
                            "end": 473
                        },
                        {
                            "start": 473,
                            "end": 718
                        },
                        {
                            "start": 720,
                            "end": 932
                        },
                        {
                            "start": 932,
                            "end": 1136
                        },
                        {
                            "start": 1136,
                            "end": 1410
                        },
                        {
                            "start": 1410,
                            "end": 1505
                        },
                        {
                            "start": 1505,
                            "end": 1652
                        },
                        {
                            "start": 1652,
                            "end": 1770
                        },
                        {
                            "start": 1770,
                            "end": 1904
                        },
                        {
                            "start": 1904,
                            "end": 2032
                        },
                        {
                            "start": 2032,
                            "end": 2207
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92626953125
                }
            ],
            "relevance_judgement": 0.92626953125,
            "relevance_judgment_input_expanded": "# Title: Intellecta Cognitiva: A Comprehensive Dataset for Advancing Academic Knowledge and Machine Reasoning\n# Venue: arXiv.org\n# Authors: PS Ajmal, PS Ditto, VG Jithin\n## Abstract\nIntellecta dataset emerges as an innovative synthetic dataset, engineered to enhance the cognitive processing capabilities of contemporary language models. With a composition of 11.53 billion tokens, integrating 8.01 billion tokens of synthetic data with 3.52 billion tokens of rich textbook data, Intellecta is crafted to foster advanced reasoning and comprehensive educational narrative generation. Leveraging the Mixtral-8x7B-Instruct-v0.1 model, the dataset facilitates the generation of complex thought processes and detailed, textbook-style explanations, thus enabling language models to engage in both critical thinking and profound educational discourse. This hybrid dataset stands as a testament to the potential of synthetic data in pushing the boundaries of AI, offering a repository that is not only vast and varied but also refined to align with ethical standards and intellectual rigor.\n## Introduction\nIntellecta arises as a strategic innovation in the realm of linguistic data resources, targeting the deficit in datasets that effectively sharpen the reasoning prowess of language models.Conventional datasets often fall short in nurturing the complex cognitive functions Rane [2023] that advanced language processing demands.To bridge this chasm, Intellecta offers an expansive and multifaceted dataset, curated to foster advanced reasoning capabilities in language models.Mirroring the success of the most accomplished large language models (LLMs) which thrive on synthetic data Li et al. [2022],Intellecta provides a breadth of highquality, synthetic datasets previously inaccessible to the wider research community.\n\nWith an eye on the triumphs of prominent LLMs, Intellecta aims for extensive model generalization, aspiring to surpass existing synthetic data paradigms by controlling the entirety of the data generation process.This initiative yields a reservoir of millions of diverse data samples that span across multiple domains, thus broadening the horizon for LLMs and strengthening their capacity for diverse problem-solving.Integral to Intellecta is the incorporation of advanced synthetic generation techniques, which fabricate a dual-composed content: one that simulates complex thought processes and another that yields textbookstyle elucidations laden with core concepts and pragmatic examples.We took available open source instruction data as the seed data to generate the synthetic data.From the instructions, we first create a textbook style Gunasekar et al. [2023] text explaining the concept required for answering the instruction.The second step is to enrich the response from the seed data with thought process how the model arrive at that result.Combining these 2 will provide a textbook style concept explanation followed by exercises and the thought process to resolve the same.This correlate to the same a student learn a chapter starting with textbook followed by exercises and thought process behind it.This synthesis forges a dataset that not only offers educational profundity but also incisive analytical clarity, catalyzing a leap forward in the domain of language modeling.",
            "reference_string": "[269294098 | Ajmal et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Executable Functional Abstractions: Inferring Generative Programs for Advanced Math Problems",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 41,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.09763, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2324992429",
                    "name": "Zaid Khan"
                },
                {
                    "authorId": "2281825070",
                    "name": "Elias Stengel-Eskin"
                },
                {
                    "authorId": "1677896557",
                    "name": "Archiki Prasad"
                },
                {
                    "authorId": "2706729",
                    "name": "Jaemin Cho"
                },
                {
                    "authorId": "2281826842",
                    "name": "Mohit Bansal"
                }
            ],
            "abstract": "Scientists often infer abstract procedures from specific instances of problems and use the abstractions to generate new, related instances. For example, programs encoding the formal rules and properties of a system have been useful in fields ranging from RL (procedural environments) to physics (simulation engines). These programs can be seen as functions which execute to different outputs based on their parameterizations (e.g., gridworld configuration or initial physical conditions). We introduce the term EFA (Executable Functional Abstraction) to denote such programs for math problems. EFA-like constructs have been shown to be useful for math reasoning as problem generators for stress-testing models. However, prior work has been limited to abstractions for grade-school math (whose simple rules are easy to encode in programs), while generating EFAs for advanced math has thus far required human engineering. We explore the automatic construction of EFAs for advanced math problems. We operationalize the task of automatically constructing EFAs as a program synthesis task, and develop EFAGen, which conditions an LLM on a seed math problem and its step-by-step solution to generate candidate EFA programs that are faithful to the generalized problem and solution class underlying the seed problem. Furthermore, we formalize properties any valid EFA must possess in terms of executable unit tests, and show how the tests can be used as verifiable rewards to train LLMs to become better writers of EFAs. We demonstrate that EFAs constructed by EFAGen behave rationally by remaining faithful to seed problems, produce learnable problem variations, and that EFAGen can infer EFAs across multiple diverse sources of competition-level math problems. Finally, we show downstream uses of model-written EFAs e.g. finding problem variations that are harder or easier for a learner to solve, as well as data generation.",
            "corpus_id": 277780529,
            "sentences": [
                {
                    "corpus_id": "277780529",
                    "title": "Executable Functional Abstractions: Inferring Generative Programs for Advanced Math Problems",
                    "text": "Past work has generally approached improving models on reasoning tasks like math by generating large amounts of broad-coverage training data. This trend builds on work in generating instruction-tuning data (Wang et al., 2023), where model-generated instructions have been used to teach models to follow prompts. Luo et al. (2023) introduced generation method based on Evol-Instruct (Xu et al., 2023), which augmented a seed dataset of math problems by generating easier and harder problems. Related lines of work have sought to expand datasets by augmenting existing math datasets (Yu et al., 2024), adding multiple reasoning strategies (Yue et al., 2024), covering challenging competition problems (Li et al., 2024), or curating responses (Liu et al., 2024). The data generated in these settings differs from our data in a number of respects: first, it is generally broad-coverage, focusing on large-scale diverse data, as opposed to targeted, instancespecific data. This direction was also explored by Khan et al. (2025), who define data generation agents that can generate specific data based on a particular model's weaknesses, covering math and several other domains. Finally, past work that has augmented a seed dataset (e.g., Yu et al. (2024); Yue et al. (2024)) has done so by modifying problems in the surface form, whereas our method first infers a latent structure and then creates problems by sampling from the structure. In contrast, EFAGen focuses on generating similar examples of existing data by inferring an underlying structure from an example; we show that this has applications to data generation for augmentation but also for stress-testing or measuring the performance gap of models on similar problems.",
                    "score": 0.3509754580961078,
                    "section_title": "Related Work",
                    "char_start_offset": 22724,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 141
                        },
                        {
                            "start": 142,
                            "end": 311
                        },
                        {
                            "start": 312,
                            "end": 490
                        },
                        {
                            "start": 491,
                            "end": 759
                        },
                        {
                            "start": 760,
                            "end": 967
                        },
                        {
                            "start": 968,
                            "end": 1172
                        },
                        {
                            "start": 1173,
                            "end": 1433
                        },
                        {
                            "start": 1434,
                            "end": 1726
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 206,
                            "end": 225,
                            "matchedPaperCorpusId": "254877310"
                        },
                        {
                            "start": 581,
                            "end": 598,
                            "matchedPaperCorpusId": "262084051"
                        },
                        {
                            "start": 637,
                            "end": 655,
                            "matchedPaperCorpusId": "266210109"
                        },
                        {
                            "start": 1004,
                            "end": 1022,
                            "matchedPaperCorpusId": "273228651"
                        },
                        {
                            "start": 1233,
                            "end": 1249,
                            "matchedPaperCorpusId": "262084051"
                        },
                        {
                            "start": 1251,
                            "end": 1268,
                            "matchedPaperCorpusId": "266210109"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9248046875
                }
            ],
            "relevance_judgement": 0.9248046875,
            "relevance_judgment_input_expanded": "# Title: Executable Functional Abstractions: Inferring Generative Programs for Advanced Math Problems\n# Venue: arXiv.org\n# Authors: Zaid Khan, Elias Stengel-Eskin, Archiki Prasad, Jaemin Cho, Mohit Bansal\n## Abstract\nScientists often infer abstract procedures from specific instances of problems and use the abstractions to generate new, related instances. For example, programs encoding the formal rules and properties of a system have been useful in fields ranging from RL (procedural environments) to physics (simulation engines). These programs can be seen as functions which execute to different outputs based on their parameterizations (e.g., gridworld configuration or initial physical conditions). We introduce the term EFA (Executable Functional Abstraction) to denote such programs for math problems. EFA-like constructs have been shown to be useful for math reasoning as problem generators for stress-testing models. However, prior work has been limited to abstractions for grade-school math (whose simple rules are easy to encode in programs), while generating EFAs for advanced math has thus far required human engineering. We explore the automatic construction of EFAs for advanced math problems. We operationalize the task of automatically constructing EFAs as a program synthesis task, and develop EFAGen, which conditions an LLM on a seed math problem and its step-by-step solution to generate candidate EFA programs that are faithful to the generalized problem and solution class underlying the seed problem. Furthermore, we formalize properties any valid EFA must possess in terms of executable unit tests, and show how the tests can be used as verifiable rewards to train LLMs to become better writers of EFAs. We demonstrate that EFAs constructed by EFAGen behave rationally by remaining faithful to seed problems, produce learnable problem variations, and that EFAGen can infer EFAs across multiple diverse sources of competition-level math problems. Finally, we show downstream uses of model-written EFAs e.g. finding problem variations that are harder or easier for a learner to solve, as well as data generation.\n## Related Work\nPast work has generally approached improving models on reasoning tasks like math by generating large amounts of broad-coverage training data. This trend builds on work in generating instruction-tuning data (Wang et al., 2023), where model-generated instructions have been used to teach models to follow prompts. Luo et al. (2023) introduced generation method based on Evol-Instruct (Xu et al., 2023), which augmented a seed dataset of math problems by generating easier and harder problems. Related lines of work have sought to expand datasets by augmenting existing math datasets (Yu et al., 2024), adding multiple reasoning strategies (Yue et al., 2024), covering challenging competition problems (Li et al., 2024), or curating responses (Liu et al., 2024). The data generated in these settings differs from our data in a number of respects: first, it is generally broad-coverage, focusing on large-scale diverse data, as opposed to targeted, instancespecific data. This direction was also explored by Khan et al. (2025), who define data generation agents that can generate specific data based on a particular model's weaknesses, covering math and several other domains. Finally, past work that has augmented a seed dataset (e.g., Yu et al. (2024); Yue et al. (2024)) has done so by modifying problems in the surface form, whereas our method first infers a latent structure and then creates problems by sampling from the structure. In contrast, EFAGen focuses on generating similar examples of existing data by inferring an underlying structure from an example; we show that this has applications to data generation for augmentation but also for stress-testing or measuring the performance gap of models on similar problems.",
            "reference_string": "[277780529 | Khan et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Exploiting Asymmetry for Synthetic Training Data Generation: SynthIE and the Case of Information Extraction",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2023,
            "reference_count": 39,
            "citation_count": 85,
            "influential_citation_count": 6,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2303.04132",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2303.04132, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "65826567",
                    "name": "Martin Josifoski"
                },
                {
                    "authorId": "2122910580",
                    "name": "Marija Sakota"
                },
                {
                    "authorId": "35512303",
                    "name": "Maxime Peyrard"
                },
                {
                    "authorId": "145387102",
                    "name": "Robert West"
                }
            ],
            "abstract": "Large language models (LLMs) have great potential for synthetic data generation. This work shows that useful data can be synthetically generated even for tasks that cannot be solved directly by LLMs: for problems with structured outputs, it is possible to prompt an LLM to perform the task in the reverse direction, by generating plausible input text for a target output structure. Leveraging this asymmetry in task difficulty makes it possible to produce large-scale, high-quality data for complex tasks. We demonstrate the effectiveness of this approach on closed information extraction, where collecting ground-truth data is challenging, and no satisfactory dataset exists to date. We synthetically generate a dataset of 1.8M data points, establish its superior quality compared to existing datasets in a human evaluation, and use it to finetune small models (220M and 770M parameters), termed SynthIE, that outperform the prior state of the art (with equal model size) by a substantial margin of 57 absolute points in micro-F1 and 79 points in macro-F1. Code, data, and models are available at https://github.com/epfl-dlab/SynthIE.",
            "corpus_id": 257378179,
            "sentences": [
                {
                    "corpus_id": "257378179",
                    "title": "Exploiting Asymmetry for Synthetic Training Data Generation: SynthIE and the Case of Information Extraction",
                    "text": "Large language models (LLMs) have demonstrated the ability to generate highly fluent and coherent textual data. One promising application of this capability is the generation of large amounts of high-quality synthetic data for training and evaluating smaller models. This becomes particularly useful for tasks where high-quality datasets are not readily available or access to real data is limited or expensive. However, in many complex natural language processing (NLP) tasks, the textual input x is mapped to a structured (rather than free-text) output y, and in such cases, LLMs may perform poorly as synthetic-data generators, since pretraining did not gear them to produce the specific required output format (even with in-context learning). Here \n\nFigure 1: Exploiting asymmetry for SDG. For hard tasks of interest with input X and output Y , the reverse task (from Y to X) may be much easier for an LLM. If so, we can generate high-quality training pairs (X,Y ) by prompting an LLM to generate plausible inputs X from outputs Y . This often holds true for tasks with structured Y , as in closed information extraction, where X would be the input text and Y would be the list of (subject, relation, object) triplets expressed in the input text. Furthermore, this ensures full control over the sampling distribution P(Y ), and thus balanced datasets. \n\nwe propose to alleviate this issue by generating synthetic data in the reverse direction by first sampling an output structure y and then prompting the LLM to generate a corresponding input text x (see Fig. 1). We postulate that for many tasks, given appropriate in-context information, an LLM can generate a meaningful x corresponding to a structure y, even when the original task cannot be solved directly by the LLM. Exploiting this asymmetry, then, will allow us to synthetically generate high-quality data even for hard tasks. Furthermore, the flexibility to choose the distribution over output structures y gives us fine-grained control over the data. \n\nA good example of such a task, on which we focus in this work, is closed information extraction (cIE). In cIE, a model must extract a set of disambiguated triplets (i.e., facts) y from a given text x.",
                    "score": 0.36572506171905184,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 111
                        },
                        {
                            "start": 112,
                            "end": 266
                        },
                        {
                            "start": 267,
                            "end": 411
                        },
                        {
                            "start": 412,
                            "end": 746
                        },
                        {
                            "start": 747,
                            "end": 751
                        },
                        {
                            "start": 754,
                            "end": 793
                        },
                        {
                            "start": 794,
                            "end": 910
                        },
                        {
                            "start": 911,
                            "end": 1036
                        },
                        {
                            "start": 1037,
                            "end": 1250
                        },
                        {
                            "start": 1251,
                            "end": 1355
                        },
                        {
                            "start": 1358,
                            "end": 1568
                        },
                        {
                            "start": 1569,
                            "end": 1777
                        },
                        {
                            "start": 1778,
                            "end": 1889
                        },
                        {
                            "start": 1890,
                            "end": 2015
                        },
                        {
                            "start": 2018,
                            "end": 2120
                        },
                        {
                            "start": 2121,
                            "end": 2218
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92431640625
                }
            ],
            "relevance_judgement": 0.92431640625,
            "relevance_judgment_input_expanded": "# Title: Exploiting Asymmetry for Synthetic Training Data Generation: SynthIE and the Case of Information Extraction\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Martin Josifoski, Marija Sakota, Maxime Peyrard, Robert West\n## Abstract\nLarge language models (LLMs) have great potential for synthetic data generation. This work shows that useful data can be synthetically generated even for tasks that cannot be solved directly by LLMs: for problems with structured outputs, it is possible to prompt an LLM to perform the task in the reverse direction, by generating plausible input text for a target output structure. Leveraging this asymmetry in task difficulty makes it possible to produce large-scale, high-quality data for complex tasks. We demonstrate the effectiveness of this approach on closed information extraction, where collecting ground-truth data is challenging, and no satisfactory dataset exists to date. We synthetically generate a dataset of 1.8M data points, establish its superior quality compared to existing datasets in a human evaluation, and use it to finetune small models (220M and 770M parameters), termed SynthIE, that outperform the prior state of the art (with equal model size) by a substantial margin of 57 absolute points in micro-F1 and 79 points in macro-F1. Code, data, and models are available at https://github.com/epfl-dlab/SynthIE.\n## Introduction\nLarge language models (LLMs) have demonstrated the ability to generate highly fluent and coherent textual data. One promising application of this capability is the generation of large amounts of high-quality synthetic data for training and evaluating smaller models. This becomes particularly useful for tasks where high-quality datasets are not readily available or access to real data is limited or expensive. However, in many complex natural language processing (NLP) tasks, the textual input x is mapped to a structured (rather than free-text) output y, and in such cases, LLMs may perform poorly as synthetic-data generators, since pretraining did not gear them to produce the specific required output format (even with in-context learning). Here \n\nFigure 1: Exploiting asymmetry for SDG. For hard tasks of interest with input X and output Y , the reverse task (from Y to X) may be much easier for an LLM. If so, we can generate high-quality training pairs (X,Y ) by prompting an LLM to generate plausible inputs X from outputs Y . This often holds true for tasks with structured Y , as in closed information extraction, where X would be the input text and Y would be the list of (subject, relation, object) triplets expressed in the input text. Furthermore, this ensures full control over the sampling distribution P(Y ), and thus balanced datasets. \n\nwe propose to alleviate this issue by generating synthetic data in the reverse direction by first sampling an output structure y and then prompting the LLM to generate a corresponding input text x (see Fig. 1). We postulate that for many tasks, given appropriate in-context information, an LLM can generate a meaningful x corresponding to a structure y, even when the original task cannot be solved directly by the LLM. Exploiting this asymmetry, then, will allow us to synthetically generate high-quality data even for hard tasks. Furthermore, the flexibility to choose the distribution over output structures y gives us fine-grained control over the data. \n\nA good example of such a task, on which we focus in this work, is closed information extraction (cIE). In cIE, a model must extract a set of disambiguated triplets (i.e., facts) y from a given text x.",
            "reference_string": "[257378179 | Josifoski et al. | 2023 | Citations: 85]"
        },
        {
            "title": "Rigorous Experimental Analysis of Tabular Data Generated using TVAE and CTGAN",
            "venue": "International Journal of Advanced Computer Science and Applications",
            "year": 2024,
            "reference_count": 38,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://thesai.org/Downloads/Volume15No4/Paper_125-Rigorous_Experimental_Analysis_of_Tabular_Data_Generated.pdf",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.14569/ijacsa.2024.01504125?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.14569/ijacsa.2024.01504125, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2300552611",
                    "name": "Parul Yadav"
                },
                {
                    "authorId": "2300755856",
                    "name": "Manish Gaur"
                },
                {
                    "authorId": "94670511",
                    "name": "Rahul Kumar Madhukar"
                },
                {
                    "authorId": "2300646169",
                    "name": "Gaurav Verma"
                },
                {
                    "authorId": "2300984779",
                    "name": "Pankaj Kumar"
                },
                {
                    "authorId": "2206080534",
                    "name": "Nishat Fatima"
                },
                {
                    "authorId": "2277471527",
                    "name": "Saqib Sarwar"
                },
                {
                    "authorId": "2300596310",
                    "name": "Yash Raj Dwivedi"
                }
            ],
            "abstract": "\u2014Synthetic data generation research has been progressing at a rapid pace and novel methods are being designed every now and then. Earlier, statistical methods were used to learn the distributions of real data and then sample synthetic data from those distributions. Recent advances in generative models have led to more efficient modeling of complex high-dimensional datasets. Also, privacy concerns have led to the development of robust models with lesser risk of privacy breaches. Firstly, the paper presents a comprehensive survey of existing techniques for tabular data generation and evaluation matrices. Secondly, it elaborates on a comparative analysis of state-of-the-art synthetic data generation techniques, specifically CTGAN and TVAE for small, medium, and large-scale datasets with varying data distributions. It further evaluates the synthetic data using quantitative and qualitative metrics/techniques. Finally, this paper presents the outcomes and also highlights the issues and shortcomings which are still need to be addressed.",
            "corpus_id": 269702974,
            "sentences": [
                {
                    "corpus_id": "269702974",
                    "title": "Rigorous Experimental Analysis of Tabular Data Generated using TVAE and CTGAN",
                    "text": "\u2014Synthetic data generation research has been progressing at a rapid pace and novel methods are being designed every now and then. Earlier, statistical methods were used to learn the distributions of real data and then sample synthetic data from those distributions. Recent advances in generative models have led to more efficient modeling of complex high-dimensional datasets. Also, privacy concerns have led to the development of robust models with lesser risk of privacy breaches. Firstly, the paper presents a comprehensive survey of existing techniques for tabular data generation and evaluation matrices. Secondly, it elaborates on a comparative analysis of state-of-the-art synthetic data generation techniques, specifically CTGAN and TVAE for small, medium, and large-scale datasets with varying data distributions. It further evaluates the synthetic data using quantitative and qualitative metrics/techniques. Finally, this paper presents the outcomes and also highlights the issues and shortcomings which are still need to be addressed.",
                    "score": 0.42482347851273183,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92333984375
                }
            ],
            "relevance_judgement": 0.92333984375,
            "relevance_judgment_input_expanded": "# Title: Rigorous Experimental Analysis of Tabular Data Generated using TVAE and CTGAN\n# Venue: International Journal of Advanced Computer Science and Applications\n# Authors: Parul Yadav, Manish Gaur, Rahul Kumar Madhukar, Gaurav Verma, Pankaj Kumar, Nishat Fatima, Saqib Sarwar, Yash Raj Dwivedi\n## Abstract\n\u2014Synthetic data generation research has been progressing at a rapid pace and novel methods are being designed every now and then. Earlier, statistical methods were used to learn the distributions of real data and then sample synthetic data from those distributions. Recent advances in generative models have led to more efficient modeling of complex high-dimensional datasets. Also, privacy concerns have led to the development of robust models with lesser risk of privacy breaches. Firstly, the paper presents a comprehensive survey of existing techniques for tabular data generation and evaluation matrices. Secondly, it elaborates on a comparative analysis of state-of-the-art synthetic data generation techniques, specifically CTGAN and TVAE for small, medium, and large-scale datasets with varying data distributions. It further evaluates the synthetic data using quantitative and qualitative metrics/techniques. Finally, this paper presents the outcomes and also highlights the issues and shortcomings which are still need to be addressed.\n",
            "reference_string": "[269702974 | Yadav et al. | 2024 | Citations: 1]"
        },
        {
            "title": "The Potential and Limitations of Large Language Models for Text Classification through Synthetic Data Generation",
            "venue": "INTERNATIONAL RESEARCH JOURNAL OF ENGINEERING &amp; APPLIED SCIENCES",
            "year": 2024,
            "reference_count": 0,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.55083/irjeas.2024.v12i02002",
                "status": "BRONZE",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.55083/irjeas.2024.v12i02002?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.55083/irjeas.2024.v12i02002, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2307701429",
                    "name": "Ashok Kumar Pamidi venkata"
                },
                {
                    "authorId": "2307701571",
                    "name": "Leeladhar Gudala"
                }
            ],
            "abstract": "Large language models (LLMs), such as GPT-3 and BERT, have revolutionized the field of natural language processing (NLP), offering remarkable capabilities in text generation, translation, summarization, and classification. Among their many applications, LLMs show promise in text classification tasks, where they can automatically categorize text data into predefined categories or labels. This paper presents a comprehensive review of the potential and limitations of utilizing LLMs for text classification through synthetic data generation techniques. We delve into the methodologies employed in generating synthetic data using LLMs, which include techniques such as data augmentation, adversarial training, and transfer learning. These approaches aim to address issues of data scarcity and domain adaptation in text classification tasks. We explore their effectiveness in enhancing text classification performance, demonstrating how synthetic data can improve model generalization and robustness across diverse domains and languages. Additionally, we discuss the challenges and ethical considerations associated with synthetic data generation, including issues related to data privacy, bias amplification, and model fairness. Furthermore, we examine the impact of model size, pretraining data, and fine-tuning strategies on the performance of LLMs in text classification tasks. Recent studies have shown that larger models with access to more diverse pretraining data tend to achieve higher accuracy and better generalization on downstream tasks. Fine-tuning strategies, such as curriculum learning and self-training, can further improve model performance by adapting the model to task-specific data distributions. Through a critical analysis of existing literature and empirical studies, we provide insights into the current state-of-the-art techniques, identify key research gaps, and propose future directions for advancing the utilization of LLMs in text classification through synthetic data generation. This includes exploring novel approaches for generating diverse and representative synthetic data, developing evaluation metrics for assessing the quality of synthetic data, and investigating the long-term societal impacts of deploying LLMs in real-world applications.",
            "corpus_id": 270658562,
            "sentences": [],
            "relevance_judgement": 0.92236328125,
            "relevance_judgment_input_expanded": "# Title: The Potential and Limitations of Large Language Models for Text Classification through Synthetic Data Generation\n# Venue: INTERNATIONAL RESEARCH JOURNAL OF ENGINEERING &amp; APPLIED SCIENCES\n# Authors: Ashok Kumar Pamidi venkata, Leeladhar Gudala\n## Abstract\nLarge language models (LLMs), such as GPT-3 and BERT, have revolutionized the field of natural language processing (NLP), offering remarkable capabilities in text generation, translation, summarization, and classification. Among their many applications, LLMs show promise in text classification tasks, where they can automatically categorize text data into predefined categories or labels. This paper presents a comprehensive review of the potential and limitations of utilizing LLMs for text classification through synthetic data generation techniques. We delve into the methodologies employed in generating synthetic data using LLMs, which include techniques such as data augmentation, adversarial training, and transfer learning. These approaches aim to address issues of data scarcity and domain adaptation in text classification tasks. We explore their effectiveness in enhancing text classification performance, demonstrating how synthetic data can improve model generalization and robustness across diverse domains and languages. Additionally, we discuss the challenges and ethical considerations associated with synthetic data generation, including issues related to data privacy, bias amplification, and model fairness. Furthermore, we examine the impact of model size, pretraining data, and fine-tuning strategies on the performance of LLMs in text classification tasks. Recent studies have shown that larger models with access to more diverse pretraining data tend to achieve higher accuracy and better generalization on downstream tasks. Fine-tuning strategies, such as curriculum learning and self-training, can further improve model performance by adapting the model to task-specific data distributions. Through a critical analysis of existing literature and empirical studies, we provide insights into the current state-of-the-art techniques, identify key research gaps, and propose future directions for advancing the utilization of LLMs in text classification through synthetic data generation. This includes exploring novel approaches for generating diverse and representative synthetic data, developing evaluation metrics for assessing the quality of synthetic data, and investigating the long-term societal impacts of deploying LLMs in real-world applications.\n",
            "reference_string": "[270658562 | venkata et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Optimization Techniques for Unsupervised Complex Table Reasoning via Self-Training Framework",
            "venue": "IEEE Transactions on Knowledge and Data Engineering",
            "year": 2022,
            "reference_count": 67,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2212.10097",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2212.10097, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2155354022",
                    "name": "Zhenyu Li"
                },
                {
                    "authorId": "2116521868",
                    "name": "Xiuxing Li"
                },
                {
                    "authorId": "2272781120",
                    "name": "Sunqi Fan"
                },
                {
                    "authorId": "2115642141",
                    "name": "Jianyong Wang"
                }
            ],
            "abstract": "Structured tabular data is a fundamental data type in numerous fields, and the capacity to reason over tables is crucial for answering questions and validating hypotheses. However, constructing labeled data for complex reasoning tasks is labor-intensive, and the quantity of annotated data remains insufficient to support the intricate demands of real-world applications. To address the insufficient annotation challenge, we present a self-training framework for unsupervised complex tabular reasoning (UCTR-ST) by generating diverse synthetic data with complex logic. Specifically, UCTR-ST incorporates several essential techniques: we aggregate diverse programs and execute them on tables based on a \u201cProgram-Management\u201d component, and we bridge the gap between programs and text with a powerful \u201cProgram-Transformation\u201d module that generates natural language sentences with complex logic. Furthermore, we optimize the procedure using \u201cTable-Text Manipulator\u201d to handle joint table-text reasoning scenarios. The entire framework utilizes self-training techniques to leverage the unlabeled training data, which results in significant performance improvements when tested on real-world data. Experimental results demonstrate that UCTR-ST achieves above 90% of the supervised model performance on different tasks and domains, reducing the dependence on manual annotation. Additionally, our approach can serve as a data augmentation technique, significantly boosting the performance of supervised models in low-resourced domains.",
            "corpus_id": 254877131,
            "sentences": [
                {
                    "corpus_id": "254877131",
                    "title": "Optimization Techniques for Unsupervised Complex Table Reasoning via Self-Training Framework",
                    "text": "Unsupervised data generation has been extensively studied on various tasks like question answering and natural language inference, and has shown surprising performances [51], [53] [54]. Recently, methods for synthesizing human-like tabular reasoning samples have also been proposed [52], [56]. Chemmengath et al. [46] sample complex SQL queries and generate natural language questions in a seq2seq manner. Eisenschlos et al. [29] generate factual claims leveraging context-free grammar (CFG) and counterfactual heuristics. Unfortunately, these methods focus on a specific task or scenario. Based on the modules and predefined operators, our approach can convert different types of programs into natural language questions or claims with tabular evidence or a hybrid of tabular and textual evidence. \n\nSelf-training has been widely explored in the realm of semi-supervised learning [60], [63], [65]. For example, Li et al. [66] proposed FlexKBQA, a method that combines selftraining and synthetic data to improve the performance of few-shot knowledge based question answering. Most works employ self-training techniques in the few-shot setting, where a small number of labeled samples are available. However, this study effectively combines data generation methods with selftraining to achieve good results in an unsupervised scenario.",
                    "score": 0.38764643923795655,
                    "section_title": "B. Unsupervised Data Generation and Self-Training.",
                    "char_start_offset": 45826,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 185
                        },
                        {
                            "start": 186,
                            "end": 293
                        },
                        {
                            "start": 294,
                            "end": 405
                        },
                        {
                            "start": 406,
                            "end": 522
                        },
                        {
                            "start": 523,
                            "end": 589
                        },
                        {
                            "start": 590,
                            "end": 798
                        },
                        {
                            "start": 801,
                            "end": 898
                        },
                        {
                            "start": 899,
                            "end": 1075
                        },
                        {
                            "start": 1076,
                            "end": 1198
                        },
                        {
                            "start": 1199,
                            "end": 1334
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 881,
                            "end": 885,
                            "matchedPaperCorpusId": "250451588"
                        },
                        {
                            "start": 887,
                            "end": 891,
                            "matchedPaperCorpusId": "7890982"
                        },
                        {
                            "start": 893,
                            "end": 897,
                            "matchedPaperCorpusId": "18507866"
                        },
                        {
                            "start": 922,
                            "end": 926,
                            "matchedPaperCorpusId": "261076103"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.919921875
                }
            ],
            "relevance_judgement": 0.919921875,
            "relevance_judgment_input_expanded": "# Title: Optimization Techniques for Unsupervised Complex Table Reasoning via Self-Training Framework\n# Venue: IEEE Transactions on Knowledge and Data Engineering\n# Authors: Zhenyu Li, Xiuxing Li, Sunqi Fan, Jianyong Wang\n## Abstract\nStructured tabular data is a fundamental data type in numerous fields, and the capacity to reason over tables is crucial for answering questions and validating hypotheses. However, constructing labeled data for complex reasoning tasks is labor-intensive, and the quantity of annotated data remains insufficient to support the intricate demands of real-world applications. To address the insufficient annotation challenge, we present a self-training framework for unsupervised complex tabular reasoning (UCTR-ST) by generating diverse synthetic data with complex logic. Specifically, UCTR-ST incorporates several essential techniques: we aggregate diverse programs and execute them on tables based on a \u201cProgram-Management\u201d component, and we bridge the gap between programs and text with a powerful \u201cProgram-Transformation\u201d module that generates natural language sentences with complex logic. Furthermore, we optimize the procedure using \u201cTable-Text Manipulator\u201d to handle joint table-text reasoning scenarios. The entire framework utilizes self-training techniques to leverage the unlabeled training data, which results in significant performance improvements when tested on real-world data. Experimental results demonstrate that UCTR-ST achieves above 90% of the supervised model performance on different tasks and domains, reducing the dependence on manual annotation. Additionally, our approach can serve as a data augmentation technique, significantly boosting the performance of supervised models in low-resourced domains.\n## B. Unsupervised Data Generation and Self-Training.\nUnsupervised data generation has been extensively studied on various tasks like question answering and natural language inference, and has shown surprising performances [51], [53] [54]. Recently, methods for synthesizing human-like tabular reasoning samples have also been proposed [52], [56]. Chemmengath et al. [46] sample complex SQL queries and generate natural language questions in a seq2seq manner. Eisenschlos et al. [29] generate factual claims leveraging context-free grammar (CFG) and counterfactual heuristics. Unfortunately, these methods focus on a specific task or scenario. Based on the modules and predefined operators, our approach can convert different types of programs into natural language questions or claims with tabular evidence or a hybrid of tabular and textual evidence. \n\nSelf-training has been widely explored in the realm of semi-supervised learning [60], [63], [65]. For example, Li et al. [66] proposed FlexKBQA, a method that combines selftraining and synthetic data to improve the performance of few-shot knowledge based question answering. Most works employ self-training techniques in the few-shot setting, where a small number of labeled samples are available. However, this study effectively combines data generation methods with selftraining to achieve good results in an unsupervised scenario.",
            "reference_string": "[254877131 | Li et al. | 2022 | Citations: 6]"
        },
        {
            "title": "SynDEc: A Synthetic Data Ecosystem",
            "venue": "Electronic Markets",
            "year": 2025,
            "reference_count": 136,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1007/s12525-024-00746-8",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s12525-024-00746-8?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s12525-024-00746-8, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2277750192",
                    "name": "F. Karst"
                },
                {
                    "authorId": "2263394895",
                    "name": "M. Li"
                },
                {
                    "authorId": "1737216",
                    "name": "J. Leimeister"
                }
            ],
            "abstract": "\n Given the critical role of data availability for growth and innovation in financial services, especially small and mid-sized banks lack the data volumes required to fully leverage AI advancements for enhancing fraud detection, operational efficiency, and risk management. With existing solutions facing challenges in scalability, inconsistent standards, and complex privacy regulations, we introduce a synthetic data sharing ecosystem (SynDEc) using generative AI. Employing design science research in collaboration with two banks, among them UnionBank of the Philippines, we developed and validated a synthetic data sharing ecosystem for financial institutions. The derived design principles highlight synthetic data setup, training configurations, and incentivization. Furthermore, our findings show that smaller banks benefit most from SynDEcs and our solution is viable even with limited participation. Thus, we advance data ecosystem design knowledge, show its viability for financial services, and offer practical guidance for privacy-resilient synthetic data sharing, laying groundwork for future applications of SynDEcs.",
            "corpus_id": 275916340,
            "sentences": [
                {
                    "corpus_id": "275916340",
                    "title": "SynDEc: A Synthetic Data Ecosystem",
                    "text": "Synthetic data can be defined as \"data that has been generated using a purpose-built mathematical model or algorithm, with the aim of solving a (set of) data science task(s)\" (Jordon et al., 2022, p. 5). This generation process can take many forms as comprehensively categorized by Bauer et al. (2024) into 20 distinct method types. Among these, generative adversarial networks (GANs) are the most popular. GANs learn by pitting a generator (synthesizes data from random noise) and a discriminator (classifies samples as real or fake) against each other, resulting in two highly skilled networks (Goodfellow et al., 2014). This architecture is highly adaptable, as discriminator and generator can be easily adjusted to new tasks (e.g., time series or graph generation) while being frequently the best-performing synthetic data generation method (Bauer et al., 2024). Another commonly employed synthetic data generation method is autoencoder-based architectures, especially variational autoencoder (VAE) (Kingma & Welling, 2013). VAEs are trained by mapping an input sample to a hidden representation, which is then mapped back to the original vector, thus creating a model that synthesizes valid data from a lower dimensional representation. This decoder model is then used to generate data from random noise which makes it especially useful for learning from data with disentangled features (Bauer et al., 2024). Third, recurrent neural networks, feedforward neural networks which include recurrent edges, are able to generate sequential data of arbitrary length. This makes them ideal for sequence generation tasks such as speech synthesis, music, and time series generation (Lipton et al., 2015). Finally, virtual environments are computer simulations in which algorithms interact with each other based on predefined rules, generating synthetic data in the process (Bonabeau, 2002). \n\nIn the context of machine learning, synthetic data is primarily utilized in three key areas: (i) private data release, (ii) data de-biasing and fairness, and (iii) data augmentation for robustness (Jordon et al., 2022).",
                    "score": 0.4372537097965159,
                    "section_title": "Synthetic data generation and its application",
                    "char_start_offset": 12134,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 203
                        },
                        {
                            "start": 204,
                            "end": 332
                        },
                        {
                            "start": 333,
                            "end": 406
                        },
                        {
                            "start": 407,
                            "end": 622
                        },
                        {
                            "start": 623,
                            "end": 866
                        },
                        {
                            "start": 867,
                            "end": 1028
                        },
                        {
                            "start": 1029,
                            "end": 1241
                        },
                        {
                            "start": 1242,
                            "end": 1413
                        },
                        {
                            "start": 1414,
                            "end": 1564
                        },
                        {
                            "start": 1565,
                            "end": 1699
                        },
                        {
                            "start": 1700,
                            "end": 1885
                        },
                        {
                            "start": 1888,
                            "end": 2107
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1868,
                            "end": 1884,
                            "matchedPaperCorpusId": "14402473"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91748046875
                }
            ],
            "relevance_judgement": 0.91748046875,
            "relevance_judgment_input_expanded": "# Title: SynDEc: A Synthetic Data Ecosystem\n# Venue: Electronic Markets\n# Authors: F. Karst, M. Li, J. Leimeister\n## Abstract\n\n Given the critical role of data availability for growth and innovation in financial services, especially small and mid-sized banks lack the data volumes required to fully leverage AI advancements for enhancing fraud detection, operational efficiency, and risk management. With existing solutions facing challenges in scalability, inconsistent standards, and complex privacy regulations, we introduce a synthetic data sharing ecosystem (SynDEc) using generative AI. Employing design science research in collaboration with two banks, among them UnionBank of the Philippines, we developed and validated a synthetic data sharing ecosystem for financial institutions. The derived design principles highlight synthetic data setup, training configurations, and incentivization. Furthermore, our findings show that smaller banks benefit most from SynDEcs and our solution is viable even with limited participation. Thus, we advance data ecosystem design knowledge, show its viability for financial services, and offer practical guidance for privacy-resilient synthetic data sharing, laying groundwork for future applications of SynDEcs.\n## Synthetic data generation and its application\nSynthetic data can be defined as \"data that has been generated using a purpose-built mathematical model or algorithm, with the aim of solving a (set of) data science task(s)\" (Jordon et al., 2022, p. 5). This generation process can take many forms as comprehensively categorized by Bauer et al. (2024) into 20 distinct method types. Among these, generative adversarial networks (GANs) are the most popular. GANs learn by pitting a generator (synthesizes data from random noise) and a discriminator (classifies samples as real or fake) against each other, resulting in two highly skilled networks (Goodfellow et al., 2014). This architecture is highly adaptable, as discriminator and generator can be easily adjusted to new tasks (e.g., time series or graph generation) while being frequently the best-performing synthetic data generation method (Bauer et al., 2024). Another commonly employed synthetic data generation method is autoencoder-based architectures, especially variational autoencoder (VAE) (Kingma & Welling, 2013). VAEs are trained by mapping an input sample to a hidden representation, which is then mapped back to the original vector, thus creating a model that synthesizes valid data from a lower dimensional representation. This decoder model is then used to generate data from random noise which makes it especially useful for learning from data with disentangled features (Bauer et al., 2024). Third, recurrent neural networks, feedforward neural networks which include recurrent edges, are able to generate sequential data of arbitrary length. This makes them ideal for sequence generation tasks such as speech synthesis, music, and time series generation (Lipton et al., 2015). Finally, virtual environments are computer simulations in which algorithms interact with each other based on predefined rules, generating synthetic data in the process (Bonabeau, 2002). \n\nIn the context of machine learning, synthetic data is primarily utilized in three key areas: (i) private data release, (ii) data de-biasing and fairness, and (iii) data augmentation for robustness (Jordon et al., 2022).",
            "reference_string": "[275916340 | Karst et al. | 2025 | Citations: 2]"
        },
        {
            "title": "Attention-Based Synthetic Data Generation for Calibration-Enhanced Survival Analysis: A Case Study for Chronic Kidney Disease Using Electronic Health Records",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 103,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.06096, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2265851129",
                    "name": "Nicholas I-Hsien Kuo"
                },
                {
                    "authorId": "2292255090",
                    "name": "Blanca Gallego"
                },
                {
                    "authorId": "2292472962",
                    "name": "Louisa R Jorm"
                }
            ],
            "abstract": "Access to real-world healthcare data is limited by stringent privacy regulations and data imbalances, hindering advancements in research and clinical applications. Synthetic data presents a promising solution, yet existing methods often fail to ensure the realism, utility, and calibration essential for robust survival analysis. Here, we introduce Masked Clinical Modelling (MCM), an attention-based framework capable of generating high-fidelity synthetic datasets that preserve critical clinical insights, such as hazard ratios, while enhancing survival model calibration. Unlike traditional statistical methods like SMOTE and machine learning models such as VAEs, MCM supports both standalone dataset synthesis for reproducibility and conditional simulation for targeted augmentation, addressing diverse research needs. Validated on a chronic kidney disease electronic health records dataset, MCM reduced the general calibration loss over the entire dataset by 15%; and MCM reduced a mean calibration loss by 9% across 10 clinically stratified subgroups, outperforming 15 alternative methods. By bridging data accessibility with translational utility, MCM advances the precision of healthcare models, promoting more efficient use of scarce healthcare resources.",
            "corpus_id": 276903896,
            "sentences": [
                {
                    "corpus_id": "276903896",
                    "title": "Attention-Based Synthetic Data Generation for Calibration-Enhanced Survival Analysis: A Case Study for Chronic Kidney Disease Using Electronic Health Records",
                    "text": "Advances in Synthetic Data Generation Synthetic data generation has long been a core component of ML development, progressing from autoencoders [43] to sophisticated models like variational autoencoders (VAEs) [17], generative adversarial networks (GANs) [18], and denoising diffusion probabilistic models (DDPMs) [44]. These methods have achieved exceptional success in domains such as image generation [45], natural language processing in text [46], and text-to-video generation [47].",
                    "score": 0.3544289123930477,
                    "section_title": "Related Work",
                    "char_start_offset": 10787,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 319
                        },
                        {
                            "start": 320,
                            "end": 486
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 144,
                            "end": 148,
                            "matchedPaperCorpusId": "6138085"
                        },
                        {
                            "start": 210,
                            "end": 214,
                            "matchedPaperCorpusId": "211146177"
                        },
                        {
                            "start": 314,
                            "end": 318,
                            "matchedPaperCorpusId": "219955663"
                        },
                        {
                            "start": 404,
                            "end": 408,
                            "matchedPaperCorpusId": "195944196"
                        },
                        {
                            "start": 446,
                            "end": 450,
                            "matchedPaperCorpusId": "1563370"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91552734375
                }
            ],
            "relevance_judgement": 0.91552734375,
            "relevance_judgment_input_expanded": "# Title: Attention-Based Synthetic Data Generation for Calibration-Enhanced Survival Analysis: A Case Study for Chronic Kidney Disease Using Electronic Health Records\n# Venue: arXiv.org\n# Authors: Nicholas I-Hsien Kuo, Blanca Gallego, Louisa R Jorm\n## Abstract\nAccess to real-world healthcare data is limited by stringent privacy regulations and data imbalances, hindering advancements in research and clinical applications. Synthetic data presents a promising solution, yet existing methods often fail to ensure the realism, utility, and calibration essential for robust survival analysis. Here, we introduce Masked Clinical Modelling (MCM), an attention-based framework capable of generating high-fidelity synthetic datasets that preserve critical clinical insights, such as hazard ratios, while enhancing survival model calibration. Unlike traditional statistical methods like SMOTE and machine learning models such as VAEs, MCM supports both standalone dataset synthesis for reproducibility and conditional simulation for targeted augmentation, addressing diverse research needs. Validated on a chronic kidney disease electronic health records dataset, MCM reduced the general calibration loss over the entire dataset by 15%; and MCM reduced a mean calibration loss by 9% across 10 clinically stratified subgroups, outperforming 15 alternative methods. By bridging data accessibility with translational utility, MCM advances the precision of healthcare models, promoting more efficient use of scarce healthcare resources.\n## Related Work\nAdvances in Synthetic Data Generation Synthetic data generation has long been a core component of ML development, progressing from autoencoders [43] to sophisticated models like variational autoencoders (VAEs) [17], generative adversarial networks (GANs) [18], and denoising diffusion probabilistic models (DDPMs) [44]. These methods have achieved exceptional success in domains such as image generation [45], natural language processing in text [46], and text-to-video generation [47].",
            "reference_string": "[276903896 | Kuo et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Visual Chain of Thought: Bridging Logical Gaps with Multimodal Infillings",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 38,
            "citation_count": 47,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2305.02317",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.02317, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2215927057",
                    "name": "Daniel Philip Rose"
                },
                {
                    "authorId": "2215926792",
                    "name": "Vaishnavi Himakunthala"
                },
                {
                    "authorId": "2215914015",
                    "name": "Andy Ouyang"
                },
                {
                    "authorId": "2215915076",
                    "name": "Ryan He"
                },
                {
                    "authorId": "2185480449",
                    "name": "Alex Mei"
                },
                {
                    "authorId": "47006228",
                    "name": "Yujie Lu"
                },
                {
                    "authorId": "48227633",
                    "name": "Michael Stephen Saxon"
                },
                {
                    "authorId": "26961225",
                    "name": "Chinmay Sonar"
                },
                {
                    "authorId": "1705929",
                    "name": "Diba Mirza"
                },
                {
                    "authorId": "1682479",
                    "name": "William Yang Wang"
                }
            ],
            "abstract": "Recent advances in large language models elicit reasoning in a chain-of-thought that allows models to decompose problems in a human-like fashion. Though this paradigm improves multi-step reasoning ability in language models, it is limited by being unimodal and applied mainly to question-answering tasks. We claim that incorporating visual augmentation into reasoning is essential, especially for complex, imaginative tasks. Consequently, we introduce VCoT, a novel method that leverages chain-of-thought prompting with vision-language grounding to recursively bridge the logical gaps within sequential data. Our method uses visual guidance to generate synthetic multimodal infillings that add consistent and novel information to reduce the logical gaps for downstream tasks that can benefit from temporal reasoning, as well as provide interpretability into models' multi-step reasoning. We apply VCoT to the Visual Storytelling and WikiHow summarization datasets and demonstrate through human evaluation that VCoT offers novel and consistent synthetic data augmentation beating chain-of-thought baselines, which can be used to enhance downstream performance.",
            "corpus_id": 258461502,
            "sentences": [
                {
                    "corpus_id": "258461502",
                    "title": "Visual Chain of Thought: Bridging Logical Gaps with Multimodal Infillings",
                    "text": "First, multi-step reasoning with visuals better fills logical gaps because images capture additional information that unimodal text cannot. Second, visual chains mimic human imagination which creates novel solutions (Tan & Bansal, 2021;Lu et al., 2022;Zhu et al., 2022) and provide interpretability (Wang et al., 2022b) into decision making. One imagined picture provides a thousand-word insight to enhance computer reasoning. \n\nWe propose Visual Chain-of-Thought (VCOT), which combines the efficiency, robustness, and multi-step reasoning of COT with the multimodal capabilities of vision-language models. VCOT synthetically augments sequential datasets and bridges logical gaps by recursively generating multimodal infillings and using the synthetic data to improve downstream task performance. These synthetic generations also serve as human-interpretable insights into AI systems' ability of multi-step reasoning. We demonstrate that VCOT creates consistent and novel synthetic data that enhances downstream performance on the VIST (Huang et al., 2016) and WIKIHOW (Koupaee & Wang, 2018) datasets. Our main contributions are: \n\n\u2022 We propose Visual Chain-of-Thought for sequential data to generate synthetic text-visual pairs as data augmentation for downstream reasoning. \u2022 We devise a consistency and novelty-driven approach to recursively generate multimodal infillings that augment faithful, relevant context. \u2022 We demonstrate the effectiveness of our method through human evaluation, showing improvements in sequential reasoning.",
                    "score": 0.3493478763314063,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 2000,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 139
                        },
                        {
                            "start": 140,
                            "end": 341
                        },
                        {
                            "start": 342,
                            "end": 426
                        },
                        {
                            "start": 429,
                            "end": 606
                        },
                        {
                            "start": 607,
                            "end": 796
                        },
                        {
                            "start": 797,
                            "end": 917
                        },
                        {
                            "start": 918,
                            "end": 1101
                        },
                        {
                            "start": 1102,
                            "end": 1129
                        },
                        {
                            "start": 1132,
                            "end": 1275
                        },
                        {
                            "start": 1276,
                            "end": 1416
                        },
                        {
                            "start": 1417,
                            "end": 1537
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9150390625
                }
            ],
            "relevance_judgement": 0.9150390625,
            "relevance_judgment_input_expanded": "# Title: Visual Chain of Thought: Bridging Logical Gaps with Multimodal Infillings\n# Venue: arXiv.org\n# Authors: Daniel Philip Rose, Vaishnavi Himakunthala, Andy Ouyang, Ryan He, Alex Mei, Yujie Lu, Michael Stephen Saxon, Chinmay Sonar, Diba Mirza, William Yang Wang\n## Abstract\nRecent advances in large language models elicit reasoning in a chain-of-thought that allows models to decompose problems in a human-like fashion. Though this paradigm improves multi-step reasoning ability in language models, it is limited by being unimodal and applied mainly to question-answering tasks. We claim that incorporating visual augmentation into reasoning is essential, especially for complex, imaginative tasks. Consequently, we introduce VCoT, a novel method that leverages chain-of-thought prompting with vision-language grounding to recursively bridge the logical gaps within sequential data. Our method uses visual guidance to generate synthetic multimodal infillings that add consistent and novel information to reduce the logical gaps for downstream tasks that can benefit from temporal reasoning, as well as provide interpretability into models' multi-step reasoning. We apply VCoT to the Visual Storytelling and WikiHow summarization datasets and demonstrate through human evaluation that VCoT offers novel and consistent synthetic data augmentation beating chain-of-thought baselines, which can be used to enhance downstream performance.\n## INTRODUCTION\nFirst, multi-step reasoning with visuals better fills logical gaps because images capture additional information that unimodal text cannot. Second, visual chains mimic human imagination which creates novel solutions (Tan & Bansal, 2021;Lu et al., 2022;Zhu et al., 2022) and provide interpretability (Wang et al., 2022b) into decision making. One imagined picture provides a thousand-word insight to enhance computer reasoning. \n\nWe propose Visual Chain-of-Thought (VCOT), which combines the efficiency, robustness, and multi-step reasoning of COT with the multimodal capabilities of vision-language models. VCOT synthetically augments sequential datasets and bridges logical gaps by recursively generating multimodal infillings and using the synthetic data to improve downstream task performance. These synthetic generations also serve as human-interpretable insights into AI systems' ability of multi-step reasoning. We demonstrate that VCOT creates consistent and novel synthetic data that enhances downstream performance on the VIST (Huang et al., 2016) and WIKIHOW (Koupaee & Wang, 2018) datasets. Our main contributions are: \n\n\u2022 We propose Visual Chain-of-Thought for sequential data to generate synthetic text-visual pairs as data augmentation for downstream reasoning. \u2022 We devise a consistency and novelty-driven approach to recursively generate multimodal infillings that augment faithful, relevant context. \u2022 We demonstrate the effectiveness of our method through human evaluation, showing improvements in sequential reasoning.",
            "reference_string": "[258461502 | Rose et al. | 2023 | Citations: 47]"
        },
        {
            "title": "Synthetic continued pretraining",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "reference_count": 105,
            "citation_count": 16,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.07431, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2283514777",
                    "name": "Zitong Yang"
                },
                {
                    "authorId": "2294359410",
                    "name": "Neil Band"
                },
                {
                    "authorId": "2320941550",
                    "name": "Shuangping Li"
                },
                {
                    "authorId": "2283307289",
                    "name": "Emmanuel J. Candes"
                },
                {
                    "authorId": "2294362683",
                    "name": "Tatsunori Hashimoto"
                }
            ],
            "abstract": "Pretraining on large-scale, unstructured internet text enables language models to acquire a significant amount of world knowledge. However, this knowledge acquisition is data-inefficient--to learn a given fact, models must be trained on hundreds to thousands of diverse representations of it. This poses a challenge when adapting a pretrained model to a small corpus of domain-specific documents, where each fact may appear rarely or only once. We propose to bridge this gap with synthetic continued pretraining: using the small domain-specific corpus to synthesize a large corpus more amenable to learning, and then performing continued pretraining on the synthesized corpus. We instantiate this proposal with EntiGraph, a synthetic data augmentation algorithm that extracts salient entities from the source documents and then generates diverse text by drawing connections between the sampled entities. Synthetic continued pretraining with EntiGraph enables a language model to answer questions and follow generic instructions related to the source documents without access to them. If, instead, the source documents are available at inference time, we show that the knowledge acquired through our approach compounds with retrieval-augmented generation. To better understand these results, we build a simple mathematical model of EntiGraph, and show how synthetic data augmentation can\"rearrange\"knowledge to enable more data-efficient learning.",
            "corpus_id": 272593422,
            "sentences": [
                {
                    "corpus_id": "272593422",
                    "title": "Synthetic continued pretraining",
                    "text": "We next discuss recent work most related to our setting of synthetic data generation for continued pretraining. In Appendix A, we provide an extended survey of classical work on synthetic data generation and continual learning. \n\nSynthetic generation of pretraining data. Recent approaches synthesize pretraining data using hierarchical prompting methods to promote dataset diversity. Eldan & Li (2023) prompt API-based LLMs to generate children's stories containing sampled keywords, and demonstrate that even small language models trained on their dataset can generate fluent text. Gunasekar et al. (2023) synthesize a diverse dataset of textbooks and code exercises by conditioning on topic, target audience, and function names, and later release strong LLMs pretrained on synthetic data in follow-up work (Li et al., 2023b;Abdin et al., 2023;2024). However, their datasets and prompts are not publicly available. Maini et al. (2024) prompt an LM to rephrase documents for pretraining, improving training efficiency. Different from all above works, our focus is teaching a pretrained LLM the knowledge of a small corpus. Mecklenburg et al. (2024) consider task-specific finetuning and propose a fact-based synthetic QA generation procedure, but do not show improvement on generic instruction following tasks beyond simple QA. We instead focus on teaching a model generally useful knowledge about a small corpus, untied to a particular downstream task. Ovadia et al. (2024) continually pretrain Llama 2-based language models on synthetic paraphrases of Wikipedia articles, but do not observe consistent performance improvements. We adapt the approach of Maini et al. (2024) and Mecklenburg et al. (2024) to our small corpus setting as the \"Rephrase baseline\" in \u00a74. We find that our graph-based augmentation algorithm outperforms it, likely because our approach enforces diversity through entity-based generation. \n\nContinued pretraining.",
                    "score": 0.39018229507054425,
                    "section_title": "RELATED WORK",
                    "char_start_offset": 7836,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 111
                        },
                        {
                            "start": 112,
                            "end": 227
                        },
                        {
                            "start": 230,
                            "end": 271
                        },
                        {
                            "start": 272,
                            "end": 384
                        },
                        {
                            "start": 385,
                            "end": 583
                        },
                        {
                            "start": 584,
                            "end": 852
                        },
                        {
                            "start": 853,
                            "end": 916
                        },
                        {
                            "start": 917,
                            "end": 1019
                        },
                        {
                            "start": 1020,
                            "end": 1123
                        },
                        {
                            "start": 1124,
                            "end": 1328
                        },
                        {
                            "start": 1329,
                            "end": 1454
                        },
                        {
                            "start": 1455,
                            "end": 1630
                        },
                        {
                            "start": 1631,
                            "end": 1767
                        },
                        {
                            "start": 1768,
                            "end": 1915
                        },
                        {
                            "start": 1918,
                            "end": 1940
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 917,
                            "end": 936,
                            "matchedPaperCorpusId": "267312030"
                        },
                        {
                            "start": 1656,
                            "end": 1675,
                            "matchedPaperCorpusId": "267312030"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.912109375
                }
            ],
            "relevance_judgement": 0.912109375,
            "relevance_judgment_input_expanded": "# Title: Synthetic continued pretraining\n# Venue: International Conference on Learning Representations\n# Authors: Zitong Yang, Neil Band, Shuangping Li, Emmanuel J. Candes, Tatsunori Hashimoto\n## Abstract\nPretraining on large-scale, unstructured internet text enables language models to acquire a significant amount of world knowledge. However, this knowledge acquisition is data-inefficient--to learn a given fact, models must be trained on hundreds to thousands of diverse representations of it. This poses a challenge when adapting a pretrained model to a small corpus of domain-specific documents, where each fact may appear rarely or only once. We propose to bridge this gap with synthetic continued pretraining: using the small domain-specific corpus to synthesize a large corpus more amenable to learning, and then performing continued pretraining on the synthesized corpus. We instantiate this proposal with EntiGraph, a synthetic data augmentation algorithm that extracts salient entities from the source documents and then generates diverse text by drawing connections between the sampled entities. Synthetic continued pretraining with EntiGraph enables a language model to answer questions and follow generic instructions related to the source documents without access to them. If, instead, the source documents are available at inference time, we show that the knowledge acquired through our approach compounds with retrieval-augmented generation. To better understand these results, we build a simple mathematical model of EntiGraph, and show how synthetic data augmentation can\"rearrange\"knowledge to enable more data-efficient learning.\n## RELATED WORK\nWe next discuss recent work most related to our setting of synthetic data generation for continued pretraining. In Appendix A, we provide an extended survey of classical work on synthetic data generation and continual learning. \n\nSynthetic generation of pretraining data. Recent approaches synthesize pretraining data using hierarchical prompting methods to promote dataset diversity. Eldan & Li (2023) prompt API-based LLMs to generate children's stories containing sampled keywords, and demonstrate that even small language models trained on their dataset can generate fluent text. Gunasekar et al. (2023) synthesize a diverse dataset of textbooks and code exercises by conditioning on topic, target audience, and function names, and later release strong LLMs pretrained on synthetic data in follow-up work (Li et al., 2023b;Abdin et al., 2023;2024). However, their datasets and prompts are not publicly available. Maini et al. (2024) prompt an LM to rephrase documents for pretraining, improving training efficiency. Different from all above works, our focus is teaching a pretrained LLM the knowledge of a small corpus. Mecklenburg et al. (2024) consider task-specific finetuning and propose a fact-based synthetic QA generation procedure, but do not show improvement on generic instruction following tasks beyond simple QA. We instead focus on teaching a model generally useful knowledge about a small corpus, untied to a particular downstream task. Ovadia et al. (2024) continually pretrain Llama 2-based language models on synthetic paraphrases of Wikipedia articles, but do not observe consistent performance improvements. We adapt the approach of Maini et al. (2024) and Mecklenburg et al. (2024) to our small corpus setting as the \"Rephrase baseline\" in \u00a74. We find that our graph-based augmentation algorithm outperforms it, likely because our approach enforces diversity through entity-based generation. \n\nContinued pretraining.",
            "reference_string": "[272593422 | Yang et al. | 2024 | Citations: 16]"
        },
        {
            "title": "GraDA: Graph Generative Data Augmentation for Commonsense Reasoning",
            "venue": "International Conference on Computational Linguistics",
            "year": 2022,
            "reference_count": 85,
            "citation_count": 96,
            "influential_citation_count": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2022.dlg4nlp-1.6.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2022.coling-1.397, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "8785371",
                    "name": "Adyasha Maharana"
                },
                {
                    "authorId": "143977268",
                    "name": "Mohit Bansal"
                }
            ],
            "abstract": "Recent advances in commonsense reasoning have been fueled by the availability of large-scale human annotated datasets. Manual annotation of such datasets, many of which are based on existing knowledge bases, is expensive and not scalable. Moreover, it is challenging to build augmentation data for commonsense reasoning because the synthetic questions need to adhere to real-world scenarios. Hence, we present GraDA, a graph-generative data augmentation framework to synthesize factual data samples from knowledge graphs for commonsense reasoning datasets. First, we train a graph-to-text model for conditional generation of questions from graph entities and relations. Then, we train a generator with GAN loss to generate distractors for synthetic questions. Our approach improves performance for SocialIQA, CODAH, HellaSwag and CommonsenseQA, and works well for generative tasks like ProtoQA. We show improvement in robustness to semantic adversaries after training with GraDA and provide human evaluation of the quality of synthetic datasets in terms of factuality and answerability. Our work provides evidence and encourages future research into graph-based generative data augmentation.",
            "corpus_id": 250390686,
            "sentences": [
                {
                    "corpus_id": "250390686",
                    "title": "GraDA: Graph Generative Data Augmentation for Commonsense Reasoning",
                    "text": "Recent work has seen the emergence of several datasets for improving commonsense reasoning of language models through tasks like question answering (QA) (Sap et al., 2019b;Talmor et al., 2019;Bisk et al., 2020) and natural language inference (Bhagavatula et al., 2020;Zellers et al., 2019;Sakaguchi et al., 2020). Some of these datasets are based on existing knowledge graphs that represent different aspects of commonsense through entities and relations. For example, annotators for SocialIQA (Sap et al., 2019b) were shown an event from the inferential knowledge graph ATOMIC (Sap et al., 2019a) and instructed to turn it into a sentence by adding names, filling placeholders and adding context, etc. For multiple-choice QA datasets, annotators are also instructed to write distractor choices for each question. These useful datasets are collected through a time-taking and money-intensive crowdsourcing process which is hard to scale. Large pretrained models like GPT2 (Radford et al., 2018) can be finetuned to generate sentences from narrow data distributions, and it has recently been leveraged to augment datasets for text classification (Anaby-Tavor et al., 2020) and question answering (Puri et al., 2020;Yang et al., 2020). However, it is challenging to generate augmentation data for commonsense reasoning because the generated questions and answers (referred to as \"synthetic\" in rest of the paper) need to depict plausible real-world scenarios accurately. Hence, we develop GRADA, a graph-based generative data augmentation framework to generate synthetic samples from existing knowledge graphs that encode information about the real world. We focus on generating synthetic samples for models that perform discriminative and generative commonsense question answering.\n\nEach sample in commonsense reasoning datasets comprises a question which describes a real-world scenario and can be mapped to a set of predefined entities and relations from knowledge bases like ConceptNet and ATOMIC. For instance, the question \"Besides a mattress, name something people sleep on.\" from the ProtoQA dataset (Boratko et al",
                    "score": 0.3692202581975264,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91064453125
                }
            ],
            "relevance_judgement": 0.91064453125,
            "relevance_judgment_input_expanded": "# Title: GraDA: Graph Generative Data Augmentation for Commonsense Reasoning\n# Venue: International Conference on Computational Linguistics\n# Authors: Adyasha Maharana, Mohit Bansal\n## Abstract\nRecent advances in commonsense reasoning have been fueled by the availability of large-scale human annotated datasets. Manual annotation of such datasets, many of which are based on existing knowledge bases, is expensive and not scalable. Moreover, it is challenging to build augmentation data for commonsense reasoning because the synthetic questions need to adhere to real-world scenarios. Hence, we present GraDA, a graph-generative data augmentation framework to synthesize factual data samples from knowledge graphs for commonsense reasoning datasets. First, we train a graph-to-text model for conditional generation of questions from graph entities and relations. Then, we train a generator with GAN loss to generate distractors for synthetic questions. Our approach improves performance for SocialIQA, CODAH, HellaSwag and CommonsenseQA, and works well for generative tasks like ProtoQA. We show improvement in robustness to semantic adversaries after training with GraDA and provide human evaluation of the quality of synthetic datasets in terms of factuality and answerability. Our work provides evidence and encourages future research into graph-based generative data augmentation.\n## Introduction\nRecent work has seen the emergence of several datasets for improving commonsense reasoning of language models through tasks like question answering (QA) (Sap et al., 2019b;Talmor et al., 2019;Bisk et al., 2020) and natural language inference (Bhagavatula et al., 2020;Zellers et al., 2019;Sakaguchi et al., 2020). Some of these datasets are based on existing knowledge graphs that represent different aspects of commonsense through entities and relations. For example, annotators for SocialIQA (Sap et al., 2019b) were shown an event from the inferential knowledge graph ATOMIC (Sap et al., 2019a) and instructed to turn it into a sentence by adding names, filling placeholders and adding context, etc. For multiple-choice QA datasets, annotators are also instructed to write distractor choices for each question. These useful datasets are collected through a time-taking and money-intensive crowdsourcing process which is hard to scale. Large pretrained models like GPT2 (Radford et al., 2018) can be finetuned to generate sentences from narrow data distributions, and it has recently been leveraged to augment datasets for text classification (Anaby-Tavor et al., 2020) and question answering (Puri et al., 2020;Yang et al., 2020). However, it is challenging to generate augmentation data for commonsense reasoning because the generated questions and answers (referred to as \"synthetic\" in rest of the paper) need to depict plausible real-world scenarios accurately. Hence, we develop GRADA, a graph-based generative data augmentation framework to generate synthetic samples from existing knowledge graphs that encode information about the real world. We focus on generating synthetic samples for models that perform discriminative and generative commonsense question answering.\n\nEach sample in commonsense reasoning datasets comprises a question which describes a real-world scenario and can be mapped to a set of predefined entities and relations from knowledge bases like ConceptNet and ATOMIC. For instance, the question \"Besides a mattress, name something people sleep on.\" from the ProtoQA dataset (Boratko et al",
            "reference_string": "[250390686 | Maharana et al. | 2022 | Citations: 96]"
        },
        {
            "title": "Synthetically generating human-like data for sequential decision-making tasks via reward-shaped imitation learning",
            "venue": "Defense + Commercial Sensing",
            "year": 2023,
            "reference_count": 40,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2304.07280",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2304.07280, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "6467714",
                    "name": "Bryan C. Brandt"
                },
                {
                    "authorId": "34787658",
                    "name": "P. Dasgupta"
                }
            ],
            "abstract": "We consider the problem of synthetically generating data that can closely resemble human decisions made in the context of an interactive human-AI system like a computer game. We propose a novel algorithm that can generate synthetic, human-like, decision making data while starting from a very small set of decision making data collected from humans. Our proposed algorithm integrates the concept of reward shaping with an imitation learning algorithm to generate the synthetic data. We have validated our synthetic data generation technique by using the synthetically generated data as a surrogate for human interaction data to solve three sequential decision making tasks of increasing complexity within a small computer game-like setup. Different empirical and statistical analyses of our results show that the synthetically generated data can substitute the human data and perform the game-playing tasks almost indistinguishably, with very low divergence, from a human performing the same tasks.",
            "corpus_id": 258170048,
            "sentences": [
                {
                    "corpus_id": "258170048",
                    "title": "Synthetically generating human-like data for sequential decision-making tasks via reward-shaped imitation learning",
                    "text": "We consider the problem of synthetically generating data that can closely resemble human decisions made in the context of an interactive human-AI system like a computer game. We propose a novel algorithm that can generate synthetic, human-like, decision making data while starting from a very small set of decision making data collected from humans. Our proposed algorithm integrates the concept of reward shaping with an imitation learning algorithm to generate the synthetic data. We have validated our synthetic data generation technique by using the synthetically generated data as a surrogate for human interaction data to solve three sequential decision making tasks of increasing complexity within a small computer game-like setup. Different empirical and statistical analyses of our results show that the synthetically generated data can substitute the human data and perform the game-playing tasks almost indistinguishably, with very low divergence, from a human performing the same tasks.",
                    "score": 0.42987121895931385,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91015625
                }
            ],
            "relevance_judgement": 0.91015625,
            "relevance_judgment_input_expanded": "# Title: Synthetically generating human-like data for sequential decision-making tasks via reward-shaped imitation learning\n# Venue: Defense + Commercial Sensing\n# Authors: Bryan C. Brandt, P. Dasgupta\n## Abstract\nWe consider the problem of synthetically generating data that can closely resemble human decisions made in the context of an interactive human-AI system like a computer game. We propose a novel algorithm that can generate synthetic, human-like, decision making data while starting from a very small set of decision making data collected from humans. Our proposed algorithm integrates the concept of reward shaping with an imitation learning algorithm to generate the synthetic data. We have validated our synthetic data generation technique by using the synthetically generated data as a surrogate for human interaction data to solve three sequential decision making tasks of increasing complexity within a small computer game-like setup. Different empirical and statistical analyses of our results show that the synthetically generated data can substitute the human data and perform the game-playing tasks almost indistinguishably, with very low divergence, from a human performing the same tasks.\n",
            "reference_string": "[258170048 | Brandt et al. | 2023 | Citations: 1]"
        },
        {
            "title": "Key-Point-Driven Data Synthesis with its Enhancement on Mathematical Reasoning",
            "venue": "AAAI Conference on Artificial Intelligence",
            "year": 2024,
            "reference_count": 85,
            "citation_count": 43,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.02333, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2261394934",
                    "name": "Yiming Huang"
                },
                {
                    "authorId": "49544272",
                    "name": "Xiao Liu"
                },
                {
                    "authorId": "2254121650",
                    "name": "Yeyun Gong"
                },
                {
                    "authorId": "1797090",
                    "name": "Zhibin Gou"
                },
                {
                    "authorId": "2237948786",
                    "name": "Yelong Shen"
                },
                {
                    "authorId": "2269471632",
                    "name": "Nan Duan"
                },
                {
                    "authorId": "2249538838",
                    "name": "Weizhu Chen"
                }
            ],
            "abstract": "Large language models have shown great potential in complex reasoning tasks, yet their performance is often hampered by the scarcity of high-quality and reasoning-focused training datasets. Addressing this challenge, we propose Key-PointDriven Data Synthesis (KPDDS), a novel data synthesis framework that synthesizes question-answer pairs by leveraging key points and exemplar practices from authentic data sources. KPDDS ensures the generation of novel questions with rigorous quality control and substantial scalability. As a result, we present KPMath, an extensive synthetic dataset tailored for mathematical reasoning, comprising over 800K questionanswer pairs. Utilizing KPMath and augmenting it with additional reasoning-intensive corpora, we create the comprehensive KPMath-Plus dataset. Our experiments demonstrate that this dataset can enhance the mathematical reasoning performance of models across various architectures and sizes. The Qwen1.5-72B model, fine-tuned on KPMath-Plus, achieves 87.0% accuracy on GSM8K and 58.3% on MATH, surpassing competitors in the 7B to 72B range and best commercial models like GPT-4 across multiple math reasoning datasets.",
            "corpus_id": 268247488,
            "sentences": [
                {
                    "corpus_id": "268247488",
                    "title": "Key-Point-Driven Data Synthesis with its Enhancement on Mathematical Reasoning",
                    "text": "In this paper, we propose a new data synthesis paradigm that is focused on the generation of large-scale, high-quality, symbolically-driven training datasets.Leveraging this paradigm, we have developed an extensive synthetic dataset tailored for mathematical reasoning.By utilizing this data set, our fine-tuned model achieved excellent performance in multiple data sets including MATH and GSM8K, and the performance exceeded all 7B to 70B competitors.Our research underscores the efficacy of integrating key points in data synthesis and applying stringent quality control protocols to both questions and answers.\n\nPre-print",
                    "score": 0.4064586197702927,
                    "section_title": "Conclusion",
                    "char_start_offset": 23823,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 158
                        },
                        {
                            "start": 158,
                            "end": 269
                        },
                        {
                            "start": 269,
                            "end": 452
                        },
                        {
                            "start": 452,
                            "end": 613
                        },
                        {
                            "start": 615,
                            "end": 624
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90380859375
                }
            ],
            "relevance_judgement": 0.90380859375,
            "relevance_judgment_input_expanded": "# Title: Key-Point-Driven Data Synthesis with its Enhancement on Mathematical Reasoning\n# Venue: AAAI Conference on Artificial Intelligence\n# Authors: Yiming Huang, Xiao Liu, Yeyun Gong, Zhibin Gou, Yelong Shen, Nan Duan, Weizhu Chen\n## Abstract\nLarge language models have shown great potential in complex reasoning tasks, yet their performance is often hampered by the scarcity of high-quality and reasoning-focused training datasets. Addressing this challenge, we propose Key-PointDriven Data Synthesis (KPDDS), a novel data synthesis framework that synthesizes question-answer pairs by leveraging key points and exemplar practices from authentic data sources. KPDDS ensures the generation of novel questions with rigorous quality control and substantial scalability. As a result, we present KPMath, an extensive synthetic dataset tailored for mathematical reasoning, comprising over 800K questionanswer pairs. Utilizing KPMath and augmenting it with additional reasoning-intensive corpora, we create the comprehensive KPMath-Plus dataset. Our experiments demonstrate that this dataset can enhance the mathematical reasoning performance of models across various architectures and sizes. The Qwen1.5-72B model, fine-tuned on KPMath-Plus, achieves 87.0% accuracy on GSM8K and 58.3% on MATH, surpassing competitors in the 7B to 72B range and best commercial models like GPT-4 across multiple math reasoning datasets.\n## Conclusion\nIn this paper, we propose a new data synthesis paradigm that is focused on the generation of large-scale, high-quality, symbolically-driven training datasets.Leveraging this paradigm, we have developed an extensive synthetic dataset tailored for mathematical reasoning.By utilizing this data set, our fine-tuned model achieved excellent performance in multiple data sets including MATH and GSM8K, and the performance exceeded all 7B to 70B competitors.Our research underscores the efficacy of integrating key points in data synthesis and applying stringent quality control protocols to both questions and answers.\n\nPre-print",
            "reference_string": "[268247488 | Huang et al. | 2024 | Citations: 43]"
        },
        {
            "title": "Balancing Cost and Effectiveness of Synthetic Data Generation Strategies for LLMs",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 34,
            "citation_count": 10,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.19759, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2324074843",
                    "name": "Yung-Chieh Chan"
                },
                {
                    "authorId": "2323513733",
                    "name": "George Pu"
                },
                {
                    "authorId": "1390689296",
                    "name": "Apaar Shanker"
                },
                {
                    "authorId": "2268781105",
                    "name": "Parth Suresh"
                },
                {
                    "authorId": "2323512524",
                    "name": "Penn Jenks"
                },
                {
                    "authorId": "2323512547",
                    "name": "John Heyer"
                },
                {
                    "authorId": "2323515242",
                    "name": "Sam Denton"
                }
            ],
            "abstract": "As large language models (LLMs) are applied to more use cases, creating high quality, task-specific datasets for fine-tuning becomes a bottleneck for model improvement. Using high quality human data has been the most common approach to unlock model performance, but is prohibitively expensive in many scenarios. Several alternative methods have also emerged, such as generating synthetic or hybrid data, but the effectiveness of these approaches remain unclear, especially in resource-constrained scenarios and tasks that are not easily verified. To investigate this, we group various synthetic data generation strategies into three representative categories -- Answer Augmentation, Question Rephrase and New Question -- and study the performance of student LLMs trained under various constraints, namely seed instruction set size and query budget. We demonstrate that these strategies are not equally effective across settings. Notably, the optimal data generation strategy depends strongly on the ratio between the available teacher query budget and the size of the seed instruction set. When this ratio is low, generating new answers to existing questions proves most effective, but as this ratio increases, generating new questions becomes optimal. Across all tasks, we find that choice of augmentation method and other design choices matter substantially more in low to mid data regimes than in high data regimes. We provide a practical framework for selecting the appropriate augmentation method across settings, taking into account additional factors such as the scalability of each method, the importance of verifying synthetic data, and the use of different LLMs for synthetic data generation.",
            "corpus_id": 272988100,
            "sentences": [
                {
                    "corpus_id": "272988100",
                    "title": "Balancing Cost and Effectiveness of Synthetic Data Generation Strategies for LLMs",
                    "text": "Synthetic Data for Fine-Tuning. Fine-tuning on synthetic and hybrid data has proven successful across a wide range of tasks (Liu et al., 2024). In the domain of mathematical reasoning, highquality instructions are scarce, so many works leverage LLM-generated synthetic data to significantly improved the math reasoning ability of small LLMs (Yu et al., 2023;Li et al., 2024;Setlur et al., 2024;Luo et al., 2023). In code generation, synthetic data from LLMs can be further verified by running test cases or the code directly, which helps close the gap between closed-source LLMs and smaller LLMs (Wei et al., 2024;Yang et al., 2024). Similar approaches have been applied in instruction-following, where LLMs are effectively trained on diverse synthetic instructions with minimal to no human supervision (Xu et al., 2024;Wang et al., 2022;Xu et al., 2023). However, most works focus on a single domain and do not explore how these techniques perform under varying data constraints and strategies, leaving uncertainty in adapting them to new applications. We aim to compare and extend these methods in cost constrained settings, while investigating which factors in synthetic data generation remain impactful across multiple tasks and data budgets. \n\nEfficient Synthetic Data Generation. Although synthetic data is significantly cheaper than real data, its scalability encourages researchers to generate it at extremely large scales, making generation costs a substantial component of fine-tuning expert models (Li et al., 2024). Other works focus on aggressively filtering synthetic datasets for diversity and correctness with custom tricks for each domain (Long et al., 2024). Current research on training LLMs with synthetic data emphasizes scalability and performance, but to make these methods more applicable to more tasks, we also need to disentangle and understand cost-efficiency across different scales. Bansal et al. (2024) explores and optimizes the choice of LLMs to sample synthetic data for overall cost reductions. Our work addresses this challenge from a new perspective by offering a general framework that guides model trainers in defining and refining their synthetic data generation strategies to maximize cost-efficiency within budget constraints.",
                    "score": 0.392628114487577,
                    "section_title": "Related Work",
                    "char_start_offset": 3318,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 31
                        },
                        {
                            "start": 32,
                            "end": 143
                        },
                        {
                            "start": 144,
                            "end": 412
                        },
                        {
                            "start": 413,
                            "end": 633
                        },
                        {
                            "start": 634,
                            "end": 855
                        },
                        {
                            "start": 856,
                            "end": 1053
                        },
                        {
                            "start": 1054,
                            "end": 1246
                        },
                        {
                            "start": 1249,
                            "end": 1285
                        },
                        {
                            "start": 1286,
                            "end": 1527
                        },
                        {
                            "start": 1528,
                            "end": 1676
                        },
                        {
                            "start": 1677,
                            "end": 1911
                        },
                        {
                            "start": 1912,
                            "end": 2028
                        },
                        {
                            "start": 2029,
                            "end": 2267
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 358,
                            "end": 374,
                            "matchedPaperCorpusId": "269930153"
                        },
                        {
                            "start": 394,
                            "end": 411,
                            "matchedPaperCorpusId": "261030818"
                        },
                        {
                            "start": 596,
                            "end": 614,
                            "matchedPaperCorpusId": "270358041"
                        },
                        {
                            "start": 1509,
                            "end": 1526,
                            "matchedPaperCorpusId": "269930153"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90283203125
                }
            ],
            "relevance_judgement": 0.90283203125,
            "relevance_judgment_input_expanded": "# Title: Balancing Cost and Effectiveness of Synthetic Data Generation Strategies for LLMs\n# Venue: arXiv.org\n# Authors: Yung-Chieh Chan, George Pu, Apaar Shanker, Parth Suresh, Penn Jenks, John Heyer, Sam Denton\n## Abstract\nAs large language models (LLMs) are applied to more use cases, creating high quality, task-specific datasets for fine-tuning becomes a bottleneck for model improvement. Using high quality human data has been the most common approach to unlock model performance, but is prohibitively expensive in many scenarios. Several alternative methods have also emerged, such as generating synthetic or hybrid data, but the effectiveness of these approaches remain unclear, especially in resource-constrained scenarios and tasks that are not easily verified. To investigate this, we group various synthetic data generation strategies into three representative categories -- Answer Augmentation, Question Rephrase and New Question -- and study the performance of student LLMs trained under various constraints, namely seed instruction set size and query budget. We demonstrate that these strategies are not equally effective across settings. Notably, the optimal data generation strategy depends strongly on the ratio between the available teacher query budget and the size of the seed instruction set. When this ratio is low, generating new answers to existing questions proves most effective, but as this ratio increases, generating new questions becomes optimal. Across all tasks, we find that choice of augmentation method and other design choices matter substantially more in low to mid data regimes than in high data regimes. We provide a practical framework for selecting the appropriate augmentation method across settings, taking into account additional factors such as the scalability of each method, the importance of verifying synthetic data, and the use of different LLMs for synthetic data generation.\n## Related Work\nSynthetic Data for Fine-Tuning. Fine-tuning on synthetic and hybrid data has proven successful across a wide range of tasks (Liu et al., 2024). In the domain of mathematical reasoning, highquality instructions are scarce, so many works leverage LLM-generated synthetic data to significantly improved the math reasoning ability of small LLMs (Yu et al., 2023;Li et al., 2024;Setlur et al., 2024;Luo et al., 2023). In code generation, synthetic data from LLMs can be further verified by running test cases or the code directly, which helps close the gap between closed-source LLMs and smaller LLMs (Wei et al., 2024;Yang et al., 2024). Similar approaches have been applied in instruction-following, where LLMs are effectively trained on diverse synthetic instructions with minimal to no human supervision (Xu et al., 2024;Wang et al., 2022;Xu et al., 2023). However, most works focus on a single domain and do not explore how these techniques perform under varying data constraints and strategies, leaving uncertainty in adapting them to new applications. We aim to compare and extend these methods in cost constrained settings, while investigating which factors in synthetic data generation remain impactful across multiple tasks and data budgets. \n\nEfficient Synthetic Data Generation. Although synthetic data is significantly cheaper than real data, its scalability encourages researchers to generate it at extremely large scales, making generation costs a substantial component of fine-tuning expert models (Li et al., 2024). Other works focus on aggressively filtering synthetic datasets for diversity and correctness with custom tricks for each domain (Long et al., 2024). Current research on training LLMs with synthetic data emphasizes scalability and performance, but to make these methods more applicable to more tasks, we also need to disentangle and understand cost-efficiency across different scales. Bansal et al. (2024) explores and optimizes the choice of LLMs to sample synthetic data for overall cost reductions. Our work addresses this challenge from a new perspective by offering a general framework that guides model trainers in defining and refining their synthetic data generation strategies to maximize cost-efficiency within budget constraints.",
            "reference_string": "[272988100 | Chan et al. | 2024 | Citations: 10]"
        },
        {
            "title": "Hybrid synthetic data generation pipeline that outperforms real data",
            "venue": "J. Electronic Imaging",
            "year": 2023,
            "reference_count": 77,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.spiedigitallibrary.org/journals/journal-of-electronic-imaging/volume-32/issue-2/023011/Hybrid-synthetic-data-generation-pipeline-that-outperforms-real-data/10.1117/1.JEI.32.2.023011.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1117/1.JEI.32.2.023011?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1117/1.JEI.32.2.023011, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "143927102",
                    "name": "S. Natarajan"
                },
                {
                    "authorId": "32418087",
                    "name": "M. G. Madden"
                }
            ],
            "abstract": "Abstract. Fine-tuning a pretrained model with real data for a machine learning task requires many hours of manual work, especially for computer vision tasks, where collection and annotation of data can be very time-consuming. We present a framework and methodology for synthetic data collection that is not only efficient in terms of time taken to collect and annotate data, making use of free- and open-source software tools and 3D assets but also beats the state-of-the-art against real data, which is the ultimate test for any similar-to-real approach. We test our approach on a set of image classes from ObjectNet, which is a challenging image classification benchmark test dataset that is designed to be similar in many respects to ImageNet but with a wider variety of viewpoints, rotations, and backgrounds, which can make it more difficult for transfer learning problems. The novelty of our approach stems from the way we create complex backgrounds for 3D models using 2D images laid out as decals in a 3D game engine, where synthetic images are captured programmatically with a large number of systematic variations. We demonstrate that our approach is highly effective, resulting in a deep learning model with a top-1 accuracy of 72% on the ObjectNet data, which is a new state-of-the-art result. In addition, we present an efficient strategy for learning rate tuning that is an order of magnitude faster than regular grid search.",
            "corpus_id": 257607791,
            "sentences": [
                {
                    "corpus_id": "257607791",
                    "title": "Hybrid synthetic data generation pipeline that outperforms real data",
                    "text": "4][15] We therefore exploit advances in open-source plugins and game engines, combined with unique techniques in synthetic data generation to perform state-of-the-art synthetic to real transfer learning, and make the following contributions. \n\n1. We present a synthetic data generation framework with an approach of introducing background complexity to synthetic images, in addition to the ability to programmatically vary rotation, lighting, backgrounds, and scale, making the resulting classifier very robust. We have made our framework publicly available (https://github.com/saiabinesh/hybrid-synth), which can be used to generate a dataset with any number of arbitrary classes. The dataset used for this work can also be downloaded to reproduce our experiments directly. 16 . We test the efficacy of the collected synthetic data on a set of classes from the challenging ObjectNet dataset 2 and demonstrate that fine-tuning with synthetic data can outperform fine-tuning with real photographs. 3. We evaluate the effect of various parameters in the synthetic data generation pipeline through ablation studies. 4. We present an efficient learning rate (LR) tuning strategy that is robust to covariate shift, helps set the LR 75\u00d7 faster and converges 10\u00d7 faster compared to regular grid search. \n\n2 Related Research",
                    "score": 0.3529039045611988,
                    "section_title": ".3.1).",
                    "char_start_offset": 1878,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 241
                        },
                        {
                            "start": 244,
                            "end": 511
                        },
                        {
                            "start": 512,
                            "end": 681
                        },
                        {
                            "start": 682,
                            "end": 777
                        },
                        {
                            "start": 778,
                            "end": 779
                        },
                        {
                            "start": 780,
                            "end": 996
                        },
                        {
                            "start": 997,
                            "end": 1112
                        },
                        {
                            "start": 1113,
                            "end": 1295
                        },
                        {
                            "start": 1298,
                            "end": 1316
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 2,
                            "end": 6,
                            "matchedPaperCorpusId": "6383532"
                        },
                        {
                            "start": 892,
                            "end": 893,
                            "matchedPaperCorpusId": "202777185"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90283203125
                }
            ],
            "relevance_judgement": 0.90283203125,
            "relevance_judgment_input_expanded": "# Title: Hybrid synthetic data generation pipeline that outperforms real data\n# Venue: J. Electronic Imaging\n# Authors: S. Natarajan, M. G. Madden\n## Abstract\nAbstract. Fine-tuning a pretrained model with real data for a machine learning task requires many hours of manual work, especially for computer vision tasks, where collection and annotation of data can be very time-consuming. We present a framework and methodology for synthetic data collection that is not only efficient in terms of time taken to collect and annotate data, making use of free- and open-source software tools and 3D assets but also beats the state-of-the-art against real data, which is the ultimate test for any similar-to-real approach. We test our approach on a set of image classes from ObjectNet, which is a challenging image classification benchmark test dataset that is designed to be similar in many respects to ImageNet but with a wider variety of viewpoints, rotations, and backgrounds, which can make it more difficult for transfer learning problems. The novelty of our approach stems from the way we create complex backgrounds for 3D models using 2D images laid out as decals in a 3D game engine, where synthetic images are captured programmatically with a large number of systematic variations. We demonstrate that our approach is highly effective, resulting in a deep learning model with a top-1 accuracy of 72% on the ObjectNet data, which is a new state-of-the-art result. In addition, we present an efficient strategy for learning rate tuning that is an order of magnitude faster than regular grid search.\n## .3.1).\n4][15] We therefore exploit advances in open-source plugins and game engines, combined with unique techniques in synthetic data generation to perform state-of-the-art synthetic to real transfer learning, and make the following contributions. \n\n1. We present a synthetic data generation framework with an approach of introducing background complexity to synthetic images, in addition to the ability to programmatically vary rotation, lighting, backgrounds, and scale, making the resulting classifier very robust. We have made our framework publicly available (https://github.com/saiabinesh/hybrid-synth), which can be used to generate a dataset with any number of arbitrary classes. The dataset used for this work can also be downloaded to reproduce our experiments directly. 16 . We test the efficacy of the collected synthetic data on a set of classes from the challenging ObjectNet dataset 2 and demonstrate that fine-tuning with synthetic data can outperform fine-tuning with real photographs. 3. We evaluate the effect of various parameters in the synthetic data generation pipeline through ablation studies. 4. We present an efficient learning rate (LR) tuning strategy that is robust to covariate shift, helps set the LR 75\u00d7 faster and converges 10\u00d7 faster compared to regular grid search. \n\n2 Related Research",
            "reference_string": "[257607791 | Natarajan et al. | 2023 | Citations: 2]"
        },
        {
            "title": "Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal Sampling",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "reference_count": 62,
            "citation_count": 49,
            "influential_citation_count": 6,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.16737, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2317010356",
                    "name": "Hritik Bansal"
                },
                {
                    "authorId": "2090537547",
                    "name": "Arian Hosseini"
                },
                {
                    "authorId": "2317013277",
                    "name": "Rishabh Agarwal"
                },
                {
                    "authorId": "2317984275",
                    "name": "Vinh Q. Tran"
                },
                {
                    "authorId": "2317010095",
                    "name": "Mehran Kazemi"
                }
            ],
            "abstract": "Training on high-quality synthetic data from strong language models (LMs) is a common strategy to improve the reasoning performance of LMs. In this work, we revisit whether this strategy is compute-optimal under a fixed inference budget (e.g., FLOPs). To do so, we investigate the trade-offs between generating synthetic data using a stronger but more expensive (SE) model versus a weaker but cheaper (WC) model. We evaluate the generated data across three key metrics: coverage, diversity, and false positive rate, and show that the data from WC models may have higher coverage and diversity, but also exhibit higher false positive rates. We then finetune LMs on data from SE and WC models in different settings: knowledge distillation, self-improvement, and a novel weak-to-strong improvement setup where a weaker LM teaches reasoning to a stronger LM. Our findings reveal that models finetuned on WC-generated data consistently outperform those trained on SE-generated data across multiple benchmarks and multiple choices of WC and SE models. These results challenge the prevailing practice of relying on SE models for synthetic data generation, suggesting that WC may be the compute-optimal approach for training advanced LM reasoners.",
            "corpus_id": 272146630,
            "sentences": [
                {
                    "corpus_id": "272146630",
                    "title": "Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal Sampling",
                    "text": "As a result, several recent works aim to understand the utility of the weak but cheaper LMs in comparison to the strong but expensive LMs for reasoning. Specifically, Brown et al. (2024); Snell et al. (2024); Song et al. (2024) show that the solve rate of the small LMs can increase significantly with repeated sampling. In addition, Hassid et al. (2024) demonstrate that repeated generations from smaller LMs can outperform the data generated by larger LMs at a fixed sampling computational budget during inference for coding tasks. In this work, we go beyond these works and show the utility of the synthetic data from the small LMs for training strong reasoners across a diverse set of supervised finetuning setups.",
                    "score": 0.397114205704469,
                    "section_title": "Related Work",
                    "char_start_offset": 41122,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 152
                        },
                        {
                            "start": 153,
                            "end": 320
                        },
                        {
                            "start": 321,
                            "end": 533
                        },
                        {
                            "start": 534,
                            "end": 718
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90234375
                }
            ],
            "relevance_judgement": 0.90234375,
            "relevance_judgment_input_expanded": "# Title: Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal Sampling\n# Venue: International Conference on Learning Representations\n# Authors: Hritik Bansal, Arian Hosseini, Rishabh Agarwal, Vinh Q. Tran, Mehran Kazemi\n## Abstract\nTraining on high-quality synthetic data from strong language models (LMs) is a common strategy to improve the reasoning performance of LMs. In this work, we revisit whether this strategy is compute-optimal under a fixed inference budget (e.g., FLOPs). To do so, we investigate the trade-offs between generating synthetic data using a stronger but more expensive (SE) model versus a weaker but cheaper (WC) model. We evaluate the generated data across three key metrics: coverage, diversity, and false positive rate, and show that the data from WC models may have higher coverage and diversity, but also exhibit higher false positive rates. We then finetune LMs on data from SE and WC models in different settings: knowledge distillation, self-improvement, and a novel weak-to-strong improvement setup where a weaker LM teaches reasoning to a stronger LM. Our findings reveal that models finetuned on WC-generated data consistently outperform those trained on SE-generated data across multiple benchmarks and multiple choices of WC and SE models. These results challenge the prevailing practice of relying on SE models for synthetic data generation, suggesting that WC may be the compute-optimal approach for training advanced LM reasoners.\n## Related Work\nAs a result, several recent works aim to understand the utility of the weak but cheaper LMs in comparison to the strong but expensive LMs for reasoning. Specifically, Brown et al. (2024); Snell et al. (2024); Song et al. (2024) show that the solve rate of the small LMs can increase significantly with repeated sampling. In addition, Hassid et al. (2024) demonstrate that repeated generations from smaller LMs can outperform the data generated by larger LMs at a fixed sampling computational budget during inference for coding tasks. In this work, we go beyond these works and show the utility of the synthetic data from the small LMs for training strong reasoners across a diverse set of supervised finetuning setups.",
            "reference_string": "[272146630 | Bansal et al. | 2024 | Citations: 49]"
        },
        {
            "title": "OpenCodeReasoning: Advancing Data Distillation for Competitive Coding",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 53,
            "citation_count": 13,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.01943, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "38123220",
                    "name": "Wasi Uddin Ahmad"
                },
                {
                    "authorId": "2135333375",
                    "name": "Sean Narenthiran"
                },
                {
                    "authorId": "9099952",
                    "name": "Somshubra Majumdar"
                },
                {
                    "authorId": "2186740325",
                    "name": "Aleksander Ficek"
                },
                {
                    "authorId": "2353320260",
                    "name": "Siddhartha Jain"
                },
                {
                    "authorId": "1390669078",
                    "name": "Jocelyn Huang"
                },
                {
                    "authorId": "2353998",
                    "name": "V. Noroozi"
                },
                {
                    "authorId": "2353271013",
                    "name": "Boris Ginsburg"
                }
            ],
            "abstract": "Since the advent of reasoning-based large language models, many have found great success from distilling reasoning capabilities into student models. Such techniques have significantly bridged the gap between reasoning and standard LLMs on coding tasks. Despite this, much of the progress on distilling reasoning models remains locked behind proprietary datasets or lacks details on data curation, filtering and subsequent training. To address this, we construct a superior supervised fine-tuning (SFT) dataset that we use to achieve state-of-the-art coding capability results in models of various sizes. Our distilled models use only SFT to achieve 61.8% on LiveCodeBench and 24.6% on CodeContests, surpassing alternatives trained with reinforcement learning. We then perform analysis on the data sources used to construct our dataset, the impact of code execution filtering, and the importance of instruction/solution diversity. We observe that execution filtering negatively affected benchmark accuracy, leading us to prioritize instruction diversity over solution correctness. Finally, we also analyze the token efficiency and reasoning patterns utilized by these models. We will open-source these datasets and distilled models to the community.",
            "corpus_id": 277502040,
            "sentences": [
                {
                    "corpus_id": "277502040",
                    "title": "OpenCodeReasoning: Advancing Data Distillation for Competitive Coding",
                    "text": "Large Language Models (LLMs) have demonstrated a remarkable capacity to excel at a variety of coding capabilities (Hui et al., 2024;Li et al., 2023a;Guo et al., 2024;Roziere et al., 2023;Ahmad et al., 2021). While fine-tuning on code question-solution pairs has led to much of the improved performance, high-quality human-labeled data is limited and expensive to curate. To overcome this bottleneck, many have successfully leveraged LLMs to generate high-quality synthetic code data (Luo et al., 2024;Yu et al., 2024). Notably, works like Wei et al. (2024a) and Huang et al. (2025) have generated diverse instruction-solution pairs, subsequently fine-tuning base models to achieve top results in HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021) and BigCodeBench (Zhuo et al., 2025) benchmarks. \n\nSince the successes of synthetic data generation for code, reasoning-based LLMs have presented the next paradigm in advancing large language model capabilities (Team et al., 2025). Past works such as DeepSeek-AI et al. (2025) have led the way in improving LLM capabilities on reasoning oriented tasks such as math and coding by leveraging large-scale reinforcement learning (Luo et al., 2025) and rule-based reward models. By continuously applying reinforcement learning (RL) with a ground truth verifier on a task such as coding, models learn to apply continuous test-time computation to solve more difficult reasoningbased tasks (Hosseini et al., 2024;Setlur et al., 2025). \n\nGiven the improvements from fine-tuning on synthetic data and reasoning capabilities for coding capabilities, many works have found continuous enhancements by combining the two. This has involved distilling the chain-of-thought responses (Wei et al., 2022) from better reasoning models to smaller student models by means of supervised fine-tuning (SFT).",
                    "score": 0.40639484920604296,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 207
                        },
                        {
                            "start": 208,
                            "end": 370
                        },
                        {
                            "start": 371,
                            "end": 518
                        },
                        {
                            "start": 519,
                            "end": 802
                        },
                        {
                            "start": 805,
                            "end": 985
                        },
                        {
                            "start": 986,
                            "end": 1227
                        },
                        {
                            "start": 1228,
                            "end": 1480
                        },
                        {
                            "start": 1483,
                            "end": 1660
                        },
                        {
                            "start": 1661,
                            "end": 1836
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 187,
                            "end": 206,
                            "matchedPaperCorpusId": "232185260"
                        },
                        {
                            "start": 483,
                            "end": 501,
                            "matchedPaperCorpusId": "259164815"
                        },
                        {
                            "start": 501,
                            "end": 517,
                            "matchedPaperCorpusId": "270258158"
                        },
                        {
                            "start": 1436,
                            "end": 1459,
                            "matchedPaperCorpusId": "267617275"
                        },
                        {
                            "start": 1459,
                            "end": 1479,
                            "matchedPaperCorpusId": "276422443"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.900390625
                }
            ],
            "relevance_judgement": 0.900390625,
            "relevance_judgment_input_expanded": "# Title: OpenCodeReasoning: Advancing Data Distillation for Competitive Coding\n# Venue: arXiv.org\n# Authors: Wasi Uddin Ahmad, Sean Narenthiran, Somshubra Majumdar, Aleksander Ficek, Siddhartha Jain, Jocelyn Huang, V. Noroozi, Boris Ginsburg\n## Abstract\nSince the advent of reasoning-based large language models, many have found great success from distilling reasoning capabilities into student models. Such techniques have significantly bridged the gap between reasoning and standard LLMs on coding tasks. Despite this, much of the progress on distilling reasoning models remains locked behind proprietary datasets or lacks details on data curation, filtering and subsequent training. To address this, we construct a superior supervised fine-tuning (SFT) dataset that we use to achieve state-of-the-art coding capability results in models of various sizes. Our distilled models use only SFT to achieve 61.8% on LiveCodeBench and 24.6% on CodeContests, surpassing alternatives trained with reinforcement learning. We then perform analysis on the data sources used to construct our dataset, the impact of code execution filtering, and the importance of instruction/solution diversity. We observe that execution filtering negatively affected benchmark accuracy, leading us to prioritize instruction diversity over solution correctness. Finally, we also analyze the token efficiency and reasoning patterns utilized by these models. We will open-source these datasets and distilled models to the community.\n## Introduction\nLarge Language Models (LLMs) have demonstrated a remarkable capacity to excel at a variety of coding capabilities (Hui et al., 2024;Li et al., 2023a;Guo et al., 2024;Roziere et al., 2023;Ahmad et al., 2021). While fine-tuning on code question-solution pairs has led to much of the improved performance, high-quality human-labeled data is limited and expensive to curate. To overcome this bottleneck, many have successfully leveraged LLMs to generate high-quality synthetic code data (Luo et al., 2024;Yu et al., 2024). Notably, works like Wei et al. (2024a) and Huang et al. (2025) have generated diverse instruction-solution pairs, subsequently fine-tuning base models to achieve top results in HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021) and BigCodeBench (Zhuo et al., 2025) benchmarks. \n\nSince the successes of synthetic data generation for code, reasoning-based LLMs have presented the next paradigm in advancing large language model capabilities (Team et al., 2025). Past works such as DeepSeek-AI et al. (2025) have led the way in improving LLM capabilities on reasoning oriented tasks such as math and coding by leveraging large-scale reinforcement learning (Luo et al., 2025) and rule-based reward models. By continuously applying reinforcement learning (RL) with a ground truth verifier on a task such as coding, models learn to apply continuous test-time computation to solve more difficult reasoningbased tasks (Hosseini et al., 2024;Setlur et al., 2025). \n\nGiven the improvements from fine-tuning on synthetic data and reasoning capabilities for coding capabilities, many works have found continuous enhancements by combining the two. This has involved distilling the chain-of-thought responses (Wei et al., 2022) from better reasoning models to smaller student models by means of supervised fine-tuning (SFT).",
            "reference_string": "[277502040 | Ahmad et al. | 2025 | Citations: 13]"
        },
        {
            "title": "Noise-Aware Statistical Inference with Differentially Private Synthetic Data",
            "venue": "International Conference on Artificial Intelligence and Statistics",
            "year": 2022,
            "reference_count": 71,
            "citation_count": 11,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2205.14485",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2205.14485, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2117329684",
                    "name": "Ossi R\u00e4is\u00e4"
                },
                {
                    "authorId": "7804135",
                    "name": "Joonas J\u00e4lk\u00f6"
                },
                {
                    "authorId": "69057684",
                    "name": "S. Kaski"
                },
                {
                    "authorId": "145766200",
                    "name": "Antti Honkela"
                }
            ],
            "abstract": "While generation of synthetic data under differential privacy (DP) has received a lot of attention in the data privacy community, analysis of synthetic data has received much less. Existing work has shown that simply analysing DP synthetic data as if it were real does not produce valid inferences of population-level quantities. For example, confidence intervals become too narrow, which we demonstrate with a simple experiment. We tackle this problem by combining synthetic data analysis techniques from the field of multiple imputation (MI), and synthetic data generation using noise-aware (NA) Bayesian modeling into a pipeline NA+MI that allows computing accurate uncertainty estimates for population-level quantities from DP synthetic data. To implement NA+MI for discrete data generation using the values of marginal queries, we develop a novel noise-aware synthetic data generation algorithm NAPSU-MQ using the principle of maximum entropy. Our experiments demonstrate that the pipeline is able to produce accurate confidence intervals from DP synthetic data. The intervals become wider with tighter privacy to accurately capture the additional uncertainty stemming from DP noise.",
            "corpus_id": 249191979,
            "sentences": [
                {
                    "corpus_id": "249191979",
                    "title": "Noise-Aware Statistical Inference with Differentially Private Synthetic Data",
                    "text": "There is a sizable literature on DP synthetic data generation. Most recent work in the area either releases the values of a set of simple queries, such as counting queries, under DP and uses them as the basis of synthetic data (Aydore et al. 2021;Bernstein et al. 2017;Cai et al. 2021;R. Chen et al. 2015;Hardt et al. 2012;T. Liu et al. 2021;McKenna et al. 2018;McKenna et al. 2021;McKenna et al. 2022;McKenna et al. 2019;Nixon et al. 2022;Vietri et al. 2020;Zhang et al. 2017), or trains some kind of generative model, often a GAN, using the whole real dataset under DP (D. Chen et al. 2020;J\u00e4lk\u00f6 et al. 2021;Long et al. 2021;Xie et al. 2018;Yoon et al. 2019). There are also hybrid approaches that use sophisticated queries that can capture all features of the dataset, and train a generative model using those (Harder et al. 2021;Liew et al. 2022). Of the existing DP synthetic data generation algorithms, NAPSU-MQ is closest to the PGM algorithm (McKenna et al. 2019), which does maximum likelihood estimation with the same data model as NAPSU-MQ instead of noiseaware Bayesian inference. We describe this connection in more detail in Supplemental Section D. \n\nRubin's rules were originally developed for analyses on missing data, as part of an approach called multiple imputation (Rubin 1987), which was later applied to generate and analyse synthetic data (Rubin 1993) without DP. The variant of Rubin's rules that we use, and describe in Supplemental Section B, was developed specifically for synthetic data generation (Raghunathan et al. 2003;Reiter 2002). Raab et al. (2018) have developed simpler alternatives to Rubin's rules under more restrictive assumptions, but these assumptions rule out DP data synthesisers. \n\nRubin's rules have not been widely used with DP synthetic data generation, and we are only aware of four existing works studying the combination.",
                    "score": 0.35383724562520563,
                    "section_title": "Related Work",
                    "char_start_offset": 4814,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 62
                        },
                        {
                            "start": 63,
                            "end": 661
                        },
                        {
                            "start": 662,
                            "end": 851
                        },
                        {
                            "start": 852,
                            "end": 1092
                        },
                        {
                            "start": 1093,
                            "end": 1162
                        },
                        {
                            "start": 1165,
                            "end": 1386
                        },
                        {
                            "start": 1387,
                            "end": 1564
                        },
                        {
                            "start": 1565,
                            "end": 1725
                        },
                        {
                            "start": 1728,
                            "end": 1873
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 227,
                            "end": 247,
                            "matchedPaperCorpusId": "232185574"
                        },
                        {
                            "start": 247,
                            "end": 269,
                            "matchedPaperCorpusId": "2334513"
                        },
                        {
                            "start": 269,
                            "end": 285,
                            "matchedPaperCorpusId": "237012543"
                        },
                        {
                            "start": 285,
                            "end": 305,
                            "matchedPaperCorpusId": "15417421"
                        },
                        {
                            "start": 305,
                            "end": 323,
                            "matchedPaperCorpusId": "947901"
                        },
                        {
                            "start": 323,
                            "end": 342,
                            "matchedPaperCorpusId": "235421632"
                        },
                        {
                            "start": 342,
                            "end": 362,
                            "matchedPaperCorpusId": "51869463"
                        },
                        {
                            "start": 362,
                            "end": 382,
                            "matchedPaperCorpusId": "236976348"
                        },
                        {
                            "start": 382,
                            "end": 402,
                            "matchedPaperCorpusId": "246430835"
                        },
                        {
                            "start": 402,
                            "end": 422,
                            "matchedPaperCorpusId": "59317012"
                        },
                        {
                            "start": 422,
                            "end": 440,
                            "matchedPaperCorpusId": "246275811"
                        },
                        {
                            "start": 440,
                            "end": 459,
                            "matchedPaperCorpusId": "211203580"
                        },
                        {
                            "start": 459,
                            "end": 477,
                            "matchedPaperCorpusId": "13095103"
                        },
                        {
                            "start": 575,
                            "end": 592,
                            "matchedPaperCorpusId": "219687936"
                        },
                        {
                            "start": 610,
                            "end": 627,
                            "matchedPaperCorpusId": "245634703"
                        },
                        {
                            "start": 643,
                            "end": 660,
                            "matchedPaperCorpusId": "53342261"
                        },
                        {
                            "start": 813,
                            "end": 833,
                            "matchedPaperCorpusId": "225077562"
                        },
                        {
                            "start": 833,
                            "end": 850,
                            "matchedPaperCorpusId": "235377198"
                        },
                        {
                            "start": 950,
                            "end": 971,
                            "matchedPaperCorpusId": "59317012"
                        },
                        {
                            "start": 1285,
                            "end": 1297,
                            "matchedPaperCorpusId": "63858430"
                        },
                        {
                            "start": 1526,
                            "end": 1551,
                            "matchedPaperCorpusId": "118255787"
                        },
                        {
                            "start": 1551,
                            "end": 1563,
                            "matchedPaperCorpusId": "26911794"
                        },
                        {
                            "start": 1565,
                            "end": 1583,
                            "matchedPaperCorpusId": "36523940"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.900390625
                }
            ],
            "relevance_judgement": 0.900390625,
            "relevance_judgment_input_expanded": "# Title: Noise-Aware Statistical Inference with Differentially Private Synthetic Data\n# Venue: International Conference on Artificial Intelligence and Statistics\n# Authors: Ossi R\u00e4is\u00e4, Joonas J\u00e4lk\u00f6, S. Kaski, Antti Honkela\n## Abstract\nWhile generation of synthetic data under differential privacy (DP) has received a lot of attention in the data privacy community, analysis of synthetic data has received much less. Existing work has shown that simply analysing DP synthetic data as if it were real does not produce valid inferences of population-level quantities. For example, confidence intervals become too narrow, which we demonstrate with a simple experiment. We tackle this problem by combining synthetic data analysis techniques from the field of multiple imputation (MI), and synthetic data generation using noise-aware (NA) Bayesian modeling into a pipeline NA+MI that allows computing accurate uncertainty estimates for population-level quantities from DP synthetic data. To implement NA+MI for discrete data generation using the values of marginal queries, we develop a novel noise-aware synthetic data generation algorithm NAPSU-MQ using the principle of maximum entropy. Our experiments demonstrate that the pipeline is able to produce accurate confidence intervals from DP synthetic data. The intervals become wider with tighter privacy to accurately capture the additional uncertainty stemming from DP noise.\n## Related Work\nThere is a sizable literature on DP synthetic data generation. Most recent work in the area either releases the values of a set of simple queries, such as counting queries, under DP and uses them as the basis of synthetic data (Aydore et al. 2021;Bernstein et al. 2017;Cai et al. 2021;R. Chen et al. 2015;Hardt et al. 2012;T. Liu et al. 2021;McKenna et al. 2018;McKenna et al. 2021;McKenna et al. 2022;McKenna et al. 2019;Nixon et al. 2022;Vietri et al. 2020;Zhang et al. 2017), or trains some kind of generative model, often a GAN, using the whole real dataset under DP (D. Chen et al. 2020;J\u00e4lk\u00f6 et al. 2021;Long et al. 2021;Xie et al. 2018;Yoon et al. 2019). There are also hybrid approaches that use sophisticated queries that can capture all features of the dataset, and train a generative model using those (Harder et al. 2021;Liew et al. 2022). Of the existing DP synthetic data generation algorithms, NAPSU-MQ is closest to the PGM algorithm (McKenna et al. 2019), which does maximum likelihood estimation with the same data model as NAPSU-MQ instead of noiseaware Bayesian inference. We describe this connection in more detail in Supplemental Section D. \n\nRubin's rules were originally developed for analyses on missing data, as part of an approach called multiple imputation (Rubin 1987), which was later applied to generate and analyse synthetic data (Rubin 1993) without DP. The variant of Rubin's rules that we use, and describe in Supplemental Section B, was developed specifically for synthetic data generation (Raghunathan et al. 2003;Reiter 2002). Raab et al. (2018) have developed simpler alternatives to Rubin's rules under more restrictive assumptions, but these assumptions rule out DP data synthesisers. \n\nRubin's rules have not been widely used with DP synthetic data generation, and we are only aware of four existing works studying the combination.",
            "reference_string": "[249191979 | Raisa et al. | 2022 | Citations: 11]"
        },
        {
            "title": "Advancing Process Verification for Large Language Models via Tree-Based Preference Learning",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "reference_count": 41,
            "citation_count": 7,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.00390, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2309218600",
                    "name": "Mingqian He"
                },
                {
                    "authorId": "1471660296",
                    "name": "Yongliang Shen"
                },
                {
                    "authorId": "2135282890",
                    "name": "Wenqi Zhang"
                },
                {
                    "authorId": "2092670555",
                    "name": "Zeqi Tan"
                },
                {
                    "authorId": "1776903",
                    "name": "Weiming Lu"
                }
            ],
            "abstract": "Large Language Models (LLMs) have demonstrated remarkable potential in handling complex reasoning tasks by generating step-by-step rationales. Some methods have proven effective in boosting accuracy by introducing extra verifiers to assess these paths. However, existing verifiers, typically trained on binary-labeled reasoning paths, fail to fully utilize the relative merits of intermediate steps, thereby limiting the effectiveness of the feedback provided. To overcome this limitation, we propose Tree-based Preference Learning Verifier (Tree-PLV), a novel approach that constructs reasoning trees via a best-first search algorithm and collects step-level paired data for preference training. Compared to traditional binary classification, step-level preferences more finely capture the nuances between reasoning steps, allowing for a more precise evaluation of the complete reasoning path. We empirically evaluate Tree-PLV across a range of arithmetic and commonsense reasoning tasks, where it significantly outperforms existing benchmarks. For instance, Tree-PLV achieved substantial performance gains over the Mistral-7B self-consistency baseline on GSM8K (67.55% \u2192 82.79%), MATH (17.00% \u2192 26.80%), CSQA (68.14% \u2192 72.97%), and StrategyQA (82.86% \u2192 83.25%). Additionally, our study explores the appropriate granularity for applying preference learning, revealing that step-level guidance provides feedback that better aligns with the evaluation of the reasoning process.",
            "corpus_id": 270870071,
            "sentences": [
                {
                    "corpus_id": "270870071",
                    "title": "Advancing Process Verification for Large Language Models via Tree-Based Preference Learning",
                    "text": "Multi-step Reasoning Challenging reasoning tasks have spurred innovative research in large language models (LLMs), which are essential for handling complex queries (Kaddour et al., 2023;Lightman et al., 2023;Huang et al., 2023).A prominent strategy employed is the Chain-of-Thought (CoT) prompting technique (Wei et al., 2022), along with its derivatives (Kojima et al., 2022;Wang et al., 2022;Yao et al., 2024).These methods decompose the reasoning process into sequential steps, systematically approaching problem-solving by mimicking human-like reasoning.To further enhance the accuracy of these intermediate steps, recent studies leverage extensive synthetic datasets, which are either distilled from cutting-edge models (Yu et al., 2023b;Luo et al., 2023) or composed of selfgenerated rationales (Zelikman et al., 2022;Yuan et al., 2023;Ni et al., 2022), to fine-tune the LLMs.\n\nSuch training effectively sharpens the models' ability to produce CoT reasoning that leads to correct answers.\n\nIn addition, there is an increasing emphasis on test-time verification, a process that generates multiple solutions and ranks them through a separate verifier (Cobbe et al., 2021) to select the most accurate one.The DIVERSE framework (Li et al., 2022) demonstrates the effectiveness of employing a variety of CoT prompts in conjunction with a verifier to tackle reasoning challenges.CoRe (Zhu et al., 2022) implements a dual-process system by finetuning both the generator and the verifier, thereby enhancing the performance of LLMs on math word problems.",
                    "score": 0.393129940397938,
                    "section_title": "A Related Work",
                    "char_start_offset": 26179,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 228
                        },
                        {
                            "start": 228,
                            "end": 412
                        },
                        {
                            "start": 412,
                            "end": 558
                        },
                        {
                            "start": 558,
                            "end": 882
                        },
                        {
                            "start": 884,
                            "end": 994
                        },
                        {
                            "start": 996,
                            "end": 1208
                        },
                        {
                            "start": 1208,
                            "end": 1379
                        },
                        {
                            "start": 1379,
                            "end": 1551
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.89599609375
                }
            ],
            "relevance_judgement": 0.89599609375,
            "relevance_judgment_input_expanded": "# Title: Advancing Process Verification for Large Language Models via Tree-Based Preference Learning\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Mingqian He, Yongliang Shen, Wenqi Zhang, Zeqi Tan, Weiming Lu\n## Abstract\nLarge Language Models (LLMs) have demonstrated remarkable potential in handling complex reasoning tasks by generating step-by-step rationales. Some methods have proven effective in boosting accuracy by introducing extra verifiers to assess these paths. However, existing verifiers, typically trained on binary-labeled reasoning paths, fail to fully utilize the relative merits of intermediate steps, thereby limiting the effectiveness of the feedback provided. To overcome this limitation, we propose Tree-based Preference Learning Verifier (Tree-PLV), a novel approach that constructs reasoning trees via a best-first search algorithm and collects step-level paired data for preference training. Compared to traditional binary classification, step-level preferences more finely capture the nuances between reasoning steps, allowing for a more precise evaluation of the complete reasoning path. We empirically evaluate Tree-PLV across a range of arithmetic and commonsense reasoning tasks, where it significantly outperforms existing benchmarks. For instance, Tree-PLV achieved substantial performance gains over the Mistral-7B self-consistency baseline on GSM8K (67.55% \u2192 82.79%), MATH (17.00% \u2192 26.80%), CSQA (68.14% \u2192 72.97%), and StrategyQA (82.86% \u2192 83.25%). Additionally, our study explores the appropriate granularity for applying preference learning, revealing that step-level guidance provides feedback that better aligns with the evaluation of the reasoning process.\n## A Related Work\nMulti-step Reasoning Challenging reasoning tasks have spurred innovative research in large language models (LLMs), which are essential for handling complex queries (Kaddour et al., 2023;Lightman et al., 2023;Huang et al., 2023).A prominent strategy employed is the Chain-of-Thought (CoT) prompting technique (Wei et al., 2022), along with its derivatives (Kojima et al., 2022;Wang et al., 2022;Yao et al., 2024).These methods decompose the reasoning process into sequential steps, systematically approaching problem-solving by mimicking human-like reasoning.To further enhance the accuracy of these intermediate steps, recent studies leverage extensive synthetic datasets, which are either distilled from cutting-edge models (Yu et al., 2023b;Luo et al., 2023) or composed of selfgenerated rationales (Zelikman et al., 2022;Yuan et al., 2023;Ni et al., 2022), to fine-tune the LLMs.\n\nSuch training effectively sharpens the models' ability to produce CoT reasoning that leads to correct answers.\n\nIn addition, there is an increasing emphasis on test-time verification, a process that generates multiple solutions and ranks them through a separate verifier (Cobbe et al., 2021) to select the most accurate one.The DIVERSE framework (Li et al., 2022) demonstrates the effectiveness of employing a variety of CoT prompts in conjunction with a verifier to tackle reasoning challenges.CoRe (Zhu et al., 2022) implements a dual-process system by finetuning both the generator and the verifier, thereby enhancing the performance of LLMs on math word problems.",
            "reference_string": "[270870071 | He et al. | 2024 | Citations: 7]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "276250072",
            "title": "GSM-Infinite: How Do Your LLMs Behave over Infinitely Increasing Context Length and Reasoning Complexity?",
            "text": "In this section, we present key techniques that enable the synthetic dataset to be diverse in operations in Section 4.1, LLM-understandable, and enable the evaluation to be free from non-reasoning factors in Section 4.2. Then, we present synthetic problem generators capable of generating grade-school math questions with arbitrary reasoning difficulty and context length. Thus, we generate a suite of benchmarks called GSM-\u221e detailed in Section 4.3. \n\n4.1 Challenge 1: How to Generate Implicit \u2212 and \u00f7 Operations? \n\nFirstly, we review why the abstract-instance construct can only generate \"+\" and \u00d7 but not \"\u2212\" and \u00f7. The key limitation of Ye et al. (2024a) abstract parameters and instance parameter design is that it is only able to generate problems with solutions with the \"forward\" and constructive ordering. Shown in Figure 6 (a) and (b), the design dictates that the specific and detailed variables should be defined before a more abstract variable. \n\nFor example, \"the number of Lions in Zoo\" and \"the number of Monkeys in Zoo\" have to be defined before \"Total Animal in Zoo\" is defined. The \"forward\" ordering leads to the inability to generate implicit '-' operations for 2-entity problems and implicit \"\u00f7\" operations for 3-entity problems that require the more abstract variables, e.g. \"Total Animal in Zoo\", to be defined before a more specific variable, e.g. \"the number of Monkeys in Zoo\". \n\nTo generate all four kinds of implicit operations, we introduce a \"reverse mode\" to generate the computation graph. Essentially, the graph construction still continues as before: starting with specific detailed variables and growing to incorporate more abstract variables. When it completes and we know all the values of nodes in the graph, we then randomly mask out specific initial low-level variables and force the solution to traverse in the reverse direction as in the \"forward\" ordering. We present the illustration of data generation in Figures 6 (a) and (b) for the 2-entity and 3-entity, respectively. However, for 3-entity problems, it can result in quadratic equations leading to multiple possible solutions. We develop some techniques that effectively reduce the probability of the situation. Details are presented in Appendix C.3 of the Appendix.",
            "score": 0.4812093544952659,
            "section_title": "GSM-\u221e",
            "char_start_offset": 16378,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 450
                },
                {
                    "start": 453,
                    "end": 514
                },
                {
                    "start": 517,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 957
                },
                {
                    "start": 960,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1372
                },
                {
                    "start": 1373,
                    "end": 1404
                },
                {
                    "start": 1407,
                    "end": 1522
                },
                {
                    "start": 1523,
                    "end": 1679
                },
                {
                    "start": 1680,
                    "end": 1900
                },
                {
                    "start": 1901,
                    "end": 2017
                },
                {
                    "start": 2018,
                    "end": 2126
                },
                {
                    "start": 2127,
                    "end": 2211
                },
                {
                    "start": 2212,
                    "end": 2266
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79248046875
        },
        {
            "corpus_id": "270688337",
            "title": "On LLMs-Driven Synthetic Data Generation, Curation, and Evaluation: A Survey",
            "text": "Current multi-step generation algorithms depend on the model's understanding of task requirements, requiring it to perform complex logical reasoning with limited information.However, in realworld complex scenarios, this limited information may not adequately support effective decisionmaking.For instance, the generation of mathematical problem-solution pairs entails multiple reasoning steps and may necessitate the utilization of calculator tools for validation.To date, there remains a lack of systematic investigation on how to activate the reasoning and planning capabilities of LLMs for autonomous synthetic data generation.Inspired by prevalent LLMs-based agents like HuggingGPT (Shen et al., 2023) and MetaGPT (Hong et al., 2023), we believe it would also be quite valuable to develop a data generation agent for industrial applications.",
            "score": 0.4745883375529401,
            "section_title": "Complex Task Decomposition",
            "char_start_offset": 26785,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 174,
                    "end": 292
                },
                {
                    "start": 292,
                    "end": 464
                },
                {
                    "start": 464,
                    "end": 630
                },
                {
                    "start": 630,
                    "end": 845
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74951171875
        },
        {
            "corpus_id": "276617498",
            "title": "General Intelligence Requires Reward-based Pretraining",
            "text": "Retrieval-Augmented Generation (RAG) (Berges et al., 2024) provide static memory augmentation but lack dynamism. Neural Turing Machines (NTMs) (Ale\u0161, 2016) explored differentiable memory architectures but face optimization challenges due to the backpropagation through time. Recent innovations (Berges et al., 2024) embed memory as a persistent KV-cache into the model's weights, but because the memory is intricately tied to the models' representations, it becomes difficult to add new knowledge without retraining. Jin et al. (2024) proposed a novel framework using trainable \u2329memory\u232a and \u2329reason\u232a tokens to separate knowledge retrieval from reasoning, addressing key challenges in LLMs such as hallucinations and logical inconsistencies. However, this method still ties knowledge with reasoning in a single model. \n\nSynthetic Data. Most current efforts (Chen et al., 2024;HuggingFace, 2024;Microsoft, 2024) focus on generating synthetic data directly in natural language using pretrained models. In contrast, there is a less explored avenue that involves using simple, symbolic synthetic datasets for pretraining (Wu et al., 2022;2021;Krishna et al., 2021). These approaches typically depend on manually crafted rules (e.g., Set, Identity) to generate data. On top of this lack of scalability, they often lack the complexity required for emergent behavior, limiting their ability to support more complex structures or patterns. The simplicity of these rules can restrict the diversity and depth of the learned priors. \n\nIn other various domains, synthetic data has also shown promise. Studies in vision tasks (Wang et al., 2023b;Baradad et al., 2022) and RL (Baradad et al., 2023;Wang et al., 2024b) demonstrate the effectiveness of synthetic data derived from random Markov chains and noise-based images.",
            "score": 0.47314738006545437,
            "section_title": "Memory Architectures. Current memory approaches like",
            "char_start_offset": 40446,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 816
                },
                {
                    "start": 819,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 998
                },
                {
                    "start": 999,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1260
                },
                {
                    "start": 1261,
                    "end": 1430
                },
                {
                    "start": 1431,
                    "end": 1520
                },
                {
                    "start": 1523,
                    "end": 1587
                },
                {
                    "start": 1588,
                    "end": 1808
                }
            ],
            "ref_mentions": [
                {
                    "start": 1116,
                    "end": 1133,
                    "matchedPaperCorpusId": "249890002"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74951171875
        },
        {
            "corpus_id": "270559271",
            "title": "A Peek into Token Bias: Large Language Models Are Not Yet Genuine Reasoners",
            "text": "Our framework is summarized in Figure 2. This general framework is grounded on the premise that for a given reasoning task, a capable reasoning agent will consistently reach the same conclusion regardless of how the task is framed, as long as the underlying logic remains the same (Hastie and Dawes, 2009). This assumption lays the foundation of our null hypothesis, H 0 . In our setup, if an agent consistently applies reasoning in its decisionmaking process, the only source of failure should be the procedural mistakes during the agent's abstract reasoning steps, which we assume to come up in an i.i.d. fashion. Our general framework contains three major parts as follows. \n\nSynthetic Data Generation Once the underlying logic of a reasoning task is defined, we create an algorithm to generate a synthetic dataset with n samples. While it is helpful to leverage LLMs for linguistic coherence in the process, the data generation should be carefully controlled, utilizing information from real-world data or established datasets to mitigate potential biases from purely AI-generated texts. The process begins with the creation of a curated list of entities, encompassing diverse names, genders, ages, occupations, cultural backgrounds, and events where applicable, along with a textual template that dictates the structure of the task description. By sampling from this list, we generate task descriptions that maintain the integrity and novelty of the dataset. This method ensures that while the LLM of interest might be familiar with the individual entities, it has never seen the specific combinations of these entities and narratives, thus bypassing data contamination. \n\nThe following example illustrates one approach we leverage to generate synthetic conjunction fallacy questions. We randomly sample a commonsense story curated by Mostafazadeh et al. (2016) and convert it into the following prompt: Your task is to complete the last sentence of the following problem to create a conjunction fallacy quiz: \n\nMichelle was extremely hungry. She opened the refrigerator to find nothing. Which is more likely? (a) Michelle would likely buy food at the grocery store. \n\n(b) Michelle would likely buy food at the grocery store because \n\nWe expect the LLM to complete the story by providing us with a plausible reason after \"because\", such as \"she found nothing to eat at home\".",
            "score": 0.4682275677436876,
            "section_title": "The General Framework",
            "char_start_offset": 5409,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 40
                },
                {
                    "start": 41,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 676
                },
                {
                    "start": 679,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1349
                },
                {
                    "start": 1350,
                    "end": 1463
                },
                {
                    "start": 1464,
                    "end": 1675
                },
                {
                    "start": 1678,
                    "end": 1789
                },
                {
                    "start": 1790,
                    "end": 2014
                },
                {
                    "start": 2017,
                    "end": 2047
                },
                {
                    "start": 2048,
                    "end": 2092
                },
                {
                    "start": 2093,
                    "end": 2114
                },
                {
                    "start": 2115,
                    "end": 2171
                },
                {
                    "start": 2174,
                    "end": 2237
                },
                {
                    "start": 2240,
                    "end": 2380
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6923828125
        },
        {
            "corpus_id": "271088512",
            "title": "Teaching Transformers Causal Reasoning through Axiomatic Training",
            "text": "To understand how transformers can be optimized for learning through axiomatic training and generalizing to unseen larger causal structures, we also examine different types of positional encoding such as no positional encoding (PE), Learnable PEs (Radford et al., 2018) and Sinusoidal PEs (Vaswani et al., 2023). \n\nSynthetic data generation for teaching transformers reasoning: Synthetic data generation has been explored for optimising model training for reasoning. For example, (Li et al., 2023;Gunasekar et al., 2023) use LLM-generated synthetic text for training Phi-1 and Phi-1.5 models and show impressive performance for reasoning-based tasks. (Trinh et al., 2024) introduce a novel neuro-symbolic framework to pre-train a transformer model for Olympiad-level math problems. (Morishita et al., 2024) construct synthetic training data to improve language models' performance on logical reasoning tasks. Building on this stream of work, we apply synthetic data generation for teaching causal reasoning.",
            "score": 0.4628996966311564,
            "section_title": "Related Work",
            "char_start_offset": 7807,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 312
                },
                {
                    "start": 315,
                    "end": 466
                },
                {
                    "start": 467,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 1007
                }
            ],
            "ref_mentions": [
                {
                    "start": 651,
                    "end": 671,
                    "matchedPaperCorpusId": "267032902"
                },
                {
                    "start": 782,
                    "end": 806,
                    "matchedPaperCorpusId": "274141027"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9267578125
        },
        {
            "corpus_id": "272753174",
            "title": "LogicPro: Improving Complex Logical Reasoning via Program-Guided Learning",
            "text": "Synthetic data has played an important role in enhancing LLMs' reasoning capabilities (Dubey et al., 2024), especially in mathematics and coding domains. \n\nFor mathematical reasoning, synthetic data generation includes problem-driven methods such as evol-instruct (Luo et al., 2023a;Zeng et al., 2024), problem restatement (Yu et al., 2023), backtranslation (Lu et al., 2024), and few-shot examples (Dalvi et al., 2021), as well as knowledge-driven methods that rely on knowledge bases (Dalvi et al., 2021) or concept graphs (Tang et al., 2024) to generate new problems by sampling key reasoning points (Huang et al., 2024). \n\nIn terms of code reasoning, from Code Alpaca's (Wang et al., 2023) use of self-instruct to generate 20K instruction data based on seed tasks, to Wiz-ardCoder's (Luo et al., 2023b) use of Code evolinstruct to generate more complex tasks, to Magicoder's use of oss-instruct (Wei et al., 2024) to extract 75K instructions from open source code. Synthesizing data continuously improves the model's code reasoning capabilities. \n\nIn contrast, there is less research on synthetic data for complex logical reasoning. Learning combines formal logic theory (Morishita et al., 2023) to synthesize data using basic reasoning rules to train language models' multi-step reasoning abilities. LogicBench (Parmar et al., 2024) not only constructs logical reasoning benchmark datasets but also provides synthetic data based on formal logic. \n\nLeetCode-style algorithm problems contain rich reasoning processes. This paper synthesizes highquality complex logical sreasoning data based on the formal logic of programming languages to enhance models' reasoning capabilities.",
            "score": 0.46012733454595806,
            "section_title": "A Related work A.1 Synthetic Reasoning Data",
            "char_start_offset": 25934,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 156,
                    "end": 624
                },
                {
                    "start": 627,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1049
                },
                {
                    "start": 1052,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1304
                },
                {
                    "start": 1305,
                    "end": 1450
                },
                {
                    "start": 1453,
                    "end": 1520
                },
                {
                    "start": 1521,
                    "end": 1681
                }
            ],
            "ref_mentions": [
                {
                    "start": 399,
                    "end": 419,
                    "matchedPaperCorpusId": "233297051"
                },
                {
                    "start": 486,
                    "end": 506,
                    "matchedPaperCorpusId": "233297051"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93896484375
        },
        {
            "corpus_id": "277244110",
            "title": "Neuro-Symbolic Scene Graph Conditioning for Synthetic Image Dataset Generation",
            "text": "Recently, research on synthetic dataset generation has emerged as a critical direction for advancing deep learning architectures. As models continue to scale in complexity and parameter count, acquiring and annotating large-scale real-world datasets remains prohibitively resource-intensive due to costs, privacy regulations, and domain-specific data scarcity. Synthetic datasets offer a scalable alternative with reduced annotation overhead and controllable attribute distribution. Despite these advantages, models trained exclusively on synthetic data consistently underperform compared to those trained on real datasets, particularly on high-level reasoning tasks. This performance gap widens further as task complexity increases, even when scaling dataset size. To address these limitations, various conditioning techniques for generative models have been explored. \n\nSimultaneously, Neuro-Symbolic (NeSy) approaches have gained traction in the machine learning community [1]. These methods integrate neural representation learning with symbolic reasoning frameworks, combining data-driven optimization with structured knowledge. While NeSy architectures have demonstrated enhanced generalization in reasoning tasks, their potential for improving synthetic data generation remains largely unexplored. Scene graphs-structured representations where nodes represent objects and edges represent relations-offer a promising symbolic framework for conditioning image generation. This work explores the integration of such NeSy approaches into dataset generation to improve performance on complex tasks, particularly Scene Graph Generation (SGG) from images. The hypothesis posits that scene graphs encode useful background knowledge that can guide the generation process, ensuring synthetic data adhere to structural and semantic constraints. To the best of current knowledge, this represents the first systematic exploration of NeSy conditioning for synthetic dataset generation in the context of SGG tasks. The experimental framework employs SGAdapter [2] for Scene Graph-to-Image generation, with Stable Diffusion 2.0 [3] as the baseline model. CausalTDE [4] serves as the SGG evaluation model. \n\nThe key contributions of this work are threefold: (1) proposing a novel framework that integrates scene graph-based symbolic knowledge into synthetic dataset generation; (2) demonstrating that structurally-guided generation produces complementary training signals that enhance augmentation efficacy despite lower perceptual fidelity; and (3) providing empirical evidence that NeSy conditioning addresses specific limitations in conventional synthetic data generation for complex visual reasoning tasks.",
            "score": 0.4568283775548592,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 869
                },
                {
                    "start": 872,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1304
                },
                {
                    "start": 1305,
                    "end": 1476
                },
                {
                    "start": 1477,
                    "end": 1655
                },
                {
                    "start": 1656,
                    "end": 1840
                },
                {
                    "start": 1841,
                    "end": 2006
                },
                {
                    "start": 2007,
                    "end": 2145
                },
                {
                    "start": 2146,
                    "end": 2195
                },
                {
                    "start": 2198,
                    "end": 2700
                }
            ],
            "ref_mentions": [
                {
                    "start": 976,
                    "end": 979,
                    "matchedPaperCorpusId": "234469907"
                },
                {
                    "start": 2119,
                    "end": 2122,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 2156,
                    "end": 2159,
                    "matchedPaperCorpusId": "211532586"
                },
                {
                    "start": 2248,
                    "end": 2251,
                    "matchedPaperCorpusId": "234469907"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.984375
        },
        {
            "corpus_id": "272988100",
            "title": "Balancing Cost and Effectiveness of Synthetic Data Generation Strategies for LLMs",
            "text": "In this section, we present the prompts used to generate synthetic instruction and responses across generation methods and tasks. We adapt prompts from prior work in math reasoning into all of our representative tasks -math, coding (SQL), and general question answering. We perform additional prompt engineering to ensure the generated data resembles the original instructions. We validate this with small experiments to compare the effectiveness of training on synthetic data against real data. Similar to the experiment described in the \"Comparison of Synthetic SFT Data versus Real Data\" section in Li et al. (2024), we first generate synthetic data with size equal to the original training data. Then, we train Llama 2 7B model on the synthetic data and ensure the synthetic data maintains a level of effectiveness comparable to the real data. \n\nFor our initial evaluation results in",
            "score": 0.4565151773435293,
            "section_title": "G Prompts and Evaluation Details",
            "char_start_offset": 30150,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 699
                },
                {
                    "start": 700,
                    "end": 847
                },
                {
                    "start": 850,
                    "end": 887
                }
            ],
            "ref_mentions": [
                {
                    "start": 602,
                    "end": 618,
                    "matchedPaperCorpusId": "269930153"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5126953125
        },
        {
            "corpus_id": "263605725",
            "title": "Towards LogiGLUE: A Brief Survey and A Benchmark for Analyzing Logical Reasoning Capabilities of Language Models",
            "text": "Human Annotation. Crowdsourcing is one of the major approaches to create datasets, such as for NLI tasks. The advantages of this methodology include a richer linguistic grammar and potentially increased task complexity. However, it comes with drawbacks. In addition to being a costintensive process, crowdsourced datasets tend to harbor biases (as highlighted in numerous previous studies (Yu et al., 2019)). These biases can be leveraged by neural models to artificially inflate accuracy scores. Furthermore, assembling a dataset for logical reasoning tasks demands a level of expertise that poses a significant challenge. \n\nExtraction from Academic Challenge. It is hard for crowdsourcing workers to produce questions requiring complicated logical reasoning since such reasoning tasks require extensive training and practice. Fortunately, questions in some standardized tests are aligned with the goal of logical reasoning and can be utilized to create such datasets after some preprocessing (Yu et al., 2019;Liu et al., 2020). However, the domains of these examinations are limited and the dataset size is small. Synthetic Generation. Synthetic generation is more efficient to create large data than manually created ones (Luo et al., 2022b). There are two ways, simulation based (Weston et al., 2015b) and rule-based (Clark et al., 2020;Saeed et al., 2021;Banerjee et al., 2020). In rule based methods, logic programs (either written by humans or mined from knowledge graphs) are generated, and then implications are drawn by automatic theorem prover. Last, the rules and facts in the logic programs are converted into English form using natural language patterns. Synthetic generation has issues that the rules or facts do not have real-world meaning and the language could be simple.",
            "score": 0.45503311596832513,
            "section_title": "Dataset Creation Techniques",
            "char_start_offset": 11533,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 17
                },
                {
                    "start": 18,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 623
                },
                {
                    "start": 626,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1383
                },
                {
                    "start": 1384,
                    "end": 1555
                },
                {
                    "start": 1556,
                    "end": 1668
                },
                {
                    "start": 1669,
                    "end": 1789
                }
            ],
            "ref_mentions": [
                {
                    "start": 389,
                    "end": 406,
                    "matchedPaperCorpusId": "209485573"
                },
                {
                    "start": 994,
                    "end": 1011,
                    "matchedPaperCorpusId": "209485573"
                },
                {
                    "start": 1011,
                    "end": 1028,
                    "matchedPaperCorpusId": "220483148"
                },
                {
                    "start": 1225,
                    "end": 1244,
                    "matchedPaperCorpusId": "250311154"
                },
                {
                    "start": 1321,
                    "end": 1341,
                    "matchedPaperCorpusId": "211126663"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58203125
        },
        {
            "corpus_id": "253384598",
            "title": "Learning to Infer from Unlabeled Data: A Semi-supervised Learning Approach for Robust Natural Language Inference",
            "text": "With the advancement of text generation models in recent years (Radford et al., 2019;Brown et al., 2020;Lewis et al., 2020), researchers have started to adopt synthetic text generation as a data augmentation strategy for various tasks such as commonsense reasoning (Yang et al., 2020), sentence classification (Anaby-Tavor et al., 2020) and question answering (Puri et al., 2020). More recently, synthetic text generation methods have also been employed to remove spurious correlations in human annotated data for fact verification (Lee et al., 2021) and NLI (Wu et al., 2022). However, to our knowledge, no prior work aims at tackling low-resource NLI scenarios nor do they address the unavailability of hypotheses for unlabeled premises by using synthetic data generation methods.",
            "score": 0.4483924175418692,
            "section_title": "Synthetic Data Generation",
            "char_start_offset": 28074,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 63,
                    "end": 85,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 104,
                    "end": 123,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 265,
                    "end": 284,
                    "matchedPaperCorpusId": "250390686"
                },
                {
                    "start": 360,
                    "end": 379,
                    "matchedPaperCorpusId": "211258652"
                },
                {
                    "start": 532,
                    "end": 550,
                    "matchedPaperCorpusId": "238227205"
                },
                {
                    "start": 559,
                    "end": 576,
                    "matchedPaperCorpusId": "247628095"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92626953125
        },
        {
            "corpus_id": "271270817",
            "title": "A Survey of Prompt Engineering Methods in Large Language Models for Different NLP Tasks",
            "text": "The authors of Shao et al. (2023) come up with Synthetic prompting using LLMs to generate synthetic examples which are augmented to the existing hand-crafted examples as seen in a conventional few-shot setting.This prompting method involves two steps: (1) the backward step, where the LLM synthesizes a query based on a self-generated reasoning chain; and (2) the forward step, where the LLM generates a reasoning chain for the synthesized query, making the reasoning chain to be more accurate.Finally, to choose the best examples, this work uses an in-cluster complexity and the most complex examples with the longest reasoning chains are used during inference.The results show Synthetic prompting achieving up to 15.6% absolute gains when experimented with different Mathematical Problem Solving, Commonsense Reasoning and Logical Reasoning task datasets.",
            "score": 0.44599991101460845,
            "section_title": "SYNTHETIC PROMPTING",
            "char_start_offset": 15478,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 210
                },
                {
                    "start": 210,
                    "end": 494
                },
                {
                    "start": 494,
                    "end": 662
                },
                {
                    "start": 662,
                    "end": 857
                }
            ],
            "ref_mentions": [
                {
                    "start": 15,
                    "end": 33,
                    "matchedPaperCorpusId": "256459681"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7412109375
        },
        {
            "corpus_id": "272146498",
            "title": "Self-Improving Diffusion Models with Synthetic Data",
            "text": "Thanks to the ongoing rapid advances in the field of generative artificial intelligence (AI), we are witnessing a proliferation of synthetic data of various modalities that have been rapidly integrated into popular online platforms. The voracious appetite of generative models for training data (Yahoo-Finance, 2024;The Economist, 2023a;b;Villalobos et al., 2022) has caused practitioners to train new models either partially or completely using synthetic data from previous generations of models. Synthetic training data is actually hard to avoid, because many of today's popular training datasets have been inadvertently polluted with synthetic data (Alemohammad et al., 2023;2024). \n\nUnfortunately, there are hidden costs to synthetic data training. Training new generative models with synthetic data from current or past generation models creates an autophagous (self-consuming) loop (Alemohammad et al., 2023;2024) that can have a detrimental effect on performance. In the limit over many generations of training, the quality and/or diversity of the synthetic data will decrease, in what has been termed Model Autophagy Disorder (MAD) (Alemohammad et al., 2023;2024) and Model Collapse (Shumailov et al., 2024). MAD generative models also have major fairness issues, as they produce increasingly biased samples that lead to inaccurate representations across the attributes present in real data (e.g., related to demographic factors such as gender and race) (Wyllie et al., 2024). \n\nMADness arises because synthetic data, regardless of how accurately it is modeled and generated, is still an approximation of samples from the real data distribution. 1 An autophagous loop causes any approximation errors to be compounded, ultimately resulting in performance deterioration and bias amplification. \n\nSafely advancing the performance of generative AI systems in the synthetic data era requires that we make progress on both of the following open questions: \n\nQ1. How can we best exploit synthetic data in generative model training to improve real data modeling and synthesis? Q2. How can we exploit synthetic data in generative model training in a way that does not lead to MADness in the future?",
            "score": 0.4419607698719876,
            "section_title": "INTRODUCTION",
            "char_start_offset": 384,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 684
                },
                {
                    "start": 687,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1216
                },
                {
                    "start": 1217,
                    "end": 1484
                },
                {
                    "start": 1487,
                    "end": 1655
                },
                {
                    "start": 1656,
                    "end": 1799
                },
                {
                    "start": 1802,
                    "end": 1957
                },
                {
                    "start": 1960,
                    "end": 2076
                },
                {
                    "start": 2077,
                    "end": 2080
                },
                {
                    "start": 2081,
                    "end": 2197
                }
            ],
            "ref_mentions": [
                {
                    "start": 1191,
                    "end": 1215,
                    "matchedPaperCorpusId": "271448069"
                },
                {
                    "start": 1462,
                    "end": 1483,
                    "matchedPaperCorpusId": "268363462"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75048828125
        },
        {
            "corpus_id": "271309962",
            "title": "Open Artificial Knowledge",
            "text": "The rapid advancement in Artificial Intelligence (AI) and Machine Learning (ML) has underscored the critical need for large, diverse, and high-quality datasets to train and evaluate foundation models (Bommasani et al., 2021).However, acquiring such datasets presents significant challenges, including data scarcity, privacy concerns, and high costs associated with data collection and annotation.Artificial (synthetic) data has emerged as a promising solution to these challenges, offering a way to generate data that mimics realworld patterns and characteristics (Ben Allal et al., 2024;Liu et al., 2024;Sun et al., 2023;Li et al., 2023b;Long et al., 2024;Borisov et al., 2022).The importance of ar-tificial data in AI research has grown substantially due to several factors:\n\n-Scalability: Synthetic data can be generated at scale, addressing the need for massive datasets required by modern AI models.\n\n-Privacy preservation: Artificial data can help mitigate privacy issues by creating anonymized datasets free from sensitive personal information.\n\n-Diversity and representation: Synthetic data can be controlled (conditioned) to cover a wide range of scenarios, potentially addressing biases present in realworld datasets.\n\n-Cost-effectiveness: Generating artificial data can be more economical than collecting and annotating realworld data.\n\nThe use of synthetic datasets in training state-of-the-art language models (LLMs) has become increasingly prevalent.This trend is evident in models like Llama-31 , which built upon its predecessor, Llama 2 (Touvron et al., 2023), by incorporating synthetic data in its training process.Similar approaches have been applied to other advanced models (Young et al., 2024;Li et al., 2023a;Tunstall et al., 2023).\n\nWhile handcrafted human data has shown significant improvements in supervised fine-tuning (SFT) of LLMs, particularly for tasks like code generation and mathematical reasoning (Roziere et al., 2023;Wan et al., 2024), the scarcity and cost of creating such high-quality data have led to the increasing use of synthetic data as a proxy.",
            "score": 0.44087633900359224,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 225
                },
                {
                    "start": 225,
                    "end": 396
                },
                {
                    "start": 396,
                    "end": 679
                },
                {
                    "start": 679,
                    "end": 776
                },
                {
                    "start": 778,
                    "end": 904
                },
                {
                    "start": 906,
                    "end": 1051
                },
                {
                    "start": 1053,
                    "end": 1227
                },
                {
                    "start": 1229,
                    "end": 1346
                },
                {
                    "start": 1348,
                    "end": 1464
                },
                {
                    "start": 1464,
                    "end": 1634
                },
                {
                    "start": 1634,
                    "end": 1756
                },
                {
                    "start": 1758,
                    "end": 2092
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8798828125
        },
        {
            "corpus_id": "269294098",
            "title": "Intellecta Cognitiva: A Comprehensive Dataset for Advancing Academic Knowledge and Machine Reasoning",
            "text": "The synthetic portion of the dataset is crafted using the advanced Mixtral-8x7B-Instruct-v0.1 model, which is pivotal in generating data that simulates complex thought processes and detailed textbook-like content.This generation phase is designed to fill the gaps in existing datasets by covering a wider array of topics and presenting information in a format that mimics human reasoning and explanation styles.The synthetic data is tailored to enhance the cognitive processing capabilities of AI models, ensuring they can navigate and interpret a multitude of scenarios and subjects effectively.",
            "score": 0.4396506628741437,
            "section_title": "Synthetic Data Generation",
            "char_start_offset": 4056,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 213
                },
                {
                    "start": 213,
                    "end": 411
                },
                {
                    "start": 411,
                    "end": 596
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7666015625
        },
        {
            "corpus_id": "260846474",
            "title": "Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic",
            "text": "A synthetic deduction corpus can be one promising approach for language models (LMs) to acquire logical deductive reasoning ability. The automatic (programmatic) generation ensures the validity of the resulting deductive proof examples. \n\nFurther, since we can bypass high-cost human annotations we can generate many examples, which should be required by LMs to learn deductive reasoning inductively. \n\nRuleTaker (Clark et al., 2021) proposed a deduction corpus composed of synthetically generated multistep deductive proofs written in natural languages. Each deductive proof (dis-)proves a hypothesis by applying deduction rules multiple times to a given set of facts. As the deduction rules, rules of the implication kind were used. They showed that Transformer (Vaswani et al., 2017) LMs can solve these problems in the sense that they can predict the final answer (i.e., \"proved\", \"disproved\", or \"unknown\") of each deductive proof given the fact set. Later studies (Saha et al., 2020;Dalvi et al., 2021;Tafjord et al., 2021;Sanyal et al., 2022) showed that generative LMs can generate even the intermediate proofs as well as the final answer. \n\nArtificial Argument Corpus (Betz et al., 2021) is another corpus composed of synthetically generated single-step deductive proofs. As the deduction rules, hand-selected rules useful for critical thinking were used. They showed that the LMs trained on this corpus not only solve the task of this corpus itself but generalize to other NLI tasks from GLUE benchmark (Wang et al., 2018). However, at the same time, they showed that such LMs do not generalize well to more challenging logical reasoning tasks such as ARC (Habernal et al., 2018) and LogiQA (Liu et al., 2020). Gontier et al. (2020) investigated the deductive reasoning ability of LMs on a corpus, which is composed of a specific type of multistep inference: kinship relationship on synthetic kinship graphs.",
            "score": 0.4376819636234917,
            "section_title": "A.1. Synthetic Corpus for Acquiring Deductive Reasoning Ability",
            "char_start_offset": 34565,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 236
                },
                {
                    "start": 239,
                    "end": 400
                },
                {
                    "start": 403,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1147
                },
                {
                    "start": 1150,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1533
                },
                {
                    "start": 1534,
                    "end": 1720
                },
                {
                    "start": 1721,
                    "end": 1918
                }
            ],
            "ref_mentions": [
                {
                    "start": 413,
                    "end": 433,
                    "matchedPaperCorpusId": "211126663"
                },
                {
                    "start": 764,
                    "end": 786,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 970,
                    "end": 989,
                    "matchedPaperCorpusId": "222141025"
                },
                {
                    "start": 989,
                    "end": 1008,
                    "matchedPaperCorpusId": "233297051"
                },
                {
                    "start": 1008,
                    "end": 1029,
                    "matchedPaperCorpusId": "229371222"
                },
                {
                    "start": 1029,
                    "end": 1049,
                    "matchedPaperCorpusId": "247594506"
                },
                {
                    "start": 1177,
                    "end": 1196,
                    "matchedPaperCorpusId": "221702898"
                },
                {
                    "start": 1513,
                    "end": 1532,
                    "matchedPaperCorpusId": "5034059"
                },
                {
                    "start": 1666,
                    "end": 1689,
                    "matchedPaperCorpusId": "3555187"
                },
                {
                    "start": 1701,
                    "end": 1719,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.796875
        },
        {
            "corpus_id": "275916340",
            "title": "SynDEc: A Synthetic Data Ecosystem",
            "text": "Synthetic data can be defined as \"data that has been generated using a purpose-built mathematical model or algorithm, with the aim of solving a (set of) data science task(s)\" (Jordon et al., 2022, p. 5). This generation process can take many forms as comprehensively categorized by Bauer et al. (2024) into 20 distinct method types. Among these, generative adversarial networks (GANs) are the most popular. GANs learn by pitting a generator (synthesizes data from random noise) and a discriminator (classifies samples as real or fake) against each other, resulting in two highly skilled networks (Goodfellow et al., 2014). This architecture is highly adaptable, as discriminator and generator can be easily adjusted to new tasks (e.g., time series or graph generation) while being frequently the best-performing synthetic data generation method (Bauer et al., 2024). Another commonly employed synthetic data generation method is autoencoder-based architectures, especially variational autoencoder (VAE) (Kingma & Welling, 2013). VAEs are trained by mapping an input sample to a hidden representation, which is then mapped back to the original vector, thus creating a model that synthesizes valid data from a lower dimensional representation. This decoder model is then used to generate data from random noise which makes it especially useful for learning from data with disentangled features (Bauer et al., 2024). Third, recurrent neural networks, feedforward neural networks which include recurrent edges, are able to generate sequential data of arbitrary length. This makes them ideal for sequence generation tasks such as speech synthesis, music, and time series generation (Lipton et al., 2015). Finally, virtual environments are computer simulations in which algorithms interact with each other based on predefined rules, generating synthetic data in the process (Bonabeau, 2002). \n\nIn the context of machine learning, synthetic data is primarily utilized in three key areas: (i) private data release, (ii) data de-biasing and fairness, and (iii) data augmentation for robustness (Jordon et al., 2022).",
            "score": 0.4372537097965159,
            "section_title": "Synthetic data generation and its application",
            "char_start_offset": 12134,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 866
                },
                {
                    "start": 867,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1241
                },
                {
                    "start": 1242,
                    "end": 1413
                },
                {
                    "start": 1414,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1699
                },
                {
                    "start": 1700,
                    "end": 1885
                },
                {
                    "start": 1888,
                    "end": 2107
                }
            ],
            "ref_mentions": [
                {
                    "start": 1868,
                    "end": 1884,
                    "matchedPaperCorpusId": "14402473"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91748046875
        },
        {
            "corpus_id": "270210215",
            "title": "Physics of Language Models: Part 1, Learning Hierarchical Language Structures",
            "text": "In this paper, we analyzed how transformers like GPT-2 perform hierarchical structure reasoning on challenging synthetic CFGs, showing that their internal states correlate strongly with the dynamic-programming computations underpinning such reasoning (i.e., for parsing and generation). This work provides a controlled interpretability setting and offers insights into how language models can effectively reason over complex, hierarchical structures and generate valid continuations. We also introduced multi-head linear probing-a tool that may enable deeper analyses of larger models on similarly complex tasks. \n\nWe further derived several corollary findings: including showing why absolute positional embeddings is inferior to relative and rotary embeddings; uniform attention alone is surprisingly effective (motivating our follow-up work on Canon layers [1]); encoder-only models (e.g., BERT, DeBERTa) struggle with deep structure reasoning on CFGs compared to autoregressive models (e.g., GPT); and injecting structural or syntactic noise into pretraining data markedly improves robustness to corrupted language prompts. \n\nWhile synthetic CFGs offer well-defined benchmarks for compositional and hierarchical behavior, they do not capture the full diversity of language or intelligence-much like sorting or ListOps tasks. For this reason, we explore grade-school math and reasoning in Parts 2.1+2.2 [37,38], knowledge storage, extraction, and manipulation in Parts 3.1+3.2+3.3 [3][4][5], and integrate these into a unified synthetic-data architecture playground in Part 4 [1].",
            "score": 0.43626202123994795,
            "section_title": "Conclusion.",
            "char_start_offset": 41526,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 612
                },
                {
                    "start": 615,
                    "end": 1126
                },
                {
                    "start": 1129,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1404
                },
                {
                    "start": 1405,
                    "end": 1582
                }
            ],
            "ref_mentions": [
                {
                    "start": 1405,
                    "end": 1409,
                    "matchedPaperCorpusId": "271544257"
                },
                {
                    "start": 1409,
                    "end": 1412,
                    "matchedPaperCorpusId": "272146356"
                },
                {
                    "start": 1483,
                    "end": 1486,
                    "matchedPaperCorpusId": "262825178"
                },
                {
                    "start": 1486,
                    "end": 1489,
                    "matchedPaperCorpusId": "262898066"
                },
                {
                    "start": 1489,
                    "end": 1492,
                    "matchedPaperCorpusId": "269005957"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4306640625
        },
        {
            "corpus_id": "252222361",
            "title": "Rule-adhering synthetic data - the lingua franca of learning",
            "text": "AI-generated synthetic data allows to distill the general patterns of existing data, that can then be shared safely as granular-level representative, yet novel data samples within the original semantics. In this work we explore approaches of incorporating domain expertise into the data synthesis, to have the statistical properties as well as pre-existing domain knowledge of rules be represented. The resulting synthetic data generator, that can be probed for any number of new samples, can then serve as a common source of intelligence, as a lingua franca of learning, consumable by humans and machines alike. We demonstrate the concept for a publicly available data set, and evaluate its benefits via descriptive analysis as well as a downstream ML model.",
            "score": 0.4310919816026011,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84716796875
        },
        {
            "corpus_id": "258170048",
            "title": "Synthetically generating human-like data for sequential decision-making tasks via reward-shaped imitation learning",
            "text": "We consider the problem of synthetically generating data that can closely resemble human decisions made in the context of an interactive human-AI system like a computer game. We propose a novel algorithm that can generate synthetic, human-like, decision making data while starting from a very small set of decision making data collected from humans. Our proposed algorithm integrates the concept of reward shaping with an imitation learning algorithm to generate the synthetic data. We have validated our synthetic data generation technique by using the synthetically generated data as a surrogate for human interaction data to solve three sequential decision making tasks of increasing complexity within a small computer game-like setup. Different empirical and statistical analyses of our results show that the synthetically generated data can substitute the human data and perform the game-playing tasks almost indistinguishably, with very low divergence, from a human performing the same tasks.",
            "score": 0.42987121895931385,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91015625
        },
        {
            "corpus_id": "276421244",
            "title": "Theorem Prover as a Judge for Synthetic Data Generation",
            "text": "Recent studies have shown that large language models (LLMs) are increasingly capable of tackling mathematical problems (Dubey et al., 2024;Jaech et al., 2024;Yang et al., 2024a;Guo et al., 2025). Techniques such as Chain of Thought (CoT) aim to enhance the reasoning process by breaking complex problems into intermediate steps (Wei et al., 2022;Kojima et al., 2022). Although these methods have achieved success on benchmarks like GSM8K (Cobbe et al., 2021), they are prone to Find the area of a circle with radius 2 1. The formula for area is 2. By performing substitution: \n\n3. The answer is 1. The formula for area is 2. By performing substitution: \n\n3. The answer is Final Answer: critical flaws. Specifically, LLM-generated CoT reasoning steps may often include errors, leading to incorrect solutions even when the final answer appears plausible (Leang et al., 2024). \n\nTo address these flaws, coding-based tools such as Python (Chen et al., 2022;Lyu et al., 2023) and SMT Solvers like the Z3 solver (De Moura and Bj\u00f8rner, 2008) have been widely used to improve reasoning capabilities, as well as theorem provers such as Lean (De Moura et al., 2015) and Isabelle (Paulson, 1994). These systems provide formal verification on the logical validity of LLM reasoning steps. Nevertheless, they are not without challenges; autoformalisation errors often lead to execution failures during theorem prover verification in complex tasks (Xu et al., 2024;Quan et al., 2024;Gou et al., 2023a;Olausson et al., 2023), including Python's struggles with Olympiad-level problems (Leang et al., 2024). \n\nParallel to these strategies, synthetic data generation emerged as a promising approach to enhance mathematical reasoning (Xin et al., 2024;Lu et al., 2024b).",
            "score": 0.42963012725067207,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 575
                },
                {
                    "start": 578,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 652
                },
                {
                    "start": 655,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 873
                },
                {
                    "start": 876,
                    "end": 1185
                },
                {
                    "start": 1186,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1589
                },
                {
                    "start": 1592,
                    "end": 1750
                }
            ],
            "ref_mentions": [
                {
                    "start": 328,
                    "end": 346,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 346,
                    "end": 366,
                    "matchedPaperCorpusId": "249017743"
                },
                {
                    "start": 1006,
                    "end": 1034,
                    "matchedPaperCorpusId": "15912959"
                },
                {
                    "start": 1132,
                    "end": 1155,
                    "matchedPaperCorpusId": "232990"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77978515625
        },
        {
            "corpus_id": "270560130",
            "title": "Scaling Synthetic Logical Reasoning Datasets with Context-Sensitive Declarative Grammars",
            "text": "Language models trained only on natural language show lackluster capabilities at logical reasoning (McCoy et al., 2023;Mahowald et al., 2024).As a countermeasure, we can train neural models to match the output of symbolic reasoning systems (e.g., logic theorem provers, or other algorithms) on procedurally generated problems, to sharpen their reasoning capabilities.This process improves accuracy on some human-authored problems (Wu et al., 2021;Clark et al., 2020;Wu et al., 2022;Liu et al., 2023).\n\nPrevious work on synthetic first-order logic (FOL) reasoning datasets, RuleTaker (Clark et al., 2020), LogicNLI (Goodwin et al., 2020a) and FLD (Morishita et al., 2023), write dedicated code reimplementing the FOL axioms from scratch to generate proofs, and translate the generated problems to natural language.We propose Unigram, a framework for synthetic reasoning data generation, specifically designed to generate problems jointly into multiple languages.We represent grammars with concise and expressive rules binding two languages, and constraints to prune unwanted generations.\n\nWe write the most extensive grammar of FOL semantic fragments to our knowledge.We structure the generated expressions into PREMISE, HY-POTHESIS pairs, and annotate their logical relationship (entailment/contradiction/neutral) with a FOL solver, following the natural language inference (NLI) framework (Goodwin et al., 2020b).A simplistic FOL NLI problem is: PREMISE: Everyone who is happy is rich.Mary is rich.HYPOTHESIS: Mary is happy LABEL: Neutral.\n\nWe fine-tune DeBERTa NLI models (He et al., 2021) on Unigram-FOL and compare it with previous similar datasets.The 184M parameters (basesize) beats GPT-4 augmented or not with external theorem provers, on the FOLIO (Han et al., 2022) dataset.",
            "score": 0.42817481714409705,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 142,
                    "end": 367
                },
                {
                    "start": 367,
                    "end": 500
                },
                {
                    "start": 502,
                    "end": 813
                },
                {
                    "start": 813,
                    "end": 961
                },
                {
                    "start": 961,
                    "end": 1086
                },
                {
                    "start": 1088,
                    "end": 1167
                },
                {
                    "start": 1167,
                    "end": 1414
                },
                {
                    "start": 1414,
                    "end": 1486
                },
                {
                    "start": 1486,
                    "end": 1499
                },
                {
                    "start": 1499,
                    "end": 1540
                },
                {
                    "start": 1542,
                    "end": 1653
                },
                {
                    "start": 1653,
                    "end": 1784
                }
            ],
            "ref_mentions": [
                {
                    "start": 119,
                    "end": 141,
                    "matchedPaperCorpusId": "268551442"
                },
                {
                    "start": 430,
                    "end": 447,
                    "matchedPaperCorpusId": "231627491"
                },
                {
                    "start": 447,
                    "end": 466,
                    "matchedPaperCorpusId": "211126663"
                },
                {
                    "start": 466,
                    "end": 482,
                    "matchedPaperCorpusId": "249890002"
                },
                {
                    "start": 583,
                    "end": 603,
                    "matchedPaperCorpusId": "211126663"
                },
                {
                    "start": 646,
                    "end": 670,
                    "matchedPaperCorpusId": "260846474"
                },
                {
                    "start": 1390,
                    "end": 1413,
                    "matchedPaperCorpusId": "218581294"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76123046875
        },
        {
            "corpus_id": "272421662",
            "title": "A Systematic Review of Synthetic Data Generation Techniques Using Generative AI",
            "text": "To conduct and report this review, we followed the guidelines for systematic literature reviews [24], systematic mapping studies [25], and PRISMA statement guidelines [26]. In our study, this approach helped us identify and map the various techniques used in synthetic data generation with generative AI. This allowed us to understand the extent to which generative AI applications have been developed and applied to synthetic data generation. In addition, a mapping study helped identify potential research gaps. This systematic review further enabled us to explore the current trends in the technical approaches, methodologies, and frameworks employed in the development of generative AI for synthetic data generation.",
            "score": 0.4268795056642635,
            "section_title": "Research Methodology",
            "char_start_offset": 24898,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 720
                }
            ],
            "ref_mentions": [
                {
                    "start": 129,
                    "end": 133,
                    "matchedPaperCorpusId": "15817433"
                },
                {
                    "start": 167,
                    "end": 171,
                    "matchedPaperCorpusId": "1344981"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6416015625
        },
        {
            "corpus_id": "277104955",
            "title": "Synthetic Data Generation Using Large Language Models: Advances in Text and Code",
            "text": "Large Language Models (LLMs) have achieved remarkable success across natural language and code generation tasks, owing in part to training on massive datasets. However, acquiring sufficient high-quality training data remains a bottleneck in many domains [60]. Data scarcity, high annotation costs, and privacy constraints often limit the availability of large supervised corpora. These challenges have spurred growing interest in synthetic data generation, where additional training examples are produced artificially rather than collected from the real world. Recent advances in generative AI -particularly LLMs like Anthropic's Claude 3.7 Sonnet, DeepSeek's R1, Meta's Llama 3, and OpenAI's GPT-o3 -provide powerful new tools to generate synthetic text and code that mimic real data distributions. This paper surveys and analyzes the latest developments in LLM-driven synthetic data generation for both natural language text and programming code, highlighting techniques, applications, challenges, and future directions. \n\nLLMs can produce human-like text and code, making them attractive data generators for tasks where obtaining real data is costly or infeasible. For example, instead of manually labeling thousands of sentences for a classifier, one can prompt an LLM to create diverse labeled examples [33], or have it generate code snippets to augment code model training. Synthetic data generated by LLMs has shown promise in boosting model performance in low-resource settings, reducing annotation costs, and enabling data augmentation for improved robustness [11]. At the same time, using LLMs as data generators raises questions about quality control, realism, and biases in the generated data. \n\nA comprehensive review of LLM-based approaches for text and code generation is currently lacking, despite their increasing adoption [52]. This paper aims to fill that gap by providing a structured survey of existing techniques and challenges. The focus is specifically on text and code generation, as opposed to other forms of data, due to several key reasons. First, visual data synthesis has been extensively studied, making an additional review redundant [10,8]. Second, while synthetic signal data (e.g., audio) is a developing field, it remains outside the scope of this study, though future work could extend these analyses [24].",
            "score": 0.4267670029697321,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 1022
                },
                {
                    "start": 1025,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1379
                },
                {
                    "start": 1380,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1705
                },
                {
                    "start": 1708,
                    "end": 1845
                },
                {
                    "start": 1846,
                    "end": 1950
                },
                {
                    "start": 1951,
                    "end": 2068
                },
                {
                    "start": 2069,
                    "end": 2173
                },
                {
                    "start": 2174,
                    "end": 2343
                }
            ],
            "ref_mentions": [
                {
                    "start": 254,
                    "end": 258,
                    "matchedPaperCorpusId": "273458929"
                },
                {
                    "start": 1569,
                    "end": 1573,
                    "matchedPaperCorpusId": "254877171"
                },
                {
                    "start": 2166,
                    "end": 2170,
                    "matchedPaperCorpusId": "234357997"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97607421875
        },
        {
            "corpus_id": "265456227",
            "title": "LLM-Assisted Code Cleaning For Training Accurate Code Generators",
            "text": "Synthetic data for LLMS. Recent works have explored using synthetic datasets for generalpurpose or task-specific finetuning of LLMS. These approaches work by generating synthetic datasets from a strong LLM (like GPT-3.5-TURBO or GPT-4) using a set of existing tasks (Taori et al., 2023;Chiang et al., 2023) or generating new tasks using self-instruct (Wang et al., 2022) or evol-instruct (Xu et al., 2023) approaches. This has been also applied for task-specific finetuningin common-sense reasoning (West et al., 2022), text-summarization (Sclar et al., 2022), mathematical reasoning (Luo et al., 2023a;Yue et al., 2023), tool use (Patil et al., 2023), coding (Luo et al., 2023b), and general-purpose reasoning Li et al. (2023b); Zelikman et al. (2022). \n\nMore specifically, Yue et al. (2023) curates diverse corpus of mathematics problems with chain-ofthought or program-of-thought (Chen et al., 2022b) annotations for mathematical reasoning analogous to our plans. Gunasekar et al. (2023) proposed pre-training models on programming \"textbooks\" generated synthetically from GPT-3.5-TURBO. Haluptzok et al. (2023) similarly generates programming puzzles and corresponding solutions from language models. Our work also studies curating synthetic data for code-generation space. However, instead of directly generating data using LLMS, we identify good programming patterns and clean existing datasets using them. \n\nAlgorithmic Code Generation. Code generation is a broad domain and is covered in Appendix D. We only discuss pertinent algorithmic code generation works here. Hendrycks et al. (2021) released the APPS dataset while Li et al. (2022) released the CODE-CONTESTS dataset with the ALPHACODE models.",
            "score": 0.4251144303761493,
            "section_title": "RELATED WORK",
            "char_start_offset": 24209,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 24
                },
                {
                    "start": 25,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 753
                },
                {
                    "start": 756,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1204
                },
                {
                    "start": 1205,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1412
                },
                {
                    "start": 1415,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1573
                },
                {
                    "start": 1574,
                    "end": 1708
                }
            ],
            "ref_mentions": [
                {
                    "start": 499,
                    "end": 517,
                    "matchedPaperCorpusId": "238857304"
                },
                {
                    "start": 711,
                    "end": 728,
                    "matchedPaperCorpusId": "259251773"
                },
                {
                    "start": 730,
                    "end": 752,
                    "matchedPaperCorpusId": "247762790"
                },
                {
                    "start": 1091,
                    "end": 1114,
                    "matchedPaperCorpusId": "251197051"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95654296875
        },
        {
            "corpus_id": "272704041",
            "title": "Efficacy of Synthetic Data as a Benchmark",
            "text": "In this section, we begin by illustrating our synthetic data generation mechanism. We then describe our two evaluation protocols designed to assess the effectiveness of synthetic data as a benchmark as well as the bias introduced by using the same LLM for generation and using it to solve the task.",
            "score": 0.4249566003719546,
            "section_title": "II. SETUP AND EVALUATION PROTOCOL",
            "char_start_offset": 3402,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 82
                },
                {
                    "start": 83,
                    "end": 298
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3173828125
        },
        {
            "corpus_id": "269702974",
            "title": "Rigorous Experimental Analysis of Tabular Data Generated using TVAE and CTGAN",
            "text": "\u2014Synthetic data generation research has been progressing at a rapid pace and novel methods are being designed every now and then. Earlier, statistical methods were used to learn the distributions of real data and then sample synthetic data from those distributions. Recent advances in generative models have led to more efficient modeling of complex high-dimensional datasets. Also, privacy concerns have led to the development of robust models with lesser risk of privacy breaches. Firstly, the paper presents a comprehensive survey of existing techniques for tabular data generation and evaluation matrices. Secondly, it elaborates on a comparative analysis of state-of-the-art synthetic data generation techniques, specifically CTGAN and TVAE for small, medium, and large-scale datasets with varying data distributions. It further evaluates the synthetic data using quantitative and qualitative metrics/techniques. Finally, this paper presents the outcomes and also highlights the issues and shortcomings which are still need to be addressed.",
            "score": 0.42482347851273183,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92333984375
        },
        {
            "corpus_id": "272753174",
            "title": "LogicPro: Improving Complex Logical Reasoning via Program-Guided Learning",
            "text": "With the rapid development of artificial intelligence, Large Language Models (LLMs) (Bi et al., 2024;Liu et al., 2024a) demonstrate excellent performance in reasoning tasks. The success of these models is inseparable from the support of largescale and high-quality reasoning data. However, data acquisition and processing face numerous challenges in the real world. As a viable alternative, synthetic data (Wang et al., 2024) can effectively alleviate this problem and further enhance (Dubey et al., 2024;Adler et al., 2024) the model's reasoning capabilities. \n\nSynthetic data (Liu et al., 2024b) has wide applications in mathematics and code domains. For 1 Code and data are publicly available at https://github. com/jiangjin1999/LogicPro You are given an integer array cards of length 4. You have four cards, each containing a number in the range [1,9]. You should arrange the numbers on these cards in a mathematical expression using the operators ['+', '-', '*', '/'] and the parentheses '(' and ')' to get the value 24. Please give code to solve it.",
            "score": 0.4227106823263234,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 560
                },
                {
                    "start": 563,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1055
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78271484375
        },
        {
            "corpus_id": "268096377",
            "title": "Generative artificial intelligence: synthetic datasets in dentistry",
            "text": "There are various AI techniques for generating synthetic datasets. The mechanisms of more common ones have been described in the following text.",
            "score": 0.4227106823263234,
            "section_title": "GENERATIVE AI TECHNIQUES",
            "char_start_offset": 5448,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 66
                },
                {
                    "start": 67,
                    "end": 144
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1844482421875
        },
        {
            "corpus_id": "272689782",
            "title": "Causal Language Modeling Can Elicit Search and Reasoning Capabilities on Logic Puzzles",
            "text": "We have shown that even on complex logical reasoning tasks such as Sudoku and Zebra puzzles, simple next-token prediction provided with a high-level decomposition of the reasoning steps during training is able to learn to solve the task. This suggests that, given the right level of detail and breakdown of reasoning steps in the training data, a pre-trained model might already present as a strong reasoning engine (without the need for post-training techniques such as fine-tuning, prompt engineering, self-consistency, tree-of-thoughts etc). These techniques might help significantly boost the baseline performance of a model or potentially make up for deficiencies in the pre-training data however. To move towards more general reasoning systems, an interesting challenge to overcome would be to simulate the decomposed reasoning data in an efficient manner. These tasks capture many different types of constraint satisfaction problems and we believe the framework and results should generalize to other settings as well. \n\nFinally, we conclude with some limitations of our study. Firstly, we note that we studied a synthetic setting on a toy task and real-world reasoning and planning tasks can be much more abstract and challenging. More specifically, Sudoku is a task which doesn't require the same degree of long-term planning as some harder benchmarks. That is, any cell we can make progress on is progress unlike constraint problems where one might need to backtrack. Moreover, we focused on a reasoning setting where creative thinking was not required. That is, the model did not need to invent new strategies to solve any test time puzzle. It is an interesting future direction to study to what extent causal language modeling can yield novel reasoning strategies. Moreover, there can be many different types of reasoning tasks which are not logic puzzles (for instance probabilistic puzzles or rule-less puzzles, see e.g. [GLFS24]) and our experiments do not explore those. 4. Neighbour of: This relation type provides the information that an entity with a particular attribute value is the neighbor of another entity. This relation type generalizes the \"immediate left\" relationship to include the immediate neighbors of the left and right sides. An example this type of clue is the person with a dog is next to the person who drinks milk.",
            "score": 0.4214192411983769,
            "section_title": "Conclusion",
            "char_start_offset": 49980,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 1025
                },
                {
                    "start": 1028,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1238
                },
                {
                    "start": 1239,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1776
                },
                {
                    "start": 1777,
                    "end": 1934
                },
                {
                    "start": 1935,
                    "end": 1986
                },
                {
                    "start": 1987,
                    "end": 2131
                },
                {
                    "start": 2132,
                    "end": 2260
                },
                {
                    "start": 2261,
                    "end": 2353
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.396484375
        },
        {
            "corpus_id": "256459681",
            "title": "Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models",
            "text": "Large language models can perform various reasoning tasks by using chain-of-thought prompting, which guides them to find answers through step-by-step demonstrations. However, the quality of the prompts depends on the demonstrations given to the models, and creating many of them by hand is costly. We introduce Synthetic prompting, a method that leverages a few handcrafted examples to prompt the model to generate more examples by itself, and selects effective demonstrations to elicit better reasoning. Our method alternates between a backward and forward process to generate new examples. The backward process generates a question that match a sampled reasoning chain, so that the question is solvable and clear. The forward process produces a more detailed reasoning chain for the question, improving the quality of the example. We evaluate our method on numerical, symbolic, and algorithmic reasoning tasks, and show that it outperforms existing prompting techniques.",
            "score": 0.4210838829794825,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8818359375
        },
        {
            "corpus_id": "273229503",
            "title": "Improving LLM Reasoning through Scaling Inference Computation with Collaborative Verification",
            "text": "Large language models (Brown et al., 2020;Achiam et al., 2023;Touvron et al., 2023a;b;Jiang et al., 2023;Team et al., 2024) have demonstrated exceptional performance across various natural language tasks. Notably, the reasoning tasks such as math problem solving (Cobbe et al., 2021;Hendrycks et al., 2021), code completion (Austin et al., 2021;Chen et al., 2021), multi-modal reasoning (Yue et al., 2024a;Liang et al., 2024a) have attracted significant attention from AI researchers. Since reasoning is a critical component of many important high-level tasks, such as scientific discovery (Liang et al., 2024a;Miret & Krishnan, 2024), world model (Hao et al., 2023), embodied agents (Song et al., 2023), etc. However, even the most advanced LLMs still face challenges in complex multi-step reasoning problems (Zhang et al., 2024a;Shi et al., 2024;Trinh et al., 2024). To improve the performance of LLMs on reasoning, recent studies (Yu et al., 2024b;Yue et al., 2024b;Gou et al., 2024;Luo et al., 2023;Wei et al., 2024;Tang et al., 2024;Yue et al., 2024c) have mainly focused on generating synthetic question-answering pairs from stronger LLMs like GPT-4 (Achiam et al., 2023) or utilizing human-annotated rationales (Toshniwal et al., 2024) for supervised fine-tuning. These approaches have achieved outstanding performance on reasoning benchmarks like GSM8k (Cobbe et al., 2021), MATH (Hendrycks et al., 2021;Lightman et al., 2023), MBPP (Austin et al., 2021), etc. While these straightforward data generation methods have proven effective, these LLMs are primarily trained to produce outputs that align with the correct reasoning steps they encountered during training.",
            "score": 0.42045223614496896,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1673
                }
            ],
            "ref_mentions": [
                {
                    "start": 22,
                    "end": 42,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 387,
                    "end": 406,
                    "matchedPaperCorpusId": "265466525"
                },
                {
                    "start": 648,
                    "end": 666,
                    "matchedPaperCorpusId": "258865812"
                },
                {
                    "start": 684,
                    "end": 703,
                    "matchedPaperCorpusId": "254408960"
                },
                {
                    "start": 848,
                    "end": 867,
                    "matchedPaperCorpusId": "267032902"
                },
                {
                    "start": 933,
                    "end": 951,
                    "matchedPaperCorpusId": "262084051"
                },
                {
                    "start": 951,
                    "end": 969,
                    "matchedPaperCorpusId": "261696697"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.587890625
        },
        {
            "corpus_id": "273403575",
            "title": "MIND: Math Informed syNthetic Dialogues for Pretraining LLMs",
            "text": "In parallel, having both raw data and conversation is beneficial for specialized and general purpose reasoning tasks, aligning with the findings in Maini et al. (2024). Since synthetic data tends to remove special tags, styles, and code indentations, the inclusion of raw data helps improve the generalizability of LLMs across diverse domains. Additionally, to measure the maximum gain we can achieve from conversations for a limited data, we continuously train C with all synthetic dialogues generated from OWM-4B. As shown in As shown in Table 6, even with a smaller M, the MIND-generated data provides a significant boost in math and general reasoning abilities compared to the raw/rephrased data. This demonstrates that the gains are not solely dependent on the capabilities of the larger M but are largely driven by the quality and structure of the MIND-generated dialogues. Additionally, regardless of model size and method of synthetic data generation, all LLM-generated synthetic data involves some form of knowledge distillation. However, we demonstrate an effective distillation approach that significantly enhances the reasoning ability of LLMs compared to existing approaches (Maini et al., 2024).",
            "score": 0.4191255734054764,
            "section_title": "ABLATIONS",
            "char_start_offset": 26080,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1209
                }
            ],
            "ref_mentions": [
                {
                    "start": 148,
                    "end": 167,
                    "matchedPaperCorpusId": "267312030"
                },
                {
                    "start": 1188,
                    "end": 1208,
                    "matchedPaperCorpusId": "267312030"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64208984375
        },
        {
            "corpus_id": "270766239",
            "title": "Compositional Structured Explanation Generation with Dynamic Modularized Reasoning",
            "text": "Other works probed the compositional abilities of models in natural language inference (Geiger et al., 2020;Goodwin et al., 2020;Yanaka et al., 2020Yanaka et al., , 2021;;Fu andFrank, 2023, 2024), focusing on specific linguistic phenomena, such as quantifiers, negation, or predicate replacements.I.e., they investigate compositionality in phenomena manifesting at the word level, in contrast to inferential processes holding between sentences.\n\nTo fill this gap, we examine compositional generalization in a multi-step entailment tree generation task, where different inference rules need to be composed.Concurrent work (Saparov et al., 2023) also concentrates on sentence-level compositionality in reasoning, but is limited in using a synthetic dataset.In comparison, we employ both natural language and synthetic data, and introduce a new model, with potential for further improvement, that can serve as a strong baseline for the task.\n\nNeural-Symbolic and Neural Methods Prior works show that symbolic approaches (Angeli and Manning, 2014;Mineshima et al., 2015;Mart\u00ednez-G\u00f3mez et al., 2017) that adopt pre-defined inference rules to establish derivations through iterative reasoning, endow models with strong compositionality.But being dependent on pre-defined rules, the models are limited to well-defined tasks.Recently, Yi et al. (2018); Yin et al. (2018); Li et al. (2020); Jiang et al. (2021) used neural networks to map raw signals to symbolic representations and subsequently performed symbolic reasoning to make predictions.As symbolic reasoning is brittle, novel works based on Neural Modular Networks (NMN) (Andreas et al., 2016;Hu et al., 2017) combine individual neural modules endowed with specialized reasoning capabilities.E.g., Jiang and Bansal (2019); Gupta et al. (2019) designed various modules in an NMN to perform unique types of reasoning in end-to-end manner.Similarly, Khot et al. (2021Khot et al. ( , 2023) ) proposed a Text Module Network for complex reasoning tasks, where each module is an existing QA system.",
            "score": 0.4182238672308906,
            "section_title": "Related Work",
            "char_start_offset": 6111,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 297
                },
                {
                    "start": 297,
                    "end": 444
                },
                {
                    "start": 446,
                    "end": 605
                },
                {
                    "start": 605,
                    "end": 755
                },
                {
                    "start": 755,
                    "end": 938
                },
                {
                    "start": 940,
                    "end": 1230
                },
                {
                    "start": 1230,
                    "end": 1317
                },
                {
                    "start": 1317,
                    "end": 1536
                },
                {
                    "start": 1536,
                    "end": 1742
                },
                {
                    "start": 1742,
                    "end": 1886
                },
                {
                    "start": 1886,
                    "end": 2041
                }
            ],
            "ref_mentions": [
                {
                    "start": 87,
                    "end": 108,
                    "matchedPaperCorpusId": "222396695"
                },
                {
                    "start": 108,
                    "end": 129,
                    "matchedPaperCorpusId": "218581294"
                },
                {
                    "start": 129,
                    "end": 148,
                    "matchedPaperCorpusId": "216868459"
                },
                {
                    "start": 148,
                    "end": 171,
                    "matchedPaperCorpusId": "231709707"
                },
                {
                    "start": 621,
                    "end": 643,
                    "matchedPaperCorpusId": "258865898"
                },
                {
                    "start": 1017,
                    "end": 1043,
                    "matchedPaperCorpusId": "2854390"
                },
                {
                    "start": 1043,
                    "end": 1066,
                    "matchedPaperCorpusId": "17291409"
                },
                {
                    "start": 1066,
                    "end": 1094,
                    "matchedPaperCorpusId": "17619888"
                },
                {
                    "start": 1327,
                    "end": 1343,
                    "matchedPaperCorpusId": "52919654"
                },
                {
                    "start": 1345,
                    "end": 1362,
                    "matchedPaperCorpusId": "49325612"
                },
                {
                    "start": 1364,
                    "end": 1380,
                    "matchedPaperCorpusId": "219573265"
                },
                {
                    "start": 1382,
                    "end": 1401,
                    "matchedPaperCorpusId": "235485085"
                },
                {
                    "start": 1621,
                    "end": 1643,
                    "matchedPaperCorpusId": "5276660"
                },
                {
                    "start": 1643,
                    "end": 1659,
                    "matchedPaperCorpusId": "18682"
                },
                {
                    "start": 1748,
                    "end": 1771,
                    "matchedPaperCorpusId": "202565945"
                },
                {
                    "start": 1773,
                    "end": 1792,
                    "matchedPaperCorpusId": "209202200"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.381103515625
        },
        {
            "corpus_id": "276250232",
            "title": "VersaPRM: Multi-Domain Process Reward Model via Synthetic Reasoning Data",
            "text": "We proposed VersaPRM trained using synthetic reasoning data to address the limitations of existing math PRMs. By leveraging a cost-efficient synthetic data generation pipeline, we enabled production of high-quality step-wise reasoning data and demonstrate that PRMs can effectively scale reasoning ability at inference time in diverse domains. \n\nFuture Work. Several directions remain for advancing multi-domain PRMs. Can VersaPRM be effectively used as a reward model for RL training? Can it improves RL training beyond math PRMs? Could more sophisticated counterfactual augmentation enhance PRM effectiveness? Also, evaluating PRMs on harder, open-ended problems would better assess their generalization. Lastly, more thorough exploration of PRMs with large models (e.g., GPT-4 (Achiam et al., 2023) and DeepSeek-R1) could clarify their scalability and role in state-of-the-art reasoning systems.",
            "score": 0.41728281853229787,
            "section_title": "Discussion and Future Directions",
            "char_start_offset": 26556,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 343
                },
                {
                    "start": 346,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 898
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95654296875
        },
        {
            "corpus_id": "275133286",
            "title": "Verbosity-Aware Rationale Reduction: Effective Reduction of Redundant Rationale via Principled Criteria",
            "text": "Recent advances in Large Language Models (LLMs) have demonstrated remarkable reasoning capabilities comparable to human cognitive abilities (Madaan et al., 2024;Shinn et al., 2024;Kumar et al., 2024). These models can solve complex reasoning tasks by explicitly generating extended reasoning paths. The generation of such paths involves producing explicit reasoning units (e.g., tokens, steps) (Yu et al., 2024b), which additionally enhances model performance through iterative prompting (Wang et al., 2023;Yao et al., 2023). This iterative generation of explicit reasoning paths allows the model to refine and expand its thought processes, incorporating strategic planning and ongoing cognitive generation (Xi et al., 2023;Yang et al., 2024). \n\nAlthough the extensive generation of explicit reasoning units leads to better peformance, it inherently leads to higher inference costs and increased latency (Yu et al., 2024b;Wang et al., 2024). Moreover, fine-tuning LLMs using a complete reasoning path does not consistently guarantee stronger performance (Yu et al., 2024b;Deng et al., 2024b;Liu et al., 2024), suggesting the need for methods that can maintain reasoning performance while reducing the generation of reasoning units. Despite this apparent requirement, it remains underexplored how to maintain LLM reasoning capabilities while reducing intermediate reasoning paths across diverse tasks. \n\nPrevious methods (Yu et al., 2024b) have drawn inspiration from human cognitive processes to address the aforementioned limitations. Some studies have proposed training pipelines that utilize augmented datasets, iteratively generated by foundation LLMs, to fine-tune subsequent LLMs (Yu et al., 2024b;Liu et al., 2024). However, they are inherently vulnerable as they significantly depend on the generative capabilities of LLMs. In response, other works have directly trained LLMs without augmenting datasets that facilitate implicit reasoning before answer generation. Deng et al. (2023) introduced a knowledge distillation method to distill explicit reasoning into implicit reasoning through token-level hidden states.",
            "score": 0.4160728214696439,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 743
                },
                {
                    "start": 746,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1400
                },
                {
                    "start": 1403,
                    "end": 1535
                },
                {
                    "start": 1536,
                    "end": 1722
                },
                {
                    "start": 1723,
                    "end": 1831
                },
                {
                    "start": 1832,
                    "end": 1972
                },
                {
                    "start": 1973,
                    "end": 2123
                }
            ],
            "ref_mentions": [
                {
                    "start": 140,
                    "end": 161,
                    "matchedPaperCorpusId": "257900871"
                },
                {
                    "start": 161,
                    "end": 180,
                    "matchedPaperCorpusId": "258833055"
                },
                {
                    "start": 724,
                    "end": 742,
                    "matchedPaperCorpusId": "257900871"
                },
                {
                    "start": 1091,
                    "end": 1108,
                    "matchedPaperCorpusId": "273811942"
                },
                {
                    "start": 1704,
                    "end": 1721,
                    "matchedPaperCorpusId": "273811942"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63134765625
        },
        {
            "corpus_id": "278171321",
            "title": "RV-Syn: Rational and Verifiable Mathematical Reasoning Data Synthesis based on Structured Function Library",
            "text": "The development of advanced reasoning Large Language Models (LLMs) (Zhao et al., 2023;OpenAI, 2024) has markedly improved their ability to address complex tasks across domains such as mathematics, science, and coding. This highlights the importance of synthesizing complex reasoning data to drive further advancements, given the limited availability of high-quality annotated instructions (Shah et al., 2024;Yuan et al., 2023). \n\nTo address this scarcity, researchers have explored various synthesizing methods, particularly in the mathematics domain. The mainstream methods involve data augmentation based on existing annotated training sets, such as GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021), ranging from self-evolving instructions (Xu et al., 2024;Zeng et al., 2024) and question paraphrasing (Yu et al., 2024), to solution augmentation (Lu et al., 2024). However, these methods are limited by the available training data, constraining the synthesis diversity (Li et al., 2024c(Li et al., , 2023)). To enhance diversity, recent approaches enable LLMs to generate a large scale of questions from various mathematicsrelated sources, including web pages (Yue et al., 2024) and knowledge points (Tang et al., 2024) from web corpora or textbooks. \n\nHowever, as shown in Figure 1, these direct problem generation methods encounter challenges in lacking a deep understanding and simulation of the underlying reasoning processes. As a result, the synthesized problems may appear superficially plausible but actually suffer from insufficient reasoning depth and logical inconsistencies (such as internal contradictions), especially when generating complex multi-step reasoning problems. Furthermore, a long-standing challenge in synthetic data generation is the difficulty in supporting the validation of solution correctness during the synthesis process, ultimately compromising training efficiency. \n\nTo address the aforementioned challenges, we draw inspiration from how human educators create problems. Just as one cannot directly write down an Olympic-level math problem without deep consideration and careful curation of the underlying problem-solving process, the same holds true for models.",
            "score": 0.4142472820137965,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 427
                },
                {
                    "start": 430,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1264
                },
                {
                    "start": 1267,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1700
                },
                {
                    "start": 1701,
                    "end": 1914
                },
                {
                    "start": 1917,
                    "end": 2020
                },
                {
                    "start": 2021,
                    "end": 2212
                }
            ],
            "ref_mentions": [
                {
                    "start": 688,
                    "end": 712,
                    "matchedPaperCorpusId": "268247488"
                },
                {
                    "start": 771,
                    "end": 789,
                    "matchedPaperCorpusId": "270213007"
                },
                {
                    "start": 1214,
                    "end": 1233,
                    "matchedPaperCorpusId": "268247902"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93798828125
        },
        {
            "corpus_id": "273549775",
            "title": "Unleashing LLM Reasoning Capability via Scalable Question Synthesis from Scratch",
            "text": "In this work, we propose ScaleQuest, a novel data synthesis framework that unlocks the ability of open-source smaller models to independently generate large-scale, high-quality reasoning data from scratch, at a low cost. By training the problem-solving models on a small subset of questions, we effectively activate their question-generation capabilities. We also introduce a response enhancement method. With these techniques, we successfully developed a fully synthetic math reasoning dataset consisting of 1 million question-answer pairs. Using this dataset, we fine-tuned the model and achieved remarkable improvements, with gains ranging from 29.2% to 46.4% compared to the base model. The fine-tuned 7B model, Qwen2-Math-7B-ScaleQuest, outperforms all competitors in the 7B-70B range and even surpasses proprietary models like GPT-4-Turbo and Claude-3.5-Sonnet. \n\nDue to time and cost constraints, there are several areas where our approach can be further optimized. \n\nFor instance, leveraging more powerful, larger problem-solving models like Qwen2.5-Math-72B-Instruct (Yang et al., 2024b) for question and response generation, using advanced models such as GPT-4o for constructing preference data for Question Preference Optimization, and further scaling up the generation of synthetic data. Each stage of our process has significant room for improvement. \n\nIn this paper, we have demonstrated the potential of this framework, laying the groundwork for future enhancements. \n\nFurthermore, despite the progress made in this work, there are still several limitations that need to be addressed. In our future research, we will concentrate on the following areas: \n\n\u2022 Large-scale and diverse high-quality data: This work chooses mathematical reasoning as a case study to demonstrate the effectiveness of our method. In the future, we will focus on broader and more complex tasks such as science and competitive programming. Additionally, future research will aim to continuously scale data synthesis to explore the scaling laws for synthetic data and seek a more efficient approach to scaling data generation. \u2022 Self-improvement capability: Our experiments demonstrate the model's self-improvement capability, meaning that it can generate data of higher quality than its original training set.",
            "score": 0.41141244983564634,
            "section_title": "CONCLUSION",
            "char_start_offset": 27670,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 867
                },
                {
                    "start": 870,
                    "end": 972
                },
                {
                    "start": 975,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1363
                },
                {
                    "start": 1366,
                    "end": 1481
                },
                {
                    "start": 1484,
                    "end": 1599
                },
                {
                    "start": 1600,
                    "end": 1667
                },
                {
                    "start": 1670,
                    "end": 1819
                },
                {
                    "start": 1820,
                    "end": 1927
                },
                {
                    "start": 1928,
                    "end": 2113
                },
                {
                    "start": 2114,
                    "end": 2297
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.966796875
        },
        {
            "corpus_id": "277993790",
            "title": "LongPerceptualThoughts: Distilling System-2 Reasoning for System-1 Perception",
            "text": "Recent reasoning models through test-time scaling have demonstrated that long chain-of-thoughts can unlock substantial performance boosts in hard reasoning tasks such as math and code. However, the benefit of such long thoughts for system-2 reasoning is relatively less explored in other domains such as perceptual tasks where shallower, system-1 reasoning seems sufficient. In this paper, we introduce LongPerceptualThoughts, a new synthetic dataset with 30K long-thought traces for perceptual tasks. The key challenges in synthesizing elaborate reasoning thoughts for perceptual tasks are that off-the-shelf models are not yet equipped with such thinking behavior and that it is not straightforward to build a reliable process verifier for perceptual tasks. Thus, we propose a novel three-stage data synthesis framework that first synthesizes verifiable multiple-choice questions from dense image descriptions, then extracts simple CoTs from VLMs for those verifiable problems, and finally expands those simple thoughts to elaborate long thoughts via frontier reasoning models. In controlled experiments with a strong instruction-tuned 7B model, we demonstrate notable improvements over existing visual reasoning data-generation methods. Our model, trained on the generated dataset, achieves an average +3.4 points improvement over 5 vision-centric benchmarks, including +11.8 points on V$^*$ Bench. Notably, despite being tuned for vision tasks, it also improves performance on the text reasoning benchmark, MMLU-Pro, by +2 points.",
            "score": 0.41138228880200545,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73779296875
        },
        {
            "corpus_id": "253237103",
            "title": "RLET: A Reinforcement Learning Based Approach for Explainable QA with Entailment Trees",
            "text": "2022) propose ParaPattern by creating synthetic logical deduction data from Wikipedia, which serves as a good supplement to train the generative model. Different from existing approaches, RLET models the cumulative signals across the whole tree in training, benefiting from our designed reward function under reinforcement learning framework.\n\nPath Reasoning using Reinforcement Learning Our work also aligns well with multiple automated reasoning tasks built with RL (Xian et al., 2019;Liu et al., 2021;Poesia et al., 2021). Especially, reinforcement learning has exhibited its attractiveness in knowledge graph reasoning (Xiong et al., 2017;Das et al., 2018;Lin et al., 2018), where the multi-hop path can be represented as sequential decision problems. Similar to the above RL methods, we formulate the entailment tree generation task as a trajectory of reasoning steps. In contrast to KG based multi-hop reasoning path, however, RLET generates intermediate conclusions of the reasoning steps, providing fluent natural language explanations in detail.",
            "score": 0.41119321949018744,
            "section_title": "Related Work",
            "char_start_offset": 27747,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 468,
                    "end": 487,
                    "matchedPaperCorpusId": "186206810"
                },
                {
                    "start": 487,
                    "end": 504,
                    "matchedPaperCorpusId": "233024725"
                },
                {
                    "start": 623,
                    "end": 643,
                    "matchedPaperCorpusId": "20667722"
                },
                {
                    "start": 643,
                    "end": 660,
                    "matchedPaperCorpusId": "13206339"
                },
                {
                    "start": 660,
                    "end": 677,
                    "matchedPaperCorpusId": "52143467"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5693359375
        },
        {
            "corpus_id": "267032902",
            "title": "Solving olympiad geometry without human demonstrations",
            "text": "Our method for generating synthetic data is shown in Fig. 3.We first sample a random set of theorem premises, serving as the input to the symbolic deduction engine to generate its derivations.A full list of actions used for this sampling can be found in Extended Data Table 1.In our work, we sampled nearly 1 billion of such premises in a highly parallelized setting, described in Methods.Note that we do not make use of any existing theorem premises from human-designed problem sets and sampled the eligible constructions uniformly randomly.\n\nNext we use a symbolic deduction engine on the sampled premises.The engine quickly deduces new true statements by following forward inference rules as shown in Fig. 3b.This returns a directed acyclic graph of all reachable conclusions.Each node in the directed acyclic graph is a reachable conclusion, with edges connecting to its parent nodes thanks to the traceback algorithm described in Methods.This allows a traceback process to run recursively starting from any node N, at the end returning its dependency subgraph G(N), with its root being N and its leaves being a subset of the sampled premises.Denoting this subset as P, we obtained a synthetic training example (premises, conclusion, proof) = (P, N, G(N)).\n\nIn geometry, the symbolic deduction engine is deductive database (refs.10,17), with the ability to efficiently deduce new statements from the premises by means of geometric rules.DD follows deduction rules in the form of definite Horn clauses, that is, Q(x) \u2190 P 1 (x),\u2026, P k (x), in which x are points objects, whereas P 1 ,\u2026, P k and Q are predicates such as 'equal segments' or 'collinear'.A full list of deduction rules",
            "score": 0.41045944639441756,
            "section_title": "Synthetic theorems and proofs generation",
            "char_start_offset": 5208,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 60
                },
                {
                    "start": 60,
                    "end": 192
                },
                {
                    "start": 192,
                    "end": 276
                },
                {
                    "start": 276,
                    "end": 389
                },
                {
                    "start": 389,
                    "end": 542
                },
                {
                    "start": 544,
                    "end": 608
                },
                {
                    "start": 608,
                    "end": 712
                },
                {
                    "start": 712,
                    "end": 779
                },
                {
                    "start": 779,
                    "end": 943
                },
                {
                    "start": 943,
                    "end": 1147
                },
                {
                    "start": 1147,
                    "end": 1260
                },
                {
                    "start": 1262,
                    "end": 1333
                },
                {
                    "start": 1333,
                    "end": 1441
                },
                {
                    "start": 1441,
                    "end": 1654
                },
                {
                    "start": 1654,
                    "end": 1684
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.462158203125
        },
        {
            "corpus_id": "273185896",
            "title": "Did You Hear That? Introducing AADG: A Framework for Generating Benchmark Data in Audio Anomaly Detection",
            "text": "Recently, (Evans et al. 2024) introduced a diffusion transformer. \n\nLLMs for synthetic data generation -Recent works have shown significant progress in synthetic data generation in the image space in (Liang et al. 2024) (Ramesh et al. 2021) (Sun et al. 2024) (Bae et al. 2023) etc. However, similar progress has not been seen in the audio space. Zeroshot text-to-image generation approaches have expanded the scope of synthetic data applications by enabling the generation of novel image data from unseen textual prompts, highlighting the model's ability to generalize from limited examples (Ramesh et al. 2021). Digiface-1m (Bae et al. 2023) dataset exemplifies the practical applications of these technologies, providing a robust framework for testing and improving face recognition algorithms through access to one million digital face images. (Ye et al. 2022) outlines a method to leverage LLMs to create synthetic datasets produced entirely using pre-trained language models (PLMs) without human interference while emphasizing the efficiency and flexibility of using synthetic datasets to train task-specific models. (Yu et al. 2024) explores generation of training data that not only focuses on diversity, but also addresses inherent biases within the data generated by LLMs. It highlights the critical role of using diversely attributed prompts that enhance quality and utility of synthetic datasets improving model performance across NLP tasks. (Patel, Raffel, and Callison-Burch 2024) presents a tool designed to streamline synthetic data generation using LLMs providing a platform to generate, train, and share data sets and models. \n\nTraining on synthetic data can improve the performance of the model (Nvidia et al. 2024). In (Dubey et al. 2024) it has been used to generate training data for text quality classifiers. \n\nLLMs for planning Chain-of-thought (CoT) prompting has emerged as a powerful technique to enhance the reasoning capabilities of LLMs by generating intermediate reasoning steps, thereby improving performance on complex tasks such as arithmetic and common sense reasoning.",
            "score": 0.409616034664173,
            "section_title": "Related Work",
            "char_start_offset": 9094,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 65
                },
                {
                    "start": 68,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 846
                },
                {
                    "start": 847,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1452
                },
                {
                    "start": 1453,
                    "end": 1642
                },
                {
                    "start": 1645,
                    "end": 1734
                },
                {
                    "start": 1735,
                    "end": 1830
                },
                {
                    "start": 1833,
                    "end": 2103
                }
            ],
            "ref_mentions": [
                {
                    "start": 200,
                    "end": 219,
                    "matchedPaperCorpusId": "266348349"
                },
                {
                    "start": 220,
                    "end": 239,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 259,
                    "end": 276,
                    "matchedPaperCorpusId": "252735071"
                },
                {
                    "start": 591,
                    "end": 611,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 625,
                    "end": 641,
                    "matchedPaperCorpusId": "252735071"
                },
                {
                    "start": 1122,
                    "end": 1137,
                    "matchedPaperCorpusId": "259275123"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.931640625
        },
        {
            "corpus_id": "273695832",
            "title": "On Memorization of Large Language Models in Logical Reasoning",
            "text": "Logical reasoning benchmarks. To evaluate logical reasoning capabilities in LLMs, synthetic benchmarks have been developed to enable scalable generation of samples with varying configurations and difficulty levels (Clark et al., 2020;Giadikiaroglou et al., 2024;Parmar et al., 2024). For instance, DyVal (Zhu et al., 2024) uses directed acyclic graphs to dynamically generate samples on reasoning tasks including deductive, Boolean, and abductive reasoning. (Chen et al., 2024) focus on propositional logical problems involving definite clauses, and synthetically generate variations with different premise orders, such as forward, backward, and shuffled. (Dziri et al., 2024) explore the limitations of LLMs in tasks requiring compositional reasoning, including multiplication, logic grid puzzles, and dynamic programming problems. ZebraLogic (Lin et al., 2024) is an extended benchmark that systematically tests logical reasoning capabilities. BoardgameQA (Kazemi et al., 2024) presents a question-answering dataset characterized by contradictory facts and rules in the questions. PRONTOQA (Saparov & He, 2023) is a synthetic question-answering dataset where each example is generated from a synthetic world model represented in first-order logic. This dataset enables parsing the generated chain of thought into symbolic proofs, facilitating formal analysis. TruthQuest (Mondorf & Plank, 2024) is the most similar task to our work, which provides evaluation samples based on K&K-type of puzzles involving 3-6 person. Our work provides more comprehensive dynamic set of K&K puzzles that support automatic generation of perturbations, solutions and detailed reasoning steps. Moreover, based on this benchmark, we define and measure memorization in reasoning tasks, revealing intricate interplay between memorization and reasoning in LLMs. \n\nImproving reasoning via fine-tuning. Prior work has explored fine-tuning LLMs on synthetic reasoning data to enhance their performance on reasoning. DyVal (Zhu et al., 2024) shows that fine-tuning Llama2-13B-chat on their synthetic reasoning benchmark improves its performance on other popular reasoning benchmarks.",
            "score": 0.40951608146292773,
            "section_title": "Conclusion",
            "char_start_offset": 30685,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 29
                },
                {
                    "start": 30,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1519
                },
                {
                    "start": 1520,
                    "end": 1675
                },
                {
                    "start": 1676,
                    "end": 1839
                },
                {
                    "start": 1842,
                    "end": 1878
                },
                {
                    "start": 1879,
                    "end": 1990
                },
                {
                    "start": 1991,
                    "end": 2157
                }
            ],
            "ref_mentions": [
                {
                    "start": 214,
                    "end": 234,
                    "matchedPaperCorpusId": "211126663"
                },
                {
                    "start": 234,
                    "end": 262,
                    "matchedPaperCorpusId": "267751102"
                },
                {
                    "start": 262,
                    "end": 282,
                    "matchedPaperCorpusId": "269330143"
                },
                {
                    "start": 458,
                    "end": 477,
                    "matchedPaperCorpusId": "267657940"
                },
                {
                    "start": 958,
                    "end": 979,
                    "matchedPaperCorpusId": "259144942"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82373046875
        },
        {
            "corpus_id": "268032846",
            "title": "Benchmarking GPT-4 on Algorithmic Problems: A Systematic Evaluation of Prompting Strategies",
            "text": "Reasoning tasks such as solving school-level math word problems, commonsense reasoning or algorithmic reasoning have always been one of the hardest class of tasks for Large Language Models. Such problems necessitate several 'thinking steps' to arrive at the final solution, which usually needs to be derived from intermediate calculations. \n\nThe machine learning community has proposed a variety of tasks specifically designed to study the ability of neural architectures to generalize in a systematic and compositional way (Lake and Baroni, 2018;Liska et al., 2018;Kim and Linzen, 2020). \n\nInspired by these tasks, we consider the general problem of iteratively simplifying a formula. We consider a problem framework in which it is possible to generate synthetic formulas with different levels of complexity, which can be characterized by two parameters: the maximum nesting depth of any operation in a formula (Nesting), and the maximum number of operands involved in each operation (Operands). Given its generality, this problem framework can be applied to different domains where nested formulas with an arbitrary number of operands can be defined. We thus define three tasks with different levels of complexity: operations on lists of integers, arithmetical operations and algebraic operations. \n\nBy varying the values of the Nesting and Operands parameters, we could define an arbitrary number of data splits for each task, each featuring a different level of difficulty. In our experiments, we consider the nine data splits which result from taking the values in the Cartesian product of the sets N = {2, 3, 4} and O = {2, 3, 4}, representing values of the Nesting and Operands parameters, respectively. In Table 1 we report, as a reference, examples of formulas from the arithmetic task which have been drawn from the nine data splits (-55-41+(-12*-73)-32))+(84-74+((13-27+17-90)-( 15+75+93)+(54+37-62)+(71-23+46-4))-((61+14)-(-32-87)+(68-22-25)-(14*-7)))) \n\nTable 1: Examples of inputs of the arithmetic task for the nine data splits considered.",
            "score": 0.40788442531596325,
            "section_title": "Reasoning Tasks",
            "char_start_offset": 6579,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 339
                },
                {
                    "start": 342,
                    "end": 588
                },
                {
                    "start": 591,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1299
                },
                {
                    "start": 1302,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1710
                },
                {
                    "start": 1711,
                    "end": 1964
                },
                {
                    "start": 1967,
                    "end": 2054
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2509765625
        },
        {
            "corpus_id": "268247488",
            "title": "Key-Point-Driven Data Synthesis with its Enhancement on Mathematical Reasoning",
            "text": "In this paper, we propose a new data synthesis paradigm that is focused on the generation of large-scale, high-quality, symbolically-driven training datasets.Leveraging this paradigm, we have developed an extensive synthetic dataset tailored for mathematical reasoning.By utilizing this data set, our fine-tuned model achieved excellent performance in multiple data sets including MATH and GSM8K, and the performance exceeded all 7B to 70B competitors.Our research underscores the efficacy of integrating key points in data synthesis and applying stringent quality control protocols to both questions and answers.\n\nPre-print",
            "score": 0.4064586197702927,
            "section_title": "Conclusion",
            "char_start_offset": 23823,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 158,
                    "end": 269
                },
                {
                    "start": 269,
                    "end": 452
                },
                {
                    "start": 452,
                    "end": 613
                },
                {
                    "start": 615,
                    "end": 624
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90380859375
        },
        {
            "corpus_id": "272826678",
            "title": "Towards Efficient and Robust VQA-NLE Data Generation with Large Vision-Language Models",
            "text": "In this section, we present a comprehensive description of our synthetic data generation meth-ods, including detailed explanations of the three prompting pipelines employed. Formally, we define T = {T 1 , \u2022 \u2022 \u2022 , T N }, where T i = (q i , a i , e i ) represent our synthetic data triplet, comprising a question q i , an answer a i , and an explanation e i . We define P i = {p 1 , p 2 , . . . } as the set of prompts used to generate T i with model M and input data D i . The process of generating each triplet T i from a sample D i can be expressed using inference denoted by the function f that relates these variables: \n\nNext, we employ three distinct prompting pipelines to generate T i , guided by the formulation in Equation 1. Each pipeline involves a two-step process: First, we produce contextually relevant q i and a i from the input data D i . Second, we generate an explanation e i to justify the answer a to the corresponding question q i . The distinctiveness of each pipeline is attributed to the specific prompting and pre-processing techniques used, as illustrated in Figure 2 and elaborated in the following explanations.",
            "score": 0.4064586197702927,
            "section_title": "Methodology",
            "char_start_offset": 3542,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 621
                },
                {
                    "start": 624,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 854
                },
                {
                    "start": 855,
                    "end": 953
                },
                {
                    "start": 954,
                    "end": 1139
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.791015625
        },
        {
            "corpus_id": "272770433",
            "title": "CraftRTL: High-quality Synthetic Data Generation for Verilog Code Models with Correct-by-Construction Non-Textual Representations and Targeted Code Repair",
            "text": "In this work, we refer to synthetic data generation as methods of using large language models (LLMs) in data generation. While our approach-ensuring correctness through correct-byconstruction-could also be considered \"synthetic\" and resembles methods explored in works like AlphaGeometry (Trinh et al., 2024), our problems are much simpler and on a smaller scale. Our observations about the variability of models on specific problems align with the findings of Meta AI (2024), where \"the model knows how to produce the right answer, but it does not know how to select it.\" Instead of striving for absolute data correctness, preference learning (Rafailov et al., 2024;Ethayarajh et al., 2024) or reinforcement learning (Bai et al., 2022;Le et al., 2022), we generate targeted repair data by analyzing errors and re-create such scenarios by injecting similar errors into open-source code, somewhat analogous to how humans consolidate memories during sleep by integrating new information with past experiences (Walker & Stickgold, 2004;Stickgold, 2005).",
            "score": 0.4064586197702927,
            "section_title": "DISCUSSIONS",
            "char_start_offset": 27160,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 1050
                }
            ],
            "ref_mentions": [
                {
                    "start": 288,
                    "end": 308,
                    "matchedPaperCorpusId": "267032902"
                },
                {
                    "start": 644,
                    "end": 667,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 1007,
                    "end": 1033,
                    "matchedPaperCorpusId": "13213984"
                },
                {
                    "start": 1033,
                    "end": 1049,
                    "matchedPaperCorpusId": "268230871"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66162109375
        },
        {
            "corpus_id": "277502040",
            "title": "OpenCodeReasoning: Advancing Data Distillation for Competitive Coding",
            "text": "Large Language Models (LLMs) have demonstrated a remarkable capacity to excel at a variety of coding capabilities (Hui et al., 2024;Li et al., 2023a;Guo et al., 2024;Roziere et al., 2023;Ahmad et al., 2021). While fine-tuning on code question-solution pairs has led to much of the improved performance, high-quality human-labeled data is limited and expensive to curate. To overcome this bottleneck, many have successfully leveraged LLMs to generate high-quality synthetic code data (Luo et al., 2024;Yu et al., 2024). Notably, works like Wei et al. (2024a) and Huang et al. (2025) have generated diverse instruction-solution pairs, subsequently fine-tuning base models to achieve top results in HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021) and BigCodeBench (Zhuo et al., 2025) benchmarks. \n\nSince the successes of synthetic data generation for code, reasoning-based LLMs have presented the next paradigm in advancing large language model capabilities (Team et al., 2025). Past works such as DeepSeek-AI et al. (2025) have led the way in improving LLM capabilities on reasoning oriented tasks such as math and coding by leveraging large-scale reinforcement learning (Luo et al., 2025) and rule-based reward models. By continuously applying reinforcement learning (RL) with a ground truth verifier on a task such as coding, models learn to apply continuous test-time computation to solve more difficult reasoningbased tasks (Hosseini et al., 2024;Setlur et al., 2025). \n\nGiven the improvements from fine-tuning on synthetic data and reasoning capabilities for coding capabilities, many works have found continuous enhancements by combining the two. This has involved distilling the chain-of-thought responses (Wei et al., 2022) from better reasoning models to smaller student models by means of supervised fine-tuning (SFT).",
            "score": 0.40639484920604296,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 802
                },
                {
                    "start": 805,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1227
                },
                {
                    "start": 1228,
                    "end": 1480
                },
                {
                    "start": 1483,
                    "end": 1660
                },
                {
                    "start": 1661,
                    "end": 1836
                }
            ],
            "ref_mentions": [
                {
                    "start": 187,
                    "end": 206,
                    "matchedPaperCorpusId": "232185260"
                },
                {
                    "start": 483,
                    "end": 501,
                    "matchedPaperCorpusId": "259164815"
                },
                {
                    "start": 501,
                    "end": 517,
                    "matchedPaperCorpusId": "270258158"
                },
                {
                    "start": 1436,
                    "end": 1459,
                    "matchedPaperCorpusId": "267617275"
                },
                {
                    "start": 1459,
                    "end": 1479,
                    "matchedPaperCorpusId": "276422443"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.900390625
        },
        {
            "corpus_id": "268553593",
            "title": "Agentic AI: The Era of Semantic Decoding",
            "text": "Data, itself composed of semantic tokens, can become the focal point for Flows dedicated to manipulating or synthesizing data. Similarly, specialized trainer Flows can be aimed at training semantic processors (e.g., models) or even entire (sub-)Flows. By combining trainer Flows with synthetic data generation Flows, opportunities emerge for creating sophisticated self-training loops. The synthetic data generation Flows can leverage domain knowledge (Tang et al., 2023), task properties (Lu et al., 2024;Veselovsky et al., 2023;Josifoski et al., 2023c), or collaboration (Abdullin et al., 2024), and synthesize data of notably higher quality than what a single model or simple heuristics can achieve. This sets the stage for effective self-improvement loops where a language model participates in a semantic decoding algorithm producing high-quality synthetic data. Then, the language model improves itself through fine-tuning, thereby improving the Flow's capacity to generate even better synthetic data in a virtuous cycle (Silver et al., 2017;Burns et al., 2023;Singh et al., 2023;Chen et al., 2024b). An example of such a Flow is MAGDi (Chen et al., 2024a), a framework designed to distill reasoning interactions among multiple LLMs into smaller ones. This approach surpasses single-teacher distillation (Li et al., 2023c;Magister et al., 2023) and finetuning based on reasoning trajectories sampled from GPT-4 (Chen et al., 2023a).",
            "score": 0.40463605752393145,
            "section_title": "Synthetic Data Flow",
            "char_start_offset": 45140,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1438
                }
            ],
            "ref_mentions": [
                {
                    "start": 530,
                    "end": 554,
                    "matchedPaperCorpusId": "257378179"
                },
                {
                    "start": 1310,
                    "end": 1328,
                    "matchedPaperCorpusId": "259251773"
                },
                {
                    "start": 1328,
                    "end": 1350,
                    "matchedPaperCorpusId": "254823156"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95166015625
        },
        {
            "corpus_id": "273233870",
            "title": "The Cognitive Capabilities of Generative AI: A Comparative Analysis with Human Benchmarks",
            "text": "Results demonstrate that Generative AI models are capable of exceptional performance compared to normative human ability in key domains of cognition. First, and perhaps not surprising, all models demonstrate exceptional capabilities in storage, retrieval, and manipulation of arbitrary tokenized information such as sequences of numbers and letters while being significantly weaker in mathematical reasoning. While poor performance was most pronounced in small parameter models, the discrepancy between reasoning and information management persisted across model generations and developers indicating a generally stable discrepancy in ability. \n\nGenerative models also demonstrated exceptional verbal abilities, with variability in performance according to the size of the model. Once again, models were strongest in retrieval of stored information with consistent relative weaknesses in tasks that require understanding of linguistic concepts or the relationships between words and concepts. However, with the exception of Gemini Nano, all models demonstrated understanding as well as crystalized knowledge well above normative ability indicating that models generally excel in language-based tasks, even those requiring reasoning and understanding beyond simple regurgitation of acquired knowledge. In the case of small parameter models, they may be best for storing and retrieving information naturalistically (e.g., in the context of hardware constraints) but may not be capable of understanding or manipulating that information to solve a problem. \n\nFinally, the dramatically poorer performance on visual processing tasks indicates that generative models, as they stand today, have profound deficits in the ability to understand the meaning or There is some indication that current modeling approaches can lead to the acquisition of these abilities as Claude 3.5 Sonnet demonstrated profound increases over the previous generation (Claude 3 Opus). Claude 3.5 Sonnet showed advances over its predecessor in Matrix Reasoning (0.1 \u210e %ile vs. 25 \u210e %ile), which measures the ability to detect meaningful patterns in visual stimuli and Figure Weights which indexes the ability to understand and reason on mathematical relationships that are visually presented (0.1 \u210e %ile vs. 50 \u210e %ile). No such improvement was observed in visual puzzle solving or image completion tasks that test the ability to understand and make sense of visual information.",
            "score": 0.40438568719097334,
            "section_title": "Discussion",
            "char_start_offset": 22048,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 643
                },
                {
                    "start": 646,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1552
                },
                {
                    "start": 1555,
                    "end": 1952
                },
                {
                    "start": 1953,
                    "end": 2286
                },
                {
                    "start": 2287,
                    "end": 2444
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.097412109375
        },
        {
            "corpus_id": "247762790",
            "title": "STaR: Bootstrapping Reasoning With Reasoning",
            "text": "Human decision-making is often the result of extended chains of thought [1,2]. Recent work has shown that explicit intermediate reasoning (\"rationales\") can improve large language model (LLM) performance as well [3][4][5][6][7][8]. For example, [5] demonstrated that LLMs explicitly trained to use \"scratchpads\" for intermediate steps can attain perfect in-distribution performance on arithmetic, and strong out-of-distribution generalization, while models trained to predict answers directly fail to do either. These works suggest that generating explicit rationales before giving a final answer (\"rationale generation\") is valuable for LLMs across diverse tasks including mathematical reasoning, commonsense reasoning, code evaluation, social bias inference, and natural language inference. However, the two primary methods for inducing rationale generation both have serious drawbacks. \n\nOne approach to rationale generation is the construction of a fine-tuning dataset of rationales, either manually by human annotators or automatically with hand-crafted templates [3][4][5]9]. Manual methods are expensive, and it is infeasible to construct such a dataset for each interesting problem [3]. Meanwhile, template-based methods rely on automatically-generated rationales but only work when a general solution is already known [5] or reasonable hard-coded heuristics can be made [4]. \n\nAn alternative is to leverage in-context learning by including only a few rationale examples in the language model prompt. This has been shown to improve accuracy on mathematical and symbolic reasoning tasks relative to prompting without rationales (\"direct\" prompting) [5,6]. Yet, while fewshot techniques with rationales tend to outperform their non-reasoning counterparts, they generally substantially underperform models fine-tuned to directly predict answers using larger datasets [5,6].",
            "score": 0.4037058988862527,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 79,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 888
                },
                {
                    "start": 891,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1383
                },
                {
                    "start": 1386,
                    "end": 1508
                },
                {
                    "start": 1509,
                    "end": 1662
                },
                {
                    "start": 1663,
                    "end": 1878
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1549072265625
        },
        {
            "corpus_id": "276318012",
            "title": "MIH-TCCT: Mitigating Inconsistent Hallucinations in LLMs via Event-Driven Text-Code Cyclic Training",
            "text": "Recent studies have emphasized the effectiveness of synthetic datasets in mitigating inconsistent hallucinations related to specific tasks. For instance, synthetic mathematical datasets (Toshniwal et al., 2024;Huang et al., 2024) have shown reduced contradictions in the calculation process and significant improvements in solving mathematical problems. Additionally, some research has synthesized reasoning data in the logical reasoning (Nie et al., 2020;Saeed et al., 2021) field while providing the reasoning process (Dalvi et al., 2021) during synthesis. LOGIC-LM and SymbCOT (Pan et al., 2023;Xu et al., 2024) integrate LLMs with symbolic solvers to enhance logical problem-solving. Unfortunately, the synthetic datasets that have been developed are often tailored to specific tasks or domains, which can restrict their generalizability. \n\nAdvancements in training LLMs on extensive code datasets, such as CoCoGen and CodeRL, have significantly reduced inconsistent hallucinations in code generation tasks (Madaan et al., 2022;Le et al., 2022). However, these improvements have not effectively transferred to general natural language tasks due to fundamental differences in semantics, language styles, and structures between code and natural language. The precision of code contrasts with the ambiguity of human language, posing challenges in adapting code-focused techniques for natural language understanding and generation. Recent approaches, such as program-ofthought (Chen et al., 2023) and program-assisted LMs (Gao et al., 2023), have sought to bridge this divide by interpreting natural language problems and generating corresponding code solutions. However, these methodologies fail to successfully transfer the logical consistency capabilities gained from code training to a broader spectrum of NLP tasks. \n\nInspired by code-oriented LLMs that excel in generating logically consistent code, we propose that integrating code data is vital for enhancing the ability of LLMs to maintain logical consistency across various NLP applications. Furthermore, we observe that there exists a correspondence between the structure of event-based text and that of code.",
            "score": 0.40291844636989843,
            "section_title": "Inconsistent Hallucinations Explanation",
            "char_start_offset": 1803,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 842
                },
                {
                    "start": 845,
                    "end": 1049
                },
                {
                    "start": 1050,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1662
                },
                {
                    "start": 1663,
                    "end": 1820
                },
                {
                    "start": 1823,
                    "end": 2051
                },
                {
                    "start": 2052,
                    "end": 2170
                }
            ],
            "ref_mentions": [
                {
                    "start": 438,
                    "end": 456,
                    "matchedPaperCorpusId": "207756753"
                },
                {
                    "start": 456,
                    "end": 475,
                    "matchedPaperCorpusId": "237940562"
                },
                {
                    "start": 520,
                    "end": 540,
                    "matchedPaperCorpusId": "233297051"
                },
                {
                    "start": 580,
                    "end": 598,
                    "matchedPaperCorpusId": "258833332"
                },
                {
                    "start": 1011,
                    "end": 1032,
                    "matchedPaperCorpusId": "252873120"
                },
                {
                    "start": 1032,
                    "end": 1048,
                    "matchedPaperCorpusId": "250280117"
                },
                {
                    "start": 1477,
                    "end": 1496,
                    "matchedPaperCorpusId": "253801709"
                },
                {
                    "start": 1522,
                    "end": 1540,
                    "matchedPaperCorpusId": "253708270"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.859375
        },
        {
            "corpus_id": "277104955",
            "title": "Synthetic Data Generation Using Large Language Models: Advances in Text and Code",
            "text": "We are likely to see even more innovative solutions, like integrated human-in-the-loop generation and domain-specific synthetic data models, being realized soon. \n\nIn conclusion, LLM-based synthetic data generation stands as a promising paradigm shift in how we obtain training data for AI. By leveraging the generative power of advanced models, we reduce our reliance on large hand-curated datasets and open up opportunities to develop AI systems in domains and languages that suffer from data scarcity. This democratizes the development of AI, enabling customization and improvement even when real data is lacking. \n\nAs we refine these techniques, backed by rigorous evaluations and ethical safeguards, synthetic data generation will become an increasingly standard part of the machine learning toolkit-complementing real data, accelerating development, and perhaps one day even supplanting the need for certain kinds of manual data collection altogether. \n\nThe synergy between learning from data and generating data to learn from encapsulates a fascinating frontier for AI research, one that blurs the line between model and data and pushes us toward more autonomous, self-improving AI systems.",
            "score": 0.4024982055363106,
            "section_title": "Conclusion",
            "char_start_offset": 66340,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 164,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 616
                },
                {
                    "start": 619,
                    "end": 957
                },
                {
                    "start": 960,
                    "end": 1197
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93896484375
        },
        {
            "corpus_id": "276903767",
            "title": "SCoRE: Benchmarking Long-Chain Reasoning in Commonsense Scenarios",
            "text": "In this paper, we introduce SCoRE, a bilingual dataset designed to evaluate complex logical reasoning in commonsense scenarios through multi-hop inference chains generated through a knowledge-driven synthetic data strategy. The dataset contains 100,000 questions across three difficulty levels and commonsense in four domains. Our experimental results show that, despite advances in areas like code generation and math problem-solving, LLMs still face challenges in commonsense reasoning. Case studies reveal common errors, such as misinterpreting low-frequency commonsense, logical inconsistencies, and overthinking. The proposed data synthesis approach enables automated generation of large-scale, high-accuracy reasoning data with QA pairs, which could serve as training data to further improve LLMs' reasoning capabilities.",
            "score": 0.40206750702346616,
            "section_title": "Conclusion",
            "char_start_offset": 23841,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 488
                },
                {
                    "start": 489,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 827
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.744140625
        },
        {
            "corpus_id": "276250072",
            "title": "GSM-Infinite: How Do Your LLMs Behave over Infinitely Increasing Context Length and Reasoning Complexity?",
            "text": "With the synthetic problem generators detailed in Section 4, we then use them to generate problems to build a suite of reasoning tasks with increasing complexity. For the brevity of reference, we refer to the generated problems with only explicit operations as \"Easy\", the generated problems with 2-entity variables at maximum as \"Medium\", and the generated problems with 3-entity variables at maximum as \"Hard. \n\nIdeally, when evaluating an LLM, we want to evaluate all difficulty levels, from the most basic logic complexity to when it completely fails to solve any problem. For the Easy subset of problems, it usually leads to large operation counts for powerful LLMs. However, although complexity-wise not challenging, LLMs trained with internal COT tend to generate very long arguments, saturating their API output generation limit (4K for many models). Thus, we observe a sudden decay in accuracy in large ops, not because of LLMs' ability bottlenecks, but because of the above-mentioned nuance. Thus, we make a tweak to its problem: Instead of asking the LLM to find the value of Table 1 18 selected models are evaluated on GSM-\u221e zero-noise benchmarks using Area-Under-Curve (AUC), which is computed by taking the Riemann Sum of accuracy versus op count from 2 to when the model accuracy drops below 5%. We also present detailed statics of the first op number for the model to have an accuracy lower than 50%, 10%, and the average accuracy of the first 30 ops settings. Besides, we also highlight the reasoning models, linear attention hybrid models, and SSM hybrid models. Due to space constraint, \"Mistral-Large-Instruct-2411\" is shortened as \"Mistral-Large\";\"Claude-3.5-Sonnet\" and \"Claude-3.5-Haiku\" has version number 20241022; \"GPT-4o-2024-11-20\" is shortened as \"GPT-4o\" and \"GPT-4o-mini-2024-07-18\" is shortened as \"GPT-4o-mini\". one variable, we ask the LLM to find all the variables that have some value specified, effectively increasing the difficulty of the problem.",
            "score": 0.40144762803603395,
            "section_title": "Benchmark Details",
            "char_start_offset": 19728,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 411
                },
                {
                    "start": 414,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1476
                },
                {
                    "start": 1477,
                    "end": 1580
                },
                {
                    "start": 1581,
                    "end": 1710
                },
                {
                    "start": 1711,
                    "end": 1844
                },
                {
                    "start": 1845,
                    "end": 1985
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10174560546875
        },
        {
            "corpus_id": "270199509",
            "title": "Unveiling the Impact of Coding Data Instruction Fine-Tuning on Large Language Models Reasoning",
            "text": "Table 1: Task data statistics.To thoroughly assess the models' reasoning capabilities, we evaluate them across twelve generative tasks, spanning three distinct types of reasoning: symbolic, logical, and arithmetic, shown in Table 1.Symbolic: We focus on four tasks [Wei et al., 2022b]: (1) First Letter Concatenation, (2) Last Letter Concatenation, (3) Reverse List, and (4) Coin Flip.Logical: We utilize four tasks, requiring strong logical ability, for evaluation: (1) Cluttr [Sinha et al., 2019], (2) List Functions [Rule, 2020], (3) Babi-Induction and (4) Babi-Deduction [Weston et al., 2015].Arithmetic: Four arithmetic benchmarks are involved to comprehensive evaluating the mathematics world problem-solving ability (1) GSM8K [Cobbe et al., 2021], (2) SVAMP [Patel et al., 2021], (3) ASDiv [Miao et al., 2020], and ( 4) MAWPS [Koncel-Kedziorski et al., 2016].For symbolic reasoning, we generate synthetic datasets following Fortes [2023].We ensure a balanced representation of difficulty levels for each task.For example, we generate 500 instances for names containing 2 to 4 words in letter concatenation tasks.For other tasks, we evaluate the models with the test sets for each task when publicly available.Otherwise, we use the development sets instead.We defer detailed information on the reasoning domain selection and synthetic datasets generation to the Appendix A.3.",
            "score": 0.400699795923277,
            "section_title": "Task Description",
            "char_start_offset": 11690,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 30
                },
                {
                    "start": 30,
                    "end": 232
                },
                {
                    "start": 232,
                    "end": 385
                },
                {
                    "start": 385,
                    "end": 597
                },
                {
                    "start": 597,
                    "end": 866
                },
                {
                    "start": 866,
                    "end": 945
                },
                {
                    "start": 945,
                    "end": 1016
                },
                {
                    "start": 1016,
                    "end": 1119
                },
                {
                    "start": 1119,
                    "end": 1216
                },
                {
                    "start": 1216,
                    "end": 1263
                },
                {
                    "start": 1263,
                    "end": 1381
                }
            ],
            "ref_mentions": [
                {
                    "start": 797,
                    "end": 816,
                    "matchedPaperCorpusId": "220047831"
                },
                {
                    "start": 833,
                    "end": 865,
                    "matchedPaperCorpusId": "2228719"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.46923828125
        },
        {
            "corpus_id": "268032846",
            "title": "Benchmarking GPT-4 on Algorithmic Problems: A Systematic Evaluation of Prompting Strategies",
            "text": "In this work, we investigated the effectiveness of a variety of LLMs prompting methods on three algorithmic tasks designed to analyse how systematic reasoning capabilities might emerge in relation to an increase in problem complexity. We also compared the performance of state-of-the-art LLMs with that of a much simpler neural architecture, the Neural Data Router, specifically designed to solve algorithmic problems characterized by a recursive structure. \n\nWe found that none of the models and prompting methods could exhibit proper systematic generalization capacities. Although the performance of the ad-hoc Neural Data Router model was competitive with that of a general purpose language model such as GPT-3.5, it turns out that more advanced language models like GPT-4 currently represent the state-of-the-art for solving this type of reasoning tasks, even on relatively complex problem instances. By comparing the performance of the different prompting methods, we found that explicitly producing reasoning steps in a verbal form can generally improve model performance on all tasks. However, our experiments also highlight the limitations these prompting methods, showing that their effectiveness could be limited for complex symbolic reasoning problems. \n\nWhile synthetic algorithmic problems are quite far from the typical scenario of application of Large Language Models, they are useful for precisely characterizing the limitations of such models, as done in the present work. Future work could extend our analysis by expanding the set of prompting methods considered, for example by including Least-to-Most prompting (Zhou et al., 2023), which could in principle be applicable to problems requiring to solve nested formulas. Other nuances of prompts themselves could also be taken into account, such as the phrasing used when contextualizing the agent role, the number of exemplars provided in the fewshot learning regimen, or the way rationales for Chain-of-Thought prompting are constructed. \n\nResearchers working on the design of prompting techniques for Large Language Models could propose novel methods to improve the capacity of the models to apply solution strategies that generalize well on complex problem instances. More precisely, future research could be dedicated to understand the extent to which current limitations depend on the ability to effectively retrieve and manipulate information in a very large context, a scenario which could become more and more common in future dialogue-based real-world applications of LLMs.",
            "score": 0.4005223404018934,
            "section_title": "Conclusion",
            "char_start_offset": 25597,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 457
                },
                {
                    "start": 460,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1263
                },
                {
                    "start": 1266,
                    "end": 1489
                },
                {
                    "start": 1490,
                    "end": 1738
                },
                {
                    "start": 1739,
                    "end": 2007
                },
                {
                    "start": 2010,
                    "end": 2239
                },
                {
                    "start": 2240,
                    "end": 2551
                }
            ],
            "ref_mentions": [
                {
                    "start": 1631,
                    "end": 1650,
                    "matchedPaperCorpusId": "248986239"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.34619140625
        },
        {
            "corpus_id": "276079713",
            "title": "Spend Wisely: Maximizing Post-Training Gains in Iterative Synthetic Data Boostrapping",
            "text": "A growing focus in the community is on post-training regimes, where models first acquire general knowledge during pretraining but require further refinement to excel in complex problem-solving tasks, such as agentic behavior (Shao et al., 2023), math reasoning (Poesia et al., 2024), and advanced code generation (Guo et al., 2024a). In these settings, the availability of high-quality human-annotated data often becomes a significant bottleneck due to the demand for expert input and substantial resources. \n\nTo address this limitation, synthetic data have emerged as a promising alternative, offering scalability and costeffectiveness. Despite concerns about potential risks of model collapse (Shumailov et al., 2024;Dohmatob et al., Figure 1. Iterative learning with synthetic data. In this framework, synthetic data is generated, filtered using a reward model, and the selected data is used to further train the generator. The budget policy is defined as the quantity of data retained after selection, nt. Our goal is to identify the optimal policy across iterations to achieve the best final performance, given a fixed budget. 2024c), synthetic data is typically selected and verified before use in post-training (Feng et al., 2024;Setlur et al., 2024), ensuring its quality. A common paradigm for leveraging synthetic data employs an iterative bootstrapping process (Zelikman et al., 2022;Trung et al., 2024): the model generates synthetic data, rewards or verifiers are used to filter and select high-quality data, and the model is then fine-tuned on selected data. This process is repeated iteratively to fully improve performance. An illustration of this approach is provided in Figure 1. \n\nHowever, for practitioners implementing this approach, an important question arises: how should a fixed computational budget be allocated to decide the amount of synthetic data to generate and select at each iteration to maximize performance? \n\nIn this paper, we establish foundational principles for crafting optimal strategies for synthetic data generation across iterations. To the best of our knowledge, this is the first attempt to address this problem.",
            "score": 0.40049478399282545,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 507
                },
                {
                    "start": 510,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 1009
                },
                {
                    "start": 1010,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1572
                },
                {
                    "start": 1573,
                    "end": 1639
                },
                {
                    "start": 1640,
                    "end": 1697
                },
                {
                    "start": 1700,
                    "end": 1942
                },
                {
                    "start": 1945,
                    "end": 2077
                },
                {
                    "start": 2078,
                    "end": 2158
                }
            ],
            "ref_mentions": [
                {
                    "start": 225,
                    "end": 244,
                    "matchedPaperCorpusId": "264145862"
                },
                {
                    "start": 261,
                    "end": 282,
                    "matchedPaperCorpusId": "270870392"
                },
                {
                    "start": 695,
                    "end": 719,
                    "matchedPaperCorpusId": "271448069"
                },
                {
                    "start": 1218,
                    "end": 1237,
                    "matchedPaperCorpusId": "271213167"
                },
                {
                    "start": 1237,
                    "end": 1257,
                    "matchedPaperCorpusId": "270620658"
                },
                {
                    "start": 1372,
                    "end": 1395,
                    "matchedPaperCorpusId": "247762790"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93017578125
        },
        {
            "corpus_id": "273022743",
            "title": "Targeted synthetic data generation for tabular data via hardness characterization",
            "text": "Training complex machine learning models requires large amounts of data, but in real-world applications, data may be of poor quality, insufficient in amount, or subject to privacy, safety, and regulatory limitations. Such challenges have sparked an interest in synthetic data generators (SDGs), which use available training data to generate realistic synthetic samples [Lu et al., 2024]. In this work, we argue that when the objective is to use synthetic data to make an existing machine learning model better generalize to unseen data, augmenting only the hardest training points is more effective and has significant computational advantages compared to augmenting the entire training dataset. In other words, we propose a novel scalable targeted synthetic data gen-eration framework, focusing on binary classification tasks. The underlying intuition is that within a dataset, some of the observations are obvious to classify, whereas others play a more significant role in determining the decision boundary of the trained model. Focusing only on these harder examples when generating additional data may allow the model to efficiently learn a more robust decision boundary with the additional advantage of reducing computational costs associated with the training of a synthetic data generator. \n\nIn particular, the method proposed in this work is specifically targeted toward binary classifiers on tabular data, which often require adaptations such as oversampling, undersampling, or synthetic data generation to improve model performance under class imbalance [Chawla et al., 2002, He et al., 2008]. Although deep neural networks have proven successful across domains such as images, audio, or text, they are still regularly outperformed on tabular data by simpler and more interpretable tree-based architectures [Shwartz-Ziv and Armon, 2021]. Therefore, we choose trees as the reference model for binary classification on tabular data, while deep learning models are used for synthetic data generation. In general, we will use D = {(x i , y i )} n i=1 to denote the training data, where x i \u2208 R d is a d-dimensional feature vector and y i \u2208 {0, 1} is a binary label.",
            "score": 0.39924575745298296,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 1031
                },
                {
                    "start": 1032,
                    "end": 1297
                },
                {
                    "start": 1300,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1848
                },
                {
                    "start": 1849,
                    "end": 2008
                },
                {
                    "start": 2009,
                    "end": 2172
                }
            ],
            "ref_mentions": [
                {
                    "start": 1565,
                    "end": 1585,
                    "matchedPaperCorpusId": "1554582"
                },
                {
                    "start": 1585,
                    "end": 1603,
                    "matchedPaperCorpusId": "1438164"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.857421875
        },
        {
            "corpus_id": "272421662",
            "title": "A Systematic Review of Synthetic Data Generation Techniques Using Generative AI",
            "text": "Two studies examined synthetic data generation methods, focusing on different aspects. The first study analyzed algorithms for tabular and latent space data, proposing a unified taxonomy and reviewing 70 algorithms across six ML problems [19]. It discusses metrics for evaluating synthetic data quality but faces limitations, including a lack of focus on real-world application challenges, potential biases in the selected algorithms, and insufficient analysis of scalability in large industrial datasets, highlighting the need for further research to validate practical effectiveness. The second study focused on GANs and their applications, providing an introduction, an overview of synthetic data generation methods, an examination of GAN architectures, and an exploration of evaluation methods [23]. However, it predominantly focuses on GAN-based techniques, potentially overlooking other innovative methods, lacking empirical benchmarks and direct performance comparisons of GAN models, and providing limited practical guidance on implementing evaluation techniques, which could hinder real-world application. \n\nFurthermore, a study reviewed recent advancements in generating synthetic data using large language models (LLMs) [14]. It discusses methodologies, evaluation techniques, and applications, with a focus on addressing data scarcity and privacy issues in various domains. This paper is structured into sections covering an introduction, methods for generating synthetic data, application scenarios, challenges, and future research directions. However, the scope is broad, potentially leading to oversimplification of domain-specific challenges. In addition, the discussion on practical deployment scenarios is limited, providing insufficient guidance for real-world implementations. \n\nAll the above limitations collectively highlight the necessity for a more comprehensive literature review that not only synthesizes existing research but also addresses the gaps and inconsistencies in current knowledge. Unlike previous studies that focused narrowly on deep learning methods or specific applications, this literature review thoroughly covers all the major technologies and models used for synthetic data generation, including generative adversarial networks (GANs), variational autoencoders (VAEs), and large language models (LLMs) such as GPT-3 and BERT. We compare these models across various dimensions such as performance, scalability, data diversity, and practical implementation. In doing so, this review aims to provide a nuanced understanding of each model's strengths and weaknesses, facilitating more informed decisions for future research and application in synthetic data generation.",
            "score": 0.398120524765276,
            "section_title": "Primary Study Year Overview Limitations",
            "char_start_offset": 22002,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 86
                },
                {
                    "start": 87,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 803
                },
                {
                    "start": 804,
                    "end": 1114
                },
                {
                    "start": 1117,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1556
                },
                {
                    "start": 1557,
                    "end": 1658
                },
                {
                    "start": 1659,
                    "end": 1796
                },
                {
                    "start": 1799,
                    "end": 2018
                },
                {
                    "start": 2019,
                    "end": 2370
                },
                {
                    "start": 2371,
                    "end": 2500
                },
                {
                    "start": 2501,
                    "end": 2710
                }
            ],
            "ref_mentions": [
                {
                    "start": 238,
                    "end": 242,
                    "matchedPaperCorpusId": "259503802"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.869140625
        },
        {
            "corpus_id": "268230871",
            "title": "LAB: Large-Scale Alignment for ChatBots",
            "text": "Xu et al. (2023) introduces Evol-Instruct, another variant of Self-Instruct, that synthesizes iteratively more complex instruction to overcome shortcomings of previous methods. Mukherjee et al. (2023), Mitra et al. (2023) present a synthetic data generation approach to enhance task diversity and scalability, alongside a progressive training framework aimed at improving the model's reasoning ability and response style to match teacher models. This is achieved by generating rich reasoning signals in the generated answer and progressively training on datasets of varying difficulty in incremental phases. \n\nSimilar to LAB, concurrent work, GLAN (Li et al., 2024), employs a semi-automatic approach to synthetic data generation that uses a human-curated taxonomy to generate instruction tuning data from a teacher model. However, as explained in section 3.2.2, unlike LAB, GLAN cannot be used to generate synthetic data from domains that are not captured in the teacher model's support. As such, while LAB uses the open-source Mixtral model as the teacher, like many other synthetic data generation approaches, GLAN has to rely on a large proprietary model (GPT-4). This poses complicated questions about the usability of generated data (especially for commercial purposes) since the terms of use of proprietary models typically forbid using the model to improve other models.",
            "score": 0.39711861865809817,
            "section_title": "RELATED WORK",
            "char_start_offset": 6021,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 607
                },
                {
                    "start": 610,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1378
                }
            ],
            "ref_mentions": [
                {
                    "start": 648,
                    "end": 665,
                    "matchedPaperCorpusId": "259129398"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.951171875
        },
        {
            "corpus_id": "272146630",
            "title": "Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal Sampling",
            "text": "As a result, several recent works aim to understand the utility of the weak but cheaper LMs in comparison to the strong but expensive LMs for reasoning. Specifically, Brown et al. (2024); Snell et al. (2024); Song et al. (2024) show that the solve rate of the small LMs can increase significantly with repeated sampling. In addition, Hassid et al. (2024) demonstrate that repeated generations from smaller LMs can outperform the data generated by larger LMs at a fixed sampling computational budget during inference for coding tasks. In this work, we go beyond these works and show the utility of the synthetic data from the small LMs for training strong reasoners across a diverse set of supervised finetuning setups.",
            "score": 0.397114205704469,
            "section_title": "Related Work",
            "char_start_offset": 41122,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 718
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90234375
        },
        {
            "corpus_id": "270521623",
            "title": "Benchmarking Generative Models on Computational Thinking Tests in Elementary Visual Programming",
            "text": "We generate the data using the same generation pipeline. However, we have the flexibility to vary the nature of synthetic evaluation data and how it differs from its training counterpart. We do this by conditioning the types of generated codes C (i.e., concepts covered, nesting structure). Thus, we obtain four parts of the synthetic evaluation dataset: \n\n\u2022 HOC-SYNTH: this part corresponds to 758 solution synthesis tasks generated using the same codes as used for training but with newly generated grids. In particular, the samples encountered during evaluation are similar to samples encountered during training in terms of codes (output) but not in terms of grids (input). \u2022 HOC-SYNTH hard : this part corresponds to 378 hard solution synthesis tasks that are a subset of 758 HOC-SYNTH tasks. We selected those tasks for which the solution code requires a condition and are generally more difficult for generative models. \u2022 HOC-FILTERED: this part corresponds to 758 solution synthesis tasks. These are also generated using the same generation pipeline as used for generating the training datasets; however, we have ensured that the samples are distinct from training both in terms of codes (output) and grids (inputs). \u2022 HOC-OOD: this part corresponds to 100 solution synthesis tasks, which are \"out-ofdistribution\" (OoD), meaning that the solution structures for these tasks are different and more complex than those used in training datasets. For instance, this set contains tasks that require over three for-loops or a combination of multiple for-loops and while-loops. \u2022 ACE-SYNTH: this part corresponds to 922 MCQ tasks generated using the same codes as used for training but with newly generated grids. Similarly to HOC-SYNTH, the samples encountered during evaluation are similar to samples encountered during training in terms of codes (output) but not in terms of grids (input). We evaluate each of the following selected techniques over three seeds: LLAMA3-8B-INSTRUCT, GPT4O vis+text , and LLAMACT:HOC+MCQ+AUG exp .",
            "score": 0.3970261115635949,
            "section_title": "E.1 Generation of the Synthetic Evaluation Segment",
            "char_start_offset": 38448,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 56
                },
                {
                    "start": 57,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 354
                },
                {
                    "start": 357,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1450
                },
                {
                    "start": 1451,
                    "end": 1578
                },
                {
                    "start": 1579,
                    "end": 1714
                },
                {
                    "start": 1715,
                    "end": 1893
                },
                {
                    "start": 1894,
                    "end": 2032
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5390625
        },
        {
            "corpus_id": "273403575",
            "title": "MIND: Math Informed syNthetic Dialogues for Pretraining LLMs",
            "text": "The ability to reason is a fundamental element of human cognition, encompassing our ability to think logically, draw conclusions, and make decisions based on available information (Gendron et al., 2024). Large Language Models (LLMs) have demonstrated remarkable performance across wide range of general reasoning and specialized knowledge tasks. In particular, the improvement of LLMs in solving complex mathematical reasoning tasks (Hendrycks et al., 2021b;Cobbe et al., 2021a) has been significant in recent years (Gemini, 2024;Nvidia et al., 2024;OpenAI, 2024). \n\nStrong mathematical reasoning ability heavily relies on the abundance of high-quality, composite, and structured pretraining corpora. An effective mathematical corpus should not only contain relevant content but also be formatted to guide models break down complex problems into smaller sub-problems and solve each part step-by-step-enhancing the model's ability to process and reason about complex problems (Wei et al., 2022). Prior studies show that structured and well-formatted corpora play a crucial role in enhancing multi-hop and logical reasoning abilities (Cobbe et al., 2021a;Li et al., 2023;Gunasekar et al., 2023), underscoring the importance of well-organized mathematical datasets in pretraining LLMs. \n\nCurating complex, high-quality structured mathematical data is costly and resource-intensive, largely due to the uneven distribution of high-quality sources. Most advanced models (OpenAI, 2024;Gemini, 2024) are not publicly accessible, and it is unclear how their approach is enhancing math reasoning. To mitigate this challenge, synthetic data generation has emerged as a scalable, and cost-effective alternative for creating a more balanced and diverse training corpus for pretraining LLMs (Maini et al., 2024;Eldan & Li, 2023;Gunasekar et al., 2023;Shah et al., 2024). However, while these techniques have shown promise in improving general reasoning tasks, their data often lack the step-by-step problem solving structure crucial for multi-hop reasoning and complex mathematical tasks (Maini et al., 2024), making them sub-optimal for such reasoning.",
            "score": 0.3963354243497743,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 564
                },
                {
                    "start": 567,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1282
                },
                {
                    "start": 1285,
                    "end": 1442
                },
                {
                    "start": 1443,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1856
                },
                {
                    "start": 1857,
                    "end": 2139
                }
            ],
            "ref_mentions": [
                {
                    "start": 433,
                    "end": 458,
                    "matchedPaperCorpusId": "232134851"
                },
                {
                    "start": 975,
                    "end": 993,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 1169,
                    "end": 1192,
                    "matchedPaperCorpusId": "259203998"
                },
                {
                    "start": 1777,
                    "end": 1797,
                    "matchedPaperCorpusId": "267312030"
                },
                {
                    "start": 1814,
                    "end": 1837,
                    "matchedPaperCorpusId": "259203998"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76123046875
        },
        {
            "corpus_id": "276960951",
            "title": "OR-LLM-Agent: Automating Modeling and Solving of Operations Research Optimization Problem with Reasoning Large Language Model",
            "text": "These studies have enhanced LLMs' capabilities in reasoning tasks. However, since these CoT techniques guide the model's reasoning through prompt engineering, they do not constitute a true development of intrinsic reasoning ability in the LLM. \n\nReasoning LLMs are large language models specifically designed to exhibit advanced reasoning capabilities, including logical inference, step-by-step deduction, and complex problem-solving. These models demonstrate superior performance across a range of tasks, such as mathematical reasoning (e.g., MATH-500 (Hendrycks et al., 2021), AIME 2024), code generation and algorithmic problem solving (e.g., Codeforces, LiveCodeBench (Jain et al., 2024)), scientific reasoning (e.g., GPQA Diamond (Rein et al., 2024)), and commonsense reasoning (e.g., MMLU (Hendrycks et al., 2020)). In contrast to general-purpose LLMs, reasoning models are optimized to emulate human cognitive processes through structured deduction and incremental analysis. This design enables internal reasoning and self-verification prior to response generation, thereby reducing hallucinations and enhancing solution accuracy. Representative state-of-the-art reasoning LLMs include GPT-o3-mini, Gemini 2.0 Flash Thinking, DeepSeek-R1 (Guo et al., 2025), and Claude 3.7. \n\nReasoning LLMs acquire CoT reasoning capabilities through Reinforcement Learning (RL). The training process involves guiding the model to discover and refine effective reasoning strategies via reward signals (Guo et al., 2025). By setting rewards, such as those based on accuracy and output format. RL algorithms iteratively optimize the LLM model's performance. Over time, the model implicitly learns to adopt step-by-step reasoning strategies, such as CoT, integrating them into its problem-solving approach. Importantly, the emergence of CoT reasoning is not explicitly hard-coded but arises naturally as an effective strategy through the RL optimization process. Several leading commercial solvers have long held a dominant position in this domain:",
            "score": 0.39601874788335134,
            "section_title": "Chain-of-Thought and Reasoning LLM",
            "char_start_offset": 8997,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 66
                },
                {
                    "start": 67,
                    "end": 243
                },
                {
                    "start": 246,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1280
                },
                {
                    "start": 1283,
                    "end": 1369
                },
                {
                    "start": 1370,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1581
                },
                {
                    "start": 1582,
                    "end": 1645
                },
                {
                    "start": 1646,
                    "end": 1793
                },
                {
                    "start": 1794,
                    "end": 1949
                },
                {
                    "start": 1950,
                    "end": 2035
                }
            ],
            "ref_mentions": [
                {
                    "start": 735,
                    "end": 754,
                    "matchedPaperCorpusId": "265295009"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1483154296875
        },
        {
            "corpus_id": "270493785",
            "title": "Nemotron-4 340B Technical Report",
            "text": "Specifically, we show that Nemotron-4-340B-Base is competitive with open access base models like Llama-3 70B (MetaAI, 2024), Mixtral 8x22B (Mistral-AI-Team, 2024b) and the recently released Qwen-2 72B model on commonsense reasoning tasks like ARC-Challenge, MMLU, and the BigBench Hard benchmark. Nemotron-4-340B-Instruct surpasses the corresponding instruct models (MetaAI, 2024; Mistral-AI-Team, 2024b; Qwen-Team, 2024) in terms of instruction following and chat capabilities. Nemotron-4-340B-Reward achieves top accuracy on RewardBench (Allen AI, 2024) as of the time of publication, surpassing even proprietary models such as GPT-4o-0513 and Gemini 1.5 Pro-0514. We release our reward model in order to support the ongoing development of LLMs in the community. \n\nOne promising application of these models is synthetic data generation, which has already demonstrated significant value in improving data quality for pretraining. For instance, data synthesis has been used to rephrase web-text (Maini et al., 2024), generate training data for the text-quality classifiers (MetaAI, 2024;Guilherme Penedo, 2024), and create data for domains that are under-represented in the pretraining set. Additionally, synthetic data generation is crucial for alignment, due to the high cost of collecting human annotated data. We use synthetic data heavily to create Nemotron-4-340B-Instruct: over 98% of our training data has been synthetically generated throughout our alignment process. In addition to sharing our model and alignment strategies, we are also releasing our synthetic data generation pipeline, which includes synthetic prompt generation, response and dialogue generation, quality filtering, and preference ranking. This pipeline has been designed to support both supervised fine-tuning and preference fine-tuning, and we believe it has the potential to benefit the community by enabling the creation of high-quality data that can adapt to a wide range of domains.",
            "score": 0.39455366602603664,
            "section_title": "Introduction",
            "char_start_offset": 1496,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 764
                },
                {
                    "start": 767,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1190
                },
                {
                    "start": 1191,
                    "end": 1313
                },
                {
                    "start": 1314,
                    "end": 1476
                },
                {
                    "start": 1477,
                    "end": 1718
                },
                {
                    "start": 1719,
                    "end": 1967
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83154296875
        },
        {
            "corpus_id": "267782588",
            "title": "Beyond A*: Better Planning with Transformers via Search Dynamics Bootstrapping",
            "text": "Figure 1 provides an overview of our synthetic dataset generation process. We consider two domains: maze navigation (Figure 1(a)) and solving Sokoban puzzles (Figure 5 in Appendix C). In maze navigation, the goal is to find the shortest path through an n-by-n maze. In Sokoban, a worker can move up, down, left, or right and has to push each box onto a dock to solve the puzzle. An incorrect move may immediately lead to a dead end and thus reasoning across multiple time steps is required to solve the puzzle. Each state in a puzzle consists of a combination of box and worker positions, making Sokoban computationally more difficult to solve than maze navigation.",
            "score": 0.39452066001546404,
            "section_title": "Problem Setup",
            "char_start_offset": 7414,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 74
                },
                {
                    "start": 75,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 665
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12322998046875
        },
        {
            "corpus_id": "259145356",
            "title": "TART: A plug-and-play Transformer module for task-agnostic reasoning",
            "text": "We look at the problem of task-agnostic learning with LLMs. We show that LLMs lack the ability to perform simple reasoning over their learned representations and introduce Tart, a task, model and domain agnostic method for improving their reasoning abilities. In this work, we focus on binary classification tasks, showing that synthetic, logistic regression task data can be used to train a generic reasoning module capable of completing this class of tasks. Extensions to multi-class classification tasks are possible either using a one-vs-all approach or by training Tart's reasoning module using multi-class synthetic data. In future work, we seek to understand whether synthetic tasks exist for training other generic reasoning modules, capable of improving base LLM performance on tasks such as generation or summarization. \n\nThe U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of NIH, ONR, or the U.S. Government. \n\n[ [WK21] Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax. May 2021 (Cited on page 10). \n\n[Wan+22a] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. \"Rationale-augmented ensembles in language models\". In: arXiv preprint arXiv:2207.00747 (2022) (Cited on page 4). \n\n[Wan+22b] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. \"Self-consistency improves chain of thought reasoning in language models\". In: arXiv preprint arXiv:2203.11171 (2022) (Cited on page 4). \n\n[War18] P. Warden. \"Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition\".",
            "score": 0.39449304596428725,
            "section_title": "Discussion",
            "char_start_offset": 28920,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 59
                },
                {
                    "start": 60,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 829
                },
                {
                    "start": 832,
                    "end": 976
                },
                {
                    "start": 977,
                    "end": 1222
                },
                {
                    "start": 1225,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1379
                },
                {
                    "start": 1380,
                    "end": 1408
                },
                {
                    "start": 1411,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1546
                },
                {
                    "start": 1547,
                    "end": 1582
                },
                {
                    "start": 1583,
                    "end": 1608
                },
                {
                    "start": 1611,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 1769
                },
                {
                    "start": 1770,
                    "end": 1805
                },
                {
                    "start": 1806,
                    "end": 1831
                },
                {
                    "start": 1834,
                    "end": 1852
                },
                {
                    "start": 1853,
                    "end": 1924
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.438720703125
        },
        {
            "corpus_id": "272146630",
            "title": "Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal Sampling",
            "text": "LMs for reasoning. The ability to solve reasoning tasks has been a long standing goal of artificial intelligence (Achiam et al., 2023;AI, 2024;Anthropic, 2024;Dubey et al., 2024;Reid et al., 2024;Team, 2024). In this regard, LMs trained on the internet-scale data have achieved great success for math, code, and other reasoning tasks (Azerbayev et al., 2023;Kazemi et al., 2024;Lewkowycz et al., 2022). There have been several works that aim to enhance the reasoning capabilities of the LMs either via prompting (Kazemi et al., 2022;Kojima et al., 2022;Wang et al., 2022;Zheng et al., 2023a) or finetuning (Yu et al., 2023;Yue et al., 2023). In this work, we focus on finetuning the LMs The fitted trendlines suggest that the quality of smaller LMs is improving more rapidly than that of larger LMs over time. This highlights that our findings on utilizing smaller LMs for training strong reasoners will become increasingly relevant in the future. \n\nwith task-specific datasets to build strong reasoners. Specifically, our method closely aligns with the widely adopted STaR (Zelikman et al., 2022) where the synthetic data from the LMs are used to elicit strong reasoning capabilities. \n\nFinetuning LMs. Within the finetuning paradigm, there have been several works that improve reasoning with synthetic data. Broadly, these works focus on knowledge distillation from a strong but expensive LM (Wu et al., 2024;Yue et al., 2023) or self-improvement (Gulcehre et al., 2023;Singh et al., 2023).",
            "score": 0.3944922947380743,
            "section_title": "Related Work",
            "char_start_offset": 38168,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 18
                },
                {
                    "start": 19,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 947
                },
                {
                    "start": 950,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1185
                },
                {
                    "start": 1188,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1492
                }
            ],
            "ref_mentions": [
                {
                    "start": 378,
                    "end": 401,
                    "matchedPaperCorpusId": "250144408"
                },
                {
                    "start": 533,
                    "end": 553,
                    "matchedPaperCorpusId": "249017743"
                },
                {
                    "start": 1074,
                    "end": 1097,
                    "matchedPaperCorpusId": "247762790"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68359375
        },
        {
            "corpus_id": "258461502",
            "title": "Visual Chain of Thought: Bridging Logical Gaps with Multimodal Infillings",
            "text": "Recent advances in large language models elicit reasoning in a chain-of-thought that allows models to decompose problems in a human-like fashion. Though this paradigm improves multi-step reasoning ability in language models, it is limited by being unimodal and applied mainly to question-answering tasks. We claim that incorporating visual augmentation into reasoning is essential, especially for complex, imaginative tasks. Consequently, we introduce VCoT, a novel method that leverages chain-of-thought prompting with vision-language grounding to recursively bridge the logical gaps within sequential data. Our method uses visual guidance to generate synthetic multimodal infillings that add consistent and novel information to reduce the logical gaps for downstream tasks that can benefit from temporal reasoning, as well as provide interpretability into models' multi-step reasoning. We apply VCoT to the Visual Storytelling and WikiHow summarization datasets and demonstrate through human evaluation that VCoT offers novel and consistent synthetic data augmentation beating chain-of-thought baselines, which can be used to enhance downstream performance.",
            "score": 0.3939116477437888,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8828125
        },
        {
            "corpus_id": "277313659",
            "title": "Scaling Laws of Synthetic Data for Language Models",
            "text": "Synthetic Data Generation. Synthetic data has emerged as a promising approach to augment real data, addressing challenges related to data scarcity and the high cost of data collection and annotation. It has been widely applied in pre-training, instruction following, and reasoning tasks, demonstrating notable improvements in model performance. In this work, we focus on synthetic data generation for the post-training phase. Existing synthetic data generation methods typically rely on a small set of high-quality human-annotated seed samples, leveraging LLMs to generate diverse augmentations through various techniques: 1) sampling seed instructions as few-shot examples to elicit LLMs to generate new instructions [19,23,47,50]; 2) leveraging LLMs to rephrase seed samples [12,21,34,35,53]. However, the scarcity of high-quality seed examples limits the scalability and diversity of generated data. To address this, Xu et al. [51] and Li et al. [24] propose generating synthetic data from scratch, leveraging either the inherent uncertainty of LLMs or a predefined knowledge taxonomy to guide the generation process. In contrast, pre-training data-being both vast and highly diverse-still remains an underutilized resource for scalable post-training data generation. Recent methods have explored extracting high-quality samples directly from web documents (similar to our Level-1 method) [54,55,56] or employing document backtranslation to generate questions [26]. Our SYNTHLLM aligns with this line of research and offers a more efficient approach to leveraging web documents for scaling diverse sample generation compared to aforementioned methods. \n\nScaling Law of LLMs. Scaling laws provide a predictive framework for estimating model performance based on key factors such as model size and pre-training data size, and have been extensively studied [5,17,18,20,42]. These laws offer valuable insights into how performance scales with computational resources, enabling more informed decisions on the optimal allocation of compute for pre-training LLMs [18,20]. Recently, more fine-grained scaling laws have been proposed, including data-constrained scaling [37], hyperparameter scaling [6], and model distillation [8].",
            "score": 0.39369950346573385,
            "section_title": "Related Work",
            "char_start_offset": 4862,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 26
                },
                {
                    "start": 27,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 1120
                },
                {
                    "start": 1121,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1654
                },
                {
                    "start": 1657,
                    "end": 1677
                },
                {
                    "start": 1678,
                    "end": 1873
                },
                {
                    "start": 1874,
                    "end": 2067
                },
                {
                    "start": 2068,
                    "end": 2225
                }
            ],
            "ref_mentions": [
                {
                    "start": 725,
                    "end": 728,
                    "matchedPaperCorpusId": "273025760"
                },
                {
                    "start": 790,
                    "end": 793,
                    "matchedPaperCorpusId": "262084051"
                },
                {
                    "start": 1396,
                    "end": 1399,
                    "matchedPaperCorpusId": "269605607"
                },
                {
                    "start": 1399,
                    "end": 1402,
                    "matchedPaperCorpusId": "269981934"
                },
                {
                    "start": 1463,
                    "end": 1467,
                    "matchedPaperCorpusId": "260866107"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96337890625
        },
        {
            "corpus_id": "278165289",
            "title": "SynLexLM: Scaling Legal LLMs with Synthetic Data and Curriculum Learning",
            "text": "Our research integrates concepts from synthetic data generation, curriculum learning, large language models, and legal NLP benchmarks. Synthetic Data Generation: The challenge of data scarcity in specialized domains has spurred research into synthetic data. Nikolenko's work [13] provides a foundational overview of techniques for generating synthetic data for deep learning, particularly relevant in domains like law where data can be scarce or sensitive. While various methods exist, generative models are increasingly used. Guan et al. [7] showed the potential of generating synthetic text in the medical domain. Although sequence GANs combined with reinforcement learning (e.g., SeqGAN [15]) offer an alternative for text generation, their computational demands led us to prefer leveraging large pre-trained models like Gemini Pro for controlled, high-quality generation via prompting. \n\nCurriculum Learning: The idea of training models on progressively harder examples, mimicking human learning, was formalized by Bengio et al. [5]. Curriculum learning has been shown to improve convergence speed and generalization performance in various tasks. We apply this by ordering our combined real and synthetic legal data based on complexity metrics like document length and legal concept density. \n\nLarge Language Models and Domain Adaptation: Our work inherently relies on the power of Transformer-based LLMs [14], such as Llama [2] and Gemma 3 12b. Adapting these generalist models to specific domains is a key research area. Techniques like ULM-FiT [9] demonstrated effective fine-tuning strategies for adapting pre-trained language models to downstream tasks. Our approach extends fine-tuning by incorporating both targeted synthetic data generation and a curriculum strategy specifically for the complexities of the legal domain. \n\nLegal NLP Benchmarks: To evaluate our approach rigorously, we rely on established benchmarks designed for legal language understanding. EUR-Lex-Sum [3] focuses on long-form summarization in the EU legal context. Broader benchmarks like LexGLUE [6] and the industry-focused BigLaw-Bench [1] provide standardized tasks for evaluating capabilities such as question answering, text classification, and named entity recognition within the legal field.",
            "score": 0.39349571863776023,
            "section_title": "RELATED WORKS",
            "char_start_offset": 3074,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 889
                },
                {
                    "start": 892,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1295
                },
                {
                    "start": 1298,
                    "end": 1449
                },
                {
                    "start": 1450,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1662
                },
                {
                    "start": 1663,
                    "end": 1833
                },
                {
                    "start": 1836,
                    "end": 1971
                },
                {
                    "start": 1972,
                    "end": 2047
                },
                {
                    "start": 2048,
                    "end": 2282
                }
            ],
            "ref_mentions": [
                {
                    "start": 539,
                    "end": 542,
                    "matchedPaperCorpusId": "54457460"
                },
                {
                    "start": 690,
                    "end": 694,
                    "matchedPaperCorpusId": "3439214"
                },
                {
                    "start": 1033,
                    "end": 1036,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 1409,
                    "end": 1413,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1551,
                    "end": 1554,
                    "matchedPaperCorpusId": "259601654"
                },
                {
                    "start": 1984,
                    "end": 1987,
                    "matchedPaperCorpusId": "253098967"
                },
                {
                    "start": 2080,
                    "end": 2083,
                    "matchedPaperCorpusId": "238259595"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82470703125
        },
        {
            "corpus_id": "270870071",
            "title": "Advancing Process Verification for Large Language Models via Tree-Based Preference Learning",
            "text": "Multi-step Reasoning Challenging reasoning tasks have spurred innovative research in large language models (LLMs), which are essential for handling complex queries (Kaddour et al., 2023;Lightman et al., 2023;Huang et al., 2023).A prominent strategy employed is the Chain-of-Thought (CoT) prompting technique (Wei et al., 2022), along with its derivatives (Kojima et al., 2022;Wang et al., 2022;Yao et al., 2024).These methods decompose the reasoning process into sequential steps, systematically approaching problem-solving by mimicking human-like reasoning.To further enhance the accuracy of these intermediate steps, recent studies leverage extensive synthetic datasets, which are either distilled from cutting-edge models (Yu et al., 2023b;Luo et al., 2023) or composed of selfgenerated rationales (Zelikman et al., 2022;Yuan et al., 2023;Ni et al., 2022), to fine-tune the LLMs.\n\nSuch training effectively sharpens the models' ability to produce CoT reasoning that leads to correct answers.\n\nIn addition, there is an increasing emphasis on test-time verification, a process that generates multiple solutions and ranks them through a separate verifier (Cobbe et al., 2021) to select the most accurate one.The DIVERSE framework (Li et al., 2022) demonstrates the effectiveness of employing a variety of CoT prompts in conjunction with a verifier to tackle reasoning challenges.CoRe (Zhu et al., 2022) implements a dual-process system by finetuning both the generator and the verifier, thereby enhancing the performance of LLMs on math word problems.",
            "score": 0.393129940397938,
            "section_title": "A Related Work",
            "char_start_offset": 26179,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 228,
                    "end": 412
                },
                {
                    "start": 412,
                    "end": 558
                },
                {
                    "start": 558,
                    "end": 882
                },
                {
                    "start": 884,
                    "end": 994
                },
                {
                    "start": 996,
                    "end": 1208
                },
                {
                    "start": 1208,
                    "end": 1379
                },
                {
                    "start": 1379,
                    "end": 1551
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89599609375
        },
        {
            "corpus_id": "272988100",
            "title": "Balancing Cost and Effectiveness of Synthetic Data Generation Strategies for LLMs",
            "text": "Synthetic Data for Fine-Tuning. Fine-tuning on synthetic and hybrid data has proven successful across a wide range of tasks (Liu et al., 2024). In the domain of mathematical reasoning, highquality instructions are scarce, so many works leverage LLM-generated synthetic data to significantly improved the math reasoning ability of small LLMs (Yu et al., 2023;Li et al., 2024;Setlur et al., 2024;Luo et al., 2023). In code generation, synthetic data from LLMs can be further verified by running test cases or the code directly, which helps close the gap between closed-source LLMs and smaller LLMs (Wei et al., 2024;Yang et al., 2024). Similar approaches have been applied in instruction-following, where LLMs are effectively trained on diverse synthetic instructions with minimal to no human supervision (Xu et al., 2024;Wang et al., 2022;Xu et al., 2023). However, most works focus on a single domain and do not explore how these techniques perform under varying data constraints and strategies, leaving uncertainty in adapting them to new applications. We aim to compare and extend these methods in cost constrained settings, while investigating which factors in synthetic data generation remain impactful across multiple tasks and data budgets. \n\nEfficient Synthetic Data Generation. Although synthetic data is significantly cheaper than real data, its scalability encourages researchers to generate it at extremely large scales, making generation costs a substantial component of fine-tuning expert models (Li et al., 2024). Other works focus on aggressively filtering synthetic datasets for diversity and correctness with custom tricks for each domain (Long et al., 2024). Current research on training LLMs with synthetic data emphasizes scalability and performance, but to make these methods more applicable to more tasks, we also need to disentangle and understand cost-efficiency across different scales. Bansal et al. (2024) explores and optimizes the choice of LLMs to sample synthetic data for overall cost reductions. Our work addresses this challenge from a new perspective by offering a general framework that guides model trainers in defining and refining their synthetic data generation strategies to maximize cost-efficiency within budget constraints.",
            "score": 0.392628114487577,
            "section_title": "Related Work",
            "char_start_offset": 3318,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 31
                },
                {
                    "start": 32,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1246
                },
                {
                    "start": 1249,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1527
                },
                {
                    "start": 1528,
                    "end": 1676
                },
                {
                    "start": 1677,
                    "end": 1911
                },
                {
                    "start": 1912,
                    "end": 2028
                },
                {
                    "start": 2029,
                    "end": 2267
                }
            ],
            "ref_mentions": [
                {
                    "start": 358,
                    "end": 374,
                    "matchedPaperCorpusId": "269930153"
                },
                {
                    "start": 394,
                    "end": 411,
                    "matchedPaperCorpusId": "261030818"
                },
                {
                    "start": 596,
                    "end": 614,
                    "matchedPaperCorpusId": "270358041"
                },
                {
                    "start": 1509,
                    "end": 1526,
                    "matchedPaperCorpusId": "269930153"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90283203125
        },
        {
            "corpus_id": "270226405",
            "title": "Meta-Designing Quantum Experiments with Language Models",
            "text": "An alternative strategy is to take existing large natural language models and then fine-tune them on code data.For example, Codex (Chen et al., 2021) fine-tuned GPT3 (Brown et al., 2020) on the source code from the public available sources, and CodeGemma is an adpation from the Gemma (Gemma-Team, 2024) models.There has also been research exploring the use of program synthesis techniques as a tool to enhance the algorithmic reasoning capabilities of large language models.Methods like PAL (Gao et al., 2023) and PoT (Chen et al., 2022) prompt language models to break down reasoning problems into a series of intermediate steps.These step-by-step decompositions are then offloaded to an external runtime environment, such as a Python interpreter, to execute and solve each step programmatically.Another technique called Think-and-Execute (Chae et al., 2024) follows a similar philosophy, where the language model is tasked with generating high-level pseudocode that outlines the solution approach for a given problem.This pseudocode is then simulated and executed, allowing the model to reason through the problem in a structured, interpretable manner.In addition, program synthesis has been used to explore mechanistic interpretability for LLMs.Michaud et al. (2024) translate algorithms encoded by machine-learning models into human-readable Python code.LLMs have been used to generate programs to solve mathematical problems (Romera-Paredes et al., 2024) and to discover scientific equations from data (Shojaee et al., 2024).\n\nTransformers for math and physics Transformer architectures have demonstrated remarkable success in solving a wide range of mathematics and physics reasoning tasks.Lample & Charton (2019) and (Kamienny et al., 2022) show that a transformer-based sequence-to-sequence model can tackle symbolic math problems such as symbolic integration, differential equations and symbolic regression.AlphaGeometry (Trinh et al., 2024) has achieved remarkable performance in solving geometry problems at an olympiad level.Alfarano et al. (2023) finds that by training transformers on synthetic data, they can accurately predict the Lyapunov functions of polynomial and non-polynomial dynamical systems.",
            "score": 0.39103036409498826,
            "section_title": "Related Work",
            "char_start_offset": 4415,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 111,
                    "end": 311
                },
                {
                    "start": 311,
                    "end": 475
                },
                {
                    "start": 475,
                    "end": 631
                },
                {
                    "start": 631,
                    "end": 798
                },
                {
                    "start": 798,
                    "end": 1020
                },
                {
                    "start": 1020,
                    "end": 1155
                },
                {
                    "start": 1155,
                    "end": 1249
                },
                {
                    "start": 1249,
                    "end": 1359
                },
                {
                    "start": 1359,
                    "end": 1531
                },
                {
                    "start": 1533,
                    "end": 1697
                },
                {
                    "start": 1697,
                    "end": 1917
                },
                {
                    "start": 1917,
                    "end": 2038
                },
                {
                    "start": 2038,
                    "end": 2218
                }
            ],
            "ref_mentions": [
                {
                    "start": 166,
                    "end": 186,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 492,
                    "end": 510,
                    "matchedPaperCorpusId": "253708270"
                },
                {
                    "start": 1431,
                    "end": 1460,
                    "matchedPaperCorpusId": "266223700"
                },
                {
                    "start": 1725,
                    "end": 1748,
                    "matchedPaperCorpusId": "248366384"
                },
                {
                    "start": 1931,
                    "end": 1951,
                    "matchedPaperCorpusId": "267032902"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.269775390625
        },
        {
            "corpus_id": "270620658",
            "title": "RL on Incorrect Synthetic Data Scales the Efficiency of LLM Math Reasoning by Eight-Fold",
            "text": "While our work provides some results and conceptual models to understand the role of synthetic data for reasoning, there are still many open questions that need to be answered to fully understand its utility.While synthetic data from LLMs like Gemini and GPT-4 holds great potential, for more complex reasoning problems (more complicated than the datasets evaluated in our work), synthetic data generated from more capable models can contain errors.Generating negative/positive data by referencing synthetic data answers can reinforce unwanted spurious correlations highlighted in our work.This means that novel recipes for generating synthetic problems may be utilized in the future, and our analysis might need to be re-done.That said, we believe that our insights about algorithmic behavior with synthetic data are still quite general and should transfer to these novel settings as well.Ultimately, we would want that training on synthetic data improves transfer and generalization abilities of the model in general reasoning scenarios, and to this end, an evaluation of transfer capabilities is an important avenue that future work should focus on.",
            "score": 0.3908534054751986,
            "section_title": "Limitations.",
            "char_start_offset": 48473,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 208
                },
                {
                    "start": 208,
                    "end": 449
                },
                {
                    "start": 449,
                    "end": 590
                },
                {
                    "start": 590,
                    "end": 727
                },
                {
                    "start": 727,
                    "end": 890
                },
                {
                    "start": 890,
                    "end": 1152
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65966796875
        },
        {
            "corpus_id": "264555527",
            "title": "TarGEN: Targeted Data Generation with Large Language Models",
            "text": "Similarly, (Chia et al., 2022) introduces structured prompts for tasks like relation triplet extraction. Moreover, (Liu et al., 2022a) and (Wiegreffe et al., 2022) propose synthetic data generation methods for natural language entailment (NLI) tasks and free-text explanations in a human-AI collaborative setting. There also have been approaches proposed to generate tabular data (Borisov et al., 2023) and instruction data (Peng et al., 2023;Sun et al., 2023). \n\nThe research direction in synthetic data generation predominantly focuses on zero/few-shot classification or entails finetuning (Chen et al., 2023) or iterative finetuning of open-source LLMs (Yu et al., 2023). In contrast, our method is simple, lightweight, and adaptable even to closed-source LLMs. Additionally, the aforementioned methods often rely on seed samples where expert annotators are employed to curate and label a sample set of considerable size. These seed samples are used to generate additional synthetic samples for various downstream tasks. \n\nWe exploit the generative capabilities of LLMs to completely generate seed instances that drive the diverse synthetic dataset generation. Furthermore, our approach uses a multi-step prompting strategy along with self-correction to for targeted data generation while preserving quality in terms of diversity, bias, noise, and mislabelling. Finally, existing dataset generation methods often are limited by reliance on seed tasks from the original dataset (Wang et al., 2023) whereas our seedless pipeline leverages high-level dataset characteristics for the generation process. \n\nWe compare our framework with other popular data generation approaches in the table below. Axes of comparison: \n\n\u2022 Seedless: Whether the approach requires labeled task samples. The row values show the number of labels that were originally assigned to that label (ent, non: entailment, non-entailment; neutr, contr: neutral, contradiction). The number in a cell (i, j) reflects the number of labels originally assigned to label i which were re-labeled to label j after self-correction.",
            "score": 0.3908534054751986,
            "section_title": "Introduction",
            "char_start_offset": 5733,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 461
                },
                {
                    "start": 464,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1023
                },
                {
                    "start": 1026,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1602
                },
                {
                    "start": 1605,
                    "end": 1695
                },
                {
                    "start": 1696,
                    "end": 1715
                },
                {
                    "start": 1718,
                    "end": 1781
                },
                {
                    "start": 1782,
                    "end": 1944
                },
                {
                    "start": 1945,
                    "end": 2089
                }
            ],
            "ref_mentions": [
                {
                    "start": 11,
                    "end": 30,
                    "matchedPaperCorpusId": "247518984"
                },
                {
                    "start": 115,
                    "end": 134,
                    "matchedPaperCorpusId": "246016339"
                },
                {
                    "start": 139,
                    "end": 163,
                    "matchedPaperCorpusId": "245219282"
                },
                {
                    "start": 1480,
                    "end": 1499,
                    "matchedPaperCorpusId": "254877310"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88427734375
        },
        {
            "corpus_id": "273025893",
            "title": "Towards a Theoretical Understanding of Synthetic Data in LLM Post-Training: A Reverse-Bottleneck Perspective",
            "text": "\u2022 What is the reason for the effectiveness of synthetic data in LLM post-training? \n\nIn response to these inquiries, we introduce a theoretical framework designed to dissect the impacts of synthetic data on LLM post-training. The principal contributions of our study are outlined as follows: \n\n1. We develop a modeling of synthetic data generation from a distributional perspective, providing a theoretical foundation for understanding the generation process and its implications on LLM post-training. \n\n2. Drawing on this modeling, we propose a reverse-bottleneck framework that elucidates the mechanisms through which synthetic data influences LLM post-training. \n\n3. We perform a theoretical analysis from an information-theoretic standpoint, delivering several upper bounds that quantifies the expected generalization capabilities of LLMs when trained with synthetic data. \n\nThe remainder of this paper is structured as follows. In Section 2, we provide a comprehensive review of literature pertinent to our research. In Section 3, we first delineate the symbols and foundational concepts critical to our analysis, then introduce the modeling for synthetic data generation and bridge its connection with generalization capability of post-trained models. Section 4 introduces our novel reverse-bottleneck framework, designed to assess the effects of synthetic data on posttraining stages of LLMs, and to establish generalization error upper bounds. The paper concludes with Section 5, summarizing our findings and discussing potential avenues for future research. et al., 2014), Normalizing Flows (Rezende & Mohamed, 2015), and, more recently, diffusion models (Rombach et al., 2022). Building on this premise, generative data augmentation has emerged as a promising approach to bolster machine learning model performance (Yamaguchi et al., 2020). This technique involves scaling up the available training dataset by generating new data points from a limited pool of labeled data using generative models. Empirical evidence suggests that generative data augmentation is particularly effective across various tasks, including knowledge graph reasoning (Maharana & Bansal, 2022), text-to-image generation (Yin et al., 2023), and relation extraction from natural language texts (Hu et al., 2023).",
            "score": 0.3908432652713219,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2133,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 82
                },
                {
                    "start": 85,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 291
                },
                {
                    "start": 294,
                    "end": 501
                },
                {
                    "start": 504,
                    "end": 664
                },
                {
                    "start": 667,
                    "end": 876
                },
                {
                    "start": 879,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1451
                },
                {
                    "start": 1452,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1687
                },
                {
                    "start": 1688,
                    "end": 1850
                },
                {
                    "start": 1851,
                    "end": 2007
                },
                {
                    "start": 2008,
                    "end": 2296
                }
            ],
            "ref_mentions": [
                {
                    "start": 1600,
                    "end": 1625,
                    "matchedPaperCorpusId": "12554042"
                },
                {
                    "start": 1664,
                    "end": 1686,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1825,
                    "end": 1849,
                    "matchedPaperCorpusId": "209501093"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86962890625
        },
        {
            "corpus_id": "272593422",
            "title": "Synthetic continued pretraining",
            "text": "We next discuss recent work most related to our setting of synthetic data generation for continued pretraining. In Appendix A, we provide an extended survey of classical work on synthetic data generation and continual learning. \n\nSynthetic generation of pretraining data. Recent approaches synthesize pretraining data using hierarchical prompting methods to promote dataset diversity. Eldan & Li (2023) prompt API-based LLMs to generate children's stories containing sampled keywords, and demonstrate that even small language models trained on their dataset can generate fluent text. Gunasekar et al. (2023) synthesize a diverse dataset of textbooks and code exercises by conditioning on topic, target audience, and function names, and later release strong LLMs pretrained on synthetic data in follow-up work (Li et al., 2023b;Abdin et al., 2023;2024). However, their datasets and prompts are not publicly available. Maini et al. (2024) prompt an LM to rephrase documents for pretraining, improving training efficiency. Different from all above works, our focus is teaching a pretrained LLM the knowledge of a small corpus. Mecklenburg et al. (2024) consider task-specific finetuning and propose a fact-based synthetic QA generation procedure, but do not show improvement on generic instruction following tasks beyond simple QA. We instead focus on teaching a model generally useful knowledge about a small corpus, untied to a particular downstream task. Ovadia et al. (2024) continually pretrain Llama 2-based language models on synthetic paraphrases of Wikipedia articles, but do not observe consistent performance improvements. We adapt the approach of Maini et al. (2024) and Mecklenburg et al. (2024) to our small corpus setting as the \"Rephrase baseline\" in \u00a74. We find that our graph-based augmentation algorithm outperforms it, likely because our approach enforces diversity through entity-based generation. \n\nContinued pretraining.",
            "score": 0.39018229507054425,
            "section_title": "RELATED WORK",
            "char_start_offset": 7836,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 227
                },
                {
                    "start": 230,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1328
                },
                {
                    "start": 1329,
                    "end": 1454
                },
                {
                    "start": 1455,
                    "end": 1630
                },
                {
                    "start": 1631,
                    "end": 1767
                },
                {
                    "start": 1768,
                    "end": 1915
                },
                {
                    "start": 1918,
                    "end": 1940
                }
            ],
            "ref_mentions": [
                {
                    "start": 917,
                    "end": 936,
                    "matchedPaperCorpusId": "267312030"
                },
                {
                    "start": 1656,
                    "end": 1675,
                    "matchedPaperCorpusId": "267312030"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.912109375
        },
        {
            "corpus_id": "272753147",
            "title": "Enhancing Logical Reasoning in Large Language Models through Graph-based Synthetic Data",
            "text": "Despite recent advances in training and prompting strategies for Large Language Models (LLMs), these models continue to face challenges with complex logical reasoning tasks that involve long reasoning chains. In this work, we explore the potential and limitations of using graph-based synthetic reasoning data as training signals to enhance LLMs' reasoning capabilities. Our extensive experiments, conducted on two established natural language reasoning tasks -- inductive reasoning and spatial reasoning -- demonstrate that supervised fine-tuning (SFT) with synthetic graph-based reasoning data effectively enhances LLMs' reasoning performance without compromising their effectiveness on other standard evaluation benchmarks.",
            "score": 0.38998210231146757,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.943359375
        },
        {
            "corpus_id": "270380393",
            "title": "Distributional MIPLIB: a Multi-Domain Library for Advancing ML-Guided MILP Methods",
            "text": "Below we outline suggestions for potential research paths using Distributional MIPLIB to facilitate step-change in the ability to solve hard real-world MILP problems. \n\nFaster Inference. Due to computational constraints, prior work has focused on training and testing on relatively small and/or easy MILP distributions. In addition to Learn2Branch, much of the existing work on ML for MILP focuses on replacing an expensive procedure with an ML oracle, such as ML for LNS. Our empirical results highlighted that often the advantage of the ML policy is outweighed by its cost of inference on large MILPs. This calls for investigations of ML model architectures or hardware solutions that specifically target this challenge. \n\nSynthetic Data Generation. Synthetic Data Generation (SDG) captures the underlying distribution of a dataset and synthesizes targeted data through a generative process [3]. SDG has been applied to multiple domains including finance [4] and healthcare [35] to address the problem of limited available data or preserve the privacy of real data. SDG could also be used to improve ML-based methods for MILPs, as collecting algorithmic decision data from solving instances can be expensive, as discussed in Section 4. Moreover, for some real-world domains, the number of instances is also limited, such as SRPN in this library. Synthetic data could be used to complement existing data in these cases. There has been existing work that uses data augmentation to generate MILP instances [54,25] or algorithm decision data inside B&B [52]. Distributional MIPLIB could be used to develop theoretical and algorithmic frameworks that generate targeted data forming the same distributions. \n\nFoundation Model for Combinatorial Optimization. Deep learning foundation models that leverage vast amounts of data to learn general-purpose representation can adapt to a wide range of downstream tasks, which has drastically transformed the domains of language, vision, and scientific discovery [11]. Distributional MIPLIB contains MILPs from a wide range of domains and hardness levels, which can be suited for a wide range of tracks (B&B, LNS, and finding primal solutions).",
            "score": 0.389297554720451,
            "section_title": "Potential Research Paths",
            "char_start_offset": 22166,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 169,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 722
                },
                {
                    "start": 725,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1347
                },
                {
                    "start": 1348,
                    "end": 1420
                },
                {
                    "start": 1421,
                    "end": 1556
                },
                {
                    "start": 1557,
                    "end": 1702
                },
                {
                    "start": 1705,
                    "end": 1753
                },
                {
                    "start": 1754,
                    "end": 2005
                },
                {
                    "start": 2006,
                    "end": 2181
                }
            ],
            "ref_mentions": [
                {
                    "start": 893,
                    "end": 896,
                    "matchedPaperCorpusId": "259138670"
                },
                {
                    "start": 957,
                    "end": 960,
                    "matchedPaperCorpusId": "226858255"
                },
                {
                    "start": 976,
                    "end": 980,
                    "matchedPaperCorpusId": "248162973"
                },
                {
                    "start": 1509,
                    "end": 1512,
                    "matchedPaperCorpusId": "263620283"
                },
                {
                    "start": 1551,
                    "end": 1555,
                    "matchedPaperCorpusId": "267499838"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82421875
        },
        {
            "corpus_id": "267681847",
            "title": "Chain-of-Thought Reasoning Without Prompting",
            "text": "Compared to existing works that improve model's reasoning via better human-written prompts, a key distinction of our proposed approach lies in the complete elimination of human-provided prompts. This modification enables a more truthful assessment of a language model's intrinsic reasoning capabilities. In the previous section, we show that language models inherently possess reasoning capabilities for grade-school-level math problems and simple commonsense reasoning tasks. In this section, we will systematically vary the difficulty levels of synthetic tasks to gain a more comprehensive understanding of language models' inherent reasoning abilities via CoT-decoding. \n\nWe consider the following symbolic reasoning tasks: (1) the Coin Flip task from (Wei et al., 2022), with 2, 3, 4 rounds of potential flips; and two tasks from Big-Bench-Hard (bench authors, 2023; Suzgun et al., 2022) : (2) Web of lies, with 3, 4, 5 truth/lie statements, and (3) Multi-step arithmetic with various depth level  and length . For each task, we produce 100 examples for each difficulty level, except for Web-of-Lies (5) we use the existing dataset from (Suzgun et al., 2022). We also include two natural-language-based but synthetic tasks from Big-Bench, Sports Understanding and Object Counting, to probe model's intrinsic abilities in solving synthetic tasks. Table 6 | The model's intrinsic reasoning ability varies depending on the task difficulty levels. \n\nThe presence of correct CoT paths depends on the task difficulty levels and correlates with task prominence in the pre-training distribution. The results in Table 6 (on PaLM-2 L) show that despite CoT-decoding helps elicit better reasoning across almost all tasks, the gains vary significantly depending on the task difficulty level: the simpler the task is, the better chance that a correct reasoning path can be found. We also looked at the model's top- decoding paths, and found that models can generate the correct CoT paths when the solution involves at most 1 or 2 step knowledge manipulation, and the model starts to struggle with generating the correct CoT-paths when the steps become 3 or more.",
            "score": 0.38915577298262094,
            "section_title": "CoT-decoding Enables a Better Understanding of Model's Intrinsic Reasoning Abilities",
            "char_start_offset": 18746,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 476
                },
                {
                    "start": 477,
                    "end": 672
                },
                {
                    "start": 675,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1349
                },
                {
                    "start": 1350,
                    "end": 1447
                },
                {
                    "start": 1450,
                    "end": 1591
                },
                {
                    "start": 1592,
                    "end": 1870
                },
                {
                    "start": 1871,
                    "end": 2153
                }
            ],
            "ref_mentions": [
                {
                    "start": 755,
                    "end": 773,
                    "matchedPaperCorpusId": "246411621"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.366455078125
        },
        {
            "corpus_id": "276421244",
            "title": "Theorem Prover as a Judge for Synthetic Data Generation",
            "text": "While TP-as-a-Judge demonstrates strong capabilities in verifying intermediate reasoning for synthetic data generation, several limitations persist. \n\nFirstly, theorem prover verification is highly effective in mathematical reasoning but remains challenging in other domains, making its extension beyond mathematics an open research question. \n\nWhile formalisation enhances verification compared to standard methods, it is not flawless, requiring further refinement for broader applicability. Secondly, our current implementation primarily focuses on algebra, counting, probability, and problem-solving datasets. Expanding to additional mathematical areas could improve generalisation and model performance. Thirdly, dataset complexity is limited by the LLM's ability to reliably solve problems. When generating more challenging problems, the model often fails, restricting the complexity of synthetic data and preventing the creation of more advanced mathematical questions. Next, computational constraints limit scalability, particularly for models exceeding 8 billion parameters, requiring substantial resources for large-scale verification and training. Finally, the iterative nature of formalisation incurs a high computational cost, as multiple refinement cycles are needed to ensure correct execution. Addressing these challenges would enable more efficient and scalable synthetic data generation.",
            "score": 0.3890739765197723,
            "section_title": "Limitations",
            "char_start_offset": 24089,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 151,
                    "end": 342
                },
                {
                    "start": 345,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 707
                },
                {
                    "start": 708,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1404
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79638671875
        },
        {
            "corpus_id": "273695832",
            "title": "On Memorization of Large Language Models in Logical Reasoning",
            "text": "DyVal (Zhu et al., 2024) shows that fine-tuning Llama2-13B-chat on their synthetic reasoning benchmark improves its performance on other popular reasoning benchmarks. BoardgameQA (Kazemi et al., 2024) find that fine-tuning BERT-large and T5-XXL on their training dataset with synthetic proofs outperforms few-shot CoT prompting using PaLM. (Ye et al., 2024) pretrain GPT2 from scratch on synthetic math problems, synthetic CoT steps and solutions and show that model can solve problems from the same distribution and generalize to out-of-distribution (OOD) problems. However, (Dziri et al., 2024) show that while GPT-3 fine-tuned on their compositional reasoning tasks with/without reasoning steps can solve in-distribution (ID) problems, it fails to generalize to OOD tasks with increased problem sizes. Besides using synthetic CoTs, there are work using model-generated CoTs to enhance the models' reasoning capabilities (Chung et al., 2024) In our study, we employ both direct fine-tuning and CoT fine-tuning to achieve memorization on K&K training data. Notably, our findings show that the fine-tuned GPT4o-mini and Llama3-8B models can effectively generalize to unseen OOD and ID K&K problems, contributing new insights to the topic of LLM fine-tuning for reasoning. \n\nOrthogonal to our work, inference-time techniques have been explored to enhance reasoning performance such as selfconsistency (Wang et al., 2023b), self-verification (Weng et al., 2023), and integration with external symbolic solvers (Pan et al., 2023). \n\nGrokking. Our findings are related to Grokking, first identified by (Power et al., 2022) on a small algorithmic dataset, where validation accuracy suddenly improves from random chance to near-perfect generalization long after severe overfitting. Follow-up studies expanded the range of tasks where grokking occurs and proposed various explanations (Liu et al., 2022a;Murty et al., 2023;Liu et al., 2022b).",
            "score": 0.3885513162643509,
            "section_title": "Conclusion",
            "char_start_offset": 32676,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 804
                },
                {
                    "start": 805,
                    "end": 1057
                },
                {
                    "start": 1058,
                    "end": 1271
                },
                {
                    "start": 1274,
                    "end": 1527
                },
                {
                    "start": 1530,
                    "end": 1539
                },
                {
                    "start": 1540,
                    "end": 1775
                },
                {
                    "start": 1776,
                    "end": 1935
                }
            ],
            "ref_mentions": [
                {
                    "start": 179,
                    "end": 200,
                    "matchedPaperCorpusId": "259144942"
                },
                {
                    "start": 923,
                    "end": 943,
                    "matchedPaperCorpusId": "253018554"
                },
                {
                    "start": 1400,
                    "end": 1420,
                    "matchedPaperCorpusId": "247595263"
                },
                {
                    "start": 1508,
                    "end": 1526,
                    "matchedPaperCorpusId": "258833332"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.49462890625
        },
        {
            "corpus_id": "254877131",
            "title": "Optimization Techniques for Unsupervised Complex Table Reasoning via Self-Training Framework",
            "text": "Unsupervised data generation has been extensively studied on various tasks like question answering and natural language inference, and has shown surprising performances [51], [53] [54]. Recently, methods for synthesizing human-like tabular reasoning samples have also been proposed [52], [56]. Chemmengath et al. [46] sample complex SQL queries and generate natural language questions in a seq2seq manner. Eisenschlos et al. [29] generate factual claims leveraging context-free grammar (CFG) and counterfactual heuristics. Unfortunately, these methods focus on a specific task or scenario. Based on the modules and predefined operators, our approach can convert different types of programs into natural language questions or claims with tabular evidence or a hybrid of tabular and textual evidence. \n\nSelf-training has been widely explored in the realm of semi-supervised learning [60], [63], [65]. For example, Li et al. [66] proposed FlexKBQA, a method that combines selftraining and synthetic data to improve the performance of few-shot knowledge based question answering. Most works employ self-training techniques in the few-shot setting, where a small number of labeled samples are available. However, this study effectively combines data generation methods with selftraining to achieve good results in an unsupervised scenario.",
            "score": 0.38764643923795655,
            "section_title": "B. Unsupervised Data Generation and Self-Training.",
            "char_start_offset": 45826,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 589
                },
                {
                    "start": 590,
                    "end": 798
                },
                {
                    "start": 801,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1198
                },
                {
                    "start": 1199,
                    "end": 1334
                }
            ],
            "ref_mentions": [
                {
                    "start": 881,
                    "end": 885,
                    "matchedPaperCorpusId": "250451588"
                },
                {
                    "start": 887,
                    "end": 891,
                    "matchedPaperCorpusId": "7890982"
                },
                {
                    "start": 893,
                    "end": 897,
                    "matchedPaperCorpusId": "18507866"
                },
                {
                    "start": 922,
                    "end": 926,
                    "matchedPaperCorpusId": "261076103"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.919921875
        },
        {
            "corpus_id": "264436468",
            "title": "LINC: A Neurosymbolic Approach for Logical Reasoning by Combining Language Models with First-Order Logic Provers",
            "text": "Neurosymbolic approaches for reasoning: Methods which combine neural networks with symbolic techniques have seen broad uptake in domains adjacent to logical reasoning, such as generating outputs consistent with a pre-existing symbolic knowledge base (Marra et al., 2019;Manhaeve et al., 2018;Zhang et al., 2023a) and performing algorithmic reasoning over symbolically grounded inputs (Ebrahimi et al., 2021;Ibarz et al., 2022;Veli\u010dkovi\u0107 et al., 2022). As for logical reasoning with LLMs in particular, there have been a few different proposals for when and how to best combine the LLM with a symbolic component. Zhang et al. ( 2022) finetune a language model to synthesize potential facts paired with likelihoods and then use a handwritten differentiable symbolic reasoner in order to deduce other facts. Weir and Van Durme (2022) relax the solver by instead training neural \"entailment\" models to decide if and how a given inference rule applies at each stage. Concurrently to this work, Logic-LM (Pan et al., 2023) and SATLM (Ye et al., 2023) propose neurosymbolic approaches which have much in common with LINC. However, other than the models and datasets considered, their contributions have a few key differences to ours. First, we place particular emphasis on establishing an in-depth understanding of the relative benefits and drawbacks of a neurosymbolic approach to reasoning when compared to traditional in-context reasoning strategies like Chain-of-Thought. Second, Logic-LM employs a self-refinement strategy, which has shown promise across code generation and NLP tasks (Zhang et al., 2023b;Chen et al., 2023a;Peng et al., 2023;Madaan et al., 2023;Olausson et al., 2023) but which we do not consider here. Third, SATLM studies arithmetic reasoning in addition to logical reasoning, showcasing the versatility of the neurosymbolic approach. Fourth, and finally, we use an FOL representation that we believe is easier for humans to read and models to learn.",
            "score": 0.3875975678326027,
            "section_title": "Related Work",
            "char_start_offset": 25741,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 804
                },
                {
                    "start": 805,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1718
                },
                {
                    "start": 1719,
                    "end": 1852
                },
                {
                    "start": 1853,
                    "end": 1968
                }
            ],
            "ref_mentions": [
                {
                    "start": 250,
                    "end": 270,
                    "matchedPaperCorpusId": "58004690"
                },
                {
                    "start": 270,
                    "end": 292,
                    "matchedPaperCorpusId": "44098071"
                },
                {
                    "start": 384,
                    "end": 407,
                    "matchedPaperCorpusId": "231859814"
                },
                {
                    "start": 426,
                    "end": 450,
                    "matchedPaperCorpusId": "249210177"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0823974609375
        },
        {
            "corpus_id": "251953302",
            "title": "Generating Intermediate Steps for NLI with Next-Step Supervision",
            "text": "The NLP community has introduced several NLU tasks and datasets with accompanying explanations (Wiegreffe and Marasovi\u0107, 2021), ranging from highlighted context, natural language based or structured proof trees. Highlighted explanations hardly help in scenarios requiring external knowledge. Humanprovided natural language explanations (Camburu et al., 2018) are hard to validate; and structured explanations are hard to scale. For certain closedworld setting (Tafjord et al., 2021) and symbolic domains, synthetically generated explanations has been popular. Authors in (Nye et al., 2021) explore NATURALPROOFS, where the proof consists of both natural language and mathematical symbols. However, the underlying reasoning task being mathematical, makes the task more well-defined. A large body of work in the ATP community (Paliwal et al., 2020;Ayg\u00fcn et al., 2020;Agarwal et al., 2021) generates synthetic fine-grained steps as proofs for symbolic domains. Here, we define constraints over natural language proofs to ease validation and generation. \n\nIterative Proof Generation and Search: Tafjord et al. (2021) utilize synthetically generated end-to-end proof trees as supervision. Authors train a strong encoder-decoder model such as T5 to iteratively generate the next step of a proof, and use search to generate full proofs. Similarly, in the Automated Theorem Proving literature, recent work (Paliwal et al., 2020;Ayg\u00fcn et al., 2020;Agarwal et al., 2021) has shown that Graph Neural Networks and Transformers can be trained to perform impressively on the theorem-proving task as part of a neuro-symbolic system. Theorem-proving is a multiple-step search-based task, where the neural method learns to simplify an input goal into several sub-goals. This is done recursively using search, until all sub-goals are proven (i.e., simplified to empty goals). Further, These proof systems are easier to define as the number of operations (or theorems or tactics) comes from a known closed set. Thus, it is non-trivial to extend above methods to generate natural language proof without full end-to-end supervision.",
            "score": 0.3872793119416275,
            "section_title": "Categories of Proofs:",
            "char_start_offset": 22800,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 957
                },
                {
                    "start": 958,
                    "end": 1049
                },
                {
                    "start": 1052,
                    "end": 1183
                },
                {
                    "start": 1184,
                    "end": 1329
                },
                {
                    "start": 1330,
                    "end": 1617
                },
                {
                    "start": 1618,
                    "end": 1752
                },
                {
                    "start": 1753,
                    "end": 1857
                },
                {
                    "start": 1858,
                    "end": 1991
                },
                {
                    "start": 1992,
                    "end": 2111
                }
            ],
            "ref_mentions": [
                {
                    "start": 95,
                    "end": 126,
                    "matchedPaperCorpusId": "260513338"
                },
                {
                    "start": 336,
                    "end": 358,
                    "matchedPaperCorpusId": "54040953"
                },
                {
                    "start": 460,
                    "end": 482,
                    "matchedPaperCorpusId": "229371222"
                },
                {
                    "start": 824,
                    "end": 846,
                    "matchedPaperCorpusId": "165163797"
                },
                {
                    "start": 865,
                    "end": 886,
                    "matchedPaperCorpusId": "233443775"
                },
                {
                    "start": 1398,
                    "end": 1420,
                    "matchedPaperCorpusId": "165163797"
                },
                {
                    "start": 1439,
                    "end": 1460,
                    "matchedPaperCorpusId": "233443775"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6318359375
        },
        {
            "corpus_id": "259503802",
            "title": "Tabular and latent space synthetic data generation: a literature review",
            "text": "As discussed throughout our analysis, it appears that synthetic data generation research is generally isolated within ML problems and/or domains. Given the breadth and complexity of input-level and latent-level data generation mechanisms, it is increasingly important to find an a priori approach to efficiently determine appropriate data generation policies and techniques. However, the complexity of this task is determined by various factors: different data types, ML problems, model architectures, computational resources, performance metrics, and contextual constraints. Auto-augmentation and meta-learning aim to address this challenge and are still subject to active research. \n\nFinally, various synthetic data generation algorithms are research-based, and might not be usable or feasible to be implemented by practitioners [24]. One way to address this problem is to publish the code developed, and ideally make them available as opensource libraries for out-of-the-box usage.",
            "score": 0.38704866882156796,
            "section_title": "Future work",
            "char_start_offset": 76418,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 683
                },
                {
                    "start": 686,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 984
                }
            ],
            "ref_mentions": [
                {
                    "start": 831,
                    "end": 835,
                    "matchedPaperCorpusId": "235755489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6142578125
        },
        {
            "corpus_id": "273507276",
            "title": "Synthetic Network Traffic Data Generation: A Comparative Study",
            "text": "This section presents an in-depth comparative evaluation of twelve synthetic data generation methods applied to the NSL-KDD and CIC-IDS2017 datasets. The analysis is structured around four key objectives: fidelity, utility, class balance, and computational efficiency. Tables VI and VII summarize these results for both datasets. \n\nThe evaluation focuses on two distinct categories of methods: Non-AI (Statistical) and AI-based (Classical + Generative AI). The performance of these methods is assessed based on statistical similarity (SS), class balance (CB), and machine learning utility (measured as accuracy in TRTR vs. TSTR). Additionally, AI-based methods' computational cost and training efficiency are critically analyzed.",
            "score": 0.38704866882156796,
            "section_title": "VI. RESULTS AND DISCUSSION",
            "char_start_offset": 29849,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 329
                },
                {
                    "start": 332,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 629
                },
                {
                    "start": 630,
                    "end": 729
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4404296875
        },
        {
            "corpus_id": "277633831",
            "title": "DeduCE: Deductive Consistency as a Framework to Evaluate LLM Reasoning",
            "text": "Why Use Synthetic Dataset? Synthetic datasets offer a controlled framework for evaluating deductive accuracy, as all data points are generated according to predefined rules with precisely derived ground truth. This setup enables meticulous regulation of the underlying computation graph, allowing us to specify the total number of reasoning steps (i.e., edges in the graph) needed to arrive at the final answer. \n\nFurthermore, it becomes straightforward to craft questions that traverse designated nodes in a prescribed order. For example, a path can be orchestrated to move from the initial premise to an intermediate node A (\"prefix k\") and then from node A to the target node B (\"hop l\"), such that Distance(seed premise, B) = k+l. This level of control over the graph's structure proves highly valuable for generating datasets with specific properties and systematically assessing deductive performance. \n\nDataset Generation In constructing of SynDeduct dataset, we begin by sampling a set of DAGs according to parameters that define constants, variable distributions, and arithmetic operators. We then extract paths from each DAG as programmatically computed reasoning traces. The resulting ground-truth derivations are converted into Chain of Thought representations by applying a set of verbalization templates, yielding readable textual explanations. Unlike in GSM8K, our approach does not require code generation or templated Chains of Thought, as the underlying computation graph is already available. \n\nWe quantify a path's difficulty by counting the number of reasoning steps (graph edges) it takes to move from the initial (base) node to the final (target) node. Accordingly, we generate N sets of questions, where the n-th set contains questions that require n steps. \n\nTo accommodate varying input-premise lengths, we create additional questions by progressively appending segments of the ground-truth reasoning chain to converge on the same target premise. We then place these questions into bins based on how many hops are needed, intermixing different prefix lengths within each bin. This organization yields n bins, each focused on questions requiring n hops but differing in the prefixed portion of the chain. Such binning enables robust averaging of model performance for varying prefix lengths within the same number of steps. Details are present in Appendix A.7.",
            "score": 0.38704866882156796,
            "section_title": "REASONING EVALUATION ON SYNTHETIC DATASETS",
            "char_start_offset": 17753,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 26
                },
                {
                    "start": 27,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 411
                },
                {
                    "start": 414,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 907
                },
                {
                    "start": 910,
                    "end": 1098
                },
                {
                    "start": 1099,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1511
                },
                {
                    "start": 1514,
                    "end": 1675
                },
                {
                    "start": 1676,
                    "end": 1781
                },
                {
                    "start": 1784,
                    "end": 1972
                },
                {
                    "start": 1973,
                    "end": 2101
                },
                {
                    "start": 2102,
                    "end": 2229
                },
                {
                    "start": 2230,
                    "end": 2348
                },
                {
                    "start": 2349,
                    "end": 2385
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8359375
        },
        {
            "corpus_id": "269032965",
            "title": "A Survey on the Integration of Generative AI for Critical Thinking in Mobile Networks",
            "text": "In this section, we examine the latest advancements in utilizing GenAI algorithms for reasoning tasks.Our analysis employs a dual-axis classification: the first axis categorizes the reasoning tasks targeted by the approach, and the second axis identifies the reasoning type, as detailed in section III-A.To streamline our review of the state of the art, we divide our discussion into subsections.Each subsection addresses a distinct category of GenAI algorithms, namely, transformer, GAN, CVAE and diffusion model, introduced in Table I.\n\n1) LLMs: A thorough review of reasoning approaches using LLMs was presented in [8].In this section, we augment the insights of this survey by incorporating recent research that has emerged.An abbreviated version of reviewed literature is illustrated in figure 2.\n\nAs illustrated in the figure, we distinguish between three types of approaches, namely \"prompt engineering\", \"finetuning\" and \"hybrid\".\n\nPrompt engineering involves carefully crafting input queries to guide the model's responses and achieve desired outputs.It requires human input to design prompts that elicit the desired information or behavior from the model.\n\nOn the other hand, fine-tuning is a process where pretrained models are further trained on specific tasks or domains using labeled data.It involves adjusting model parameters to optimize performance for a particular task, making it more tailored and specialized.Both techniques play crucial roles in optimizing models for specific applications.Finally, hybrid solutions incorporate LLMs with other types of AI algorithms, such as symbolic and discriminative models.The intention is to combine the content generation and associative memory capabilities of LLMs, with the deterministic output and analytical abilities of symbolic solvers or classifiers.",
            "score": 0.3866288560196334,
            "section_title": "B. Reasoning Approaches in GenAI",
            "char_start_offset": 19125,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 102,
                    "end": 304
                },
                {
                    "start": 304,
                    "end": 396
                },
                {
                    "start": 396,
                    "end": 537
                },
                {
                    "start": 539,
                    "end": 622
                },
                {
                    "start": 622,
                    "end": 728
                },
                {
                    "start": 728,
                    "end": 801
                },
                {
                    "start": 803,
                    "end": 938
                },
                {
                    "start": 940,
                    "end": 1060
                },
                {
                    "start": 1060,
                    "end": 1165
                },
                {
                    "start": 1167,
                    "end": 1303
                },
                {
                    "start": 1303,
                    "end": 1429
                },
                {
                    "start": 1429,
                    "end": 1511
                },
                {
                    "start": 1511,
                    "end": 1632
                },
                {
                    "start": 1632,
                    "end": 1818
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75341796875
        },
        {
            "corpus_id": "261822699",
            "title": "Dynamic MOdularized Reasoning for Compositional Structured Explanation Generation",
            "text": "Performing reasoning, the capacity to draw conclusions from new or existing information, has remained an elusive goal of artificial intelligence research for decades. It yields multiple exploratory research directions, such as reading comprehension (Rajpurkar et al., 2018), natural language inference (Williams et al., 2018), story generation (Mostafazadeh et al., 2016), etc. Recently, neural models have shown remarkable performance on these tasks (Raffel et al., 2019). However, Russin et al. (2020); Mitchell (2021) argued that these models lack human-like reasoning capabilities. \n\nHumans excel in adopting compositional generalization (Hupkes et al., 2020), a capacity to com-sent1: puddles of water are outside during the day sent2: temperature is a measure of heat energy sent3: if something receives sunlight, it will increase in temperature hypothesis: the puddles of water will increase in heat energy hypothesis: the puddles of water will increase in heat energy int1: the puddles of water will increase in temperature sent2: temperature is a measure of heat energy sent3: if something receives sunlight, it will increase in temperature sent1: puddles of water will receive sunlight rt: if-then rt: substitution entailment tree candidate sentences & a hypothesis Figure 1: Structured explanation generation: generate an entailment tree including intermediate nodes (grey) for a hypothesis (green) and candidate sentences. Each reasoning step (e.g., sent1 & sent3 \u2192 int1) is independent and from one of six reasoning types (rt). bine an inventory of known constituents to predict larger compounds, during reasoning. For example, humans who understand calculation constituents, e.g., subtraction sub(X, Y) and addition-subtraction mixed operation sub(X, add(Y, Z)), are able to solve a larger compound, e.g., sub(W, sub(X, add(Y, Z))). Various benchmarks have been proposed to explore compositional generalization abilities in reasoning tasks.",
            "score": 0.38615531245325607,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 585
                },
                {
                    "start": 588,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1540
                },
                {
                    "start": 1541,
                    "end": 1627
                },
                {
                    "start": 1628,
                    "end": 1846
                },
                {
                    "start": 1847,
                    "end": 1954
                }
            ],
            "ref_mentions": [
                {
                    "start": 249,
                    "end": 273,
                    "matchedPaperCorpusId": "47018994"
                },
                {
                    "start": 302,
                    "end": 325,
                    "matchedPaperCorpusId": "3432876"
                },
                {
                    "start": 344,
                    "end": 371,
                    "matchedPaperCorpusId": "1726501"
                },
                {
                    "start": 642,
                    "end": 663,
                    "matchedPaperCorpusId": "211259383"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.152587890625
        },
        {
            "corpus_id": "277625872",
            "title": "Synthetic Data in Healthcare and Drug Development: Definitions, Regulatory Frameworks, Issues",
            "text": "In the context of data-driven (hence not process-driven) generation processes [5], generated synthetic data (also referenced as \"artificial\" [20] or \"simulated\" data [21,22]) have been in use for many years, albeit to a lesser extent than today. An example is imputation, i.e., filling in missing values: Artificial Intelligence (AI) models allow users to go beyond simpler heuristics such as using the mean (for numerical) or mode (for categorical) value by learning how other features influence the value to be filled in. Dong et al. \n\nprovide an example of this as an application in healthcare [23]. \n\nHowever, it is only in the last decade that methodological advances in AI (e.g., the attention mechanism [24]) and sufficient computing power [25] have made it possible to reliably generate high-fidelity synthetic values at the level of entire datasets. Another important enabling factor is self-supervised pre-training: a family of techniques to train an AI model to be generally useful on a variety of tasks by utilizing large amounts of data, without the need for human annotations [26,27]. When it comes to clinical trials, pretraining may enable synthetic data generation for ECAs, as data from control arms of different trials may be combined in a sufficiently large dataset to feed these algorithms [16]. \n\nModern, data-driven, generative AI models include GANs, VAEs, Diffusion Models (DMs), and Transformers [24,28,29,30]. These models operate in two phases. Firstly, a model is trained using observed data: model parameters are adjusted such that the synthetic data produced by the model is similar to the original data. After the training, the model parameters are fixed, with the model now ready to generate synthetic data with statistical properties that are quasi-identical to those of the original observed data source [31] without the generated data being directly linked to any particular individual present in the originating data [32]. In this second phase (inference), the AI model can be used to generate, or \"sample,\" synthetic data at will. For a more detailed review on the topic, we refer to [33].",
            "score": 0.3847311003339635,
            "section_title": "| AI-Generated Data: New Frontier With New Complexities",
            "char_start_offset": 5526,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 535
                },
                {
                    "start": 538,
                    "end": 602
                },
                {
                    "start": 605,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 1098
                },
                {
                    "start": 1099,
                    "end": 1316
                },
                {
                    "start": 1319,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1472
                },
                {
                    "start": 1473,
                    "end": 1635
                },
                {
                    "start": 1636,
                    "end": 1959
                },
                {
                    "start": 1960,
                    "end": 2068
                },
                {
                    "start": 2069,
                    "end": 2127
                }
            ],
            "ref_mentions": [
                {
                    "start": 78,
                    "end": 81,
                    "matchedPaperCorpusId": "269611568"
                },
                {
                    "start": 141,
                    "end": 145,
                    "matchedPaperCorpusId": "259304385"
                },
                {
                    "start": 166,
                    "end": 170,
                    "matchedPaperCorpusId": "69536528"
                },
                {
                    "start": 170,
                    "end": 173,
                    "matchedPaperCorpusId": "265192245"
                },
                {
                    "start": 597,
                    "end": 601,
                    "matchedPaperCorpusId": "233313557"
                },
                {
                    "start": 710,
                    "end": 714,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1090,
                    "end": 1094,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1094,
                    "end": 1097,
                    "matchedPaperCorpusId": "219687051"
                },
                {
                    "start": 1311,
                    "end": 1315,
                    "matchedPaperCorpusId": "221861770"
                },
                {
                    "start": 1422,
                    "end": 1426,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1426,
                    "end": 1429,
                    "matchedPaperCorpusId": "1033682"
                },
                {
                    "start": 1429,
                    "end": 1432,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 1839,
                    "end": 1843,
                    "matchedPaperCorpusId": "259857336"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88525390625
        },
        {
            "corpus_id": "269137153",
            "title": "Distilling Algorithmic Reasoning from LLMs via Explaining Solution Programs",
            "text": "Recent Large Language Models (LLMs) have shown impressive capabilities for various reasoning tasks, including multi-hop question answering (Wang et al., 2022;Lyu et al., 2023), symbolic reasoning (Hua & Zhang, 2022), and math word problem-solving (Chen et al., 2022;Zhou et al., 2023).Chain-of-thought (CoT) prompting (Wei et al., 2022) addresses limitations of previous LLMs by instructing them to generate intermediate steps towards the final answer, thereby decomposing complex problems step-by-step.\n\nHowever, challenges remain, particularly in complex reasoning tasks like algorithmic programming.For example, the majority of human competitors still outperform advanced models like GPT-4 in Codeforces contests (OpenAI, 2023b).Complex programming problems have stringent time and space complexity constraints, where straightforward implementation methods like (Chen et al., 2021;Yin et al., 2018;Hendrycks et al., 2021), often yield time-consuming brute-force solutions.\n\nA number of efforts have been made to tackle this challenging task (Li et al., 2022;Zhang et al., 2023;Olausson et al., 2023;Ridnik et al., 2024) by adding extra clustering or verification steps to filter or iteratively refine generated programs.While those methods focus on flow engineering for code generation, there have been limited attempts to explicitly enhance models' intrinsic reasoning abilities in this context.\n\nHuman-written rationales for solving algorithmic reasoning problems, known as editorials, are hard to collect as they are often posted on personal blogs or as tutorial videos.An alternative is to distill such natural-language-described problem-solving strategies from larger models.Distilling explicit chain-of-thoughts (CoT) reasoning processes has been shown as an effective method to learn multi-step reasoning from larger models (Hsieh et al., 2023;Yue et al., 2023).Usually, a teacher model is required to solve a set of problems while giving CoT reasoning paths at the same time, 1 arXiv:2404.08148v1[cs.CL] 11 Apr 2024 as illustrated in Figure 1.",
            "score": 0.38418249405154237,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 285
                },
                {
                    "start": 285,
                    "end": 503
                },
                {
                    "start": 505,
                    "end": 602
                },
                {
                    "start": 602,
                    "end": 732
                },
                {
                    "start": 732,
                    "end": 975
                },
                {
                    "start": 977,
                    "end": 1223
                },
                {
                    "start": 1223,
                    "end": 1399
                },
                {
                    "start": 1401,
                    "end": 1576
                },
                {
                    "start": 1576,
                    "end": 1683
                },
                {
                    "start": 1683,
                    "end": 1872
                },
                {
                    "start": 1872,
                    "end": 2007
                },
                {
                    "start": 2007,
                    "end": 2054
                }
            ],
            "ref_mentions": [
                {
                    "start": 139,
                    "end": 158,
                    "matchedPaperCorpusId": "253098851"
                },
                {
                    "start": 196,
                    "end": 215,
                    "matchedPaperCorpusId": "256631030"
                },
                {
                    "start": 1044,
                    "end": 1061,
                    "matchedPaperCorpusId": "246527904"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.336181640625
        },
        {
            "corpus_id": "273403896",
            "title": "A Survey on Data Synthesis and Augmentation for Large Language Models",
            "text": "To enhance the reasoning capability of LLMs, additional reasoning steps are generated in the process of data synthesis. The synthetic question-response pairs in MMIQC [131] are iteratively constructed by augmenting the initial problems and adding additional reasoning steps without altering their intrinsic logical structure. Similarly, an effective generation strategy is put forward in which an LLM is requested to synthesize chain-of-thought (CoT) answers after question generation and verification [109]. Based on the generation of question-CoT pairs through Self-Instruct, MathInstruct [244] further supplements the Program-of-Thought (PoT) rationale to simplify the math-solving process. \n\nSynthesize with Controllability. To control the quality of synthetic data, researches are conducted into techniques for data synthesis with controllability. Driven by the goal of reducing the potential bias of synthetic data, OSS-Instruct [215] utilizes opensource seed code snippets to prompt an LLM in generating coding problems and corresponding solutions. The seed snippets provide controllability of the generation and encourage the LLM to synthesize a variety of coding problems. Similarly, Genie [236] prompts an Table 1: Data synthesis and augmentation in data preparation. In the table, method outlines the techniques presented by each research. Data source and synthetic data indicate the original data used to generate synthetic data and the synthetic data created for training purposes, respectively. A dash (-) in any cell denotes that the respective content was not mentioned in the cited literature.",
            "score": 0.3828842772962145,
            "section_title": "Data Preparation",
            "char_start_offset": 16060,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 693
                },
                {
                    "start": 696,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1350
                },
                {
                    "start": 1351,
                    "end": 1508
                },
                {
                    "start": 1509,
                    "end": 1610
                }
            ],
            "ref_mentions": [
                {
                    "start": 167,
                    "end": 172,
                    "matchedPaperCorpusId": "267028678"
                },
                {
                    "start": 935,
                    "end": 940,
                    "matchedPaperCorpusId": "270358041"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86181640625
        },
        {
            "corpus_id": "265043634",
            "title": "Enhancing LLM Intelligence with ARM-RAG: Auxiliary Rationale Memory for Retrieval Augmented Generation",
            "text": "Human decision-making often results from extended chains of thought. It has been demonstrated that prompting with explicit intermediate reasoning can enhance the performance of language models on complex tasks. (Zelikman et al., 2022) investigate how these reasoning chains, also known as rationales, can be utilized in a feedback loop to improve the performance of LLMs. The project's goal is to enhance the quality of the rationales generated by LLMs, thereby improving the model's accuracy on problems that require reasoning. They observe that improving rationale generation can be achieved by fine-tuning a set of rationales; however, manually creating such a dataset can be extremely labor-intensive. Their approach is to leverage the pre-existing reasoning abilities of LLMs to iteratively bootstrap a model's capacity to generate high-quality rationales. Their bootstrapping protocol unfolds in three steps. First, they prompt an LLM with a few examples to \"selfgenerate\" rationales. Next, they refine the model's ability to produce better rationales by fine-tuning it with those rationales that lead to correct answers. Finally, they repeat the process with the improved model until no further performance enhancements are observed. \n\nThey note that their bootstrapping routine enhances performance on familiar problems but falls short in solving new ones because the model does not receive feedback for incorrectly answered problems. To address this, for every problem the model fails to solve, they manually create a rationalization that includes the correct answer and incorpo-rate both the problem and its rationalization into the fine-tuning training set. This method improves performance on previously unseen problems. \n\nTo evaluate their system, they use three data sets. The first is a generated data set that synthesizes multi-digit integer addition problems. Each example includes an input, an answer, and a \"scratchpad\" that breaks down the individual steps required to solve the problem correctly. An example of the multi-digit addition problem is depicted in Figure 5.",
            "score": 0.38273697429978826,
            "section_title": "Chain-of-thought reasoning",
            "char_start_offset": 9070,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 68
                },
                {
                    "start": 69,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 705
                },
                {
                    "start": 706,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1127
                },
                {
                    "start": 1128,
                    "end": 1240
                },
                {
                    "start": 1243,
                    "end": 1442
                },
                {
                    "start": 1443,
                    "end": 1668
                },
                {
                    "start": 1669,
                    "end": 1732
                },
                {
                    "start": 1735,
                    "end": 1786
                },
                {
                    "start": 1787,
                    "end": 1876
                },
                {
                    "start": 1877,
                    "end": 2017
                },
                {
                    "start": 2018,
                    "end": 2089
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2132568359375
        },
        {
            "corpus_id": "258236273",
            "title": "Neurosymbolic Models for Computer Graphics",
            "text": "Program synthesis is an important area of research in programming languages that investigates methods for automatically generating programs that satisfy some high-level specifications. While the dream of automating code generation can be traced back to the birth of computer science, and the first paper that proposed a synthesis algorithm dates from 1957 [BBB * 57], it was not until the last couple of decades that the field saw great advances in what is called inductive synthesis [SLTB * 06, ABJ * 13]. In inductive synthesis, users give a (potentially partial) specification to describe the desired intent, and search methods are used to explore the space of possible programs generating one that satisfies the specifications. This involves (i) developing methods for specifying user intent, (ii) defining a search space by restricting an existent language or designing novel domain-specific languages, and (iii) developing search algorithms to efficiently explore this space. \n\nThe field of program synthesis has advanced by proposing different interfaces for specifying intent, ranging from input-output examples, demonstrations, natural languages, partial programs, and assertions. Search algorithms include enumeration, constraint solving, probabilistic search, and combinations thereof. The first commercial application of program synthesis was FlashFill in Excel 2013, which derives small programs from data manipulation examples [Gul11]. Today, program synthesis is used in many applications, and frameworks such as Sketch [SLTB * 06] and Rosette [TB13] make it easy to develop synthesis tools for new languages. We refer the reader to [GPS * 17] for an overview of traditional synthesis techniques; we will discuss search algorithms relevant to neural symbolic reasoning in further detail in Section 3.3. \n\nIn recent years, program synthesis techniques have also been applied in novel and exciting ways to solve problems in computer graphics. These applications are inspired by procedural representations of shapes which transform modeling into a code generation task. Despite these advances, there are still fundamental challenges in expanding the reasoning capabilities of synthesis techniques to complex domains. This is because (i) the search space grows exponentially with the size of the synthesized code and (ii) because inferring intent from natural forms of human interaction is challenging [GSLT * 18].",
            "score": 0.38211868075454036,
            "section_title": "Program Synthesis",
            "char_start_offset": 16647,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 981
                },
                {
                    "start": 984,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1449
                },
                {
                    "start": 1450,
                    "end": 1624
                },
                {
                    "start": 1625,
                    "end": 1817
                },
                {
                    "start": 1820,
                    "end": 1955
                },
                {
                    "start": 1956,
                    "end": 2081
                },
                {
                    "start": 2082,
                    "end": 2228
                },
                {
                    "start": 2229,
                    "end": 2425
                }
            ],
            "ref_mentions": [
                {
                    "start": 1559,
                    "end": 1565,
                    "matchedPaperCorpusId": "16038701"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4951171875
        },
        {
            "corpus_id": "257921623",
            "title": "REFINER: Reasoning Feedback on Intermediate Representations",
            "text": "Problem Formulation. In this paper, we view natural language reasoning (NLR) as an autoregressive generation task where, given input context x, a model needs to generate y, such that y satisfies the constraints of the task. Usually, to generate correct or plausible y, the model needs to make the correct inference z as intermediate steps. 4 We decompose NLR tasks as follows: p(y|x) = p(y|x, z)p(z|x). In practice, one can compute each conditional using an LM that includes its conditioning variables as a part of its input. \n\nBefore continuing with the model description, we describe three NLR tasks where we conduct our study and their respective intermediate representation z. We deliberately chose these three tasks since they broadly cover two types of reasoning: (i) logical reasoning and (ii) normative reasoning. They are exemplified in Appx Fig. 6 and detailed below. \n\nMath word problem (MWP), where given a word problem x consisting of a context and question, the goal is to map x to a valid mathematical expression z (the intermediate representation) and then to a solution y. This task requires the model to perform deduction using mathematical reasoning. Synthetic natural language reasoning (sNLR), where given a reasoning scenario x consisting of 5 synthetic rules and a fact, the model needs to deduce a conclusion y. This task requires the model to perform deductive reasoning and generate intermediate steps z and the conclusion y using closedworld rules and facts. Moral norm and action generation for moral stories (MS), where given a context x consisting of a situation, an intention, and an immoral action, the model needs to generate the moral norm z and the moral action y. Moral actions are encouraged by the moral norm. This task requires the model to perform abductive reasoning to generate moral norms and deductive reasoning for moral action. \n\nWe propose to solve these tasks by forcing the model to generate intermediate hypotheses (z) and improving them via structured feedback.",
            "score": 0.3814511059886691,
            "section_title": "REFINER",
            "char_start_offset": 7970,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 20
                },
                {
                    "start": 21,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 525
                },
                {
                    "start": 528,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 877
                },
                {
                    "start": 880,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1485
                },
                {
                    "start": 1486,
                    "end": 1699
                },
                {
                    "start": 1700,
                    "end": 1747
                },
                {
                    "start": 1748,
                    "end": 1873
                },
                {
                    "start": 1876,
                    "end": 2012
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2666015625
        },
        {
            "corpus_id": "272753147",
            "title": "Enhancing Logical Reasoning in Large Language Models through Graph-based Synthetic Data",
            "text": "The reasoning capabilities of Large Language Models (LLMs) (Touvron et al., 2023;Jiang et al., 2023;Dubey et al., 2024) can be greatly enhanced by post-training techniques (Ouyang et al., 2022;Zhang et al., 2023) and prompting strategies (Wei et al., 2022b;Yao et al., 2023;Madaan et al., 2023). However, even with the aforementioned techniques, the multi-hop reasoning tasks remain challenging (Touvron et al., 2023;Jiang et al., 2023;Dubey et al., 2024): LLMs struggle to reason over steps (Agrawal et al., 2024;Zhao and Zhang, 2024), and are fragile to minor perturbations (Ullman, 2023;Chen et al., 2024) in the input prompt. \n\nRecently, several works (Xu et al., 2024;Abdin et al., 2024;Anil et al., 2023) have demonstrated the efficacy of boosting the LLMs' reasoning capacity via fine-tuning on synthetic data generated by stronger LLMs. However, how to make such synthetic data generation effective and controllable for specific applications remains an open question. \n\nExtensive prompt engineering and quality filtering are required to guide LLMs' generation, yet the quality of generated reasoning questions and their labels remains uncertain (Gudibande et al., 2023;Wang et al., 2023;Tan et al., 2024). Motivated by the fact that natural language reasoning tasks can be represented as structured data with finite nodes and edges (Jin et al., 2024), and inspired by existing works on constructing reasoning benchmarks (Fatemi et al., 2024;Kazemi et al., 2023;Agrawal et al., 2024), we propose to leverage synthetic graph-based data for task-specific posttraining adaptation to improve the correctness of the generated reasoning questions and labels. \n\nIn this paper, we carefully design a random walk sampling algorithm on graphs and introduce a new prompting strategy that first extracts a reasoning chain and then derives the answer.",
            "score": 0.3810596343791894,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 629
                },
                {
                    "start": 632,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 975
                },
                {
                    "start": 978,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1659
                },
                {
                    "start": 1662,
                    "end": 1845
                }
            ],
            "ref_mentions": [
                {
                    "start": 590,
                    "end": 608,
                    "matchedPaperCorpusId": "267657940"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.951171875
        },
        {
            "corpus_id": "272770433",
            "title": "CraftRTL: High-quality Synthetic Data Generation for Verilog Code Models with Correct-by-Construction Non-Textual Representations and Targeted Code Repair",
            "text": "Synthetic Data Generation for Model Fine-tuning. The performance of large language models (LLMs) hinge on the quality and diversity of their training data. To address the limitations of manual datasets, synthetic data generation methods (Wang et al., 2022;Xu et al., 2023) have been developed \n\nPreprint to automatically create instruction-following examples from LLMs, reducing reliance on human annotations. Various techniques enhance data quality: Wang et al. (2022) generates multiple reasoning traces and selects the most frequent output to improve robustness, while other approaches (Lightman et al., 2023;Zhang et al., 2024b) assess response quality based on these traces. Self-training methods utilize synthetic data for iterative fine-tuning, boosting reasoning capabilities (Singh et al., 2023;Feng et al., 2023). These advancements show how synthetic data can effectively scale and optimize models through iterative feedback. \n\nLarge Language Models for Code Generation. Recent breakthroughs in large language models (LLMs) have greatly enhanced their capability to tackle complex code generation tasks. Much of the research focuses on developing LLMs specialized for code by continuing their pretraining on code data (Guo et al., 2024;Bai et al., 2023;Roziere et al., 2023;DeepSeek-AI et al., 2024) from open-source repositories like GitHub (Kocetkov et al., 2022;Lozhkov et al., 2024) and commit histories (Muennighoff et al., 2023). Further improvements to these models come from reinforcement learning (Le et al., 2022) and more often instruction fine-tuning, which involves techniques to address more complex coding problems (Luo et al., 2024b), increasing diversity with unlabeled open-source code (Wei et al., 2023;Yu et al., 2024;Wu et al., 2024), ensuring solution correctness through self-written tests (Chen et al., 2022), and validating and debugging code execution through interactions with LLM agents (Lei et al., 2024).",
            "score": 0.3799239128319761,
            "section_title": "RELATED WORK",
            "char_start_offset": 24430,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 48
                },
                {
                    "start": 49,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 292
                },
                {
                    "start": 295,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 936
                },
                {
                    "start": 939,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1446
                },
                {
                    "start": 1447,
                    "end": 1945
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95361328125
        },
        {
            "corpus_id": "276318012",
            "title": "MIH-TCCT: Mitigating Inconsistent Hallucinations in LLMs via Event-Driven Text-Code Cyclic Training",
            "text": "Based on the investigation of synthetic datasets, Chain-of-Thought (CoT) strategies have also reduced hallucinations during reasoning tasks (Wei et al., 2022). Some research has synthesized reasoning data in the field of logical reasoning (Nie et al., 2020;Saeed et al., 2021) while providing the reasoning process (Dalvi et al., 2021) during synthesis. For example, LOGIC-LM integrates LLMs with symbolic solvers, transforming natural language problems into symbolic formulations to minimize inconsistencies (Pan et al., 2023). Similarly, Sym-bCOT enhances CoT prompting by incorporating symbolic expressions and logical rules (Xu et al., 2024). \n\nThe expressiveness of symbolic solvers limits the applicability of these models. Not all problems can be easily encoded in first-order logic, and complexities may arise when dealing with intricate grammatical structures, such as those found in probabilistic soft logic. Therefore, while these approaches show promise, they have constraints in flexibility and generalizability, highlighting the need for solutions that can effectively address a broader range of reasoning scenarios.",
            "score": 0.37955109953353117,
            "section_title": "COT Data to Reduce Hallucinations",
            "char_start_offset": 7723,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 646
                },
                {
                    "start": 649,
                    "end": 729
                },
                {
                    "start": 730,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1130
                }
            ],
            "ref_mentions": [
                {
                    "start": 140,
                    "end": 158,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 239,
                    "end": 257,
                    "matchedPaperCorpusId": "207756753"
                },
                {
                    "start": 257,
                    "end": 276,
                    "matchedPaperCorpusId": "237940562"
                },
                {
                    "start": 315,
                    "end": 334,
                    "matchedPaperCorpusId": "233297051"
                },
                {
                    "start": 509,
                    "end": 527,
                    "matchedPaperCorpusId": "258833332"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4013671875
        },
        {
            "corpus_id": "258832686",
            "title": "LogiCoT: Logical Chain-of-Thought Instruction-Tuning Data Collection with GPT-4",
            "text": "This notion inspired researchers to use self-generated rationales for demonstrations. In particular, Zelikman et al. (2022) demonstrated the practicality of using LLMs to generate rationales. They prompted GPT-J (Wang and Komatsuzaki, 2021) to generate rationales and then selected the ones leading to the correct answer. We adopt this method for GPT-4 generation. Our approach, however, tackles complex logical reasoning scenarios utilizing questions with annotated answers. \n\nLogical reasoning. Logical reasoning is a key aspect of human cognition and a critical capability for AI systems. Researchers have been exploring various approaches to achieve this goal, including rule-based methods, symbolic systems (Mac-Cartney and Manning, 2007), fine-tuning large language models (Wang et al., 2018), and combining both neural and symbolic approaches (Li and Srikumar, 2019). Logical reasoning tasks often require multi-step, complex reasoning, which makes them an ideal target for CoT instruction tuning. By integrating logical reasoning tasks into CoT instruction tuning, we can push the boundaries of what AI systems can achieve and get closer to systems that can understand and reason about the world in a human-like way.",
            "score": 0.37955109953353117,
            "section_title": "Related Work",
            "char_start_offset": 7277,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 85
                },
                {
                    "start": 86,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 475
                },
                {
                    "start": 478,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1224
                }
            ],
            "ref_mentions": [
                {
                    "start": 779,
                    "end": 798,
                    "matchedPaperCorpusId": "5034059"
                },
                {
                    "start": 850,
                    "end": 873,
                    "matchedPaperCorpusId": "189898023"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50830078125
        },
        {
            "corpus_id": "265213146",
            "title": "Transformers in the Service of Description Logic-Based Contexts",
            "text": "We generated the only large dataset (384K) in the literature that targets expressive DLs (namely, ALCQ), that enjoys both high expressivity and high linguistic complexity and is publicly available for further understanding of the functionality of TLMs. We showed that our DeBERTa-based model, DELTAM , can carry out entailment checking over expressive synthetic datasets with very high accuracy, regardless of the linguistic complexity of the context. Differently from recent results in the literature, we showed that our model has learned to generalize on unseen reasoning depths, smaller or greater. Zeroshot tests showed that DELTAM is mostly robust to other distributions. Tests with the GPT family showed that GPT-4 can have significant performance with only a few shots. The high accuracy of zero-shot testings in a real-world scenario demonstrates the potential of TLMs for performing reasoning tasks bypassing the necessity for domain experts to be familiar with formal representations. \n\nOur qualitative tests revealed the need for the development of systematic evaluation techniques of synthetically generated datasets. Hence, this will be our next step in future work. Furthermore, we plan to explore the upper limit of the expressivity of the logic language so that a transformer-based model will be able to perform reasoning tasks with high accuracy. \n\nFinally, we will expand our evaluation section with other state-ofthe-art generative models. \n\nTo generate the KBs, we employ a random sampling technique to select a subset of individuals, roles, and atomic Concepts from the pools mentioned above. An item from each pool has the same probability of being chosen. \n\nThe probabilistic context-free grammar for the linguistic complexity level L = 0 is shown in Table 12, the rest can be found in the supplementary material files. The PCFG shown is for Pool B. The grammars for the Pool A are similar. The probabilities in the PCFGs were determined experimentally to generate appropriate KBs that would yield the desired inferences in the minimum amount of time.",
            "score": 0.37955109953353117,
            "section_title": "Conclusions and Future Work",
            "char_start_offset": 33856,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 994
                },
                {
                    "start": 997,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1363
                },
                {
                    "start": 1366,
                    "end": 1458
                },
                {
                    "start": 1461,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1678
                },
                {
                    "start": 1681,
                    "end": 1842
                },
                {
                    "start": 1843,
                    "end": 1913
                },
                {
                    "start": 1914,
                    "end": 2074
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.818359375
        },
        {
            "corpus_id": "253098787",
            "title": "MetaLogic: Logical Reasoning Explanations with Fine-Grained Structure",
            "text": "To investigate how well current models can learn to generate the reasoning graphs, We use different ratios of training data to train the models and present the results in Figure 5. Overall, the model performances show a rapid increase within 20% of training data, then a flat and steady increase and do not reach a platform, indicating that the models can still benefit from more structural reasoning data. Among the models, MetGen has the most significant growth trend and performs data efficiently with small data, showing the advantages of the module-based learning framework in symbolic reasoning. Interestingly, we find out that the performance of multitask T5 decreases after seeing half of the training data. A possible explanation is that the decomposed logical structure as independent sub-tasks prevents the models from a holistic understanding of the logical passages. Besides that, the flat increasing rate after seeing 20% of the training data also suggests that blindly increasing the training data scale may not be the most efficient way of teaching models to conduct such a complex reasoning task.",
            "score": 0.37935159632858795,
            "section_title": "Data Scale for Logical Inference",
            "char_start_offset": 25793,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0552978515625
        },
        {
            "corpus_id": "275212256",
            "title": "An Overview and Discussion on Using Large Language Models for Implementation Generation of Solutions to Open-Ended Problems",
            "text": "Retrieval-Augmented Generation (RAG) addresses one of the major issues of LLMs, which are their lack of a persistent, reliable memory and factual grounding [78]. RAG methods integrate external knowledge sources into the generation process. Instead of relying solely on learned representations within the model's parameters, the system retrieves relevant documents, facts, or structured data at inference time and incorporates this information into its output. This grounding reduces hallucinations, ensures that the model's reasoning steps reference accurate and up-to-date information, and can improve the alignment of the solution with real-world constraints [79]. The versatility of RAG has led to significant advancements in various domains, such as healthcare, finance, education, and scientific research facilitated by novel frameworks tailored to address challenges in reasoning, problem-solving, and knowledge integration. This review categorized these advancements into four areas: task-specific and schema-based techniques, self-aware and adaptive mechanisms, long-term memory integration, and multi-hop and multi-modal reasoning. The four areas are discussed next. \n\na) Task-Specific and Schema-Based Retrieval (TSR): TSR approaches leverage structured methods to solve problems in domains such as mathematics and knowledge-intensive tasks. For instance, Schema-Based Instruction Retrieval-Augmented Generation (SBI-RAG) [80] employs schema-based instruction to solve math word problems by predicting relevant schemas, offering a structured problem-solving paradigm. Schemas, which act as templates for organizing and applying domainspecific knowledge, are inherently tied to knowledge graphs that map relationships between concepts, enhancing reasoning capabilities. The model selects the most suitable schema by aligning the problem context with predefined patterns and uses it to guide the solution process in a systematic manner. Similarly, Knowledge Graph-Enhanced RAG Framework (KRAGEN) [81] employs advanced prompting techniques, notably the graph-of-thoughts (GoT) method, to dynamically decompose complex problems into smaller subproblems. Each subproblem is addressed using relevant knowledge retrieved through the RAG framework, minimizing hallucinations and enhancing solution accuracy.",
            "score": 0.37934455473088313,
            "section_title": "C. Knowledge Retrieval",
            "char_start_offset": 30026,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1175
                },
                {
                    "start": 1178,
                    "end": 1351
                },
                {
                    "start": 1352,
                    "end": 1577
                },
                {
                    "start": 1578,
                    "end": 1778
                },
                {
                    "start": 1779,
                    "end": 1944
                },
                {
                    "start": 1945,
                    "end": 2159
                },
                {
                    "start": 2160,
                    "end": 2309
                }
            ],
            "ref_mentions": [
                {
                    "start": 156,
                    "end": 160,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 2004,
                    "end": 2008,
                    "matchedPaperCorpusId": "270225393"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.603515625
        },
        {
            "corpus_id": "265150088",
            "title": "Are LLMs Rigorous Logical Reasoner? Empowering Natural Language Proof Generation with Contrastive Stepwise Decoding",
            "text": "Stepwise Generation \n\nStepwise Correction Direction Search Verifier HA Proof Stage \n\nCoT (Wei et al., 2022) \n\nEntailmentWriter (Dalvi et al., 2021) (Bostrom et al., 2022) FT (Hong et al., 2022a) FT Addressing this gap, we introduce a novel strategy dubbed ConDec (Contrastive Decoding), designed to enhance the generative aspect of LLMs for deductive reasoning tasks. ConDec leverages carefully constructed hard negatives -outputs that are deceivingly similar in form yet differ semantically -to refine generation precision. These hard negatives can be simple sequence alterations or products of an intricate sampling and reasoning process, aided by an external reasoner and checker. Finetuning with these hard negatives notably advances the LLMs' proficiency in intermediate step and conclusion generation, culminating in overall improved proof accuracy. The main contributions of this study are threefold: \n\n\u2022 We reveal that LLMs equipped with various in-context learning strategies still struggle to perform rigorous logical reasoning in natural language proof generation tasks; \n\n\u2022 We introduce ConDec, a contrastive decoding strategy that enhances stepwise generative quality in proof generation tasks, and devise an automatic method for hard negative generation involving a reasoner and a checker; \n\n\u2022 We conduct an extensive empirical analysis on the Entailment benchmark, demonstrating the effectiveness of the proposed method. \n\n2 Related Work",
            "score": 0.37920347311690494,
            "section_title": "Type",
            "char_start_offset": 2550,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 19
                },
                {
                    "start": 22,
                    "end": 82
                },
                {
                    "start": 85,
                    "end": 107
                },
                {
                    "start": 110,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 907
                },
                {
                    "start": 910,
                    "end": 1081
                },
                {
                    "start": 1084,
                    "end": 1303
                },
                {
                    "start": 1306,
                    "end": 1435
                },
                {
                    "start": 1438,
                    "end": 1452
                }
            ],
            "ref_mentions": [
                {
                    "start": 127,
                    "end": 147,
                    "matchedPaperCorpusId": "233297051"
                },
                {
                    "start": 148,
                    "end": 170,
                    "matchedPaperCorpusId": "246015349"
                },
                {
                    "start": 174,
                    "end": 194,
                    "matchedPaperCorpusId": "248524641"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.181640625
        },
        {
            "corpus_id": "278171013",
            "title": "Can Large Language Models Learn Formal Logic? A Data-Driven Training and Evaluation Framework",
            "text": "We believe that our results indicate that LLMs can be taught to develop generalizable capabilities for formal reasoning, up to some limits. Our experiments (Section 4) show that fine-tuning with a small synthetic proof data set boosts reasoning ability, surpassing the performance that significantly larger models such as GPT-4o attain with few-shot learning. \n\nMatching intuition, template transformations considerably improve reasoning accuracy. \n\nThe sharp drop in accuracy on the depth metric does not necessarily indicate an inability to reason. In fact, such a decrease should be expected of any reasoning method given the co-NP hardness of the Boolean reasoning problem. These results support the observation that current techniques struggle to maintain reasoning performance for out-of-distribution depth instances (cf. (Anil et al., 2022)). \n\nWhile the proposed data generation and evaluation approaches demonstrate potential, significant challenges remain in developing models capable of accurate generalpurpose reasoning. A key hurdle to overcome is the gap between real-world problems and specialized LLMs. Everyday human reasoning ranges over multiple, diverse formal frameworks, including first-order quantification such as \"for all\" and \"there exists;\" temporal concepts like \"yesterday,\" \"tomorrow,\" \"eventually,\" and \"previously;\" and modal notions like \"possible\" and \"inevitable.\" General reasoning also relies on so-called commonsense facts about our world (e.g., a ball dropped from a height will bounce back). Methods that allow LLMs to acquire, integrate, and reason with such knowledge are currently limited (cyc; nvi). \n\nThese findings highlight the need for further research. A Boolean-reasoning LLM, as introduced in this paper, is not intended as a solution to the general reasoning question. \n\nOne should rather view it as a convenient, precise, and fully automatable setting in which to experiment with approaches to the reasoning task.",
            "score": 0.3789978664134545,
            "section_title": "Discussion and Conclusion",
            "char_start_offset": 31578,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 359
                },
                {
                    "start": 362,
                    "end": 447
                },
                {
                    "start": 450,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 849
                },
                {
                    "start": 852,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1531
                },
                {
                    "start": 1532,
                    "end": 1643
                },
                {
                    "start": 1646,
                    "end": 1701
                },
                {
                    "start": 1702,
                    "end": 1820
                },
                {
                    "start": 1823,
                    "end": 1966
                }
            ],
            "ref_mentions": [
                {
                    "start": 828,
                    "end": 847,
                    "matchedPaperCorpusId": "250425737"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55126953125
        },
        {
            "corpus_id": "273403575",
            "title": "MIND: Math Informed syNthetic Dialogues for Pretraining LLMs",
            "text": "Additionally, our experiment with entire OpenWebMath (OWM-14B) and its corresponding synthetic conversa-tions shows a consistent trend, indicating that the benefits of conversational data continue to hold as the data scales. In fact, with all conversations generated from OWM-4B, we can outperform model trained with OWM-14B, a 3.6\u00d7 larger data-2.94% average improvement across GSM8K and MATH tasks, 1.56% across all benchmarks (Figure 1). This underlines the value of synthetic conversations, particularly when high-quality in-domain data is limited. Moreover, our analysis with other datasets reveals that conversational data further amplifies reasoning capabilities in models even when the raw data originates from high-quality sources. We hope that MIND will pave a way to improve complex reasoning ability of smaller models with limited training data and accelerate further innovation towards building strong reasoning ability with structured high-quality data. To generate high-quality data at scale, current synthetic data generation approach explores rephrasing texts using LLMs in varied syntax while preserving the core content (Maini et al., 2024). However, their proposed approach limits up-sampling high-quality data in a way that does not go beyond grammatical styles or surface form transformations-leading little to no improvement when it comes to performance across complex and logical reasoning tasks. We hypothesize that simple rephrasing does not leverage the full potential of the synthetic data to improve the mathematical and complex multi-hop reasoning ability of LLM. Therefore, we propose, MIND, a conversational synthetic data generation approach that adds semantic variations and structured complexity to the raw text which is required to improve complex reasoning ability of the LLMs. In addition, multi-turn conversations can break down the original context step-by-step while each step addresses a sub-context at a time by often injecting complimentary reasoning or explanations. This resonates with how human solves a complex problem using consecutive chain-of-thought reasoning.",
            "score": 0.37877369099434166,
            "section_title": "INTRODUCTION",
            "char_start_offset": 5946,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1419
                },
                {
                    "start": 1420,
                    "end": 1592
                },
                {
                    "start": 1593,
                    "end": 1813
                },
                {
                    "start": 1814,
                    "end": 2010
                },
                {
                    "start": 2011,
                    "end": 2111
                }
            ],
            "ref_mentions": [
                {
                    "start": 1138,
                    "end": 1158,
                    "matchedPaperCorpusId": "267312030"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9326171875
        },
        {
            "corpus_id": "270766239",
            "title": "Compositional Structured Explanation Generation with Dynamic Modularized Reasoning",
            "text": "In this work, we propose a new task, compositional structured explanation generation (CSEG), to facilitate research on compositional generalization in reasoning. Despite the success of language models in solving reasoning tasks, their compositional generalization capabilities are under-researched. Our new CSEG task tests a model\u2019s ability to generalize from generating entailment trees with a limited number of inference steps \u2013 to more steps, focusing on the length and shapes of entailment trees. CSEG is challenging in requiring both reasoning and compositional generalization abilities, and by being framed as a generation task. Besides the CSEG task, we propose a new dynamic modularized reasoning model, MORSE, that factorizes the inference process into modules, where each module represents a functional unit. We adopt modularized self-attention to dynamically select and route inputs to dedicated heads, which specializes them to specific functions. Using CSEG, we compare MORSE to models from prior work. Our analyses show that the task is challenging, but that the dynamic reasoning modules of MORSE are effective, showing competitive compositional generalization abilities in a generation setting.",
            "score": 0.3784417858676211,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5615234375
        },
        {
            "corpus_id": "257766470",
            "title": "Natural Language Reasoning, A Survey",
            "text": "Some datasets explicitly target classical reasoning types in philosophy and logic, e.g. deduction, abduction and induction, following the definitions in the two areas. Thus, we call them \"classical logical reasoning tasks\". A key characteristic of this topic is that tasks are mostly artificial to study reasoning. There are both deductive reasoning and defeasible reasoning. \n\n5.1.1 Deductive reasoning. Classical deductive reasoning tasks are defined formally based on formal logic, such as propositional logic and first-order logic. There are mainly three types of task: inference [6,100,160], theorem proving [5,25,49,142] and reasoning path generation [100]. The inference task is to reason the conclusion given the premises in a single step, while theorem proving is to predict whether the given proposition is true or false with the given knowledge bases, which usually requires multiple steps. Obviously, inference is the fundamental task that forms the basic capability of multi-step reasoning tasks such as theorem proving, while reasoning path generation is an interpretable task that can be complementary to multi-step reasoning. However, except FOLIO [49], all the existing explicit deductive reasoning datasets are synthesized. We list the classical deductive reasoning datasets in Table 7. \n\nDataset Size Data Source Task Remark bAbI-15 [160] synthetic inference basic deduction RuleTaker \u2020 [25]/ProofWriter \u2020 [142] 500k synthetic theorem proving the first natural language theorem proving PARARULE-Plus [5] 400k synthetic theorem proving addresses the depth imbalance issue on ParaRules AAC [6] 710k synthetic inference based on 8 syllogistic argument schemes \n\nLogicInference [100] 200k synthetic inference reasoning path generation - \n\n1.4k expert-written theorem proving more diverse patterns Table 7. Datasets of classical deductive reasoning, where bAbI-15 means \"the 15-th task in bAbI tasks\". \u2020 denotes there are ground reasoning paths. \n\nProof-finding and faithful reasoning.",
            "score": 0.3784142765274334,
            "section_title": "Classical Logical Reasoning",
            "char_start_offset": 55696,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 88,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 375
                },
                {
                    "start": 378,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1241
                },
                {
                    "start": 1242,
                    "end": 1304
                },
                {
                    "start": 1307,
                    "end": 1675
                },
                {
                    "start": 1678,
                    "end": 1751
                },
                {
                    "start": 1754,
                    "end": 1915
                },
                {
                    "start": 1916,
                    "end": 1959
                },
                {
                    "start": 1962,
                    "end": 1999
                }
            ],
            "ref_mentions": [
                {
                    "start": 584,
                    "end": 587,
                    "matchedPaperCorpusId": "221702898"
                },
                {
                    "start": 591,
                    "end": 595,
                    "matchedPaperCorpusId": "3178759"
                },
                {
                    "start": 613,
                    "end": 616,
                    "matchedPaperCorpusId": "251135345"
                },
                {
                    "start": 616,
                    "end": 619,
                    "matchedPaperCorpusId": "211126663"
                },
                {
                    "start": 622,
                    "end": 626,
                    "matchedPaperCorpusId": "229371222"
                },
                {
                    "start": 1352,
                    "end": 1357,
                    "matchedPaperCorpusId": "3178759"
                },
                {
                    "start": 1406,
                    "end": 1410,
                    "matchedPaperCorpusId": "211126663"
                },
                {
                    "start": 1425,
                    "end": 1430,
                    "matchedPaperCorpusId": "229371222"
                },
                {
                    "start": 1519,
                    "end": 1522,
                    "matchedPaperCorpusId": "251135345"
                },
                {
                    "start": 1607,
                    "end": 1610,
                    "matchedPaperCorpusId": "221702898"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.13232421875
        },
        {
            "corpus_id": "249538396",
            "title": "Unveiling Transformers with LEGO: a synthetic reasoning task",
            "text": "We propose a synthetic reasoning task, LEGO (Learning Equality and Group Operations), that encapsulates the problem of following a chain of reasoning, and we study how the Transformer architectures learn this task. We pay special attention to data effects such as pretraining (on seemingly unrelated NLP tasks) and dataset composition (e.g., differing chain length at training and test time), as well as architectural variants such as weight-tied layers or adding convolutional components. We study how the trained models eventually succeed at the task, and in particular, we manage to understand some of the attention heads as well as how the information flows in the network. In particular, we have identified a novel \\emph{association} pattern that globally attends only to identical tokens. Based on these observations we propose a hypothesis that here pretraining helps for LEGO tasks due to certain structured attention patterns, and we experimentally verify this hypothesis. We also observe that in some data regime the trained transformer finds ``shortcut\"solutions to follow the chain of reasoning, which impedes the model's robustness, and moreover we propose ways to prevent it. Motivated by our findings on structured attention patterns, we propose the LEGO attention module, a drop-in replacement for vanilla attention heads. This architectural change significantly reduces Flops and maintains or even \\emph{improves} the model's performance at large-scale pretraining.",
            "score": 0.37791095716166595,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.27001953125
        },
        {
            "corpus_id": "257050308",
            "title": "Hyena Hierarchy: Towards Larger Convolutional Language Models",
            "text": "Our synthetic reasoning are inspired by mechanistic interpretability (Elhage et al., 2021), in-context learning (ICL) (Garg et al., 2022) and language model benchmarking (Liang et al., 2022) research. The evaluation revolves around 4 main tasks: \n\n\u2022 Associative recall: Each string is produced by concatenating key-value tuples from a different random dictionary. This test verifies whether a model is able to extract right value given a key as prompt, effectively applying a data-controlled shift (delay). \n\n\u2022 Majority voting and counting: Testing if a model can densely activate its data-controlled matrix i.e., through many non-zero entries (consider the string 'a a a a a a a a a a b \u2192 a'). \n\n\u2022 ICL of linear functions: Verifying whether a model can perform ICL on real-valued inputs. Prompts are generated as x 1 , w k x 1 , . . . , x n \u2192 w k x n , where both x k and w k \u2208 R no are sampled from a normal distribution. \n\n\u2022 Arithmetic: Basic capability check. \n\nFor each task, we train models using the hyperparameters shown in Table A.1. We consider increasing settings of difficulty controlled by sequence length, spanning values 1024, 2048, 4098, 8196, 16392, 32784, 65568, 131136 and vocabulary sizes 10, 20, 30, 40. For ICL of functions, we vary instead the dimension n o . \n\nNote that for associative recall on longer sequences, multiple copies of key-value tuples appear in the prompt. To see this, consider how likely it is to sample multiple copies of a particular key-value pair with a vocabulary size of 40, in order to form a sequence of 100k characters. Models capable of looking further back in the sequence effectively see more data, and can solve challenging versions of the in-context learning task. Increasing the vocabulary size has the increasing the average distance between instances of the same key-value pair in each prompt, highlighting performance gaps between different approaches.",
            "score": 0.37647210079429383,
            "section_title": "A.1 Mechanistic Design Synthetic Benchmarks",
            "char_start_offset": 31690,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 245
                },
                {
                    "start": 248,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 506
                },
                {
                    "start": 509,
                    "end": 694
                },
                {
                    "start": 697,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 923
                },
                {
                    "start": 926,
                    "end": 963
                },
                {
                    "start": 966,
                    "end": 1042
                },
                {
                    "start": 1043,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1282
                },
                {
                    "start": 1285,
                    "end": 1396
                },
                {
                    "start": 1397,
                    "end": 1570
                },
                {
                    "start": 1571,
                    "end": 1720
                },
                {
                    "start": 1721,
                    "end": 1912
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.302978515625
        },
        {
            "corpus_id": "273403896",
            "title": "A Survey on Data Synthesis and Augmentation for Large Language Models",
            "text": "They identify that more automatically generated data may achieve better performance, and mark it as future work. LawGPT [272] refine open-source datasets by prompting chatGPT for instructiontuning data to generate more formal, polite, and clear answers. WisdomInterrogatory [270] prompt GPT-3.5 models as agents to imitate conversations between a law agent and a user to generate multi-turn instruction text. \n\n3.6.6 Others. In addition to these previous applications, the potential of synthetic datasets is also explored in financial [12] and education [48,102]. These areas are more challenging for data synthesis due to the higher knowledge density and demands on quality. As research continues, these areas may become increasingly promising.",
            "score": 0.375857175152596,
            "section_title": "Applications",
            "char_start_offset": 78511,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 408
                },
                {
                    "start": 411,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 745
                }
            ],
            "ref_mentions": [
                {
                    "start": 554,
                    "end": 558,
                    "matchedPaperCorpusId": "265709732"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76611328125
        },
        {
            "corpus_id": "268032153",
            "title": "NeSy is alive and well: A LLM-driven symbolic approach for better code comment data generation and classification",
            "text": "This section discusses existing techniques that couple symbolic forms of learning and neural models with a particular focus on LLMs as well as some proposed strategies in the literature for synthetic data generation.",
            "score": 0.375857175152596,
            "section_title": "Related Work",
            "char_start_offset": 3323,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53515625
        },
        {
            "corpus_id": "268041519",
            "title": "Large Language Models(LLMs) on Tabular Data: Prediction, Generation, and Understanding - A Survey",
            "text": "The ability to synthesize real and highquality data is essential for model development. Data generation is used for augmentation when the data is sparse (Onishi & Meguro, 2023), imputing missing values (Jolicoeur-Martineau et al., 2023), and class rebalancing in imbalanced data (Sauber-Cole & Khoshgoftaar, 2022). Traditional methods for synthetic data generation are mostly based on Copulas (Patki et al., 2016;Li et al., 2020b) and Bayesian networks (Zhang et al., 2017;Madl et al., 2023) while recent advancement in generative models such as Variational Autoencoders (VAEs) (Ma et al., 2020;Darabi & Elor, 2021;Vardhan & Kok, 2020;Liu et al., 2023d;Xu et al., 2023b)), generative adversarial networks (GANs) (Park et al., 2018;Choi et al., 2018;Baowaly et al., 2019;Xu et al., 2019), diffusion models (Kotelnikov et al., 2022;Xu et al., 2023a;Kim et al., 2022b;a;Lee et al., 2023;Zhang et al., 2023c), and LLMs, opened up many new opportunities. These deep learning approaches have demonstrated superior performance over classical methods such as Bayesian networks ( (Xu et al., 2019)). A comprehensive understanding of the strengths and weaknesses of different tabular data synthesis methods can be found in Du & Li (2024). \n\nTable understanding is a broad field, covering various tasks like question answering (QA), natural language inference (NLI), Text2SQL tasks, and more.",
            "score": 0.375857175152596,
            "section_title": "Traditional and deep learning in tabular data",
            "char_start_offset": 9697,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 88,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 949
                },
                {
                    "start": 950,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1228
                },
                {
                    "start": 1231,
                    "end": 1381
                }
            ],
            "ref_mentions": [
                {
                    "start": 279,
                    "end": 313,
                    "matchedPaperCorpusId": "251725737"
                },
                {
                    "start": 393,
                    "end": 413,
                    "matchedPaperCorpusId": "9027681"
                },
                {
                    "start": 453,
                    "end": 473,
                    "matchedPaperCorpusId": "13095103"
                },
                {
                    "start": 615,
                    "end": 635,
                    "matchedPaperCorpusId": "229131432"
                },
                {
                    "start": 635,
                    "end": 653,
                    "matchedPaperCorpusId": "259298730"
                },
                {
                    "start": 712,
                    "end": 731,
                    "matchedPaperCorpusId": "47017667"
                },
                {
                    "start": 749,
                    "end": 770,
                    "matchedPaperCorpusId": "54479855"
                },
                {
                    "start": 830,
                    "end": 847,
                    "matchedPaperCorpusId": "264439324"
                },
                {
                    "start": 847,
                    "end": 865,
                    "matchedPaperCorpusId": "249847841"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93017578125
        },
        {
            "corpus_id": "273811804",
            "title": "Exploring the Landscape for Generative Sequence Models for Specialized Data Synthesis",
            "text": "Synthetic data generation has emerged as a vital solution in artificial intelligence (AI) and machine learning, offering unique advantages for both research and practical applications. In response to growing privacy concerns and limited access to real-world data, synthetic data has evolved as a powerful alternative, enabling model training, testing, and deployment without compromising sensitive information. This survey examines the diverse applications of synthetic data generation, from vision and voice technologies to business intelligence, and highlights its potential to transform data-driven fields. By synthesizing insights from recent studies, this survey aims to provide a comprehensive overview of how synthetic data is revolutionizing AI across various domains while addressing privacy and ethical considerations.",
            "score": 0.375857175152596,
            "section_title": "VI Synthetic Data Generation: A Survey",
            "char_start_offset": 19503,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 828
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81103515625
        },
        {
            "corpus_id": "272421662",
            "title": "A Systematic Review of Synthetic Data Generation Techniques Using Generative AI",
            "text": "GANs constantly push the limits of what is possible in machine-generated content. However, a team of researchers at OpenAI introduced a framework in 2018 [8] to improve language understanding through generative pre-training. This framework consists of a sophisticated network of neurons, known as large language models. LLMs can generate synthetic data by using their ability to interpret and synthesize human-like text from massive volumes of training data. By entering precise prompts or instructions, these models can generate a wide range of realistic text data, such as dialogues, stories, and domain-specific information. These synthetic data can be used for various purposes, including training machine-learning models, enhancing datasets, and replicating scenarios for testing and development. \n\nThis study aims to conduct a systematic literature review of various synthetic data generation techniques that leverage generative AI. Through a comprehensive analysis of the available and relevant frameworks, this study identifies the limitations of the current generative AI methods for synthetic data generation. Section 2 examines related work available in the form of reviews and surveys for synthetic data generation and justifies the need for this systematic literature review. Section 3 details the methodology employed in conducting this systematic literature review, including the inclusion and exclusion criteria, and the selection process for the studies reviewed. Section 4 provides an overview of the technologies used in synthetic data generation, encompassing both machine learning and programmatic approaches. Section 5 discusses the findings of this literature review and answers the research questions. Section 6 presents the most critical challenges that occur during synthetic data generation using generative AI and proposes future research directions. Finally, Section 7 presents conclusions drawn from the literature review.",
            "score": 0.375857175152596,
            "section_title": "Introduction",
            "char_start_offset": 7710,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 82,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 458
                },
                {
                    "start": 459,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 801
                },
                {
                    "start": 804,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1480
                },
                {
                    "start": 1481,
                    "end": 1630
                },
                {
                    "start": 1631,
                    "end": 1725
                },
                {
                    "start": 1726,
                    "end": 1878
                },
                {
                    "start": 1879,
                    "end": 1952
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88134765625
        },
        {
            "corpus_id": "269870446",
            "title": "Exploring Innovative Approaches to Synthetic Tabular Data Generation",
            "text": "As we navigate the varied terrain of synthetic data generation, we aim to distill knowledge, draw connections, and provide a comprehensive overview of the evolving methodologies that address these sophisticated challenges.Through a rigorous inspection of the reviewed papers, we seek to unravel the potential confluences between the need for extensive, balanced datasets, the intricacies of handling imbalances, and privacy requirements-paving the way for a more informed and ethical approach to data-intensive applications in the field of machine learning and neural networks.",
            "score": 0.375857175152596,
            "section_title": "Introduction",
            "char_start_offset": 7169,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 222
                },
                {
                    "start": 222,
                    "end": 577
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65625
        },
        {
            "corpus_id": "277104955",
            "title": "Synthetic Data Generation Using Large Language Models: Advances in Text and Code",
            "text": "Second, while synthetic signal data (e.g., audio) is a developing field, it remains outside the scope of this study, though future work could extend these analyses [24]. Lastly, text and code share a common modality, as both are purely symbolic representations that can be generated and processed using the same fundamental methods-unlike images, sounds, or videos, which require distinct encoding formats and specialized equipment [29]. \n\nThis paper provides a comprehensive analysis of LLM-based synthetic data generation, with contributions summarized as follows: \n\n\u2022 Survey of Techniques: We review major approaches for LLM-driven data generation in text and code, including prompt-based augmentation, retrieval-augmented generation, self-instruct methods, and reinforcement learning with feedback. We categorize methods by their strategies (e.g., zero-shot vs. few-shot prompting, knowledge integration, iterative refinement) and discuss representative examples from recent literature [3] \u2022 Advances in Text Data Generation: We explore how LLMs are used to create synthetic text datasets for tasks such as classification, question answering, and instruction-following. Key studies are examined to understand the impact of synthetic data on model performance, data diversity, and efficiency. We highlight empirical findings (e.g., improvements of 3-26% with synthetic augmentation in low-data regimes) and analyze how prompt design and data curation affect quality [33]. \n\n\u2022 Advances in Code Data Generation: We survey the parallel emergence of LLM-generated code data for training code intelligence models. We discuss techniques for generating code through LLM prompts, executing code to validate correctness, and synthesizing coding instruction data at scale [37]. Examples include synthetic programming problems and solutions, code edits for diversity, and instruction-tuning datasets (e.g., Code Alpaca, WizardCoder) generated by LLMs. \n\n\u2022 Challenges and Mitigations: We outline the main challenges in using LLMs for synthetic data, such as ensuring data fidelity and factuality, avoiding modeling biases, maintaining diversity without drifting from real data distribution, and preventing model collapse from iterative self-training on AI-generated data.",
            "score": 0.37474108971800546,
            "section_title": "Introduction",
            "char_start_offset": 2189,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 437
                },
                {
                    "start": 440,
                    "end": 566
                },
                {
                    "start": 569,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 1173
                },
                {
                    "start": 1174,
                    "end": 1295
                },
                {
                    "start": 1296,
                    "end": 1474
                },
                {
                    "start": 1477,
                    "end": 1611
                },
                {
                    "start": 1612,
                    "end": 1770
                },
                {
                    "start": 1771,
                    "end": 1943
                },
                {
                    "start": 1946,
                    "end": 2262
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94580078125
        },
        {
            "corpus_id": "273346538",
            "title": "SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data",
            "text": "We conduct extensive experiments to evaluate both the generation quality of our synthetic data (Sec. 5.1) and its effectiveness in FER tasks (Sec. 5.2). For more details on experimental setup, implementation details are provided in the appendix.",
            "score": 0.37463120143033174,
            "section_title": "EXPERIMENTS",
            "char_start_offset": 18616,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 245
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1417236328125
        },
        {
            "corpus_id": "259503802",
            "title": "Tabular and latent space synthetic data generation: a literature review",
            "text": "The generation of tabular and latent space synthetic data has applications in multiple ML tasks and domains. Specifically, we found six areas that were shown to benefit from synthetic data: data privacy, regularization, oversampling, active learning, semi-supervised learning, and self-supervised learning. Synthetic data may be used either as an accessory task to improve an ML model's performance over a primary task (e.g., regularization and oversampling), an intermediate task (e.g., feature extraction), or as a final product itself (e.g., data anonymization). The analysis of data generation algorithms for each relevant learning problem led to the proposal of a general-purpose taxonomy primarily focused on the underlying mechanisms used for data generation. We characterized every algorithm discussed in this work into four categories: (1) architecture, (2) application level, (3) data space, and (4) scope. The successful implementation of synthetic data generation generally requires a few considerations: \n\n1 Ensuring the dataset's features are comprised within similar, fixed boundaries. For example, any method using a neighbors-based approach will rely on distance measurements (typically the Euclidean distance), which is sensitive to the scale of the data and a nearest-neighbors estimation may vary depending on whether the data was scaled a priori. This can be achieved with data scaling. 2 Various generation mechanisms require a manifold. There are two approaches to address non-manifold input data: (1) Adopt methods sensitive to the presence of non-metric features, or (2) project the input data into a manifold (i.e., a latent space). 3 The smoothness assumption is prevalent in linear and perturbation-based data generation mechanisms. If a classification problem has low class separation and it is difficult to solve, the choice in the design of the generator algorithm is also difficult. Generally, generation algorithms with a global scope might adapt better to classification problems with low separability. On the other hand, problems with higher separability might require a definition of more uniform decision boundaries to prevent overfitting, which can be achieved with generation algorithms with a local scope. 4 Considering the trade-off between performance and computational power. It is generally understood that computationally-intensive approaches tend to produce synthetic data with higher quality.",
            "score": 0.37454221776919994,
            "section_title": "Discussion",
            "char_start_offset": 71354,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 109,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1016
                },
                {
                    "start": 1019,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1367
                },
                {
                    "start": 1368,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1658
                },
                {
                    "start": 1659,
                    "end": 1760
                },
                {
                    "start": 1761,
                    "end": 1914
                },
                {
                    "start": 1915,
                    "end": 2036
                },
                {
                    "start": 2037,
                    "end": 2245
                },
                {
                    "start": 2246,
                    "end": 2318
                },
                {
                    "start": 2319,
                    "end": 2439
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.791015625
        },
        {
            "corpus_id": "271546289",
            "title": "Apple Intelligence Foundation Language Models",
            "text": "In the field of mathematics, the wide-ranging subjects and difficulty level make it exceptionally resource-intensive for collecting human demonstrations, since it requires expert knowledge from the human writers. It also becomes impractical to solely rely on human-written content as the model continuously improves. As a consequence, exploring the potential of synthetic data becomes essential to effectively address the challenges. \n\nThe creation of synthetic data for mathematics involves two primary stages: generating synthetic math problems and producing their corresponding solutions. For math problem synthesis, we employ several \"evolution\" strategies where a seed set of prompts are transformed into a much larger set of diverse prompts: \n\nProblem rephrase and reversion. Following the approach in [Yu et al., 2023], we prompt AFM to rephrase seed math questions, and curate reverse questions to derive a specific number in a raw problem statement when provided with the final answer. \n\nProblem evolution. Inspired by the instruction evolving technique [Xu et al., 2023], given a seed problem set D seed we prompt AFM to generate two distinct sets of math problems, i.e. F (D seed ) depth \u2212\u2212\u2212\u2192 D depth , and \n\nThe in-depth evolution enhances instructions by adding complexities while the in-breadth evolution improves the topic coverage. For both D breadth and D depth , we first perform de-duplication with an embedding model, and subsequently prompt LLMs to ensure the coherence and solvability of the math problems. In addition, for D depth a difficulty level is assigned and we only select math problems that score above a specified threshold. \n\nWith an augmented set of math questions, we then prompt AFM to synthesize N responses with chain-of-thought per question. If the initial seed data has ground truth, they can be used as an \"outcome reward signal\" to filter synthesized answers. For problems that require less reasoning steps, we observe that a correct final answer often gets associated with correct intermediate steps. If direct answer checking is unsuccessful or ground truth is unavailable, we instead assess the response correctness by querying an LLM judge. We find that the filtered answers, when fed into the training data, boost our models' math capabilities by a large margin.",
            "score": 0.3733036854916246,
            "section_title": "Mathematics",
            "char_start_offset": 20459,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 433
                },
                {
                    "start": 436,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 747
                },
                {
                    "start": 750,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 994
                },
                {
                    "start": 997,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1217
                },
                {
                    "start": 1220,
                    "end": 1347
                },
                {
                    "start": 1348,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1657
                },
                {
                    "start": 1660,
                    "end": 1781
                },
                {
                    "start": 1782,
                    "end": 1902
                },
                {
                    "start": 1903,
                    "end": 2044
                },
                {
                    "start": 2045,
                    "end": 2187
                },
                {
                    "start": 2188,
                    "end": 2310
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73828125
        },
        {
            "corpus_id": "267499786",
            "title": "Neural networks for abstraction and reasoning",
            "text": "In this work, we look at several novel approaches for solving the Abstraction & Reasoning Corpus (ARC), a dataset of abstract visual reasoning tasks introduced to test algorithms on broad generalization. Despite three international competitions with $100,000 in prizes, the best algorithms still fail to solve a majority of ARC tasks and rely on complex hand-crafted rules, without using machine learning at all. We revisit whether recent advances in neural networks allow progress on this task, or whether an entirely different class of models are required. \n\nFirst, we adapt the DreamCoder neurosymbolic reasoning solver to ARC. DreamCoder automatically writes programs in a bespoke domain-specific language to perform reasoning, using a neural network to mimic human intuition. We present the Perceptual Abstraction and Reasoning Language (PeARL) language, which allows DreamCoder to solve ARC tasks, and propose a new recognition model that allows us to significantly improve on the previous best implementation. \n\nWe also propose a new encoding and augmentation scheme that allows large language models (LLMs) to solve ARC tasks, and find that the largest models can solve some ARC tasks. LLMs are able to solve a different group of problems to state-of-the-art solvers, and provide an interesting way to complement other approaches. \n\nWe perform an ensemble analysis, combining models to achieve better results than any system alone. \n\nFinally, we publish the arckit Python library to make future research on ARC easier.",
            "score": 0.37291576005484944,
            "section_title": "body",
            "char_start_offset": 1,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 558
                },
                {
                    "start": 561,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 1016
                },
                {
                    "start": 1019,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1338
                },
                {
                    "start": 1341,
                    "end": 1439
                },
                {
                    "start": 1442,
                    "end": 1526
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.322021484375
        },
        {
            "corpus_id": "273025893",
            "title": "Towards a Theoretical Understanding of Synthetic Data in LLM Post-Training: A Reverse-Bottleneck Perspective",
            "text": "In preceding sections, we established a comprehensive modeling for synthetic data generation and elucidated the connection between this process and the generalization error as delineated in Lemma 3.1. This section delves deeper into the implications of the synthetic data generation process on the generalization capabilities.",
            "score": 0.3728047291462725,
            "section_title": "MAIN RESULT",
            "char_start_offset": 13997,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 326
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09075927734375
        },
        {
            "corpus_id": "272421662",
            "title": "A Systematic Review of Synthetic Data Generation Techniques Using Generative AI",
            "text": "When comparing these techniques, it is evident that each has strengths and limitations tailored to specific types of data. \n\nImage data. For purposes of artificial image generation, cGANs and VAEs are the most efficient. In medical imaging and other contexts where realistic image synthesis is needed, cGANs offer high-quality, contextually appropriate images [40]. On the other hand, VAEs can produce somewhat blurrier pictures but possess robust performance in generating diverse and coherent image samples [6]. \n\nTabular data. TGANs and VAEs are ideal for synthesizing synthetic tabular data. TGANs are effective at capturing intricate relationships between features, which are important for healthcare and financial data synthesis [78]. This area also has strong performance in VAEs, especially when combined with techniques that address discrete data [55]. \n\nTextual data. GPT-3 and BERT belong to the family of LLM models that are best suited for generating synthetic textual data. The ability of GPT-3 to generate coherent text that makes sense in its surroundings makes it highly applicable to natural language generation tasks [64]. BERT, on the other hand, has a deep contextual understanding; thus, it performs better than any other model in terms of text comprehension and generation tasks that require nuanced understanding, such as question answering or sentiment analysis [66]. \n\nSpeech data. GAN-based models are employed for tasks such as voice conversion, which transforms one speaker's voice into another while retaining the linguistic content, making them useful in personalized voice systems. Google's AudioLM model enhances speech generation by modeling long-term dependencies in audio sequences, thereby enabling coherent and context-aware speech synthesis for conversational AI systems [84].",
            "score": 0.3724832346855099,
            "section_title": "RQ2. What frameworks are available for synthetic data generation for specific use cases?",
            "char_start_offset": 109655,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 125,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 513
                },
                {
                    "start": 516,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 861
                },
                {
                    "start": 864,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1392
                },
                {
                    "start": 1395,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1815
                }
            ],
            "ref_mentions": [
                {
                    "start": 735,
                    "end": 739,
                    "matchedPaperCorpusId": "7739761"
                },
                {
                    "start": 1387,
                    "end": 1391,
                    "matchedPaperCorpusId": "226096901"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.892578125
        },
        {
            "corpus_id": "273403896",
            "title": "A Survey on Data Synthesis and Augmentation for Large Language Models",
            "text": "In the data preparation phase, data synthesis and augmentation aim to generate diverse and high-quality datasets for the training of LLMs, addressing the challenge of the scarcity of real-world data. According to the taxonomy discussed in Section 2, We divide the present subsection into general model distillation and data augmentation. 3.1.1 General Model Distillation. This way aims to leverage the powerful capabilities of general LLMs to distill high-quality data. According to the approach and data modality, we further divided general model distillation into five categories: synthesize from seeds, synthesize reasoning steps, synthesize with controllability, synthesize from scratch, and synthesize multimodal data. Synthesize from Seeds. To synthesize datasets for specific tasks, prompting LLMs with a small number of relevant examples can effectively produce high-quality datasets at a low cost. For instance, to investigate \"how small can an LLM be to achieve certain capabilities\", TinyStories [53] is constructed by instructing an LLM to generate stories that combine three words randomly chosen from 1500 basic words, and it can be used to train and evaluate language models. Based on the collected large-scale functions, Case2Code [180] incorporates LLMs to generate suitable inputs for these functions and utilizes the code interpreter to calculate their corresponding outputs. Due to the potential insufficiency in quantity and diversity of single-round synthetic data, methods for iterative data synthesis are investigated. For example, Self-Instruct [210] can be repeated for many iterations to accumulate a substantial volume of tasks. In each iteration, an LLM is prompted to generate new instructions from a small seed set, then creates input-output instances for each instruction independently. Similarly, Evol-Instruct [223] can be conducted over multiple rounds to gather a sufficient dataset encompassing various complexities. In each evolution, in-depth and in-breadth evolving are employed to either enhance the basic instructions to more sophisticated ones or innovate entirely new directives. \n\nSynthesize Reasoning Steps. To enhance the reasoning capability of LLMs, additional reasoning steps are generated in the process of data synthesis.",
            "score": 0.37243196880413965,
            "section_title": "Data Preparation",
            "char_start_offset": 13906,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 746
                },
                {
                    "start": 747,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1190
                },
                {
                    "start": 1191,
                    "end": 1394
                },
                {
                    "start": 1395,
                    "end": 1542
                },
                {
                    "start": 1543,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1818
                },
                {
                    "start": 1819,
                    "end": 1953
                },
                {
                    "start": 1954,
                    "end": 2123
                },
                {
                    "start": 2126,
                    "end": 2153
                },
                {
                    "start": 2154,
                    "end": 2273
                }
            ],
            "ref_mentions": [
                {
                    "start": 1570,
                    "end": 1575,
                    "matchedPaperCorpusId": "254877310"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88818359375
        },
        {
            "corpus_id": "272310191",
            "title": "Generative AI in Ship Design",
            "text": "Artificial Intelligence (AI) refers to the simulation of human intelligence processes by machines, especially computer systems. It encompasses various approaches and techniques aimed at enabling machines to perform tasks that typically require human intelligence, such as reasoning, problemsolving, learning, perception, understanding natural language, and more [14]. \n\nGenerative AI is a specific subfield of artificial intelligence focused on creating or generating new content, often in the form of images, text, music, or other types of media. Instead of simply analyzing and processing existing data, generative AI systems are designed to generate novel outputs based on patterns learned from large datasets. \n\nOne of the key techniques used in generative AI is Generative Adversarial Networks (GANs), where two neural networks, known as the generator and the discriminator, are trained simultaneously. The generator generates synthetic data while the discriminator evaluates the authenticity of that data. Through this adversarial process, the generator learns to create increasingly realistic outputs. \n\nGenerative Generative models can assist in generating new molecules with desired properties for drug discovery and material science.",
            "score": 0.3723632359645008,
            "section_title": "Overview",
            "char_start_offset": 2827,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 367
                },
                {
                    "start": 370,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 713
                },
                {
                    "start": 716,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 1011
                },
                {
                    "start": 1012,
                    "end": 1108
                },
                {
                    "start": 1111,
                    "end": 1243
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.44873046875
        },
        {
            "corpus_id": "248472095",
            "title": "Community Partitioning over Feature-Rich Networks Using an Extended K-Means Method",
            "text": "Our synthetic data generator coincides with that described in our paper [11]. Thus, we describe further on rules for generating (a) network, (b) categorical features, and (c) quantitative features by following the description in [11].",
            "score": 0.3721991141151649,
            "section_title": "Generating Synthetic Data Sets",
            "char_start_offset": 27770,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 77
                },
                {
                    "start": 78,
                    "end": 234
                }
            ],
            "ref_mentions": [
                {
                    "start": 72,
                    "end": 76,
                    "matchedPaperCorpusId": "236483497"
                },
                {
                    "start": 229,
                    "end": 233,
                    "matchedPaperCorpusId": "236483497"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.254150390625
        },
        {
            "corpus_id": "273403996",
            "title": "A Little Human Data Goes A Long Way",
            "text": "From BERT (Devlin et al., 2019) to GPT-4 (Achiam et al., 2023), the explosive growth of language models (LMs) has been underpinned by exponential increases in the size of available training data. However, the more complex and specialized the task, the more expensive and challenging it is to collect human generated data at scale (Wang et al., 2021). Combined with growing concerns that LMs may soon exhaust the stock of publicly available training data (Villalobos et al., 2024), many turn to synthetic data generation, hoping to eliminate their reliance on human annotation. \n\nSynthetic data generation has long been used to increase the amount of training data available (Simard et al., 2002;Krizhevsky et al., Figure 1: Change in model performance as the proportion of synthetic points in the training data is increased. Across datasets, the performance decrease when moving from 0% to 90% synthetic data is often less than that of moving from 90% to purely synthetic data. 2012). Early NLP approaches use rule based methods (De Gispert et al., 2005;Chen et al., 2012), paraphrasing (Wang and Yang, 2015;Kobayashi, 2018), noising (Xie et al., 2017;Wang et al., 2018), and backtranslation (Sennrich et al., 2016;Yu et al., 2018), but are limited in their capability. \n\nModern LMs demonstrate the capability to solve myriad NLP tasks with minimal task specific data (Brown, 2020;Wei et al., 2022a,b;Ouyang et al., 2022;Ashok and Lipton, 2023), making them more powerful synthetic data generators.",
            "score": 0.3721991141151649,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 576
                },
                {
                    "start": 579,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 977
                },
                {
                    "start": 978,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1269
                },
                {
                    "start": 1272,
                    "end": 1498
                }
            ],
            "ref_mentions": [
                {
                    "start": 10,
                    "end": 31,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 330,
                    "end": 349,
                    "matchedPaperCorpusId": "237363383"
                },
                {
                    "start": 454,
                    "end": 479,
                    "matchedPaperCorpusId": "272330241"
                },
                {
                    "start": 674,
                    "end": 695,
                    "matchedPaperCorpusId": "16067356"
                },
                {
                    "start": 695,
                    "end": 713,
                    "matchedPaperCorpusId": "195908774"
                },
                {
                    "start": 1029,
                    "end": 1054,
                    "matchedPaperCorpusId": "2571493"
                },
                {
                    "start": 1054,
                    "end": 1072,
                    "matchedPaperCorpusId": "1326531"
                },
                {
                    "start": 1087,
                    "end": 1108,
                    "matchedPaperCorpusId": "3257353"
                },
                {
                    "start": 1108,
                    "end": 1124,
                    "matchedPaperCorpusId": "21725995"
                },
                {
                    "start": 1134,
                    "end": 1152,
                    "matchedPaperCorpusId": "10635893"
                },
                {
                    "start": 1152,
                    "end": 1170,
                    "matchedPaperCorpusId": "52078335"
                },
                {
                    "start": 1192,
                    "end": 1215,
                    "matchedPaperCorpusId": "15600925"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79931640625
        },
        {
            "corpus_id": "269772787",
            "title": "ALPINE: Unveiling the Planning Capability of Autoregressive Learning in Language Models",
            "text": "LLMs have demonstrated capabilities that exceed the theoretically predicted lower bounds of expressiveness.To demystify this paradox, numerous studies have employed experimental methodologies akin to those used in the physical and biological sciences.Their aim is to decode the mechanisms of LLMs.The foundational strategy is to generate controlled synthetic datasets to analyze how language models (not necessarily the LLMs) complete various tasks.Standard methods for this analysis include visualizing attention patterns to examine computational properties (such as locality and time invariance) and employing linear probing on the hidden states to determine the extent of learning.Given that the data is synthetic and the ground-truth mappings are generally known, it becomes feasible to isolate the influence of various factors (e.g., prompting strategies, chain-of-thought reasoning, and data formatting).For example, a dataset designed for learning group operations, as detailed in [ZBB + 22], facilitates the exploration of how pretraining, data composition, and neural architecture influence reasoning tasks within LLMs.Similarly, the generation of synthetic context-free grammar (CFG) data, as described in [AZL23a], enables training GPT-2 models, uncovering their capacity to learn dynamic programming algorithms for parsing CFGs.Synthetic datasets focusing on biographical knowledge, presented in [AZL23b,AZL23c,AZL24], probe into the mechanisms of knowledge storage, retrieval, manipulation, and the implications of scaling laws.Moreover, the work in [LSL + 23] introduces synthetic datasets aimed at understanding how smaller LLMs tackle basic arithmetic operations, like addition, and examines the effects of few-shot prompting, pretraining, and model scaling [LSL + 23].This paper builds upon these investigations by conducting controlled experiments with a path planning dataset, thereby shedding light on the complexities and challenges of planning in LLMs.",
            "score": 0.3716312806333051,
            "section_title": "Mechansims of LLMs",
            "char_start_offset": 57275,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 107,
                    "end": 251
                },
                {
                    "start": 251,
                    "end": 297
                },
                {
                    "start": 297,
                    "end": 449
                },
                {
                    "start": 449,
                    "end": 684
                },
                {
                    "start": 684,
                    "end": 910
                },
                {
                    "start": 910,
                    "end": 1128
                },
                {
                    "start": 1128,
                    "end": 1340
                },
                {
                    "start": 1340,
                    "end": 1541
                },
                {
                    "start": 1541,
                    "end": 1785
                },
                {
                    "start": 1785,
                    "end": 1974
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65283203125
        },
        {
            "corpus_id": "259858981",
            "title": "SORTIE: Dependency-Aware Symbolic Reasoning for Logical Data-to-text Generation",
            "text": "Early data-to-text generation mainly focuses on surface-level descriptions of the table contents (Lebret et al., 2016;Liu et al., 2018;Ma et al., 2019;Wang et al., 2020). However, in spite of generation fluency, neural-based generation models struggle to perform rich inference based on the facts in table (Chen et al., 2020a,c). To make up for that, logical table-to-text generation is proposed as a new task with the aim of generating logically-consistent descriptions from open-domain tables (Chen et al., 2020a,c). \n\nIn recent years, to endow neural models with complex reasoning ability, DCVED (Chen et al., 2021) applies causal intervention methods to reduce the spurious correlation in entities. PLOG (Liu et al., 2022) and TABT5 (Andrejczuk et al., 2022) introduce table-to-logical-form or table denoising as self-supervision tasks in the pretraining stage. Similarly, REASTAP (Zhao et al., 2022) introduces 7 pre-training tasks to mimic the 7 types of reasoning skills of humans. It is worth noting that this line of research is orthogonal to ours since they primarily concentrate on developing training instances that reflect the desired reasoning skills. Similar to the programming language in our proposal, Saha et al. (2022) introduce logic string as an intermediate step to guide generation. However, the surface realization from the logic string to the final description is very prone to hallucinations as it is done purely by neural language models.",
            "score": 0.3716015854861993,
            "section_title": "Data-to-text Generation",
            "char_start_offset": 3520,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 518
                },
                {
                    "start": 521,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1305
                },
                {
                    "start": 1306,
                    "end": 1465
                }
            ],
            "ref_mentions": [
                {
                    "start": 97,
                    "end": 118,
                    "matchedPaperCorpusId": "1238927"
                },
                {
                    "start": 118,
                    "end": 135,
                    "matchedPaperCorpusId": "7672408"
                },
                {
                    "start": 135,
                    "end": 151,
                    "matchedPaperCorpusId": "196189186"
                },
                {
                    "start": 151,
                    "end": 169,
                    "matchedPaperCorpusId": "218487237"
                },
                {
                    "start": 599,
                    "end": 618,
                    "matchedPaperCorpusId": "236459782"
                },
                {
                    "start": 708,
                    "end": 726,
                    "matchedPaperCorpusId": "249062588"
                },
                {
                    "start": 885,
                    "end": 904,
                    "matchedPaperCorpusId": "253097905"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5283203125
        },
        {
            "corpus_id": "277113664",
            "title": "Reasoning Effort and Problem Complexity: A Scaling Analysis in LLMs",
            "text": "Large language models (LLMs) have demonstrated remarkable abilities in a wide range of natural language tasks, from text generation to complex problem-solving. Recent advances, particularly with models trained for enhanced reasoning, have pushed the boundaries of what machines can achieve in tasks requiring logical inference and deduction. Figure 1: An example instance of a partially solved 6 by 6 tents puzzle. Tents need to be placed next to trees, away from other tents and fulfilling the row and column constraints. \n\nA critical factor in the success of these advanced models is the ability to leverage increased computational resources at test time, allowing them to explore more intricate solution spaces. This capability raises a fundamental question: how does the \"reasoning effort\" of these models scale as the complexity of the problem increases? \n\nUnderstanding this scaling relationship is crucial for several reasons. First, it sheds light on the fundamental nature of reasoning within LLMs, moving beyond simply measuring accuracy on isolated tasks. By examining how the computational demands, reflected in token usage, evolve with problem difficulty, we can gain insights into the efficiency and potential bottlenecks of current LLM architectures. Second, characterizing this scaling behavior is essential for designing more effective and resource-efficient reasoning models in the future. \n\nIn this work, we address this question by investigating the scaling of reasoning effort in LLMs using a specific, infinitely scalable logic puzzle: the Tents puzzle1 (see Figure 1). This puzzle offers a controlled environment for studying algorithmic reasoning, as its problem size can be systematically increased, and it possesses a known linear-time solution. Our analysis focuses on how the number of tokens used by state-of-the-art reasoning LLMs changes as the puzzle grid size grows. In addition to reasoning effort, we also evaluate the success rate across different puzzle sizes to provide a comprehensive view of their performance.",
            "score": 0.37144471810895374,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 522
                },
                {
                    "start": 525,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 859
                },
                {
                    "start": 862,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 1066
                },
                {
                    "start": 1067,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1407
                },
                {
                    "start": 1410,
                    "end": 1591
                },
                {
                    "start": 1592,
                    "end": 1771
                },
                {
                    "start": 1772,
                    "end": 1899
                },
                {
                    "start": 1900,
                    "end": 2050
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.46630859375
        },
        {
            "corpus_id": "257496749",
            "title": "Breaking Common Sense: WHOOPS! A Vision-and-Language Benchmark of Synthetic and Compositional Images",
            "text": "The field of commonsense reasoning has recently gained significant attention, with various tasks proposed both in natural language processing (NLP) [33,45,46,34,4,16] and computer vision [40,8]. In the field of vision-andlanguage, models are being developed to solve complex visual reasoning tasks. These include visual understanding tasks, such as VCR [44], as well as tasks that evaluate commonsense reasoning in association and analogy tasks, like WinoGAViL [5] and VASR [8]. Other tasks evaluate compositionality (e.g., Winoground; [37]), visual abductive reasoning (e.g., Sherlock; [19]) and comprehension and explanation of multi-modal humor [20]. Recent progress in large language models is making way for models that can solve these tasks using instructions like BLIP2 [25] and incontext learning, or zero-shot learning, like Flamingo [2] and MLLM [22]. These recent advances pave the way for our work, which provides a challenging resource for commonsense and compositionality. \n\nWHOOPS!, is distinct from prior work that focuses on reasoning with pre-existing images. Instead, it contains synthetic images that are specifically designed to challenge AI models' abilities to reason about commonsense and compositionality, with an emphasis on images that violate expectations. Our approach uses image generation models to create unique and complex images that would be difficult or impossible to obtain otherwise, providing an opportunity to evaluate critical aspects of visual reasoning, including compositionality and commonsense reasoning.",
            "score": 0.3713750084819645,
            "section_title": "Related Work",
            "char_start_offset": 8330,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 986
                },
                {
                    "start": 989,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1550
                }
            ],
            "ref_mentions": [
                {
                    "start": 148,
                    "end": 152,
                    "matchedPaperCorpusId": "233240926"
                },
                {
                    "start": 152,
                    "end": 155,
                    "matchedPaperCorpusId": "52019251"
                },
                {
                    "start": 155,
                    "end": 158,
                    "matchedPaperCorpusId": "159041722"
                },
                {
                    "start": 158,
                    "end": 161,
                    "matchedPaperCorpusId": "53170360"
                },
                {
                    "start": 161,
                    "end": 163,
                    "matchedPaperCorpusId": "208290939"
                },
                {
                    "start": 187,
                    "end": 191,
                    "matchedPaperCorpusId": "6974607"
                },
                {
                    "start": 353,
                    "end": 357,
                    "matchedPaperCorpusId": "53734356"
                },
                {
                    "start": 587,
                    "end": 591,
                    "matchedPaperCorpusId": "246706255"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64697265625
        },
        {
            "corpus_id": "267782588",
            "title": "Beyond A*: Better Planning with Transformers via Search Dynamics Bootstrapping",
            "text": "Transformer-based architectures (Vaswani et al., 2017) have demonstrated impressive performance in different tasks, including holding conversations at the human level (Shuster et al., 2022;OpenAI, 2022OpenAI, , 2023;;Touvron et al., 2023), high-quality image understanding (Caron et al., 2021;Oquab et al., 2024;Assran et al., 2023) and video generation (Singer et al., 2023), multi-modal generation (Girdhar et al., 2023;Radford et al., 2021), and code completion (Roziere et al., 2023;OpenAI, 2021). By training these architectures on internet-scale datasets, the resulting models, such as Large Language Models (LLMs), can generalize well in real-world use cases. \n\nDespite these successes, Transformer-based architectures and LLMs still struggle when it comes to solving planning and reasoning tasks. Previous studies demonstrate that LLMs fall short in multi-step planning tasks (Valmeekam et al., 2023a,b) or when performing higher-order reasoning (Momennejad et al., 2023;Fan et al., 2020). \n\nIn recent years, various methods have been proposed to improve the performance of Transformers in these settings. One approach is to simulate the human thinking process and produce intermediate \"thoughts\" before outputting a response. Chain-of-Thought (CoT) prompting (Wei et al., 2022) and the Tree-of-thoughts (ToT) method (Yao et al., 2023) encourage the model to \"think\" step by step. While these techniques are often effective, they can also lead to worse performance, for example due to self-enforcing (Huang et al., 2023). Furthermore, techniques effective on one dataset may not work well on others due to changes in the type of reasoning involved (e.g., spatial reasoning vs. mathematical reasoning). How to enable Transformers and LLMs to plan, solve multi-step decision making tasks, and perform reasoning still remains elusive and an active area of research.",
            "score": 0.3710784562812318,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 666
                },
                {
                    "start": 669,
                    "end": 804
                },
                {
                    "start": 805,
                    "end": 997
                },
                {
                    "start": 1000,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1709
                },
                {
                    "start": 1710,
                    "end": 1870
                }
            ],
            "ref_mentions": [
                {
                    "start": 273,
                    "end": 293,
                    "matchedPaperCorpusId": "233444273"
                },
                {
                    "start": 293,
                    "end": 312,
                    "matchedPaperCorpusId": "258170077"
                },
                {
                    "start": 312,
                    "end": 332,
                    "matchedPaperCorpusId": "255999752"
                },
                {
                    "start": 354,
                    "end": 375,
                    "matchedPaperCorpusId": "252595919"
                },
                {
                    "start": 422,
                    "end": 443,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1268,
                    "end": 1286,
                    "matchedPaperCorpusId": "246411621"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.322509765625
        },
        {
            "corpus_id": "270214176",
            "title": "A Survey on Large Language Models for Code Generation",
            "text": "The empirical evidence supports the efficacy of this synthetic data generation methodology.Building upon this concept, the Alpaca [213] model, fine-tuned on 52k pieces of instruction-following data from a 7B parameter LLaMa [217] model, exhibits performance comparable to the text-davinci-003 model.WizardLM [250] introduced the Evol-Instruct technique, which incrementally transforms simple instructions into more complex variants.The fine-tuned LLaMa model using this technique has shown promising results in comparison to established proprietary LLMs such as ChatGPT [171] and GPT-4 [5], to some extent.Moreover, Microsoft has contributed to this field with their Phi series of models, predominantly trained on synthetic high-quality data, which includes Phi-1 (1.3B) [75] for Python coding, Phi-1.5 (1.3B) [135] for common sense reasoning and language understanding, Phi-2 (2.7B) [161] for advanced reasoning and language understanding, and Phi-3 (3.8B) [4] for general purposes.These models have consistently outperformed larger counterparts across various benchmarks, demonstrating the efficacy of synthetic data in model training.\n\nDrawing on the successes of data synthesis for general-purpose Large Language Models (LLMs), researchers have expanded the application of synthetic data to the realm of code generation.The Code Alpaca model, as described in [40], has been fine-tuned on a 7B and 13B LLaMA model using a dataset of 20k instruction-following examples for code generation.This dataset was created by text-davinci-00310 and employed the Self-Instruct technique [231].Building on this, the WizardCoder 15B [154] utilizes the Evol-Instruct technique to create an enhanced dataset of 78k evolved code instruction examples.This dataset originates from the initial 20k instructionfollowing dataset used by Code Alpaca [40], which was also generated by text-davinci-003.The WizardCoder model, fine-tuned on the StarCoder [132] base model, achieved a 57.3% pass@1 on the HumanEval benchmarks.",
            "score": 0.3704743737051335,
            "section_title": "Data Synthesis",
            "char_start_offset": 34254,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 91,
                    "end": 299
                },
                {
                    "start": 299,
                    "end": 432
                },
                {
                    "start": 432,
                    "end": 606
                },
                {
                    "start": 606,
                    "end": 983
                },
                {
                    "start": 983,
                    "end": 1137
                },
                {
                    "start": 1139,
                    "end": 1324
                },
                {
                    "start": 1324,
                    "end": 1491
                },
                {
                    "start": 1491,
                    "end": 1585
                },
                {
                    "start": 1585,
                    "end": 1737
                },
                {
                    "start": 1737,
                    "end": 1882
                },
                {
                    "start": 1882,
                    "end": 2003
                }
            ],
            "ref_mentions": [
                {
                    "start": 1579,
                    "end": 1584,
                    "matchedPaperCorpusId": "254877310"
                },
                {
                    "start": 1623,
                    "end": 1628,
                    "matchedPaperCorpusId": "259164815"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.876953125
        },
        {
            "corpus_id": "269187896",
            "title": "Can Language Models Solve Olympiad Programming?",
            "text": "In this paper, we additional provide insights on their performance in competitive programming, a much more difficult domain.Our instantiation of retrieval augmented generation additionally takes inspiration from cognitive architectures for humans reasoning (Sumers et al., 2023) and classical case-based reasoning literature (Aamodt & Plaza, 1994;Schank, 1983), mirroring the types of information humans find useful for problem solving.Sumers et al. (2022) investigates agent learning from human provided feedback under synthetic tasks.Macina et al. (2023) aims to provide a tutoring ruleset to effectively engage LMs in dialogue math problem solving.In this paper we adopt a similar setup to code, applying a specified interaction ruleset to gauge the ability of models to respond to feedback.",
            "score": 0.37047426249752563,
            "section_title": "Related Work",
            "char_start_offset": 6225,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 124,
                    "end": 436
                },
                {
                    "start": 436,
                    "end": 536
                },
                {
                    "start": 536,
                    "end": 651
                },
                {
                    "start": 651,
                    "end": 794
                }
            ],
            "ref_mentions": [
                {
                    "start": 325,
                    "end": 347,
                    "matchedPaperCorpusId": "7069926"
                },
                {
                    "start": 436,
                    "end": 456,
                    "matchedPaperCorpusId": "252815954"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1331787109375
        },
        {
            "corpus_id": "276575915",
            "title": "Logic Haystacks: Probing LLMs Long-Context Logical Reasoning (Without Easily Identifiable Unrelated Padding)",
            "text": "Our work stands at the intersection of two lines of research: logical reasoning with synthetic simplified English data, and context length evaluation. Levy et al. (2024) and Kuratov et al. (2024) explored the intersection of the two, but they imported the padding technique from previous work on LLM stress-texting, while we scale the dataset's original generation process instead of padding it with other text. Human annotated long context benchmarks are very valuable but hard to annotate (Bowman et al., 2022;Wang et al., 2024;Bai et al., 2024), which causes current language models to saturate them. \n\nSynthetic datasets for reasoning Numerous works investigate the logical capabilities of NLP models using textual datasets and symbolic reasoning (Helwe et al., 2022). We focus on the grammarderived synthetic datasets. RuleTaker (Clark et al., 2020) LogicNLI (Tian et al., 2021), FLD (Morishita et al., 2023) and FOL-NLI (Sileo, 2024) address different subsets of first-order logic with English translations. Other works also explore non-standard logic with synthetic datasets, notably probabilistic (Jin et al., 2023;Sileo and Moens, 2023), paraconsistent (Kazemi et al., 2024), epistemic (Sileo and Lernould, 2023) logics. \n\nThese approaches focus on input sizes typically suitable to a standard BERT (Devlin et al., 2018) encoder (<512 tokens). Here, we push the number of expressions in the input while avoiding paradoxes. This is related to the satisfiability problem which was explored by Richardson et al. (2020); Richardson and Sabharwal (2022) who use a solver to study the satisfiability in natural language using the Z3 solver and dedicated generation logic on constrained problems. However, they also focus on relatively moderate text size while we use satisfiability checking as a stepping stone to generate large text and not only as a task in itself.",
            "score": 0.36986548342898684,
            "section_title": "Related work",
            "char_start_offset": 10514,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 603
                },
                {
                    "start": 606,
                    "end": 772
                },
                {
                    "start": 773,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 1013
                },
                {
                    "start": 1014,
                    "end": 1229
                },
                {
                    "start": 1232,
                    "end": 1352
                },
                {
                    "start": 1353,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1698
                },
                {
                    "start": 1699,
                    "end": 1870
                }
            ],
            "ref_mentions": [
                {
                    "start": 751,
                    "end": 771,
                    "matchedPaperCorpusId": "257696994"
                },
                {
                    "start": 834,
                    "end": 854,
                    "matchedPaperCorpusId": "211126663"
                },
                {
                    "start": 864,
                    "end": 883,
                    "matchedPaperCorpusId": "243865235"
                },
                {
                    "start": 889,
                    "end": 913,
                    "matchedPaperCorpusId": "260846474"
                },
                {
                    "start": 926,
                    "end": 939,
                    "matchedPaperCorpusId": "270560130"
                },
                {
                    "start": 1105,
                    "end": 1123,
                    "matchedPaperCorpusId": "266044166"
                },
                {
                    "start": 1123,
                    "end": 1145,
                    "matchedPaperCorpusId": "253383825"
                },
                {
                    "start": 1162,
                    "end": 1183,
                    "matchedPaperCorpusId": "259144942"
                },
                {
                    "start": 1195,
                    "end": 1221,
                    "matchedPaperCorpusId": "258547259"
                },
                {
                    "start": 1500,
                    "end": 1524,
                    "matchedPaperCorpusId": "202583828"
                },
                {
                    "start": 1526,
                    "end": 1557,
                    "matchedPaperCorpusId": "245219217"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7109375
        },
        {
            "corpus_id": "278171297",
            "title": "ReasonIR: Training Retrievers for Reasoning Tasks",
            "text": "Previous work has shown that LLM-generated reasoning-intensive questions often lack the requisite diversity and difficulty, and they often involve human-in-the-loop to generate high-quality difficult questions (Shah et al., 2024;Chiu et al., 2024). To improve diversity and eliminate the need for human effort, we synthesize reasoning-intensive training data by generating hard queries (HQ) from high-quality documents using a \"human-like brainstorm guideline\" for hard query generation. \n\nReasoning-worthy seed document selection. We define a reasoning-worthy document as one that contains knowledge that can potentially aid in understanding and solving reasoning tasks. We assume that such documents are more helpful for reasoning-intensive query generation. In contrast, when working with less informative sources such as subjective Web forum comments or sparse event descriptions, LLMs typically struggle to generate challenging questions. The documents collected by Su et al. ( 2024) cover a diverse range of scientific domains, such as biology, economics, mathematics, and coding, and many of them have been cited in human answers to reasoning-intensive questions on forums. Therefore, we use these documents as the initial knowledge pool and further apply the FineWeb-Edu classifier (Penedo et al., 2024) to score each document based on its educational value. We remove those with scores lower than 2, which usually contain gibberish or subjective content, and use the filtered documents as seed documents for hard query generation. \n\nReasoning-intensive document-to-query generation. An ideal set of reasoning-intensive queries has three properties: (1) challenging -demanding reasoning beyond simple lexical or superficial semantic matching; \n\n(2) self-contained -understandable without the presence of the seed document; (3) diverse -imitating diverse question styles in various problem-solving scenarios.",
            "score": 0.3698542570865704,
            "section_title": "Reasoning-intensive Document-to-query Generation (HQ)",
            "char_start_offset": 15894,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 487
                },
                {
                    "start": 490,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 943
                },
                {
                    "start": 944,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1366
                },
                {
                    "start": 1367,
                    "end": 1539
                },
                {
                    "start": 1542,
                    "end": 1591
                },
                {
                    "start": 1592,
                    "end": 1750
                },
                {
                    "start": 1753,
                    "end": 1915
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.377685546875
        },
        {
            "corpus_id": "250390686",
            "title": "GraDA: Graph Generative Data Augmentation for Commonsense Reasoning",
            "text": "Recent work has seen the emergence of several datasets for improving commonsense reasoning of language models through tasks like question answering (QA) (Sap et al., 2019b;Talmor et al., 2019;Bisk et al., 2020) and natural language inference (Bhagavatula et al., 2020;Zellers et al., 2019;Sakaguchi et al., 2020). Some of these datasets are based on existing knowledge graphs that represent different aspects of commonsense through entities and relations. For example, annotators for SocialIQA (Sap et al., 2019b) were shown an event from the inferential knowledge graph ATOMIC (Sap et al., 2019a) and instructed to turn it into a sentence by adding names, filling placeholders and adding context, etc. For multiple-choice QA datasets, annotators are also instructed to write distractor choices for each question. These useful datasets are collected through a time-taking and money-intensive crowdsourcing process which is hard to scale. Large pretrained models like GPT2 (Radford et al., 2018) can be finetuned to generate sentences from narrow data distributions, and it has recently been leveraged to augment datasets for text classification (Anaby-Tavor et al., 2020) and question answering (Puri et al., 2020;Yang et al., 2020). However, it is challenging to generate augmentation data for commonsense reasoning because the generated questions and answers (referred to as \"synthetic\" in rest of the paper) need to depict plausible real-world scenarios accurately. Hence, we develop GRADA, a graph-based generative data augmentation framework to generate synthetic samples from existing knowledge graphs that encode information about the real world. We focus on generating synthetic samples for models that perform discriminative and generative commonsense question answering.\n\nEach sample in commonsense reasoning datasets comprises a question which describes a real-world scenario and can be mapped to a set of predefined entities and relations from knowledge bases like ConceptNet and ATOMIC. For instance, the question \"Besides a mattress, name something people sleep on.\" from the ProtoQA dataset (Boratko et al",
            "score": 0.3692202581975264,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91064453125
        },
        {
            "corpus_id": "264439655",
            "title": "MuSR: Testing the Limits of Chain-of-thought with Multistep Soft Reasoning",
            "text": "While large language models (LLMs) equipped with techniques like chain-of-thought prompting have demonstrated impressive capabilities, they still fall short in their ability to reason robustly in complex settings. However, evaluating LLM reasoning is challenging because system capabilities continue to grow while benchmark datasets for tasks like logical deduction have remained static. We introduce MuSR, a dataset for evaluating language models on multistep soft reasoning tasks specified in a natural language narrative. This dataset has two crucial features. First, it is created through a novel neurosymbolic synthetic-to-natural generation algorithm, enabling the construction of complex reasoning instances that challenge GPT-4 (e.g., murder mysteries roughly 1000 words in length) and which can be scaled further as more capable LLMs are released. Second, our dataset instances are free text narratives corresponding to real-world domains of reasoning; this makes it simultaneously much more challenging than other synthetically-crafted benchmarks while remaining realistic and tractable for human annotators to solve with high accuracy. We evaluate a range of LLMs and prompting techniques on this dataset and characterize the gaps that remain for techniques like chain-of-thought to perform robust reasoning.",
            "score": 0.3688472385533037,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78369140625
        },
        {
            "corpus_id": "258437058",
            "title": "Beyond Classification: Financial Reasoning in State-of-the-Art Language Models",
            "text": "Large Language Models (LLMs), consisting of 100 billion or more parameters, have demonstrated remarkable ability in complex multi-step reasoning tasks. However, the application of such generic advancements has been limited to a few fields, such as clinical or legal, with the field of financial reasoning remaining largely unexplored. To the best of our knowledge, the ability of LLMs to solve financial reasoning problems has never been dealt with, and whether it can be performed at any scale remains unknown. To address this knowledge gap, this research presents a comprehensive investigation into the potential application of LLMs in the financial domain. The investigation includes a detailed exploration of a range of subjects, including task formulation, synthetic data generation, prompting methods, and evaluation capability. Furthermore, the study benchmarks various GPT variants with parameter scales ranging from 2.8B to 13B, with and without instruction tuning, on diverse dataset sizes. By analyzing the results, we reveal that the ability to generate coherent financial reasoning first emerges at 6B parameters, and continues to improve with better instruction-tuning or larger datasets. Additionally, the study provides a publicly accessible dataset named sFIOG (Synthetic-Financial Investment Opinion Generation), consisting of 11,802 synthetic investment thesis samples, to support further research in the field of financial reasoning. Overall, this research seeks to contribute to the understanding of the efficacy of language models in the field of finance, with a particular emphasis on their ability to engage in sophisticated reasoning and analysis within the context of investment decision-making.",
            "score": 0.3687127565580741,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61767578125
        },
        {
            "corpus_id": "259503802",
            "title": "Tabular and latent space synthetic data generation: a literature review",
            "text": "We also discuss generation mechanisms used in the latent space, since the generation mechanisms in tabular data and latent space may be used interchangeably. In addition, we focus on the ML perspective of synthetic data, as opposed to the practical perspective; according to the practical perspective, synthetic data is used as a proxy of real data when it is inaccessible, essential, and a secondary asset for tasks like education, software development, or systems demonstrations [36]. The ML perspective focuses on the generation of synthetic data based on existing, naturally occurring data to either improve a ML task or replace the original data. \n\nThe different taxonomies of synthetic data generation established in the literature follow a similar philosophy but vary in terminology and are often specific to the technique discussed. Regardless, it is possible to establish a broader taxonomy without giving up on specificity. This study provides a joint overview of the different data generation approaches, domains, and ML techniques where data generation is being used, as well as a common taxonomy across domains. It extends the analyses found in these articles and uses the compiled knowledge to identify research gaps. We compare the strengths and weaknesses of the models developed within each of these 1 Results obtained using Google Scholar, limited to articles published since 2019, using the search query (\"synthetic data generation\" OR \"oversampling\" OR \"imbalanced learning\" OR \"data augmentation\") AND (\"literature review\" OR \"survey\") Retrieved on August 11 th , 2022. More articles were added later whenever found relevant. \n\nfields. Finally, we identify possible future research directions to address some of the limitations found. The contributions of this paper are summarized below: \n\n\u2022 Bridge different ML concepts that use synthetic data generation techniques (Sections \"Background\" and \"Algorithmic applications\"); \u2022 Propose a synthetic data generation/data augmentation taxonomy to address the ambiguity in the various taxonomies proposed in the literature (Section \"Data generation taxonomy\"); \u2022 Characterize all the relevant data generation methods using the proposed taxonomy (Sections \"Data generation taxonomy\" and \"Algorithmic applications\"); \u2022 Consolidate the current generation mechanisms across the different techniques and methods to evaluate the quality of synthetic data generation (Sections \"Generation mechanisms\" and \"Evaluating the quality of synthetic data\"); [31] Image --Broad overview of image data augmentation.",
            "score": 0.3685763966626367,
            "section_title": "Motivation, scope and contributions",
            "char_start_offset": 6567,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 651
                },
                {
                    "start": 654,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 1124
                },
                {
                    "start": 1125,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1590
                },
                {
                    "start": 1591,
                    "end": 1646
                },
                {
                    "start": 1649,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1755
                },
                {
                    "start": 1756,
                    "end": 1809
                },
                {
                    "start": 1812,
                    "end": 2563
                }
            ],
            "ref_mentions": [
                {
                    "start": 481,
                    "end": 485,
                    "matchedPaperCorpusId": "204812234"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.49365234375
        },
        {
            "corpus_id": "269042851",
            "title": "Best Practices and Lessons Learned on Synthetic Data for Language Models",
            "text": "The rapid advancement of artificial intelligence (AI) technologies has led to their widespread adoption across numerous domains, from assistant agents (e.g., ACT-1, from Adept AI1 ) and software development (e.g., Devin, from Cognition Lab2 ) to healthcare (Singhal et al., 2022) and finance (Zheng et al., 2022). However, the success of AI models heavily relies on the availability of large, diverse, and high-quality datasets for training and evaluation. Acquiring such datasets can be a significant challenge due to data scarcity (Babbar & Sch \u00f6lkopf, 2019), privacy concerns (Abay et al., 2019), and the sheer cost of data collection and annotation (Gilardi et al., 2023a). Pessimists predict that we will run out of fresh text data in 2050 and image data in 2060 (Villalobos et al., 2022). Synthetic data has emerged as a promising solution to address these challenges (Nikolenko, 2021). Synthetic data refers to artificially generated data that mimics the characteristics and patterns of real-world data, but is created through algorithms (Saxton et al., 2019), generative models (Borisov et al., 2022;Meng et al., 2022), or even simulations (Vezhnevets et al., 2023;Liu et al., 2023c), rather than being directly created by humans. By leveraging synthetic data, we can not only overcome the limitations of real-world data but also unlock the potential to develop more robust, reliable, and fair AI models (Lucini, 2021;Lu et al., 2023). \n\nOne of the many benefits of synthetic data is that it can be generated at scale, providing an abundant supply of training and testing data for AI models. This is particularly valuable in domains where real-world data is scarce or difficult to obtain (e.g., weather data covering all conditions (Li et al., 2023a;Lam et al., 2023)).",
            "score": 0.3685763966626367,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 1238
                },
                {
                    "start": 1239,
                    "end": 1443
                },
                {
                    "start": 1446,
                    "end": 1599
                },
                {
                    "start": 1600,
                    "end": 1777
                }
            ],
            "ref_mentions": [
                {
                    "start": 292,
                    "end": 312,
                    "matchedPaperCorpusId": "248525885"
                },
                {
                    "start": 579,
                    "end": 598,
                    "matchedPaperCorpusId": "259950018"
                },
                {
                    "start": 653,
                    "end": 676,
                    "matchedPaperCorpusId": "257766307"
                },
                {
                    "start": 1045,
                    "end": 1066,
                    "matchedPaperCorpusId": "85504763"
                },
                {
                    "start": 1108,
                    "end": 1126,
                    "matchedPaperCorpusId": "246680398"
                },
                {
                    "start": 1426,
                    "end": 1442,
                    "matchedPaperCorpusId": "256662279"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76318359375
        },
        {
            "corpus_id": "277856866",
            "title": "SLURG: Investigating the Feasibility of Generating Synthetic Online Fallacious Discourse",
            "text": "Although it used to take much longer for machine learning models to reach or surpass human-level performance on benchmark tasks, modern machine learning models are saturating benchmarks at in-creasingly faster rates Kiela et al. (2021). As models improve faster and faster, the demand for highquality, challenging datasets has also risen, ushering in the era of synthetic data generation. Generating synthetic data, specifically from large language models (LLMs) offer a possible solution to both increasing demand for large volumes of data, as well as reducing the cost incurred when using human annotators. \n\nSynthetic data generation has shown great promise for improving model performance across many tasks in NLP. Niu et al. (2024) show that generating artificial user-agent interactions can improve the performance of dialogue state tracking models, even with lower amounts of real data. Of particular interest for this task, research such as that from Balog et al. (2024) investigate the feasibility of producing realistic synthetic user social media discussions.",
            "score": 0.3685763966626367,
            "section_title": "The Rise of Synthetic Data",
            "char_start_offset": 3473,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 608
                },
                {
                    "start": 611,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 893
                },
                {
                    "start": 894,
                    "end": 1070
                }
            ],
            "ref_mentions": [
                {
                    "start": 719,
                    "end": 736,
                    "matchedPaperCorpusId": "269982904"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86083984375
        },
        {
            "corpus_id": "268536850",
            "title": "LeanReasoner: Boosting Complex Logical Reasoning with Lean",
            "text": "Logical reasoning, a bedrock of intelligence and a core capability of humans, has been a challenging issue for machine learning systems for a long time.LLMs, despite their impressive abilities to understand and generate natural language, often fall short when dealing with complex logical reasoning tasks.They frequently suffer from logical inconsistencies, where the model hallucinates and makes statements not grounded in premises, leading to spurious results (Saparov and He, 2023;Dasgupta et al., 2022).\n\nRecent advances in AI have adopted a structured approach to tackle these reasoning problems by splitting them into symbolic formalization and problem-solving (He-Yueya et al., 2023;Pan et al., 2023;Ye et al., 2023).Specifically, the formalization step is often handled by a large language model, while problem-solving is handled by an offthe-shelf symbolic solver.In this approach, sym-1 Our code and data is available at https://github.com/Some-random/theorem-proving-reasoning.\n\nbolic solvers essentially act as a rigorous checkpoint, ensuring that the model outputs align with logical rules, thereby mitigating the issue of logic inconsistency.In these approaches, solvers may range from being completely deterministic, like SymPy (He-Yueya et al., 2023), or relying on a combination of heuristics and basic machine learning techniques, as is the case with Pyke (Pan et al., 2023) and Z3 (Ye et al., 2023;de Moura and Bj\u00f8rner, 2008).While this approach successfully addresses hallucinations, it still struggles with more complex problems.\n\nAs a powerful theorem prover and a versatile programming language, Lean (de Moura et al., 2015) presents a compelling solution to connect symbolic solvers with linguistic resources.Much like symbolic solvers, Lean has a strict checking system that ensures each reasoning step is certified.What distinguishes it, however, is its additional functionality as a programming language developed specifically for theorem proving.Every day, a substantial amount of code is written in Lean, capturing reasoning \"nuggets\" with step-by-step rationals that are useful for training LLMs.",
            "score": 0.36773383239591656,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 152,
                    "end": 305
                },
                {
                    "start": 305,
                    "end": 507
                },
                {
                    "start": 509,
                    "end": 724
                },
                {
                    "start": 724,
                    "end": 873
                },
                {
                    "start": 873,
                    "end": 962
                },
                {
                    "start": 962,
                    "end": 988
                },
                {
                    "start": 990,
                    "end": 1156
                },
                {
                    "start": 1156,
                    "end": 1445
                },
                {
                    "start": 1445,
                    "end": 1550
                },
                {
                    "start": 1552,
                    "end": 1733
                },
                {
                    "start": 1733,
                    "end": 1841
                },
                {
                    "start": 1841,
                    "end": 1974
                },
                {
                    "start": 1974,
                    "end": 2126
                }
            ],
            "ref_mentions": [
                {
                    "start": 1417,
                    "end": 1444,
                    "matchedPaperCorpusId": "15912959"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1767578125
        },
        {
            "corpus_id": "268249187",
            "title": "Data Augmentation using Large Language Models: Data Perspectives, Learning Paradigms and Challenges",
            "text": "Automate-CoT (Shum et al., 2023) proposes a fully automatic pipeline for finding better chainof-thought prompts to solve commonsense reasoning question answering tasks. This pipeline leverages powerful LLMs like GPT-3 to augment reasoning paths, prune incorrect paths, and select optimal combinations of exemplars. \n\n\u2022 Story Completing: Whitehouse et al. (2023) explore the generation of synthetic data for question answering under the multilingual setting using various LLMs. This work conducts experiments with dolly-12B, stablevicuna-13B, ChatGPT, and GPT-4 on three datasets that covers over 20 languages on commonsense reasoning and story completing tasks. \n\n\u2022 Fact Checking: SCOTT (Wang et al., 2023b) prompts a GPT-neox-20B model to generate annotated data to train a smaller student model to perform QA tasks. Experiments conducted on four QA datasets across commonsense reasoning and fact checking showcase the efficiency of this data augmentation technique. \n\n\u2022 MRC: Samuel et al. (2023) introduce a GPT-4 based data augmentation technique targeting the low source setting on machine reading comprehension task. This technique separately generates passsages, questions, and answers using in-context learning. \n\n\u2022 MedicalQA: Dr.LLaMA (Guo et al., 2023) employs ChatGPT and GPT-4 to either rewrite existing medical question-answering pairs or generate new pairs from the training dataset with zero-shot prompting. Such data augmentation helps to train a LLaMA model specialised on medical knowledge. \n\nLogical Reasoning Logical reasoning involves the ability to analyze and draw conclusions based on given information, a crucial cognitive skill that LLMs aim to master. In the context of LLMs, logical reasoning reflects the model's aptitude for understanding and manipulating symbolic representations, solving problems, and making deductions. The significance of logical reasoning lies in its application across various domains, from puzzlesolving and decision-making to complex problemsolving tasks, showcasing the model's capacity for high-level cognitive functions. \n\nWithin the logical reasoning task, Chain-of-Thought has been a popular technique that improves the reasoning process for the language models.",
            "score": 0.3674427787682181,
            "section_title": "\u2022 Commonsense Reasoning:",
            "char_start_offset": 39211,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 314
                },
                {
                    "start": 317,
                    "end": 476
                },
                {
                    "start": 477,
                    "end": 661
                },
                {
                    "start": 664,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 967
                },
                {
                    "start": 970,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1218
                },
                {
                    "start": 1221,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1507
                },
                {
                    "start": 1510,
                    "end": 1677
                },
                {
                    "start": 1678,
                    "end": 1851
                },
                {
                    "start": 1852,
                    "end": 2077
                },
                {
                    "start": 2080,
                    "end": 2221
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.783203125
        },
        {
            "corpus_id": "269187952",
            "title": "On the Empirical Complexity of Reasoning and Planning in LLMs",
            "text": "Chain-of-thought (CoT), tree-of-thought (ToT), and related techniques work surprisingly well in practice for some complex reasoning tasks with Large Language Models (LLMs), but why? This work seeks the underlying reasons by conducting experimental case studies and linking the performance benefits to well-established sample and computational complexity principles in machine learning. We experimented with 6 reasoning tasks, ranging from grade school math, air travel planning, ..., to Blocksworld. The results suggest that (i) both CoT and ToT benefit significantly from task decomposition, which breaks a complex reasoning task into a sequence of steps with low sample complexity and explicitly outlines the reasoning structure, and (ii) for computationally hard reasoning tasks, the more sophisticated tree structure of ToT outperforms the linear structure of CoT. These findings provide useful guidelines for the use of LLM in solving reasoning tasks in practice.",
            "score": 0.36690629757898274,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.418212890625
        },
        {
            "corpus_id": "277065581",
            "title": "ThinkPatterns-21k: A Systematic Study on the Impact of Thinking Patterns in LLMs",
            "text": "Prompt Engineering for Reasoning The development of language model reasoning has witnessed significant progress through prompt engineering innovations. Following the seminal work of Chainof-Thought(CoT) (Wei et al., 2022), researchers have proposed diverse thinking frameworks. These include Self-Ask (Press et al., 2023) for interactive problem decomposition, Self-Debate (Liang et al., 2024) for dialectical reasoning, and Divideand-Conquer (Wang et al., 2023;Khot et al., 2023) for systematic problem solving. However, these approaches merely focus on prompt engineering without internalizing these thinking patterns into the model's parameters. \n\nModel Training for Enhanced Reasoning Complementing prompt engineering efforts, another research direction focuses on enhancing reasoning capabilities through data synthesis and model training. Recent advances include the application of Monte Carlo Tree Search for reasoning path generation (Qin et al., 2024;Guan et al., 2025) and the utilization of GPT-4 for demonstration synthesis (Chen et al., 2024). Knowledge distillation from advanced reasoning models has also shown promise (Min et al., 2024;Guo et al., 2025;Team, 2025;Thoughts, 2025;Muennighoff et al., 2025;Ye et al., 2025;Xu et al., 2025). While these approaches have shown promising results, they have not thoroughly investigated the advantages of structured thinking patterns, and the relationship between model size and thinking pattern effectiveness remains largely unexplored. Test Time Scaling for LLMs The application of test-time compute scaling has emerged as a promising approach to enhance the reasoning capabilities of large language models. Broadly, this methodology can be divided into two strategies: generating a single, extended CoT and performing repeated sampling of CoT responses. In the single long CoT approach, models-such as OpenAI's O1/O3, QwQ and Deepseek R1 trained to produce individual, lengthy CoT sequences that incorporate in-context reflection and backtracking to address complex reasoning tasks (Guo et al., 2025;Jaech et al., 2024;Team, 2024b).",
            "score": 0.3668604267813358,
            "section_title": "Related Work",
            "char_start_offset": 4948,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 648
                },
                {
                    "start": 651,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1495
                },
                {
                    "start": 1496,
                    "end": 1667
                },
                {
                    "start": 1668,
                    "end": 1814
                },
                {
                    "start": 1815,
                    "end": 2093
                }
            ],
            "ref_mentions": [
                {
                    "start": 203,
                    "end": 221,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 301,
                    "end": 321,
                    "matchedPaperCorpusId": "252762102"
                },
                {
                    "start": 373,
                    "end": 393,
                    "matchedPaperCorpusId": "258967540"
                },
                {
                    "start": 462,
                    "end": 480,
                    "matchedPaperCorpusId": "252715485"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.45458984375
        },
        {
            "corpus_id": "271309962",
            "title": "Open Artificial Knowledge",
            "text": "While handcrafted human data has shown significant improvements in supervised fine-tuning (SFT) of LLMs, particularly for tasks like code generation and mathematical reasoning (Roziere et al., 2023;Wan et al., 2024), the scarcity and cost of creating such high-quality data have led to the increasing use of synthetic data as a proxy.This method primarily leverages strongly capable LLMs, such as the GPT family (Achiam et al., 2023) to produce high-quality synthetic data (Li et al., 2023b;Josifoski et al., 2023;Taori et al., 2023).\n\nRecent research has highlighted LLMs' ability to rephrase for improved responses and boost synthetic data for effective SFT (Gallego, 2024;Chen et al., 2024).These developments suggest that the use of synthetic data in model training will continue to grow in the future, with ongoing research exploring various techniques to leverage synthetic data effectively The process begins with extracting general topics from extensive human knowledge databases such as Wikipedia and GPT-4o models.These high-level and sub-level topics are then used in an automatic prompt generation step, which employs two methods: meta prompt engineering using large language models (LLMs) and cost-effective programming prompt engineering.The generated prompts are subsequently fed into state-of-the-art open-source LLMs (at the time of writing, five models were used: Llama3-8B, Llama-70B, Mixtral7x8B, Gemma-7B (Team et al., 2024), and Gemma-2-9B (Team, 2024)) to create the OAK dataset.\n\nfor improving LLM performance and alignment (Hao et al., 2024).",
            "score": 0.36628082592433026,
            "section_title": "Introduction",
            "char_start_offset": 1773,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 334
                },
                {
                    "start": 334,
                    "end": 534
                },
                {
                    "start": 536,
                    "end": 694
                },
                {
                    "start": 694,
                    "end": 1024
                },
                {
                    "start": 1024,
                    "end": 1252
                },
                {
                    "start": 1252,
                    "end": 1502
                },
                {
                    "start": 1504,
                    "end": 1567
                }
            ],
            "ref_mentions": [
                {
                    "start": 491,
                    "end": 514,
                    "matchedPaperCorpusId": "257378179"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87451171875
        },
        {
            "corpus_id": "249890002",
            "title": "Insights into Pre-training via Simpler Synthetic Tasks",
            "text": "For each synthetic task, we generated one million examples. To generate synthetic task data, we sampled tokens a vocabulary of size 32K, matching the vocabulary size of Pre-trained_T5. Details about data generation can be found in the original papers. We provide an example of each task in Figure 2. Top Left: LIME deduct task, whose input consists of a \"rule string\" and \"substitution dictionary\" and output is a \"result string.\" The \"rule symbols\" and \"math symbols\" are written as actual letters and math operations for reader clarity. \n\nIn real generated data, each example's rule and math symbols are randomly sampled tokens. Top Middle: T5 style masked language modelling with Dyck language. In the input, the grey boxes indicate masked out spans. The model is trained to predict the masked spans, each separated by a special token <s>. In this example, the \"noise_density\" is 0.33 (so 5 out of 15 input brackets are corrupted) and the \"span_corruption_length\" is 1 (so each masked span is of length 1). In actual generated data, the \"span_corruption_length\" is 3 and the \"noise_density\" is 0.15, which are the same parameter values used for natural language Pre-trained_T5. Top Right: Nonsense_Summary task. In this example, two operations to create the summary are \"copy sentence containing a keyword\" and \"identify keyword sentiment.\" Bottom Left: Set task. Bottom Right: Identity task. \n\nLIME LIME is a set of three tasks-Deduct, Induct, and Abduct-inspired by Charles Peirce's three reasoning primitives (Wu et al., 2021). Each task consists of three elements: a rule string, a dictionary that represents substitutions, and a result string that is the result of applying those substitutions to the rule string. The three tasks are then constructed by using two of the three elements as inputs to predict the remaining element.",
            "score": 0.36599109317703554,
            "section_title": "Synthetic Tasks",
            "char_start_offset": 5154,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 59
                },
                {
                    "start": 60,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 538
                },
                {
                    "start": 541,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 1009
                },
                {
                    "start": 1010,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1343
                },
                {
                    "start": 1344,
                    "end": 1366
                },
                {
                    "start": 1367,
                    "end": 1395
                },
                {
                    "start": 1398,
                    "end": 1533
                },
                {
                    "start": 1534,
                    "end": 1721
                },
                {
                    "start": 1722,
                    "end": 1837
                }
            ],
            "ref_mentions": [
                {
                    "start": 1515,
                    "end": 1532,
                    "matchedPaperCorpusId": "231627491"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.31201171875
        },
        {
            "corpus_id": "257378179",
            "title": "Exploiting Asymmetry for Synthetic Training Data Generation: SynthIE and the Case of Information Extraction",
            "text": "Large language models (LLMs) have demonstrated the ability to generate highly fluent and coherent textual data. One promising application of this capability is the generation of large amounts of high-quality synthetic data for training and evaluating smaller models. This becomes particularly useful for tasks where high-quality datasets are not readily available or access to real data is limited or expensive. However, in many complex natural language processing (NLP) tasks, the textual input x is mapped to a structured (rather than free-text) output y, and in such cases, LLMs may perform poorly as synthetic-data generators, since pretraining did not gear them to produce the specific required output format (even with in-context learning). Here \n\nFigure 1: Exploiting asymmetry for SDG. For hard tasks of interest with input X and output Y , the reverse task (from Y to X) may be much easier for an LLM. If so, we can generate high-quality training pairs (X,Y ) by prompting an LLM to generate plausible inputs X from outputs Y . This often holds true for tasks with structured Y , as in closed information extraction, where X would be the input text and Y would be the list of (subject, relation, object) triplets expressed in the input text. Furthermore, this ensures full control over the sampling distribution P(Y ), and thus balanced datasets. \n\nwe propose to alleviate this issue by generating synthetic data in the reverse direction by first sampling an output structure y and then prompting the LLM to generate a corresponding input text x (see Fig. 1). We postulate that for many tasks, given appropriate in-context information, an LLM can generate a meaningful x corresponding to a structure y, even when the original task cannot be solved directly by the LLM. Exploiting this asymmetry, then, will allow us to synthetically generate high-quality data even for hard tasks. Furthermore, the flexibility to choose the distribution over output structures y gives us fine-grained control over the data. \n\nA good example of such a task, on which we focus in this work, is closed information extraction (cIE). In cIE, a model must extract a set of disambiguated triplets (i.e., facts) y from a given text x.",
            "score": 0.36572506171905184,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 746
                },
                {
                    "start": 747,
                    "end": 751
                },
                {
                    "start": 754,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1250
                },
                {
                    "start": 1251,
                    "end": 1355
                },
                {
                    "start": 1358,
                    "end": 1568
                },
                {
                    "start": 1569,
                    "end": 1777
                },
                {
                    "start": 1778,
                    "end": 1889
                },
                {
                    "start": 1890,
                    "end": 2015
                },
                {
                    "start": 2018,
                    "end": 2120
                },
                {
                    "start": 2121,
                    "end": 2218
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92431640625
        },
        {
            "corpus_id": "272689782",
            "title": "Causal Language Modeling Can Elicit Search and Reasoning Capabilities on Logic Puzzles",
            "text": "There are many works which study the ability of language models to perform reasoning tasks which involve search and planning with mixed evidence as to whether they are actually learning to reason and plan. [BMR + 20] was a seminal work which showed that large language models (LLMs) are few-shot learners and [KGR + 22] argued that they can be zero-shot reasoners. In contrast to the above, [VOSK22] show that LLMs when acting alone or when combined with techniques such as Chain-of-Thought or Tree-of-thought cannot solve some standard planning and reasoning benchmarks when the questions are rephrased with a new terminology. This is even when we use techniques such as Chain-of-Thought, fine-tuning etc. [XZC + 24] show that even the biggest LLMs perform very poorly at real-world travel planning tasks with a multitude of soft and hard constraints. [DLS + 24] show that LLMs are limited and brittle in their ability to perform compositional tasks such as multi-digit multiplication, logic grid puzzles and dynamic programming. [MHF + 23] argue that LLMs have weak cognitive maps which are crucial for planning. [BN24] show that rather than the architecture, the training objective of next-token prediction might be crippling the planning and reasoning ability of a language model. \n\nThere are many works which use the help of synthetic tasks to gain insights into how Transformer language models work. We present a non-exhaustive list here. Comparison to Traditional Solvers and other ML approaches. Traditional constraint satisfaction libraries use very powerful combinatorial search algorithms to solve logic puzzles and are much more powerful than any deep model we learn here. In addition, many prior works study machine learning-based approaches for solving general combinatorial problems [BPL + 16, MSIB21, CFK + 23]. In addition, there are several approaches that tend to handcraft the architecture or loss to the puzzle using human understanding of the puzzle structure [MKPZ11,PPW18,Zhu]. Even though our goal is to understand the capabilities and limitations of causal language modeling and not to compete with such solvers, we discuss some of these works more in detail. \n\n[MKPZ11] try to setup a Hopfield network to solve Sudoku puzzles.",
            "score": 0.36565607927683474,
            "section_title": "Related work",
            "char_start_offset": 12875,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1284
                },
                {
                    "start": 1287,
                    "end": 1405
                },
                {
                    "start": 1406,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1503
                },
                {
                    "start": 1504,
                    "end": 1684
                },
                {
                    "start": 1685,
                    "end": 1827
                },
                {
                    "start": 1828,
                    "end": 2001
                },
                {
                    "start": 2002,
                    "end": 2185
                },
                {
                    "start": 2188,
                    "end": 2253
                }
            ],
            "ref_mentions": [
                {
                    "start": 1115,
                    "end": 1121,
                    "matchedPaperCorpusId": "263829051"
                },
                {
                    "start": 1982,
                    "end": 1990,
                    "matchedPaperCorpusId": "16864069"
                },
                {
                    "start": 1990,
                    "end": 1996,
                    "matchedPaperCorpusId": "46208513"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1007080078125
        },
        {
            "corpus_id": "272911172",
            "title": "HDFlow: Enhancing LLM Complex Problem-Solving with Hybrid Thinking and Dynamic Workflows",
            "text": "To enhance reasoning task diversity and coverage, our data synthesis pipeline consists of three steps (Figure 3). In the first step, we strategically leverage human-authored seed tasks to inspire the creation of new reasoning problems (similar to Self-Instruct (Wang et al., 2023)) or let the LLM brainstorm reasoning puzzles that cover a variety of task formats, difficulty levels, and problem domains. This step only focuses on generating high-level task descriptions to encourage diversity. In the second step, we apply deduplication to remove near-identical tasks. Finally, we apply LLMs again to write three specific problems based on the task descriptions and validate those problems. \n\nTask Generation Inspired by Seed Tasks. The first step of our reasoning data synthesis pipeline is generating an expanded set of reasoning tasks. We augment the few-shot prompts with 10 high-level task descriptions randomly sampled from the 214 BigBench tasks (Srivastava et al., 2022). Next, Preprint we employ the 10 seed tasks as in-context examples to prompt LLMs6 to generate 10 new tasks inspired by seed tasks. To encourage additional diversity in the generated tasks, we also let the LLM to brainstorm different genres of puzzles, such as crossword puzzles, math puzzles, number puzzles, relational puzzles, logic puzzles, etc. By repeating two strategies, we produce an expanded pool of 45K candidate reasoning tasks that creatively cover diverse reasoning types and scenarios. \n\nData Filtering and Deduplication. The previous task generation step produces a sizable pool of candidate reasoning tasks. However, the generated data is likely to contain duplicate or highly similar entries. To address this, we employ a comprehensive data filtering and deduplication process. First, we apply n-gram to identify nearly identical tasks. Next, we filter out any tasks or problems that fail to meet our quality criteria, such as insufficient complexity (e.g., trivial one-step questions), or ambiguity in the description by prompting GPT-4-Turbo. This helps ensure that only high-quality, unambiguous reasoning tasks are retained in the final dataset. Through this rigorous deduplication and filtering process, we condense the pool of 45K generated tasks down to 18K deduplicated tasks. \n\nReasoning Problem Synthesis.",
            "score": 0.36498851303176744,
            "section_title": "REASONING PROBLEMS SYNTHESIS",
            "char_start_offset": 23760,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 690
                },
                {
                    "start": 693,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1110
                },
                {
                    "start": 1111,
                    "end": 1328
                },
                {
                    "start": 1329,
                    "end": 1479
                },
                {
                    "start": 1482,
                    "end": 1515
                },
                {
                    "start": 1516,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 1689
                },
                {
                    "start": 1690,
                    "end": 1774
                },
                {
                    "start": 1775,
                    "end": 1833
                },
                {
                    "start": 1834,
                    "end": 2041
                },
                {
                    "start": 2042,
                    "end": 2146
                },
                {
                    "start": 2147,
                    "end": 2281
                },
                {
                    "start": 2284,
                    "end": 2312
                }
            ],
            "ref_mentions": [
                {
                    "start": 261,
                    "end": 280,
                    "matchedPaperCorpusId": "254877310"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.413330078125
        },
        {
            "corpus_id": "273375535",
            "title": "FLARE: Faithful Logic-Aided Reasoning and Exploration",
            "text": "Modern Question Answering (QA) and Reasoning approaches based on Large Language Models (LLMs) commonly use prompting techniques, such as Chain-of-Thought (CoT), assuming the resulting generation will have a more granular exploration and reasoning over the question space and scope. However, such methods struggle with generating outputs that are faithful to the intermediate chain of reasoning produced by the model. On the other end of the spectrum, neuro-symbolic methods such as Faithful CoT (F-CoT) propose to combine LLMs with external symbolic solvers. While such approaches boast a high degree of faithfulness, they usually require a model trained for code generation and struggle with tasks that are ambiguous or hard to formalise strictly. We introduce $\\textbf{F}$aithful $\\textbf{L}$ogic-$\\textbf{A}$ided $\\textbf{R}$easoning and $\\textbf{E}$xploration ($\\textbf{FLARE}$), a novel interpretable approach for traversing the problem space using task decompositions. We use the LLM to plan a solution, soft-formalise the query into facts and predicates using a logic programming code and simulate that code execution using an exhaustive multi-hop search over the defined space. Our method allows us to compute the faithfulness of the reasoning process w.r.t. the generated code and analyse the steps of the multi-hop search without relying on external solvers. Our methods achieve SOTA results on $\\mathbf{7}$ out of $\\mathbf{9}$ diverse reasoning benchmarks. We also show that model faithfulness positively correlates with overall performance and further demonstrate that $\\textbf{FLARE}$ allows pinpointing the decisive factors sufficient for and leading to the correct answer with optimal reasoning during the multi-hop search.",
            "score": 0.3644818997695952,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1693115234375
        },
        {
            "corpus_id": "269294098",
            "title": "Intellecta Cognitiva: A Comprehensive Dataset for Advancing Academic Knowledge and Machine Reasoning",
            "text": "Intellecta arises as a strategic innovation in the realm of linguistic data resources, targeting the deficit in datasets that effectively sharpen the reasoning prowess of language models.Conventional datasets often fall short in nurturing the complex cognitive functions Rane [2023] that advanced language processing demands.To bridge this chasm, Intellecta offers an expansive and multifaceted dataset, curated to foster advanced reasoning capabilities in language models.Mirroring the success of the most accomplished large language models (LLMs) which thrive on synthetic data Li et al. [2022],Intellecta provides a breadth of highquality, synthetic datasets previously inaccessible to the wider research community.\n\nWith an eye on the triumphs of prominent LLMs, Intellecta aims for extensive model generalization, aspiring to surpass existing synthetic data paradigms by controlling the entirety of the data generation process.This initiative yields a reservoir of millions of diverse data samples that span across multiple domains, thus broadening the horizon for LLMs and strengthening their capacity for diverse problem-solving.Integral to Intellecta is the incorporation of advanced synthetic generation techniques, which fabricate a dual-composed content: one that simulates complex thought processes and another that yields textbookstyle elucidations laden with core concepts and pragmatic examples.We took available open source instruction data as the seed data to generate the synthetic data.From the instructions, we first create a textbook style Gunasekar et al. [2023] text explaining the concept required for answering the instruction.The second step is to enrich the response from the seed data with thought process how the model arrive at that result.Combining these 2 will provide a textbook style concept explanation followed by exercises and the thought process to resolve the same.This correlate to the same a student learn a chapter starting with textbook followed by exercises and thought process behind it.This synthesis forges a dataset that not only offers educational profundity but also incisive analytical clarity, catalyzing a leap forward in the domain of language modeling.",
            "score": 0.36357988730454766,
            "section_title": "Introduction",
            "char_start_offset": 1028,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 187,
                    "end": 325
                },
                {
                    "start": 325,
                    "end": 473
                },
                {
                    "start": 473,
                    "end": 718
                },
                {
                    "start": 720,
                    "end": 932
                },
                {
                    "start": 932,
                    "end": 1136
                },
                {
                    "start": 1136,
                    "end": 1410
                },
                {
                    "start": 1410,
                    "end": 1505
                },
                {
                    "start": 1505,
                    "end": 1652
                },
                {
                    "start": 1652,
                    "end": 1770
                },
                {
                    "start": 1770,
                    "end": 1904
                },
                {
                    "start": 1904,
                    "end": 2032
                },
                {
                    "start": 2032,
                    "end": 2207
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92626953125
        },
        {
            "corpus_id": "268297231",
            "title": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM",
            "text": "Generative performance on hard CoT reasoning tasks. We compare different methods with LLaMA3-8B, LLaMA2-13B, and Mistral-7B on three challenging CoT generative tasks: GSM8k, AQuA, and BBH with 8-shot CoT demonstrations. GSM8k (Cobbe et al., 2021) and AQuA (Ling et al., 2017) are widely used math reasoning datasets that test models' ability of arithmetic reasoning. BBH (Suzgun et al., 2022) is a suite of language and symbolic reasoning problems consisting of 6.5k problems within 23 subsets. Given the complexity of these tasks, we use the chain-of-thought prompts created by (Fu et al., 2023) to improve reasoning, which contains 8-shot demonstrations of multi-step reasoning. With the CoT demonstrations, we have the average prefill densely correlated information across multiple reasoning steps and models need to pay close attention across steps to derive answers correctly. Hence, a relatively small compression error can be magnified along generation steps, resulting in significant deviation in model generations. \n\nTable 1 presents experimental results on these hard CoT reasoning tasks. We see that GEAR and GEAR-L achieves better or on par performance compared with baseline methods on all datasets and all models in both 4-bit and 2-bit compression. For example, in the case of 2-bit compression, GEAR achieves 47.83% average accuracy on LLaMA3-8B across three datasets, which is near-lossless compared to FP16 baseline (48.69%) and significantly outperforms the best-performing baseline (28.82%, KIVI). Notably, GEAR-L also establish remarkable performance -near-lossless 4-bit compression and superior 2-bit performance compared to baselines, while demonstrating lower KV size and higher inference efficiency. Meanwhile, as shown in Table 1 and Figrue 2c, regardless quantization backbone we choose, our method can always improve upon them by integrating the error-reduction techniques, showcasing its generalization ability as an efficient error-reduction framework.",
            "score": 0.3627578315319048,
            "section_title": "Main Results",
            "char_start_offset": 17737,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 51
                },
                {
                    "start": 52,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 1023
                },
                {
                    "start": 1026,
                    "end": 1098
                },
                {
                    "start": 1099,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1725
                },
                {
                    "start": 1726,
                    "end": 1983
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6904296875
        },
        {
            "corpus_id": "252222361",
            "title": "Rule-adhering synthetic data - the lingua franca of learning",
            "text": "a fictional record that represents the case of a 38-year old single mother with 3 children, who earns a minimum salary while she pays back an outstanding housing loan of $120k, can easily be reasoned upon with and without an education in statistics.Thus, synthetic data can serve as a lingua franca, as a common language, of learning, for humans as well as for machines alike, as it allows to share the patterns and the diversity of a population via an unlimited number of representative samples, all without infringing anyone's privacy.\n\nA key emerging trend in AI is the marriage of symbolic and subsymbolic AI into a hybrid neurosymbolic AI, that makes best use of both approaches, by learning from existing data as well as from existing domain expertise.It is motivated by the need to become more data efficient, and to generalize well into sparse areas of the data space.In particular, for rare yet high impact cases, like fraud, defects or accidents, practitioners seek to gain more confidence in spotting and understanding these.In a similar vein, we demonstrate with this paper the concept of rule-adhering synthetic data.AIgenerated synthetic data that can both learn from data as well as from rules, and can thus provide insights and confidence with respect to otherwise sparsely populated data regions.We explore both approaches, as well as a combination of these.We do so by extending the underlying model of a commercial solution provider for structured synthetic data 3 .All generated data sets, as well as the corresponding analysis, are made available at https://github.com/mostly-ai/paper-rule-adherence.",
            "score": 0.362586680268735,
            "section_title": "body",
            "char_start_offset": 1,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 249
                },
                {
                    "start": 249,
                    "end": 537
                },
                {
                    "start": 539,
                    "end": 758
                },
                {
                    "start": 758,
                    "end": 876
                },
                {
                    "start": 876,
                    "end": 1036
                },
                {
                    "start": 1036,
                    "end": 1130
                },
                {
                    "start": 1130,
                    "end": 1313
                },
                {
                    "start": 1313,
                    "end": 1375
                },
                {
                    "start": 1375,
                    "end": 1485
                },
                {
                    "start": 1485,
                    "end": 1621
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8544921875
        },
        {
            "corpus_id": "276421602",
            "title": "Boost, Disentangle, and Customize: A Robust System2-to-System1 Pipeline for Code Generation",
            "text": "Large language models show significant intelligence in various domains, striking both the academic and industrial institutions. Despite their prominent problem-solving abilities in system 1 tasks, the mechanism behind the system 2 task solving procedure remain opaque. In this paper, we focus on the code generation task, which emerges as a captivating frontier (Zheng et al., 2023;Roziere et al., 2023;Shen et al., 2023), promising to revolutionize software development by enabling machines to write and optimize code with minimal human intervention. Recent research of llms for code focus on inference-time computation (System 2 methods) (Yang et al., 2024;Yao et al., 2024b;Zhang et al., 2023) and post-training. While during post-training, distilling system 2 knowledge into system 1 backbones is important and widely-used (Yu et al., 2024b). \n\nHowever, the complex hidden reasoning process and the heterogeneous data distribution pose challenges to the existing System2-to-System1 pipeline. On one hand, the hidden reasoning process for code generation is complex and hard to explore (C1). On the other hand, the heterogeneous data distribution, e.g., jumping structure like branching, recursion, etc., makes the existing train-once-for-all strategy hard to fit the complex latent patterns for robust and generalizable llm solvers (C2). \n\nFor (C1), we propose to disentangle the prob-lem solving process into problem2thought and thought2solution stages, exploring the inherent reasoning clues via combining the strengths of multiple llms by mutually-verification and boosting. \n\nThe exploration is integrated into a Monte-Carlo Tree Search process, where reflexion-based pruning and refinement are designed for more efficient and effective reasoning clues search. \n\nFor (C2), we propose to disentangle the heterogeneous data into clusters, finetuning llms capable of different aspects of tasks to obtain the meta LoRA experts hub, and then adaptively generate customized problem solver for each code problem.",
            "score": 0.36240090721896034,
            "section_title": "Introduction",
            "char_start_offset": 1664,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 846
                },
                {
                    "start": 849,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1341
                },
                {
                    "start": 1344,
                    "end": 1581
                },
                {
                    "start": 1584,
                    "end": 1768
                },
                {
                    "start": 1771,
                    "end": 2013
                }
            ],
            "ref_mentions": [
                {
                    "start": 659,
                    "end": 677,
                    "matchedPaperCorpusId": "258762525"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2064208984375
        },
        {
            "corpus_id": "269005050",
            "title": "Exploring Autonomous Agents through the Lens of Large Language Models: A Review",
            "text": "The memory permits an LLM to recall previous interactions.\n\nPrompting is also utilized in common sense reasoning tasks, where the model is anticipated to make inferences or judgments that are evident to humans but may not be explicitly stated [88].For instance, if someone uploads an image of a piece of furniture and provides instructions, the model can utilize these prompts to generate an advertisement for the furniture.Liu et al. [89] explores the application of generated knowledge prompting, which involves generating knowledge from a language model and then providing the knowledge as additional input when answering a question.This method enhances the performance of large-scale models on various commonsense reasoning tasks.\n\nPrompting is a crucial aspect of utilizing Large Language Models effectively.It involves crafting clear and specific instructions or queries to elicit the desired responses from the language model [90].The LLM Retrieval Augmented Generation augments the content produced by LLMs through the addition of relevant material retrieved from external sources.The core strength of RAG lies in its method of data retrieval, providing LLMs with extra context that significantly enriches the content generation process.\n\nConstructive prompt data generation is a method where prompts are meticulously crafted to steer AI models toward generating synthetic data.This artificially created data serves as a valuable resource for training and testing AI models, especially when real-world data for a specific task or domain is scarce.The spectrum of data generation techniques spans from simple methods such as data augmentation, which involves minor alterations to existing data, to more intricate methods like employing generative models to fabricate entirely new data samples.Ensuring Diversity in generated datasets is paramount for the training of robust and generalizable AI models [92].Techniques to enhance diversity encompass:\n\n\u2022 Utilizing various data generation methods like bootstrapping lays the groundwork for generating diverse models via resampling.\n\n\u2022 Incorporating an assortment of data sources could entail collecting data from disparate databases or employing different types of data (e.g., text, images, audio) for a multi-modal approach.\n\n\u2022 Data augmentation can be applied to amplify the size and diversity of an existing training set without manually collecting new data [91].This process fabricates additional training data from existing examples by augmenting them using random transformations that yield plausible images.",
            "score": 0.361990452124373,
            "section_title": "Prompting is all you need",
            "char_start_offset": 45532,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 58
                },
                {
                    "start": 60,
                    "end": 248
                },
                {
                    "start": 248,
                    "end": 424
                },
                {
                    "start": 424,
                    "end": 636
                },
                {
                    "start": 636,
                    "end": 734
                },
                {
                    "start": 736,
                    "end": 813
                },
                {
                    "start": 813,
                    "end": 938
                },
                {
                    "start": 938,
                    "end": 1089
                },
                {
                    "start": 1089,
                    "end": 1245
                },
                {
                    "start": 1247,
                    "end": 1386
                },
                {
                    "start": 1386,
                    "end": 1555
                },
                {
                    "start": 1555,
                    "end": 1800
                },
                {
                    "start": 1800,
                    "end": 1914
                },
                {
                    "start": 1914,
                    "end": 1956
                },
                {
                    "start": 1958,
                    "end": 2086
                },
                {
                    "start": 2088,
                    "end": 2280
                },
                {
                    "start": 2282,
                    "end": 2421
                },
                {
                    "start": 2421,
                    "end": 2569
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.841796875
        },
        {
            "corpus_id": "270766147",
            "title": "FtG-CoT at SemEval-2024 Task 9: Solving Sentence Puzzles Using Fine-Tuned Language Models and Zero-Shot CoT Prompting",
            "text": "Recent large language models (LLMs) can solve puzzles that require creativity and lateral thinking. To advance this front of research, we tackle SemEval-2024 Task 9: BRAINTEASER: A Novel Task Defying Common Sense. We approach this task by introducing a technique that we call Fine-tuned Generated Chain-of-Thought (FtG-CoT). It is a novel few-shot prompting method that combines a fine-tuned BERT classifier encoder with zero-shot chain-of-thought generation and a fine-tuned LLM. The fine-tuned BERT classifier provides a context-rich encoding of each example question and choice list. Zero-shot chain-of-thought generation leverages the benefits of chain-of-thought prompting without requiring manual creation of the reasoning chains. We fine-tune the LLM on the generated chains-of-thought and include a set of generated reasoning chains in the final few-shot LLM prompt to maximize the relevance and correctness of the final generated response. In this paper, we show that FtG-CoT outperforms the zero-shot prompting baseline presented in the task paper and is highly effective at solving challenging sentence puzzles achieving a perfect score on the practice set and a 0.9 score on the evaluation set.",
            "score": 0.36190097369062013,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5263671875
        },
        {
            "corpus_id": "277622127",
            "title": "Synthetic Data Generation & Multi-Step RL for Reasoning & Tool Use",
            "text": "This granular approach enables us to apply direct feedback after each step of the trajectory, and to do so in a manner that is contextually aware. Unlike prior RL finetuning approaches used in frontier open-source models like DeepSeek-R1 (DeepSeek-AI and others, 2025) and Llama-3 (Grattafiori et al., 2024), we do not solely optimize for final performance, and use no golden labels; however, by optimizing for the reasonableness of each step given prior steps, SWiRL does in fact improve final performance. \n\nIn addition to evaluating SWiRL on challenging multi-hop question-answering and mathematical problem-solving tasks, we also study the generalization properties of this methodology. This is of key interest because there is an explosion of agentic applications for language models, and methods that generalize across datasets and tasks will be easier, cheaper and faster to adapt to new environments. We also measure the effectiveness of different synthetic data filtering strategies, study SWiRL's ability to generalize across datasets and tasks, measure the impact of model size and dataset size, and explore the mechanism driving these performance improvements. \n\nOur contributions are as follows: \n\n\u2022 We propose Step-Wise Reinforcement Learning (SWiRL), an approach to synthetic data generation and offline RL that advances multi-step reasoning and tool use.",
            "score": 0.3618909738593167,
            "section_title": "Introduction",
            "char_start_offset": 3505,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 507
                },
                {
                    "start": 510,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 1172
                },
                {
                    "start": 1175,
                    "end": 1208
                },
                {
                    "start": 1211,
                    "end": 1370
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.806640625
        },
        {
            "corpus_id": "271218754",
            "title": "Zero-shot Cross-Lingual Transfer for Synthetic Data Generation in Grammatical Error Detection",
            "text": "As all previous work using AEG for GED has been in monolingual settings, we introduce a monolingual variant of our approach.Here, the GED model is exclusively fine-tuned on synthetic data from the target language.\n\nTable 2 shows that our synthetic data generation technique achieves the best performance among annotation-free synthetic data generation methods applied to GED.Given that rule-based methods apply a set of transformations without considering the sentence context, the average improvement of 9.2 points of F 0.5 over these methods highlights the significance of generating context-dependent errors in synthetic data generation.Additionally, given that NAT is not trained to generate errors but to produce translations, outperforming this method by 8.3 points of F 0.5 highlights the advantage of learning to generate errors from authentic instances, even when these instances originate from different languages.\n\nWe hypothesize that the ability to synthesize context-dependent errors combined with the acquisition of error-generation insights from authentic instances empower our method to yield errors more akin to human errors, thus leading to better performance.We further analyze this hypothesis in 6.1.Additionally, our monolingual setup outperforms DirectCLT in four out of five languages.This is a notable achievement given other synthetic data generation methods' inability to meet this benchmark.Both approaches leverage the CLT of mPLMs, albeit differently: ours uses it for artificial error generation in target languages with a generative mPLM, while DirectCLT leverages it directly to perform error detection across target languages.This comparison suggests that our method creates tailored error patterns in target languages that a GED model trained only on source language annotations cannot detect, indicating that our approach to CLT in GED could generalize to other NLU tasks, which is a promising avenue for future research.Table 3: Comparison of F 0.5 of our method where first-stage fine-tuning is performed on various data configurations.",
            "score": 0.36153413185096533,
            "section_title": "Evaluation of AEG",
            "char_start_offset": 15442,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 124,
                    "end": 213
                },
                {
                    "start": 215,
                    "end": 375
                },
                {
                    "start": 375,
                    "end": 640
                },
                {
                    "start": 640,
                    "end": 924
                },
                {
                    "start": 926,
                    "end": 1178
                },
                {
                    "start": 1178,
                    "end": 1220
                },
                {
                    "start": 1220,
                    "end": 1308
                },
                {
                    "start": 1308,
                    "end": 1418
                },
                {
                    "start": 1418,
                    "end": 1659
                },
                {
                    "start": 1659,
                    "end": 1956
                },
                {
                    "start": 1956,
                    "end": 2073
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4853515625
        },
        {
            "corpus_id": "272421662",
            "title": "A Systematic Review of Synthetic Data Generation Techniques Using Generative AI",
            "text": "While conducting this systematic literature review of synthetic data generation using generative AI, we selected primary papers by searching several scientific databases using specific search prompts. Six prominent scientific databases were included in the search: MDPI, IEEE Xplore, Science Direct, Research Gate, NeurIPS, and Arxiv. These databases were selected to ensure the inclusion of peer-reviewed articles published in reputable journals, conferences, workshops, and symposiums. \n\nTo search the databases, we used the following search strings: \"synthetic data generation using AI\", \"GANs for synthetic data generation\", \"LLMs for synthetic data generation\", and \"different types of VAEs\". Additionally, we crafted composite search strings to broaden our search, such as \"synthetic data\" AND (\"generative AI\" OR \"GANs\" OR \"VAEs\" OR \"LLMs\"), and others, such as \"data generation\" AND (\"generative models\" OR \"neural networks\" OR \"AI\" OR \"deep learning\"). \n\nThis search string selection was based on pilot searches, in which we tested various common terms and acronyms related to synthetic data and generative AI. These pilot searches ensured that our search strings comprehensively captured the relevant literature without missing any key papers that might have used alternative terminologies. This approach allowed us to systematically identify and review studies pertinent to synthetic data generation using generative AI, thereby providing a robust foundation for our literature review.",
            "score": 0.3614349632155474,
            "section_title": "Information Source",
            "char_start_offset": 26709,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 487
                },
                {
                    "start": 490,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 961
                },
                {
                    "start": 964,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1496
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5400390625
        },
        {
            "corpus_id": "257833837",
            "title": "Going Beyond Nouns With Vision & Language Models Using Synthetic Data",
            "text": "We first present our synthetic data generation pipeline (Sec. 3.1), then describe how we leverage it for significant gains in VLC understanding and compositional reasoning capabilities of strong pre-trained VL models (Sec. 3.2). Our entire approach is illustrated in detail in Fig. 2.",
            "score": 0.3614349632155474,
            "section_title": "Method",
            "char_start_offset": 9122,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 61
                },
                {
                    "start": 62,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 284
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8095703125
        },
        {
            "corpus_id": "269626411",
            "title": "Differentially Private Synthetic Data with Private Density Estimation",
            "text": "We now describe several concrete examples of the theory we have developed in this paper.We also provide remarks regarding the practical implementation of the synthetic data generation mechanisms.",
            "score": 0.3614349632155474,
            "section_title": "V. APPLICATIONS",
            "char_start_offset": 11844,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 88,
                    "end": 195
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.328125
        },
        {
            "corpus_id": "273811804",
            "title": "Exploring the Landscape for Generative Sequence Models for Specialized Data Synthesis",
            "text": "To further advance the field of synthetic data generation, several key areas warrant additional exploration and development. One significant avenue is the capability to generate larger and more diverse datasets. Expanding the capacity to synthesize extensive datasets with high variability would greatly enhance the applicability of synthetic data in machine learning tasks, especially in domains where data scarcity remains a challenge. \n\nMoreover, exploring innovative architectures beyond the current models can lead to substantial advancements. Investigating new generative models or enhancing existing ones could improve the quality and diversity of synthetic data. Importantly, demonstrating that these advancements can be achieved using accessible computational resources, such as a personal computer with a well-coded pipeline, would underscore the feasibility of cutting-edge AI developments without the need for extensive infrastructure. This democratization of technology could encourage broader participation in the field and accelerate innovation. \n\nAdditionally, integrating more robust privacy-preserving techniques into the data generation process remains a critical area for future work. As privacy concerns continue to grow, developing methods that ensure data utility while rigorously protecting sensitive information is essential. Combining differential privacy mechanisms with generative models could provide stronger guarantees and expand the adoption of synthetic data in sensitive domains. \n\nFinally, applying synthetic data generation techniques to a wider range of applications, including those with complex data types such as time-series, graphs, and multimodal data, would significantly broaden the impact of this research. Tailoring generative models to handle these complex data structures effectively could open new opportunities in various fields, from healthcare to finance, where such data types are prevalent.",
            "score": 0.3614349632155474,
            "section_title": "X Future Work",
            "char_start_offset": 38230,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 437
                },
                {
                    "start": 440,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1060
                },
                {
                    "start": 1063,
                    "end": 1204
                },
                {
                    "start": 1205,
                    "end": 1350
                },
                {
                    "start": 1351,
                    "end": 1513
                },
                {
                    "start": 1516,
                    "end": 1751
                },
                {
                    "start": 1752,
                    "end": 1944
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.943359375
        },
        {
            "corpus_id": "270560836",
            "title": "Meta Reasoning for Large Language Models",
            "text": "Large language models (LLMs) have shown remarkable capabilities in natural language understanding and generation, making significant strides in various reasoning tasks.However, the diversity and complexity of real-world problems require advanced reasoning methods that surpass the capabilities of a single, static approach.While existing reasoning techniques, such as Chain-of-Thoughts [27], Tree-of-Thoughts [32], Analogical Prompting [33], and Solo Performance Prompting [26], offer valuable tools for enhancing reasoning, they often fall short in consistently achieving state-of-the-art performance across different tasks.\n\nThese challenges highlight the need for a more adaptive and flexible approach to reasoning in LLMs.In human cognition, meta-reasoning involves monitoring and regulating reasoning and problemsolving activities, adjusting strategies based on the context and specific task requirements [5,4].This adaptive capability allows humans to efficiently allocate cognitive resources, balancing trade-offs between accuracy, complexity, and computational cost.Inspired by this, we propose Meta-Reasoning Prompting (MRP) to endow LLMs with similar adaptive reasoning capabilities.\n\nMeta-Reasoning Prompting (MRP) is a simple yet effective system prompt designed to guide LLMs in dynamically selecting and applying the most suitable reasoning method for a specific task.By incorporating meta-reasoning principles, MRP transforms task-specific prompt engineering into a more general and flexible approach.Under the guidance of MRP, the LLM evaluates the task input and selects an appropriate reasoning method from a predefined set (Reasoning Pool).This selection is informed by objective descriptions and evaluations of the available methods.The chosen method is then applied to complete the task, ensuring the model uses the most effective strategy for the given problem.\n\nRecent advances in reasoning techniques, such as those described in [30,24], introduce a meta-buffer for storing high-level thoughts or use ensemble mechanisms to improve model generalizability.While some of these approaches align with the inherent logic of meta-reasoning, our proposed MRP achieves simple and efficient meta-cognitive effects by directly leveraging the meta-reasoning capabilities of LLMs through prompts, without introducing complex mechanisms.\n\nTo evaluate the effectiveness of MRP, we conducted experiments using multiple widely used benchmarks.",
            "score": 0.3613574164671499,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 168,
                    "end": 323
                },
                {
                    "start": 323,
                    "end": 625
                },
                {
                    "start": 627,
                    "end": 726
                },
                {
                    "start": 726,
                    "end": 916
                },
                {
                    "start": 916,
                    "end": 1074
                },
                {
                    "start": 1074,
                    "end": 1193
                },
                {
                    "start": 1195,
                    "end": 1382
                },
                {
                    "start": 1382,
                    "end": 1516
                },
                {
                    "start": 1516,
                    "end": 1659
                },
                {
                    "start": 1659,
                    "end": 1753
                },
                {
                    "start": 1753,
                    "end": 1883
                },
                {
                    "start": 1885,
                    "end": 2079
                },
                {
                    "start": 2079,
                    "end": 2348
                },
                {
                    "start": 2350,
                    "end": 2451
                }
            ],
            "ref_mentions": [
                {
                    "start": 386,
                    "end": 390,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 409,
                    "end": 413,
                    "matchedPaperCorpusId": "258762525"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1806640625
        },
        {
            "corpus_id": "273186721",
            "title": "DOTS: Learning to Reason Dynamically in LLMs via Optimal Reasoning Trajectories Search",
            "text": "Enhancing the capability of large language models (LLMs) in reasoning has gained significant attention in recent years. Previous studies have demonstrated the effectiveness of various prompting strategies in aiding LLMs in reasoning (called\"reasoning actions\"), such as step-by-step thinking, reflecting before answering, solving with programs, and their combinations. However, these approaches often applied static, predefined reasoning actions uniformly to all questions, without considering the specific characteristics of each question or the capability of the task-solving LLM. In this paper, we propose DOTS, an approach enabling LLMs to reason dynamically via optimal reasoning trajectory search, tailored to the specific characteristics of each question and the inherent capability of the task-solving LLM. Our approach involves three key steps: i) defining atomic reasoning action modules that can be composed into various reasoning action trajectories; ii) searching for the optimal action trajectory for each training question through iterative exploration and evaluation for the specific task-solving LLM; and iii) using the collected optimal trajectories to train an LLM to plan for the reasoning trajectories of unseen questions. In particular, we propose two learning paradigms, i.e., fine-tuning an external LLM as a planner to guide the task-solving LLM, or directly fine-tuning the task-solving LLM with an internalized capability for reasoning actions planning. Our experiments across eight reasoning tasks show that our method consistently outperforms static reasoning techniques and the vanilla instruction tuning approach. Further analysis reveals that our method enables LLMs to adjust their computation based on problem complexity, allocating deeper thinking and reasoning to harder problems.",
            "score": 0.3611334019764707,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.418212890625
        },
        {
            "corpus_id": "267750113",
            "title": "Can LLMs Compute with Reasons?",
            "text": "Various methods are employed to improve the mathematical reasoning capabilities of LLMs. One commonly applied method is continual pre-training (Azerbayev et al., 2023). The model is trained on large-scale mathematical datasets, fine-tuning it by continuing the pre-training process. \n\nAnother approach employed is supervised fine-tuning (Yuan et al., 2023;Luo et al., 2023), where high-quality questionanswer pairs are collected through various techniques and then used to fine-tune the model to enhance its performance. Synthetically constructed datasets are often used for this process. These methods use LLMs to generate the data, followed by various augmentation methods (Yuan et al., 2023;Yu et al., 2023;Li et al., 2023a) to filter the data. \n\nVarious reasoning frameworks are also implemented to bring out the best reasoning answers. This includes promptingbased (Wei et al., 2022;Chen et al., 2023) and selfconsistency (Wang et al., 2023) methods where the model uses majority voting to decide among various rational paths. \n\nThe most commonly used approaches are Chain-of-Thought reasoning (Wei et al., 2022;Nye et al., 2021) and Programof-Thought reasoning (Chen et al., 2023). Over the recent past, new methods like Equation-of-Thought Distillation (Zhu et al., 2024), which work on a similar principle, have also emerged. \n\nThere is also the emergence of Program-aided Language Model (PAL) (Gao et al., 2023), where code is generated from LLMs, which is later passed through an external API to generate the final output. A similar approach also uses symbolic solvers to implement the same (He-Yueya et al., 2023).",
            "score": 0.3610178188974883,
            "section_title": "Improving Mathematical Reasoning of LLMs",
            "char_start_offset": 1381,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 282
                },
                {
                    "start": 285,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 747
                },
                {
                    "start": 750,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 1031
                },
                {
                    "start": 1034,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1333
                },
                {
                    "start": 1336,
                    "end": 1532
                },
                {
                    "start": 1533,
                    "end": 1625
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70849609375
        },
        {
            "corpus_id": "277856876",
            "title": "Enhancing the Geometric Problem-Solving Ability of Multimodal LLMs via Symbolic-Neural Integration",
            "text": "Recent advances in Multimodal Large Language Models (MLLMs) have achieved remarkable progress in general domains and demonstrated promise in multimodal mathematical reasoning. However, applying MLLMs to geometry problem solving (GPS) remains challenging due to lack of accurate step-by-step solution data and severe hallucinations during reasoning. In this paper, we propose GeoGen, a pipeline that can automatically generates step-wise reasoning paths for geometry diagrams. By leveraging the precise symbolic reasoning, GeoGen produces large-scale, high-quality questionanswer pairs. To further enhance the logical reasoning ability of MLLMs, we train GeoLogic, a Large Language Model (LLM) using synthetic data generated by GeoGen. Serving as a bridge between natural language and symbolic systems, GeoLogic enables symbolic tools to help verifying MLLM outputs, making the reasoning process more rigorous and alleviating hallucinations. Experimental results show that our approach consistently improves the performance of MLLMs, achieving remarkable results on benchmarks for geometric reasoning tasks. This improvement stems from our",
            "score": 0.3609422784586038,
            "section_title": "ABSTRACT",
            "char_start_offset": 11,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1138
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8134765625
        },
        {
            "corpus_id": "267751185",
            "title": "Plato: Plan to Efficiently Decode for Large Language Model Inference",
            "text": "To better exploit the reasoning ability of LLMs, many methods have been proposed to construct more informative and helpful prompts for LLMs to solve complex problems. Chain-of-Thought (CoT) (Wei et al., 2022;Wang et al., 2023) introduces intermediate steps between inputs and outputs as demonstrations in prompts, improving generation quality on complex tasks. Tree-of-Thought (ToT) (Yao et al., 2023) advances LLMs' reasoning capabilities by representing the reasoning process as a tree, with nodes representing different solutions. Graph-of-Thought (GoT) (Besta et al., 2023) extends ToT structure by considering a graph-based approach, introducing aggregation operations on different sub-solutions to further improve reasoning. These methods focus on improving response quality for complex prompts, but ignore generation efficiency; in contrast, our work focus on improving the generation efficiency while not sacrificing quality by only parallelizing independent subtasks by only parallelizing independent subtasks.",
            "score": 0.3607950521996192,
            "section_title": "Prompting for Higher Quality",
            "char_start_offset": 5929,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 1019
                }
            ],
            "ref_mentions": [
                {
                    "start": 190,
                    "end": 208,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 208,
                    "end": 226,
                    "matchedPaperCorpusId": "247595263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.260986328125
        },
        {
            "corpus_id": "272753174",
            "title": "LogicPro: Improving Complex Logical Reasoning via Program-Guided Learning",
            "text": "Table 1 shows the main results where LogicPro outperforms previous synthesis methods across multiple benchmarks. On the representative Big Bench Hard benchmark, LogicPro improves the average performance by 2.3% -4.7% compared to the previous best baseline across different model type and scales, and shows at least 10% improvement over general baseline data. On LogicBench (a benchmark measuring propositional logic capabilities), LogicPro achieves the best performance except when LogicBench itself is used as the training set. On FOLIO (a benchmark measuring firstorder logic capabilities), except for Qwen2-7B, the performance of the other three models trained with LogicPro is inferior to other baseline data. This may be because some baseline synthetic data is essentially generated based on first-order logic, making their data distribution closer to FOLIO, leading to better performance. On GSM8K (mathematical reasoning benchmark), different synthetic data has relatively minor impact on model performance. On OOD benchmarks such as DROP (reading comprehension reasoning) and ARLSAT (law school admission test reasoning), LogicPro also demonstrates advantages across multiple model founda-tions, further validating its performance on out-ofdistribution tasks. The weighted average results across all benchmarks show that on the Qwen2 foundation, 7B and 72B models improve by 3.9%-8% and 3.1%-9.1% respectively compared to baselines; on the Llama3 foundation, 8B and 70B models improve by 2.5%-8.8% and 1.6%-9.7% respectively compared to baselines. Additionally, for different scale models, we find that our LogicPro still shows advantages on largescale models. This indicates that although largescale models possess stronger reasoning capabilities, our data still provides important value for improving their performance. It is speculated that this may be related to the high difficulty level of LogicPro itself, with detailed analysis in section 4.4. As shown in Table 2, we conduct ablation studies to validate LogicPro's effectiveness in synthesizing questions and generating high-quality reasoning processes. First, we compare Source_LeetCode with LogicPro w/o Inter-Var to analyze the effect on synthesizing text reasoning questions.",
            "score": 0.3607726157741914,
            "section_title": "Main Results",
            "char_start_offset": 16524,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1267
                },
                {
                    "start": 1268,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1519
                },
                {
                    "start": 1520,
                    "end": 1555
                },
                {
                    "start": 1556,
                    "end": 1668
                },
                {
                    "start": 1669,
                    "end": 1829
                },
                {
                    "start": 1830,
                    "end": 1959
                },
                {
                    "start": 1960,
                    "end": 2120
                },
                {
                    "start": 2121,
                    "end": 2246
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.438232421875
        },
        {
            "corpus_id": "271600915",
            "title": "ABC Align: Large Language Model Alignment for Safety & Accuracy",
            "text": "The utilisation of 'synthetic data' (Nikolenko, 2021) represents a novel source of data for training LLMs. It is often cited in wider discussions about the finite quantity of human-created data (Villalobos et al., 2024). As with human data, not all synthetic data are created equal. Indeed, lowquality synthetic data, when riddled with hallucinations, inaccuracies, and biases, can significantly impact downstream task performance (van Breugel et al., 2023). \n\nTo improve the quality of our synthetic data and mitigate these issues, we condition each sample on organisational data: news articles. Beyond the content itself, these articles comply with the ABC's editorial standards and policies (Australian Broadcasting Corporation, 2024b) at the time of their publication, and provide a real-world example of 'human annotated data' that goes well beyond crowd-sourced methods such as or 'Mechanical Turk' or similar. We also go beyond algorithms like Self-Instruct (Wang et al., 2023) that use 'seed instructions' to generate synthetic data, by providing a unique input for each generated sample. \n\nKnowledge distillation techniques have diversified well beyond the original work of Hinton et al. (2015). In the context of this paper, the teacher/student dynamic remains, but occurs between frontier model/smaller open-source model. The distillation occurs per-input, given a prompt. The structure of this prompt confines the possible sub-distribution of the synthetic data used by the student model in our fine-tuning process. Here we build on the work of Mukherjee et al. (2023) on ORCA, where they elicit 'reasoning traces' from OpenAI's GPT-4 based on synthetic samples, with the goal of improving specific capabilities of a smaller 13B parameter model. \n\nWe note here that our synthetic data samples are created for the specific purpose of improving underlying model capability rather than meeting editorial standards, following the methods outlined in ORCA. We do not intend on making this data publicly available. This is to avoid any confusion with the published content it is derived from.",
            "score": 0.3604767879316045,
            "section_title": "Synthetic Data and Knowledge Distillation: ORCA",
            "char_start_offset": 10902,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 458
                },
                {
                    "start": 461,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1096
                },
                {
                    "start": 1099,
                    "end": 1204
                },
                {
                    "start": 1205,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1383
                },
                {
                    "start": 1384,
                    "end": 1527
                },
                {
                    "start": 1528,
                    "end": 1757
                },
                {
                    "start": 1760,
                    "end": 1963
                },
                {
                    "start": 1964,
                    "end": 2020
                },
                {
                    "start": 2021,
                    "end": 2098
                }
            ],
            "ref_mentions": [
                {
                    "start": 1183,
                    "end": 1203,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67236328125
        },
        {
            "corpus_id": "270870231",
            "title": "Combinatorial Reasoning: Selecting Reasons in Generative AI Pipelines via Combinatorial Optimization",
            "text": "Our research proposes the integration of an external reasoning engine that interfaces with existing LLM pipelines to fully automate the creation of CoT style prompts.As the reasoning engine sits outside the LLM black box, our work is not an attempt to change the foundational auto-regressive architecture of LLMs but a proposed tool to analyze and possibly augment their reasoning faculties through automated prompt engineering.By employing combinatorial optimization, the engine generates structured prompts that guide the LLM towards the correct answer.Our work intersects two distinct fields -generative AI and probabilistic combinatorial optimization -to tackle human level reasoning tasks.We construct a first of a kind LLM pipeline with physicsinspired solvers and benchmark the pipeline across a variety of well known Natural Language Processing (NLP) reasoning benchmarks.\n\nIn the following sections, we review the state of art and then present our Combinatorial Reasoning (CR) framework as a versatile technique that leverages a probabilistic combinatorial optimizer to construct a Chain-of-Though style prompt with no human intervention.Our proof-ofconcept results demonstrate that in some cases CR achieves improvements over other zero-shot prompting strategies on a few reasoning tasks from BigBench-Hard, and human-level reasoning performance on several reasoning tasks.\n\nIn the conclusions, we will discuss the lessons learned, how the CR framework can be further optimized beyond our preliminary baseline experiments and outline multiple promising research avenues.",
            "score": 0.36043227435894093,
            "section_title": "Introduction",
            "char_start_offset": 2209,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 166,
                    "end": 428
                },
                {
                    "start": 428,
                    "end": 555
                },
                {
                    "start": 555,
                    "end": 694
                },
                {
                    "start": 694,
                    "end": 880
                },
                {
                    "start": 882,
                    "end": 1147
                },
                {
                    "start": 1147,
                    "end": 1383
                },
                {
                    "start": 1385,
                    "end": 1580
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.385986328125
        },
        {
            "corpus_id": "261681733",
            "title": "A supervised generative optimization approach for tabular data",
            "text": "Synthetic data generation has emerged as a crucial topic for financial institutions, driven by multiple factors, such as privacy protection and data augmentation. Many algorithms have been proposed for synthetic data generation but reaching the consensus on which method we should use for the specific data sets and use cases remains challenging. Moreover, the majority of existing approaches are \u201cunsupervised\u201d in the sense that they do not take into account the downstream task. To address these issues, this work presents a novel synthetic data generation framework. The framework integrates a supervised component tailored to the specific downstream task and employs a meta-learning approach to learn the optimal mixture distribution of existing synthetic distributions.",
            "score": 0.36037863272895426,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80712890625
        },
        {
            "corpus_id": "259137462",
            "title": "Recursion of Thought: A Divide-and-Conquer Approach to Multi-Context Reasoning with Language Models",
            "text": "Recently, LMs have become a prominent direction to solve reasoning. Given a question sequence, the models are tasked to predict the following answer sequence. One recent line of research for reasoning with LMs is chain of thought (CoT) generation (Nye et al., 2021;Wei et al., 2022;Kojima et al., 2022). In CoT generation, complex reasoning problems are solved by generating intermediate reasoning steps, or chain of thought, before producing the final answer. This allows the problem's complexity to be spread across multiple token generations, making each generation more straightforward given the previous tokens. \n\nAlthough CoT dramatically increases reasoning accuracy, there is a critical issue that limits its utility: the effective context size of sequence models cannot grow unbounded. Context refers to the set of input tokens that a model is conditioned on when generating output. Practically, all sequence models have a limit on the maximum context length due to various reasons. For instance, Transformers (Vaswani et al., 2017) suffer from a quadratic computational cost on the context length, and RNNs (Hochreiter and Schmidhuber, 1997) struggle with long-term dependency modeling. Therefore, even the state-of-the-art LMs limit the maximum context length to a few thousand tokens. However, complex real-world problems may take even millions of tokens of reasoning steps to reach the answer. \n\nWhile there has been extensive research on Transformers with longer contexts (Tay et al., 2020b), we explore an orthogonal direction: divide and conquer. Our new model-agnostic inference framework Recursion of Thought (RoT) lets an LM recursively create multiple contexts by producing special tokens. Therefore, even if a problem's solution exceeds the maximum context size, the model can divide it into multiple short contexts. We show the potential of RoT with our new synthetic benchmark consisting of eight arithmetic and algorithmic tasks. One can easily adjust the difficulty of the tasks to produce problems with extremely long (100K+ tokens) reasoning steps. Without any taskspecific component, such as a calculator, the models with RoT can easily learn to solve extremely complex problems whose solutions consist of hundreds of thousands of tokens.",
            "score": 0.36005871899762754,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 67
                },
                {
                    "start": 68,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 616
                },
                {
                    "start": 619,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1196
                },
                {
                    "start": 1197,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1406
                },
                {
                    "start": 1409,
                    "end": 1562
                },
                {
                    "start": 1563,
                    "end": 1709
                },
                {
                    "start": 1710,
                    "end": 1837
                },
                {
                    "start": 1838,
                    "end": 1953
                },
                {
                    "start": 1954,
                    "end": 2075
                },
                {
                    "start": 2076,
                    "end": 2266
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6796875
        },
        {
            "corpus_id": "271064536",
            "title": "Prompting Techniques for Secure Code Generation: A Systematic Investigation",
            "text": "This example is directly taken from the original paper itself.Chain-of-Thought (CoT): In this prompting approach [103], the LLM is compelled to produce a sequence of intermediary logical reasoning steps in natural language, culminating in the solution to the presented problem.The goal of this approach is to replicate how humans solve a complex problem following a chain of reasoning or justification steps.In this method, the model is initially given a set of few-shot examples, consisting of <input, chain of thought, output> triplets, to guide its understanding before it tackles the actual task.This technique has been evaluated on various benchmarks including arithmetic, common sense, and symbolic reasoning.However, one can assume that CoT can also be applied to code generation tasks.Figure 7 demonstrates the CoT prompting technique for code generation.\n\nAn approach similar to this was proposed in 2017 by Ling et al. [59] where they train an attention-based sequence-tosequence model to solve complex mathematical problems using a dataset containing problems with answer rationales and the final correct answers.However, this approach focused on training rather than explicitly prompting a model, and it did not involve an LLM.Hence we identify CoT as a novel prompting technique.\n\nManuscript submitted to ACM  the most consistent response among the various final answers generated from these diverse reasoning chains.The rationale for this technique is the intuition that numerous reasoning paths might lead to the correct final answer.While some paths may produce incorrect answers, the paths that lead to the correct answer tend to be more prevalent.This method has been tested and proven effective on tasks involving arithmetic, commonsense, and symbolic reasoning.\n\nComplexity-based prompting also adopts a similar approach but posits that chains involving more reasoning steps yield better performance.Consequently, this technique emphasizes using chain-of-thought few-shot examples comprising a greater number of reasoning steps (i.e., more complexity).Furthermore, the final answer is chosen based on the consistency among responses with a larger number of reasoning steps, while responses with fewer reasoning steps are discarded.\n\nBoth techniques are particularly well-suited for tasks that have a definitive final answer, as opposed to more creative tasks like code generation.However, they can still be applied to code generation tasks.A demonstration of adapting self-consistency for code generation is included in Figure 8.",
            "score": 0.3600339243000003,
            "section_title": "Decomposition-based Techniques.",
            "char_start_offset": 30209,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 62
                },
                {
                    "start": 62,
                    "end": 277
                },
                {
                    "start": 277,
                    "end": 408
                },
                {
                    "start": 408,
                    "end": 600
                },
                {
                    "start": 600,
                    "end": 715
                },
                {
                    "start": 715,
                    "end": 793
                },
                {
                    "start": 793,
                    "end": 863
                },
                {
                    "start": 865,
                    "end": 1124
                },
                {
                    "start": 1124,
                    "end": 1239
                },
                {
                    "start": 1239,
                    "end": 1292
                },
                {
                    "start": 1294,
                    "end": 1430
                },
                {
                    "start": 1430,
                    "end": 1549
                },
                {
                    "start": 1549,
                    "end": 1665
                },
                {
                    "start": 1665,
                    "end": 1781
                },
                {
                    "start": 1783,
                    "end": 1920
                },
                {
                    "start": 1920,
                    "end": 2072
                },
                {
                    "start": 2072,
                    "end": 2251
                },
                {
                    "start": 2253,
                    "end": 2400
                },
                {
                    "start": 2400,
                    "end": 2460
                },
                {
                    "start": 2460,
                    "end": 2549
                }
            ],
            "ref_mentions": [
                {
                    "start": 113,
                    "end": 118,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 929,
                    "end": 933,
                    "matchedPaperCorpusId": "12777818"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.14013671875
        },
        {
            "corpus_id": "267411737",
            "title": "Distilling LLMs' Decomposition Abilities into Compact Language Models",
            "text": "A pivotal area for future exploration involves advancing offline RL or other suitable methodologies for distilling reasoning abilities from static datasets. This extension could contribute to more effective utilization of language models in reasoning tasks. \n\nCreation of a Larger Benchmark: Expanding our methodology, future work could focus on generating a more extensive benchmark as it requires only the access to ground truth-answers in the datset which usualy holds. This benchmark might incorporate a diverse set of reasoning datasets, such as MATH (Hendrycks et al., 2021) or AQuA (Ling et al., 2017), providing a broader assessment of reasoning capabilities. \n\nConcentration on Sub-Question Answering: Delving deeper into the sub-question answering aspect of the reasoning process presents a promising direction. While our dataset includes ChatGPT responses for sub-questions, their scoring and utilization remain unexplored. Future studies could investigate this component to enhance understanding and performance. \n\nUtilization of Open-Source Models: Exploring the application of open-source models, such as LLaMA, for sub-question generation emerges as a cost-effective alternative. Accessible without financial constraints, these models present an opportunity for researchers to delve into sub-question generation without monetary limitations. We were not able to run such kind of experiments ourselves due to the computational limitations.",
            "score": 0.359841691252682,
            "section_title": "Development of Offline RL Approaches:",
            "char_start_offset": 23949,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 257
                },
                {
                    "start": 260,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 667
                },
                {
                    "start": 670,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1024
                },
                {
                    "start": 1027,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1453
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1685791015625
        },
        {
            "corpus_id": "273374871",
            "title": "CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning",
            "text": "High-quality large-scale datasets have driven the success of large foundational AI models (Radford et al., 2021;Rombach et al., 2022;Touvron et al., 2023). Collecting and annotating datasets at large-scale is challenging and costly. One solution is to crawl data from the web; however, web data is noisy (Lai et al., 2024;Kang et al., 2023), has long-tail distributions (Udandarao et al., 2024), and often causes privacy or copyright issues (Schuhmann et al., 2022). Synthetic data presents a viable and complementary alternative to overcome these challenges, as it allows for precise control over data generation and customization to meet specific requirements. A large body of work has focused on improving the quality of synthetic data for image and text data, from the generation of high-quality images (Dunlap et al., 2023;Islam et al., 2024) to the improvement of synthetic captions (Lai et al., 2024;Fan et al., 2023). While these works have shown that synthetic data successfully improves model performance for various vision or vision-language tasks, their synthetic pipeline is often ad hoc and tailored to specific purposes such as training better CLIP models or improving domain-specific vision models (e.g., DiffuseMix uses diffusion models to augment images and improves accuracy on image classification tasks Islam et al., 2024). These data synthesis works also lack explicit fine-grained control over the generated texts or images, which are important for tasks with long-tail distribution (e.g., augmenting tail class samples) or enforcing safety requirements (e.g., mitigating biased or sensitive content generation Schramowski et al., 2023). \n\nIn this work, we aim to systematically control the synthetic pipeline for generating image-text data while accommodating different use cases (e.g., improving long-tail task performance, enhancing compositional reasoning of CLIP models, etc.).",
            "score": 0.35978191581424696,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 466
                },
                {
                    "start": 467,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1660
                },
                {
                    "start": 1663,
                    "end": 1905
                }
            ],
            "ref_mentions": [
                {
                    "start": 90,
                    "end": 112,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 112,
                    "end": 133,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 322,
                    "end": 340,
                    "matchedPaperCorpusId": "255186162"
                },
                {
                    "start": 441,
                    "end": 465,
                    "matchedPaperCorpusId": "252917726"
                },
                {
                    "start": 807,
                    "end": 828,
                    "matchedPaperCorpusId": "258887976"
                },
                {
                    "start": 828,
                    "end": 847,
                    "matchedPaperCorpusId": "270045834"
                },
                {
                    "start": 1324,
                    "end": 1343,
                    "matchedPaperCorpusId": "270045834"
                },
                {
                    "start": 1634,
                    "end": 1659,
                    "matchedPaperCorpusId": "253420366"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93359375
        },
        {
            "corpus_id": "254877678",
            "title": "Are Deep Neural Networks SMARTer Than Second Graders?",
            "text": "To set the stage, we briefly review below a few prior methods and datasets proposed towards understanding the reasoning abilities of deep neural networks. Solving IQ puzzles: via creating computer programs has been a dream since the early days of exploration into AI [28,43,44]; Evan's ANALOGY [19] and Hofstader's CopyCat, among others [30] are famous tasks in this direction. With the resurgence of deep learning, there have been several attempts at re-considering such puzzles, with varied success. In Table 1, we briefly review such tasks and datasets (see Ma\u0142ki\u0144ski and Ma\u0144dziuk [42] for an indepth survey). While, the goal of these works have been Dataset Involve language Dataset size Task nature Bongard-LOGO [47] 12K few-shot concepts, abstract shape reasoning Bongard-HOI [33] 53K few-shot concepts, human-object interaction ARC [13] 800 generate image based on abstract rules Machine Number Sense [74] 280K solving arithmetic problems RAVEN [72] 70K finding next image in sequence Image riddles [4] (fixed question) 3333 finding common linguistic descriptions VLQA [57] (variable questions) 9267 spatio-temporal reasoning, info lookup, mathematical, logical, causality, analogy, etc. PororoQA [36] (variable questions) 8913 reason from cartoon videos about action, person, abstract, detail, location, etc. CLEVR [34] (variable questions) 100K exist, count, query attributes, compare integers/attribute SMART-101 (ours) (variable questions) 200K 8 predominant algorithmic skills and their compositions (see Figure 2) Table 1. Comparison between our SMART-101 dataset with existing datasets related to visual reasoning.\n\ntowards capturing human cognition through machine learning models, their tasks are often specialized and when provided enough data, the neural networks apparently leverage shortcomings in the dataset towards achieving very high accuracy [28,64,73], defaulting the original goals. Neuro-symbolic learning and program synthesis: approaches consider solving complex tasks via decomposing a scene into entities and synthesizing computer programs that operate on these entities; thereby plausibly emulating human reasoning. The DreamCoder approach [",
            "score": 0.3597430267893754,
            "section_title": "Related works",
            "char_start_offset": 7091,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 267,
                    "end": 271,
                    "matchedPaperCorpusId": "37594932"
                },
                {
                    "start": 717,
                    "end": 721,
                    "matchedPaperCorpusId": "222125270"
                },
                {
                    "start": 782,
                    "end": 786,
                    "matchedPaperCorpusId": "247973017"
                },
                {
                    "start": 908,
                    "end": 912,
                    "matchedPaperCorpusId": "208281592"
                },
                {
                    "start": 952,
                    "end": 956,
                    "matchedPaperCorpusId": "71148268"
                },
                {
                    "start": 1006,
                    "end": 1009,
                    "matchedPaperCorpusId": "54055406"
                },
                {
                    "start": 1076,
                    "end": 1080,
                    "matchedPaperCorpusId": "222225265"
                },
                {
                    "start": 1204,
                    "end": 1208,
                    "matchedPaperCorpusId": "9096634"
                },
                {
                    "start": 1323,
                    "end": 1327,
                    "matchedPaperCorpusId": "15458100"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03436279296875
        },
        {
            "corpus_id": "270371434",
            "title": "TabPFGen - Tabular Data Generation with TabPFN",
            "text": "In particular, given a synthetic label y synth 2 {1, . . ., K}, we seek fine a generative model p(x synth | y synth ) which can synthesize new samples x synth 2 R D while taining a link to the classification task solved by TabPFN.To this end, we first note that TabPFN many classification models -induces a conditional distribution p(y | x) := exp(f (x)[y]) P y 0 exp(f (x)[y 0 ]) \u2318 x))[y], where x is the network input, f : R D !R K represents the TabPFN, : R K !R K is oftmax, and [y] denotes an indexing operation.\n\n, recalling Bayes' rule, we have p(x | y) / p(y | x) \u2022 p(x).We thus only need to specify a inal distribution in x to fully specify the desired conditional.Taking inspiration from Grathwohl 2 a massive number of possible inductive biases which may be observed ning, the learned TabPFN model can be used to generate a posterior labels y test given test features x test , along with training labels y train and use a frozen pre-trained TabPFN and choose synthetic labels y synth to , given available training labels y train and features x train .\n\ng for tabular data has witnessed significant advancements.Initially, , 14, 30] dominated, followed by diffusion models [45,22] and large 4].However, surprisingly, the simplest interpolation methods such [28] still prove to be very competitive [6,27].We conjecture that tioned generative techniques may not have adequately captured the iscriminative approaches.\n\ndiscriminators for generative tasks has expanded drastically over the tabular domain.Early work by Tu [37] showed promising results recent investigations have further demonstrated efficacy in image illame-Bert [30] also shed light on this strategy for tabular data using roach.\n\nhave also gained significant traction across machine learning domains.ut-of-distribution detection, while Florence et al. [15] applied them to work extends the research of using EBMs for data generation tasks.",
            "score": 0.3596990322037983,
            "section_title": "Introduction",
            "char_start_offset": 3820,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 58
                },
                {
                    "start": 58,
                    "end": 230
                },
                {
                    "start": 230,
                    "end": 430
                },
                {
                    "start": 430,
                    "end": 464
                },
                {
                    "start": 464,
                    "end": 517
                },
                {
                    "start": 519,
                    "end": 579
                },
                {
                    "start": 579,
                    "end": 674
                },
                {
                    "start": 674,
                    "end": 1062
                },
                {
                    "start": 1064,
                    "end": 1122
                },
                {
                    "start": 1122,
                    "end": 1204
                },
                {
                    "start": 1204,
                    "end": 1314
                },
                {
                    "start": 1314,
                    "end": 1424
                },
                {
                    "start": 1426,
                    "end": 1511
                },
                {
                    "start": 1511,
                    "end": 1703
                },
                {
                    "start": 1705,
                    "end": 1775
                },
                {
                    "start": 1775,
                    "end": 1914
                }
            ],
            "ref_mentions": [
                {
                    "start": 1267,
                    "end": 1271,
                    "matchedPaperCorpusId": "246294883"
                },
                {
                    "start": 1307,
                    "end": 1310,
                    "matchedPaperCorpusId": "52889459"
                },
                {
                    "start": 1310,
                    "end": 1313,
                    "matchedPaperCorpusId": "245334722"
                },
                {
                    "start": 1528,
                    "end": 1532,
                    "matchedPaperCorpusId": "220403493"
                },
                {
                    "start": 1636,
                    "end": 1640,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4296875
        },
        {
            "corpus_id": "273821071",
            "title": "Seq-VCR: Preventing Collapse in Intermediate Transformer Representations for Enhanced Reasoning",
            "text": "Transformer Reasoning Research in large language models (LLMs) has advanced significantly. Models like BERT (Devlin et al., 2019) and GPT (Radford & Narasimhan, 2018) demonstrated improvements in natural language understanding. However, challenges remain in tasks that require deep cognitive abilities. Integrating external knowledge has shown promise in improving reasoning capabilities (Bosselut et al., 2019). Recent advances include techniques such as chain-of-thought (CoT) prompting, which allow models such as GPT-3 (Brown et al., 2020) to perform complex reasoning by breaking tasks into intermediate steps (Wei et al., 2022) with human supervision, called process supervision (Lightman et al., 2023). Wang et al. (2022) shows that even incorrect but coherent intermediate steps can improve reasoning performance. Jin et al. (2024) found increasing the length of the reasoning steps in the prompts, even without adding new information to the prompt, improves the reasoning abilities of LLMs. Wang et al. (2024), investigated the addition of dummy tokens such as periods or hash symbols to inputs at inference time. They found that this simple modification could improve performance in arithmetic reasoning tasks. Some works show that these dummy tokens, commonly referred to as filler tokens, do not extend transformers' abilities beyond TC 0 circuit complexity, but still significantly enhance problem-solving within this class (Merrill & Sabharwal, 2023;Strobl et al., 2023). Building on this idea, Goyal et al. (2023a) introduced a more comprehensive framework called \"pause-training.\" Their approach involves incorporating learnable < pause > tokens during both pre-training and fine-tuning of language models. While there are differences between the pause tokens and filler tokens approach, they share the goal of providing additional computation time for transformers which extends the expressive power of transformers within the TC 0 class without changing the fundamental limitations of the model architecture. Chain-of-thought prompting can potentially elevate transformers beyond the TC 0 complexity class; However, it may not be necessary for all problems.",
            "score": 0.3589155204879016,
            "section_title": "LITERATURE REVIEW",
            "char_start_offset": 3871,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 91,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 999
                },
                {
                    "start": 1000,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1485
                },
                {
                    "start": 1486,
                    "end": 1596
                },
                {
                    "start": 1597,
                    "end": 1722
                },
                {
                    "start": 1723,
                    "end": 2026
                },
                {
                    "start": 2027,
                    "end": 2175
                }
            ],
            "ref_mentions": [
                {
                    "start": 108,
                    "end": 129,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 388,
                    "end": 411,
                    "matchedPaperCorpusId": "189762527"
                },
                {
                    "start": 1000,
                    "end": 1018,
                    "matchedPaperCorpusId": "259137816"
                },
                {
                    "start": 1437,
                    "end": 1464,
                    "matchedPaperCorpusId": "257050206"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60107421875
        },
        {
            "corpus_id": "268234574",
            "title": "World-Map Misalignment Detection for Visual Navigation Systems",
            "text": "The use of synthetic data to train machine learning models has become increasingly popular, particularly for time-consuming tasks such as the ones requiring semantic segmentation (Saleh et al., 2018).With a synthetic data generation pipeline, it is possible to obtain a large amount of data with minimal cost and time investment.Since the data is generated from a 3D model, the labels can be easily obtained in simulation using automated methods.The data generation technique also allows the obtainment of a controlled setup in which the variability in the data is decided a priori.Synthetic data acquisition via specially built applications, like simulators, has been widely used in different computer vision studies, for instance in egocentric vision (Leonardi et al., 2022;Quattrocchi et al., 2022), crowd counting (Wu et al., 2022), human pose estimation (Ebadi et al., 2022), and localization (Andrea Orlando et al., 2020).\n\nAddressing the gap between real and synthetic data is a significant challenge in data generation (Sankaranarayanan et al., 2018).Models often learn representations in the virtual domain that are hard to generalize to real-world scenarios (Lee et al., 2022).To smooth out the differences and learn a better representation, different approaches have been explored.Domain Randomization (Tobin et al., 2017) techniques are applied as a way to reduce the network bias induced by repetitive synthetic data.Randomizations such as light changes, texture and colour changes, and random camera rotation have proven effective in guiding models into learning robust features related to the task, reducing the domain gap (Frid-Adar et al., 2018).\n\nFollowing previous literature, we use synthetic data to tackle the proposed word-map misalignment detection task.To make the acquired frames more similar to real ones we utilize a high definition render pipeline (HDRP).We also introduce several variations of lightning, agent height and speed, navigation path of the agent, and simulate ambulatory walking.",
            "score": 0.358096997168968,
            "section_title": "Synthetic Data Generation",
            "char_start_offset": 9317,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 200,
                    "end": 329
                },
                {
                    "start": 329,
                    "end": 446
                },
                {
                    "start": 446,
                    "end": 582
                },
                {
                    "start": 582,
                    "end": 928
                },
                {
                    "start": 930,
                    "end": 1059
                },
                {
                    "start": 1059,
                    "end": 1187
                },
                {
                    "start": 1187,
                    "end": 1292
                },
                {
                    "start": 1292,
                    "end": 1430
                },
                {
                    "start": 1430,
                    "end": 1663
                },
                {
                    "start": 1665,
                    "end": 1778
                },
                {
                    "start": 1778,
                    "end": 1884
                },
                {
                    "start": 1884,
                    "end": 2021
                }
            ],
            "ref_mentions": [
                {
                    "start": 179,
                    "end": 199,
                    "matchedPaperCorpusId": "49863467"
                },
                {
                    "start": 906,
                    "end": 927,
                    "matchedPaperCorpusId": "213509214"
                },
                {
                    "start": 1027,
                    "end": 1058,
                    "matchedPaperCorpusId": "4540721"
                },
                {
                    "start": 1313,
                    "end": 1333,
                    "matchedPaperCorpusId": "2413610"
                },
                {
                    "start": 1638,
                    "end": 1662,
                    "matchedPaperCorpusId": "28111473"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76806640625
        },
        {
            "corpus_id": "272367350",
            "title": "CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation",
            "text": "Synthetic data refers to artificially generated data that mimics the characteristics of real-world data (Little et al., 1993). It can be generated using statistical (Sue, 1987;Maqsud, 2015) or deep neural approaches (Sutskever et al., 2011) with the aim of replicating the patterns, distributions, and structures found in real-world datasets. \n\nFully Synthetic Data Generation: A dataset is fully synthetic if the question or instruc-tion, the possible context, as well as the answers are generated synthetically. For instance, Self-Instruct (Wang et al., 2023b), Unnatural Instructions (Honovich et al., 2023), Alpaca (Taori et al., 2023), and Evol-Instruct (Xu et al., 2023) are examples of fully synthetic general-purpose data generated by LLMs. More focused approaches for task-specific fine-tuning data generation have also been proposed, especially based around the rephrasing of already existing taskspecific datasets (Yin et al., 2023;Gandhi et al., 2024). Methods that use general-purpose corpora have recently been proposed for generating pretraining data (Maini et al., 2024). When used for fine-tuning data generation, these methods are either based around complex and resource-intensive multi-agent workflows (Mitra et al., 2024) or are restricted to a small set of tasks, as the generation process relies on a model that has been fine-tuned for those tasks (Nayak et al., 2024). \n\nThe two greatest drawbacks of current approaches to fully synthetic data generation are repetition and low quality. Unnatural Instructions reported that a majority of their samples have a BERTScore (Zhang et al., 2020) of above 45% when compared to other samples in the generated dataset. Self-Instruct faces similar issues, with generated instructions often having ROUGE-L scores (Lin, 2004) greater than 0.4 compared to the provided seed instructions.",
            "score": 0.3579152567309107,
            "section_title": "Synthetic Data Generation",
            "char_start_offset": 6140,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 342
                },
                {
                    "start": 345,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1392
                },
                {
                    "start": 1395,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1683
                },
                {
                    "start": 1684,
                    "end": 1848
                }
            ],
            "ref_mentions": [
                {
                    "start": 165,
                    "end": 176,
                    "matchedPaperCorpusId": "56396300"
                },
                {
                    "start": 216,
                    "end": 240,
                    "matchedPaperCorpusId": "263865370"
                },
                {
                    "start": 542,
                    "end": 562,
                    "matchedPaperCorpusId": "254877310"
                },
                {
                    "start": 587,
                    "end": 610,
                    "matchedPaperCorpusId": "254853659"
                },
                {
                    "start": 925,
                    "end": 943,
                    "matchedPaperCorpusId": "258841263"
                },
                {
                    "start": 1066,
                    "end": 1086,
                    "matchedPaperCorpusId": "267312030"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85302734375
        },
        {
            "corpus_id": "273970265",
            "title": "Diverse capability and scaling of diffusion and auto-regressive models when learning abstract rules",
            "text": "Humans excel at discovering regular structures from limited samples and applying inferred rules to novel settings. We investigate whether modern generative models can similarly learn underlying rules from finite samples and perform reasoning through conditional sampling. Inspired by Raven's Progressive Matrices task, we designed GenRAVEN dataset, where each sample consists of three rows, and one of 40 relational rules governing the object position, number, or attributes applies to all rows. We trained generative models to learn the data distribution, where samples are encoded as integer arrays to focus on rule learning. We compared two generative model families: diffusion (EDM, DiT, SiT) and autoregressive models (GPT2, Mamba). We evaluated their ability to generate structurally consistent samples and perform panel completion via unconditional and conditional sampling. We found diffusion models excel at unconditional generation, producing more novel and consistent samples from scratch and memorizing less, but performing less well in panel completion, even with advanced conditional sampling methods. Conversely, autoregressive models excel at completing missing panels in a rule-consistent manner but generate less consistent samples unconditionally. We observe diverse data scaling behaviors: for both model families, rule learning emerges at a certain dataset size - around 1000s examples per rule. With more training data, diffusion models improve both their unconditional and conditional generation capabilities. However, for autoregressive models, while panel completion improves with more training data, unconditional generation consistency declines. Our findings highlight complementary capabilities and limitations of diffusion and autoregressive models in rule learning and reasoning tasks, suggesting avenues for further research into their mechanisms and potential for human-like reasoning.",
            "score": 0.3579152567309107,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69091796875
        },
        {
            "corpus_id": "278481056",
            "title": "Nature's Insight: A Novel Framework and Comprehensive Analysis of Agentic Reasoning Through the Lens of Neuroscience",
            "text": "Current logical datasets often rely on synthetic patterns or exam-style questions, lacking real-world abstraction, multihop reasoning, and higher-order logic. Future datasets should emphasize diverse, scalable formats with generative reasoning techniques and better capture symbolic structure, uncertainty, and generalization potential.",
            "score": 0.3579152567309107,
            "section_title": "IV. BENCHMARKS AND DATASETS",
            "char_start_offset": 124514,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 336
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.169921875
        },
        {
            "corpus_id": "250390443",
            "title": "OmniTab: Pretraining with Natural and Synthetic Data for Few-shot Table-based Question Answering",
            "text": "As mentioned in the introduction, Learning on real sentences can endow models to grasp the nuances in language and align with structured tables. On the other hand, synthetic data is flexible, manipulable, and easy to obtain. It is costless to generate synthetic data via manipulating different aspects of the generated data to incorporate various desired properties. As a result, we can generate large amounts of complicated synthetic data covering various reasoning operations, which is lacking in the NL corpora. This motivates us to explore both types of data.",
            "score": 0.3579152567309107,
            "section_title": "OmniTab: Pretraining with Natural and Synthetic Data",
            "char_start_offset": 7726,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82470703125
        },
        {
            "corpus_id": "268537371",
            "title": "Enhancing Code Generation Performance of Smaller Models by Distilling the Reasoning Ability of LLMs",
            "text": "Automatic code generation has a history spanning decades, aiming to create executable programs from problem specifications (Backus et al., 1957;Waldinger and Lee, 1969;Manna and Waldinger, 1971).As artificial intelligence technology rapidly advances, the application of neural network techniques in intelligent code generation is increasingly gaining attention in the field of software engineering (Ling et al., 2016;Yin and Neubig, 2018;Lyu et al., 2021).Recently, large language models (LLMs) such as ChatGPT (OpenAI, 2022) have made significant advances in code generation owing to their superior reasoning capabilities.However, deploying these mammoth models comes with significant computational, time, and financial demands, coupled with data and security risks.Consequently, many enterprises and teams still prefer more manageable, smaller models.\n\nIn the realm of code generation, smaller models lag in reasoning capabilities compared to LLMs, leading to challenges with complex programming tasks.Our empirical studies highlight the exceptional in-context learning (ICL) of LLMs.By uti-lizing \"Chain-of-Thought (CoT)\" (Wei et al., 2022) as human-defined solution steps, LLMs can bolster their reasoning, allowing them to craft solution plans from these in-context examples.This methodology elevates LLMs' problem-solving accuracy and is notably effective in code generation (Jiang et al., 2023;Huang et al., 2024).However, while CoT strategies shine with massiveparameter models, smaller models, even after finetuning, struggle in deriving CoT-based solution plans due to ICL and reasoning constraints.Yet, we have observed that a smaller model, around 1B parameters in size, when fine-tuned and given both problem description and a precise CoT-based solution plan (labeled as \"best plan\"), sees a substantial boost in code generation capabilities, as shown in Figure 1.\n\nThis finding prompted us to further explore strategies for providing smaller models with a \"best plan\" when addressing programming tasks.However, we face two significant challenges: 1) Dependency on Large Language Models (LLMs) during inference.",
            "score": 0.35780519845832326,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 195,
                    "end": 456
                },
                {
                    "start": 456,
                    "end": 623
                },
                {
                    "start": 623,
                    "end": 767
                },
                {
                    "start": 767,
                    "end": 853
                },
                {
                    "start": 855,
                    "end": 1004
                },
                {
                    "start": 1004,
                    "end": 1086
                },
                {
                    "start": 1086,
                    "end": 1280
                },
                {
                    "start": 1280,
                    "end": 1421
                },
                {
                    "start": 1421,
                    "end": 1609
                },
                {
                    "start": 1609,
                    "end": 1877
                },
                {
                    "start": 1879,
                    "end": 2016
                },
                {
                    "start": 2016,
                    "end": 2124
                }
            ],
            "ref_mentions": [
                {
                    "start": 123,
                    "end": 144,
                    "matchedPaperCorpusId": "13973980"
                },
                {
                    "start": 144,
                    "end": 168,
                    "matchedPaperCorpusId": "32480327"
                },
                {
                    "start": 168,
                    "end": 194,
                    "matchedPaperCorpusId": "9539039"
                },
                {
                    "start": 438,
                    "end": 455,
                    "matchedPaperCorpusId": "232404009"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10858154296875
        },
        {
            "corpus_id": "259503802",
            "title": "Tabular and latent space synthetic data generation: a literature review",
            "text": "In addition, several synthetic data generation methods are specific to the domain, data type, or target ML task. Generally, these methods rely on the domain data's structure and are not easily transferable to tabular data. \n\nOverall, synthetic data generation techniques for tabular data are not as explored as image or text data, despite their popularity and ubiquity [16]. Furthermore, these techniques are invariant to the original data format; they can be applied to both the latent space [3] or tabular data. On one hand, data generation in the latent space uses a generative model to learn a manifold, lower-dimensional abstraction over the input space [2]. At this level, any tabular data generation mechanism can be applied and reconstructed into the input space if necessary. On the other hand, synthetic data generation on tabular data can be applied to most problems. Although, the choice of generation mechanism depends on (1) the importance of the original statistical information and the relationships among features, (2) the target ML task, and (3) the role synthetic data plays in the process (i.e., anonymization, regularization, class balancing, etc.). For example, when generating data to address an imbalanced learning problem (i.e., oversampling), the relationships between the different features are not necessarily kept, since the goal is to reinforce the learning of the minority class by redefining an ML classifier's decision boundaries. If the goal is to anonymize a dataset, perform some type of descriptive task, or ensure consistent model interpretability, statistical information must be preserved. \n\nDepending on the context, evaluating the quality of the generated data is a complex task. For example, for image and time series data, perceptually small changes in the original data can lead to large changes in the Euclidean distance [4,17]. The evaluation of generative models typically accounts primarily for the performance in a specific task, since good performance in one criterion does not imply good performance on another [17]. However, in computationally intensive tasks it is often impracticable to search for the optimal configurations of generative models.",
            "score": 0.3575741882243563,
            "section_title": "Introduction",
            "char_start_offset": 1999,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 222
                },
                {
                    "start": 225,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1463
                },
                {
                    "start": 1464,
                    "end": 1629
                },
                {
                    "start": 1632,
                    "end": 1721
                },
                {
                    "start": 1722,
                    "end": 1874
                },
                {
                    "start": 1875,
                    "end": 2068
                },
                {
                    "start": 2069,
                    "end": 2201
                }
            ],
            "ref_mentions": [
                {
                    "start": 369,
                    "end": 373,
                    "matchedPaperCorpusId": "220055739"
                },
                {
                    "start": 659,
                    "end": 662,
                    "matchedPaperCorpusId": "174802445"
                },
                {
                    "start": 1867,
                    "end": 1870,
                    "matchedPaperCorpusId": "226858255"
                },
                {
                    "start": 1870,
                    "end": 1873,
                    "matchedPaperCorpusId": "2187805"
                },
                {
                    "start": 2063,
                    "end": 2067,
                    "matchedPaperCorpusId": "2187805"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67041015625
        },
        {
            "corpus_id": "270766239",
            "title": "Compositional Structured Explanation Generation with Dynamic Modularized Reasoning",
            "text": "Generalization in Reasoning Despite the success of language models in solving reasoning tasks, their generalization abilities have attracted attention, e.g., length generalization (Clark et al., 2020;Wu et al., 2021;Anil et al., 2022), compositional generalization (Liu et al., 2022), domain generalization (Niu et al., 2023), etc.In this work, we explore compositional generalization in reasoning.\n\nCompositional generalization has been researched for decades (Fodor and Pylyshyn, 1988;Marcus, 2003;Lake and Baroni, 2018), including two significant properties: productivity and systematicity (Hupkes et al., 2020).Among these, productivity is similar to length generalization, in that both evaluate generalization to deeper reasoning chains.But for evaluating productivity, primitive units needed for solving deeper samples must have been learned during training.In contrast to the related length-generalization work of Clark et al. (2020), our CSEG task aims to evaluate productivity in a structured compositional generalization reasoning task.We therefore guarantee that primitive units (rule types) needed for solving deeper samples have been learned in training.Importantly, we frame CSEG as a generation task, which unlike classification settings as in Clark et al. (2020), makes it harder for models to exploit shortcuts.\n\nRecently, there has been renewed interest in exploring compositional generalization in reasoning tasks.Johnson et al. (2017); Hudson and Manning (2019); Bogin et al. (2021); Gao et al. (2022) proposed challenging compositional tasks in visual QA.Liu et al. (2022) designed compositional questions for QA and found even the strongest model struggled with these challenging questions.Other works probed the compositional abilities of models in natural language inference (Geiger et al., 2020;Goodwin et al., 2020;Yanaka et al., 2020Yanaka et al., , 2021;;Fu andFrank, 2023, 2024), focusing on specific linguistic phenomena, such as quantifiers, negation, or predicate replacements.",
            "score": 0.3574321115362913,
            "section_title": "Related Work",
            "char_start_offset": 4399,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 331
                },
                {
                    "start": 331,
                    "end": 398
                },
                {
                    "start": 400,
                    "end": 615
                },
                {
                    "start": 615,
                    "end": 742
                },
                {
                    "start": 742,
                    "end": 864
                },
                {
                    "start": 864,
                    "end": 1046
                },
                {
                    "start": 1046,
                    "end": 1167
                },
                {
                    "start": 1167,
                    "end": 1328
                },
                {
                    "start": 1330,
                    "end": 1433
                },
                {
                    "start": 1433,
                    "end": 1576
                },
                {
                    "start": 1576,
                    "end": 1712
                },
                {
                    "start": 1712,
                    "end": 2009
                }
            ],
            "ref_mentions": [
                {
                    "start": 180,
                    "end": 200,
                    "matchedPaperCorpusId": "211126663"
                },
                {
                    "start": 200,
                    "end": 216,
                    "matchedPaperCorpusId": "220363813"
                },
                {
                    "start": 216,
                    "end": 234,
                    "matchedPaperCorpusId": "250425737"
                },
                {
                    "start": 265,
                    "end": 283,
                    "matchedPaperCorpusId": "237417170"
                },
                {
                    "start": 307,
                    "end": 325,
                    "matchedPaperCorpusId": "258685630"
                },
                {
                    "start": 461,
                    "end": 487,
                    "matchedPaperCorpusId": "29043627"
                },
                {
                    "start": 500,
                    "end": 522,
                    "matchedPaperCorpusId": "46761158"
                },
                {
                    "start": 593,
                    "end": 614,
                    "matchedPaperCorpusId": "211259383"
                },
                {
                    "start": 921,
                    "end": 940,
                    "matchedPaperCorpusId": "211126663"
                },
                {
                    "start": 1259,
                    "end": 1278,
                    "matchedPaperCorpusId": "211126663"
                },
                {
                    "start": 1433,
                    "end": 1454,
                    "matchedPaperCorpusId": "15458100"
                },
                {
                    "start": 1483,
                    "end": 1502,
                    "matchedPaperCorpusId": "237592834"
                },
                {
                    "start": 1504,
                    "end": 1521,
                    "matchedPaperCorpusId": "239998781"
                },
                {
                    "start": 1576,
                    "end": 1593,
                    "matchedPaperCorpusId": "237417170"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.481201171875
        },
        {
            "corpus_id": "277104955",
            "title": "Synthetic Data Generation Using Large Language Models: Advances in Text and Code",
            "text": "Synthetic data generation using large language models has emerged as a powerful technique to address data scarcity and enhance model training in both natural language and programming language domains. We have reviewed the state-of-the-art advances in this area, highlighting how LLMs can generate high-quality text and code data that, when used judiciously, lead to significant performance gains on downstream tasks. \n\nIn the text domain, LLM-generated data has proven especially useful for low-resource scenarios, delivering substantial improvements in tasks like classification and QA when human-labeled data is limited [33]. Techniques such as promptbased augmentation (zero-shot, few-shot, etc.), retrieval augmentation for grounding facts [3], and iterative refinement have pushed synthetic text data closer in effectiveness to real data. \n\nIn the code domain, LLMs have unlocked new possibilities by generating code snippets, programming instructions, and even whole problem solutions, facilitating better training of code models. The ability to verify code correctness via execution [37] provides a strong advantage, allowing the curation of large-scale, correct-by-construction synthetic code datasets (e.g., Code Alpaca, WizardCoder) that have propelled open-source code models to approach the competency of their proprietary counterparts. \n\nThroughout this survey, we have also underscored the practical considerations: synthetic data must be used with care to ensure quality and avoid pitfalls like distribution shift, bias amplification, or model collapse from feedback loops. Encouraging findings, such as the avoidance of model collapse by combining synthetic with real data [18], give confidence that these pitfalls can be managed with thoughtful strategies. The community has developed numerous methods (from filtering heuristics to advanced RL techniques) to maximize the signal and minimize the noise in synthetic datasets [33]. \n\nOur discussion on challenges reflects that this field is still evolving. Issues of factual accuracy, diversity, and evaluation metrics remain open for continued research. However, the rapid progress in just the last two years -with surveys cataloging dozens of new works [6] -indicates a vibrant research momentum. We are likely to see even more innovative solutions, like integrated human-in-the-loop generation and domain-specific synthetic data models, being realized soon.",
            "score": 0.3572887998312396,
            "section_title": "Conclusion",
            "char_start_offset": 64076,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 416
                },
                {
                    "start": 419,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 843
                },
                {
                    "start": 846,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1348
                },
                {
                    "start": 1351,
                    "end": 1588
                },
                {
                    "start": 1589,
                    "end": 1773
                },
                {
                    "start": 1774,
                    "end": 1946
                },
                {
                    "start": 1949,
                    "end": 2021
                },
                {
                    "start": 2022,
                    "end": 2119
                },
                {
                    "start": 2120,
                    "end": 2263
                },
                {
                    "start": 2264,
                    "end": 2425
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97412109375
        },
        {
            "corpus_id": "270063216",
            "title": "iREL at SemEval-2024 Task 9: Improving Conventional Prompting Methods for Brain Teasers",
            "text": "Secondly, we enable the model to generate explanations for correct answer choices during training through reason generation, thereby deepening its comprehension of the examples.These approaches have demonstrated improvements in the evaluation scores.\n\nand Singhal et al. (2023) demonstrate recent advancements in multiple-choice question answering.They achieve this by developing new datasets and evaluating large language models (LLMs) on them, thus contributing significantly to the field's progress.Xie et al. (2023) presents OlaGPT, an innovative framework designed to enhance the reasoning capabilities of large language models (LLMs) by drawing inspiration from human cognitive architecture.OlaGPT integrates cognitive modules such as attention, memory, reasoning, and learning, emphasizing a reasoning module that simulates human-like thought processes.The module then enables OlaGPT to create multiple agents and utilize various thinking templates, including lateral and integrative thinking, to solve reasoning problems effectively.Huang et al. (2023) introduces a novel evaluation benchmark to assess a model's lateral thinking abilities in an interactive framework, utilizing Lateral Thinking Puzzles as the context.Meng et al. (2024) proposes a divide-andconquer approach to LLM reasoning.It involves categorizing questions into subsets based on statistical confidence scores (CS), followed by targeted interventions such as Prior Knowledge-based Reasoning (PKR) and Filter Choices-based Reasoning (FCR) to address nuanced and demanding tasks.",
            "score": 0.3563799460932963,
            "section_title": "Introduction",
            "char_start_offset": 2146,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 177,
                    "end": 250
                },
                {
                    "start": 252,
                    "end": 348
                },
                {
                    "start": 348,
                    "end": 502
                },
                {
                    "start": 502,
                    "end": 697
                },
                {
                    "start": 697,
                    "end": 860
                },
                {
                    "start": 860,
                    "end": 1041
                },
                {
                    "start": 1041,
                    "end": 1227
                },
                {
                    "start": 1227,
                    "end": 1301
                },
                {
                    "start": 1301,
                    "end": 1555
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.38720703125
        },
        {
            "corpus_id": "273507937",
            "title": "EnvBridge: Bridging Diverse Environments with Cross-Environment Knowledge Transfer for Embodied AI",
            "text": "LLMs have made significant strides in natural language processing tasks, including text generation, summarization, translation, and tasks that require elements of reasoning such as reading, comprehension, and question answering. Though their capacity for reasoning and complex problem-solving is impressive, there is still much room for improvement. To enhance their reasoning skills, researchers are exploring techniques like hierarchical reasoning, where complex problems are broken down into smaller sub-problems. Wei et al. (2023) 2022)) is an embodied agent tailored for task planning. In this system, scene graphs and environmental feedback function as the agent's short-term memory, directing its actions. The Generative Agent (Park et al. ( 2023)) utilizes a hybrid memory structure to support the agent's behavior. Its short-term memory holds contextual information about the agent's present situation, while its long-term memory stores past behaviors and thoughts, which can be accessed based on current events.",
            "score": 0.35522687044400086,
            "section_title": "ADVANCEMENTS IN REASONING WITH LLMS",
            "char_start_offset": 5497,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 1021
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.23193359375
        },
        {
            "corpus_id": "265213146",
            "title": "Transformers in the Service of Description Logic-Based Contexts",
            "text": "Multiple surveys [27,11,28] in the literature describe the most recent research developments on the use of transformers for reasoning tasks. One of the first datasets generated for this purpose was from Clark et al. [3] with RuleTaker, demonstrating the potential of transformers to perform logical question answering under CWA by training LLMs on synthetic datasets. However, their approach was limited to short expressions of simple conjunctive rules. Tafjord et al. [22], generated the ProofWriter datasets (under CWA and OWA) and with a T5 [18]-based model fined-tuned on ProofWriter showed that LLMs can generate proofs with high accuracy (94.8% for depth 5). \n\nThe generation approach of the DELTAD was based on the approach for the generation of the datasets RuleTaker and ProofWriter, i.e., using PCFGs. However, DELTAD is different from these datasets as i) ALCQ is a much more expressive logic language hence we new PCFGs; ii) we have defined different PCFGs for each linguistic complexity level (which has not been done for any other dataset in the literature); iii) it is balanced regarding the aspects discussed in Section 3.5. \n\nIn more expressive contexts, Onta\u00f1\u00f3n et al. [16] showed that LLMs perform well (up to 90.5%) over contexts generated by propositional logic and a small subset of FOL. Han et al. [5], with the FOLIO dataset (1.4K), generated from FOL sentences -but without number restrictions-, tested the ability of various LLMs for the same reasoning task and concluded that RoBERTa [12] performed best among all tested models (including GPT-3 and Codex) but still, the performance was low. Tian et al. [24] introduced the much richer synthetic dataset LogicNLI (30K), under OWA for diagnosing LLMs' ability in FOL reasoning, showing that even their best-performing model does not learn to perform reasoning tasks and cannot generalize to different scenarios.",
            "score": 0.35510742425556746,
            "section_title": "Related Work",
            "char_start_offset": 29750,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 664
                },
                {
                    "start": 667,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 1140
                },
                {
                    "start": 1143,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1618
                },
                {
                    "start": 1619,
                    "end": 1887
                }
            ],
            "ref_mentions": [
                {
                    "start": 21,
                    "end": 24,
                    "matchedPaperCorpusId": "254877753"
                },
                {
                    "start": 216,
                    "end": 219,
                    "matchedPaperCorpusId": "211126663"
                },
                {
                    "start": 469,
                    "end": 473,
                    "matchedPaperCorpusId": "229371222"
                },
                {
                    "start": 544,
                    "end": 548,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74853515625
        },
        {
            "corpus_id": "270571222",
            "title": "Comparative Analysis of Supervised Classification Algorithms for Residential Water End Uses",
            "text": "Therefore, the algorithms that generate synthetic data must be robust and capture the patterns in the actual data (Figueira & Vaz, 2022).Synthetic data generation methods have evolved significantly over time, from simple resampling and augmentation techniques to sophisticated generative models emerging from Deep Learning such as Variational AutoEncoders (Kingma & Welling, 2013) and Generative Adversarial Networks (GANs) (Goodfellow et al., 2020), which have undergone several modifications since they were first proposed to adapt to different data structures in different domains.The main focus of models like GANs has been on computer vision tasks where the domain consists generically of images; therefore, modeling tabular data, such as water end-use consumption, can pose challenges for GANs.The generation of synthetic tabular data is a noted research need (Figueira & Vaz, 2022), and recent studies have proposed adaptive methods like Conditional Tabular GANs (CTGANs) for synthetic tabular data generation (Xu et al., 2019), enabling new opportunities for developing models to analyze smart water meter data for water end-use consumption estimation in residential households, where ground-truth labeled data are limited (Di Mauro et al., 2021;Gilbertson et al., 2011).\n\nSeveral methods of analyzing fine temporal resolution data (e.g., 5 s or 10 s) have been proposed to extract water event information, such as flow trace analysis (DeOreo et al., 1996), derivative signals (Bethke et al., 2021), pressure sensing (Froehlich et al., 2011;Larson et al., 2012), and classification techniques (Heydari et al., 2022;Meyer et al., 2020;Vitter & Webber, 2018).Given the significance and potential of classification techniques, they form the main focus of this paper.Literature reviews from studies such as Chen et al. (2017) highlight that various machine learning (ML) classification algorithms possess distinct strengths and weaknesses based on the data characteristics of different study areas.Although numerous ML algorithms have been developed, their efficacy in addressing practical tasks is highly contingent upon these data characteristics, thus underscoring the need for comparative analyses.",
            "score": 0.35457432383487464,
            "section_title": "Introduction",
            "char_start_offset": 3489,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 137,
                    "end": 584
                },
                {
                    "start": 584,
                    "end": 800
                },
                {
                    "start": 800,
                    "end": 1279
                },
                {
                    "start": 1281,
                    "end": 1665
                },
                {
                    "start": 1665,
                    "end": 1771
                },
                {
                    "start": 1771,
                    "end": 2002
                },
                {
                    "start": 2002,
                    "end": 2206
                }
            ],
            "ref_mentions": [
                {
                    "start": 356,
                    "end": 380,
                    "matchedPaperCorpusId": "125287703"
                },
                {
                    "start": 424,
                    "end": 449,
                    "matchedPaperCorpusId": "1033682"
                },
                {
                    "start": 1017,
                    "end": 1034,
                    "matchedPaperCorpusId": "195767064"
                },
                {
                    "start": 1254,
                    "end": 1278,
                    "matchedPaperCorpusId": "132929560"
                },
                {
                    "start": 1443,
                    "end": 1464,
                    "matchedPaperCorpusId": "109114181"
                },
                {
                    "start": 1485,
                    "end": 1506,
                    "matchedPaperCorpusId": "234066140"
                },
                {
                    "start": 1525,
                    "end": 1549,
                    "matchedPaperCorpusId": "11307979"
                },
                {
                    "start": 1549,
                    "end": 1569,
                    "matchedPaperCorpusId": "6355767"
                },
                {
                    "start": 1601,
                    "end": 1623,
                    "matchedPaperCorpusId": "251651158"
                },
                {
                    "start": 1623,
                    "end": 1642,
                    "matchedPaperCorpusId": "219017582"
                },
                {
                    "start": 1811,
                    "end": 1829,
                    "matchedPaperCorpusId": "89610503"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84130859375
        },
        {
            "corpus_id": "274656618",
            "title": "A Graph-Based Synthetic Data Pipeline for Scaling High-Quality Reasoning Instructions",
            "text": "Despite the remarkable capabilities large language models (LLMs) have demonstrated in various linguistic tasks, significant gaps remain in their ability to comprehend and solve intricate reasoning tasks (e.g., mathematics, coding, physics, and chemistry). One effective approach to bridging these gaps is using large-scale, high-quality synthetic data. However, it is still a challenge to develop a low-cost and effective synthesis pipeline. Take mathematics as an example. The two main approaches for building high-quality mathematics reasoning datasets are data filtering and data synthesis. Data filtering (Yue et al., 2024b;Shao et al., 2024;Ying et al., 2024) involves extracting data from pre-training corpora such as Common Crawl, and rewriting it using advanced commercial models or human annotation. However, the vast scale and inherent noise of these corpora result in high post-processing costs and inconsistent data quality. Data synthesis (Yu et al., 2023;Luo et al., 2023;Yue et al., 2024a;Tang et al., 2024;Huang et al., 2024a;Li et al., 2024a;Toshniwal et al., 2024;Li et al., 2024b) leverages frontier large language models, such as GPT-3.5 (Floridi & Chiriatti, 2020) and GPT-4 (Achiam et al., 2023), to augment or regenerate high-quality mathematical reasoning datasets. One approach entails rewriting or regenerating similar problems based on seed data for data augmentation. Another approach entails generating new problems using knowledge points. The \"knowledge points\" refers to fine-grained math concepts (e.g., the Pythagorean theorem, polynomial factorization skills) in problem solving, and they can be generated freshly via LLMs or extracted from existing seed data. Although data synthesis is straightforward, it still suffers from three significant drawbacks: (1) Limited scalability: Existing methods have poor scalability, making it difficult to synthesize larger-scale data from smaller seed data.",
            "score": 0.3544289123930477,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1289
                },
                {
                    "start": 1290,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 1930
                }
            ],
            "ref_mentions": [
                {
                    "start": 986,
                    "end": 1004,
                    "matchedPaperCorpusId": "261696697"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95361328125
        },
        {
            "corpus_id": "260316264",
            "title": "Deep Generative Models, Synthetic Tabular Data, and Differential Privacy: An Overview and Synthesis",
            "text": "The paper provided a cohesive overview of deep generative models for synthetic data generation, focusing on the under-explored domain of tabular data. We have discussed the flexibility and potential uses of neural networks in probabilistic models, the various deep generative models employed for synthetic data generation, their inference algorithms, their adaption to tabular datasets, considerations in privacy-sensitive settings, and the evaluation metrics for assessing the fidelity of high-dimensional synthetic datasets. \n\nWhile deep generative models have gained immense popularity and provided state-of-the-art results for image and text datasets, they are often criticized for their poor performance and excessive complexity in less common machine learning use cases, such as medium-sized or smaller tabular datasets. However, we argue that no inherent limitation in these models would hinder their ability to excel in such scenarios. Overfitting and the lack of generalization are common concerns in deep learning methods due to the heavily parameterized nature of neural networks. However, in synthetic data generation, these concerns could be more relevant as our goal is to estimate and sample from the data-generating process of the observed dataset. It is precisely, because we aim to capture the data-generating process, that privacy-aware parameter estimation becomes crucial for sensitive applications. \n\nWe believe that the potential of deep generative models in generating and releasing synthetic versions of confidential data sources has only begun to be explored, and note that recently-available software package Synthcity (Qian et al., 2023) serves as a promising enabler for experimentation for several approaches discussed herein. \n\nThere are numerous exciting avenues for methodological development, such as scalable nonlinear extensions to existing generative modelling approaches, private inference for probabilistic models like unsupervised clustering techniques, and scalable approaches to model fitting and comparisons. These advancements will pave the way for future applications involving confidential data sources. The article focused on the most prominent generative models. It is worth noting, however, that in recent years a class of generative models called diffusion models has enjoyed considerable success in domains outside of the tabular data setting; for two survey articles, see Yang et al. (2022) andCroitoru et al. (2023).",
            "score": 0.3544289123930477,
            "section_title": "Discussion",
            "char_start_offset": 77920,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 526
                },
                {
                    "start": 529,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 943
                },
                {
                    "start": 944,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1420
                },
                {
                    "start": 1423,
                    "end": 1756
                },
                {
                    "start": 1759,
                    "end": 2051
                },
                {
                    "start": 2052,
                    "end": 2149
                },
                {
                    "start": 2150,
                    "end": 2210
                },
                {
                    "start": 2211,
                    "end": 2469
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85693359375
        },
        {
            "corpus_id": "254366803",
            "title": "Harnessing Knowledge and Reasoning for Human-Like Natural Language Generation: A Brief Review",
            "text": "Human language is a good vehicle for reasoning. Thus, the generation of language naturally resembles the way humans think and reason. With the success of PLMs, there is a growing interest in the AI community to use NLG models to generate a chain of reasoning for problem-solving. \n\nGenerative Reasoning Generative reasoning aims to generate intermediate reasons with NLG models for better problem solving. Such intermediate reasons take many forms, including deduction and abduction reasons (Tafjord et al., 2021), explanations (Jhamtani and Clark, 2020), or decomposed subtasks of a complex one (Khot et al., 2021). Some work (Shwartz et al., 2020;Betz et al., 2021a) even show that expanding the context of the input by generating more information would also help solve reasoning tasks. Moreover, Clark et al. (2020) find that transformer-based PLMs are effective soft reasoners on a toy deduction dataset, which consists of collections of text verbalized from artificial if-then rules and facts. Other studies (Bostrom et al., 2021;Betz et al., 2021b) corroborate this discovery and show that training generative models with artificial textual data that verbalize rulebased reasoning helps downstream logical reasoning tasks. We remark that generative reasoning provides a new perspective of breaking the blackbox prediction of neural networks, which demonstrates the potential of achieving reasoning with NLG systems. \n\nReasoning with Large Language Models Entering the era of large language models (LLMs) such as GPT-3 (Brown et al., 2020;Ouyang et al., 2022) and PaLM (Chowdhery et al., 2022), there is a recent growing interest in exploring few-/zeroshot reasoning skills of these LLMs Since finetuning such tremendous language models is hardly possible, current work adopts prompt-based incontext learning methods (Brown et al., 2020;Lu et al., 2022a) to achieve few-/zero-shot learning with LLMs, where instructions and examples are demonstrated within the input prompts. \n\nSimilar to the above discussion of generative reasoning, this line of work aims to guide the language models to explicitly generate the intermediate thinking steps (or reasons) during reasoning.",
            "score": 0.3544289123930477,
            "section_title": "Reasoning by NLG",
            "char_start_offset": 24788,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 47
                },
                {
                    "start": 48,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 279
                },
                {
                    "start": 282,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 998
                },
                {
                    "start": 999,
                    "end": 1228
                },
                {
                    "start": 1229,
                    "end": 1421
                },
                {
                    "start": 1424,
                    "end": 1980
                },
                {
                    "start": 1983,
                    "end": 2177
                }
            ],
            "ref_mentions": [
                {
                    "start": 491,
                    "end": 513,
                    "matchedPaperCorpusId": "229371222"
                },
                {
                    "start": 528,
                    "end": 554,
                    "matchedPaperCorpusId": "222178328"
                },
                {
                    "start": 596,
                    "end": 615,
                    "matchedPaperCorpusId": "221448158"
                },
                {
                    "start": 627,
                    "end": 649,
                    "matchedPaperCorpusId": "215745286"
                },
                {
                    "start": 799,
                    "end": 818,
                    "matchedPaperCorpusId": "211126663"
                },
                {
                    "start": 1013,
                    "end": 1035,
                    "matchedPaperCorpusId": "237450610"
                },
                {
                    "start": 1035,
                    "end": 1054,
                    "matchedPaperCorpusId": "221702898"
                },
                {
                    "start": 1524,
                    "end": 1544,
                    "matchedPaperCorpusId": "54040953"
                },
                {
                    "start": 1544,
                    "end": 1564,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1822,
                    "end": 1842,
                    "matchedPaperCorpusId": "54040953"
                },
                {
                    "start": 1842,
                    "end": 1859,
                    "matchedPaperCorpusId": "233296494"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6240234375
        },
        {
            "corpus_id": "267523454",
            "title": "How Realistic Is Your Synthetic Data? Constraining Deep Generative Models for Tabular Data",
            "text": "Synthetic data generation represents an important area of machine learning (ML) due to its numerous applications in the real world. Indeed, synthetic data have been increasingly used to augment real data to improve the predictive performance of ML models (see, e.g., Han et al. (2005)), to remedy data scarcity (see, e.g., Choi et al. (2017)), and to promote fairness (see, e.g., van Breugel et al. (2021)), and they are now even used to generate brand-new datasets to ensure privacy in sensitive settings (see, e.g., Jordon et al. (2019); Yoon et al. (2020); Lee et al. (2021)). \n\nDeep Generative Models (DGMs) have been shown to be powerful tools for generating tabular data, as they have been progressively more able to capture the complex distributions that characterize such data (see, e.g., Kim et al. (2023)). However, for generating realistic tabular data, it is insufficient to just learn a good distribution approximation; it is necessary to create samples that obey a set of constraints expressing background knowledge about known characteristics of the features and/or existing relationships among them. For example, if we are generating data from a clinical trial dataset, then, for every synthetic sample, we surely want the value associated with the \"maximum level of hemoglobin recorded\" column to be greater than or equal to the one associated with the \"minimum level of hemoglobin recorded\" column. Indeed, any sample violating such constraint is not realistic. Existing methods, while excellent at capturing complex distributions, are still not able to learn even from this simple background knowledge, and thus do not provide any guarantee of constraint satisfaction. In addition, currently, no work incorporates background knowledge in DGMs for tabular data except for GOGGLE (Liu et al., 2022), which, however, is only able to inject very simple background knowledge about existing correlations among features. \n\nIn this paper, we address this limitation, and we show how to integrate any background knowledge that can be expressed as a set of linear inequalities into different standard DGMs for tabular data.",
            "score": 0.3544289123930477,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 579
                },
                {
                    "start": 582,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1416
                },
                {
                    "start": 1417,
                    "end": 1479
                },
                {
                    "start": 1480,
                    "end": 1687
                },
                {
                    "start": 1688,
                    "end": 1932
                },
                {
                    "start": 1935,
                    "end": 2132
                }
            ],
            "ref_mentions": [
                {
                    "start": 267,
                    "end": 284,
                    "matchedPaperCorpusId": "12126950"
                },
                {
                    "start": 323,
                    "end": 341,
                    "matchedPaperCorpusId": "7739761"
                },
                {
                    "start": 380,
                    "end": 405,
                    "matchedPaperCorpusId": "239768540"
                },
                {
                    "start": 518,
                    "end": 538,
                    "matchedPaperCorpusId": "53342261"
                },
                {
                    "start": 540,
                    "end": 558,
                    "matchedPaperCorpusId": "212706996"
                },
                {
                    "start": 560,
                    "end": 577,
                    "matchedPaperCorpusId": "244959037"
                },
                {
                    "start": 797,
                    "end": 814,
                    "matchedPaperCorpusId": "252780718"
                },
                {
                    "start": 1797,
                    "end": 1815,
                    "matchedPaperCorpusId": "259298730"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7548828125
        },
        {
            "corpus_id": "276903896",
            "title": "Attention-Based Synthetic Data Generation for Calibration-Enhanced Survival Analysis: A Case Study for Chronic Kidney Disease Using Electronic Health Records",
            "text": "Advances in Synthetic Data Generation Synthetic data generation has long been a core component of ML development, progressing from autoencoders [43] to sophisticated models like variational autoencoders (VAEs) [17], generative adversarial networks (GANs) [18], and denoising diffusion probabilistic models (DDPMs) [44]. These methods have achieved exceptional success in domains such as image generation [45], natural language processing in text [46], and text-to-video generation [47].",
            "score": 0.3544289123930477,
            "section_title": "Related Work",
            "char_start_offset": 10787,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 486
                }
            ],
            "ref_mentions": [
                {
                    "start": 144,
                    "end": 148,
                    "matchedPaperCorpusId": "6138085"
                },
                {
                    "start": 210,
                    "end": 214,
                    "matchedPaperCorpusId": "211146177"
                },
                {
                    "start": 314,
                    "end": 318,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 404,
                    "end": 408,
                    "matchedPaperCorpusId": "195944196"
                },
                {
                    "start": 446,
                    "end": 450,
                    "matchedPaperCorpusId": "1563370"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91552734375
        },
        {
            "corpus_id": "263835175",
            "title": "Deep Aramaic: Towards a synthetic data paradigm enabling machine learning in epigraphy",
            "text": "Synthetic data generation provides a potential solution by producing labeled corpora for teaching ML models [KWPP+23; Tsi22]. Recent advances in procedural modeling and computer graphics enable the creation of simulated environments with high visual realism [KWPP+23]. However, frameworks tailored to the complexities of historical scripts like Old",
            "score": 0.3544289123930477,
            "section_title": "Introduction",
            "char_start_offset": 2263,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 348
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.33642578125
        },
        {
            "corpus_id": "273549253",
            "title": "A Systematic Survey on Instructional Text: From Representation Formats to Downstream NLP Tasks",
            "text": "Safa, et al. \n\nquestion answering [190,195,202], classification [131,218], detecting implicit information in instructions [4], learning task hierarchy [217], generation (e.g., next step, all steps) [98,122] and benchmarking procedural language understanding and planning abilities [172,173] without any additional annotation layers. Even though recipes and WikiHow are mostly exploited, we find that many resources such as troubleshooting websites from tech companies (e.g., Canon are overlooked in the literature with some exceptions [51]. The reasons might be i) their small size (which is smaller) ii) the lack of a consistent structure that makes them hard to parse. [162] and ABCD [25], employ crowdworkers to generate dialogue datasets grounded on instructional documents such as recipes and call center guidelines by assigning different roles to crowdworkers (e.g., call center employee, information giver on a certain recipe). It is also commonly used to annotate existing small corpora for a specific tasks. Such tasks are mostly related to extracting some form of information from instructional text, e.g., tools from fixing manuals [117], ingredients from recipes [211]. Due to the associated costs with crowdsourcing, this technique has been mostly used for generating scripts, grounded dialogues or to add small annotation layers to existing corpora. We find that, even though, for example, the corpus DeScript [180] and the alignments are publicly available, such resources are overlooked and not used for related downstream tasks e.g., event paraphrasing or alignment to the best of our knowledge, with some exceptions [179]. \n\n4.1.3 Synthetic data generation. Synthetic datasets and environments are also common for creating toy setups under simplifying assumptions. For example, Weston et al. [181] formulate 20 question answering tasks to evaluate different linguistic and reasoning abilities such as co-reference resolution, temporal and spatial reasoning. The stories are generated in a simulated world which is defined by manually written rules (e.g., one should find food if hungry) on a predefined set of entities with predefined attributes such as location and size.",
            "score": 0.35415674542499,
            "section_title": "Web Corpora.",
            "char_start_offset": 13891,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 12
                },
                {
                    "start": 15,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1640
                },
                {
                    "start": 1643,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1675
                },
                {
                    "start": 1676,
                    "end": 1782
                },
                {
                    "start": 1783,
                    "end": 1975
                },
                {
                    "start": 1976,
                    "end": 2190
                }
            ],
            "ref_mentions": [
                {
                    "start": 34,
                    "end": 39,
                    "matchedPaperCorpusId": "233219638"
                },
                {
                    "start": 43,
                    "end": 47,
                    "matchedPaperCorpusId": "221738970"
                },
                {
                    "start": 64,
                    "end": 69,
                    "matchedPaperCorpusId": "13825603"
                },
                {
                    "start": 69,
                    "end": 73,
                    "matchedPaperCorpusId": "201632901"
                },
                {
                    "start": 122,
                    "end": 125,
                    "matchedPaperCorpusId": "218974015"
                },
                {
                    "start": 151,
                    "end": 156,
                    "matchedPaperCorpusId": "247446828"
                },
                {
                    "start": 198,
                    "end": 202,
                    "matchedPaperCorpusId": "236469319"
                },
                {
                    "start": 202,
                    "end": 206,
                    "matchedPaperCorpusId": "22457893"
                },
                {
                    "start": 281,
                    "end": 286,
                    "matchedPaperCorpusId": "268248850"
                },
                {
                    "start": 286,
                    "end": 290,
                    "matchedPaperCorpusId": "261706169"
                },
                {
                    "start": 686,
                    "end": 690,
                    "matchedPaperCorpusId": "233004708"
                },
                {
                    "start": 1143,
                    "end": 1148,
                    "matchedPaperCorpusId": "218973875"
                },
                {
                    "start": 1175,
                    "end": 1180,
                    "matchedPaperCorpusId": "252624094"
                },
                {
                    "start": 1424,
                    "end": 1429,
                    "matchedPaperCorpusId": "10551763"
                },
                {
                    "start": 1810,
                    "end": 1815,
                    "matchedPaperCorpusId": "3178759"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.272216796875
        },
        {
            "corpus_id": "249191979",
            "title": "Noise-Aware Statistical Inference with Differentially Private Synthetic Data",
            "text": "There is a sizable literature on DP synthetic data generation. Most recent work in the area either releases the values of a set of simple queries, such as counting queries, under DP and uses them as the basis of synthetic data (Aydore et al. 2021;Bernstein et al. 2017;Cai et al. 2021;R. Chen et al. 2015;Hardt et al. 2012;T. Liu et al. 2021;McKenna et al. 2018;McKenna et al. 2021;McKenna et al. 2022;McKenna et al. 2019;Nixon et al. 2022;Vietri et al. 2020;Zhang et al. 2017), or trains some kind of generative model, often a GAN, using the whole real dataset under DP (D. Chen et al. 2020;J\u00e4lk\u00f6 et al. 2021;Long et al. 2021;Xie et al. 2018;Yoon et al. 2019). There are also hybrid approaches that use sophisticated queries that can capture all features of the dataset, and train a generative model using those (Harder et al. 2021;Liew et al. 2022). Of the existing DP synthetic data generation algorithms, NAPSU-MQ is closest to the PGM algorithm (McKenna et al. 2019), which does maximum likelihood estimation with the same data model as NAPSU-MQ instead of noiseaware Bayesian inference. We describe this connection in more detail in Supplemental Section D. \n\nRubin's rules were originally developed for analyses on missing data, as part of an approach called multiple imputation (Rubin 1987), which was later applied to generate and analyse synthetic data (Rubin 1993) without DP. The variant of Rubin's rules that we use, and describe in Supplemental Section B, was developed specifically for synthetic data generation (Raghunathan et al. 2003;Reiter 2002). Raab et al. (2018) have developed simpler alternatives to Rubin's rules under more restrictive assumptions, but these assumptions rule out DP data synthesisers. \n\nRubin's rules have not been widely used with DP synthetic data generation, and we are only aware of four existing works studying the combination.",
            "score": 0.35383724562520563,
            "section_title": "Related Work",
            "char_start_offset": 4814,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 62
                },
                {
                    "start": 63,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1162
                },
                {
                    "start": 1165,
                    "end": 1386
                },
                {
                    "start": 1387,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1725
                },
                {
                    "start": 1728,
                    "end": 1873
                }
            ],
            "ref_mentions": [
                {
                    "start": 227,
                    "end": 247,
                    "matchedPaperCorpusId": "232185574"
                },
                {
                    "start": 247,
                    "end": 269,
                    "matchedPaperCorpusId": "2334513"
                },
                {
                    "start": 269,
                    "end": 285,
                    "matchedPaperCorpusId": "237012543"
                },
                {
                    "start": 285,
                    "end": 305,
                    "matchedPaperCorpusId": "15417421"
                },
                {
                    "start": 305,
                    "end": 323,
                    "matchedPaperCorpusId": "947901"
                },
                {
                    "start": 323,
                    "end": 342,
                    "matchedPaperCorpusId": "235421632"
                },
                {
                    "start": 342,
                    "end": 362,
                    "matchedPaperCorpusId": "51869463"
                },
                {
                    "start": 362,
                    "end": 382,
                    "matchedPaperCorpusId": "236976348"
                },
                {
                    "start": 382,
                    "end": 402,
                    "matchedPaperCorpusId": "246430835"
                },
                {
                    "start": 402,
                    "end": 422,
                    "matchedPaperCorpusId": "59317012"
                },
                {
                    "start": 422,
                    "end": 440,
                    "matchedPaperCorpusId": "246275811"
                },
                {
                    "start": 440,
                    "end": 459,
                    "matchedPaperCorpusId": "211203580"
                },
                {
                    "start": 459,
                    "end": 477,
                    "matchedPaperCorpusId": "13095103"
                },
                {
                    "start": 575,
                    "end": 592,
                    "matchedPaperCorpusId": "219687936"
                },
                {
                    "start": 610,
                    "end": 627,
                    "matchedPaperCorpusId": "245634703"
                },
                {
                    "start": 643,
                    "end": 660,
                    "matchedPaperCorpusId": "53342261"
                },
                {
                    "start": 813,
                    "end": 833,
                    "matchedPaperCorpusId": "225077562"
                },
                {
                    "start": 833,
                    "end": 850,
                    "matchedPaperCorpusId": "235377198"
                },
                {
                    "start": 950,
                    "end": 971,
                    "matchedPaperCorpusId": "59317012"
                },
                {
                    "start": 1285,
                    "end": 1297,
                    "matchedPaperCorpusId": "63858430"
                },
                {
                    "start": 1526,
                    "end": 1551,
                    "matchedPaperCorpusId": "118255787"
                },
                {
                    "start": 1551,
                    "end": 1563,
                    "matchedPaperCorpusId": "26911794"
                },
                {
                    "start": 1565,
                    "end": 1583,
                    "matchedPaperCorpusId": "36523940"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.900390625
        },
        {
            "corpus_id": "246823249",
            "title": "End-to-end Algorithm Synthesis with Recurrent Networks: Logical Extrapolation Without Overthinking",
            "text": "Machine learning systems perform well on pattern matching tasks, but their ability to perform algorithmic or logical reasoning is not well understood. One important reasoning capability is algorithmic extrapolation, in which models trained only on small/simple reasoning problems can synthesize complex strategies for large/complex problems at test time. Algorithmic extrapolation can be achieved through recurrent systems, which can be iterated many times to solve difficult reasoning problems. We observe that this approach fails to scale to highly complex problems because behavior degenerates when many iterations are applied -- an issue we refer to as\"overthinking.\"We propose a recall architecture that keeps an explicit copy of the problem instance in memory so that it cannot be forgotten. We also employ a progressive training routine that prevents the model from learning behaviors that are specific to iteration number and instead pushes it to learn behaviors that can be repeated indefinitely. These innovations prevent the overthinking problem, and enable recurrent systems to solve extremely hard extrapolation tasks.",
            "score": 0.3537606440614319,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53271484375
        },
        {
            "corpus_id": "272524149",
            "title": "Generalization vs. Memorization in the Presence of Statistical Biases in Transformers",
            "text": "To address this question, we conducted empirical evaluations across different synthetic tasks, and, models of varying complexity, systematically controlling the amount of statistical bias present. \n\nThe tasks are solvable by straightforward algorithms but still require significant generalization by the model. These tasks present several challenges: (1) assessing reasoning capabilities necessary to solve the tasks (i.e., algorithmic generalization), (2) evaluating generalization beyond the context length present in the training data (i.e., extrapolation), and, (3) examining robustness against statistical biases.",
            "score": 0.3537469355029581,
            "section_title": "Introduction",
            "char_start_offset": 1781,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 199,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 618
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5087890625
        },
        {
            "corpus_id": "254877131",
            "title": "Optimization Techniques for Unsupervised Complex Table Reasoning via Self-Training Framework",
            "text": "Supervised models tend to show powerful results in an ideal environment where sufficient high-quality data is available. Unfortunately, we often face situations with a limited amount of labeled data or no labeled data in the real world, under which the model's performance suffers a severe decline inevitably. This dilemma leads to the research direction of unsupervised data generation, aiming to synthesize human-like training instances [33]. \n\nFormally, for tabular reasoning tasks, supervised models assume labeled training data \n\nwhere n is the number of training instances. But under unsupervised settings, we only have \n\nas available information, where t i , p i , l i , and o i are an unlabeled table, the related text, a natural language question/claim, and the corresponding golden label, respectively. The data generation method tries to reconstruct a synthetic training dataset \n\n\u2032 n )} using these raw tables and texts. Based on this synthetic dataset, supervised models can be applied successfully. \n\nHowever, the distribution of the generated data in the above manner may have a significant gap from the distribution of questions/claims from real users. Therefore, we can adopt a relaxed but more practical unsupervised data generation setting: we have \n\nas available information. In the subsequent experiments, we demonstrate that, guided by the information of real questions/claims, the model can achieve better performance on real test data.",
            "score": 0.3530914622530935,
            "section_title": "B. Unsupervised Data Generation.",
            "char_start_offset": 11746,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 444
                },
                {
                    "start": 447,
                    "end": 532
                },
                {
                    "start": 535,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 625
                },
                {
                    "start": 628,
                    "end": 812
                },
                {
                    "start": 813,
                    "end": 889
                },
                {
                    "start": 892,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1012
                },
                {
                    "start": 1015,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1267
                },
                {
                    "start": 1270,
                    "end": 1295
                },
                {
                    "start": 1296,
                    "end": 1459
                }
            ],
            "ref_mentions": [
                {
                    "start": 439,
                    "end": 443,
                    "matchedPaperCorpusId": "17287557"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79443359375
        },
        {
            "corpus_id": "273403575",
            "title": "MIND: Math Informed syNthetic Dialogues for Pretraining LLMs",
            "text": "By leveraging MIND with seven conversational prompts and the raw OWM-4B, we generate a new corpus of 43 billion tokens (All Conversations). Additionally, employing the entire OWM-14B dataset and TWO STUDENTS conversation style, MIND produces an additional 21 billion tokensresulting in a total of 64 billion tokens. This underscores MIND's potential to generate vast amount of high-quality data from relatively limited source material3 . \n\nPerformance across Individual Prompt Style. We observe the effect of each conversation style by generating synthetic data with seven prompts for a smaller subset of OpenWebMath, denoted as OWM-4B. To establish a baseline, we continue pretraining C using D = {R \u222a R pt }, where R \u2208 OWM-4B. To further assess the significance of MIND over other synthetic data generation approach, we add another baseline 'Rephrase' introduced by Maini et al. (2024). We generate rephrases with M using the highest performing prompt from their paper to maintain consistency among generation quality and training setup. We continuously train C with D where R \u2208 Rephrase-OWM-4B. In subsequent experiments, we replace R with S \u2032 where S \u2032 = MIND-OWM-4B, corresponding to a particular conversation style, and repeat the training. To assess the utility of combining multiple conversations, we create a new dataset by selecting the longest conversation for each context from the seven generated conversations, labeling it as the LONGEST CONVERSATION dataset. \n\nAs shown in Table 1, models trained on MIND-generated data of individual styles consistently outperform those trained on rephrased or raw data across all reasoning tasks. Specifically, models trained on synthetic data exhibit significant improvements in mathematical reasoning compared to the baseline, achieving absolute gains ranging from 4.78% to 12.82% on GSM8K, 0.54% to 1.28% on MATH, and 0.79% to 4.28% on MMLU-STEM. In specialized knowledge tasks such as MMLU, synthetic data leads to improvements ranging from 1.08% to 4.55%. Furthermore, synthetic data yields an overall enhancement in general reasoning ability, with up to a 2% absolute average improvement across the ten reasoning tasks.",
            "score": 0.3529271980144235,
            "section_title": "EXPERIMENTS AND RESULTS",
            "char_start_offset": 17208,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 437
                },
                {
                    "start": 440,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 636
                },
                {
                    "start": 637,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1473
                },
                {
                    "start": 1476,
                    "end": 1646
                },
                {
                    "start": 1647,
                    "end": 1899
                },
                {
                    "start": 1900,
                    "end": 2010
                },
                {
                    "start": 2011,
                    "end": 2175
                }
            ],
            "ref_mentions": [
                {
                    "start": 868,
                    "end": 887,
                    "matchedPaperCorpusId": "267312030"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7919921875
        },
        {
            "corpus_id": "257607791",
            "title": "Hybrid synthetic data generation pipeline that outperforms real data",
            "text": "4][15] We therefore exploit advances in open-source plugins and game engines, combined with unique techniques in synthetic data generation to perform state-of-the-art synthetic to real transfer learning, and make the following contributions. \n\n1. We present a synthetic data generation framework with an approach of introducing background complexity to synthetic images, in addition to the ability to programmatically vary rotation, lighting, backgrounds, and scale, making the resulting classifier very robust. We have made our framework publicly available (https://github.com/saiabinesh/hybrid-synth), which can be used to generate a dataset with any number of arbitrary classes. The dataset used for this work can also be downloaded to reproduce our experiments directly. 16 . We test the efficacy of the collected synthetic data on a set of classes from the challenging ObjectNet dataset 2 and demonstrate that fine-tuning with synthetic data can outperform fine-tuning with real photographs. 3. We evaluate the effect of various parameters in the synthetic data generation pipeline through ablation studies. 4. We present an efficient learning rate (LR) tuning strategy that is robust to covariate shift, helps set the LR 75\u00d7 faster and converges 10\u00d7 faster compared to regular grid search. \n\n2 Related Research",
            "score": 0.3529039045611988,
            "section_title": ".3.1).",
            "char_start_offset": 1878,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 241
                },
                {
                    "start": 244,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1112
                },
                {
                    "start": 1113,
                    "end": 1295
                },
                {
                    "start": 1298,
                    "end": 1316
                }
            ],
            "ref_mentions": [
                {
                    "start": 2,
                    "end": 6,
                    "matchedPaperCorpusId": "6383532"
                },
                {
                    "start": 892,
                    "end": 893,
                    "matchedPaperCorpusId": "202777185"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90283203125
        },
        {
            "corpus_id": "262217162",
            "title": "Can LLMs Augment Low-Resource Reading Comprehension Datasets? Opportunities and Challenges",
            "text": "Language models have played a key role in the creation of synthetic datasets for various NLP tasks. Models such as GPT-2 (Radford et al., 2019) and CTRL (Keskar et al., 2019) have been applied to areas including general language understanding (Meng et al., 2022;He et al., 2022), classification (Kumar et al., 2020;Anaby-Tavor et al., 2019), dialogue tasks (Mohapatra et al., 2021), commonsense reasoning (Yang et al., 2020), and relation extraction (Papanikolaou and Pierleoni, 2020), among others. Recently, large language models have significantly improved the quality and scope of synthetic dataset generation. They have been instrumental in augmenting datasets for tasks such as NLI and sentiment analysis (Dixit et al., 2022), classification (Yoo et al., 2021), and even creating datasets for personalized dialogue generation (Lee et al., 2022), hate speech detection (Hartvigsen et al., 2022), and textual similarity (Schick and Sch\u00fctze, 2021) to name a few. \n\nMost prior work in synthetic data generation for QA (Riabi et al., 2021;Chakravarti et al., 2020;Du and Cardie, 2018;Alberti et al., 2019) has concentrated on generating questions from Wikipedia passages to produce supplementary training examples. More recently, Kalpakchi and Boye introduced the use of GPT-3 for creating extra training data for Swedish multiple choice questions. Our approach is the first to utilize in-context learning with LLMs for synthesizing contexts, questions, and answers for low-resource MRC.",
            "score": 0.35276337044879286,
            "section_title": "Related Work",
            "char_start_offset": 2884,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 100,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 965
                },
                {
                    "start": 968,
                    "end": 1215
                },
                {
                    "start": 1216,
                    "end": 1349
                },
                {
                    "start": 1350,
                    "end": 1488
                }
            ],
            "ref_mentions": [
                {
                    "start": 243,
                    "end": 262,
                    "matchedPaperCorpusId": "246680398"
                },
                {
                    "start": 295,
                    "end": 315,
                    "matchedPaperCorpusId": "211987786"
                },
                {
                    "start": 357,
                    "end": 381,
                    "matchedPaperCorpusId": "239016412"
                },
                {
                    "start": 405,
                    "end": 424,
                    "matchedPaperCorpusId": "250390686"
                },
                {
                    "start": 711,
                    "end": 730,
                    "matchedPaperCorpusId": "252780515"
                },
                {
                    "start": 832,
                    "end": 850,
                    "matchedPaperCorpusId": "252819096"
                },
                {
                    "start": 924,
                    "end": 950,
                    "matchedPaperCorpusId": "233241169"
                },
                {
                    "start": 1020,
                    "end": 1040,
                    "matchedPaperCorpusId": "225067135"
                },
                {
                    "start": 1040,
                    "end": 1065,
                    "matchedPaperCorpusId": "227231636"
                },
                {
                    "start": 1065,
                    "end": 1085,
                    "matchedPaperCorpusId": "21702856"
                },
                {
                    "start": 1085,
                    "end": 1106,
                    "matchedPaperCorpusId": "189762081"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86767578125
        },
        {
            "corpus_id": "268531976",
            "title": "RankPrompt: Step-by-Step Comparisons Make Language Models Better Reasoners",
            "text": "We conduct experiments with gpt-3.5-turboacross 8 widely-used reasoning tasks, spanning arithmetic, commonsense, and symbolic reasoning.For arithmetic reasoning, we use 4 math word problem datasets: AQUA-RAT (Ling et al., 2017), ASDiv (Miao et al., 2020), GSM8K (Cobbe et al., 2021), and SVAMP (Patel et al., 2021).For commonsense reasoning, which requires multi-step problem-solving, we utilize ARC Challenge (Clark et al., 2018), CommonsenseQA (Talmor et al., 2019), and StrategyQA (Geva et al., 2021).We evaluate symbolic reasoning with the Last Letter Concatenation task (Wei et al., 2022b).Given the high API cost 2 , we reserve gpt-4 for 3 challenging reasoning tasks from BIG-Bench-Hard (Suzgun et al., 2023) Ranking Setups.We leverage language models to rank their outputs.For each task, a task-specific comparison exemplar is generated using the same model utilized for candidate generation.These exemplars systematically evaluate 5 unique candidate responses, ultimately guiding models to the correct answer.Following this, we integrate these exemplars into the ranking template, as detailed in Table 2. Despite the diverse nature of tasks, we maintain a uniform application of comparison instructions and task-specific exemplars, introducing minor modifications to the output format depending on the task type.We restrict our use of comparison exemplars to a single one, as our findings suggest that increasing the number of exemplars has a negligible effect on improving performance but significantly extends the input, often exceeding the maximum length limit of OpenAI models.In Section 5, we conduct a comprehensive examination of how various facets of comparison exemplars influence the final performance.\n\nBaselines.We compare our methods with 4 baseline methods: CoT Prompting (Wei et al., 2022b), Majority Voting (Wang et al., 2023d), Direct Scoring (Zheng et al., 2023)",
            "score": 0.3527596872817393,
            "section_title": "Tasks and Datasets.",
            "char_start_offset": 15815,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 41
                },
                {
                    "start": 41,
                    "end": 136
                },
                {
                    "start": 136,
                    "end": 315
                },
                {
                    "start": 315,
                    "end": 504
                },
                {
                    "start": 504,
                    "end": 595
                },
                {
                    "start": 595,
                    "end": 731
                },
                {
                    "start": 731,
                    "end": 781
                },
                {
                    "start": 781,
                    "end": 900
                },
                {
                    "start": 900,
                    "end": 1018
                },
                {
                    "start": 1018,
                    "end": 1321
                },
                {
                    "start": 1321,
                    "end": 1590
                },
                {
                    "start": 1590,
                    "end": 1721
                },
                {
                    "start": 1723,
                    "end": 1733
                },
                {
                    "start": 1733,
                    "end": 1889
                }
            ],
            "ref_mentions": [
                {
                    "start": 208,
                    "end": 227,
                    "matchedPaperCorpusId": "12777818"
                },
                {
                    "start": 294,
                    "end": 314,
                    "matchedPaperCorpusId": "232223322"
                },
                {
                    "start": 446,
                    "end": 467,
                    "matchedPaperCorpusId": "53296520"
                },
                {
                    "start": 484,
                    "end": 503,
                    "matchedPaperCorpusId": "230799347"
                },
                {
                    "start": 694,
                    "end": 715,
                    "matchedPaperCorpusId": "252917648"
                },
                {
                    "start": 1832,
                    "end": 1852,
                    "matchedPaperCorpusId": "247595263"
                },
                {
                    "start": 1869,
                    "end": 1889,
                    "matchedPaperCorpusId": "259129398"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.309326171875
        },
        {
            "corpus_id": "271097404",
            "title": "Skywork-Math: Data Scaling Laws for Mathematical Reasoning in Large Language Models - The Story Goes On",
            "text": "We aim to answer the following question: as we gradually increase the quantity  of the Skywork-MathQA dataset, does the models' math reasoning ability improve correspondingly? For a given query/problem x  , particularly the challenging competition-level math problems, manually annotating the response/answer y  is time-consuming and often infeasible for non-experts due to the required specific domain knowledge. Therefore, we utilize the top-performing GPT-4 models to synthesize diverse, high-quality SFT data (Li et al., 2024). \n\nThe data synthesis process in the Skywork-MathQA dataset consists of two stages. In stage 1, we generate 2.1 million normal synthetic problems. In stage 2, we further generate 0.4 million hard synthetic problems, increasing the Skywork-MathQA dataset to a total of 2.5 million instances. Note that all data samples in the Skywork-MathQA dataset strictly adhere to the",
            "score": 0.35266775544437384,
            "section_title": "Synthesis Process.",
            "char_start_offset": 15637,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 531
                },
                {
                    "start": 534,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 901
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4873046875
        },
        {
            "corpus_id": "267027689",
            "title": "Adapting Large Language Models for Education: Foundational Capabilities, Potentials, and Challenges",
            "text": "This innovation underscored the model's inherent capability to generate deductive pathways, significantly enhancing its applicability and versatility in problem-solving scenarios without extensive domain-specific adaptations. Similarly, Wang et al. [133] introduced a self-consistency strategy that enhances model performance by sampling various reasoning paths and selecting the most consistent answer. This approach diversified the exploration of reasoning strategies. It ensured the conclusions' reliability, showcasing an innovative way to leverage the model's capabilities for improved decision-making and problem-solving across different contexts. Confronting the limitations of relying on static, manually annotated demonstrations, which can restrict the adaptability of LLMs to the varying complexities of real-world tasks, Diao et al. [23] introduced an active selection approach. This technique dynamically pinpointed the most pertinent demonstrations aligned with the specific demands of a task from a broad set of queries. In this way, the approach enhanced the flexibility and effectiveness of LLMs in adapting to diverse and evolving problem contexts. Concurrently, Zhou et al. [162] devised a prompting methodology that broke down intricate problems into their simpler constituent sub-problems. This tactic not only promoted a step-by-step problem-solving process but also hold promise for augmenting the efficacy of LLMs in handling complex tasks. \n\nBuilding upon the CoT methodology, subsequent developments have introduced more sophisticated frameworks for enhancing the reasoning capabilities of LLMs. The Tree of Thoughts (ToT) [147] framework extended CoT by enabling LLMs to explore multiple reasoning paths through a hierarchical structure, thereby improving decision-making for tasks requiring strategic planning. Following the ToT, the Boosting of Thoughts (BoT) [17] framework introduced a novel approach by iteratively exploring and self-evaluating multiple trees of thoughts. This process accumulated an ensemble of trial-and-error reasoning experiences, offering a new form of prompting designed to tackle complex problems. Starting with simple prompts, BOT iteratively refined reasoning steps through error analysis, significantly improving the generation of reasoning paths and achieving higher problem-solving rates on complex tasks than existing advanced prompting strategies. To structure thoughts through prompts without depending on fine-tuning, the Graph of Thoughts (GoT) framework [9] introduced a new angle by arranging thoughts generated by LLMs into a graph structure.",
            "score": 0.35260444032451466,
            "section_title": "Supervised Fine-tuning for Reasoning. Previous studies have primarily focused on fully supervised fine-tuning",
            "char_start_offset": 49852,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 1034
                },
                {
                    "start": 1035,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1463
                },
                {
                    "start": 1466,
                    "end": 1620
                },
                {
                    "start": 1621,
                    "end": 1837
                },
                {
                    "start": 1838,
                    "end": 2003
                },
                {
                    "start": 2004,
                    "end": 2152
                },
                {
                    "start": 2153,
                    "end": 2409
                },
                {
                    "start": 2410,
                    "end": 2610
                }
            ],
            "ref_mentions": [
                {
                    "start": 1648,
                    "end": 1653,
                    "matchedPaperCorpusId": "258762525"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.37353515625
        },
        {
            "corpus_id": "277824142",
            "title": "HypoBench: Towards Systematic and Principled Benchmarking for Hypothesis Generation",
            "text": "Students with an A in Math will be admitted, otherwise rejected. Distractor Features: \n\n\u2022 Extracurricular activities \u2022 Legacy Predictive Power Hypothesis Discovery Rate Figure 1: An overview of our benchmark. We curate 194 datasets spanning 7 real-world and 5 synthetic domains. We illustrate how difficulty levels are controlled in our synthetic settings by showing an example from the college admission task. Our evaluation measures explanatory power and interestingness of generate hypotheses. ideation primarily aims to generate new research directions from existing scientific literature. An example is proposing an alternative architecture to transformer. Ideation, especially in AI research, is often not about explaining a phenomenon and has a strong emphasis on differing from the existing literature. Recognizing this difference, our benchmark thus focuses primarily on curating observations about phenomena of interest. \n\nSecond, what capabilities need to be benchmarked for hypothesis generation? Given our focus on explaining an observed phenomenon, hypothesis generation builds on the following capabilities: 1) inductive reasoning, 2) abstraction and communication, and optionally 3) synthesis, integrating new observations with existing knowledge. Inductive reasoning is necessary for proposing possible theories for a given observed phenomenon. Abstraction and communication is necessary for expressing hypotheses in natural language that humans can comprehend and appreciate. When existing literature is available, synthesis allows the models to build on relevant information. We would like our dataset to capture the complexity and diversity of hypothesis generation across different domains. In particular, we build synthetic datasets that enables arbitrary control of complexity. \n\nThird, how can we evaluate hypothesis generation? Existing work (Radensky et al., 2025) tends to conflate explanatory power with novelty for hypothesis generation because it is natural for scientists to expect hypotheses to contribute to scientific advances (hence novel). However, this is not necessarily required in many other settings, e.g., proposing reasons why one did not get admitted to college. Therefore, we argue that explanatory power of hypotheses should be the first-order consideration. Interestingness and contributions to the existing literature are separate considerations, and are often subjective. 1 We focus on operationalizing explanatory power and provide preliminary measurements of \"interestingness\". \n\nPreprint.",
            "score": 0.35221769613826753,
            "section_title": "Add distractor features:",
            "char_start_offset": 2235,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 64
                },
                {
                    "start": 65,
                    "end": 85
                },
                {
                    "start": 88,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 810
                },
                {
                    "start": 811,
                    "end": 930
                },
                {
                    "start": 933,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1594
                },
                {
                    "start": 1595,
                    "end": 1711
                },
                {
                    "start": 1712,
                    "end": 1800
                },
                {
                    "start": 1803,
                    "end": 1852
                },
                {
                    "start": 1853,
                    "end": 2075
                },
                {
                    "start": 2076,
                    "end": 2206
                },
                {
                    "start": 2207,
                    "end": 2304
                },
                {
                    "start": 2305,
                    "end": 2528
                },
                {
                    "start": 2531,
                    "end": 2540
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07122802734375
        },
        {
            "corpus_id": "278327588",
            "title": "Recursive Decomposition with Dependencies for Generic Divide-and-Conquer Reasoning",
            "text": "The hypotheses we aim to validate through our empirical study are the following: \n\n\u2022 Hypothesis 1: RDD increases accuracy in complex reasoning problems over state-of-the-art methods in a compute-matched setting. \u2022 Hypothesis 2: The recursive decomposition technique augments the model's reasoning abilities even in the absence of task-specific data. \u2022 Hypothesis 3: Solving reasoning problems via RDD reduces the time taken to reach a solution compared to solving the entire problem via step-by-step prompting strategies. \n\n\u2022 Hypothesis 4: RDD reduces the average amount of tokens per generation process, thus lessening the strain on the context window. \n\nAs baselines, we consider Chain-of-Thought (CoT; Wei et al. (2022)) and Least-to-Most prompting (LtM; Zhou et al. (2022)). We use self-consistency (SC; (Wang et al., 2022b)) to align the amount of computation between our method and the baselines. In our implementation of SC, we employ the LLM itself to decide the most consistent answer given the set of sampled solutions. The first solution candidate is sampled greedily, while the rest are sampled with a temperature of 0.7 to produce a variety of reasoning chains. Given that each SC sample will produce a large number of generated tokens and including them all in a single context window can be challenging, we propose to use binary search to find the most consistent answer. RDD employs a single CoT or LtM chain at the unit-solving prompt but does not use self-consistency to aggregate multiple answers. We evaluate all methods on benchmark tasks of increasing difficulty. For each difficulty, scores are averaged across the same 100 randomly sampled problem instances for all methods. We employed the instruction-tuned Llama 3 70B (Meta, 2024) as the underlying model. This model was run on NVIDIA A100 and H100 GPUs. \n\nApp. E provides resource usage statistics for all experiments in this section. It is additionally possible to parallelize the solving of independent sub-problems for a speedup.",
            "score": 0.3516805651684923,
            "section_title": "Empirical Evaluation",
            "char_start_offset": 10503,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 83,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 521
                },
                {
                    "start": 524,
                    "end": 653
                },
                {
                    "start": 656,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1386
                },
                {
                    "start": 1387,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1698
                },
                {
                    "start": 1699,
                    "end": 1782
                },
                {
                    "start": 1783,
                    "end": 1831
                },
                {
                    "start": 1834,
                    "end": 1912
                },
                {
                    "start": 1913,
                    "end": 2010
                }
            ],
            "ref_mentions": [
                {
                    "start": 705,
                    "end": 722,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 758,
                    "end": 776,
                    "matchedPaperCorpusId": "248986239"
                },
                {
                    "start": 808,
                    "end": 828,
                    "matchedPaperCorpusId": "247595263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2705078125
        },
        {
            "corpus_id": "273403996",
            "title": "A Little Human Data Goes A Long Way",
            "text": "We study a synthetic data generation pipeline representative of the methods used in the FV (Ni et al., 2024;He et al., 2023) and QA (Schimanski et al., 2024;Wan et al., 2024)  In total, we use 4 FV/NLI datasets: FEVER (Thorne et al., 2018), SciFact (Wadden et al., 2020), WANLI (Liu et al., 2022) and FACTIFY1.0 (Mishra et al., 2022), as well as 4 QA datasets: ROPES (Lin et al., 2019), CoQA (Reddy et al., 2019), QAConv (Wu et al., 2021) and FairyTaleQA (Xu et al., 2022). Together, the datasets span a variety of domains (science, news, social media, reasoning, conversation, fiction). More details, including a discussion on data leakage, can be found in Appendix C. \n\nFV performance is measured by test accuracy, while QA is measured using Exact Match, String Inclusion, BLEU (Papineni et al., 2002), ROUGE-1 (Lin, 2004) and BERTScore (Zhang* et al., 2020). Evaluation is always conducted on the (humangenerated) test split of each dataset. \n\nWe use GPT-3.5-Turbo (Brown, 2020) for prompting and LoRA (Hu et al., 2022) on Llama3-8B (Dubey et al., 2024) for fine-tuning. Implementation details are provided below (Appendix E).1 \n\n3 Can Synthetic Data Replace Humans? \n\nWe investigate the potential of synthetic data to replace human annotation by holding the number of training data points fixed, incrementally increasing the proportion of the data that is synthetic, and fine-tuning a model on each training set. \n\nResults: Across all datasets, using purely synthetic data typically leads to worse performance than the same amount of real data (Figure 1). This confirms that despite advances in synthetic generation, human annotation yields more useful data. \n\nThe performance decline is not uniform as we increase the synthetic proportion.",
            "score": 0.3514475446558039,
            "section_title": "Synthetic Data Generation from Evidence Texts",
            "char_start_offset": 5129,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 669
                },
                {
                    "start": 672,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 944
                },
                {
                    "start": 947,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1130
                },
                {
                    "start": 1133,
                    "end": 1169
                },
                {
                    "start": 1172,
                    "end": 1416
                },
                {
                    "start": 1419,
                    "end": 1559
                },
                {
                    "start": 1560,
                    "end": 1662
                },
                {
                    "start": 1665,
                    "end": 1744
                }
            ],
            "ref_mentions": [
                {
                    "start": 91,
                    "end": 108,
                    "matchedPaperCorpusId": "267750827"
                },
                {
                    "start": 108,
                    "end": 124,
                    "matchedPaperCorpusId": "252907242"
                },
                {
                    "start": 132,
                    "end": 157,
                    "matchedPaperCorpusId": "267636562"
                },
                {
                    "start": 218,
                    "end": 239,
                    "matchedPaperCorpusId": "4711425"
                },
                {
                    "start": 249,
                    "end": 270,
                    "matchedPaperCorpusId": "216867133"
                },
                {
                    "start": 278,
                    "end": 296,
                    "matchedPaperCorpusId": "246016339"
                },
                {
                    "start": 312,
                    "end": 333,
                    "matchedPaperCorpusId": "252016186"
                },
                {
                    "start": 367,
                    "end": 385,
                    "matchedPaperCorpusId": "201058633"
                },
                {
                    "start": 392,
                    "end": 411,
                    "matchedPaperCorpusId": "52055325"
                },
                {
                    "start": 455,
                    "end": 472,
                    "matchedPaperCorpusId": "247762948"
                },
                {
                    "start": 780,
                    "end": 803,
                    "matchedPaperCorpusId": "11080756"
                },
                {
                    "start": 813,
                    "end": 824,
                    "matchedPaperCorpusId": "964287"
                },
                {
                    "start": 839,
                    "end": 860,
                    "matchedPaperCorpusId": "127986044"
                },
                {
                    "start": 1005,
                    "end": 1022,
                    "matchedPaperCorpusId": "235458009"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63720703125
        },
        {
            "corpus_id": "270702523",
            "title": "Can LLM Graph Reasoning Generalize beyond Pattern Memorization?",
            "text": "\u2022 Graph Size: Small-size graphs have 3 to 10 nodes while large-size graphs have 11 to 25 nodes. \n\n\u2022 Graph Generator: we use two different graph generator algorithms to generate graphs, specifically, Erdos-Renyi (Erd\u0151s et al., 1960) and Barabasi-Albert (Barab\u00e1si and Albert, 1999). \n\n\u2022 Graph Transitivity: we calculate the graph transitivity (Luce and Perry, 1949;Wasserman and Faust, 1994) and partition them into low-transitivity and high-transitivity subsets. \n\nGraph transitivity (T) is calculated as T = 3\u00d7number of triangles number of all triplets . Reasoning Patterns For the previous three patterns, we train and test LLMs on the same graph tasks, while a robust graph reasoner should learn universal principles of graph reasoning to generalize across different graph reasoning problems. To evaluate this, we use the four graph reasoning tasks ( \u00a72), instruction-tune the LLM on one synthetic graph task and evaluate it on both the same task and three other graph tasks. \n\nReal-world Patterns While the previous four patterns all train and test on synthetic graph data, we argue that the ultimate goal of graph synthetic tuning is to benefit real-world problems with underlying graph structures: after all, these synthetic problems could be solved with 100% accuracy by conventional algorithms. Thus for the real-world pattern, we fine-tune LLMs with synthetic graph data, and then evaluate with real-world problems that have implicit graph structures. We specifically employ two types of datasets for evaluation: \n\n\u2022 Multi-Hop QA: Multi-hop QA involves answering questions that require multi-hop reasoning, which is inherently related to synthetic graph problems such as connectivity or shortest path, as the solving process can be viewed as navigating through a network of concepts and relations or trying to find the shortest path between two concepts using existing relations. We adopt Strat-egyQA (Geva et al., 2021) and Knowledge Crosswords (Ding et al., 2023) for the multi-hop QA task. (Saha et al., 2021) and Proscript (Sakaguchi et al., 2021) for this task. \n\nFinally, we obtain the NLGIFT Benchmark with 33,000 synthetic problems and 4,000 realistic problems.",
            "score": 0.3514018128683166,
            "section_title": "Graph reasoning patterns",
            "char_start_offset": 9350,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 98,
                    "end": 280
                },
                {
                    "start": 283,
                    "end": 461
                },
                {
                    "start": 464,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 977
                },
                {
                    "start": 980,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1520
                },
                {
                    "start": 1523,
                    "end": 1887
                },
                {
                    "start": 1888,
                    "end": 2000
                },
                {
                    "start": 2001,
                    "end": 2074
                },
                {
                    "start": 2077,
                    "end": 2177
                }
            ],
            "ref_mentions": [
                {
                    "start": 211,
                    "end": 231,
                    "matchedPaperCorpusId": "6829589"
                },
                {
                    "start": 252,
                    "end": 279,
                    "matchedPaperCorpusId": "524106"
                },
                {
                    "start": 341,
                    "end": 363,
                    "matchedPaperCorpusId": "16186758"
                },
                {
                    "start": 1909,
                    "end": 1928,
                    "matchedPaperCorpusId": "230799347"
                },
                {
                    "start": 2001,
                    "end": 2020,
                    "matchedPaperCorpusId": "233240926"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.368896484375
        },
        {
            "corpus_id": "270094729",
            "title": "Towards Next-Generation Urban Decision Support Systems through AI-Powered Generation of Scientific Ontology using Large Language Models - A Case in Optimizing Intermodal Freight Transportation",
            "text": "The swift evolution of AI technologies has catalyzed the emergence of autonomous systems (Brustoloni, 1991), which are sophisticated computer systems designed to undertake tasks and make decisions autonomously, with little to no human oversight (Wooldridge and Jennings, 1995). These agents have demonstrated significant potential across diverse fields such as robotics, healthcare, transportation, and finance, showcasing benefits like enhanced efficiency, diminished errors, and the capability to manage intricate tasks that might be impractical or exceedingly laborious for humans (Stone et al., 2022). Diverging from automated systems, such as automated irrigation systems and home automation systems, which execute predefined tasks based on specific sets of commands, inputs, rules, and environmental conditions, autonomous systems exhibit higher complexity, better adaptability to their surroundings, and the ability to make informed decisions and content generation from collected data (Albrecht and Stone, 2018). \n\nThe capability of most existing rule-or algorithm-based reasoning cores is confined to certain or closed environments, limiting their reasoning capacity for tasks that involve creating new content or computer codes. With the advancement of generative AI models, LLMs, such as GPT-3 (Brown et al., 2020) and GPT-4 (Sun et al., 2023), has significantly propelled forward the research and development of autonomous systems. These models exhibit a profound comprehension of human natural language, enabling them to adeptly perform tasks across various domains including reasoning, creative writing, code generation, translation, and information retrieval (Li and Ning, 2023). With recent advancements in prompt engineering and tuning techniques, LLMs are increasingly being integrated as the reasoning core to enable AI-powered assistants capable of performing specialized tasks, such as generating content within specific domain contexts, automating code generation, and synthesizing data (Shin et al., 2023). This integration highlights the transformative impact of LLMs in enhancing automated processes and driving innovation across various fields, shedding light on an autonomous approach for generating scientific ontologies.",
            "score": 0.35115682952293703,
            "section_title": "AI-powered Autonomous System",
            "char_start_offset": 15720,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 1020
                },
                {
                    "start": 1023,
                    "end": 1238
                },
                {
                    "start": 1239,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 2029
                },
                {
                    "start": 2030,
                    "end": 2249
                }
            ],
            "ref_mentions": [
                {
                    "start": 245,
                    "end": 276,
                    "matchedPaperCorpusId": "221342993"
                },
                {
                    "start": 993,
                    "end": 1019,
                    "matchedPaperCorpusId": "3212631"
                },
                {
                    "start": 1305,
                    "end": 1325,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1674,
                    "end": 1693,
                    "matchedPaperCorpusId": "258615570"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.244873046875
        },
        {
            "corpus_id": "270560130",
            "title": "Scaling Synthetic Logical Reasoning Datasets with Context-Sensitive Declarative Grammars",
            "text": "The 184M parameters (basesize) beats GPT-4 augmented or not with external theorem provers, on the FOLIO (Han et al., 2022) dataset.Our contributions are as follows: (i) A dataset of reasoning problems expressed in English and TPTP (a language that can be interfaced with numerous theorem provers) alongside Vampire proof annotations, covering FOL with equality and both finite and open domains, improved compositionality, and more extensive quantifiers.(ii) Ablations measuring the effect of constraining material conditionals usage, of using realistic English predicates, and of reimplementing LogicNLI with declarative generation instead of proof tree generation, highlighting that declarative can work better but that a richer logical modeling drives most of the improvement.(iii) A general reasoning problem grammar-based generation framework relying on solvers.The generation library, grammars, models, and generated dataset are publicly available1 .\n\nSynthetic datasets for reasoning Numerous works investigate the logical capabilities of NLP models using textual datasets and symbolic reasoning (Helwe et al., 2022).We focus on the grammarderived synthetic datasets.RuleTaker (Clark et al., 2020) explores this area with a subset of first-order logic.LogicNLI addresses a broader FOL subset (Tian et al., 2021).FLD explores full FOL (Morishita et al., 2023) and increased compositionality.Richardson et al. (2020); Richardson and Sabharwal (2022) use a solver to study the satisfiability in natural language using the Z3 solver and dedicated generation logic on constrained problems.Other work explore non-standard logic with synthetic dataset, notably probabilistic (Sileo and Moens, 2023), paraconsistant (Kazemi et al., 2024), epistemic (Sileo and Lernould, 2023) logics.\n\nGeneration frameworks Multiple frameworks already implement generation from handwritten grammars.NLTK (Bird and Loper, 2004) has a context-free grammar tool, but cannot natively handle multiple languages or large-scale generation.",
            "score": 0.3509754580961078,
            "section_title": "Introduction",
            "char_start_offset": 1668,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 131,
                    "end": 453
                },
                {
                    "start": 453,
                    "end": 778
                },
                {
                    "start": 778,
                    "end": 866
                },
                {
                    "start": 866,
                    "end": 955
                },
                {
                    "start": 957,
                    "end": 1123
                },
                {
                    "start": 1123,
                    "end": 1173
                },
                {
                    "start": 1173,
                    "end": 1258
                },
                {
                    "start": 1258,
                    "end": 1318
                },
                {
                    "start": 1318,
                    "end": 1396
                },
                {
                    "start": 1396,
                    "end": 1590
                },
                {
                    "start": 1590,
                    "end": 1781
                },
                {
                    "start": 1783,
                    "end": 1880
                },
                {
                    "start": 1880,
                    "end": 2013
                }
            ],
            "ref_mentions": [
                {
                    "start": 1102,
                    "end": 1122,
                    "matchedPaperCorpusId": "257696994"
                },
                {
                    "start": 1183,
                    "end": 1203,
                    "matchedPaperCorpusId": "211126663"
                },
                {
                    "start": 1298,
                    "end": 1317,
                    "matchedPaperCorpusId": "243865235"
                },
                {
                    "start": 1340,
                    "end": 1364,
                    "matchedPaperCorpusId": "260846474"
                },
                {
                    "start": 1396,
                    "end": 1420,
                    "matchedPaperCorpusId": "202583828"
                },
                {
                    "start": 1422,
                    "end": 1453,
                    "matchedPaperCorpusId": "245219217"
                },
                {
                    "start": 1674,
                    "end": 1697,
                    "matchedPaperCorpusId": "253383825"
                },
                {
                    "start": 1714,
                    "end": 1735,
                    "matchedPaperCorpusId": "259144942"
                },
                {
                    "start": 1747,
                    "end": 1773,
                    "matchedPaperCorpusId": "258547259"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85302734375
        },
        {
            "corpus_id": "252918216",
            "title": "Comparing Synthetic Tabular Data Generation Between a Probabilistic Model and a Deep Learning Model for Education Use Cases",
            "text": "Although deep learning architectures are popular for the generation of synthetic data, fundamental probabilistic models have a use case in this regard. Furthermore, if synthetic data needs to be generated for the purpose of machine learning, probabilistic models seem to be a better fit for synthetic tabular data generation than generative deep learning models, such as a GAN. This is because the probabilistic models seem to have higher accuracy scores for classification tasks based on our results. We acknowledge that with complex datasets, a lot of the nuance of what we illustrate in this paper will be lost under the complex interpretation of the latent variables that may or may not be present (as with visual, sound or even text data). Furthermore, we acknowledge that there are a variety of probabilistic and deep learning models that still need to be further explored in terms of their contribution to the creation of synthetic data that display variable associations that are important for this type of variable interdependence. It is also relatively easy to insert expert specific knowledge about a probabilistic distribution. We therefore recommend that other models and other data types be further explored for the creation of synthetic data to identify the limitation of both approaches for synthetic data generation used in education.",
            "score": 0.3509754580961078,
            "section_title": "Conclusion",
            "char_start_offset": 11831,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1351
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5888671875
        },
        {
            "corpus_id": "277780529",
            "title": "Executable Functional Abstractions: Inferring Generative Programs for Advanced Math Problems",
            "text": "Past work has generally approached improving models on reasoning tasks like math by generating large amounts of broad-coverage training data. This trend builds on work in generating instruction-tuning data (Wang et al., 2023), where model-generated instructions have been used to teach models to follow prompts. Luo et al. (2023) introduced generation method based on Evol-Instruct (Xu et al., 2023), which augmented a seed dataset of math problems by generating easier and harder problems. Related lines of work have sought to expand datasets by augmenting existing math datasets (Yu et al., 2024), adding multiple reasoning strategies (Yue et al., 2024), covering challenging competition problems (Li et al., 2024), or curating responses (Liu et al., 2024). The data generated in these settings differs from our data in a number of respects: first, it is generally broad-coverage, focusing on large-scale diverse data, as opposed to targeted, instancespecific data. This direction was also explored by Khan et al. (2025), who define data generation agents that can generate specific data based on a particular model's weaknesses, covering math and several other domains. Finally, past work that has augmented a seed dataset (e.g., Yu et al. (2024); Yue et al. (2024)) has done so by modifying problems in the surface form, whereas our method first infers a latent structure and then creates problems by sampling from the structure. In contrast, EFAGen focuses on generating similar examples of existing data by inferring an underlying structure from an example; we show that this has applications to data generation for augmentation but also for stress-testing or measuring the performance gap of models on similar problems.",
            "score": 0.3509754580961078,
            "section_title": "Related Work",
            "char_start_offset": 22724,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1172
                },
                {
                    "start": 1173,
                    "end": 1433
                },
                {
                    "start": 1434,
                    "end": 1726
                }
            ],
            "ref_mentions": [
                {
                    "start": 206,
                    "end": 225,
                    "matchedPaperCorpusId": "254877310"
                },
                {
                    "start": 581,
                    "end": 598,
                    "matchedPaperCorpusId": "262084051"
                },
                {
                    "start": 637,
                    "end": 655,
                    "matchedPaperCorpusId": "266210109"
                },
                {
                    "start": 1004,
                    "end": 1022,
                    "matchedPaperCorpusId": "273228651"
                },
                {
                    "start": 1233,
                    "end": 1249,
                    "matchedPaperCorpusId": "262084051"
                },
                {
                    "start": 1251,
                    "end": 1268,
                    "matchedPaperCorpusId": "266210109"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9248046875
        },
        {
            "corpus_id": "275993778",
            "title": "Examining the Expanding Role of Synthetic Data Throughout the AI Development Pipeline",
            "text": "Advances in generative AI have led to a rapid expansion of the use of synthetic data. We briefly review use cases from the literature and direct readers to the surveys of Jordon et al. [48] and Liu et al. [58] for a comprehensive overview. Within the training stage of the AI pipeline, synthetic data is used to improve generative model capabilities. For instance, researchers have designed synthetic datasets geared to teaching models mathematical reasoning [60,108] and computer programming [41]. Synthetic language-image datasets have been designed to improve the performance of multi-modal applications [57], while datasets of synthetic multilingual question-answer pairs have been used to improve cross-lingual performance [80,84]. Synthetic data is also commonly used for knowledge distillation, which aims to transfer knowledge from a large model to a smaller one [29,39]. Across these varied use cases, synthetic data is positioned as a way to augment real-world datasets and introduce controlled variability [58]. \n\nWithin the evaluation stage, researchers have generated synthetic test cases to evaluate models for characteristics like factual consistency [23] and harmful behavior [77], and have published synthetic benchmark datasets, for instance, for detecting harmful output [36]. Synthetic test cases can take the form of static model prompts or, since auxiliary models can generate responses on the fly, interactive simulations of a user's interactions with a model or system. Beyond creating test cases, auxiliary models are increasingly used to evaluate or score outputs from a primary model [20,26], an idea colloquially known as 'LLM-as-a-judge' [13]. We view this as a form of synthetic data as the scores produced by the auxiliary model take the place of labels that often would have been collected from human evaluators.",
            "score": 0.3509754580961078,
            "section_title": "Synthetic Data Usage in Modern AI Development",
            "char_start_offset": 7933,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 85
                },
                {
                    "start": 86,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 1022
                },
                {
                    "start": 1025,
                    "end": 1295
                },
                {
                    "start": 1296,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1672
                },
                {
                    "start": 1673,
                    "end": 1844
                }
            ],
            "ref_mentions": [
                {
                    "start": 607,
                    "end": 611,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 728,
                    "end": 732,
                    "matchedPaperCorpusId": "225067135"
                },
                {
                    "start": 871,
                    "end": 875,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 1611,
                    "end": 1615,
                    "matchedPaperCorpusId": "258865545"
                },
                {
                    "start": 1615,
                    "end": 1618,
                    "matchedPaperCorpusId": "257766307"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8876953125
        },
        {
            "corpus_id": "277502095",
            "title": "Style over Substance: Distilled Language Models Reason Via Stylistic Replication",
            "text": "Collectively, these contributions deepen our understanding of how language models perform reasoning tasks, and the datasets we release can serve as valuable resources for future research into synthetic data generation and fine-tuning methodologies that explicitly target the relationship between reasoning trace style and substance. Let me double-check to make sure I didn't miss any other perfect square factors. The other factors of 242 are 1, 2, 11, 22, 121, and 242. Among these, the only perfect squares are 1 and 121. \n\nSince 121 is the larger one, I already used that in my factorization. So there are no other perfect squares to consider. \n\nTherefore, the simplified form of sqrt( 242) is 11\u221a2. \n\nEmergent Synthetic",
            "score": 0.3509754580961078,
            "section_title": "Introduction",
            "char_start_offset": 4299,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 523
                },
                {
                    "start": 526,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 646
                },
                {
                    "start": 649,
                    "end": 702
                },
                {
                    "start": 705,
                    "end": 723
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.44287109375
        },
        {
            "corpus_id": "275993778",
            "title": "Examining the Expanding Role of Synthetic Data Throughout the AI Development Pipeline",
            "text": "Data is a critical building block of AI systems [33]. As the tech industry shifts attention and resources towards building large-scale, data-hungry generative models, traditional data sources are being exhausted, giving rise to an intense search for more data [64,81]. However, developing high-quality datasets remains complex and resource-intensive, with substantial costs, time, and expertise required for data collection, annotation, and curation [83]. Increasingly, synthetic data-artificial data generated using models or algorithmic techniques [76]-is seen as an appealing alternative [69]. OpenAI [3], Apple [32,44], Microsoft [2], Google [91], Meta [30], and IBM [43] have all reported using synthetic data in their AI development pipelines or advertised the ability of their models to create synthetic data. Synthetic data is not new. Explorations into data simulation to address data scarcity date back at least to the 1970s [69]. The idea began to gain momentum with the introduction of practical methods for data generation, like generative adversarial networks [28] and variational autoencoders [51]. Throughout much of the past decade, the use of synthetic data was viewed as a way to mitigate concerns about fairness, bias, and privacy. Proponents argued that privacy concerns could be addressed by replacing traditional datasets with differentially private, synthetic alternatives [1,16,49,105] and that model biases could be mitigated by augmenting traditional datasets with simulations of data from underrepresented groups or rare scenarios [46,85]. The latter approach was especially popular for applications like facial recognition, where collecting diverse data can be prohibitively challenging [7,53]. Practitioners have since applied synthetic data in applications within the robotics [42,56], automotive [106], finance [5], and healthcare [63,71] industries, among others. Now, the widespread availability of large, off-the-shelf generative models has given practitioners the ability to automate the creation of synthetic data without requiring extensive domain-specific expertise or custom-built tools. This has made synthetic data more accessible and rapidly expanded the scope of its use in AI development [48,58].",
            "score": 0.3509754580961078,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 53
                },
                {
                    "start": 54,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1567
                },
                {
                    "start": 1568,
                    "end": 1723
                },
                {
                    "start": 1724,
                    "end": 1896
                },
                {
                    "start": 1897,
                    "end": 2127
                },
                {
                    "start": 2128,
                    "end": 2241
                }
            ],
            "ref_mentions": [
                {
                    "start": 48,
                    "end": 52,
                    "matchedPaperCorpusId": "14300215"
                },
                {
                    "start": 450,
                    "end": 454,
                    "matchedPaperCorpusId": "231829607"
                },
                {
                    "start": 1074,
                    "end": 1078,
                    "matchedPaperCorpusId": "1033682"
                },
                {
                    "start": 1397,
                    "end": 1400,
                    "matchedPaperCorpusId": "58015294"
                },
                {
                    "start": 1400,
                    "end": 1403,
                    "matchedPaperCorpusId": "75136844"
                },
                {
                    "start": 1403,
                    "end": 1406,
                    "matchedPaperCorpusId": "53342261"
                },
                {
                    "start": 1559,
                    "end": 1563,
                    "matchedPaperCorpusId": "216642223"
                },
                {
                    "start": 1563,
                    "end": 1566,
                    "matchedPaperCorpusId": "50786007"
                },
                {
                    "start": 1716,
                    "end": 1719,
                    "matchedPaperCorpusId": "252735071"
                },
                {
                    "start": 1719,
                    "end": 1722,
                    "matchedPaperCorpusId": "198183828"
                },
                {
                    "start": 1812,
                    "end": 1815,
                    "matchedPaperCorpusId": "252355542"
                },
                {
                    "start": 1828,
                    "end": 1833,
                    "matchedPaperCorpusId": "218571224"
                },
                {
                    "start": 1843,
                    "end": 1846,
                    "matchedPaperCorpusId": "226858255"
                },
                {
                    "start": 1863,
                    "end": 1867,
                    "matchedPaperCorpusId": "251480446"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8525390625
        },
        {
            "corpus_id": "250526626",
            "title": "Language models show human-like content effects on reasoning",
            "text": "Creating datasets While many of these tasks have been extensively studied in cognitive science, the stimuli used in cognitive experiments are often online, and thus may be present in the training data of large language models, which could compromise results (e.g. Emami et al., 2020;Dodge et al., 2021). To reduce these concerns, we generate new datasets, by following the approaches used in prior work. We briefly outline this process here; see Appx. A.1 for full details. \n\nFor each of the three tasks above, we generate multiple versions of the task stimuli. Throughout, the logical structure of the stimuli remains fixed, we simply manipulate the entities over which this logic operates (Fig. 2). We generate propositions that are: Consistent with human beliefs and knowledge (e.g. ants are smaller than whales). Violate beliefs by inverting the consistent statements (e.g. whales are smaller than ants). Nonsense tasks about which the model should not have strong beliefs, by swapping the entities out for nonsense words (e.g. kleegs are smaller than feps). \n\nFor the Wason tasks, we slightly alter our approach to fit the different character of the tasks. We generate questions with: Realistic rules involving plausible relationships (e.g. \"if the passengers are traveling outside the US, then they must have shown a passport\"). Arbitrary rules (e.g. \"if the cards have a plural word, then they have a positive emotion\"). Nonsense rules relating nonsense words (\"if the cards have more bem, then they have less stope\"). However, there are other ways that a rule can be unrealistic. The component propositions can be realistic even if the relationship between them is not. We therefore generate two control variations on realistic rules: Shuffled realistic rules, which combine realistic",
            "score": 0.3509754580961078,
            "section_title": "Methods",
            "char_start_offset": 12689,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 473
                },
                {
                    "start": 476,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 1031
                },
                {
                    "start": 1032,
                    "end": 1062
                },
                {
                    "start": 1065,
                    "end": 1161
                },
                {
                    "start": 1162,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1334
                },
                {
                    "start": 1335,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1427
                },
                {
                    "start": 1428,
                    "end": 1525
                },
                {
                    "start": 1526,
                    "end": 1587
                },
                {
                    "start": 1588,
                    "end": 1677
                },
                {
                    "start": 1678,
                    "end": 1792
                }
            ],
            "ref_mentions": [
                {
                    "start": 283,
                    "end": 302,
                    "matchedPaperCorpusId": "237568724"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28857421875
        },
        {
            "corpus_id": "273507627",
            "title": "Creativity in AI: Progresses and Challenges",
            "text": "These tests require the participants to identify and complete an abstract visual pattern based on given examples. Although several attempts have been made to solve these tasks using both symbolic-based and neural network-driven approaches [Hu et al. 2023;Lorello et al. 2024;Mirchandani et al. 2023;Santoro et al. 2018;Xu et al. 2022], modern AI systems still struggle with solving RAVEN-like [Ahrabian et al. 2024;Gendron et al. 2023;Odouard and Mitchell 2022;Zhang et al. 2019a] and ARC-like tasks [Mitchell et al. 2023;Moskvichev et al. 2023;Odouard and Mitchell 2022;Xu et al. 2023;Zhang et al. 2021]. Analysis of abstraction via a serial reproduction task [Langlois et al. 2021] where participants are asked to produce a textual stimulus for the next participant upon observing a visual stimulus and vice versa, has suggested that GPT-4 unlike humans relies heavily on linguistic representations even in vision-only paradigm [Kumar et al. 2024]. Figure 5 illustrates an example from the ARC task [Chollet 2019]. The problems in this corpus are quite hard to solve to the extent that this task has been recognized as the de facto benchmark for measuring progress towards Artificial General Intelligence (AGI) and a public competition with a grand prize of $1, 000, 000 has recently been launched2 . At the time of writing this paper, the highest score is 49.5% far from the passing threshold of 85% (human-level). \n\nAnalogy-Making. In its basic form, analogy-making is the ability to identify a relation between two concepts and apply it to a new concept. For example, Paris is to France as Tokyo is to Japan (i.e. capital:country relation). Early approaches to computational analogy-making were symbolic-based and required extensive hand-coded input i.e. structured representations of both the entities and their relations [Falkenhainer et al. 1989;Gentner 1983;Turney 2008].",
            "score": 0.3509754580961078,
            "section_title": "Creative Problem-Solving",
            "char_start_offset": 35899,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1417
                },
                {
                    "start": 1420,
                    "end": 1435
                },
                {
                    "start": 1436,
                    "end": 1559
                },
                {
                    "start": 1560,
                    "end": 1618
                },
                {
                    "start": 1619,
                    "end": 1645
                },
                {
                    "start": 1646,
                    "end": 1880
                }
            ],
            "ref_mentions": [
                {
                    "start": 239,
                    "end": 255,
                    "matchedPaperCorpusId": "258959097"
                },
                {
                    "start": 319,
                    "end": 334,
                    "matchedPaperCorpusId": "252968055"
                },
                {
                    "start": 461,
                    "end": 479,
                    "matchedPaperCorpusId": "71148268"
                },
                {
                    "start": 586,
                    "end": 603,
                    "matchedPaperCorpusId": "232380074"
                },
                {
                    "start": 661,
                    "end": 683,
                    "matchedPaperCorpusId": "232376763"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04986572265625
        },
        {
            "corpus_id": "268987468",
            "title": "GroundCocoa: A Benchmark for Evaluating Compositional&Conditional Reasoning in Language Models",
            "text": "Modern LLMs have demonstrated remarkable advancements in many tasks including those that are inherently compositional and necessitate conditional reasoning such as mathematical problem solving, and code generation and interpretation.However, discerning genuine reasoning from mere rote learning and shallow understanding continues to be a focal point of study.While LLMs have become exceedingly adept at answering questions of seemingly greater complexity, we show that they can struggle on the same skills when presented with an unfamiliar task setting.While problem size does have an impact, even the less complex samples in our dataset are challenging to the best language models today.\n\nBeyond introducing a new benchmark dataset, we conduct a thorough analysis of the effects of increasing complexity, including advanced prompting techniques, and robustness to atypical queries.Our results uncover a substantial disparity in the performance of competing language models, a distinction that is not as pronounced in most other evaluation benchmarks and highlights their respective abilities in tackling novel challenges.Our data generation process is largely automatic, with human validation at the last step.In addition to the dataset and the evaluation script, we release code for the data generation which can be easily extended to generate more examples, and increase diversity (through different slots) as well as complexity.With minor modifications, the task can be further complicated by incorporating queries with multiple answers and questions that require other forms of logical reasoning such as aggregation (e.g., \"Give me the cheapest flight matching my criteria?\"),existential quantification (e.g., \"Is there a first class seat under $5000?\")etc., which we leave for future work.",
            "score": 0.3508855947760209,
            "section_title": "Conclusion",
            "char_start_offset": 24782,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 233
                },
                {
                    "start": 233,
                    "end": 360
                },
                {
                    "start": 360,
                    "end": 554
                },
                {
                    "start": 554,
                    "end": 689
                },
                {
                    "start": 691,
                    "end": 883
                },
                {
                    "start": 883,
                    "end": 1123
                },
                {
                    "start": 1123,
                    "end": 1212
                },
                {
                    "start": 1212,
                    "end": 1433
                },
                {
                    "start": 1433,
                    "end": 1682
                },
                {
                    "start": 1682,
                    "end": 1759
                },
                {
                    "start": 1759,
                    "end": 1796
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.24853515625
        },
        {
            "corpus_id": "249626560",
            "title": "A Benchmark for Compositional Visual Reasoning",
            "text": "N training samples 20 1000 \n\nResNet-50 28.0 0 57.9 Visual reasoning benchmarks Visual reasoning has been a subject of AI research for decades and several benchmarks address many relevant tasks. This includes language-guided reasoning benchmarks such as CLEVR [14], extended in its visual composition by recent work [18], physics-based reasoning and reasoning over time dynamics [30,1]. More relevant to our work are abstract visual reasoning benchmarks. Raven's Progressive Matrices (RPMs) are one such example introduced in the 1938 [4] to test fluidic intelligence in humans. Procedural generation techniques for RPMs [28] enabled the creation of the PGM dataset and RAVEN [2,31]. They also inspired Bongard-Logo [22], a concept learning and reasoning benchmark based on Bongard's 100 visual reasoning problems [3]. Another reasoning dataset, SVRT [9], focuses on evaluating similarity-based judgment and spatial reasoning. Besides these synthetic datasets, real-world datasets were developed with similar task structures to Bongard-Logo and RPM [25,13]. In this work, we take inspiration from SVRT and develop a more extensive set of rules with careful considerations for the choice of rules and using a novel rule generation method. Finally, Abstract Reasoning Corpus [7] is a general intelligence test introduced with a new methodology for evaluating intelligence and generalization. The numerous problems presented in this benchmark are constructed with a variety of human priors. The unique nature of the task, requiring solvers to generate the answer, and the limited amount of training data render the benchmark difficult for neural network based methods. We follow a similar approach in our dataset by creating several unique problem templates. However, we restrict the number of samples to a reasonable range to evaluate the sample efficiency of candidate models. \n\nNeuroscience/Psychology Several theories attempt to propose an understanding of the mechanisms behind visual reasoning, such as gestalt psychology which outlines principles hypothesized to be used by the visual system as an initial set of abstractions. Another theory describes visual reasoning as a sequence of elemental operations called visual  routines [26] orchestrated by higher-level cognitive processes.",
            "score": 0.3508744139881015,
            "section_title": "Related Work",
            "char_start_offset": 20530,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 26
                },
                {
                    "start": 29,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 682
                },
                {
                    "start": 683,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1486
                },
                {
                    "start": 1487,
                    "end": 1664
                },
                {
                    "start": 1665,
                    "end": 1754
                },
                {
                    "start": 1755,
                    "end": 1874
                },
                {
                    "start": 1877,
                    "end": 2129
                },
                {
                    "start": 2130,
                    "end": 2288
                }
            ],
            "ref_mentions": [
                {
                    "start": 259,
                    "end": 263,
                    "matchedPaperCorpusId": "15458100"
                },
                {
                    "start": 382,
                    "end": 384,
                    "matchedPaperCorpusId": "199668689"
                },
                {
                    "start": 534,
                    "end": 537,
                    "matchedPaperCorpusId": "144992765"
                },
                {
                    "start": 620,
                    "end": 624,
                    "matchedPaperCorpusId": "1318321"
                },
                {
                    "start": 675,
                    "end": 678,
                    "matchedPaperCorpusId": "49665167"
                },
                {
                    "start": 678,
                    "end": 681,
                    "matchedPaperCorpusId": "71148268"
                },
                {
                    "start": 715,
                    "end": 719,
                    "matchedPaperCorpusId": "222125270"
                },
                {
                    "start": 850,
                    "end": 853,
                    "matchedPaperCorpusId": "12763277"
                },
                {
                    "start": 1048,
                    "end": 1052,
                    "matchedPaperCorpusId": "198967555"
                },
                {
                    "start": 1052,
                    "end": 1055,
                    "matchedPaperCorpusId": "247973017"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.312744140625
        },
        {
            "corpus_id": "271534245",
            "title": "Effective Large Language Model Debugging with Best-first Tree Search",
            "text": "Code generation belongs to the category of logical reasoning tasks, which includes mathematical reasoning [23,24,25]. Existing research works have developed many useful inference techniques to boost a model's reasoning ability, such as chain-of-thought prompting [26], tree-of-thoughts [27], and code interpreter integration [28]. Least-to-most prompting [29] takes a divide-and-conquer approach to helping LLMs tackle complex problems. Tree search is another popular algorithmic framework to iteratively build a correct reasoning path [19,25]. However, these techniques are fundamentally limited by a model's capacity to identify logical errors [30]. Collecting more training data on correct reasoning paths to fine-tune base models is a fruitful approach [31,32,33,24]. A related direction leverages LLMs to generate synthetic data based on existing logical reasoning datasets [34,35] and fine-tune base models with those data to self-improve the models. Combining LLMs with classical symbolic reasoning aims to benefit from the strong language ability of LLMs and the rigor made available by symbolic solvers [36,37]. In this work, we evaluate LLMs' ability to identify bugs in programs and present a detailed analysis on the LLMs' ability to reason about their own mistakes in code generation.",
            "score": 0.3507363293483077,
            "section_title": "Logical Reasoning in LLMs",
            "char_start_offset": 5594,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1120
                },
                {
                    "start": 1121,
                    "end": 1297
                }
            ],
            "ref_mentions": [
                {
                    "start": 113,
                    "end": 116,
                    "matchedPaperCorpusId": "267032902"
                },
                {
                    "start": 263,
                    "end": 267,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 286,
                    "end": 290,
                    "matchedPaperCorpusId": "258762525"
                },
                {
                    "start": 540,
                    "end": 543,
                    "matchedPaperCorpusId": "267032902"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66943359375
        },
        {
            "corpus_id": "265150088",
            "title": "Are LLMs Rigorous Logical Reasoner? Empowering Natural Language Proof Generation with Contrastive Stepwise Decoding",
            "text": "The main results are presented in Table 3. By analyzing the results, we can find that: Stepwise correction matters. Stepwise generation methods (MetGen, NLProofs, ConDec) outperform single-shot training (EntailmentWriter), even for EntailmentWriter with a much larger parameter size (11B). When comparing ConDec with threestage generation methods MetGen and NLProofs, ConDec achieved comparable or even better performance on Task 2 and Task 3. This shows that contrastive decoding with hard negatives can improve language models' reasoning ability, demonstrating our methods' effectiveness. While our research focuses on the generator, combining our method with theirs may still improve the final accuracy and it is worth exploring. Enhanced hard negatives facilitate reasoning. With enhanced hard negatives, we can find that the ability of proof planning over three tasks is all improved. Unlike vanilla negative construction, the enhanced hard negatives contain harder or more With contrastive decoding, our method achieves the best performance mostly over leave or step accuracy. The contrastive loss helps the generator discriminate between premises and finds semantically correlated premises to deduce a conclusion. However, deductive reasoning is still challenging because the improvement over intermediates is not as obvious as those on leaves or steps.",
            "score": 0.3501896416560711,
            "section_title": "Main Results",
            "char_start_offset": 19494,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 42
                },
                {
                    "start": 43,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1360
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.293212890625
        },
        {
            "corpus_id": "278310934",
            "title": "Synthesize-on-Graph: Knowledgeable Synthetic Data Generation for Continue Pre-training of Large Language Models",
            "text": "Large Language Models (LLMs) have achieved remarkable success but remain data-inefficient, especially when learning from small, specialized corpora with limited and proprietary data. Existing synthetic data generation methods for continue pre-training focus on intra-document content and overlook cross-document knowledge associations, limiting content diversity and depth. We propose Synthetic-on-Graph (SoG), a synthetic data generation framework that incorporates cross-document knowledge associations for efficient corpus expansion. SoG constructs a context graph by extracting entities and concepts from the original corpus, representing cross-document associations, and employing a graph walk strategy for knowledge-associated sampling. This enhances synthetic data diversity and coherence, enabling models to learn complex knowledge structures and handle rare knowledge. To further improve synthetic data quality, we integrate Chain-of-Thought (CoT) and Contrastive Clarifying (CC) synthetic, enhancing reasoning processes and discriminative power. Experiments show that SoG outperforms the state-of-the-art (SOTA) method in a multi-hop document Q&A dataset while performing comparably to the SOTA method on the reading comprehension task datasets, which also underscores the better generalization capability of SoG. Our work advances synthetic data generation and provides practical solutions for efficient knowledge acquisition in LLMs, especially in domains with limited data availability.",
            "score": 0.34991793658522774,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98193359375
        },
        {
            "corpus_id": "274597380",
            "title": "SABER: Model-agnostic Backdoor Attack on Chain-of-Thought in Neural Code Generation",
            "text": "As the scale of model parameters and the amount of training data continues to expand, LLMs have achieved significant progress in their reasoning abilities [56]. One effective strategy is CoT, with CoT prompting emerging as a key technique for improving performance [3]. Through the CoT approach, LLMs can generate more accurate responses by systematically breaking down problems and providing detailed explanations at each step of their reasoning process. To further improve this method, several strategies have been proposed to boost the accuracy and reliability of CoT generation. For example, He et al. [57] introduced the incorporation of external knowledge to support the reasoning process, thereby enhancing the fidelity of the generated CoT. Similarly, Wang et al. [58] investigated the use of self-consistency, generating multiple inference paths and selecting the most frequently occurring answer as the final output, which in turn improves the overall quality of the CoT. \n\nAdditionally, novel frameworks have been developed to enhance the CoT generation process. For instance, Creswell et al. [59] introduced a selection-inference framework, where LLMs alternate between selection and inference cycles. This approach generates a sequence of interpretable, causal reasoning steps that direct the model toward the final solution. Zhou et al. [60] proposed a leastto-most prompting method, which breaks down complex problems into smaller, more manageable subproblems, solving them sequentially for a more structured approach. \n\nBuilding on the success of CoT techniques in logical reasoning, researchers have begun to explore their potential in code-generation tasks. For instance, Jiang et al. [61] proposed a self-planning method, aiming to improve the model's ability to plan and reason autonomously during code generation. Similarly, Li et al. [62] introduced a structured CoT approach to help models better understand complex intentions, thereby reducing the challenges associated with problem-solving. \n\nAlthough progress has been made, challenges remain, particularly in deploying LLMs with over 100 billion parameters, due to their high computational demands. To tackle this, researchers have turned to smaller models, leveraging methods like knowledge distillation to enable complex reasoning without the need for massive resources.",
            "score": 0.349826514098878,
            "section_title": "Chain-of-Thought",
            "char_start_offset": 50763,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 981
                },
                {
                    "start": 984,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1533
                },
                {
                    "start": 1536,
                    "end": 1675
                },
                {
                    "start": 1676,
                    "end": 1834
                },
                {
                    "start": 1835,
                    "end": 2015
                },
                {
                    "start": 2018,
                    "end": 2175
                },
                {
                    "start": 2176,
                    "end": 2349
                }
            ],
            "ref_mentions": [
                {
                    "start": 155,
                    "end": 159,
                    "matchedPaperCorpusId": "249674500"
                },
                {
                    "start": 1104,
                    "end": 1108,
                    "matchedPaperCorpusId": "248887351"
                },
                {
                    "start": 1351,
                    "end": 1355,
                    "matchedPaperCorpusId": "248986239"
                },
                {
                    "start": 1703,
                    "end": 1707,
                    "matchedPaperCorpusId": "257495755"
                },
                {
                    "start": 1856,
                    "end": 1860,
                    "matchedPaperCorpusId": "258615421"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.456298828125
        },
        {
            "corpus_id": "265442462",
            "title": "Physics language and language use in physics\u2014What do we know and how AI might enhance language-related research and instruction",
            "text": "This established knowledge, we argue, can be expanded and advanced through ML, NLP, and LLMs.ML, NLP, and LLMs already enabled physics education researchers to detect topics in their unstructured, language-related data and helped them to automate coding of constructed response items in order to provide specific and automated guidance for students.Recently, generative LLMs solved open-ended and closed-form conceptual physics tasks and problems that tested for physics-related reasoning.LLMs were also utilized to enhance pattern detection, automate annotation and coding, provide exemplary responses for concept inventories, and create synthetic data sets.All these applications come with specific challenges.We will discuss potentials and challenges related to instruction and research in the following.",
            "score": 0.3497390000243723,
            "section_title": "Utilizing LLMs",
            "char_start_offset": 77063,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 93,
                    "end": 349
                },
                {
                    "start": 349,
                    "end": 489
                },
                {
                    "start": 489,
                    "end": 659
                },
                {
                    "start": 659,
                    "end": 712
                },
                {
                    "start": 712,
                    "end": 807
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.39404296875
        },
        {
            "corpus_id": "278714567",
            "title": "CRPE: Expanding The Reasoning Capability of Large Language Model for Code Generation",
            "text": "CodeLLM. LLMs have demonstrated impressive capabilities in code generation tasks after being trained on extensive code datasets. [13]- [15] Since high-quality data is hard to get, synthetic data becomes a crucial approach to enhance the code capabilities of models. The key point of this methodology lies in leveraging powerful foundation models to generate high-quality instructional data and corresponding code solutions. This process involves defining certain criteria for instruc-tion filtering and utilizing compilers to ensure the correctness of the generated code. Synthetic data can be used to improve the code capabilities of other weaker models or to achieve self-improve. [16], [17]Llama3 and StarCoder2 synthesize difficult and high-quality code data by building a complete data synthesis pipeline that starts from code databases. [7], [9]Evol-Instruct and OSS-instruct design special prompt to improve instruction data. Finally the synthesized code data is used in the form of QA pairs for SFT. Some other works aim to enhance the code capabilities of LLM through reinforcement learning. Since code tasks can obtain execution information through a compiler, it is possible to collect or generate test cases to execute and the execution result can be used as rewards to train the policy model [11], [12], [18], [19]. [10]StepCoder breaks down the code generation process and completes the answer incrementally, reducing the difficulty for the model to explore the correct solution. [20] CodeDPO optimizes the correctness and efficiency of solutions through preference learning. But these works only teach LLM the correct answers, rather than how to reason to arrive at those answers. \n\nEnhancing Code Reasoning capabilities of LLM. Recently, the reasoning capabilities of LLM have been further enhanced. Notable works include OpenAI O1 [21], QwQ [22], DeepSeek-R1-Lite. These reasoning LLMs have demonstrated outstanding performance in complex mathematical reasoning and code generation tasks, but it is unclear how to achieve such a coding reasoning model. Some works [23], [24] implement code reasoning models through training process reward model(PRM) and reinforcement learning (RL).",
            "score": 0.3497246926141514,
            "section_title": "II. RELATED WORK",
            "char_start_offset": 5296,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 8
                },
                {
                    "start": 9,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 682
                },
                {
                    "start": 683,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1007
                },
                {
                    "start": 1008,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1328
                },
                {
                    "start": 1329,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1589
                },
                {
                    "start": 1590,
                    "end": 1695
                },
                {
                    "start": 1698,
                    "end": 1743
                },
                {
                    "start": 1744,
                    "end": 1815
                },
                {
                    "start": 1816,
                    "end": 1881
                },
                {
                    "start": 1882,
                    "end": 2069
                },
                {
                    "start": 2070,
                    "end": 2199
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87255859375
        },
        {
            "corpus_id": "272423721",
            "title": "Strategic Chain-of-Thought: Guiding Accurate Reasoning in LLMs through Strategy Elicitation",
            "text": "Current methods for enhancing the quality of modelgenerated content are diverse and sophisticated. Some approaches utilize a voting-based mechanism. For example, Wang et al. (2022) introduced the Self-Consistency method, which improves reasoning accuracy by first generating more than 20 CoT paths and then voting for the most consistent answer. Other methods incorporate external sources. Zheng et al. (2023) introduced Step Back, which prompts models to generate an abstract of the question to capture deeper logical structures, thereby enhancing retrieval-augmented generation (RAG) capabilities. Similarly, Yang et al. (2024b) developed another RAG-based method, Buffer of Thoughts, which uses knowledge extracted from external sources and predefined knowledge categories for each task. These elements are integrated into a predefined task prompt template, enabling the model to generate more accurate answers. Additionally, some methods introduce external tools to aid problem-solving. Gao et al. (2023) proposed PAL, which leverages large language models to parse problems and generate programs as intermediate reasoning steps, delegating the solution to a runtime environment like a Python interpreter. This neural-symbolic collaboration has demonstrated improved accuracy across various tasks. Suzgun and Kalai (2024) introduced metaprompting, which integrates existing prompt-based frameworks, enabling dynamic selection of the most effective reasoning strategy. These strategies, with their complex templates and multi-stage prompting, provide models with sophisticated tools for advancing CoT generation in LLMs. \n\nThese methods are inherently complex, with some being task-sensitive and others involving multi-turn prompting; however, they have demonstrated substantial efficacy in enhancing the reasoning capabilities of LLMs, thereby advancing the frontiers of CoT generation in machine learning.",
            "score": 0.3495207949709589,
            "section_title": "Enhancement of CoT Path",
            "char_start_offset": 6446,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1623
                },
                {
                    "start": 1626,
                    "end": 1910
                }
            ],
            "ref_mentions": [
                {
                    "start": 991,
                    "end": 1008,
                    "matchedPaperCorpusId": "253708270"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56640625
        },
        {
            "corpus_id": "258461502",
            "title": "Visual Chain of Thought: Bridging Logical Gaps with Multimodal Infillings",
            "text": "First, multi-step reasoning with visuals better fills logical gaps because images capture additional information that unimodal text cannot. Second, visual chains mimic human imagination which creates novel solutions (Tan & Bansal, 2021;Lu et al., 2022;Zhu et al., 2022) and provide interpretability (Wang et al., 2022b) into decision making. One imagined picture provides a thousand-word insight to enhance computer reasoning. \n\nWe propose Visual Chain-of-Thought (VCOT), which combines the efficiency, robustness, and multi-step reasoning of COT with the multimodal capabilities of vision-language models. VCOT synthetically augments sequential datasets and bridges logical gaps by recursively generating multimodal infillings and using the synthetic data to improve downstream task performance. These synthetic generations also serve as human-interpretable insights into AI systems' ability of multi-step reasoning. We demonstrate that VCOT creates consistent and novel synthetic data that enhances downstream performance on the VIST (Huang et al., 2016) and WIKIHOW (Koupaee & Wang, 2018) datasets. Our main contributions are: \n\n\u2022 We propose Visual Chain-of-Thought for sequential data to generate synthetic text-visual pairs as data augmentation for downstream reasoning. \u2022 We devise a consistency and novelty-driven approach to recursively generate multimodal infillings that augment faithful, relevant context. \u2022 We demonstrate the effectiveness of our method through human evaluation, showing improvements in sequential reasoning.",
            "score": 0.3493478763314063,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2000,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 426
                },
                {
                    "start": 429,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 796
                },
                {
                    "start": 797,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 1101
                },
                {
                    "start": 1102,
                    "end": 1129
                },
                {
                    "start": 1132,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1416
                },
                {
                    "start": 1417,
                    "end": 1537
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9150390625
        },
        {
            "corpus_id": "273115023",
            "title": "SynC2S: An Efficient Method for Synthesizing Tabular Data With a Learnable Pre-Processing",
            "text": "There has been a growing demand to access large public datasets to extract valuable insights or enhance their services. However, this also involves risks, such as privacy breaches and unauthorized data exposure. Data synthesis has emerged as a popular technique to address privacy preservation and data usability simultaneously. Recently, numerous methods based on deep learning have been developed, while a clear understanding of their effectiveness is still insufficient, and the necessity for more efficient frameworks persists. In this study, we propose an efficient and theoretically principled method based on a deep generative model to effectively generate high-quality synthetic tabular data. First, we introduce a novel technique called C2Smap\u2013a learnable pre-processing method that automatically transforms continuous distributions into simpler and easily generatable forms. We then develop a conditional generative model with a hierarchical structure and its corresponding learning framework, called HCIWAE, to successfully capture imbalanced categorical distributions. Combining these two components, we coin our method Synthetic data generation with C2Smap (SynC2S). Through comprehensive experimental analyses, we demonstrate the superiority and efficiency of SynC2S in generating synthetic data compared to other recent competitors. Furthermore, as a by-product, we claim that SynC2S could be a favorable option to solve over-sampling tasks, constructing high-performance prediction models by generating synthetic data for the minority class.",
            "score": 0.3493079417352395,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93603515625
        },
        {
            "corpus_id": "273403575",
            "title": "MIND: Math Informed syNthetic Dialogues for Pretraining LLMs",
            "text": "However, while these techniques have shown promise in improving general reasoning tasks, their data often lack the step-by-step problem solving structure crucial for multi-hop reasoning and complex mathematical tasks (Maini et al., 2024), making them sub-optimal for such reasoning. Figure 1: Continuous pretraining with all styles of conversations (MIND-OWM-4B) derived from a small subset (OWM-4B) and a 3.6\u00d7 large raw corpus (OWM-14B) reveals that model trained with conversations outperforms the one trained with larger corpus in GSM8K, MMLU and general reasoning-showing the significance of high-quality structured data over quantity. \n\nTo address these challenges, we propose MIND, a novel approach to generate Math Informed syNthetic Dialogue data at scale. In MIND, we provide a pretrained LLM with a web document and explicitly prompt it in a zero-shot manner to generate a conversation that-(a) decomposes the original context step-by-step into multi-turn conversations and (b) explores each step in depth within a single turn. As illustrated in Figure 2, MIND generates conversation from a raw text by prompting an open-source LLM on seven diverse conversational styles. The generated conversations are refined using heuristic filters and then can be used to pretrain a language model. MIND demonstrates that transforming raw web text into structured conversations using an offthe-shelf open-source LLM significantly enhances the mathematical and logical reasoning abilities of LLMs compared to unstructured raw or rephrased web text. Additionally, MIND provides the flexibility to preserve the diversity of the web corpora and leverage knowledge imbalances between participants for further expansion of the corpora as they either educate each other or collaboratively bridge their shared knowledge gaps through explanation and analysis in a conversation. Moreover, MIND enables the continuous generation of synthetic data from a single document by employing infinite conversational styles, further enriching the diversity. Unlike static text rephrasing (Maini et al., 2024), conversations encourage dynamic reasoning, where participants build on each other's ideas, ask questions, and offer clarifications.",
            "score": 0.34918566873812973,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1872,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 639
                },
                {
                    "start": 642,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1866
                },
                {
                    "start": 1867,
                    "end": 2034
                },
                {
                    "start": 2035,
                    "end": 2218
                }
            ],
            "ref_mentions": [
                {
                    "start": 217,
                    "end": 237,
                    "matchedPaperCorpusId": "267312030"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5849609375
        },
        {
            "corpus_id": "266361867",
            "title": "Bridging Logic and Learning: A Neural-Symbolic Approach for Enhanced Reasoning in Neural Models (ASPER)",
            "text": "Neural-symbolic learning, an intersection of neural networks and symbolic reasoning, aims to blend neural networks' learning capabilities with symbolic AI's interpretability and reasoning. This paper introduces an approach designed to improve the performance of neural models in learning reasoning tasks. It achieves this by integrating Answer Set Programming (ASP) solvers and domain-specific expertise, which is an approach that diverges from traditional complex neural-symbolic models. In this paper, a shallow artificial neural network (ANN) is specifically trained to solve Sudoku puzzles with minimal training data. The model has a unique loss function that integrates losses calculated using the ASP solver outputs, effectively enhancing its training efficiency. Most notably, the model shows a significant improvement in solving Sudoku puzzles using only 12 puzzles for training and testing without hyperparameter tuning. This advancement indicates that the model's enhanced reasoning capabilities have practical applications, extending well beyond Sudoku puzzles to potentially include a variety of other domains. The code can be found on GitHub: https://github.com/Fadi2200/ASPEN.",
            "score": 0.3490794393502489,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1400146484375
        },
        {
            "corpus_id": "261696510",
            "title": "Hypothesis Search: Inductive Reasoning with Language Models",
            "text": "Figure 4: Two examples where GPT-4 has difficulty implementing programs even with correct hypotheses. \n\nCurrently, there are two types of failures in our pipeline. First, the model may be unable to generate a correct and sufficiently precise natural language hypothesis. Second, the model can still generate incorrect programs given a correct hypothesis. Hypothesis Generation. Hypothesis generation is especially challenging for the ARC dataset as it involves recognizing visual patterns in 2D grids. While we observe that GPT-4 has a primitive ability to recognize points, lines, and rectangles, and to identify repetition and symmetry relationships, it has trouble understanding more complex shapes and visual relationships like translation, scaling, and containment. This is unsurprising as GPT-4 is trained primarily on text corpora, and the visual grid is input as text in our experiments. Furthermore, we observe that GPT-4 has difficulty proposing reasonable hypotheses for very large grids, possibly due to the limited context length. In contrast, GPT-4 was quite good at hypothesis generation on 1D-ARC. While the concepts may be easier for this dataset it is certainly the case that the visual encoding is easier. We thus tentatively suggest that current LMs are non-trivially capable of hypothesis generation for inductive learning and anticipate that vision-language models (Driess et al., 2023) may close the remaining gap for visual tasks like ARC. \n\nProgram Generation. Even with correct hypotheses, difficulties may arise when the task is hard to implement in Python. For example, task 444801d8 shown in Figure 4 was one where the language model failed when given a correct hypothesis. The task is difficult to solve programmatically, even for humans, as it requires identifying an irregular shape and then filling it according to an irregular pattern. This suggests a limitation of using generic Python programs for solving visual inductive reasoning tasks. Natural language hypotheses may also contain ambiguous concepts that mismatch the biases of the program generator. The human-written hypothesis for task 363442ee in Figure 4 is: \"In the input, you should see a color pattern on the left side and blue squares on the right. The output grid size same size as the input.",
            "score": 0.3490520941324437,
            "section_title": "Task 444801d8 Task 363442ee",
            "char_start_offset": 20378,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 104,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 895
                },
                {
                    "start": 896,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1463
                },
                {
                    "start": 1466,
                    "end": 1485
                },
                {
                    "start": 1486,
                    "end": 1584
                },
                {
                    "start": 1585,
                    "end": 1702
                },
                {
                    "start": 1703,
                    "end": 1869
                },
                {
                    "start": 1870,
                    "end": 1975
                },
                {
                    "start": 1976,
                    "end": 2090
                },
                {
                    "start": 2091,
                    "end": 2247
                },
                {
                    "start": 2248,
                    "end": 2292
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.070556640625
        },
        {
            "corpus_id": "259262225",
            "title": "On the Usefulness of Synthetic Tabular Data Generation",
            "text": "We investigated the effects of generative model-based data augmentation strategies on downstream learning tasks for commonly used tabular benchmarks. We experimented with data imbalance and scarcity learning scenarios. We deliberately excluded privacy and fairness considerations from this paper to focus on the usefulness of ML training. We found that, despite ongoing advances in tabular generative modeling, popular state-of-the-art methods are not yet able to consistently outperform the ML efficacy of using only the original training datapoints for practical use cases, such as data summarization, augmentation, and learning in imbalanced datasets. We believe that including experiments that simulate real-world situations where practitioners might need synthetic data is often missing from the literature, and should be seen as a necessary prerequisite for improving tabular synthetic data generation methods. As future work, we will focus on integrating extensive HPO, adding downstream task-informed regularization terms over training of the generative models, as well as using pre-trained LLMs that are more relevant to the domain of the training data.",
            "score": 0.3488718641196614,
            "section_title": "Conclusions",
            "char_start_offset": 19484,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 654
                },
                {
                    "start": 655,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1162
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74560546875
        },
        {
            "corpus_id": "273507197",
            "title": "SoftSRV: Learn to Generate Targeted Synthetic Data",
            "text": "In this work, we established the effectiveness of the SoftSRV framework for generating targeted synthetic fine-tuning data. We deploy the same SoftSRV pipeline across math, coding, and reasoning tasks, finding in each case that SoftSRV generates fine-tuning data that provides strong downstream performance with no manual prompt engineering or per-domain specialization needed. A potentially fruitful direction for future work is to adaptively select or generate context vectors z, in order to generate even more impactful synthetic data.",
            "score": 0.34875255336180516,
            "section_title": "CONCLUSION",
            "char_start_offset": 30640,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 538
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94921875
        },
        {
            "corpus_id": "259687754",
            "title": "Modeling Strategies as Programs: How to Study Strategy Differences in Intelligent Systems with Program Synthesis",
            "text": "You are presented with the three cards shown in Figure 1(a), and without any more specific instructions, you are asked to pick a card. What choice will you make? Now, let us assume a second set of cards, shown in Figure 1 The task we just worked through can be considered as an instance of the \"odd-one-out\" task. In this particular formulation, one thing stands out: although there is no verbal communication of the goal, most people would likely be able to form a strategy that may lead to a goal after observing the sequence of the problems presented. \n\nHumans can form strategies for novel tasks, seemingly without effort (Mumford et al. 1993), but current AI systems cannot, at least with the level of flexibility shown by people (including young children in many cases). This remains true even when recent advances in machine learning techniques are considered. Most algorithms require large amounts of training data, long training times, and immense computation computational power. \n\nIn order to build systems that could exhibit human-like fluidity in forming strategies, and for us to study the factors that affect the strategy choices of an intelligent system, we may need to have a formal way of representing these strategies. As such, in my dissertation research, I am addressing the following problem: How can we model an intelligent system's strategy formulation techniques using synthesized programs? Figure 1: A simple \"odd-one-out\" task that demonstrates fluidity of human strategy learning. \n\nIn this dissertation pursuit, I will be answering the following research questions, with a focus on a small set of visuospatial reasoning tasks: \n\n1. For a chosen task domain, how can we define a minimal set of operations to manually generate strategies that are sufficient for various levels of performance? 2. Provided a program synthesis approach is taken as a means to generate strategies for intelligent systems that reason through specific visuospatial problems, what kinds of search techniques can be applied to ensure the production of strategies? 3. How can the techniques adopted from question 2 above be applied to fit programs on human performance for the purposes of understanding their strategies on standard visuospatial reasoning tasks? \n\nThe tasks I have been working on are mainly The Block Design Test (BDT) (Kohs 1920) and The Abstract Reasoning Corpus (ARC) (Chollet 2019).",
            "score": 0.34869934708568606,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 554
                },
                {
                    "start": 557,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 989
                },
                {
                    "start": 992,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1508
                },
                {
                    "start": 1511,
                    "end": 1655
                },
                {
                    "start": 1658,
                    "end": 1660
                },
                {
                    "start": 1661,
                    "end": 1819
                },
                {
                    "start": 1820,
                    "end": 2263
                },
                {
                    "start": 2266,
                    "end": 2405
                }
            ],
            "ref_mentions": [
                {
                    "start": 626,
                    "end": 646,
                    "matchedPaperCorpusId": "144342132"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.18994140625
        },
        {
            "corpus_id": "271050364",
            "title": "Distilling System 2 into System 1",
            "text": "Generating intermediate thoughts allows a model (or human!) to reason and plan in order to successfully complete a task or respond to an instruction. We refer to such deliberate thinking as System 2 reasoning, following its description for humans in Sloman (1996); Kahneman (2011) and later for AI models (Bengio, 2017;LeCun, 2022;Weston and Sukhbaatar, 2023). In System 2 reasoning effortful mental activity is exerted, especially in situations where System 1 -more automatic thinking -is likely to make errors. In standard Large Language Models (LLMs) we thus define System 1 as application of the Transformer (Vaswani et al., 2017) to directly produce a response given an input, without generation of intermediate tokens. We define System 2 as any approach which generates intermediate tokens, including methods that perform search, or prompt multiple times, before finally generating a response. A battery of such System 2 techniques have been proposed, among them Chainof-Thought (Wei et al., 2022), Tree-of-Thoughts (Yao et al., 2024), Graph-of-Thoughts (Besta et al., 2024), Branch-Solve-Merge (Saha et al., 2023), System 2 Attention (Weston and Sukhbaatar, 2023), Rephrase and Respond (Deng et al., 2023a) and more. Many of these methods are shown to produce more accurate results due to this explicit reasoning, but typically do so at much higher inference cost and latency for a response. Due to the latter, many of these approaches are not used in production systems, which mostly use System 1 generations. For a human, the process of learning to transfer a skill from deliberate (System 2) to automatic (System 1) in psychology is referred to as automaticity, and the use of procedural memory (Cohen and Squire, 1980). For example, when driving to work for the first time one might typically expend conscious effort planning and making decisions to get there. After a driver repeats this route, the driving process becomes \"compiled\" into the subconscious (Charlton and Starkey, 2013). Similarly, playing a sport such as tennis can become \"second nature\".",
            "score": 0.34824390641820535,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1398
                },
                {
                    "start": 1399,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1730
                },
                {
                    "start": 1731,
                    "end": 1871
                },
                {
                    "start": 1872,
                    "end": 1997
                },
                {
                    "start": 1998,
                    "end": 2067
                }
            ],
            "ref_mentions": [
                {
                    "start": 250,
                    "end": 263,
                    "matchedPaperCorpusId": "13454019"
                },
                {
                    "start": 612,
                    "end": 634,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 985,
                    "end": 1003,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 1022,
                    "end": 1040,
                    "matchedPaperCorpusId": "258762525"
                },
                {
                    "start": 1060,
                    "end": 1080,
                    "matchedPaperCorpusId": "261030303"
                },
                {
                    "start": 1705,
                    "end": 1729,
                    "matchedPaperCorpusId": "19690059"
                },
                {
                    "start": 1968,
                    "end": 1996,
                    "matchedPaperCorpusId": "144117585"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.035614013671875
        },
        {
            "corpus_id": "274464813",
            "title": "Perception Tokens Enhance Visual Reasoning in Multimodal Language Models",
            "text": "Reasoning with perception tokens. The second challenge is enabling the model to condition on tokens from V aux effectively when generating each subsequent reasoning step, thereby enhancing its reasoning capabilities. To achieve this, we introduce chain-of-thought reasoning progressively, beginning with simpler, single-step reasoning tasks and advancing to more complex, multi-step inference. The model begins by learning single-step reasoning, predicting an initial reasoning step s 1 based on the input x. It then progresses to multi-step reasoning, predicting sequences s 1 , \u2022 \u2022 \u2022 , s m and effectively utilizing auxiliary tokens to support extended chains of inference. We further reinforce this process with constrained decoding and an information bottleneck: in constrained decoding, we restrict sampling to auxiliary tokens, ensuring they serve as intermediate reasoning steps; in the information bottleneck approach, we truncate the reasoning chain before the auxiliary token when generating subsequent reasoning steps, forcing the model to rely solely on auxiliary tokens to reach the correct answer. Lastly, we provide a multi-task data synthesis approach to train the model using curriculum learning across various synthetic tasks. Further details are provided in Section 3.4.",
            "score": 0.34783972602830915,
            "section_title": "Perception token prediction and reasoning",
            "char_start_offset": 11499,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 33
                },
                {
                    "start": 34,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1289
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.349609375
        },
        {
            "corpus_id": "256826996",
            "title": "Can GPT-3 Perform Statutory Reasoning?",
            "text": "Exploring GPT-3's ability to reason has yielded improvements on multiple benchmark datasets. Lu et al. [14] collect a dataset of common sense and science questions, pairing each question with background information and an explanation of the answer. The additional context, serving as a chain-of-thought, marginally improves GPT-3's performance. Zhou et al. [27] explicitly decompose reasoning problems, forcing GPT-3 to solve a complex problem in multiple increments. Khot et al. [10] show GPT-3 can be prompted to decompose problems into simpler steps, on both artificial tasks and open-domain question-answering. Zelikman et al. [24] exploit a language model's ability to generate chains of reasoning: GPT-J [21] is made to generate chains of reasoning for a set of questions and answers, and is further fine-tuned on those chains that lead to the correct answer. Adding structure to the process of natural language generation, Zhang et al. [25] run a symbolic reasoning engine on the output of a language model, yielding improvements on semi-synthetic benchmarks meant to test reasoning abilities. Thorburn and Kruger [20] evaluate GPT Neo's argumentative abilities, by making it suggest claims or reasons to support a claim, with no particular emphasis on logical reasoning. Data comes from Kialo, a collection of arguments curated by online users. Evaluations show that reasons and claims generated by GPT Neo are less coherent than those generated by humans. Saparov and He [17] systematically study GPT-3's ability to reason, using synthetic, controlled data, and metrics that measure the internal coherence of chains of reasoning. The authors show that GPT-3 has the ability to perform individual steps of reasoning, but tends to lose track when producing a proof with multiple steps. Results further show that GPT-3 leverages its background knowledge about the world to draw conclusions, so that GPT-3's ability to reason decreases as the overlap between the target domain and its real-world knowledge decreases. \n\nGPT-3 on legal tasks. Hendrycks et al. [7] use GPT-3 to solve legal multiple-choice questions, with data collected from online sources.",
            "score": 0.3475593835911745,
            "section_title": "RELATED WORK",
            "char_start_offset": 4193,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 93,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1278
                },
                {
                    "start": 1279,
                    "end": 1352
                },
                {
                    "start": 1353,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1638
                },
                {
                    "start": 1639,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 2021
                },
                {
                    "start": 2024,
                    "end": 2045
                },
                {
                    "start": 2046,
                    "end": 2159
                }
            ],
            "ref_mentions": [
                {
                    "start": 943,
                    "end": 947,
                    "matchedPaperCorpusId": "252760744"
                },
                {
                    "start": 1121,
                    "end": 1125,
                    "matchedPaperCorpusId": "253161659"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.252197265625
        },
        {
            "corpus_id": "274656104",
            "title": "First Train to Generate, then Generate to Train: UnitedSynT5 for Few-Shot NLI",
            "text": "The generation of a synthetic dataset [23] uses an extension of the Entailment Few-Shot Learning (EFL) model. The process leverages a FLAN-T5 XL (3B) generator to produce additional premisehypothesis pairs without requiring manual labeling. The following steps outline the structure and workflow involved in generating this dataset:",
            "score": 0.3475544306000783,
            "section_title": "Generation of Synthetic Dataset",
            "char_start_offset": 10868,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 332
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7001953125
        },
        {
            "corpus_id": "276250232",
            "title": "VersaPRM: Multi-Domain Process Reward Model via Synthetic Reasoning Data",
            "text": "Reasoning Data with Labels \n\nIn order to obtain step-wise reasoning data for non-Math domains, we devise a pipeline, as outlined in Figure 2, to generate synthetic reasoning CoTs from existing questionanswering data. These CoTs are then given step-wise labels based on reasoning correctness. We detail the synthetic data generation process in Sections 5.1 and 5.2, including methods to create and annotate reasoning steps. We also provide additional analysis on the quality of the generation pipeline in Section 5.3.",
            "score": 0.3475544306000783,
            "section_title": "Automatic Generation of Multi-Domain",
            "char_start_offset": 14358,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 26
                },
                {
                    "start": 29,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 516
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80322265625
        },
        {
            "corpus_id": "273550022",
            "title": "Little Giants: Synthesizing High-Quality Embedding Data at Scale",
            "text": "Many works have tried to generate synthetic data using modern LLMs for downstream tasks finetuning. Following E5 mistral (Wang et al., 2024), in order to synthesize data for training an embedding model, we generate data for four kinds of tasks: classification (long-short match), semantic textual similarity (STS), retrieval (short-long match), and text matching (short-short and long-long match). \n\nFor simplicity, we will denote the data synthesis prompts as a set P without distinction. 2 We use GPT-4 to brainstorm a pool of candidate tasks T as instructions. With a prompt p \u2208 P and a task instruction t \u2208 T , an LLM \u03c0 \u03b8 can synthesize an embedding data sample d \u223c \u03c0 \u03b8 (d | p, t). Each data example is a triplet of (query, positive document, hard negative document). For example, for a classification task, the query is a long text and documents are short labels. More information on the structure of these data can be found in Appendix D.",
            "score": 0.3475544306000783,
            "section_title": "Preliminaries",
            "char_start_offset": 7468,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 100,
                    "end": 397
                },
                {
                    "start": 400,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 944
                }
            ],
            "ref_mentions": [
                {
                    "start": 121,
                    "end": 140,
                    "matchedPaperCorpusId": "266693831"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.712890625
        },
        {
            "paperId": "161be0f5cf24111d6298d43f2cc705a313abb287",
            "corpusId": 276421534,
            "title": "Synthetic Data Generation for Culturally Nuanced Commonsense Reasoning in Low-Resource Languages",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 0,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.12932, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2185326452",
                    "name": "Salsabila Zahirah Pranida"
                },
                {
                    "authorId": "2345927380",
                    "name": "Rifo Ahmad Genadi"
                },
                {
                    "authorId": "2346108543",
                    "name": "Fajri Koto"
                }
            ],
            "abstract": "Quantifying reasoning capability in low-resource languages remains a challenge in NLP due to data scarcity and limited access to annotators. While LLM-assisted dataset construction has proven useful for medium- and high-resource languages, its effectiveness in low-resource languages, particularly for commonsense reasoning, is still unclear. In this paper, we compare three dataset creation strategies: (1) LLM-assisted dataset generation, (2) machine translation, and (3) human-written data by native speakers, to build a culturally nuanced story comprehension dataset. We focus on Javanese and Sundanese, two major local languages in Indonesia, and evaluate the effectiveness of open-weight and closed-weight LLMs in assisting dataset creation through extensive manual validation. To assess the utility of synthetic data, we fine-tune language models on classification and generation tasks using this data and evaluate performance on a human-written test set. Our findings indicate that LLM-assisted data creation outperforms machine translation.",
            "corpus_id": "276421534",
            "text": "Quantifying reasoning capability in low-resource languages remains a challenge in NLP due to data scarcity and limited access to annotators. While LLM-assisted dataset construction has proven useful for medium- and high-resource languages, its effectiveness in low-resource languages, particularly for commonsense reasoning, is still unclear. In this paper, we compare three dataset creation strategies: (1) LLM-assisted dataset generation, (2) machine translation, and (3) human-written data by native speakers, to build a culturally nuanced story comprehension dataset. We focus on Javanese and Sundanese, two major local languages in Indonesia, and evaluate the effectiveness of open-weight and closed-weight LLMs in assisting dataset creation through extensive manual validation. To assess the utility of synthetic data, we fine-tune language models on classification and generation tasks using this data and evaluate performance on a human-written test set. Our findings indicate that LLM-assisted data creation outperforms machine translation.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.724609375
        },
        {
            "paperId": "3eb056280ef0f090720df0d364f6ae6e9872ce0a",
            "corpusId": 270658562,
            "title": "The Potential and Limitations of Large Language Models for Text Classification through Synthetic Data Generation",
            "venue": "INTERNATIONAL RESEARCH JOURNAL OF ENGINEERING &amp; APPLIED SCIENCES",
            "year": 2024,
            "referenceCount": 0,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.55083/irjeas.2024.v12i02002",
                "status": "BRONZE",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.55083/irjeas.2024.v12i02002?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.55083/irjeas.2024.v12i02002, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2307701429",
                    "name": "Ashok Kumar Pamidi venkata"
                },
                {
                    "authorId": "2307701571",
                    "name": "Leeladhar Gudala"
                }
            ],
            "abstract": "Large language models (LLMs), such as GPT-3 and BERT, have revolutionized the field of natural language processing (NLP), offering remarkable capabilities in text generation, translation, summarization, and classification. Among their many applications, LLMs show promise in text classification tasks, where they can automatically categorize text data into predefined categories or labels. This paper presents a comprehensive review of the potential and limitations of utilizing LLMs for text classification through synthetic data generation techniques. We delve into the methodologies employed in generating synthetic data using LLMs, which include techniques such as data augmentation, adversarial training, and transfer learning. These approaches aim to address issues of data scarcity and domain adaptation in text classification tasks. We explore their effectiveness in enhancing text classification performance, demonstrating how synthetic data can improve model generalization and robustness across diverse domains and languages. Additionally, we discuss the challenges and ethical considerations associated with synthetic data generation, including issues related to data privacy, bias amplification, and model fairness. Furthermore, we examine the impact of model size, pretraining data, and fine-tuning strategies on the performance of LLMs in text classification tasks. Recent studies have shown that larger models with access to more diverse pretraining data tend to achieve higher accuracy and better generalization on downstream tasks. Fine-tuning strategies, such as curriculum learning and self-training, can further improve model performance by adapting the model to task-specific data distributions. Through a critical analysis of existing literature and empirical studies, we provide insights into the current state-of-the-art techniques, identify key research gaps, and propose future directions for advancing the utilization of LLMs in text classification through synthetic data generation. This includes exploring novel approaches for generating diverse and representative synthetic data, developing evaluation metrics for assessing the quality of synthetic data, and investigating the long-term societal impacts of deploying LLMs in real-world applications.",
            "corpus_id": "270658562",
            "text": "Large language models (LLMs), such as GPT-3 and BERT, have revolutionized the field of natural language processing (NLP), offering remarkable capabilities in text generation, translation, summarization, and classification. Among their many applications, LLMs show promise in text classification tasks, where they can automatically categorize text data into predefined categories or labels. This paper presents a comprehensive review of the potential and limitations of utilizing LLMs for text classification through synthetic data generation techniques. We delve into the methodologies employed in generating synthetic data using LLMs, which include techniques such as data augmentation, adversarial training, and transfer learning. These approaches aim to address issues of data scarcity and domain adaptation in text classification tasks. We explore their effectiveness in enhancing text classification performance, demonstrating how synthetic data can improve model generalization and robustness across diverse domains and languages. Additionally, we discuss the challenges and ethical considerations associated with synthetic data generation, including issues related to data privacy, bias amplification, and model fairness. Furthermore, we examine the impact of model size, pretraining data, and fine-tuning strategies on the performance of LLMs in text classification tasks. Recent studies have shown that larger models with access to more diverse pretraining data tend to achieve higher accuracy and better generalization on downstream tasks. Fine-tuning strategies, such as curriculum learning and self-training, can further improve model performance by adapting the model to task-specific data distributions. Through a critical analysis of existing literature and empirical studies, we provide insights into the current state-of-the-art techniques, identify key research gaps, and propose future directions for advancing the utilization of LLMs in text classification through synthetic data generation. This includes exploring novel approaches for generating diverse and representative synthetic data, developing evaluation metrics for assessing the quality of synthetic data, and investigating the long-term societal impacts of deploying LLMs in real-world applications.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.92236328125
        },
        {
            "paperId": "99790717b91668485751f20c0b3fa086c6c2ff6a",
            "corpusId": 275119552,
            "title": "Mask Factory: Towards High-quality Synthetic Data Generation for Dichotomous Image Segmentation",
            "venue": "Neural Information Processing Systems",
            "year": 2024,
            "referenceCount": 41,
            "citationCount": 5,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.19080, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2337737083",
                    "name": "Haotian Qian"
                },
                {
                    "authorId": "2337707823",
                    "name": "YD Chen"
                },
                {
                    "authorId": "2337691195",
                    "name": "Shengtao Lou"
                },
                {
                    "authorId": "2290294424",
                    "name": "Fahad Khan"
                },
                {
                    "authorId": "2337694497",
                    "name": "Xiaogang Jin"
                },
                {
                    "authorId": "2337692102",
                    "name": "Deng-Ping Fan"
                }
            ],
            "abstract": "Dichotomous Image Segmentation (DIS) tasks require highly precise annotations, and traditional dataset creation methods are labor intensive, costly, and require extensive domain expertise. Although using synthetic data for DIS is a promising solution to these challenges, current generative models and techniques struggle with the issues of scene deviations, noise-induced errors, and limited training sample variability. To address these issues, we introduce a novel approach, \\textbf{\\ourmodel{}}, which provides a scalable solution for generating diverse and precise datasets, markedly reducing preparation time and costs. We first introduce a general mask editing method that combines rigid and non-rigid editing techniques to generate high-quality synthetic masks. Specially, rigid editing leverages geometric priors from diffusion models to achieve precise viewpoint transformations under zero-shot conditions, while non-rigid editing employs adversarial training and self-attention mechanisms for complex, topologically consistent modifications. Then, we generate pairs of high-resolution image and accurate segmentation mask using a multi-conditional control generation method. Finally, our experiments on the widely-used DIS5K dataset benchmark demonstrate superior performance in quality and efficiency compared to existing methods. The code is available at \\url{https://qian-hao-tian.github.io/MaskFactory/}.",
            "corpus_id": "275119552",
            "text": "Dichotomous Image Segmentation (DIS) tasks require highly precise annotations, and traditional dataset creation methods are labor intensive, costly, and require extensive domain expertise. Although using synthetic data for DIS is a promising solution to these challenges, current generative models and techniques struggle with the issues of scene deviations, noise-induced errors, and limited training sample variability. To address these issues, we introduce a novel approach, \\textbf{\\ourmodel{}}, which provides a scalable solution for generating diverse and precise datasets, markedly reducing preparation time and costs. We first introduce a general mask editing method that combines rigid and non-rigid editing techniques to generate high-quality synthetic masks. Specially, rigid editing leverages geometric priors from diffusion models to achieve precise viewpoint transformations under zero-shot conditions, while non-rigid editing employs adversarial training and self-attention mechanisms for complex, topologically consistent modifications. Then, we generate pairs of high-resolution image and accurate segmentation mask using a multi-conditional control generation method. Finally, our experiments on the widely-used DIS5K dataset benchmark demonstrate superior performance in quality and efficiency compared to existing methods. The code is available at \\url{https://qian-hao-tian.github.io/MaskFactory/}.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.85791015625
        },
        {
            "paperId": "4cda972e3eeb41a3ca5dd971f2f12c216fda84e8",
            "corpusId": 275552324,
            "title": "Evaluation of Synthetic Data Generation Models for Balancing Multiclass Metabolomic Profiles",
            "venue": "International Conference on Bioinformatics Research and Applications",
            "year": 2024,
            "referenceCount": 21,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3700666.3700693?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3700666.3700693, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2340279576",
                    "name": "Saioa Elizondo"
                },
                {
                    "authorId": "2340502080",
                    "name": "Mikel Hernandez"
                },
                {
                    "authorId": "2340285751",
                    "name": "Francisco J. Londono-Hoyos"
                },
                {
                    "authorId": "2070464101",
                    "name": "Andoni Beristain"
                },
                {
                    "authorId": "2360526663",
                    "name": "Rub\u00e9n Gil Redondo"
                },
                {
                    "authorId": "2290941514",
                    "name": "Ricardo Conde"
                },
                {
                    "authorId": "2239206916",
                    "name": "\u00d3scar Millet"
                },
                {
                    "authorId": "2360521667",
                    "name": "Gorka Epelde Unanue"
                }
            ],
            "abstract": "Balancing highly imbalanced multiclass datasets, a crucial step for effective classification in health and well-being analysis, remains challenging, especially with conditions like Metabolic Syndrome (MetS). This study evaluates Synthetic Data Generation (SDG) techniques for balancing a dataset of metabolomic profiles associated with MetS. Evaluating models like Gaussian Copula, FAST ML, CTGAN, TVAE, TabDDPM, and Tabsyn reveals their effectiveness in preserving multivariate relationships but dissimilar success in replicating individual variable distributions. Results show significant improvements in classification tasks with balanced data, highlighting the potential of SDG in enhancing real-world data analysis and decision-making in health research.",
            "corpus_id": "275552324",
            "text": "Balancing highly imbalanced multiclass datasets, a crucial step for effective classification in health and well-being analysis, remains challenging, especially with conditions like Metabolic Syndrome (MetS). This study evaluates Synthetic Data Generation (SDG) techniques for balancing a dataset of metabolomic profiles associated with MetS. Evaluating models like Gaussian Copula, FAST ML, CTGAN, TVAE, TabDDPM, and Tabsyn reveals their effectiveness in preserving multivariate relationships but dissimilar success in replicating individual variable distributions. Results show significant improvements in classification tasks with balanced data, highlighting the potential of SDG in enhancing real-world data analysis and decision-making in health research.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.72412109375
        },
        {
            "paperId": "ad06f4166f36f9dac7e34ce525a72a3e5984c2d4",
            "corpusId": 276482747,
            "title": "CLIPPER: Compression enables long-context synthetic data generation",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 59,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.14854, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2264486147",
                    "name": "Chau Minh Pham"
                },
                {
                    "authorId": "144455052",
                    "name": "Yapei Chang"
                },
                {
                    "authorId": "2136562",
                    "name": "Mohit Iyyer"
                }
            ],
            "abstract": "LLM developers are increasingly reliant on synthetic data, but generating high-quality data for complex long-context reasoning tasks remains challenging. We introduce CLIPPER, a compression-based approach for generating synthetic data tailored to narrative claim verification - a task that requires reasoning over a book to verify a given claim. Instead of generating claims directly from the raw text of the book, which results in artifact-riddled claims, CLIPPER first compresses the book into chapter outlines and book summaries and then uses these intermediate representations to generate complex claims and corresponding chain-of-thoughts. Compared to naive approaches, CLIPPER produces claims that are more valid, grounded, and complex. Using CLIPPER, we construct a dataset of 19K synthetic book claims paired with their source texts and chain-of-thought reasoning, and use it to fine-tune three open-weight models. Our best model achieves breakthrough results on narrative claim verification (from 28% to 76% accuracy on our test set) and sets a new state-of-the-art for sub-10B models on the NoCha leaderboard. Further analysis shows that our models generate more detailed and grounded chain-of-thought reasoning while also improving performance on other narrative understanding tasks (e.g., NarrativeQA).",
            "corpus_id": "276482747",
            "text": "LLM developers are increasingly reliant on synthetic data, but generating high-quality data for complex long-context reasoning tasks remains challenging. We introduce CLIPPER, a compression-based approach for generating synthetic data tailored to narrative claim verification - a task that requires reasoning over a book to verify a given claim. Instead of generating claims directly from the raw text of the book, which results in artifact-riddled claims, CLIPPER first compresses the book into chapter outlines and book summaries and then uses these intermediate representations to generate complex claims and corresponding chain-of-thoughts. Compared to naive approaches, CLIPPER produces claims that are more valid, grounded, and complex. Using CLIPPER, we construct a dataset of 19K synthetic book claims paired with their source texts and chain-of-thought reasoning, and use it to fine-tune three open-weight models. Our best model achieves breakthrough results on narrative claim verification (from 28% to 76% accuracy on our test set) and sets a new state-of-the-art for sub-10B models on the NoCha leaderboard. Further analysis shows that our models generate more detailed and grounded chain-of-thought reasoning while also improving performance on other narrative understanding tasks (e.g., NarrativeQA).",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.958984375
        },
        {
            "paperId": "30fce83297f693ec2f97a0c05ec28d910c6c6b9d",
            "corpusId": 268264094,
            "title": "Generative AI for Synthetic Data Generation: Methods, Challenges and the Future",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 93,
            "citationCount": 35,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.04190, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2290239663",
                    "name": "Xu Guo"
                },
                {
                    "authorId": "2290210689",
                    "name": "Yiqiang Chen"
                }
            ],
            "abstract": "The recent surge in research focused on generating synthetic data from large language models (LLMs), especially for scenarios with limited data availability, marks a notable shift in Generative Artificial Intelligence (AI). Their ability to perform comparably to real-world data positions this approach as a compelling solution to low-resource challenges. This paper delves into advanced technologies that leverage these gigantic LLMs for the generation of task-specific training data. We outline methodologies, evaluation techniques, and practical applications, discuss the current limitations, and suggest potential pathways for future research.",
            "corpus_id": "268264094",
            "text": "The recent surge in research focused on generating synthetic data from large language models (LLMs), especially for scenarios with limited data availability, marks a notable shift in Generative Artificial Intelligence (AI). Their ability to perform comparably to real-world data positions this approach as a compelling solution to low-resource challenges. This paper delves into advanced technologies that leverage these gigantic LLMs for the generation of task-specific training data. We outline methodologies, evaluation techniques, and practical applications, discuss the current limitations, and suggest potential pathways for future research.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.95556640625
        },
        {
            "paperId": "c44d311683187293afd61986b5ba4ce267a00282",
            "corpusId": 279180141,
            "title": "Platelets Image Classification Through Data Augmentation: A Comparative Study of Traditional Imaging Augmentation and GAN-Based Synthetic Data Generation Techniques Using CNNs",
            "venue": "Journal of Imaging",
            "year": 2025,
            "referenceCount": 25,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/jimaging11060183?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/jimaging11060183, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2365672314",
                    "name": "Itunuoluwa Abidoye"
                },
                {
                    "authorId": "2365680637",
                    "name": "Frances Ikeji"
                },
                {
                    "authorId": "2144879052",
                    "name": "Charlie A. Coupland"
                },
                {
                    "authorId": "2296677962",
                    "name": "S. Calaminus"
                },
                {
                    "authorId": "2365689905",
                    "name": "Nick Sander"
                },
                {
                    "authorId": "2316676240",
                    "name": "Eva Sousa"
                }
            ],
            "abstract": "Platelets play a crucial role in diagnosing and detecting various diseases, influencing the progression of conditions and guiding treatment options. Accurate identification and classification of platelets are essential for these purposes. The present study aims to create a synthetic database of platelet images using Generative Adversarial Networks (GANs) and validate its effectiveness by comparing it with datasets of increasing sizes generated through traditional augmentation techniques. Starting from an initial dataset of 71 platelet images, the dataset was expanded to 141 images (Level 1) using random oversampling and basic transformations and further to 1463 images (Level 2) through extensive augmentation (rotation, shear, zoom). Additionally, a synthetic dataset of 300 images was generated using a Wasserstein GAN with Gradient Penalty (WGAN-GP). Eight pre-trained deep learning models (DenseNet121, DenseNet169, DenseNet201, VGG16, VGG19, InceptionV3, InceptionResNetV2, and AlexNet) and two custom CNNs were evaluated across these datasets. Performance was measured using accuracy, precision, recall, and F1-score. On the extensively augmented dataset (Level 2), InceptionV3 and InceptionResNetV2 reached 99% accuracy and 99% precision/recall/F1-score, while DenseNet201 closely followed, with 98% accuracy, precision, recall and F1-score. GAN-augmented data further improved DenseNet\u2019s performance, demonstrating the potential of GAN-generated images in enhancing platelet classification, especially where data are limited. These findings highlight the benefits of combining traditional and GAN-based augmentation techniques to improve classification performance in medical imaging tasks.",
            "corpus_id": "279180141",
            "text": "Platelets play a crucial role in diagnosing and detecting various diseases, influencing the progression of conditions and guiding treatment options. Accurate identification and classification of platelets are essential for these purposes. The present study aims to create a synthetic database of platelet images using Generative Adversarial Networks (GANs) and validate its effectiveness by comparing it with datasets of increasing sizes generated through traditional augmentation techniques. Starting from an initial dataset of 71 platelet images, the dataset was expanded to 141 images (Level 1) using random oversampling and basic transformations and further to 1463 images (Level 2) through extensive augmentation (rotation, shear, zoom). Additionally, a synthetic dataset of 300 images was generated using a Wasserstein GAN with Gradient Penalty (WGAN-GP). Eight pre-trained deep learning models (DenseNet121, DenseNet169, DenseNet201, VGG16, VGG19, InceptionV3, InceptionResNetV2, and AlexNet) and two custom CNNs were evaluated across these datasets. Performance was measured using accuracy, precision, recall, and F1-score. On the extensively augmented dataset (Level 2), InceptionV3 and InceptionResNetV2 reached 99% accuracy and 99% precision/recall/F1-score, while DenseNet201 closely followed, with 98% accuracy, precision, recall and F1-score. GAN-augmented data further improved DenseNet\u2019s performance, demonstrating the potential of GAN-generated images in enhancing platelet classification, especially where data are limited. These findings highlight the benefits of combining traditional and GAN-based augmentation techniques to improve classification performance in medical imaging tasks.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.67724609375
        },
        {
            "paperId": "2fce938889ad61673bdfd7fe65bcbb43319c8249",
            "corpusId": 263620310,
            "title": "Hate Speech Detection in Limited Data Contexts Using Synthetic Data Generation",
            "venue": "ACM J. Comput. Sustain. Soc.",
            "year": 2023,
            "referenceCount": 76,
            "citationCount": 4,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3625679",
                "status": "BRONZE",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.02876, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2357277744",
                    "name": "Aman Khullar"
                },
                {
                    "authorId": "52007170",
                    "name": "Daniel K. Nkemelu"
                },
                {
                    "authorId": "2182293607",
                    "name": "Cuong V. Nguyen"
                },
                {
                    "authorId": "2253655000",
                    "name": "Michael L. Best"
                }
            ],
            "abstract": "A growing body of work has focused on text classification methods for detecting the increasing amount of hate speech posted online. This progress has been limited to only a select number of highly resourced languages causing detection systems to either under-perform or not exist in limited data contexts. This is mostly caused by a lack of training data, which are expensive to collect and curate in these settings. In this work, we propose a data augmentation approach that addresses the problem of lack of data for online hate speech detection in limited data contexts using synthetic data generation techniques. Given a handful of hate speech examples in a high-resource language such as English, we present three methods to synthesize new examples of hate speech data in a target language that retains the hate sentiment in the original examples but transfers the hate targets. We apply our approach to generate training data for hate speech classification tasks in Hindi and Vietnamese. Our findings show that a model trained on synthetic data performs comparably to, and in some cases outperforms, a model trained only on the samples available in the target domain. This method can be adopted to bootstrap hate speech detection models from scratch in limited data contexts. As the growth of social media within these contexts continues to outstrip response efforts, this work furthers our capacities for detection, understanding, and response to hate speech. Disclaimer: This work contains terms that are offensive and hateful. These, however, cannot be avoided due to the nature of the work.",
            "corpus_id": "263620310",
            "text": "A growing body of work has focused on text classification methods for detecting the increasing amount of hate speech posted online. This progress has been limited to only a select number of highly resourced languages causing detection systems to either under-perform or not exist in limited data contexts. This is mostly caused by a lack of training data, which are expensive to collect and curate in these settings. In this work, we propose a data augmentation approach that addresses the problem of lack of data for online hate speech detection in limited data contexts using synthetic data generation techniques. Given a handful of hate speech examples in a high-resource language such as English, we present three methods to synthesize new examples of hate speech data in a target language that retains the hate sentiment in the original examples but transfers the hate targets. We apply our approach to generate training data for hate speech classification tasks in Hindi and Vietnamese. Our findings show that a model trained on synthetic data performs comparably to, and in some cases outperforms, a model trained only on the samples available in the target domain. This method can be adopted to bootstrap hate speech detection models from scratch in limited data contexts. As the growth of social media within these contexts continues to outstrip response efforts, this work furthers our capacities for detection, understanding, and response to hate speech. Disclaimer: This work contains terms that are offensive and hateful. These, however, cannot be avoided due to the nature of the work.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.5947265625
        },
        {
            "paperId": "205d23ce6e80f9be7fd9164458e634fe282c911e",
            "corpusId": 274478280,
            "title": "A Comparative Study on Synthetic Facial Data Generation Techniques for Face Recognition",
            "venue": "Anais Estendidos da XXXVII Conference on Graphics, Patterns and Images (SIBGRAPI Estendido 2024)",
            "year": 2024,
            "referenceCount": 22,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.5753/sibgrapi.est.2024.31662?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.5753/sibgrapi.est.2024.31662, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2180392739",
                    "name": "Pedro Vidal"
                },
                {
                    "authorId": "2267244352",
                    "name": "Bernardo Biesseck"
                },
                {
                    "authorId": "2275308520",
                    "name": "Luiz Coelho"
                },
                {
                    "authorId": "2275304051",
                    "name": "Roger Granada"
                },
                {
                    "authorId": "2275305765",
                    "name": "David Menotti"
                }
            ],
            "abstract": "Face recognition has become a widely adopted method for user authentication and identification, with applications in various domains such as secure access, law enforcement, and locating missing persons. The success of this technology is largely attributed to deep learning, which leverages large datasets and effective loss functions to achieve highly discriminative features. Despite its advancements, face recognition still faces challenges in areas such as explainability, demographic bias, privacy and robustness against aging, pose variations, illumination changes, occlusions, and expressions. Additionally, the emergence of privacy regulations has led to the discontinuation of several well-established datasets, raising legal, ethical, and privacy concerns. To address these issues, synthetic facial data generation has been proposed as a solution. This technique not only mitigates privacy concerns but also allows for comprehensive experimentation with facial attributes that cause bias, helps alleviate demographic bias, and provides complementary data to enhance models trained with real data. Competitions, such as the FRCSyn and SDFR, have been organized to explore the limitations and potential of face recognition technology trained with synthetic data. This paper compares the effectiveness of established synthetic face datasets with different generation techniques in face recognition tasks. We benchmark the accuracy of seven mainstream datasets, providing a vivid comparison of approaches that are not explicitly contrasted in the literature. Our experiments highlight the diverse techniques used to address the synthetic facial data generation problem and present a comprehensive benchmark of the area. The results demonstrate the effectiveness of various methods in generating synthetic facial data with realistic variations, evidencing the diverse techniques used to deal with the problem.",
            "corpus_id": "274478280",
            "text": "Face recognition has become a widely adopted method for user authentication and identification, with applications in various domains such as secure access, law enforcement, and locating missing persons. The success of this technology is largely attributed to deep learning, which leverages large datasets and effective loss functions to achieve highly discriminative features. Despite its advancements, face recognition still faces challenges in areas such as explainability, demographic bias, privacy and robustness against aging, pose variations, illumination changes, occlusions, and expressions. Additionally, the emergence of privacy regulations has led to the discontinuation of several well-established datasets, raising legal, ethical, and privacy concerns. To address these issues, synthetic facial data generation has been proposed as a solution. This technique not only mitigates privacy concerns but also allows for comprehensive experimentation with facial attributes that cause bias, helps alleviate demographic bias, and provides complementary data to enhance models trained with real data. Competitions, such as the FRCSyn and SDFR, have been organized to explore the limitations and potential of face recognition technology trained with synthetic data. This paper compares the effectiveness of established synthetic face datasets with different generation techniques in face recognition tasks. We benchmark the accuracy of seven mainstream datasets, providing a vivid comparison of approaches that are not explicitly contrasted in the literature. Our experiments highlight the diverse techniques used to address the synthetic facial data generation problem and present a comprehensive benchmark of the area. The results demonstrate the effectiveness of various methods in generating synthetic facial data with realistic variations, evidencing the diverse techniques used to deal with the problem.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.52197265625
        },
        {
            "paperId": "fa07993d20ea5b6ca7cc7fc7c92cb4a53f1a3a51",
            "corpusId": 274515203,
            "title": "Enhancing Document AI Data Generation Through Graph-Based Synthetic Layouts",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 0,
            "citationCount": 5,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.03590, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2332505408",
                    "name": "Amit Agarwal"
                },
                {
                    "authorId": "2304458545",
                    "name": "Hitesh Laxmichand Patel"
                },
                {
                    "authorId": "2331856691",
                    "name": "Priyaranjan Pattnayak"
                },
                {
                    "authorId": "2333872944",
                    "name": "Srikant Panda"
                },
                {
                    "authorId": "2303811663",
                    "name": "Bhargava Kumar"
                },
                {
                    "authorId": "2303653964",
                    "name": "Tejaswini Kumar"
                }
            ],
            "abstract": "The development of robust Document AI models has been constrained by limited access to high-quality, labeled datasets, primarily due to data privacy concerns, scarcity, and the high cost of manual annotation. Traditional methods of synthetic data generation, such as text and image augmentation, have proven effective for increasing data diversity but often fail to capture the complex layout structures present in real world documents. This paper proposes a novel approach to synthetic document layout generation using Graph Neural Networks (GNNs). By representing document elements (e.g., text blocks, images, tables) as nodes in a graph and their spatial relationships as edges, GNNs are trained to generate realistic and diverse document layouts. This method leverages graph-based learning to ensure structural coherence and semantic consistency, addressing the limitations of traditional augmentation techniques. The proposed framework is evaluated on tasks such as document classification, named entity recognition (NER), and information extraction, demonstrating significant performance improvements. Furthermore, we address the computational challenges of GNN based synthetic data generation and propose solutions to mitigate domain adaptation issues between synthetic and real-world datasets. Our experimental results show that graph-augmented document layouts outperform existing augmentation techniques, offering a scalable and flexible solution for training Document AI models.",
            "corpus_id": "274515203",
            "text": "The development of robust Document AI models has been constrained by limited access to high-quality, labeled datasets, primarily due to data privacy concerns, scarcity, and the high cost of manual annotation. Traditional methods of synthetic data generation, such as text and image augmentation, have proven effective for increasing data diversity but often fail to capture the complex layout structures present in real world documents. This paper proposes a novel approach to synthetic document layout generation using Graph Neural Networks (GNNs). By representing document elements (e.g., text blocks, images, tables) as nodes in a graph and their spatial relationships as edges, GNNs are trained to generate realistic and diverse document layouts. This method leverages graph-based learning to ensure structural coherence and semantic consistency, addressing the limitations of traditional augmentation techniques. The proposed framework is evaluated on tasks such as document classification, named entity recognition (NER), and information extraction, demonstrating significant performance improvements. Furthermore, we address the computational challenges of GNN based synthetic data generation and propose solutions to mitigate domain adaptation issues between synthetic and real-world datasets. Our experimental results show that graph-augmented document layouts outperform existing augmentation techniques, offering a scalable and flexible solution for training Document AI models.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.8544921875
        },
        {
            "paperId": "1915aff174154d0f56e84dd4a45a873ed82686e9",
            "corpusId": 275790045,
            "title": "Improving Foot Deformity Detection in Children: Synthetic Plantar Pressure Data Generation with GMM and GAN",
            "venue": "2024 4th Asian Conference on Innovation in Technology (ASIANCON)",
            "year": 2024,
            "referenceCount": 9,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ASIANCON62057.2024.10838069?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ASIANCON62057.2024.10838069, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2341538989",
                    "name": "K. A. Raj"
                },
                {
                    "authorId": "2341536211",
                    "name": "Sandra Krishnan"
                },
                {
                    "authorId": "40762611",
                    "name": "Anu Chalil"
                },
                {
                    "authorId": "2287977934",
                    "name": "Vivek Venugopal"
                },
                {
                    "authorId": "2243994738",
                    "name": "N. N"
                }
            ],
            "abstract": "The identification of leg deformities in children is pivotal for ensuring timely and effective medical intervention. Plantar pressure is the pressure field that acts between the foot and the support surface during everyday locomotor activities. Prior works on automatic identification of foot deformities based on plantar pressure made use of machine learning approaches. Of late, deep learning techniques have been providing state of the art performance in classification tasks. However, these algorithms require a sufficiently large labelled training data for good performance. This paper proposes two different methods for generating plantar pressure data synthetically. The first approach makes use of Gaussian Mixture Models(GMM) for generating the plantar pressure data for three distinct foot types: flat, normal, and pes cavus. The second approach makes use of one dimensional Generative Adversarial Network (GAN) for synthetic data production. The synthetically generated plantar pressure data is augmented into an existing database and subsequently classified using various machine learning approaches and one-dimensional Convolutional Neural Networks (CNN). Experiments conducted on the publicly available plantar pressure dataset show an improvement in accuracy on the incorporation of the synthetic plantar pressure data.",
            "corpus_id": "275790045",
            "text": "The identification of leg deformities in children is pivotal for ensuring timely and effective medical intervention. Plantar pressure is the pressure field that acts between the foot and the support surface during everyday locomotor activities. Prior works on automatic identification of foot deformities based on plantar pressure made use of machine learning approaches. Of late, deep learning techniques have been providing state of the art performance in classification tasks. However, these algorithms require a sufficiently large labelled training data for good performance. This paper proposes two different methods for generating plantar pressure data synthetically. The first approach makes use of Gaussian Mixture Models(GMM) for generating the plantar pressure data for three distinct foot types: flat, normal, and pes cavus. The second approach makes use of one dimensional Generative Adversarial Network (GAN) for synthetic data production. The synthetically generated plantar pressure data is augmented into an existing database and subsequently classified using various machine learning approaches and one-dimensional Convolutional Neural Networks (CNN). Experiments conducted on the publicly available plantar pressure dataset show an improvement in accuracy on the incorporation of the synthetic plantar pressure data.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.416259765625
        },
        {
            "paperId": "701f7b7de38436b219ceae61bb06ec406d408a5e",
            "corpusId": 270924192,
            "title": "Diffusion Models for Tabular Data Imputation and Synthetic Data Generation",
            "venue": "ACM Transactions on Knowledge Discovery from Data",
            "year": 2024,
            "referenceCount": 67,
            "citationCount": 8,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.02549, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2309480102",
                    "name": "Mario Villaiz'an-Vallelado"
                },
                {
                    "authorId": "2309482633",
                    "name": "Matteo Salvatori"
                },
                {
                    "authorId": "2366108687",
                    "name": "Carlos Segura Perales"
                },
                {
                    "authorId": "2309480030",
                    "name": "Ioannis Arapakis"
                }
            ],
            "abstract": "Data imputation and data generation have important applications across many domains where incomplete or missing data can hinder accurate analysis and decision-making. Diffusion models have emerged as powerful generative models capable of capturing complex data distributions across various data modalities such as image, audio, and time series. Recently, they have been also adapted to generate tabular data. In this paper, we propose a diffusion model for tabular data that introduces three key enhancements: (1) a conditioning attention mechanism, (2) an encoder-decoder transformer as the denoising network, and (3) dynamic masking. The conditioning attention mechanism is designed to improve the model's ability to capture the relationship between the condition and synthetic data. The transformer layers help model interactions within the condition (encoder) or synthetic data (decoder), while dynamic masking enables our model to efficiently handle both missing data imputation and synthetic data generation tasks within a unified framework. We conduct a comprehensive evaluation by comparing the performance of diffusion models with transformer conditioning against state-of-the-art techniques such as Variational Autoencoders, Generative Adversarial Networks and Diffusion Models, on benchmark datasets. Our evaluation focuses on the assessment of the generated samples with respect to three important criteria, namely: (1) Machine Learning efficiency, (2) statistical similarity, and (3) privacy risk mitigation. For the task of data imputation, we consider the efficiency of the generated samples across different levels of missing features. The results demonstrates average superior machine learning efficiency and statistical accuracy compared to the baselines, while maintaining privacy risks at a comparable level, particularly showing increased performance in datasets with a large number of features. By conditioning the data generation on a desired target variable, the model can mitigate systemic biases, generate augmented datasets to address data imbalance issues, and improve data quality for subsequent analysis. This has significant implications for domains such as healthcare and finance, where accurate, unbiased, and privacy-preserving data are critical for informed decision-making and fair model outcomes.",
            "corpus_id": "270924192",
            "text": "Data imputation and data generation have important applications across many domains where incomplete or missing data can hinder accurate analysis and decision-making. Diffusion models have emerged as powerful generative models capable of capturing complex data distributions across various data modalities such as image, audio, and time series. Recently, they have been also adapted to generate tabular data. In this paper, we propose a diffusion model for tabular data that introduces three key enhancements: (1) a conditioning attention mechanism, (2) an encoder-decoder transformer as the denoising network, and (3) dynamic masking. The conditioning attention mechanism is designed to improve the model's ability to capture the relationship between the condition and synthetic data. The transformer layers help model interactions within the condition (encoder) or synthetic data (decoder), while dynamic masking enables our model to efficiently handle both missing data imputation and synthetic data generation tasks within a unified framework. We conduct a comprehensive evaluation by comparing the performance of diffusion models with transformer conditioning against state-of-the-art techniques such as Variational Autoencoders, Generative Adversarial Networks and Diffusion Models, on benchmark datasets. Our evaluation focuses on the assessment of the generated samples with respect to three important criteria, namely: (1) Machine Learning efficiency, (2) statistical similarity, and (3) privacy risk mitigation. For the task of data imputation, we consider the efficiency of the generated samples across different levels of missing features. The results demonstrates average superior machine learning efficiency and statistical accuracy compared to the baselines, while maintaining privacy risks at a comparable level, particularly showing increased performance in datasets with a large number of features. By conditioning the data generation on a desired target variable, the model can mitigate systemic biases, generate augmented datasets to address data imbalance issues, and improve data quality for subsequent analysis. This has significant implications for domains such as healthcare and finance, where accurate, unbiased, and privacy-preserving data are critical for informed decision-making and fair model outcomes.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.95263671875
        },
        {
            "paperId": "420231d0e618683161bda3329ba7631926a6572f",
            "corpusId": 273068851,
            "title": "Bias Mitigation via Synthetic Data Generation: A Review",
            "venue": "Electronics",
            "year": 2024,
            "referenceCount": 0,
            "citationCount": 7,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.3390/electronics13193909",
                "status": "GOLD",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/electronics13193909?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/electronics13193909, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2324001480",
                    "name": "Mohamed Ashik Shahul Hameed"
                },
                {
                    "authorId": "2364874867",
                    "name": "Asifa Mehmood Qureshi"
                },
                {
                    "authorId": "2333920498",
                    "name": "Abhishek Kaushik"
                }
            ],
            "abstract": "Artificial intelligence (AI) is widely used in healthcare applications to perform various tasks. Although these models have great potential to improve the healthcare system, they have also raised significant ethical concerns, including biases that increase the risk of health disparities in medical applications. The under-representation of a specific group can lead to bias in the datasets that are being replicated in the AI models. These disadvantaged groups are disproportionately affected by bias because they may have less accurate algorithmic forecasts or underestimate the need for treatment. One solution to eliminate bias is to use synthetic samples or artificially generated data to balance datasets. Therefore, the purpose of this study is to review and evaluate how synthetic data can be generated and used to mitigate biases, specifically focusing on the medical domain. We explored high-quality peer-reviewed articles that were focused on synthetic data generation to eliminate bias. These studies were selected based on our defined inclusion criteria and exclusion criteria and the quality of the content. The findings reveal that generated synthetic data can help improve accuracy, precision, and fairness. However, the effectiveness of synthetic data is closely dependent on the quality of the data generation process and the initial datasets used. The study also highlights the need for continuous improvement in synthetic data generation techniques and the importance of evaluation metrics for fairness in AI models.",
            "corpus_id": "273068851",
            "text": "Artificial intelligence (AI) is widely used in healthcare applications to perform various tasks. Although these models have great potential to improve the healthcare system, they have also raised significant ethical concerns, including biases that increase the risk of health disparities in medical applications. The under-representation of a specific group can lead to bias in the datasets that are being replicated in the AI models. These disadvantaged groups are disproportionately affected by bias because they may have less accurate algorithmic forecasts or underestimate the need for treatment. One solution to eliminate bias is to use synthetic samples or artificially generated data to balance datasets. Therefore, the purpose of this study is to review and evaluate how synthetic data can be generated and used to mitigate biases, specifically focusing on the medical domain. We explored high-quality peer-reviewed articles that were focused on synthetic data generation to eliminate bias. These studies were selected based on our defined inclusion criteria and exclusion criteria and the quality of the content. The findings reveal that generated synthetic data can help improve accuracy, precision, and fairness. However, the effectiveness of synthetic data is closely dependent on the quality of the data generation process and the initial datasets used. The study also highlights the need for continuous improvement in synthetic data generation techniques and the importance of evaluation metrics for fairness in AI models.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.480224609375
        },
        {
            "paperId": "047209369a042f6f4504d910ee0d1b2bb2c5841b",
            "corpusId": 269888837,
            "title": "Generative Adversarial Networks for Synthetic Data Generation in Finance: Evaluating Statistical Similarities and Quality Assessment",
            "venue": "Applied Informatics",
            "year": 2024,
            "referenceCount": 57,
            "citationCount": 19,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.mdpi.com/2673-2688/5/2/35/pdf?version=1715586960",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/ai5020035?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/ai5020035, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2301905217",
                    "name": "Faisal Ramzan"
                },
                {
                    "authorId": "2274845630",
                    "name": "Claudio Sartori"
                },
                {
                    "authorId": "1748453",
                    "name": "S. Consoli"
                },
                {
                    "authorId": "1752584925",
                    "name": "Diego Reforgiato Recupero"
                }
            ],
            "abstract": "Generating synthetic data is a complex task that necessitates accurately replicating the statistical and mathematical properties of the original data elements. In sectors such as finance, utilizing and disseminating real data for research or model development can pose substantial privacy risks owing to the inclusion of sensitive information. Additionally, authentic data may be scarce, particularly in specialized domains where acquiring ample, varied, and high-quality data is difficult or costly. This scarcity or limited data availability can limit the training and testing of machine-learning models. In this paper, we address this challenge. In particular, our task is to synthesize a dataset with similar properties to an input dataset about the stock market. The input dataset is anonymized and consists of very few columns and rows, contains many inconsistencies, such as missing rows and duplicates, and its values are not normalized, scaled, or balanced. We explore the utilization of generative adversarial networks, a deep-learning technique, to generate synthetic data and evaluate its quality compared to the input stock dataset. Our innovation involves generating artificial datasets that mimic the statistical properties of the input elements without revealing complete information. For example, synthetic datasets can capture the distribution of stock prices, trading volumes, and market trends observed in the original dataset. The generated datasets cover a wider range of scenarios and variations, enabling researchers and practitioners to explore different market conditions and investment strategies. This diversity can enhance the robustness and generalization of machine-learning models. We evaluate our synthetic data in terms of the mean, similarities, and correlations.",
            "corpus_id": "269888837",
            "text": "Generating synthetic data is a complex task that necessitates accurately replicating the statistical and mathematical properties of the original data elements. In sectors such as finance, utilizing and disseminating real data for research or model development can pose substantial privacy risks owing to the inclusion of sensitive information. Additionally, authentic data may be scarce, particularly in specialized domains where acquiring ample, varied, and high-quality data is difficult or costly. This scarcity or limited data availability can limit the training and testing of machine-learning models. In this paper, we address this challenge. In particular, our task is to synthesize a dataset with similar properties to an input dataset about the stock market. The input dataset is anonymized and consists of very few columns and rows, contains many inconsistencies, such as missing rows and duplicates, and its values are not normalized, scaled, or balanced. We explore the utilization of generative adversarial networks, a deep-learning technique, to generate synthetic data and evaluate its quality compared to the input stock dataset. Our innovation involves generating artificial datasets that mimic the statistical properties of the input elements without revealing complete information. For example, synthetic datasets can capture the distribution of stock prices, trading volumes, and market trends observed in the original dataset. The generated datasets cover a wider range of scenarios and variations, enabling researchers and practitioners to explore different market conditions and investment strategies. This diversity can enhance the robustness and generalization of machine-learning models. We evaluate our synthetic data in terms of the mean, similarities, and correlations.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.857421875
        },
        {
            "paperId": "e3b78278b200242e48dc85028e54e045c0432bb4",
            "corpusId": 274167691,
            "title": "Privacy-Preserving Synthetic Data Generation Method for IoT-Sensor Network IDS Using CTGAN",
            "venue": "Italian National Conference on Sensors",
            "year": 2024,
            "referenceCount": 36,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.3390/s24227389",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11598811, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2254595191",
                    "name": "Saleh Alabdulwahab"
                },
                {
                    "authorId": "2312096607",
                    "name": "Young-Tak Kim"
                },
                {
                    "authorId": "2240924130",
                    "name": "Yunsik Son"
                }
            ],
            "abstract": "The increased usage of IoT networks brings about new privacy risks, especially when intrusion detection systems (IDSs) rely on large datasets for machine learning (ML) tasks and depend on third parties for storing and training the ML-based IDS. This study proposes a privacy-preserving synthetic data generation method using a conditional tabular generative adversarial network (CTGAN) aimed at maintaining the utility of IoT sensor network data for IDS while safeguarding privacy. We integrate differential privacy (DP) with CTGAN by employing controlled noise injection to mitigate privacy risks. The technique involves dynamic distribution adjustment and quantile matching to balance the utility\u2013privacy tradeoff. The results indicate a significant improvement in data utility compared to the standard DP method, achieving a KS test score of 0.80 while minimizing privacy risks such as singling out, linkability, and inference attacks. This approach ensures that synthetic datasets can support intrusion detection without exposing sensitive information.",
            "corpus_id": "274167691",
            "text": "The increased usage of IoT networks brings about new privacy risks, especially when intrusion detection systems (IDSs) rely on large datasets for machine learning (ML) tasks and depend on third parties for storing and training the ML-based IDS. This study proposes a privacy-preserving synthetic data generation method using a conditional tabular generative adversarial network (CTGAN) aimed at maintaining the utility of IoT sensor network data for IDS while safeguarding privacy. We integrate differential privacy (DP) with CTGAN by employing controlled noise injection to mitigate privacy risks. The technique involves dynamic distribution adjustment and quantile matching to balance the utility\u2013privacy tradeoff. The results indicate a significant improvement in data utility compared to the standard DP method, achieving a KS test score of 0.80 while minimizing privacy risks such as singling out, linkability, and inference attacks. This approach ensures that synthetic datasets can support intrusion detection without exposing sensitive information.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.5478515625
        },
        {
            "paperId": "aa28b90540e6a3656ec53f5e213fff433ff07811",
            "corpusId": 273404183,
            "title": "CCUP: A Controllable Synthetic Data Generation Pipeline for Pretraining Cloth-Changing Person Re-Identification Models",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 41,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.13567, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2326549057",
                    "name": "Yujian Zhao"
                },
                {
                    "authorId": "2326297123",
                    "name": "Chengru Wu"
                },
                {
                    "authorId": "2326541811",
                    "name": "Yinong Xu"
                },
                {
                    "authorId": "2326293568",
                    "name": "Xuanzheng Du"
                },
                {
                    "authorId": "2326667184",
                    "name": "Ruiyu Li"
                },
                {
                    "authorId": "2339669666",
                    "name": "Guanglin Niu"
                }
            ],
            "abstract": "Cloth-changing person re-identification (CC-ReID), also known as Long-Term Person Re-Identification (LT-ReID) is a critical and challenging research topic in computer vision that has recently garnered significant attention. However, due to the high cost of constructing CC-ReID data, the existing data-driven models are hard to train efficiently on limited data, causing overfitting issue. To address this challenge, we propose a low-cost and efficient pipeline for generating controllable and high-quality synthetic data simulating the surveillance of real scenarios specific to the CC-ReID task. Particularly, we construct a new self-annotated CC-ReID dataset named Cloth-Changing Unreal Person (CCUP), containing 6,000 IDs, 1,179,976 images, 100 cameras, and 26.5 outfits per individual. Based on this large-scale dataset, we introduce an effective and scalable pretrain-finetune framework for enhancing the generalization capabilities of the traditional CC-ReID models. The extensive experiments demonstrate that two typical models namely TransReID and FIRe^2, when integrated into our framework, outperform other state-of-the-art models after pretraining on CCUP and finetuning on the benchmarks such as PRCC, VC-Clothes and NKUP. The CCUP is available at: https://github.com/yjzhao1019/CCUP.",
            "corpus_id": "273404183",
            "text": "Cloth-changing person re-identification (CC-ReID), also known as Long-Term Person Re-Identification (LT-ReID) is a critical and challenging research topic in computer vision that has recently garnered significant attention. However, due to the high cost of constructing CC-ReID data, the existing data-driven models are hard to train efficiently on limited data, causing overfitting issue. To address this challenge, we propose a low-cost and efficient pipeline for generating controllable and high-quality synthetic data simulating the surveillance of real scenarios specific to the CC-ReID task. Particularly, we construct a new self-annotated CC-ReID dataset named Cloth-Changing Unreal Person (CCUP), containing 6,000 IDs, 1,179,976 images, 100 cameras, and 26.5 outfits per individual. Based on this large-scale dataset, we introduce an effective and scalable pretrain-finetune framework for enhancing the generalization capabilities of the traditional CC-ReID models. The extensive experiments demonstrate that two typical models namely TransReID and FIRe^2, when integrated into our framework, outperform other state-of-the-art models after pretraining on CCUP and finetuning on the benchmarks such as PRCC, VC-Clothes and NKUP. The CCUP is available at: https://github.com/yjzhao1019/CCUP.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.751953125
        },
        {
            "paperId": "cf774cc18a316c8189493206dd37cb60500fe5a2",
            "corpusId": 272910800,
            "title": "KIPPS: Knowledge infusion in Privacy Preserving Synthetic Data Generation",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 34,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.17315, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2040711822",
                    "name": "Anantaa Kotal"
                },
                {
                    "authorId": "2244620873",
                    "name": "Anupam Joshi"
                }
            ],
            "abstract": "The integration of privacy measures, including differential privacy techniques, ensures a provable privacy guarantee for the synthetic data. However, challenges arise for Generative Deep Learning models when tasked with generating realistic data, especially in critical domains such as Cybersecurity and Healthcare. Generative Models optimized for continuous data struggle to model discrete and non-Gaussian features that have domain constraints. Challenges increase when the training datasets are limited and not diverse. In such cases, generative models create synthetic data that repeats sensitive features, which is a privacy risk. Moreover, generative models face difficulties comprehending attribute constraints in specialized domains. This leads to the generation of unrealistic data that impacts downstream accuracy. To address these issues, this paper proposes a novel model, KIPPS, that infuses Domain and Regulatory Knowledge from Knowledge Graphs into Generative Deep Learning models for enhanced Privacy Preserving Synthetic data generation. The novel framework augments the training of generative models with supplementary context about attribute values and enforces domain constraints during training. This added guidance enhances the model's capacity to generate realistic and domain-compliant synthetic data. The proposed model is evaluated on real-world datasets, specifically in the domains of Cybersecurity and Healthcare, where domain constraints and rules add to the complexity of the data. Our experiments evaluate the privacy resilience and downstream accuracy of the model against benchmark methods, demonstrating its effectiveness in addressing the balance between privacy preservation and data accuracy in complex domains.",
            "corpus_id": "272910800",
            "text": "The integration of privacy measures, including differential privacy techniques, ensures a provable privacy guarantee for the synthetic data. However, challenges arise for Generative Deep Learning models when tasked with generating realistic data, especially in critical domains such as Cybersecurity and Healthcare. Generative Models optimized for continuous data struggle to model discrete and non-Gaussian features that have domain constraints. Challenges increase when the training datasets are limited and not diverse. In such cases, generative models create synthetic data that repeats sensitive features, which is a privacy risk. Moreover, generative models face difficulties comprehending attribute constraints in specialized domains. This leads to the generation of unrealistic data that impacts downstream accuracy. To address these issues, this paper proposes a novel model, KIPPS, that infuses Domain and Regulatory Knowledge from Knowledge Graphs into Generative Deep Learning models for enhanced Privacy Preserving Synthetic data generation. The novel framework augments the training of generative models with supplementary context about attribute values and enforces domain constraints during training. This added guidance enhances the model's capacity to generate realistic and domain-compliant synthetic data. The proposed model is evaluated on real-world datasets, specifically in the domains of Cybersecurity and Healthcare, where domain constraints and rules add to the complexity of the data. Our experiments evaluate the privacy resilience and downstream accuracy of the model against benchmark methods, demonstrating its effectiveness in addressing the balance between privacy preservation and data accuracy in complex domains.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.701171875
        }
    ],
    "quotes": {
        "cost": 0.263889,
        "quotes": [
            {
                "idx": 0,
                "key": "[249191979 | Raisa et al. | 2022 | Citations: 11]",
                "snippets": "There is a sizable literature on DP synthetic data generation. Most recent work in the area either releases the values of a set of simple queries, such as counting queries, under DP and uses them as the basis of synthetic data (Ayd\u00f6re et al., 2021)(Bernstein et al., 2017)(Cai et al., 2021)(Chen et al., 2015)(Hardt et al., 2010)(Liu et al., 2021)(McKenna et al., 2018)(McKenna et al., 2021)(McKenna et al., 2022)(McKenna et al., 2019)(Nixon et al., 2022)(Vietri et al., 2020)(Zhang et al., 2014), or trains some kind of generative model, often a GAN, using the whole real dataset under DP (D. (Chen et al., 2020)J\u00e4lk\u00f6 et al. 2021;(Long et al., 2019)Xie et al. 2018;(Jordon et al., 2018). There are also hybrid approaches that use sophisticated queries that can capture all features of the dataset, and train a generative model using those (Harder et al., 2020)(Liew et al., 2021).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[211203580 | Vietri et al. | 2020 | Citations: 77]": "We present three new algorithms for constructing differentially private synthetic data---a sanitized version of a sensitive dataset that approximately preserves the answers to a large collection of statistical queries. All three algorithms are \\emph{oracle-efficient} in the sense that they are computationally efficient when given access to an optimization oracle. Such an oracle can be implemented using many existing (non-private) optimization tools such as sophisticated integer program solvers. While the accuracy of the synthetic data is contingent on the oracle's optimization performance, the algorithms satisfy differential privacy even in the worst case. For all three algorithms, we provide theoretical guarantees for both accuracy and privacy. Through empirical evaluation, we demonstrate that our methods scale well with both the dimensionality of the data and the number of queries. Compared to the state-of-the-art method High-Dimensional Matrix Mechanism \\cite{McKennaMHM18}, our algorithms provide better accuracy in the large workload and high privacy regime (corresponding to low privacy loss $\\varepsilon$).",
                    "[219687936 | Chen et al. | 2020 | Citations: 185]": "The wide-spread availability of rich data has fueled the growth of machine learning applications in numerous domains. However, growth in domains with highly-sensitive data (e.g., medical) is largely hindered as the private nature of data prohibits it from being shared. To this end, we propose Gradient-sanitized Wasserstein Generative Adversarial Networks (GS-WGAN), which allows releasing a sanitized form of the sensitive data with rigorous privacy guarantees. In contrast to prior work, our approach is able to distort gradient information more precisely, and thereby enabling training deeper models which generate more informative samples. Moreover, our formulation naturally allows for training GANs in both centralized and federated (i.e., decentralized) data scenarios. Through extensive experiments, we find our approach consistently outperforms state-of-the-art approaches across multiple metrics (e.g., sample quality) and datasets.",
                    "[225077562 | Harder et al. | 2020 | Citations: 101]": "We propose a differentially private data generation paradigm using random feature representations of kernel mean embeddings when comparing the distribution of true data with that of synthetic data. We exploit the random feature representations for two important benefits. First, we require a minimal privacy cost for training deep generative models. This is because unlike kernel-based distance metrics that require computing the kernel matrix on all pairs of true and synthetic data points, we can detach the data-dependent term from the term solely dependent on synthetic data. Hence, we need to perturb the data-dependent term once and for all and then use it repeatedly during the generator training. Second, we can obtain an analytic sensitivity of the kernel mean embedding as the random features are norm bounded by construction. This removes the necessity of hyper-parameter search for a clipping norm to handle the unknown sensitivity of a generator network. We provide several variants of our algorithm, differentially private mean embeddings with random features (DP-MERF) to jointly generate labels and input features for datasets such as heterogeneous tabular data and image data. Our algorithm achieves drastically better privacy-utility trade-offs than existing methods when tested on several datasets.",
                    "[232185574 | Aydore et al. | 2021 | Citations: 68]": "We propose, implement, and evaluate a new algorithm for releasing answers to very large numbers of statistical queries like $k$-way marginals, subject to differential privacy. Our algorithm makes adaptive use of a continuous relaxation of the Projection Mechanism, which answers queries on the private dataset using simple perturbation, and then attempts to find the synthetic dataset that most closely matches the noisy answers. We use a continuous relaxation of the synthetic dataset domain which makes the projection loss differentiable, and allows us to use efficient ML optimization techniques and tooling. Rather than answering all queries up front, we make judicious use of our privacy budget by iteratively and adaptively finding queries for which our (relaxed) synthetic data has high error, and then repeating the projection. We perform extensive experimental evaluations across a range of parameters and datasets, and find that our method outperforms existing algorithms in many cases, especially when the privacy budget is small or the query class is large.",
                    "[2334513 | Bernstein et al. | 2017 | Citations: 25]": "We investigate the problem of learning discrete, undirected graphical models in a differentially private way. We show that the approach of releasing noisy sufficient statistics using the Laplace mechanism achieves a good trade-off between privacy, utility, and practicality. A naive learning algorithm that uses the noisy sufficient statistics \"as is\" outperforms general-purpose differentially private learning algorithms. However, it has three limitations: it ignores knowledge about the data generating process, rests on uncertain theoretical foundations, and exhibits certain pathologies. We develop a more principled approach that applies the formalism of collective graphical models to perform inference over the true sufficient statistics within an expectation-maximization framework. We show that this learns better models than competing approaches on both synthetic data and on real human mobility data used as a case study.",
                    "[235377198 | Liew et al. | 2021 | Citations: 29]": "We propose a new framework of synthesizing data using deep generative models in a differentially private manner. Within our framework, sensitive data are sanitized with rigorous privacy guarantees in a one-shot fashion, such that training deep generative models is possible without re-using the original data. Hence, no extra privacy costs or model constraints are incurred, in contrast to popular approaches such as Differentially Private Stochastic Gradient Descent (DP-SGD), which, among other issues, causes degradation in privacy guarantees as the training iteration increases. We demonstrate a realization of our framework by making use of the characteristic function and an adversarial re-weighting objective, which are of independent interest as well. Our proposal has theoretical guarantees of performance, and empirical evaluations on multiple datasets show that our approach outperforms other methods at reasonable levels of privacy.",
                    "[235421632 | Liu et al. | 2021 | Citations: 64]": "We study private synthetic data generation for query release, where the goal is to construct a sanitized version of a sensitive dataset, subject to differential privacy, that approximately preserves the answers to a large collection of statistical queries. We first present an algorithmic framework that unifies a long line of iterative algorithms in the literature. Under this framework, we propose two new methods. The first method, private entropy projection (PEP), can be viewed as an advanced variant of MWEM that adaptively reuses past query measurements to boost accuracy. Our second method, generative networks with the exponential mechanism (GEM), circumvents computational bottlenecks in algorithms such as MWEM and PEP by optimizing over generative models parameterized by neural networks, which capture a rich family of distributions while enabling fast gradient-based optimization. We demonstrate that PEP and GEM empirically outperform existing algorithms. Furthermore, we show that GEM nicely incorporates prior information from public data while overcoming limitations of PMW^Pub, the existing state-of-the-art method that also leverages public data.",
                    "[236976348 | McKenna et al. | 2021 | Citations: 126]": "We propose a general approach for differentially private synthetic data generation, that consists of three steps: (1) select a collection of low-dimensional marginals, (2) measure those marginals with a noise addition mechanism, and (3)\u00a0generate synthetic data that preserves the measured marginals well. Central to this approach is Private-PGM, a post-processing method that is used to estimate a high-dimensional data distribution from noisy measurements of its marginals. We present two mechanisms, NIST-MST and MST, that are instances of this general approach. NIST-MST was the winning mechanism in the 2018 NIST differential privacy synthetic data competition, and MST is a new mechanism that can work in more general settings, while still performing comparably to NIST-MST. We believe our general approach should be of broad interest, and can be adopted in future mechanisms for synthetic data generation.",
                    "[237012543 | Cai et al. | 2021 | Citations: 67]": "\n This paper studies the synthesis of high-dimensional datasets with differential privacy (DP). The state-of-the-art solution addresses this problem by first generating a set\n M\n of noisy low-dimensional marginals of the input data\n D\n , and then use them to approximate the data distribution in\n D\n for synthetic data generation. However, it imposes several constraints on\n M\n that considerably limits the choices of marginals. This makes it difficult to capture all important correlations among attributes, which in turn degrades the quality of the resulting synthetic data.\n \n \n To address the above deficiency, we propose PrivMRF, a method that (i) also utilizes a set\n M\n of low-dimensional marginals for synthesizing high-dimensional data with DP, but (ii) provides a high degree of flexibility in the choices of marginals. The key idea of PrivMRF is to select an appropriate\n M\n to construct a\n Markov random field (MRF)\n that models the correlations among the attributes in the input data, and then use the MRF for data synthesis. Experimental results on four benchmark datasets show that PrivMRF consistently outperforms the state of the art in terms of the accuracy of counting queries and classification tasks conducted on the synthetic data generated.\n",
                    "[245634703 | Long et al. | 2019 | Citations: 74]": "Recent advances in machine learning have largely benefited from the massive accessible training data. However, large-scale data sharing has raised great privacy concerns. In this work, we propose a novel privacy-preserving data Generative model based on the PATE framework (G-PATE), aiming to train a scalable differentially private data generator that preserves high generated data utility. Our approach leverages generative adversarial nets to generate data, combined with private aggregation among different discriminators to ensure strong privacy guarantees. Compared to existing approaches, G-PATE significantly improves the use of privacy budgets. In particular, we train a student data generator with an ensemble of teacher discriminators and propose a novel private gradient aggregation mechanism to ensure differential privacy on all information that flows from teacher discriminators to the student generator. In addition, with random projection and gradient discretization, the proposed gradient aggregation mechanism is able to effectively deal with high-dimensional gradient vectors. Theoretically, we prove that G-PATE ensures differential privacy for the data generator. Empirically, we demonstrate the superiority of G-PATE over prior work through extensive experiments. We show that G-PATE is the first work being able to generate high-dimensional image data with high data utility under limited privacy budgets ($\\epsilon \\le 1$). Our code is available at https://github.com/AI-secure/G-PATE.",
                    "[246275811 | Nixon et al. | 2022 | Citations: 2]": "Several algorithms exist for creating differentially private counts from contingency tables, such as two-way or three-way marginal counts. The resulting noisy counts generally do not correspond to a coherent contingency table, so that some post-processing step is needed if one wants the released counts to correspond to a coherent contingency table. We present a latent class modeling approach for post-processing differentially private marginal counts that can be used (i) to create differentially private synthetic data from the set of marginal counts, and (ii) to enable posterior inferences about the confidential counts. We illustrate the approach using a subset of the 2016 American Community Survey Public Use Microdata Sets and the 2004 National Long Term Care Survey.",
                    "[246430835 | McKenna et al. | 2022 | Citations: 86]": "We propose AIM, a new algorithm for differentially private synthetic data generation. AIM is a workload-adaptive algorithm within the paradigm of algorithms that first selects a set of queries, then privately measures those queries, and finally generates synthetic data from the noisy measurements. It uses a set of innovative features to iteratively select the most useful measurements, reflecting both their relevance to the workload and their value in approximating the input data. We also provide analytic expressions to bound per-query error with high probability which can be used to construct confidence intervals and inform users about the accuracy of generated data. We show empirically that AIM consistently outperforms a wide variety of existing mechanisms across a variety of experimental settings.",
                    "[51869463 | McKenna et al. | 2018 | Citations: 114]": "Differentially private algorithms for answering sets of predicate counting queries on a sensitive database have many applications. Organizations that collect individual-level data, such as statistical agencies and medical institutions, use them to safely release summary tabulations. However, existing techniques are accurate only on a narrow class of query workloads, or are extremely slow, especially when analyzing more than one or two dimensions of the data.\n In this work we propose HDMM, a new differentially private algorithm for answering a workload of predicate counting queries, that is especially effective for higher-dimensional datasets. HDMM represents query workloads using an implicit matrix representation and exploits this compact representation to efficiently search (a subset of) the space of differentially private algorithms for one that answers the input query workload with high accuracy. We empirically show that HDMM can efficiently answer queries with lower error than state-of-the-art techniques on a variety of low and high dimensional datasets.",
                    "[59317012 | McKenna et al. | 2019 | Citations: 144]": "Many privacy mechanisms reveal high-level information about a data distribution through noisy measurements. It is common to use this information to estimate the answers to new queries. In this work, we provide an approach to solve this estimation problem efficiently using graphical models, which is particularly effective when the distribution is high-dimensional but the measurements are over low-dimensional marginals. We show that our approach is far more efficient than existing estimation techniques from the privacy literature and that it can improve the accuracy and scalability of many state-of-the-art mechanisms.",
                    "[947901 | Hardt et al. | 2010 | Citations: 518]": "We present a new algorithm for differentially private data release, based on a simple combination of the Multiplicative Weights update rule with the Exponential Mechanism. Our MWEM algorithm achieves what are the best known and nearly optimal theoretical guarantees, while at the same time being simple to implement and experimentally more accurate on actual data sets than existing techniques."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 851,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 62
                            },
                            {
                                "start": 63,
                                "end": 661
                            },
                            {
                                "start": 662,
                                "end": 851
                            }
                        ],
                        "ref_mentions": [
                            "232185574",
                            "2334513",
                            "237012543",
                            "15417421",
                            "947901",
                            "235421632",
                            "51869463",
                            "236976348",
                            "246430835",
                            "59317012",
                            "246275811",
                            "211203580",
                            "13095103",
                            "219687936",
                            "245634703",
                            "53342261",
                            "225077562",
                            "235377198"
                        ],
                        "quote": "There is a sizable literature on DP synthetic data generation. Most recent work in the area either releases the values of a set of simple queries, such as counting queries, under DP and uses them as the basis of synthetic data (Ayd\u00f6re et al., 2021)(Bernstein et al., 2017)(Cai et al., 2021)(Chen et al., 2015)(Hardt et al., 2010)(Liu et al., 2021)(McKenna et al., 2018)(McKenna et al., 2021)(McKenna et al., 2022)(McKenna et al., 2019)(Nixon et al., 2022)(Vietri et al., 2020)(Zhang et al., 2014), or trains some kind of generative model, often a GAN, using the whole real dataset under DP (D. (Chen et al., 2020)J\u00e4lk\u00f6 et al. 2021;(Long et al., 2019)Xie et al. 2018;(Jordon et al., 2018). There are also hybrid approaches that use sophisticated queries that can capture all features of the dataset, and train a generative model using those (Harder et al., 2020)(Liew et al., 2021)."
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[250390686 | Maharana et al. | 2022 | Citations: 96]",
                "snippets": "Recent advances in commonsense reasoning have been fueled by the availability of large-scale human annotated datasets. Manual annotation of such datasets, many of which are based on existing knowledge bases, is expensive and not scalable. Moreover, it is challenging to build augmentation data for commonsense reasoning because the synthetic questions need to adhere to real-world scenarios. Hence, we present GraDA, a graph-generative data augmentation framework to synthesize factual data samples from knowledge graphs for commonsense reasoning datasets. First, we train a graph-to-text model for conditional generation of questions from graph entities and relations. Then, we train a generator with GAN loss to generate distractors for synthetic questions.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Recent advances in commonsense reasoning have been fueled by the availability of large-scale human annotated datasets. Manual annotation of such datasets, many of which are based on existing knowledge bases, is expensive and not scalable. Moreover, it is challenging to build augmentation data for commonsense reasoning because the synthetic questions need to adhere to real-world scenarios. Hence, we present GraDA, a graph-generative data augmentation framework to synthesize factual data samples from knowledge graphs for commonsense reasoning datasets. First, we train a graph-to-text model for conditional generation of questions from graph entities and relations. Then, we train a generator with GAN loss to generate distractors for synthetic questions.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[253384598 | Sadat et al. | 2022 | Citations: 2]",
                "snippets": "With the advancement of text generation models in recent years (Radford et al., 2019)Brown et al., 2020;(Lewis et al., 2019), researchers have started to adopt synthetic text generation as a data augmentation strategy for various tasks such as commonsense reasoning (Maharana et al., 2022), sentence classification (Anaby-Tavor et al., 2020) and question answering (Puri et al., 2020). More recently, synthetic text generation methods have also been employed to remove spurious correlations in human annotated data for fact verification (Lee et al., 2021) and NLI (Wu et al., 2022).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[250390686 | Maharana et al. | 2022 | Citations: 96]": "Recent advances in commonsense reasoning have been fueled by the availability of large-scale human annotated datasets. Manual annotation of such datasets, many of which are based on existing knowledge bases, is expensive and not scalable. Moreover, it is challenging to build augmentation data for commonsense reasoning because the synthetic questions need to adhere to real-world scenarios. Hence, we present GraDA, a graph-generative data augmentation framework to synthesize factual data samples from knowledge graphs for commonsense reasoning datasets. First, we train a graph-to-text model for conditional generation of questions from graph entities and relations. Then, we train a generator with GAN loss to generate distractors for synthetic questions. Our approach improves performance for SocialIQA, CODAH, HellaSwag and CommonsenseQA, and works well for generative tasks like ProtoQA. We show improvement in robustness to semantic adversaries after training with GraDA and provide human evaluation of the quality of synthetic datasets in terms of factuality and answerability. Our work provides evidence and encourages future research into graph-based generative data augmentation.",
                    "[204960716 | Lewis et al. | 2019 | Citations: 10856]": "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.",
                    "[211258652 | Puri et al. | 2020 | Citations: 159]": "Question and answer generation is a data augmentation method that aims to improve question answering (QA) models given the limited amount of human labeled data. However, a considerable gap remains between synthetic and human-generated question-answer pairs. This work aims to narrow this gap by taking advantage of large language models and explores several factors such as model size, quality of pretrained models, scale of data synthesized, and algorithmic choices. On the SQuAD1.1 question answering task, we achieve higher accuracy using solely synthetic questions and answers than when using the SQuAD1.1 training set questions alone. Removing access to real Wikipedia data, we synthesize questions and answers from a synthetic corpus generated by an 8.3 billion parameter GPT-2 model. With no access to human supervision and only access to other models, we are able to train state of the art question answering networks on entirely model-generated data that achieve 88.4 Exact Match (EM) and 93.9 F1 score on the SQuAD1.1 dev set. We further apply our methodology to SQuAD2.0 and show a 2.8 absolute gain on EM score compared to prior work using synthetic data.",
                    "[238227205 | Lee et al. | 2021 | Citations: 30]": "Fact verification datasets are typically constructed using crowdsourcing techniques due to the lack of text sources with veracity labels. However, the crowdsourcing process often produces undesired biases in data that cause models to learn spurious patterns. In this paper, we propose CrossAug, a contrastive data augmentation method for debiasing fact verification models. Specifically, we employ a two-stage augmentation pipeline to generate new claims and evidences from existing samples. The generated samples are then paired cross-wise with the original pair, forming contrastive samples that facilitate the model to rely less on spurious patterns and learn more robust representations. Experimental results show that our method outperforms the previous state-of-the-art debiasing technique by 3.6% on the debiased extension of the FEVER dataset, with a total performance boost of 10.13% from the baseline. Furthermore, we evaluate our approach in data-scarce settings, where models can be more susceptible to biases due to the lack of training data. Experimental results demonstrate that our approach is also effective at debiasing in these low-resource conditions, exceeding the baseline performance on the Symmetric dataset with just 1% of the original data.",
                    "[247628095 | Wu et al. | 2022 | Citations: 68]": "Natural language processing models often exploit spurious correlations between task-independent features and labels in datasets to perform well only within the distributions they are trained on, while not generalising to different task distributions. We propose to tackle this problem by generating a debiased version of a dataset, which can then be used to train a debiased, off-the-shelf model, by simply replacing its training data. Our approach consists of 1) a method for training data generators to generate high-quality, label-consistent data samples; and 2) a filtering mechanism for removing data points that contribute to spurious correlations, measured in terms of z-statistics. We generate debiased versions of the SNLI and MNLI datasets, and we evaluate on a large suite of debiased, out-of-distribution, and adversarial test sets. Results show that models trained on our debiased datasets generalise better than those trained on the original datasets in all settings. On the majority of the datasets, our method outperforms or performs comparably to previous state-of-the-art debiasing strategies, and when combined with an orthogonal technique, product-of-experts, it improves further and outperforms previous best results of SNLI-hard and MNLI-hard."
                },
                "metadata": [
                    {
                        "section_title": "Synthetic Data Generation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 577,
                        "sentence_offsets": [],
                        "ref_mentions": [
                            "160025533",
                            "204960716",
                            "250390686",
                            "211258652",
                            "238227205",
                            "247628095"
                        ],
                        "quote": "With the advancement of text generation models in recent years (Radford et al., 2019)Brown et al., 2020;(Lewis et al., 2019), researchers have started to adopt synthetic text generation as a data augmentation strategy for various tasks such as commonsense reasoning (Maharana et al., 2022), sentence classification (Anaby-Tavor et al., 2020) and question answering (Puri et al., 2020). More recently, synthetic text generation methods have also been employed to remove spurious correlations in human annotated data for fact verification (Lee et al., 2021) and NLI (Wu et al., 2022)."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[254877131 | Li et al. | 2022 | Citations: 6]",
                "snippets": "To address the insufficient annotation challenge, we present a self-training framework for unsupervised complex tabular reasoning (UCTR-ST) by generating diverse synthetic data with complex logic. Specifically, UCTR-ST incorporates several essential techniques: we aggregate diverse programs and execute them on tables based on a \"Program-Management\" component, and we bridge the gap between programs and text with a powerful \"Program-Transformation\" module that generates natural language sentences with complex logic. Furthermore, we optimize the procedure using \"Table-Text Manipulator\" to handle joint table-text reasoning scenarios.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "To address the insufficient annotation challenge, we present a self-training framework for unsupervised complex tabular reasoning (UCTR-ST) by generating diverse synthetic data with complex logic. Specifically, UCTR-ST incorporates several essential techniques: we aggregate diverse programs and execute them on tables based on a \"Program-Management\" component, and we bridge the gap between programs and text with a powerful \"Program-Transformation\" module that generates natural language sentences with complex logic. Furthermore, we optimize the procedure using \"Table-Text Manipulator\" to handle joint table-text reasoning scenarios.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[257378179 | Josifoski et al. | 2023 | Citations: 85]",
                "snippets": "Large language models (LLMs) have great potential for synthetic data generation. This work shows that useful data can be synthetically generated even for tasks that cannot be solved directly by LLMs: for problems with structured outputs, it is possible to prompt an LLM to perform the task in the reverse direction, by generating plausible input text for a target output structure. Leveraging this asymmetry in task difficulty makes it possible to produce large-scale, high-quality data for complex tasks. We demonstrate the effectiveness of this approach on closed information extraction, where collecting ground-truth data is challenging, and no satisfactory dataset exists to date.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Large language models (LLMs) have great potential for synthetic data generation. This work shows that useful data can be synthetically generated even for tasks that cannot be solved directly by LLMs: for problems with structured outputs, it is possible to prompt an LLM to perform the task in the reverse direction, by generating plausible input text for a target output structure. Leveraging this asymmetry in task difficulty makes it possible to produce large-scale, high-quality data for complex tasks. We demonstrate the effectiveness of this approach on closed information extraction, where collecting ground-truth data is challenging, and no satisfactory dataset exists to date.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[257607791 | Natarajan et al. | 2023 | Citations: 2]",
                "snippets": "We therefore exploit advances in open-source plugins and game engines, combined with unique techniques in synthetic data generation to perform state-of-the-art synthetic to real transfer learning, and make the following contributions. \n\n1. We present a synthetic data generation framework with an approach of introducing background complexity to synthetic images, in addition to the ability to programmatically vary rotation, lighting, backgrounds, and scale, making the resulting classifier very robust. We have made our framework publicly available (https://github.com/saiabinesh/hybrid-synth), which can be used to generate a dataset with any number of arbitrary classes.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": ".3.1).",
                        "pdf_hash": "",
                        "start": 7,
                        "end": 681,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 241
                            },
                            {
                                "start": 244,
                                "end": 511
                            },
                            {
                                "start": 512,
                                "end": 681
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "We therefore exploit advances in open-source plugins and game engines, combined with unique techniques in synthetic data generation to perform state-of-the-art synthetic to real transfer learning, and make the following contributions. \n\n1. We present a synthetic data generation framework with an approach of introducing background complexity to synthetic images, in addition to the ability to programmatically vary rotation, lighting, backgrounds, and scale, making the resulting classifier very robust. We have made our framework publicly available (https://github.com/saiabinesh/hybrid-synth), which can be used to generate a dataset with any number of arbitrary classes."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[258170048 | Brandt et al. | 2023 | Citations: 1]",
                "snippets": "We consider the problem of synthetically generating data that can closely resemble human decisions made in the context of an interactive human-AI system like a computer game. We propose a novel algorithm that can generate synthetic, human-like, decision making data while starting from a very small set of decision making data collected from humans. Our proposed algorithm integrates the concept of reward shaping with an imitation learning algorithm to generate the synthetic data. We have validated our synthetic data generation technique by using the synthetically generated data as a surrogate for human interaction data to solve three sequential decision making tasks of increasing complexity within a small computer game-like setup.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 738,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "We consider the problem of synthetically generating data that can closely resemble human decisions made in the context of an interactive human-AI system like a computer game. We propose a novel algorithm that can generate synthetic, human-like, decision making data while starting from a very small set of decision making data collected from humans. Our proposed algorithm integrates the concept of reward shaping with an imitation learning algorithm to generate the synthetic data. We have validated our synthetic data generation technique by using the synthetically generated data as a surrogate for human interaction data to solve three sequential decision making tasks of increasing complexity within a small computer game-like setup."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[258461502 | Rose et al. | 2023 | Citations: 47]",
                "snippets": "We propose Visual Chain-of-Thought (VCOT), which combines the efficiency, robustness, and multi-step reasoning of COT with the multimodal capabilities of vision-language models. VCOT synthetically augments sequential datasets and bridges logical gaps by recursively generating multimodal infillings and using the synthetic data to improve downstream task performance. These synthetic generations also serve as human-interpretable insights into AI systems' ability of multi-step reasoning. We demonstrate that VCOT creates consistent and novel synthetic data that enhances downstream performance on the VIST (Huang et al., 2016) and WIKIHOW (Koupaee & Wang, 2018) datasets.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "INTRODUCTION",
                        "pdf_hash": "",
                        "start": 429,
                        "end": 1101,
                        "sentence_offsets": [
                            {
                                "start": 429,
                                "end": 606
                            },
                            {
                                "start": 607,
                                "end": 796
                            },
                            {
                                "start": 797,
                                "end": 917
                            },
                            {
                                "start": 918,
                                "end": 1101
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "We propose Visual Chain-of-Thought (VCOT), which combines the efficiency, robustness, and multi-step reasoning of COT with the multimodal capabilities of vision-language models. VCOT synthetically augments sequential datasets and bridges logical gaps by recursively generating multimodal infillings and using the synthetic data to improve downstream task performance. These synthetic generations also serve as human-interpretable insights into AI systems' ability of multi-step reasoning. We demonstrate that VCOT creates consistent and novel synthetic data that enhances downstream performance on the VIST (Huang et al., 2016) and WIKIHOW (Koupaee & Wang, 2018) datasets."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[265456227 | Jain et al. | 2023 | Citations: 32]",
                "snippets": "Synthetic data for LLMS. Recent works have explored using synthetic datasets for generalpurpose or task-specific finetuning of LLMS. These approaches work by generating synthetic datasets from a strong LLM (like GPT-3.5-TURBO or GPT-4) using a set of existing tasks (Taori et al., 2023;Chiang et al., 2023) or generating new tasks using self-instruct (Wang et al., 2022) or evol-instruct (Xu et al., 2023) approaches. This has been also applied for task-specific finetuningin common-sense reasoning (West et al., 2021)), text-summarization (Sclar et al., 2022), mathematical reasoning (Luo et al., 2023a;Yue et al., 2023), tool use (Patil et al., 2023), coding (Luo et al., 2023b), and general-purpose reasoning (Li et al., 2023); (Zelikman et al., 2022). \n\nMore specifically, Yue et al. (2023) curates diverse corpus of mathematics problems with chain-ofthought or program-of-thought (Chen et al., 2022b) annotations for mathematical reasoning analogous to our plans. Gunasekar et al. (2023) proposed pre-training models on programming \"textbooks\" generated synthetically from GPT-3.5-TURBO. (Haluptzok et al., 2022) similarly generates programming puzzles and corresponding solutions from language models.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[238857304 | West et al. | 2021 | Citations: 333]": "The common practice for training commonsense models has gone from\u2013human\u2013to\u2013corpus\u2013to\u2013machine: humans author commonsense knowledge graphs in order to train commonsense models. In this work, we investigate an alternative, from\u2013machine\u2013to\u2013corpus\u2013to\u2013machine: general language models author these commonsense knowledge graphs to train commonsense models. Our study leads to a new framework, Symbolic Knowledge Distillation. As with prior art in Knowledge Distillation (Hinton et al. 2015), our approach uses larger models to teach smaller models. A key difference is that we distill knowledge symbolically\u2013as text\u2013in addition to the neural model. We distill only one aspect\u2013the commonsense of a general language model teacher, allowing the student to be a different type, a commonsense model. Altogether, we show that careful prompt engineering and a separately trained critic model allow us to selectively distill high-quality causal commonsense from GPT-3, a general language model. Empirical results demonstrate that, for the first time, a human-authored commonsense knowledge graph is surpassed by our automatically distilled variant in all three criteria: quantity, quality, and diversity. In addition, it results in a neural commonsense model that surpasses the teacher model\u2019s commonsense capabilities despite its 100x smaller size. We apply this to the ATOMIC resource, and will share our new symbolic knowledge graph and commonsense models.",
                    "[247762790 | Zelikman et al. | 2022 | Citations: 510]": "Generating step-by-step\"chain-of-thought\"rationales improves language model performance on complex reasoning tasks like mathematics or commonsense question-answering. However, inducing language model rationale generation currently requires either constructing massive rationale datasets or sacrificing accuracy by using only few-shot inference. We propose a technique to iteratively leverage a small number of rationale examples and a large dataset without rationales, to bootstrap the ability to perform successively more complex reasoning. This technique, the\"Self-Taught Reasoner\"(STaR), relies on a simple loop: generate rationales to answer many questions, prompted with a few rationale examples; if the generated answers are wrong, try again to generate a rationale given the correct answer; fine-tune on all the rationales that ultimately yielded correct answers; repeat. We show that STaR significantly improves performance on multiple datasets compared to a model fine-tuned to directly predict final answers, and performs comparably to fine-tuning a 30$\\times$ larger state-of-the-art language model on CommensenseQA. Thus, STaR lets a model improve itself by learning from its own generated reasoning.",
                    "[251197051 | Haluptzok et al. | 2022 | Citations: 82]": "Recent Language Models (LMs) achieve breakthrough performance in code generation when trained on human-authored problems, even solving some competitive-programming problems. Self-play has proven useful in games such as Go, and thus it is natural to ask whether LMs can generate their own instructive programming problems to improve their performance. We show that it is possible for an LM to synthesize programming problems and solutions, which are filtered for correctness by a Python interpreter. The LM's performance is then seen to improve when it is fine-tuned on its own synthetic problems and verified solutions; thus the model 'improves itself' using the Python interpreter. Problems are specified formally as programming puzzles [Schuster et al., 2021], a code-based problem format where solutions can easily be verified for correctness by execution. In experiments on publicly-available LMs, test accuracy more than doubles. This work demonstrates the potential for code LMs, with an interpreter, to generate instructive problems and improve their own performance.",
                    "[259251773 | Li et al. | 2023 | Citations: 143]": "Chain-of-thought prompting (e.g., \u201cLet\u2019s think step-by-ste\u201d) primes large language models to verbalize rationalization for their predictions. While chain-of-thought can lead to dramatic performance gains, benefits appear to emerge only for sufficiently large models (beyond 50B parameters). We show that orders-of-magnitude smaller models (125M\u20141.3B parameters) can still benefit from chain-of-thought prompting. To achieve this, we introduce Symbolic Chain-of-Thought Distillation (SCoTD), a method to train a smaller student model on rationalizations sampled from a significantly larger teacher model. Experiments across several commonsense benchmarks show that: 1) SCoTD enhances the performance of the student model in both supervised and few-shot settings, and especially for challenge sets; 2) sampling many reasoning chains per instance from the teacher is paramount; and 3) after distillation, student chain-of-thoughts are judged by humans as comparable to the teacher, despite orders of magnitude fewer parameters. We test several hypotheses regarding what properties of chain-of-thought samples are important, e.g., diversity vs. teacher likelihood vs. open-endedness. We release our corpus of chain-of-thought samples and code."
                },
                "metadata": [
                    {
                        "section_title": "RELATED WORK",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1204,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 24
                            },
                            {
                                "start": 25,
                                "end": 132
                            },
                            {
                                "start": 133,
                                "end": 417
                            },
                            {
                                "start": 418,
                                "end": 753
                            },
                            {
                                "start": 756,
                                "end": 966
                            },
                            {
                                "start": 967,
                                "end": 1090
                            },
                            {
                                "start": 1091,
                                "end": 1204
                            }
                        ],
                        "ref_mentions": [
                            "238857304",
                            "259251773",
                            "247762790",
                            "251197051"
                        ],
                        "quote": "Synthetic data for LLMS. Recent works have explored using synthetic datasets for generalpurpose or task-specific finetuning of LLMS. These approaches work by generating synthetic datasets from a strong LLM (like GPT-3.5-TURBO or GPT-4) using a set of existing tasks (Taori et al., 2023;Chiang et al., 2023) or generating new tasks using self-instruct (Wang et al., 2022) or evol-instruct (Xu et al., 2023) approaches. This has been also applied for task-specific finetuningin common-sense reasoning (West et al., 2021)), text-summarization (Sclar et al., 2022), mathematical reasoning (Luo et al., 2023a;Yue et al., 2023), tool use (Patil et al., 2023), coding (Luo et al., 2023b), and general-purpose reasoning (Li et al., 2023); (Zelikman et al., 2022). \n\nMore specifically, Yue et al. (2023) curates diverse corpus of mathematics problems with chain-ofthought or program-of-thought (Chen et al., 2022b) annotations for mathematical reasoning analogous to our plans. Gunasekar et al. (2023) proposed pre-training models on programming \"textbooks\" generated synthetically from GPT-3.5-TURBO. (Haluptzok et al., 2022) similarly generates programming puzzles and corresponding solutions from language models."
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[268041519 | Fang et al. | 2024 | Citations: 81]",
                "snippets": "Recent advancement in generative models such as Variational Autoencoders (VAEs) (Ma et al., 2020;Darabi & Elor, 2021;(Kok et al., 2020)(Liu et al., 2023)Xu et al., 2023b)), generative adversarial networks (GANs) (Park et al., 2018)Choi et al., 2018;(Baowaly et al., 2018)Xu et al., 2019), diffusion models (Kotelnikov et al., 2022;(Xu et al., 2023)(Kim et al., 2022)a;Lee et al., 2023;Zhang et al., 2023c), and LLMs, opened up many new opportunities. These deep learning approaches have demonstrated superior performance over classical methods such as Bayesian networks ( (Xu et al., 2019)).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[249847841 | Kim et al. | 2022 | Citations: 38]": "Score-based generative models (SGMs) are a recent breakthrough in generating fake images. SGMs are known to surpass other generative models, e.g., generative adversarial networks (GANs) and variational autoencoders (VAEs). Being inspired by their big success, in this work, we fully customize them for generating fake tabular data. In particular, we are interested in oversampling minor classes since imbalanced classes frequently lead to sub-optimal training outcomes. To our knowledge, we are the first presenting a score-based tabular data oversampling method. Firstly, we re-design our own score network since we have to process tabular data. Secondly, we propose two options for our generation method: the former is equivalent to a style transfer for tabular data and the latter uses the standard generative policy of SGMs. Lastly, we define a fine-tuning method, which further enhances the oversampling quality. In our experiments with 6 datasets and 10 baselines, our method outperforms other oversampling methods in all cases.",
                    "[264439324 | Xu et al. | 2023 | Citations: 18]": "In the burgeoning field of natural language processing (NLP), Neural Topic Models (NTMs) , Large Language Models (LLMs) and Diffusion model have emerged as areas of significant research interest. Despite this, NTMs primarily utilize contextual embeddings from LLMs, which are not optimal for clustering or capable for topic based text generation. NTMs have never been combined with diffusion model for text generation. Our study addresses these gaps by introducing a novel framework named Diffusion-Enhanced Topic Modeling using Encoder-Decoder-based LLMs (DeTiME). DeTiME leverages Encoder-Decoder-based LLMs to produce highly clusterable embeddings that could generate topics that exhibit both superior clusterability and enhanced semantic coherence compared to existing methods. Additionally, by exploiting the power of diffusion model, our framework also provides the capability to do topic based text generation. This dual functionality allows users to efficiently produce highly clustered topics and topic based text generation simultaneously. DeTiME's potential extends to generating clustered embeddings as well. Notably, our proposed framework(both encoder-decoder based LLM and diffusion model) proves to be efficient to train and exhibits high adaptability to other LLMs and diffusion model, demonstrating its potential for a wide array of applications.",
                    "[47017667 | Park et al. | 2018 | Citations: 484]": "\n Privacy is an important concern for our society where sharing data with partners or releasing data to the public is a frequent occurrence. Some of the techniques that are being used to achieve privacy are to remove identifiers, alter quasi-identifiers, and perturb values. Unfortunately, these approaches suffer from two limitations. First, it has been shown that private information can still be leaked if attackers possess some background knowledge or other information sources. Second, they do not take into account the adverse impact these methods will have on the utility of the released data. In this paper, we propose a method that meets both requirements. Our method, called\n table-GAN\n , uses generative adversarial networks (GANs) to synthesize fake tables that are statistically similar to the original table yet do not incur information leakage. We show that the machine learning models trained using our synthetic tables exhibit performance that is similar to that of models trained using the original table for unknown testing cases. We call this property\n model compatibility\n . We believe that anonymization/perturbation/synthesis methods without model compatibility are of little value. We used four real-world datasets from four different domains for our experiments and conducted indepth comparisons with state-of-the-art anonymization, perturbation, and generation techniques. Throughout our experiments, only our method consistently shows balance between privacy level and model compatibility.\n",
                    "[54479855 | Baowaly et al. | 2018 | Citations: 197]": "Objective\nThe aim of this study was to generate synthetic electronic health records (EHRs). The generated EHR data will be more realistic than those generated using the existing medical Generative Adversarial Network (medGAN) method.\n\n\nMaterials and Methods\nWe modified medGAN to obtain two synthetic data generation models-designated as medical Wasserstein GAN with gradient penalty (medWGAN) and medical boundary-seeking GAN (medBGAN)-and compared the results obtained using the three models. We used 2 databases: MIMIC-III and National Health Insurance Research Database (NHIRD), Taiwan. First, we trained the models and generated synthetic EHRs by using these three 3 models. We then analyzed and compared the models' performance by using a few statistical methods (Kolmogorov-Smirnov test, dimension-wise probability for binary data, and dimension-wise average count for count data) and 2 machine learning tasks (association rule mining and prediction).\n\n\nResults\nWe conducted a comprehensive analysis and found our models were adequately efficient for generating synthetic EHR data. The proposed models outperformed medGAN in all cases, and among the 3 models, boundary-seeking GAN (medBGAN) performed the best.\n\n\nDiscussion\nTo generate realistic synthetic EHR data, the proposed models will be effective in the medical industry and related research from the viewpoint of providing better services. Moreover, they will eliminate barriers including limited access to EHR data and thus accelerate research on medical informatics.\n\n\nConclusion\nThe proposed models can adequately learn the data distribution of real EHRs and efficiently generate realistic synthetic EHRs. The results show the superiority of our models over the existing model."
                },
                "metadata": [
                    {
                        "section_title": "Traditional and deep learning in tabular data",
                        "pdf_hash": "",
                        "start": 498,
                        "end": 1090,
                        "sentence_offsets": [
                            {
                                "start": 315,
                                "end": 949
                            },
                            {
                                "start": 950,
                                "end": 1090
                            }
                        ],
                        "ref_mentions": [
                            "229131432",
                            "259298730",
                            "47017667",
                            "54479855",
                            "264439324",
                            "249847841"
                        ],
                        "quote": "Recent advancement in generative models such as Variational Autoencoders (VAEs) (Ma et al., 2020;Darabi & Elor, 2021;(Kok et al., 2020)(Liu et al., 2023)Xu et al., 2023b)), generative adversarial networks (GANs) (Park et al., 2018)Choi et al., 2018;(Baowaly et al., 2018)Xu et al., 2019), diffusion models (Kotelnikov et al., 2022;(Xu et al., 2023)(Kim et al., 2022)a;Lee et al., 2023;Zhang et al., 2023c), and LLMs, opened up many new opportunities. These deep learning approaches have demonstrated superior performance over classical methods such as Bayesian networks ( (Xu et al., 2019))."
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[268230871 | Sudalairaj et al. | 2024 | Citations: 34]",
                "snippets": "Xu et al. (2023) introduces Evol-Instruct, another variant of Self-Instruct, that synthesizes iteratively more complex instruction to overcome shortcomings of previous methods. Mukherjee et al. (2023), Mitra et al. (2023) present a synthetic data generation approach to enhance task diversity and scalability, alongside a progressive training framework aimed at improving the model's reasoning ability and response style to match teacher models. This is achieved by generating rich reasoning signals in the generated answer and progressively training on datasets of varying difficulty in incremental phases.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "RELATED WORK",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 607,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 176
                            },
                            {
                                "start": 177,
                                "end": 445
                            },
                            {
                                "start": 446,
                                "end": 607
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Xu et al. (2023) introduces Evol-Instruct, another variant of Self-Instruct, that synthesizes iteratively more complex instruction to overcome shortcomings of previous methods. Mukherjee et al. (2023), Mitra et al. (2023) present a synthetic data generation approach to enhance task diversity and scalability, alongside a progressive training framework aimed at improving the model's reasoning ability and response style to match teacher models. This is achieved by generating rich reasoning signals in the generated answer and progressively training on datasets of varying difficulty in incremental phases."
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[268247488 | Huang et al. | 2024 | Citations: 43]",
                "snippets": "Large language models have shown great potential in complex reasoning tasks, yet their performance is often hampered by the scarcity of high-quality and reasoning-focused training datasets. Addressing this challenge, we propose Key-PointDriven Data Synthesis (KPDDS), a novel data synthesis framework that synthesizes question-answer pairs by leveraging key points and exemplar practices from authentic data sources. KPDDS ensures the generation of novel questions with rigorous quality control and substantial scalability. As a result, we present KPMath, an extensive synthetic dataset tailored for mathematical reasoning, comprising over 800K questionanswer pairs.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Large language models have shown great potential in complex reasoning tasks, yet their performance is often hampered by the scarcity of high-quality and reasoning-focused training datasets. Addressing this challenge, we propose Key-PointDriven Data Synthesis (KPDDS), a novel data synthesis framework that synthesizes question-answer pairs by leveraging key points and exemplar practices from authentic data sources. KPDDS ensures the generation of novel questions with rigorous quality control and substantial scalability. As a result, we present KPMath, an extensive synthetic dataset tailored for mathematical reasoning, comprising over 800K questionanswer pairs.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[268553593 | Peyrard et al. | 2024 | Citations: 0]",
                "snippets": "The synthetic data generation Flows can leverage domain knowledge (Tang et al., 2023), task properties (Lu et al., 2024;Veselovsky et al., 2023;(Josifoski et al., 2023), or collaboration (Abdullin et al., 2024), and synthesize data of notably higher quality than what a single model or simple heuristics can achieve. This sets the stage for effective self-improvement loops where a language model participates in a semantic decoding algorithm producing high-quality synthetic data. Then, the language model improves itself through fine-tuning, thereby improving the Flow's capacity to generate even better synthetic data in a virtuous cycle (Silver et al., 2017;Burns et al., 2023;Singh et al., 2023;Chen et al., 2024b). An example of such a Flow is MAGDi (Chen et al., 2024a), a framework designed to distill reasoning interactions among multiple LLMs into smaller ones. This approach surpasses single-teacher distillation (Li et al., 2023)(Magister et al., 2022) and finetuning based on reasoning trajectories sampled from GPT-4 (Chen et al., 2023a).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[257378179 | Josifoski et al. | 2023 | Citations: 85]": "Large language models (LLMs) have great potential for synthetic data generation. This work shows that useful data can be synthetically generated even for tasks that cannot be solved directly by LLMs: for problems with structured outputs, it is possible to prompt an LLM to perform the task in the reverse direction, by generating plausible input text for a target output structure. Leveraging this asymmetry in task difficulty makes it possible to produce large-scale, high-quality data for complex tasks. We demonstrate the effectiveness of this approach on closed information extraction, where collecting ground-truth data is challenging, and no satisfactory dataset exists to date. We synthetically generate a dataset of 1.8M data points, establish its superior quality compared to existing datasets in a human evaluation, and use it to finetune small models (220M and 770M parameters), termed SynthIE, that outperform the prior state of the art (with equal model size) by a substantial margin of 57 absolute points in micro-F1 and 79 points in macro-F1. Code, data, and models are available at https://github.com/epfl-dlab/SynthIE.",
                    "[254823156 | Magister et al. | 2022 | Citations: 269]": "Chain of thought prompting successfully improves the reasoning capabilities of large language models, achieving state of the art results on a range of datasets. However, these reasoning capabilities only appear to emerge in models with at least tens of billions of parameters. In this paper, we explore the transfer of such reasoning capabilities to smaller models via knowledge distillation, also investigating model and dataset size trade-off. Specifically, we finetune a student model on the chain of thought outputs generated by a larger teacher model. Our experiments show that the proposed method improves task performance across arithmetic, commonsense and symbolic reasoning datasets. For example, the accuracy of T5 XXL on GSM8K improves from 8.11% to 21.99% and 18.42% when finetuned on PaLM 540B and GPT-3 175B generated chains of thought, respectively.",
                    "[259251773 | Li et al. | 2023 | Citations: 143]": "Chain-of-thought prompting (e.g., \u201cLet\u2019s think step-by-ste\u201d) primes large language models to verbalize rationalization for their predictions. While chain-of-thought can lead to dramatic performance gains, benefits appear to emerge only for sufficiently large models (beyond 50B parameters). We show that orders-of-magnitude smaller models (125M\u20141.3B parameters) can still benefit from chain-of-thought prompting. To achieve this, we introduce Symbolic Chain-of-Thought Distillation (SCoTD), a method to train a smaller student model on rationalizations sampled from a significantly larger teacher model. Experiments across several commonsense benchmarks show that: 1) SCoTD enhances the performance of the student model in both supervised and few-shot settings, and especially for challenge sets; 2) sampling many reasoning chains per instance from the teacher is paramount; and 3) after distillation, student chain-of-thoughts are judged by humans as comparable to the teacher, despite orders of magnitude fewer parameters. We test several hypotheses regarding what properties of chain-of-thought samples are important, e.g., diversity vs. teacher likelihood vs. open-endedness. We release our corpus of chain-of-thought samples and code."
                },
                "metadata": [
                    {
                        "section_title": "Synthetic Data Flow",
                        "pdf_hash": "",
                        "start": 386,
                        "end": 1438,
                        "sentence_offsets": [
                            {
                                "start": 386,
                                "end": 702
                            },
                            {
                                "start": 703,
                                "end": 867
                            },
                            {
                                "start": 868,
                                "end": 1106
                            },
                            {
                                "start": 1107,
                                "end": 1257
                            },
                            {
                                "start": 1258,
                                "end": 1438
                            }
                        ],
                        "ref_mentions": [
                            "257378179",
                            "259251773",
                            "254823156"
                        ],
                        "quote": "The synthetic data generation Flows can leverage domain knowledge (Tang et al., 2023), task properties (Lu et al., 2024;Veselovsky et al., 2023;(Josifoski et al., 2023), or collaboration (Abdullin et al., 2024), and synthesize data of notably higher quality than what a single model or simple heuristics can achieve. This sets the stage for effective self-improvement loops where a language model participates in a semantic decoding algorithm producing high-quality synthetic data. Then, the language model improves itself through fine-tuning, thereby improving the Flow's capacity to generate even better synthetic data in a virtuous cycle (Silver et al., 2017;Burns et al., 2023;Singh et al., 2023;Chen et al., 2024b). An example of such a Flow is MAGDi (Chen et al., 2024a), a framework designed to distill reasoning interactions among multiple LLMs into smaller ones. This approach surpasses single-teacher distillation (Li et al., 2023)(Magister et al., 2022) and finetuning based on reasoning trajectories sampled from GPT-4 (Chen et al., 2023a)."
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[269294098 | Ajmal et al. | 2024 | Citations: 0]",
                "snippets": "Integral to Intellecta is the incorporation of advanced synthetic generation techniques, which fabricate a dual-composed content: one that simulates complex thought processes and another that yields textbookstyle elucidations laden with core concepts and pragmatic examples.We took available open source instruction data as the seed data to generate the synthetic data.From the instructions, we first create a textbook style Gunasekar et al. [2023] text explaining the concept required for answering the instruction.The second step is to enrich the response from the seed data with thought process how the model arrive at that result.Combining these 2 will provide a textbook style concept explanation followed by exercises and the thought process to resolve the same.This correlate to the same a student learn a chapter starting with textbook followed by exercises and thought process behind it.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1136,
                        "end": 2032,
                        "sentence_offsets": [
                            {
                                "start": 1136,
                                "end": 1410
                            },
                            {
                                "start": 1410,
                                "end": 1505
                            },
                            {
                                "start": 1505,
                                "end": 1652
                            },
                            {
                                "start": 1652,
                                "end": 1770
                            },
                            {
                                "start": 1770,
                                "end": 1904
                            },
                            {
                                "start": 1904,
                                "end": 2032
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Integral to Intellecta is the incorporation of advanced synthetic generation techniques, which fabricate a dual-composed content: one that simulates complex thought processes and another that yields textbookstyle elucidations laden with core concepts and pragmatic examples.We took available open source instruction data as the seed data to generate the synthetic data.From the instructions, we first create a textbook style Gunasekar et al. [2023] text explaining the concept required for answering the instruction.The second step is to enrich the response from the seed data with thought process how the model arrive at that result.Combining these 2 will provide a textbook style concept explanation followed by exercises and the thought process to resolve the same.This correlate to the same a student learn a chapter starting with textbook followed by exercises and thought process behind it."
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[269702974 | Yadav et al. | 2024 | Citations: 1]",
                "snippets": "Recent advances in generative models have led to more efficient modeling of complex high-dimensional datasets. Also, privacy concerns have led to the development of robust models with lesser risk of privacy breaches.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 266,
                        "end": 482,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Recent advances in generative models have led to more efficient modeling of complex high-dimensional datasets. Also, privacy concerns have led to the development of robust models with lesser risk of privacy breaches."
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[270870071 | He et al. | 2024 | Citations: 7]",
                "snippets": "To further enhance the accuracy of these intermediate steps, recent studies leverage extensive synthetic datasets, which are either distilled from cutting-edge models (Yu et al., 2023b;Luo et al., 2023) or composed of selfgenerated rationales (Zelikman et al., 2022;Yuan et al., 2023;Ni et al., 2022), to fine-tune the LLMs.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "A Related Work",
                        "pdf_hash": "",
                        "start": 558,
                        "end": 882,
                        "sentence_offsets": [
                            {
                                "start": 558,
                                "end": 882
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "To further enhance the accuracy of these intermediate steps, recent studies leverage extensive synthetic datasets, which are either distilled from cutting-edge models (Yu et al., 2023b;Luo et al., 2023) or composed of selfgenerated rationales (Zelikman et al., 2022;Yuan et al., 2023;Ni et al., 2022), to fine-tune the LLMs."
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[271088512 | Vashishtha et al. | 2024 | Citations: 4]",
                "snippets": "Synthetic data generation has been explored for optimising model training for reasoning. For example, (Li et al., 2023;Gunasekar et al., 2023) use LLM-generated synthetic text for training Phi-1 and Phi-1.5 models and show impressive performance for reasoning-based tasks. (Trinh et al., 2024) introduce a novel neuro-symbolic framework to pre-train a transformer model for Olympiad-level math problems. (Morishita et al., 2024) construct synthetic training data to improve language models' performance on logical reasoning tasks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[267032902 | Trinh et al. | 2024 | Citations: 390]": "Proving mathematical theorems at the olympiad level represents a notable milestone in human-level automated reasoning1\u20134, owing to their reputed difficulty among the world\u2019s best talents in pre-university mathematics. Current machine-learning approaches, however, are not applicable to most mathematical domains owing to the high cost of translating human proofs into machine-verifiable format. The problem is even worse for geometry because of its unique translation challenges1,5, resulting in severe scarcity of training data. We propose AlphaGeometry, a theorem prover for Euclidean plane geometry that sidesteps the need for human demonstrations by synthesizing millions of theorems and proofs across different levels of complexity. AlphaGeometry is a neuro-symbolic system that uses a neural language model, trained from scratch on our large-scale synthetic data, to guide a symbolic deduction engine through infinite branching points in challenging problems. On a test set of 30 latest olympiad-level problems, AlphaGeometry solves 25, outperforming the previous best method that only solves ten problems and approaching the performance of an average International Mathematical Olympiad (IMO) gold medallist. Notably, AlphaGeometry produces human-readable proofs, solves all geometry problems in the IMO 2000 and 2015 under human expert evaluation and discovers a generalized version of a translated IMO theorem in 2004.",
                    "[274141027 | Morishita et al. | 2024 | Citations: 15]": "Large language models (LLMs) are capable of solving a wide range of tasks, yet they have struggled with reasoning. To address this, we propose $\\textbf{Additional Logic Training (ALT)}$, which aims to enhance LLMs' reasoning capabilities by program-generated logical reasoning samples. We first establish principles for designing high-quality samples by integrating symbolic logic theory and previous empirical insights. Then, based on these principles, we construct a synthetic corpus named $\\textbf{Formal Logic Deduction Diverse}$ ($\\textbf{FLD}$$_{\\times 2}$), comprising numerous samples of multi-step deduction with unknown facts, diverse reasoning rules, diverse linguistic expressions, and challenging distractors. Finally, we empirically show that ALT on FLD$_{\\times2}$ substantially enhances the reasoning capabilities of state-of-the-art LLMs, including LLaMA-3.1-70B. Improvements include gains of up to 30 points on logical reasoning benchmarks, up to 10 points on math and coding benchmarks, and 5 points on the benchmark suite BBH."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 378,
                        "end": 908,
                        "sentence_offsets": [
                            {
                                "start": 315,
                                "end": 466
                            },
                            {
                                "start": 467,
                                "end": 650
                            },
                            {
                                "start": 651,
                                "end": 781
                            },
                            {
                                "start": 782,
                                "end": 908
                            }
                        ],
                        "ref_mentions": [
                            "267032902",
                            "274141027"
                        ],
                        "quote": "Synthetic data generation has been explored for optimising model training for reasoning. For example, (Li et al., 2023;Gunasekar et al., 2023) use LLM-generated synthetic text for training Phi-1 and Phi-1.5 models and show impressive performance for reasoning-based tasks. (Trinh et al., 2024) introduce a novel neuro-symbolic framework to pre-train a transformer model for Olympiad-level math problems. (Morishita et al., 2024) construct synthetic training data to improve language models' performance on logical reasoning tasks."
                    }
                ]
            },
            {
                "idx": 17,
                "key": "[272146630 | Bansal et al. | 2024 | Citations: 49]",
                "snippets": "Several recent works aim to understand the utility of the weak but cheaper LMs in comparison to the strong but expensive LMs for reasoning. Specifically, Brown et al. (2024); Snell et al. (2024); Song et al. (2024) show that the solve rate of the small LMs can increase significantly with repeated sampling. In addition, Hassid et al. (2024) demonstrate that repeated generations from smaller LMs can outperform the data generated by larger LMs at a fixed sampling computational budget during inference for coding tasks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 13,
                        "end": 533,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 152
                            },
                            {
                                "start": 153,
                                "end": 320
                            },
                            {
                                "start": 321,
                                "end": 533
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Several recent works aim to understand the utility of the weak but cheaper LMs in comparison to the strong but expensive LMs for reasoning. Specifically, Brown et al. (2024); Snell et al. (2024); Song et al. (2024) show that the solve rate of the small LMs can increase significantly with repeated sampling. In addition, Hassid et al. (2024) demonstrate that repeated generations from smaller LMs can outperform the data generated by larger LMs at a fixed sampling computational budget during inference for coding tasks."
                    }
                ]
            },
            {
                "idx": 18,
                "key": "[272593422 | Yang et al. | 2024 | Citations: 16]",
                "snippets": "Recent approaches synthesize pretraining data using hierarchical prompting methods to promote dataset diversity. Eldan & Li (2023) prompt API-based LLMs to generate children's stories containing sampled keywords, and demonstrate that even small language models trained on their dataset can generate fluent text. Gunasekar et al. (2023) synthesize a diverse dataset of textbooks and code exercises by conditioning on topic, target audience, and function names, and later release strong LLMs pretrained on synthetic data in follow-up work (Li et al., 2023b;Abdin et al., 2023;2024). However, their datasets and prompts are not publicly available. (Maini et al., 2024) prompt an LM to rephrase documents for pretraining, improving training efficiency. Different from all above works, our focus is teaching a pretrained LLM the knowledge of a small corpus. Mecklenburg et al. (2024) consider task-specific finetuning and propose a fact-based synthetic QA generation procedure, but do not show improvement on generic instruction following tasks beyond simple QA. We instead focus on teaching a model generally useful knowledge about a small corpus, untied to a particular downstream task. Ovadia et al. (2024) continually pretrain Llama 2-based language models on synthetic paraphrases of Wikipedia articles, but do not observe consistent performance improvements.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[267312030 | Maini et al. | 2024 | Citations: 66]": "Large language models are trained on massive scrapes of the web, which are often unstructured, noisy, and poorly phrased. Current scaling laws show that learning from such data requires an abundance of both compute and data, which grows with the size of the model being trained. This is infeasible both because of the large compute costs and duration associated with pre-training, and the impending scarcity of high-quality data on the web. In this work, we propose Web Rephrase Augmented Pre-training ($\\textbf{WRAP}$) that uses an off-the-shelf instruction-tuned model prompted to paraphrase documents on the web in specific styles such as\"like Wikipedia\"or in\"question-answer format\"to jointly pre-train LLMs on real and synthetic rephrases. First, we show that using WRAP on the C4 dataset, which is naturally noisy, speeds up pre-training by $\\sim3x$. At the same pre-training compute budget, it improves perplexity by more than 10% on average across different subsets of the Pile, and improves zero-shot question answer accuracy across 13 tasks by more than 2%. Second, we investigate the impact of the re-phrasing style on the performance of the model, offering insights into how the composition of the training data can impact the performance of LLMs in OOD settings. Our gains are attributed to the fact that re-phrased synthetic data has higher utility than just real data because it (i) incorporates style diversity that closely reflects downstream evaluation style, and (ii) has higher 'quality' than web-scraped data."
                },
                "metadata": [
                    {
                        "section_title": "RELATED WORK",
                        "pdf_hash": "",
                        "start": 272,
                        "end": 1630,
                        "sentence_offsets": [
                            {
                                "start": 272,
                                "end": 384
                            },
                            {
                                "start": 385,
                                "end": 583
                            },
                            {
                                "start": 584,
                                "end": 852
                            },
                            {
                                "start": 853,
                                "end": 916
                            },
                            {
                                "start": 917,
                                "end": 1019
                            },
                            {
                                "start": 1020,
                                "end": 1123
                            },
                            {
                                "start": 1124,
                                "end": 1328
                            },
                            {
                                "start": 1329,
                                "end": 1454
                            },
                            {
                                "start": 1455,
                                "end": 1630
                            }
                        ],
                        "ref_mentions": [
                            "267312030"
                        ],
                        "quote": "Recent approaches synthesize pretraining data using hierarchical prompting methods to promote dataset diversity. Eldan & Li (2023) prompt API-based LLMs to generate children's stories containing sampled keywords, and demonstrate that even small language models trained on their dataset can generate fluent text. Gunasekar et al. (2023) synthesize a diverse dataset of textbooks and code exercises by conditioning on topic, target audience, and function names, and later release strong LLMs pretrained on synthetic data in follow-up work (Li et al., 2023b;Abdin et al., 2023;2024). However, their datasets and prompts are not publicly available. (Maini et al., 2024) prompt an LM to rephrase documents for pretraining, improving training efficiency. Different from all above works, our focus is teaching a pretrained LLM the knowledge of a small corpus. Mecklenburg et al. (2024) consider task-specific finetuning and propose a fact-based synthetic QA generation procedure, but do not show improvement on generic instruction following tasks beyond simple QA. We instead focus on teaching a model generally useful knowledge about a small corpus, untied to a particular downstream task. Ovadia et al. (2024) continually pretrain Llama 2-based language models on synthetic paraphrases of Wikipedia articles, but do not observe consistent performance improvements."
                    }
                ]
            },
            {
                "idx": 19,
                "key": "[272753147 | Zhou et al. | 2024 | Citations: 4]",
                "snippets": "Despite recent advances in training and prompting strategies for Large Language Models (LLMs), these models continue to face challenges with complex logical reasoning tasks that involve long reasoning chains. In this work, we explore the potential and limitations of using graph-based synthetic reasoning data as training signals to enhance LLMs' reasoning capabilities.\n\nRecently, several works (Xu et al., 2024;Abdin et al., 2024;Anil et al., 2023) have demonstrated the efficacy of boosting the LLMs' reasoning capacity via fine-tuning on synthetic data generated by stronger LLMs. However, how to make such synthetic data generation effective and controllable for specific applications remains an open question.\n\nMotivated by the fact that natural language reasoning tasks can be represented as structured data with finite nodes and edges (Jin et al., 2024), and inspired by existing works on constructing reasoning benchmarks (Fatemi et al., 2024;Kazemi et al., 2023;Agrawal et al., 2024), we propose to leverage synthetic graph-based data for task-specific posttraining adaptation to improve the correctness of the generated reasoning questions and labels.\n\nIn this paper, we carefully design a random walk sampling algorithm on graphs and introduce a new prompting strategy that first extracts a reasoning chain and then derives the answer.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Despite recent advances in training and prompting strategies for Large Language Models (LLMs), these models continue to face challenges with complex logical reasoning tasks that involve long reasoning chains. In this work, we explore the potential and limitations of using graph-based synthetic reasoning data as training signals to enhance LLMs' reasoning capabilities.\n\nRecently, several works (Xu et al., 2024;Abdin et al., 2024;Anil et al., 2023) have demonstrated the efficacy of boosting the LLMs' reasoning capacity via fine-tuning on synthetic data generated by stronger LLMs. However, how to make such synthetic data generation effective and controllable for specific applications remains an open question.\n\nMotivated by the fact that natural language reasoning tasks can be represented as structured data with finite nodes and edges (Jin et al., 2024), and inspired by existing works on constructing reasoning benchmarks (Fatemi et al., 2024;Kazemi et al., 2023;Agrawal et al., 2024), we propose to leverage synthetic graph-based data for task-specific posttraining adaptation to improve the correctness of the generated reasoning questions and labels.\n\nIn this paper, we carefully design a random walk sampling algorithm on graphs and introduce a new prompting strategy that first extracts a reasoning chain and then derives the answer.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 20,
                "key": "[272753174 | Jiang et al. | 2024 | Citations: 7]",
                "snippets": "Synthetic data has played an important role in enhancing LLMs' reasoning capabilities (Dubey et al., 2024), especially in mathematics and coding domains. \n\nFor mathematical reasoning, synthetic data generation includes problem-driven methods such as evol-instruct (Luo et al., 2023a;Zeng et al., 2024), problem restatement (Yu et al., 2023), backtranslation (Lu et al., 2024), and few-shot examples (Dalvi et al., 2021), as well as knowledge-driven methods that rely on knowledge bases (Dalvi et al., 2021) or concept graphs (Tang et al., 2024) to generate new problems by sampling key reasoning points (Huang et al., 2024). \n\nIn terms of code reasoning, from Code Alpaca's (Wang et al., 2023) use of self-instruct to generate 20K instruction data based on seed tasks, to Wiz-ardCoder's (Luo et al., 2023b) use of Code evolinstruct to generate more complex tasks, to Magicoder's use of oss-instruct (Wei et al., 2024) to extract 75K instructions from open source code. Synthesizing data continuously improves the model's code reasoning capabilities.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[233297051 | Dalvi et al. | 2021 | Citations: 185]": "Our goal, in the context of open-domain textual question-answering (QA), is to explain answers by showing the line of reasoning from what is known to the answer, rather than simply showing a fragment of textual evidence (a \u201crationale\u201d). If this could be done, new opportunities for understanding and debugging the system\u2019s reasoning become possible. Our approach is to generate explanations in the form of entailment trees, namely a tree of multipremise entailment steps from facts that are known, through intermediate conclusions, to the hypothesis of interest (namely the question + answer). To train a model with this skill, we created ENTAILMENTBANK, the first dataset to contain multistep entailment trees. Given a hypothesis (question + answer), we define three increasingly difficult explanation tasks: generate a valid entailment tree given (a) all relevant sentences (b) all relevant and some irrelevant sentences, or (c) a corpus. We show that a strong language model can partially solve these tasks, in particular when the relevant sentences are included in the input (e.g., 35% of trees for (a) are perfect), and with indications of generalization to other domains. This work is significant as it provides a new type of dataset (multistep entailments) and baselines, offering a new avenue for the community to generate richer, more systematic explanations."
                },
                "metadata": [
                    {
                        "section_title": "A Related work A.1 Synthetic Reasoning Data",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1049,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 153
                            },
                            {
                                "start": 156,
                                "end": 624
                            },
                            {
                                "start": 627,
                                "end": 968
                            },
                            {
                                "start": 969,
                                "end": 1049
                            }
                        ],
                        "ref_mentions": [
                            "233297051",
                            "233297051"
                        ],
                        "quote": "Synthetic data has played an important role in enhancing LLMs' reasoning capabilities (Dubey et al., 2024), especially in mathematics and coding domains. \n\nFor mathematical reasoning, synthetic data generation includes problem-driven methods such as evol-instruct (Luo et al., 2023a;Zeng et al., 2024), problem restatement (Yu et al., 2023), backtranslation (Lu et al., 2024), and few-shot examples (Dalvi et al., 2021), as well as knowledge-driven methods that rely on knowledge bases (Dalvi et al., 2021) or concept graphs (Tang et al., 2024) to generate new problems by sampling key reasoning points (Huang et al., 2024). \n\nIn terms of code reasoning, from Code Alpaca's (Wang et al., 2023) use of self-instruct to generate 20K instruction data based on seed tasks, to Wiz-ardCoder's (Luo et al., 2023b) use of Code evolinstruct to generate more complex tasks, to Magicoder's use of oss-instruct (Wei et al., 2024) to extract 75K instructions from open source code. Synthesizing data continuously improves the model's code reasoning capabilities."
                    }
                ]
            },
            {
                "idx": 21,
                "key": "[272770433 | Liu et al. | 2024 | Citations: 17]",
                "snippets": "Synthetic Data Generation for Model Fine-tuning. The performance of large language models (LLMs) hinge on the quality and diversity of their training data. To address the limitations of manual datasets, synthetic data generation methods (Wang et al., 2022;Xu et al., 2023) have been developed to automatically create instruction-following examples from LLMs, reducing reliance on human annotations. Various techniques enhance data quality: Wang et al. (2022) generates multiple reasoning traces and selects the most frequent output to improve robustness, while other approaches (Lightman et al., 2023;Zhang et al., 2024b) assess response quality based on these traces. Self-training methods utilize synthetic data for iterative fine-tuning, boosting reasoning capabilities (Singh et al., 2023;Feng et al., 2023). These advancements show how synthetic data can effectively scale and optimize models through iterative feedback.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Synthetic Data Generation for Model Fine-tuning. The performance of large language models (LLMs) hinge on the quality and diversity of their training data. To address the limitations of manual datasets, synthetic data generation methods (Wang et al., 2022;Xu et al., 2023) have been developed to automatically create instruction-following examples from LLMs, reducing reliance on human annotations. Various techniques enhance data quality: Wang et al. (2022) generates multiple reasoning traces and selects the most frequent output to improve robustness, while other approaches (Lightman et al., 2023;Zhang et al., 2024b) assess response quality based on these traces. Self-training methods utilize synthetic data for iterative fine-tuning, boosting reasoning capabilities (Singh et al., 2023;Feng et al., 2023). These advancements show how synthetic data can effectively scale and optimize models through iterative feedback.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 22,
                "key": "[272988100 | Chan et al. | 2024 | Citations: 10]",
                "snippets": "In the domain of mathematical reasoning, highquality instructions are scarce, so many works leverage LLM-generated synthetic data to significantly improved the math reasoning ability of small LLMs (Yu et al., 2023;(Qin et al., 2024)Setlur et al., 2024;(Luo et al., 2023). In code generation, synthetic data from LLMs can be further verified by running test cases or the code directly, which helps close the gap between closed-source LLMs and smaller LLMs (Wei et al., 2023)Yang et al., 2024).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[261030818 | Luo et al. | 2023 | Citations: 467]": "Large language models (LLMs), such as GPT-4, have shown remarkable performance in natural language processing (NLP) tasks, including challenging mathematical reasoning. However, most existing open-source models are only pre-trained on large-scale internet data and without math-related optimization. In this paper, we present WizardMath, which enhances the mathematical CoT reasoning abilities of LLMs without using external python tools, by applying our proposed Reinforcement Learning from Evol-Instruct Feedback (RLEIF) method to the domain of math. Through extensive experiments on two mathematical reasoning benchmarks, namely GSM8k and MATH, we reveal the extraordinary capabilities of our model. Remarkably, WizardMath-Mistral 7B surpasses top-tier open-source LLMs by a substantial margin with higher data efficiency. Furthermore, WizardMath 70B even outperforms GPT-3.5-Turbo, Claude 2, Gemini Pro and GPT-4-early-version. Additionally, our preliminary exploration highlights the pivotal role of instruction evolution and process supervision in achieving exceptional math performance. For more details refer to https://github.com/nlpxucan/WizardLM",
                    "[269930153 | Qin et al. | 2024 | Citations: 59]": "While large language models (LLMs) like ChatGPT have shown impressive capabilities in Natural Language Processing (NLP) tasks, a systematic investigation of their potential in this field remains largely unexplored. This study aims to address this gap by exploring the following questions: (1) How are LLMs currently applied to NLP tasks in the literature? (2) Have traditional NLP tasks already been solved with LLMs? (3) What is the future of the LLMs for NLP? To answer these questions, we take the first step to provide a comprehensive overview of LLMs in NLP. Specifically, we first introduce a unified taxonomy including (1) parameter-frozen application and (2) parameter-tuning application to offer a unified perspective for understanding the current progress of LLMs in NLP. Furthermore, we summarize the new frontiers and the associated challenges, aiming to inspire further groundbreaking advancements. We hope this work offers valuable insights into the {potential and limitations} of LLMs in NLP, while also serving as a practical guide for building effective LLMs in NLP.",
                    "[270358041 | Wei et al. | 2023 | Citations: 118]": "We introduce Magicoder, a series of fully open-source (code, weights, and data) Large Language Models (LLMs) for code that significantly closes the gap with top code models while having no more than 7B parameters. Magicoder models are trained on 75K synthetic instruction data using OSS-Instruct, a novel approach to enlightening LLMs with open-source code snippets to generate diverse instruction data for code. Our main motivation is to mitigate the inherent bias of the synthetic data generated by LLMs through the wealth of open-source references for the production of more realistic and controllable data. The orthogonality of OSS-Instruct and other data generation methods like Evol-Instruct further enables us to build an enhanced MagicoderS. Both Magicoder and MagicoderS substantially outperform state-of-the-art code models with similar or even larger sizes on a wide range of coding benchmarks. Notably, MagicoderS-CL-7B based on CodeLlama even surpasses the prominent ChatGPT on HumanEval+ (66.5 vs. 65.9 in pass@1 ). Overall, OSS-Instruct opens a new direction for crafting diverse synthetic instruction data for code using abundant open-source references."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 144,
                        "end": 633,
                        "sentence_offsets": [
                            {
                                "start": 144,
                                "end": 412
                            },
                            {
                                "start": 413,
                                "end": 633
                            }
                        ],
                        "ref_mentions": [
                            "269930153",
                            "261030818",
                            "270358041"
                        ],
                        "quote": "In the domain of mathematical reasoning, highquality instructions are scarce, so many works leverage LLM-generated synthetic data to significantly improved the math reasoning ability of small LLMs (Yu et al., 2023;(Qin et al., 2024)Setlur et al., 2024;(Luo et al., 2023). In code generation, synthetic data from LLMs can be further verified by running test cases or the code directly, which helps close the gap between closed-source LLMs and smaller LLMs (Wei et al., 2023)Yang et al., 2024)."
                    }
                ]
            },
            {
                "idx": 23,
                "key": "[273185896 | Raghavan et al. | 2024 | Citations: 0]",
                "snippets": "Recent works have shown significant progress in synthetic data generation in the image space in (Liang et al., 2023) (Ramesh et al., 2021)) (Sun et al. 2024) (Bae et al., 2022) etc. However, similar progress has not been seen in the audio space. Zeroshot text-to-image generation approaches have expanded the scope of synthetic data applications by enabling the generation of novel image data from unseen textual prompts, highlighting the model's ability to generalize from limited examples (Ramesh et al., 2021). Digiface-1m (Bae et al., 2022)) dataset exemplifies the practical applications of these technologies, providing a robust framework for testing and improving face recognition algorithms through access to one million digital face images. (Ye et al. 2022) outlines a method to leverage LLMs to create synthetic datasets produced entirely using pre-trained language models (PLMs) without human interference while emphasizing the efficiency and flexibility of using synthetic datasets to train task-specific models. (Yu et al., 2023)) explores generation of training data that not only focuses on diversity, but also addresses inherent biases within the data generated by LLMs. It highlights the critical role of using diversely attributed prompts that enhance quality and utility of synthetic datasets improving model performance across NLP tasks. (Patel, Raffel, and Callison-Burch 2024) presents a tool designed to streamline synthetic data generation using LLMs providing a platform to generate, train, and share data sets and models.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[232035663 | Ramesh et al. | 2021 | Citations: 5000]": "Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.",
                    "[252735071 | Bae et al. | 2022 | Citations: 115]": "State-of-the-art face recognition models show impressive accuracy, achieving over 99.8% on Labeled Faces in the Wild (LFW) dataset. Such models are trained on large-scale datasets that contain millions of real human face images collected from the internet. Web-crawled face images are severely biased (in terms of race, lighting, makeup, etc) and often contain label noise. More importantly, the face images are collected without explicit consent, raising ethical concerns. To avoid such problems, we introduce a large-scale synthetic dataset for face recognition, obtained by rendering digital faces using a computer graphics pipeline1. We first demonstrate that aggressive data augmentation can significantly reduce the synthetic-to-real domain gap. Having full control over the rendering pipeline, we also study how each attribute (e.g., variation in facial pose, accessories and textures) affects the accuracy. Compared to Syn-Face, a recent method trained on GAN-generated synthetic faces, we reduce the error rate on LFW by 52.5% (accuracy from 91.93% to 96.17%). By fine-tuning the network on a smaller number of real face images that could reason-ably be obtained with consent, we achieve accuracy that is comparable to the methods trained on millions of real face images.",
                    "[259275123 | Yu et al. | 2023 | Citations: 234]": "Large language models (LLMs) have been recently leveraged as training data generators for various natural language processing (NLP) tasks. While previous research has explored different approaches to training models using generated data, they generally rely on simple class-conditional prompts, which may limit the diversity of the generated data and inherit systematic biases of LLM. Thus, we investigate training data generation with diversely attributed prompts (e.g., specifying attributes like length and style), which have the potential to yield diverse and attributed generated data. Our investigation focuses on datasets with high cardinality and diverse domains, wherein we demonstrate that attributed prompts outperform simple class-conditional prompts in terms of the resulting model's performance. Additionally, we present a comprehensive empirical study on data generation encompassing vital aspects like bias, diversity, and efficiency, and highlight three key observations: firstly, synthetic datasets generated by simple prompts exhibit significant biases, such as regional bias; secondly, attribute diversity plays a pivotal role in enhancing model performance; lastly, attributed prompts achieve the performance of simple class-conditional prompts while utilizing only 5\\% of the querying cost of ChatGPT associated with the latter. The data and code are available on \\url{https://github.com/yueyu1030/AttrPrompt}.",
                    "[266348349 | Liang et al. | 2023 | Citations: 86]": "Recent Text-to-Image (T2I) generation models such as Stable Diffusion and Imagen have made significant progress in generating high-resolution images based on text descriptions. However, many generated images still suffer from issues such as artifacts/implausibility, misalignment with text descriptions, and low aesthetic quality. Inspired by the success of Reinforcement Learning with Human Feedback (RLHF) for large language models, prior works collected human-provided scores as feedback on generated images and trained a reward model to improve the T2I generation. In this paper, we enrich the feedback signal by (i) marking image regions that are implausible or misaligned with the text, and (ii) annotating which words in the text prompt are misrepresented or missing on the image. We collect such rich human feedback on 18K generated images (RichHF-18K) and train a multimodal transformer to predict the rich feedback automatically. We show that the predicted rich human feedback can be leveraged to improve image generation, for example, by selecting high-quality training data to finetune and improve the generative models, or by creating masks with predicted heatmaps to inpaint the problematic regions. Notably, the improvements generalize to models (Muse) beyond those used to generate the images on which human feedback data were collected (Stable Diffusion variants). The RichHF-18K data set will be released in our GitHub repository: https://github.com/google-research/google-research/tree/master/richhf_18k."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 104,
                        "end": 1642,
                        "sentence_offsets": [
                            {
                                "start": 68,
                                "end": 281
                            },
                            {
                                "start": 282,
                                "end": 345
                            },
                            {
                                "start": 346,
                                "end": 612
                            },
                            {
                                "start": 613,
                                "end": 846
                            },
                            {
                                "start": 847,
                                "end": 1121
                            },
                            {
                                "start": 1122,
                                "end": 1281
                            },
                            {
                                "start": 1282,
                                "end": 1452
                            },
                            {
                                "start": 1453,
                                "end": 1642
                            }
                        ],
                        "ref_mentions": [
                            "266348349",
                            "232035663",
                            "252735071",
                            "232035663",
                            "252735071",
                            "259275123"
                        ],
                        "quote": "Recent works have shown significant progress in synthetic data generation in the image space in (Liang et al., 2023) (Ramesh et al., 2021)) (Sun et al. 2024) (Bae et al., 2022) etc. However, similar progress has not been seen in the audio space. Zeroshot text-to-image generation approaches have expanded the scope of synthetic data applications by enabling the generation of novel image data from unseen textual prompts, highlighting the model's ability to generalize from limited examples (Ramesh et al., 2021). Digiface-1m (Bae et al., 2022)) dataset exemplifies the practical applications of these technologies, providing a robust framework for testing and improving face recognition algorithms through access to one million digital face images. (Ye et al. 2022) outlines a method to leverage LLMs to create synthetic datasets produced entirely using pre-trained language models (PLMs) without human interference while emphasizing the efficiency and flexibility of using synthetic datasets to train task-specific models. (Yu et al., 2023)) explores generation of training data that not only focuses on diversity, but also addresses inherent biases within the data generated by LLMs. It highlights the critical role of using diversely attributed prompts that enhance quality and utility of synthetic datasets improving model performance across NLP tasks. (Patel, Raffel, and Callison-Burch 2024) presents a tool designed to streamline synthetic data generation using LLMs providing a platform to generate, train, and share data sets and models."
                    }
                ]
            },
            {
                "idx": 24,
                "key": "[273374871 | Cao et al. | 2024 | Citations: 1]",
                "snippets": "In this work, we aim to systematically control the synthetic pipeline for generating image-text data while accommodating different use cases (e.g., improving long-tail task performance, enhancing compositional reasoning of CLIP models, etc.).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "INTRODUCTION",
                        "pdf_hash": "",
                        "start": 1663,
                        "end": 1905,
                        "sentence_offsets": [
                            {
                                "start": 1663,
                                "end": 1905
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In this work, we aim to systematically control the synthetic pipeline for generating image-text data while accommodating different use cases (e.g., improving long-tail task performance, enhancing compositional reasoning of CLIP models, etc.)."
                    }
                ]
            },
            {
                "idx": 25,
                "key": "[273403575 | Akter et al. | 2024 | Citations: 2]",
                "snippets": "To generate high-quality data at scale, current synthetic data generation approach explores rephrasing texts using LLMs in varied syntax while preserving the core content (Maini et al., 2024). However, their proposed approach limits up-sampling high-quality data in a way that does not go beyond grammatical styles or surface form transformations-leading little to no improvement when it comes to performance across complex and logical reasoning tasks. We hypothesize that simple rephrasing does not leverage the full potential of the synthetic data to improve the mathematical and complex multi-hop reasoning ability of LLM. Therefore, we propose, MIND, a conversational synthetic data generation approach that adds semantic variations and structured complexity to the raw text which is required to improve complex reasoning ability of the LLMs. In addition, multi-turn conversations can break down the original context step-by-step while each step addresses a sub-context at a time by often injecting complimentary reasoning or explanations. This resonates with how human solves a complex problem using consecutive chain-of-thought reasoning.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[267312030 | Maini et al. | 2024 | Citations: 66]": "Large language models are trained on massive scrapes of the web, which are often unstructured, noisy, and poorly phrased. Current scaling laws show that learning from such data requires an abundance of both compute and data, which grows with the size of the model being trained. This is infeasible both because of the large compute costs and duration associated with pre-training, and the impending scarcity of high-quality data on the web. In this work, we propose Web Rephrase Augmented Pre-training ($\\textbf{WRAP}$) that uses an off-the-shelf instruction-tuned model prompted to paraphrase documents on the web in specific styles such as\"like Wikipedia\"or in\"question-answer format\"to jointly pre-train LLMs on real and synthetic rephrases. First, we show that using WRAP on the C4 dataset, which is naturally noisy, speeds up pre-training by $\\sim3x$. At the same pre-training compute budget, it improves perplexity by more than 10% on average across different subsets of the Pile, and improves zero-shot question answer accuracy across 13 tasks by more than 2%. Second, we investigate the impact of the re-phrasing style on the performance of the model, offering insights into how the composition of the training data can impact the performance of LLMs in OOD settings. Our gains are attributed to the fact that re-phrased synthetic data has higher utility than just real data because it (i) incorporates style diversity that closely reflects downstream evaluation style, and (ii) has higher 'quality' than web-scraped data."
                },
                "metadata": [
                    {
                        "section_title": "INTRODUCTION",
                        "pdf_hash": "",
                        "start": 967,
                        "end": 2111,
                        "sentence_offsets": [
                            {
                                "start": 967,
                                "end": 1159
                            },
                            {
                                "start": 1160,
                                "end": 1419
                            },
                            {
                                "start": 1420,
                                "end": 1592
                            },
                            {
                                "start": 1593,
                                "end": 1813
                            },
                            {
                                "start": 1814,
                                "end": 2010
                            },
                            {
                                "start": 2011,
                                "end": 2111
                            }
                        ],
                        "ref_mentions": [
                            "267312030"
                        ],
                        "quote": "To generate high-quality data at scale, current synthetic data generation approach explores rephrasing texts using LLMs in varied syntax while preserving the core content (Maini et al., 2024). However, their proposed approach limits up-sampling high-quality data in a way that does not go beyond grammatical styles or surface form transformations-leading little to no improvement when it comes to performance across complex and logical reasoning tasks. We hypothesize that simple rephrasing does not leverage the full potential of the synthetic data to improve the mathematical and complex multi-hop reasoning ability of LLM. Therefore, we propose, MIND, a conversational synthetic data generation approach that adds semantic variations and structured complexity to the raw text which is required to improve complex reasoning ability of the LLMs. In addition, multi-turn conversations can break down the original context step-by-step while each step addresses a sub-context at a time by often injecting complimentary reasoning or explanations. This resonates with how human solves a complex problem using consecutive chain-of-thought reasoning."
                    }
                ]
            },
            {
                "idx": 26,
                "key": "[273507197 | DeSalvo et al. | 2024 | Citations: 0]",
                "snippets": "We present a novel framework, SoftSRV, that is used to generate targeted synthetic fine-tuning data for improving task-specific model performance. Given a sample from a target distribution, our proposed framework uses a data-driven loss minimization approach to steer a frozen large language model (LLM) to generate synthetic sequences that are similar to those from the target distribution. SoftSRV provides a practical improvement over common prompt engineering approaches that rely on human-engineered prompt-templates, which can be idiosyncratic, labor-intensive to craft, and may need to be specialized per domain. We empirically evaluate our method against standard baselines guiding a large LLM to generate synthetic data to fine-tune a smaller language model on three different domains (coding, math, reasoning).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "We present a novel framework, SoftSRV, that is used to generate targeted synthetic fine-tuning data for improving task-specific model performance. Given a sample from a target distribution, our proposed framework uses a data-driven loss minimization approach to steer a frozen large language model (LLM) to generate synthetic sequences that are similar to those from the target distribution. SoftSRV provides a practical improvement over common prompt engineering approaches that rely on human-engineered prompt-templates, which can be idiosyncratic, labor-intensive to craft, and may need to be specialized per domain. We empirically evaluate our method against standard baselines guiding a large LLM to generate synthetic data to fine-tune a smaller language model on three different domains (coding, math, reasoning).",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 27,
                "key": "[273549775 | Ding et al. | 2024 | Citations: 8]",
                "snippets": "In this paper, we propose ScaleQuest, a novel, scalable, and cost-effective data synthesis method that enables the generation of large-scale mathematical reasoning datasets using lightweight 7B-scale models. ScaleQuest introduces a two-stage question-tuning process comprising Question Fine-Tuning (QFT) and Question Preference Optimization (QPO) to unlock the question generation capabilities of problem-solving models. By generating diverse questions from scratch -- without relying on powerful proprietary models or seed data -- we produce a dataset of 1 million problem-solution pairs.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "In this paper, we propose ScaleQuest, a novel, scalable, and cost-effective data synthesis method that enables the generation of large-scale mathematical reasoning datasets using lightweight 7B-scale models. ScaleQuest introduces a two-stage question-tuning process comprising Question Fine-Tuning (QFT) and Question Preference Optimization (QPO) to unlock the question generation capabilities of problem-solving models. By generating diverse questions from scratch -- without relying on powerful proprietary models or seed data -- we produce a dataset of 1 million problem-solution pairs.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 28,
                "key": "[273811804 | Zbeeb et al. | 2024 | Citations: 0]",
                "snippets": "To further advance the field of synthetic data generation, several key areas warrant additional exploration and development. One significant avenue is the capability to generate larger and more diverse datasets. Expanding the capacity to synthesize extensive datasets with high variability would greatly enhance the applicability of synthetic data in machine learning tasks, especially in domains where data scarcity remains a challenge. \n\nMoreover, exploring innovative architectures beyond the current models can lead to substantial advancements. Investigating new generative models or enhancing existing ones could improve the quality and diversity of synthetic data. Importantly, demonstrating that these advancements can be achieved using accessible computational resources, such as a personal computer with a well-coded pipeline, would underscore the feasibility of cutting-edge AI developments without the need for extensive infrastructure. This democratization of technology could encourage broader participation in the field and accelerate innovation. \n\nAdditionally, integrating more robust privacy-preserving techniques into the data generation process remains a critical area for future work. As privacy concerns continue to grow, developing methods that ensure data utility while rigorously protecting sensitive information is essential. Combining differential privacy mechanisms with generative models could provide stronger guarantees and expand the adoption of synthetic data in sensitive domains. \n\nFinally, applying synthetic data generation techniques to a wider range of applications, including those with complex data types such as time-series, graphs, and multimodal data, would significantly broaden the impact of this research. Tailoring generative models to handle these complex data structures effectively could open new opportunities in various fields, from healthcare to finance, where such data types are prevalent.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "X Future Work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1944,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 124
                            },
                            {
                                "start": 125,
                                "end": 211
                            },
                            {
                                "start": 212,
                                "end": 437
                            },
                            {
                                "start": 440,
                                "end": 548
                            },
                            {
                                "start": 549,
                                "end": 670
                            },
                            {
                                "start": 671,
                                "end": 947
                            },
                            {
                                "start": 948,
                                "end": 1060
                            },
                            {
                                "start": 1063,
                                "end": 1204
                            },
                            {
                                "start": 1205,
                                "end": 1350
                            },
                            {
                                "start": 1351,
                                "end": 1513
                            },
                            {
                                "start": 1516,
                                "end": 1751
                            },
                            {
                                "start": 1752,
                                "end": 1944
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "To further advance the field of synthetic data generation, several key areas warrant additional exploration and development. One significant avenue is the capability to generate larger and more diverse datasets. Expanding the capacity to synthesize extensive datasets with high variability would greatly enhance the applicability of synthetic data in machine learning tasks, especially in domains where data scarcity remains a challenge. \n\nMoreover, exploring innovative architectures beyond the current models can lead to substantial advancements. Investigating new generative models or enhancing existing ones could improve the quality and diversity of synthetic data. Importantly, demonstrating that these advancements can be achieved using accessible computational resources, such as a personal computer with a well-coded pipeline, would underscore the feasibility of cutting-edge AI developments without the need for extensive infrastructure. This democratization of technology could encourage broader participation in the field and accelerate innovation. \n\nAdditionally, integrating more robust privacy-preserving techniques into the data generation process remains a critical area for future work. As privacy concerns continue to grow, developing methods that ensure data utility while rigorously protecting sensitive information is essential. Combining differential privacy mechanisms with generative models could provide stronger guarantees and expand the adoption of synthetic data in sensitive domains. \n\nFinally, applying synthetic data generation techniques to a wider range of applications, including those with complex data types such as time-series, graphs, and multimodal data, would significantly broaden the impact of this research. Tailoring generative models to handle these complex data structures effectively could open new opportunities in various fields, from healthcare to finance, where such data types are prevalent."
                    }
                ]
            },
            {
                "idx": 29,
                "key": "[274656618 | Wang et al. | 2024 | Citations: 1]",
                "snippets": "Synthesizing high-quality reasoning data for continual training has been proven to be effective in enhancing the performance of Large Language Models (LLMs). However, previous synthetic approaches struggle to easily scale up data and incur high costs in the pursuit of high quality. In this paper, we propose the Graph-based Synthetic Data Pipeline (GSDP), an economical and scalable framework for high-quality reasoning data synthesis. Inspired by knowledge graphs, we extracted knowledge points from seed data and constructed a knowledge point relationships graph to explore their interconnections. By exploring the implicit relationships among knowledge, our method achieves $\\times$255 data expansion. Furthermore, GSDP led by open-source models, achieves synthesis quality comparable to GPT-4-0613 while maintaining $\\times$100 lower costs.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Synthesizing high-quality reasoning data for continual training has been proven to be effective in enhancing the performance of Large Language Models (LLMs). However, previous synthetic approaches struggle to easily scale up data and incur high costs in the pursuit of high quality. In this paper, we propose the Graph-based Synthetic Data Pipeline (GSDP), an economical and scalable framework for high-quality reasoning data synthesis. Inspired by knowledge graphs, we extracted knowledge points from seed data and constructed a knowledge point relationships graph to explore their interconnections. By exploring the implicit relationships among knowledge, our method achieves $\\times$255 data expansion. Furthermore, GSDP led by open-source models, achieves synthesis quality comparable to GPT-4-0613 while maintaining $\\times$100 lower costs.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 30,
                "key": "[275916340 | Karst et al. | 2025 | Citations: 2]",
                "snippets": "Synthetic data can be defined as \"data that has been generated using a purpose-built mathematical model or algorithm, with the aim of solving a (set of) data science task(s)\" (Jordon et al., 2022, p. 5). This generation process can take many forms as comprehensively categorized by Bauer et al. (2024) into 20 distinct method types. Among these, generative adversarial networks (GANs) are the most popular. GANs learn by pitting a generator (synthesizes data from random noise) and a discriminator (classifies samples as real or fake) against each other, resulting in two highly skilled networks (Goodfellow et al., 2014). This architecture is highly adaptable, as discriminator and generator can be easily adjusted to new tasks (e.g., time series or graph generation) while being frequently the best-performing synthetic data generation method (Bauer et al., 2024). Another commonly employed synthetic data generation method is autoencoder-based architectures, especially variational autoencoder (VAE) (Kingma & Welling, 2013). VAEs are trained by mapping an input sample to a hidden representation, which is then mapped back to the original vector, thus creating a model that synthesizes valid data from a lower dimensional representation. This decoder model is then used to generate data from random noise which makes it especially useful for learning from data with disentangled features (Bauer et al., 2024). Third, recurrent neural networks, feedforward neural networks which include recurrent edges, are able to generate sequential data of arbitrary length. This makes them ideal for sequence generation tasks such as speech synthesis, music, and time series generation (Lipton et al., 2015). Finally, virtual environments are computer simulations in which algorithms interact with each other based on predefined rules, generating synthetic data in the process (Bonabeau, 2002).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Synthetic data generation and its application",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1885,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 203
                            },
                            {
                                "start": 204,
                                "end": 332
                            },
                            {
                                "start": 333,
                                "end": 406
                            },
                            {
                                "start": 407,
                                "end": 622
                            },
                            {
                                "start": 623,
                                "end": 866
                            },
                            {
                                "start": 867,
                                "end": 1028
                            },
                            {
                                "start": 1029,
                                "end": 1241
                            },
                            {
                                "start": 1242,
                                "end": 1413
                            },
                            {
                                "start": 1414,
                                "end": 1564
                            },
                            {
                                "start": 1565,
                                "end": 1699
                            },
                            {
                                "start": 1700,
                                "end": 1885
                            }
                        ],
                        "ref_mentions": [
                            "14402473"
                        ],
                        "quote": "Synthetic data can be defined as \"data that has been generated using a purpose-built mathematical model or algorithm, with the aim of solving a (set of) data science task(s)\" (Jordon et al., 2022, p. 5). This generation process can take many forms as comprehensively categorized by Bauer et al. (2024) into 20 distinct method types. Among these, generative adversarial networks (GANs) are the most popular. GANs learn by pitting a generator (synthesizes data from random noise) and a discriminator (classifies samples as real or fake) against each other, resulting in two highly skilled networks (Goodfellow et al., 2014). This architecture is highly adaptable, as discriminator and generator can be easily adjusted to new tasks (e.g., time series or graph generation) while being frequently the best-performing synthetic data generation method (Bauer et al., 2024). Another commonly employed synthetic data generation method is autoencoder-based architectures, especially variational autoencoder (VAE) (Kingma & Welling, 2013). VAEs are trained by mapping an input sample to a hidden representation, which is then mapped back to the original vector, thus creating a model that synthesizes valid data from a lower dimensional representation. This decoder model is then used to generate data from random noise which makes it especially useful for learning from data with disentangled features (Bauer et al., 2024). Third, recurrent neural networks, feedforward neural networks which include recurrent edges, are able to generate sequential data of arbitrary length. This makes them ideal for sequence generation tasks such as speech synthesis, music, and time series generation (Lipton et al., 2015). Finally, virtual environments are computer simulations in which algorithms interact with each other based on predefined rules, generating synthetic data in the process (Bonabeau, 2002)."
                    }
                ]
            },
            {
                "idx": 31,
                "key": "[276079713 | Yang et al. | 2025 | Citations: 0]",
                "snippets": "Modern foundation models often undergo iterative ``bootstrapping'' in their post-training phase: a model generates synthetic data, an external verifier filters out low-quality samples, and the high-quality subset is used for further fine-tuning. Over multiple iterations, the model's performance improves--raising a crucial question: how should the total budget on generation and training be allocated across iterations to maximize final performance?",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Modern foundation models often undergo iterative ``bootstrapping'' in their post-training phase: a model generates synthetic data, an external verifier filters out low-quality samples, and the high-quality subset is used for further fine-tuning. Over multiple iterations, the model's performance improves--raising a crucial question: how should the total budget on generation and training be allocated across iterations to maximize final performance?",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 32,
                "key": "[276482747 | Pham et al. | 2025 | Citations: 1]",
                "snippets": "We introduce CLIPPER, a compression-based approach for generating synthetic data tailored to narrative claim verification - a task that requires reasoning over a book to verify a given claim. Instead of generating claims directly from the raw text of the book, which results in artifact-riddled claims, CLIPPER first compresses the book into chapter outlines and book summaries and then uses these intermediate representations to generate complex claims and corresponding chain-of-thoughts. Compared to naive approaches, CLIPPER produces claims that are more valid, grounded, and complex.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "We introduce CLIPPER, a compression-based approach for generating synthetic data tailored to narrative claim verification - a task that requires reasoning over a book to verify a given claim. Instead of generating claims directly from the raw text of the book, which results in artifact-riddled claims, CLIPPER first compresses the book into chapter outlines and book summaries and then uses these intermediate representations to generate complex claims and corresponding chain-of-thoughts. Compared to naive approaches, CLIPPER produces claims that are more valid, grounded, and complex.",
                        "section_title": "abstract",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 33,
                "key": "[276903896 | Kuo et al. | 2025 | Citations: 0]",
                "snippets": "Synthetic data generation has long been a core component of ML development, progressing from autoencoders (Hinton et al., 2011) to sophisticated models like variational autoencoders (VAEs) (Lopez et al., 2020), generative adversarial networks (GANs) [18], and denoising diffusion probabilistic models (DDPMs) (Ho et al., 2020). These methods have achieved exceptional success in domains such as image generation (195944196), natural language processing in text (Reed et al., 2016), and text-to-video generation [47].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[1563370 | Reed et al. | 2016 | Citations: 3149]": "Automatic synthesis of realistic images from text would be interesting and useful, but current AI systems are still far from this goal. However, in recent years generic and powerful recurrent neural network architectures have been developed to learn discriminative text feature representations. Meanwhile, deep convolutional generative adversarial networks (GANs) have begun to generate highly compelling images of specific categories, such as faces, album covers, and room interiors. In this work, we develop a novel deep architecture and GAN formulation to effectively bridge these advances in text and image modeling, translating visual concepts from characters to pixels. We demonstrate the capability of our model to generate plausible images of birds and flowers from detailed text descriptions.",
                    "[219955663 | Ho et al. | 2020 | Citations: 18352]": "We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at this https URL"
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 38,
                        "end": 486,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 319
                            },
                            {
                                "start": 320,
                                "end": 486
                            }
                        ],
                        "ref_mentions": [
                            "6138085",
                            "211146177",
                            "219955663",
                            "195944196",
                            "1563370"
                        ],
                        "quote": "Synthetic data generation has long been a core component of ML development, progressing from autoencoders (Hinton et al., 2011) to sophisticated models like variational autoencoders (VAEs) (Lopez et al., 2020), generative adversarial networks (GANs) [18], and denoising diffusion probabilistic models (DDPMs) (Ho et al., 2020). These methods have achieved exceptional success in domains such as image generation (195944196), natural language processing in text (Reed et al., 2016), and text-to-video generation [47]."
                    }
                ]
            },
            {
                "idx": 34,
                "key": "[277104955 | Nadas et al. | 2025 | Citations: 3]",
                "snippets": "In the text domain, LLM-generated data has proven especially useful for low-resource scenarios, delivering substantial improvements in tasks like classification and QA when human-labeled data is limited [33]. Techniques such as promptbased augmentation (zero-shot, few-shot, etc.), retrieval augmentation for grounding facts [3], and iterative refinement have pushed synthetic text data closer in effectiveness to real data.\n\nIn the code domain, LLMs have unlocked new possibilities by generating code snippets, programming instructions, and even whole problem solutions, facilitating better training of code models. The ability to verify code correctness via execution [37] provides a strong advantage, allowing the curation of large-scale, correct-by-construction synthetic code datasets (e.g., Code Alpaca, WizardCoder) that have propelled open-source code models to approach the competency of their proprietary counterparts.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Conclusion",
                        "pdf_hash": "",
                        "start": 348,
                        "end": 1276,
                        "sentence_offsets": [
                            {
                                "start": 201,
                                "end": 416
                            },
                            {
                                "start": 419,
                                "end": 627
                            },
                            {
                                "start": 628,
                                "end": 843
                            },
                            {
                                "start": 846,
                                "end": 1036
                            },
                            {
                                "start": 1037,
                                "end": 1348
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In the text domain, LLM-generated data has proven especially useful for low-resource scenarios, delivering substantial improvements in tasks like classification and QA when human-labeled data is limited [33]. Techniques such as promptbased augmentation (zero-shot, few-shot, etc.), retrieval augmentation for grounding facts [3], and iterative refinement have pushed synthetic text data closer in effectiveness to real data.\n\nIn the code domain, LLMs have unlocked new possibilities by generating code snippets, programming instructions, and even whole problem solutions, facilitating better training of code models. The ability to verify code correctness via execution [37] provides a strong advantage, allowing the curation of large-scale, correct-by-construction synthetic code datasets (e.g., Code Alpaca, WizardCoder) that have propelled open-source code models to approach the competency of their proprietary counterparts."
                    }
                ]
            },
            {
                "idx": 35,
                "key": "[277244110 | Savazzi et al. | 2025 | Citations: 0]",
                "snippets": "While NeSy architectures have demonstrated enhanced generalization in reasoning tasks, their potential for improving synthetic data generation remains largely unexplored. Scene graphs-structured representations where nodes represent objects and edges represent relations-offer a promising symbolic framework for conditioning image generation. This work explores the integration of such NeSy approaches into dataset generation to improve performance on complex tasks, particularly Scene Graph Generation (SGG) from images. The hypothesis posits that scene graphs encode useful background knowledge that can guide the generation process, ensuring synthetic data adhere to structural and semantic constraints.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1134,
                        "end": 1840,
                        "sentence_offsets": [
                            {
                                "start": 1134,
                                "end": 1304
                            },
                            {
                                "start": 1305,
                                "end": 1476
                            },
                            {
                                "start": 1477,
                                "end": 1655
                            },
                            {
                                "start": 1656,
                                "end": 1840
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "While NeSy architectures have demonstrated enhanced generalization in reasoning tasks, their potential for improving synthetic data generation remains largely unexplored. Scene graphs-structured representations where nodes represent objects and edges represent relations-offer a promising symbolic framework for conditioning image generation. This work explores the integration of such NeSy approaches into dataset generation to improve performance on complex tasks, particularly Scene Graph Generation (SGG) from images. The hypothesis posits that scene graphs encode useful background knowledge that can guide the generation process, ensuring synthetic data adhere to structural and semantic constraints."
                    }
                ]
            },
            {
                "idx": 36,
                "key": "[277313659 | Qin et al. | 2025 | Citations: 7]",
                "snippets": "Synthetic data has emerged as a promising approach to augment real data, addressing challenges related to data scarcity and the high cost of data collection and annotation. It has been widely applied in pre-training, instruction following, and reasoning tasks, demonstrating notable improvements in model performance. In this work, we focus on synthetic data generation for the post-training phase. Existing synthetic data generation methods typically rely on a small set of high-quality human-annotated seed samples, leveraging LLMs to generate diverse augmentations through various techniques: 1) sampling seed instructions as few-shot examples to elicit LLMs to generate new instructions [19,23,(Toshniwal et al., 2024)[50]; 2) leveraging LLMs to rephrase seed samples [12,21,34,35,(Yu et al., 2023). However, the scarcity of high-quality seed examples limits the scalability and diversity of generated data. To address this, Xu et al. [51] and Li et al. [24] propose generating synthetic data from scratch, leveraging either the inherent uncertainty of LLMs or a predefined knowledge taxonomy to guide the generation process. In contrast, pre-training data-being both vast and highly diverse-still remains an underutilized resource for scalable post-training data generation. Recent methods have explored extracting high-quality samples directly from web documents (similar to our Level-1 method) [54](Yue et al., 2024)(Zhou et al., 2024) or employing document backtranslation to generate questions (Li et al., 2023). Our SYNTHLLM aligns with this line of research and offers a more efficient approach to leveraging web documents for scaling diverse sample generation compared to aforementioned methods.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[260866107 | Li et al. | 2023 | Citations: 134]": "We present a scalable method to build a high quality instruction following language model by automatically labelling human-written text with corresponding instructions. Our approach, named instruction backtranslation, starts with a language model finetuned on a small amount of seed data, and a given web corpus. The seed model is used to construct training examples by generating instruction prompts for web documents (self-augmentation), and then selecting high quality examples from among these candidates (self-curation). This data is then used to finetune a stronger model. Finetuning LLaMa on two iterations of our approach yields a model that outperforms all other LLaMa-based models on the Alpaca leaderboard not relying on distillation data, demonstrating highly effective self-alignment.",
                    "[262084051 | Yu et al. | 2023 | Citations: 395]": "Large language models (LLMs) have pushed the limits of natural language understanding and exhibited excellent problem-solving ability. Despite the great success, most existing open-source LLMs (e.g., LLaMA-2) are still far away from satisfactory for solving mathematical problem due to the complex reasoning procedures. To bridge this gap, we propose MetaMath, a fine-tuned language model that specializes in mathematical reasoning. Specifically, we start by bootstrapping mathematical questions by rewriting the question from multiple perspectives without extra knowledge, which results in a new dataset called MetaMathQA. Then we fine-tune the LLaMA-2 models on MetaMathQA. Experimental results on two popular benchmarks (i.e., GSM8K and MATH) for mathematical reasoning demonstrate that MetaMath outperforms a suite of open-source LLMs by a significant margin. Our MetaMath-7B model achieves 66.4% on GSM8K and 19.4% on MATH, exceeding the state-of-the-art models of the same size by 11.5% and 8.7%. Particularly, MetaMath-70B achieves an accuracy of 82.3% on GSM8K, slightly better than GPT-3.5-Turbo. We release all the MetaMathQA dataset, the MetaMath models with different model sizes and the training code for public use.",
                    "[269605607 | Yue et al. | 2024 | Citations: 101]": "Instruction tuning improves the reasoning abilities of large language models (LLMs), with data quality and scalability being the crucial factors. Most instruction tuning data come from human crowd-sourcing or GPT-4 distillation. We propose a paradigm to efficiently harvest 10 million naturally existing instruction data from the pre-training web corpus to enhance LLM reasoning. Our approach involves (1) recalling relevant documents, (2) extracting instruction-response pairs, and (3) refining the extracted pairs using open-source LLMs. Fine-tuning base LLMs on this dataset, we build MAmmoTH2 models, which significantly boost performance on reasoning benchmarks. Notably, MAmmoTH2-7B's (Mistral) performance increases from 11% to 36.7% on MATH and from 36% to 68.4% on GSM8K without training on any in-domain data. Further training MAmmoTH2 on public instruction tuning datasets yields MAmmoTH2-Plus, achieving state-of-the-art performance on several reasoning and chatbot benchmarks. Our work demonstrates how to harvest large-scale, high-quality instruction data without costly human annotation or GPT-4 distillation, providing a new paradigm for building better instruction tuning data.",
                    "[269981934 | Zhou et al. | 2024 | Citations: 34]": "Mathematical reasoning is an important capability of large language models~(LLMs) for real-world applications. To enhance this capability, existing work either collects large-scale math-related texts for pre-training, or relies on stronger LLMs (\\eg GPT-4) to synthesize massive math problems. Both types of work generally lead to large costs in training or synthesis. To reduce the cost, based on open-source available texts, we propose an efficient way that trains a small LLM for math problem synthesis, to efficiently generate sufficient high-quality pre-training data. To achieve it, we create a dataset using GPT-4 to distill its data synthesis capability into the small LLM. Concretely, we craft a set of prompts based on human education stages to guide GPT-4, to synthesize problems covering diverse math knowledge and difficulty levels. Besides, we adopt the gradient-based influence estimation method to select the most valuable math-related texts. The both are fed into GPT-4 for creating the knowledge distillation dataset to train the small LLM. We leverage it to synthesize 6 million math problems for pre-training our JiuZhang3.0 model, which only needs to invoke GPT-4 API 9.3k times and pre-train on 4.6B data. Experimental results have shown that JiuZhang3.0 achieves state-of-the-art performance on several mathematical reasoning datasets, under both natural language reasoning and tool manipulation settings. Our code and data will be publicly released in \\url{https://github.com/RUCAIBox/JiuZhang3.0}.",
                    "[273025760 | Toshniwal et al. | 2024 | Citations: 71]": "Mathematical reasoning continues to be a critical challenge in large language model (LLM) development with significant interest. However, most of the cutting-edge progress in mathematical reasoning with LLMs has become \\emph{closed-source} due to lack of access to training data. This lack of data access limits researchers from understanding the impact of different choices for synthesizing and utilizing the data. With the goal of creating a high-quality finetuning (SFT) dataset for math reasoning, we conduct careful ablation experiments on data synthesis using the recently released \\texttt{Llama3.1} family of models. Our experiments show that: (a) solution format matters, with excessively verbose solutions proving detrimental to SFT performance, (b) data generated by a strong teacher outperforms equally-sized data generated by a weak student model, (c) SFT is robust to low-quality solutions, allowing for imprecise data filtering, and (d) question diversity is crucial for achieving data scaling gains. Based on these insights, we create the OpenMathInstruct-2 dataset, which consists of 14M question-solution pairs ($\\approx$ 600K unique questions), making it nearly eight times larger than the previous largest open-source math reasoning dataset. Finetuning the \\texttt{Llama-3.1-8B-Base} using OpenMathInstruct-2 outperforms \\texttt{Llama3.1-8B-Instruct} on MATH by an absolute 15.9\\% (51.9\\% $\\rightarrow$ 67.8\\%). Finally, to accelerate the open-source efforts, we release the code, the finetuned models, and the OpenMathInstruct-2 dataset under a commercially permissive license."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 27,
                        "end": 1654,
                        "sentence_offsets": [
                            {
                                "start": 27,
                                "end": 199
                            },
                            {
                                "start": 200,
                                "end": 344
                            },
                            {
                                "start": 345,
                                "end": 425
                            },
                            {
                                "start": 426,
                                "end": 794
                            },
                            {
                                "start": 795,
                                "end": 902
                            },
                            {
                                "start": 903,
                                "end": 1120
                            },
                            {
                                "start": 1121,
                                "end": 1270
                            },
                            {
                                "start": 1271,
                                "end": 1468
                            },
                            {
                                "start": 1469,
                                "end": 1654
                            }
                        ],
                        "ref_mentions": [
                            "273025760",
                            "262084051",
                            "269605607",
                            "269981934",
                            "260866107"
                        ],
                        "quote": "Synthetic data has emerged as a promising approach to augment real data, addressing challenges related to data scarcity and the high cost of data collection and annotation. It has been widely applied in pre-training, instruction following, and reasoning tasks, demonstrating notable improvements in model performance. In this work, we focus on synthetic data generation for the post-training phase. Existing synthetic data generation methods typically rely on a small set of high-quality human-annotated seed samples, leveraging LLMs to generate diverse augmentations through various techniques: 1) sampling seed instructions as few-shot examples to elicit LLMs to generate new instructions [19,23,(Toshniwal et al., 2024)[50]; 2) leveraging LLMs to rephrase seed samples [12,21,34,35,(Yu et al., 2023). However, the scarcity of high-quality seed examples limits the scalability and diversity of generated data. To address this, Xu et al. [51] and Li et al. [24] propose generating synthetic data from scratch, leveraging either the inherent uncertainty of LLMs or a predefined knowledge taxonomy to guide the generation process. In contrast, pre-training data-being both vast and highly diverse-still remains an underutilized resource for scalable post-training data generation. Recent methods have explored extracting high-quality samples directly from web documents (similar to our Level-1 method) [54](Yue et al., 2024)(Zhou et al., 2024) or employing document backtranslation to generate questions (Li et al., 2023). Our SYNTHLLM aligns with this line of research and offers a more efficient approach to leveraging web documents for scaling diverse sample generation compared to aforementioned methods."
                    }
                ]
            },
            {
                "idx": 37,
                "key": "[277502040 | Ahmad et al. | 2025 | Citations: 13]",
                "snippets": "To overcome this bottleneck, many have successfully leveraged LLMs to generate high-quality synthetic code data (Luo et al., 2023)(Yu et al., 2023). Notably, works like Wei et al. (2024a) and Huang et al. (2025) have generated diverse instruction-solution pairs, subsequently fine-tuning base models to achieve top results in HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021) and BigCodeBench (Zhuo et al., 2025) benchmarks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[259164815 | Luo et al. | 2023 | Citations: 689]": "Code Large Language Models (Code LLMs), such as StarCoder, have demonstrated exceptional performance in code-related tasks. However, most existing models are solely pre-trained on extensive raw code data without instruction fine-tuning. In this paper, we introduce WizardCoder, which empowers Code LLMs with complex instruction fine-tuning, by adapting the Evol-Instruct method to the domain of code. Through comprehensive experiments on four prominent code generation benchmarks, namely HumanEval, HumanEval+, MBPP, and DS-1000, we unveil the exceptional capabilities of our model. It surpasses all other open-source Code LLMs by a substantial margin. Moreover, our model even outperforms the largest closed LLMs, Anthropic's Claude and Google's Bard, on HumanEval and HumanEval+. Our code, model weights, and data are public at https://github.com/nlpxucan/WizardLM",
                    "[270258158 | Yu et al. | 2023 | Citations: 28]": "Recent work demonstrates that, after instruction tuning, Code Large Language Models (Code LLMs) can obtain impressive capabilities to address a wide range of code-related tasks. However, current instruction tuning methods for Code LLMs mainly focus on the traditional code generation task, resulting in poor performance in complex multi-task scenarios. In this paper, we concentrate on multiple code-related tasks and present WaveCoder, a series of Code LLMs trained with Widespread And Versatile Enhanced instruction data. To enable the models to tackle complex code-related tasks, we propose a method to stably generate diverse, high-quality instruction data from open source code dataset in multi-task scenarios and obtain CodeSeaXDataset, a dataset comprising 19,915 instruction instances across 4 code-related tasks, which is aimed at improving the generalization ability of Code LLM. Our experiments demonstrate that WaveCoder models significantly outperform other open-source models in terms of the generalization ability across different code-related tasks. Moreover, WaveCoder-Ultra-6.7B presents the state-of-the-art generalization abilities on a wide range of code-related tasks."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 371,
                        "end": 802,
                        "sentence_offsets": [
                            {
                                "start": 371,
                                "end": 518
                            },
                            {
                                "start": 519,
                                "end": 802
                            }
                        ],
                        "ref_mentions": [
                            "259164815",
                            "270258158"
                        ],
                        "quote": "To overcome this bottleneck, many have successfully leveraged LLMs to generate high-quality synthetic code data (Luo et al., 2023)(Yu et al., 2023). Notably, works like Wei et al. (2024a) and Huang et al. (2025) have generated diverse instruction-solution pairs, subsequently fine-tuning base models to achieve top results in HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021) and BigCodeBench (Zhuo et al., 2025) benchmarks."
                    }
                ]
            },
            {
                "idx": 38,
                "key": "[277780529 | Khan et al. | 2025 | Citations: 0]",
                "snippets": "Past work has generally approached improving models on reasoning tasks like math by generating large amounts of broad-coverage training data. This trend builds on work in generating instruction-tuning data (Wang et al., 2022), where model-generated instructions have been used to teach models to follow prompts. Luo et al. (2023) introduced generation method based on Evol-Instruct (Xu et al., 2023), which augmented a seed dataset of math problems by generating easier and harder problems. Related lines of work have sought to expand datasets by augmenting existing math datasets (Yu et al., 2023), adding multiple reasoning strategies (Yang et al., 2023), covering challenging competition problems (Li et al., 2024), or curating responses (Liu et al., 2024). The data generated in these settings differs from our data in a number of respects: first, it is generally broad-coverage, focusing on large-scale diverse data, as opposed to targeted, instancespecific data. This direction was also explored by (Khan et al., 2024), who define data generation agents that can generate specific data based on a particular model's weaknesses, covering math and several other domains. Finally, past work that has augmented a seed dataset (e.g., (Yu et al., 2023); (Yang et al., 2023)) has done so by modifying problems in the surface form, whereas our method first infers a latent structure and then creates problems by sampling from the structure.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[254877310 | Wang et al. | 2022 | Citations: 2251]": "Large \u201cinstruction-tuned\u201d language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT-001, which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5% absolute gap behind InstructGPT-001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning.",
                    "[262084051 | Yu et al. | 2023 | Citations: 395]": "Large language models (LLMs) have pushed the limits of natural language understanding and exhibited excellent problem-solving ability. Despite the great success, most existing open-source LLMs (e.g., LLaMA-2) are still far away from satisfactory for solving mathematical problem due to the complex reasoning procedures. To bridge this gap, we propose MetaMath, a fine-tuned language model that specializes in mathematical reasoning. Specifically, we start by bootstrapping mathematical questions by rewriting the question from multiple perspectives without extra knowledge, which results in a new dataset called MetaMathQA. Then we fine-tune the LLaMA-2 models on MetaMathQA. Experimental results on two popular benchmarks (i.e., GSM8K and MATH) for mathematical reasoning demonstrate that MetaMath outperforms a suite of open-source LLMs by a significant margin. Our MetaMath-7B model achieves 66.4% on GSM8K and 19.4% on MATH, exceeding the state-of-the-art models of the same size by 11.5% and 8.7%. Particularly, MetaMath-70B achieves an accuracy of 82.3% on GSM8K, slightly better than GPT-3.5-Turbo. We release all the MetaMathQA dataset, the MetaMath models with different model sizes and the training code for public use.",
                    "[266210109 | Yang et al. | 2023 | Citations: 91]": "3D simulated environments play a critical role in Embodied AI, but their creation requires expertise and extensive manual effort, restricting their diversity and scope. To miti-gate this limitation, we present Holodeck, a system that generates 3D environments to match a user-supplied prompt fullyautomatedly. Holodeck can generate diverse scenes, e.g., arcades, spas, and museums, adjust the designs for styles, and can capture the semantics of complex queries such as \u201capartment for a researcher with a cat\u201d and \u201coffice of a professor who is a fan of Star Wars\u201d. Holodeck leverages a large language model (i.e., GPT-4) for common sense knowledge about what the scene might look like and uses a large collection of 3D assets from Objaverse to populate the scene with diverse objects. To address the challenge of positioning objects correctly, we prompt GPT-4 to generate spatial relational constraints between objects and then optimize the layout to satisfy those constraints. Our large-scale human evaluation shows that annotators prefer Holodeck over manually designed procedural baselines in residential scenes and that Holodeck can produce high-quality outputs for diverse scene types. We also demonstrate an exciting application of Holodeck in Embodied AI, training agents to navigate in novel scenes like music rooms and daycares without human-constructed data, which is a significant step forward in developing general-purpose embodied agents.",
                    "[273228651 | Khan et al. | 2024 | Citations: 3]": "The process of creating training data to teach models is currently driven by humans, who manually analyze model weaknesses and plan how to create data that improves a student model. Approaches using LLMs as annotators reduce human effort, but still require humans to interpret feedback from evaluations and control the LLM to produce data the student needs. Automating this labor-intensive process by creating autonomous data generation agents - or teachers - is desirable, but requires environments that can simulate the feedback-driven, iterative, closed loop of data creation. To enable rapid, scalable testing for such agents and their modules, we introduce DataEnvGym, a testbed of teacher environments for data generation agents. DataEnvGym frames data generation as a sequential decision-making task, involving an agent consisting of a data generation policy (which generates a plan for creating training data) and a data generation engine (which transforms the plan into data), inside an environment that provides student feedback. The agent's goal is to improve student performance. Students are iteratively trained and evaluated on generated data, and their feedback (in the form of errors or weak skills) is reported to the agent after each iteration. DataEnvGym includes multiple teacher environment instantiations across 3 levels of structure in the state representation and action space. More structured environments are based on inferred skills and offer more interpretability and curriculum control. We support 4 domains (math, code, VQA, and tool-use) and test multiple students and teachers. Example agents in our teaching environments can iteratively improve students across tasks and settings. Moreover, we show that environments teach different skill levels and test variants of key modules, pointing to future work in improving data generation agents, engines, and feedback mechanisms."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1433,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 141
                            },
                            {
                                "start": 142,
                                "end": 311
                            },
                            {
                                "start": 312,
                                "end": 490
                            },
                            {
                                "start": 491,
                                "end": 759
                            },
                            {
                                "start": 760,
                                "end": 967
                            },
                            {
                                "start": 968,
                                "end": 1172
                            },
                            {
                                "start": 1173,
                                "end": 1433
                            }
                        ],
                        "ref_mentions": [
                            "254877310",
                            "262084051",
                            "266210109",
                            "273228651",
                            "262084051",
                            "266210109"
                        ],
                        "quote": "Past work has generally approached improving models on reasoning tasks like math by generating large amounts of broad-coverage training data. This trend builds on work in generating instruction-tuning data (Wang et al., 2022), where model-generated instructions have been used to teach models to follow prompts. Luo et al. (2023) introduced generation method based on Evol-Instruct (Xu et al., 2023), which augmented a seed dataset of math problems by generating easier and harder problems. Related lines of work have sought to expand datasets by augmenting existing math datasets (Yu et al., 2023), adding multiple reasoning strategies (Yang et al., 2023), covering challenging competition problems (Li et al., 2024), or curating responses (Liu et al., 2024). The data generated in these settings differs from our data in a number of respects: first, it is generally broad-coverage, focusing on large-scale diverse data, as opposed to targeted, instancespecific data. This direction was also explored by (Khan et al., 2024), who define data generation agents that can generate specific data based on a particular model's weaknesses, covering math and several other domains. Finally, past work that has augmented a seed dataset (e.g., (Yu et al., 2023); (Yang et al., 2023)) has done so by modifying problems in the surface form, whereas our method first infers a latent structure and then creates problems by sampling from the structure."
                    }
                ]
            },
            {
                "idx": 39,
                "key": "[278171321 | Wang et al. | 2025 | Citations: 0]",
                "snippets": "To address this scarcity, researchers have explored various synthesizing methods, particularly in the mathematics domain. The mainstream methods involve data augmentation based on existing annotated training sets, such as GSM8K (Cobbe et al., 2021) and MATH (Huang et al., 2024), ranging from self-evolving instructions (Xu et al., 2024;(Zeng et al., 2024) and question paraphrasing (Yu et al., 2024), to solution augmentation (Lu et al., 2024). However, these methods are limited by the available training data, constraining the synthesis diversity (Li et al., 2024c(Li et al., , 2023)). To enhance diversity, recent approaches enable LLMs to generate a large scale of questions from various mathematicsrelated sources, including web pages (Yue et al., 2024) and knowledge points (Tang et al., 2024) from web corpora or textbooks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[268247488 | Huang et al. | 2024 | Citations: 43]": "Large language models have shown great potential in complex reasoning tasks, yet their performance is often hampered by the scarcity of high-quality and reasoning-focused training datasets. Addressing this challenge, we propose Key-PointDriven Data Synthesis (KPDDS), a novel data synthesis framework that synthesizes question-answer pairs by leveraging key points and exemplar practices from authentic data sources. KPDDS ensures the generation of novel questions with rigorous quality control and substantial scalability. As a result, we present KPMath, an extensive synthetic dataset tailored for mathematical reasoning, comprising over 800K questionanswer pairs. Utilizing KPMath and augmenting it with additional reasoning-intensive corpora, we create the comprehensive KPMath-Plus dataset. Our experiments demonstrate that this dataset can enhance the mathematical reasoning performance of models across various architectures and sizes. The Qwen1.5-72B model, fine-tuned on KPMath-Plus, achieves 87.0% accuracy on GSM8K and 58.3% on MATH, surpassing competitors in the 7B to 72B range and best commercial models like GPT-4 across multiple math reasoning datasets.",
                    "[268247902 | Tang et al. | 2024 | Citations: 82]": "Large language models (LLMs) have demonstrated remarkable capabilities in problem-solving. However, their proficiency in solving mathematical problems remains inadequate. We propose MathScale, a simple and scalable method to create high-quality mathematical reasoning data using frontier LLMs (e.g., {\\tt GPT-3.5}). Inspired by the cognitive mechanism in human mathematical learning, it first extracts topics and knowledge points from seed math questions and then build a concept graph, which is subsequently used to generate new math questions. MathScale exhibits effective scalability along the size axis of the math dataset that we generate. As a result, we create a mathematical reasoning dataset (MathScaleQA) containing two million math question-answer pairs. To evaluate mathematical reasoning abilities of LLMs comprehensively, we construct {\\sc MwpBench}, a benchmark of Math Word Problems, which is a collection of ten datasets (including GSM8K and MATH) covering K-12, college, and competition level math problems. We apply MathScaleQA to fine-tune open-source LLMs (e.g., LLaMA-2 and Mistral), resulting in significantly improved capabilities in mathematical reasoning. Evaluated on {\\sc MwpBench}, MathScale-7B achieves state-of-the-art performance across all datasets, surpassing its best peers of equivalent size by 42.9\\% in micro average accuracy and 43.7\\% in macro average accuracy, respectively.",
                    "[270213007 | Zeng et al. | 2024 | Citations: 10]": "Fine-tuning large pre-trained language models with Evol-Instruct has achieved encouraging results across a wide range of tasks. However, designing effective evolving methods for instruction evolution requires substantial human expertise. This paper proposes Auto Evol-Instruct, an end-to-end framework that evolves instruction datasets using large language models without any human effort. The framework automatically analyzes and summarizes suitable evolutionary strategies for the given instruction data and iteratively improves the evolving method based on issues exposed during the instruction evolution process. Our extensive experiments demonstrate that the best method optimized by Auto Evol-Instruct outperforms human-designed methods on various benchmarks, including MT-Bench, AlpacaEval, GSM8K, and HumanEval."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 430,
                        "end": 1264,
                        "sentence_offsets": [
                            {
                                "start": 430,
                                "end": 551
                            },
                            {
                                "start": 552,
                                "end": 878
                            },
                            {
                                "start": 879,
                                "end": 1021
                            },
                            {
                                "start": 1022,
                                "end": 1264
                            }
                        ],
                        "ref_mentions": [
                            "268247488",
                            "270213007",
                            "268247902"
                        ],
                        "quote": "To address this scarcity, researchers have explored various synthesizing methods, particularly in the mathematics domain. The mainstream methods involve data augmentation based on existing annotated training sets, such as GSM8K (Cobbe et al., 2021) and MATH (Huang et al., 2024), ranging from self-evolving instructions (Xu et al., 2024;(Zeng et al., 2024) and question paraphrasing (Yu et al., 2024), to solution augmentation (Lu et al., 2024). However, these methods are limited by the available training data, constraining the synthesis diversity (Li et al., 2024c(Li et al., , 2023)). To enhance diversity, recent approaches enable LLMs to generate a large scale of questions from various mathematicsrelated sources, including web pages (Yue et al., 2024) and knowledge points (Tang et al., 2024) from web corpora or textbooks."
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.03894,
        "cot": "Planning dimensions for synthesizing information about recent advances in synthetic data generation techniques for solving hard reasoning tasks:\n\n1. \"Introduction to Synthetic Data Generation\" - This should be a synthesis paragraph that provides background on what synthetic data generation is, its importance, and general evolution. This sets the context for the rest of the answer.\n\n2. \"Large Language Models (LLMs) for Synthetic Data\" - This should be a synthesis paragraph as it requires explaining the role of LLMs in generating synthetic data, particularly for reasoning tasks. Many quotes discuss how LLMs have become central to modern synthetic data generation approaches.\n\n3. \"Graph-based Approaches for Reasoning Tasks\" - This should be a synthesis paragraph since several quotes mention graph-based methods for generating reasoning data, which appears to be an important recent advance worth highlighting.\n\n4. \"Mathematical Reasoning Data Synthesis\" - This should be a synthesis paragraph as multiple quotes specifically address advances in generating synthetic data for mathematical reasoning, which is a prominent hard reasoning task.\n\n5. \"Code Reasoning Data Synthesis\" - This should be a synthesis paragraph since several quotes mention specific techniques for generating synthetic data for code reasoning tasks.\n\n6. \"Key Techniques and Frameworks\" - This should be a list format to clearly enumerate the specific techniques and frameworks mentioned across papers. A list makes it easier for the reader to scan through the diverse approaches.\n\n7. \"Evaluation and Optimization Methods\" - This should be a synthesis paragraph as it requires explaining how researchers evaluate and optimize synthetic data generation processes.\n\n8. \"Future Directions\" - This should be a synthesis paragraph to discuss emerging trends and opportunities in synthetic data generation for reasoning tasks.\n\nEach dimension addresses a specific aspect of the user's query about recent advances in synthetic data generation techniques for solving hard reasoning tasks, ensuring comprehensive coverage of the available information.",
        "plan": {
            "Introduction to Synthetic Data Generation (synthesis)": [
                14,
                30,
                33,
                36
            ],
            "Large Language Models (LLMs) for Synthetic Data (synthesis)": [
                4,
                8,
                13,
                18,
                21,
                23,
                25,
                31,
                34
            ],
            "Graph-based Approaches for Reasoning Tasks (synthesis)": [
                1,
                19,
                29,
                35
            ],
            "Mathematical Reasoning Data Synthesis (synthesis)": [
                11,
                16,
                20,
                22,
                27,
                38,
                39
            ],
            "Code Reasoning Data Synthesis (synthesis)": [
                20,
                22,
                26,
                37
            ],
            "Key Techniques and Frameworks (list)": [
                0,
                2,
                3,
                5,
                6,
                7,
                9,
                10,
                12,
                15,
                17,
                24,
                28,
                32,
                36
            ],
            "Evaluation and Optimization Methods (synthesis)": [
                26,
                31
            ],
            "Future Directions (synthesis)": [
                28
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction to Synthetic Data Generation",
                "tldr": "Synthetic data generation has evolved from basic autoencoder architectures to sophisticated models like GANs, VAEs, and diffusion models, enabling high-quality data creation across various domains. These techniques address challenges of data scarcity, privacy concerns, and annotation costs by generating diverse, customizable datasets for training AI systems. (11 sources)",
                "text": "\nSynthetic data can be defined as \"data that has been generated using a purpose-built mathematical model or algorithm, with the aim of solving a (set of) data science task(s)\" <Paper corpusId=\"275916340\" paperTitle=\"(Karst et al., 2025)\" isShortName></Paper>. This approach has emerged as a promising solution to address fundamental challenges in AI development, including data scarcity and the high costs associated with data collection and annotation <Paper corpusId=\"277313659\" paperTitle=\"(Qin et al., 2025)\" isShortName></Paper>.\n\nThe field has witnessed significant evolution in generative techniques. Early approaches centered around basic autoencoder architectures <Paper corpusId=\"276903896\" paperTitle=\"(Kuo et al., 2025)\" isShortName></Paper>, but recent advances have led to more sophisticated methods for modeling complex high-dimensional datasets while addressing privacy concerns <Paper corpusId=\"269702974\" paperTitle=\"(Yadav et al., 2024)\" isShortName></Paper>. Among the most prominent approaches are:\n\n1. **Generative Adversarial Networks (GANs)**: These models work by pitting a generator against a discriminator, resulting in two highly skilled networks after training <Paper corpusId=\"275916340\" paperTitle=\"(Karst et al., 2025)\" isShortName></Paper>. GANs have demonstrated remarkable success in generating realistic images from text descriptions <Paper corpusId=\"1563370\" paperTitle=\"(Reed et al., 2016)\" isShortName></Paper> and are frequently the best-performing synthetic data generation method due to their adaptability to various tasks <Paper corpusId=\"275916340\" paperTitle=\"(Karst et al., 2025)\" isShortName></Paper>.\n\n2. **Variational Autoencoders (VAEs)**: These encode input data into a lower-dimensional representation and then decode it back, making them especially useful for learning from data with disentangled features <Paper corpusId=\"275916340\" paperTitle=\"(Karst et al., 2025)\" isShortName></Paper>. VAEs excel at generating valid data from this compressed representation.\n\n3. **Diffusion Models**: Denoising diffusion probabilistic models (DDPMs) have achieved state-of-the-art results in image generation tasks <Paper corpusId=\"219955663\" paperTitle=\"(Ho et al., 2020)\" isShortName></Paper>, with impressive quality metrics on standard datasets.\n\n4. **Recurrent Neural Networks**: These are particularly suitable for generating sequential data of arbitrary length, making them ideal for tasks like speech synthesis, music, and time series generation <Paper corpusId=\"275916340\" paperTitle=\"(Karst et al., 2025)\" isShortName></Paper>.\n\n5. **Virtual Environments**: Computer simulations where algorithms interact based on predefined rules to generate synthetic data <Paper corpusId=\"275916340\" paperTitle=\"(Karst et al., 2025)\" isShortName></Paper>.\n\nIn the context of language models, synthetic data generation for post-training has taken several approaches. Traditional methods rely on a small set of high-quality human-annotated seed samples, using large language models (LLMs) to create diverse augmentations <Paper corpusId=\"277313659\" paperTitle=\"(Qin et al., 2025)\" isShortName></Paper>. These include sampling seed instructions as few-shot examples <Paper corpusId=\"273025760\" paperTitle=\"(Toshniwal et al., 2024)\" isShortName></Paper>, rephrasing seed samples <Paper corpusId=\"262084051\" paperTitle=\"(Yu et al., 2023)\" isShortName></Paper>, or leveraging pre-training data as an underutilized resource for scalable generation <Paper corpusId=\"269605607\" paperTitle=\"(Yue et al., 2024)\" isShortName></Paper> <Paper corpusId=\"269981934\" paperTitle=\"(Zhou et al., 2024)\" isShortName></Paper>.\n\nMore recent approaches, such as instruction backtranslation, automatically label human-written text with corresponding instructions, enabling the construction of high-quality instruction-following language models <Paper corpusId=\"260866107\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper>. This method has proven particularly effective for mathematical reasoning tasks, where model performance has seen significant improvements through synthetic data generation <Paper corpusId=\"262084051\" paperTitle=\"(Yu et al., 2023)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Karst et al., 2025)",
                        "snippets": [
                            "Synthetic data can be defined as \"data that has been generated using a purpose-built mathematical model or algorithm, with the aim of solving a (set of) data science task(s)\" (Jordon et al., 2022, p. 5). This generation process can take many forms as comprehensively categorized by Bauer et al. (2024) into 20 distinct method types. Among these, generative adversarial networks (GANs) are the most popular. GANs learn by pitting a generator (synthesizes data from random noise) and a discriminator (classifies samples as real or fake) against each other, resulting in two highly skilled networks (Goodfellow et al., 2014). This architecture is highly adaptable, as discriminator and generator can be easily adjusted to new tasks (e.g., time series or graph generation) while being frequently the best-performing synthetic data generation method (Bauer et al., 2024). Another commonly employed synthetic data generation method is autoencoder-based architectures, especially variational autoencoder (VAE) (Kingma & Welling, 2013). VAEs are trained by mapping an input sample to a hidden representation, which is then mapped back to the original vector, thus creating a model that synthesizes valid data from a lower dimensional representation. This decoder model is then used to generate data from random noise which makes it especially useful for learning from data with disentangled features (Bauer et al., 2024). Third, recurrent neural networks, feedforward neural networks which include recurrent edges, are able to generate sequential data of arbitrary length. This makes them ideal for sequence generation tasks such as speech synthesis, music, and time series generation (Lipton et al., 2015). Finally, virtual environments are computer simulations in which algorithms interact with each other based on predefined rules, generating synthetic data in the process (Bonabeau, 2002)."
                        ],
                        "paper": {
                            "corpus_id": 275916340,
                            "title": "SynDEc: A Synthetic Data Ecosystem",
                            "authors": [
                                {
                                    "authorId": "2277750192",
                                    "name": "F. Karst"
                                },
                                {
                                    "authorId": "2263394895",
                                    "name": "M. Li"
                                },
                                {
                                    "authorId": "1737216",
                                    "name": "J. Leimeister"
                                }
                            ],
                            "year": 2025,
                            "venue": "Electronic Markets",
                            "n_citations": 2
                        },
                        "score": 0.91748046875
                    },
                    {
                        "id": "(Qin et al., 2025)",
                        "snippets": [
                            "Synthetic data has emerged as a promising approach to augment real data, addressing challenges related to data scarcity and the high cost of data collection and annotation. It has been widely applied in pre-training, instruction following, and reasoning tasks, demonstrating notable improvements in model performance. In this work, we focus on synthetic data generation for the post-training phase. Existing synthetic data generation methods typically rely on a small set of high-quality human-annotated seed samples, leveraging LLMs to generate diverse augmentations through various techniques: 1) sampling seed instructions as few-shot examples to elicit LLMs to generate new instructions [19,23,(Toshniwal et al., 2024)[50]; 2) leveraging LLMs to rephrase seed samples [12,21,34,35,(Yu et al., 2023). However, the scarcity of high-quality seed examples limits the scalability and diversity of generated data. To address this, Xu et al. [51] and Li et al. [24] propose generating synthetic data from scratch, leveraging either the inherent uncertainty of LLMs or a predefined knowledge taxonomy to guide the generation process. In contrast, pre-training data-being both vast and highly diverse-still remains an underutilized resource for scalable post-training data generation. Recent methods have explored extracting high-quality samples directly from web documents (similar to our Level-1 method) [54](Yue et al., 2024)(Zhou et al., 2024) or employing document backtranslation to generate questions (Li et al., 2023). Our SYNTHLLM aligns with this line of research and offers a more efficient approach to leveraging web documents for scaling diverse sample generation compared to aforementioned methods."
                        ],
                        "paper": {
                            "corpus_id": 277313659,
                            "title": "Scaling Laws of Synthetic Data for Language Models",
                            "authors": [
                                {
                                    "authorId": "2118242824",
                                    "name": "Zeyu Qin"
                                },
                                {
                                    "authorId": "2287927238",
                                    "name": "Qingxiu Dong"
                                },
                                {
                                    "authorId": "2284863493",
                                    "name": "Xingxing Zhang"
                                },
                                {
                                    "authorId": "2294850817",
                                    "name": "Li Dong"
                                },
                                {
                                    "authorId": "2116768132",
                                    "name": "Xiaolong Huang"
                                },
                                {
                                    "authorId": "2291073936",
                                    "name": "Ziyi Yang"
                                },
                                {
                                    "authorId": "2268760479",
                                    "name": "Mahmoud Khademi"
                                },
                                {
                                    "authorId": "2273919921",
                                    "name": "Dongdong Zhang"
                                },
                                {
                                    "authorId": "3032929",
                                    "name": "H. Awadalla"
                                },
                                {
                                    "authorId": "2352012716",
                                    "name": "Yi R. Fung"
                                },
                                {
                                    "authorId": "2347682196",
                                    "name": "Weizhu Chen"
                                },
                                {
                                    "authorId": "2258337019",
                                    "name": "Minhao Cheng"
                                },
                                {
                                    "authorId": "2323870436",
                                    "name": "Furu Wei"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 7
                        },
                        "score": 0.96337890625
                    },
                    {
                        "id": "(Kuo et al., 2025)",
                        "snippets": [
                            "Synthetic data generation has long been a core component of ML development, progressing from autoencoders (Hinton et al., 2011) to sophisticated models like variational autoencoders (VAEs) (Lopez et al., 2020), generative adversarial networks (GANs) [18], and denoising diffusion probabilistic models (DDPMs) (Ho et al., 2020). These methods have achieved exceptional success in domains such as image generation (195944196), natural language processing in text (Reed et al., 2016), and text-to-video generation [47]."
                        ],
                        "paper": {
                            "corpus_id": 276903896,
                            "title": "Attention-Based Synthetic Data Generation for Calibration-Enhanced Survival Analysis: A Case Study for Chronic Kidney Disease Using Electronic Health Records",
                            "authors": [
                                {
                                    "authorId": "2265851129",
                                    "name": "Nicholas I-Hsien Kuo"
                                },
                                {
                                    "authorId": "2292255090",
                                    "name": "Blanca Gallego"
                                },
                                {
                                    "authorId": "2292472962",
                                    "name": "Louisa R Jorm"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.91552734375
                    },
                    {
                        "id": "(Yadav et al., 2024)",
                        "snippets": [
                            "Recent advances in generative models have led to more efficient modeling of complex high-dimensional datasets. Also, privacy concerns have led to the development of robust models with lesser risk of privacy breaches."
                        ],
                        "paper": {
                            "corpus_id": 269702974,
                            "title": "Rigorous Experimental Analysis of Tabular Data Generated using TVAE and CTGAN",
                            "authors": [
                                {
                                    "authorId": "2300552611",
                                    "name": "Parul Yadav"
                                },
                                {
                                    "authorId": "2300755856",
                                    "name": "Manish Gaur"
                                },
                                {
                                    "authorId": "94670511",
                                    "name": "Rahul Kumar Madhukar"
                                },
                                {
                                    "authorId": "2300646169",
                                    "name": "Gaurav Verma"
                                },
                                {
                                    "authorId": "2300984779",
                                    "name": "Pankaj Kumar"
                                },
                                {
                                    "authorId": "2206080534",
                                    "name": "Nishat Fatima"
                                },
                                {
                                    "authorId": "2277471527",
                                    "name": "Saqib Sarwar"
                                },
                                {
                                    "authorId": "2300596310",
                                    "name": "Yash Raj Dwivedi"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Journal of Advanced Computer Science and Applications",
                            "n_citations": 1
                        },
                        "score": 0.92333984375
                    },
                    {
                        "id": "(Reed et al., 2016)",
                        "snippets": [
                            "Automatic synthesis of realistic images from text would be interesting and useful, but current AI systems are still far from this goal. However, in recent years generic and powerful recurrent neural network architectures have been developed to learn discriminative text feature representations. Meanwhile, deep convolutional generative adversarial networks (GANs) have begun to generate highly compelling images of specific categories, such as faces, album covers, and room interiors. In this work, we develop a novel deep architecture and GAN formulation to effectively bridge these advances in text and image modeling, translating visual concepts from characters to pixels. We demonstrate the capability of our model to generate plausible images of birds and flowers from detailed text descriptions."
                        ],
                        "paper": {
                            "corpus_id": 1563370,
                            "title": "Generative Adversarial Text to Image Synthesis",
                            "authors": [
                                {
                                    "authorId": "144828948",
                                    "name": "Scott E. Reed"
                                },
                                {
                                    "authorId": "2893664",
                                    "name": "Zeynep Akata"
                                },
                                {
                                    "authorId": "3084614",
                                    "name": "Xinchen Yan"
                                },
                                {
                                    "authorId": "2876316",
                                    "name": "Lajanugen Logeswaran"
                                },
                                {
                                    "authorId": "48920094",
                                    "name": "B. Schiele"
                                },
                                {
                                    "authorId": "1697141",
                                    "name": "Honglak Lee"
                                }
                            ],
                            "year": 2016,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 3149
                        },
                        "score": 0
                    },
                    {
                        "id": "(Ho et al., 2020)",
                        "snippets": [
                            "We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at this https URL"
                        ],
                        "paper": {
                            "corpus_id": 219955663,
                            "title": "Denoising Diffusion Probabilistic Models",
                            "authors": [
                                {
                                    "authorId": "2126278",
                                    "name": "Jonathan Ho"
                                },
                                {
                                    "authorId": "1623995772",
                                    "name": "Ajay Jain"
                                },
                                {
                                    "authorId": "1689992",
                                    "name": "P. Abbeel"
                                }
                            ],
                            "year": 2020,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 18352
                        },
                        "score": 0
                    },
                    {
                        "id": "(Toshniwal et al., 2024)",
                        "snippets": [
                            "Mathematical reasoning continues to be a critical challenge in large language model (LLM) development with significant interest. However, most of the cutting-edge progress in mathematical reasoning with LLMs has become \\emph{closed-source} due to lack of access to training data. This lack of data access limits researchers from understanding the impact of different choices for synthesizing and utilizing the data. With the goal of creating a high-quality finetuning (SFT) dataset for math reasoning, we conduct careful ablation experiments on data synthesis using the recently released \\texttt{Llama3.1} family of models. Our experiments show that: (a) solution format matters, with excessively verbose solutions proving detrimental to SFT performance, (b) data generated by a strong teacher outperforms equally-sized data generated by a weak student model, (c) SFT is robust to low-quality solutions, allowing for imprecise data filtering, and (d) question diversity is crucial for achieving data scaling gains. Based on these insights, we create the OpenMathInstruct-2 dataset, which consists of 14M question-solution pairs ($\\approx$ 600K unique questions), making it nearly eight times larger than the previous largest open-source math reasoning dataset. Finetuning the \\texttt{Llama-3.1-8B-Base} using OpenMathInstruct-2 outperforms \\texttt{Llama3.1-8B-Instruct} on MATH by an absolute 15.9\\% (51.9\\% $\\rightarrow$ 67.8\\%). Finally, to accelerate the open-source efforts, we release the code, the finetuned models, and the OpenMathInstruct-2 dataset under a commercially permissive license."
                        ],
                        "paper": {
                            "corpus_id": 273025760,
                            "title": "OpenMathInstruct-2: Accelerating AI for Math with Massive Open-Source Instruction Data",
                            "authors": [
                                {
                                    "authorId": "2634203",
                                    "name": "Shubham Toshniwal"
                                },
                                {
                                    "authorId": "2323910998",
                                    "name": "Wei Du"
                                },
                                {
                                    "authorId": "2284217750",
                                    "name": "Ivan Moshkov"
                                },
                                {
                                    "authorId": "9107622",
                                    "name": "B. Kisa\u010danin"
                                },
                                {
                                    "authorId": "2323782209",
                                    "name": "Alexan Ayrapetyan"
                                },
                                {
                                    "authorId": "25683112",
                                    "name": "Igor Gitman"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 71
                        },
                        "score": 0
                    },
                    {
                        "id": "(Yu et al., 2023)",
                        "snippets": [
                            "Large language models (LLMs) have pushed the limits of natural language understanding and exhibited excellent problem-solving ability. Despite the great success, most existing open-source LLMs (e.g., LLaMA-2) are still far away from satisfactory for solving mathematical problem due to the complex reasoning procedures. To bridge this gap, we propose MetaMath, a fine-tuned language model that specializes in mathematical reasoning. Specifically, we start by bootstrapping mathematical questions by rewriting the question from multiple perspectives without extra knowledge, which results in a new dataset called MetaMathQA. Then we fine-tune the LLaMA-2 models on MetaMathQA. Experimental results on two popular benchmarks (i.e., GSM8K and MATH) for mathematical reasoning demonstrate that MetaMath outperforms a suite of open-source LLMs by a significant margin. Our MetaMath-7B model achieves 66.4% on GSM8K and 19.4% on MATH, exceeding the state-of-the-art models of the same size by 11.5% and 8.7%. Particularly, MetaMath-70B achieves an accuracy of 82.3% on GSM8K, slightly better than GPT-3.5-Turbo. We release all the MetaMathQA dataset, the MetaMath models with different model sizes and the training code for public use."
                        ],
                        "paper": {
                            "corpus_id": 262084051,
                            "title": "MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2112584251",
                                    "name": "L. Yu"
                                },
                                {
                                    "authorId": "2152123946",
                                    "name": "Weisen Jiang"
                                },
                                {
                                    "authorId": "152751416",
                                    "name": "Han Shi"
                                },
                                {
                                    "authorId": "2193887687",
                                    "name": "Jincheng Yu"
                                },
                                {
                                    "authorId": "2239065052",
                                    "name": "Zhengying Liu"
                                },
                                {
                                    "authorId": "2153638098",
                                    "name": "Yu Zhang"
                                },
                                {
                                    "authorId": "2243335442",
                                    "name": "James T. Kwok"
                                },
                                {
                                    "authorId": "121544682",
                                    "name": "Zheng Li"
                                },
                                {
                                    "authorId": "145689461",
                                    "name": "Adrian Weller"
                                },
                                {
                                    "authorId": "2243412679",
                                    "name": "Weiyang Liu"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 395
                        },
                        "score": 0
                    },
                    {
                        "id": "(Yue et al., 2024)",
                        "snippets": [
                            "Instruction tuning improves the reasoning abilities of large language models (LLMs), with data quality and scalability being the crucial factors. Most instruction tuning data come from human crowd-sourcing or GPT-4 distillation. We propose a paradigm to efficiently harvest 10 million naturally existing instruction data from the pre-training web corpus to enhance LLM reasoning. Our approach involves (1) recalling relevant documents, (2) extracting instruction-response pairs, and (3) refining the extracted pairs using open-source LLMs. Fine-tuning base LLMs on this dataset, we build MAmmoTH2 models, which significantly boost performance on reasoning benchmarks. Notably, MAmmoTH2-7B's (Mistral) performance increases from 11% to 36.7% on MATH and from 36% to 68.4% on GSM8K without training on any in-domain data. Further training MAmmoTH2 on public instruction tuning datasets yields MAmmoTH2-Plus, achieving state-of-the-art performance on several reasoning and chatbot benchmarks. Our work demonstrates how to harvest large-scale, high-quality instruction data without costly human annotation or GPT-4 distillation, providing a new paradigm for building better instruction tuning data."
                        ],
                        "paper": {
                            "corpus_id": 269605607,
                            "title": "MAmmoTH2: Scaling Instructions from the Web",
                            "authors": [
                                {
                                    "authorId": "2284988933",
                                    "name": "Xiang Yue"
                                },
                                {
                                    "authorId": "2300091474",
                                    "name": "Tuney Zheng"
                                },
                                {
                                    "authorId": "2143853895",
                                    "name": "Ge Zhang"
                                },
                                {
                                    "authorId": "2249847177",
                                    "name": "Wenhu Chen"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 101
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhou et al., 2024)",
                        "snippets": [
                            "Mathematical reasoning is an important capability of large language models~(LLMs) for real-world applications. To enhance this capability, existing work either collects large-scale math-related texts for pre-training, or relies on stronger LLMs (\\eg GPT-4) to synthesize massive math problems. Both types of work generally lead to large costs in training or synthesis. To reduce the cost, based on open-source available texts, we propose an efficient way that trains a small LLM for math problem synthesis, to efficiently generate sufficient high-quality pre-training data. To achieve it, we create a dataset using GPT-4 to distill its data synthesis capability into the small LLM. Concretely, we craft a set of prompts based on human education stages to guide GPT-4, to synthesize problems covering diverse math knowledge and difficulty levels. Besides, we adopt the gradient-based influence estimation method to select the most valuable math-related texts. The both are fed into GPT-4 for creating the knowledge distillation dataset to train the small LLM. We leverage it to synthesize 6 million math problems for pre-training our JiuZhang3.0 model, which only needs to invoke GPT-4 API 9.3k times and pre-train on 4.6B data. Experimental results have shown that JiuZhang3.0 achieves state-of-the-art performance on several mathematical reasoning datasets, under both natural language reasoning and tool manipulation settings. Our code and data will be publicly released in \\url{https://github.com/RUCAIBox/JiuZhang3.0}."
                        ],
                        "paper": {
                            "corpus_id": 269981934,
                            "title": "JiuZhang3.0: Efficiently Improving Mathematical Reasoning by Training Small Data Synthesis Models",
                            "authors": [
                                {
                                    "authorId": "2265383494",
                                    "name": "Kun Zhou"
                                },
                                {
                                    "authorId": "2107926615",
                                    "name": "Beichen Zhang"
                                },
                                {
                                    "authorId": "2302813110",
                                    "name": "Jiapeng Wang"
                                },
                                {
                                    "authorId": "2111335050",
                                    "name": "Zhipeng Chen"
                                },
                                {
                                    "authorId": "2257376413",
                                    "name": "Wayne Xin Zhao"
                                },
                                {
                                    "authorId": "2165225571",
                                    "name": "Jing Sha"
                                },
                                {
                                    "authorId": "2125340023",
                                    "name": "Zhichao Sheng"
                                },
                                {
                                    "authorId": "2302793582",
                                    "name": "Shijin Wang"
                                },
                                {
                                    "authorId": "2274218622",
                                    "name": "Ji-Rong Wen"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 34
                        },
                        "score": 0
                    },
                    {
                        "id": "(Li et al., 2023)",
                        "snippets": [
                            "We present a scalable method to build a high quality instruction following language model by automatically labelling human-written text with corresponding instructions. Our approach, named instruction backtranslation, starts with a language model finetuned on a small amount of seed data, and a given web corpus. The seed model is used to construct training examples by generating instruction prompts for web documents (self-augmentation), and then selecting high quality examples from among these candidates (self-curation). This data is then used to finetune a stronger model. Finetuning LLaMa on two iterations of our approach yields a model that outperforms all other LLaMa-based models on the Alpaca leaderboard not relying on distillation data, demonstrating highly effective self-alignment."
                        ],
                        "paper": {
                            "corpus_id": 260866107,
                            "title": "Self-Alignment with Instruction Backtranslation",
                            "authors": [
                                {
                                    "authorId": "2116235416",
                                    "name": "Xian Li"
                                },
                                {
                                    "authorId": "2114104308",
                                    "name": "Ping Yu"
                                },
                                {
                                    "authorId": "2384711",
                                    "name": "Chunting Zhou"
                                },
                                {
                                    "authorId": "32246932",
                                    "name": "Timo Schick"
                                },
                                {
                                    "authorId": "1982950",
                                    "name": "Luke Zettlemoyer"
                                },
                                {
                                    "authorId": "39455775",
                                    "name": "Omer Levy"
                                },
                                {
                                    "authorId": "145183709",
                                    "name": "J. Weston"
                                },
                                {
                                    "authorId": "35084211",
                                    "name": "M. Lewis"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 134
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Large Language Models (LLMs) for Synthetic Data",
                "tldr": "Large language models have emerged as powerful tools for synthetic data generation, enabling the creation of high-quality training datasets for complex reasoning tasks through techniques like hierarchical prompting, reverse task generation, and iterative bootstrapping. These approaches have been successfully applied to diverse domains including mathematical reasoning, code generation, and commonsense reasoning, often outperforming traditional data collection methods. (15 sources)",
                "text": "\nThe use of large language models (LLMs) as synthetic data generators has revolutionized the way researchers approach data scarcity challenges. One key insight driving this innovation is that LLMs can generate useful data even for tasks they cannot directly solve. Josifoski et al. demonstrated this by prompting LLMs to perform tasks in the reverse direction\u2014generating plausible input text for target output structures\u2014enabling the production of large-scale, high-quality data for complex tasks such as closed information extraction <Paper corpusId=\"257378179\" paperTitle=\"(Josifoski et al., 2023)\" isShortName></Paper>.\n\nSynthetic data generation with LLMs has been applied across numerous domains. For commonsense reasoning, researchers have shown that general language models can author commonsense knowledge graphs, effectively distilling knowledge symbolically as text <Paper corpusId=\"265456227\" paperTitle=\"(Jain et al., 2023)\" isShortName></Paper> <Paper corpusId=\"238857304\" paperTitle=\"(West et al., 2021)\" isShortName></Paper>. In the realm of mathematical reasoning, researchers have curated diverse corpora of mathematics problems with chain-of-thought or program-of-thought annotations <Paper corpusId=\"265456227\" paperTitle=\"(Jain et al., 2023)\" isShortName></Paper>. For code generation, approaches include synthesizing programming textbooks and exercises, and generating programming puzzles with verified solutions <Paper corpusId=\"265456227\" paperTitle=\"(Jain et al., 2023)\" isShortName></Paper> <Paper corpusId=\"251197051\" paperTitle=\"(Haluptzok et al., 2022)\" isShortName></Paper>.\n\nSeveral innovative techniques have emerged to enhance synthetic data quality:\n\n1. **Hierarchical prompting** methods promote dataset diversity by conditioning generation on topics, target audiences, and specific attributes. This approach has been used to create children's stories, textbooks, and code exercises <Paper corpusId=\"272593422\" paperTitle=\"(Yang et al., 2024)\" isShortName></Paper>.\n\n2. **Textbook-style generation** combines conceptual explanations with practical examples. Intellecta, for instance, creates dual-composed content that simulates complex thought processes alongside textbook-style elucidations <Paper corpusId=\"269294098\" paperTitle=\"(Ajmal et al., 2024)\" isShortName></Paper>.\n\n3. **Rephrasing approaches** enhance data quality by transforming existing texts. While simple rephrasing primarily affects grammatical styles <Paper corpusId=\"273403575\" paperTitle=\"(Akter et al., 2024)\" isShortName></Paper> <Paper corpusId=\"267312030\" paperTitle=\"(Maini et al., 2024)\" isShortName></Paper>, more advanced techniques like MIND add semantic variations and structured complexity through conversational synthetic data generation <Paper corpusId=\"273403575\" paperTitle=\"(Akter et al., 2024)\" isShortName></Paper>.\n\n4. **Self-training methods** utilize synthetic data for iterative fine-tuning, significantly improving reasoning capabilities <Paper corpusId=\"272770433\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>. The Self-Taught Reasoner (STaR) technique bootstraps complex reasoning abilities by generating rationales, correcting errors, and fine-tuning on successful examples <Paper corpusId=\"265456227\" paperTitle=\"(Jain et al., 2023)\" isShortName></Paper> <Paper corpusId=\"247762790\" paperTitle=\"(Zelikman et al., 2022)\" isShortName></Paper>.\n\n5. **Symbolic Chain-of-Thought Distillation (SCoTD)** enables smaller models (125M-1.3B parameters) to benefit from chain-of-thought prompting by training them on rationalizations from larger teacher models <Paper corpusId=\"265456227\" paperTitle=\"(Jain et al., 2023)\" isShortName></Paper> <Paper corpusId=\"259251773\" paperTitle=\"(Li et al._1, 2023)\" isShortName></Paper>.\n\nAn emerging technique called \"bootstrapping\" involves an iterative process where a model generates synthetic data, an external verifier filters out low-quality samples, and the high-quality subset is used for further fine-tuning <Paper corpusId=\"276079713\" paperTitle=\"(Yang et al., 2025)\" isShortName></Paper>. This process raises important questions about optimal budget allocation across iterations.\n\nThe ability to verify the correctness of generated content provides a significant advantage in certain domains. In code generation, for instance, execution-based verification allows for the creation of large-scale, correct-by-construction synthetic datasets that have helped open-source code models approach the competency of proprietary alternatives <Paper corpusId=\"277104955\" paperTitle=\"(Nadas et al., 2025)\" isShortName></Paper>.\n\nWhile significant progress has been made in text and code domains, similar advances have been slower in other areas such as audio <Paper corpusId=\"273185896\" paperTitle=\"(Raghavan et al., 2024)\" isShortName></Paper>. However, the success of text-to-image generation models suggests promising directions for multimodal synthetic data generation <Paper corpusId=\"273185896\" paperTitle=\"(Raghavan et al., 2024)\" isShortName></Paper> <Paper corpusId=\"232035663\" paperTitle=\"(Ramesh et al., 2021)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Josifoski et al., 2023)",
                        "snippets": [
                            "Large language models (LLMs) have great potential for synthetic data generation. This work shows that useful data can be synthetically generated even for tasks that cannot be solved directly by LLMs: for problems with structured outputs, it is possible to prompt an LLM to perform the task in the reverse direction, by generating plausible input text for a target output structure. Leveraging this asymmetry in task difficulty makes it possible to produce large-scale, high-quality data for complex tasks. We demonstrate the effectiveness of this approach on closed information extraction, where collecting ground-truth data is challenging, and no satisfactory dataset exists to date."
                        ],
                        "paper": {
                            "corpus_id": 257378179,
                            "title": "Exploiting Asymmetry for Synthetic Training Data Generation: SynthIE and the Case of Information Extraction",
                            "authors": [
                                {
                                    "authorId": "65826567",
                                    "name": "Martin Josifoski"
                                },
                                {
                                    "authorId": "2122910580",
                                    "name": "Marija Sakota"
                                },
                                {
                                    "authorId": "35512303",
                                    "name": "Maxime Peyrard"
                                },
                                {
                                    "authorId": "145387102",
                                    "name": "Robert West"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 85
                        },
                        "score": 0.92431640625
                    },
                    {
                        "id": "(Jain et al., 2023)",
                        "snippets": [
                            "Synthetic data for LLMS. Recent works have explored using synthetic datasets for generalpurpose or task-specific finetuning of LLMS. These approaches work by generating synthetic datasets from a strong LLM (like GPT-3.5-TURBO or GPT-4) using a set of existing tasks (Taori et al., 2023;Chiang et al., 2023) or generating new tasks using self-instruct (Wang et al., 2022) or evol-instruct (Xu et al., 2023) approaches. This has been also applied for task-specific finetuningin common-sense reasoning (West et al., 2021)), text-summarization (Sclar et al., 2022), mathematical reasoning (Luo et al., 2023a;Yue et al., 2023), tool use (Patil et al., 2023), coding (Luo et al., 2023b), and general-purpose reasoning (Li et al., 2023); (Zelikman et al., 2022). \n\nMore specifically, Yue et al. (2023) curates diverse corpus of mathematics problems with chain-ofthought or program-of-thought (Chen et al., 2022b) annotations for mathematical reasoning analogous to our plans. Gunasekar et al. (2023) proposed pre-training models on programming \"textbooks\" generated synthetically from GPT-3.5-TURBO. (Haluptzok et al., 2022) similarly generates programming puzzles and corresponding solutions from language models."
                        ],
                        "paper": {
                            "corpus_id": 265456227,
                            "title": "LLM-Assisted Code Cleaning For Training Accurate Code Generators",
                            "authors": [
                                {
                                    "authorId": "1646458461",
                                    "name": "Naman Jain"
                                },
                                {
                                    "authorId": "1993655237",
                                    "name": "Tianjun Zhang"
                                },
                                {
                                    "authorId": "2537924",
                                    "name": "Wei-Lin Chiang"
                                },
                                {
                                    "authorId": "2254681613",
                                    "name": "Joseph Gonzalez"
                                },
                                {
                                    "authorId": "2268398121",
                                    "name": "Koushik Sen"
                                },
                                {
                                    "authorId": "2055174324",
                                    "name": "Ion Stoica"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 32
                        },
                        "score": 0.95654296875
                    },
                    {
                        "id": "(West et al., 2021)",
                        "snippets": [
                            "The common practice for training commonsense models has gone from\u2013human\u2013to\u2013corpus\u2013to\u2013machine: humans author commonsense knowledge graphs in order to train commonsense models. In this work, we investigate an alternative, from\u2013machine\u2013to\u2013corpus\u2013to\u2013machine: general language models author these commonsense knowledge graphs to train commonsense models. Our study leads to a new framework, Symbolic Knowledge Distillation. As with prior art in Knowledge Distillation (Hinton et al. 2015), our approach uses larger models to teach smaller models. A key difference is that we distill knowledge symbolically\u2013as text\u2013in addition to the neural model. We distill only one aspect\u2013the commonsense of a general language model teacher, allowing the student to be a different type, a commonsense model. Altogether, we show that careful prompt engineering and a separately trained critic model allow us to selectively distill high-quality causal commonsense from GPT-3, a general language model. Empirical results demonstrate that, for the first time, a human-authored commonsense knowledge graph is surpassed by our automatically distilled variant in all three criteria: quantity, quality, and diversity. In addition, it results in a neural commonsense model that surpasses the teacher model\u2019s commonsense capabilities despite its 100x smaller size. We apply this to the ATOMIC resource, and will share our new symbolic knowledge graph and commonsense models."
                        ],
                        "paper": {
                            "corpus_id": 238857304,
                            "title": "Symbolic Knowledge Distillation: from General Language Models to Commonsense Models",
                            "authors": [
                                {
                                    "authorId": "119659229",
                                    "name": "Peter West"
                                },
                                {
                                    "authorId": "51177106",
                                    "name": "Chandrasekhar Bhagavatula"
                                },
                                {
                                    "authorId": "2689239",
                                    "name": "Jack Hessel"
                                },
                                {
                                    "authorId": "2012510",
                                    "name": "Jena D. Hwang"
                                },
                                {
                                    "authorId": "2112504145",
                                    "name": "Liwei Jiang"
                                },
                                {
                                    "authorId": "39227408",
                                    "name": "Ronan Le Bras"
                                },
                                {
                                    "authorId": "50085131",
                                    "name": "Ximing Lu"
                                },
                                {
                                    "authorId": "2129663",
                                    "name": "S. Welleck"
                                },
                                {
                                    "authorId": "1699545",
                                    "name": "Yejin Choi"
                                }
                            ],
                            "year": 2021,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 333
                        },
                        "score": 0
                    },
                    {
                        "id": "(Haluptzok et al., 2022)",
                        "snippets": [
                            "Recent Language Models (LMs) achieve breakthrough performance in code generation when trained on human-authored problems, even solving some competitive-programming problems. Self-play has proven useful in games such as Go, and thus it is natural to ask whether LMs can generate their own instructive programming problems to improve their performance. We show that it is possible for an LM to synthesize programming problems and solutions, which are filtered for correctness by a Python interpreter. The LM's performance is then seen to improve when it is fine-tuned on its own synthetic problems and verified solutions; thus the model 'improves itself' using the Python interpreter. Problems are specified formally as programming puzzles [Schuster et al., 2021], a code-based problem format where solutions can easily be verified for correctness by execution. In experiments on publicly-available LMs, test accuracy more than doubles. This work demonstrates the potential for code LMs, with an interpreter, to generate instructive problems and improve their own performance."
                        ],
                        "paper": {
                            "corpus_id": 251197051,
                            "title": "Language Models Can Teach Themselves to Program Better",
                            "authors": [
                                {
                                    "authorId": "2214247",
                                    "name": "Patrick M. Haluptzok"
                                },
                                {
                                    "authorId": "2058735196",
                                    "name": "Matthew Bowers"
                                },
                                {
                                    "authorId": "2186481",
                                    "name": "A. Kalai"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 82
                        },
                        "score": 0
                    },
                    {
                        "id": "(Yang et al., 2024)",
                        "snippets": [
                            "Recent approaches synthesize pretraining data using hierarchical prompting methods to promote dataset diversity. Eldan & Li (2023) prompt API-based LLMs to generate children's stories containing sampled keywords, and demonstrate that even small language models trained on their dataset can generate fluent text. Gunasekar et al. (2023) synthesize a diverse dataset of textbooks and code exercises by conditioning on topic, target audience, and function names, and later release strong LLMs pretrained on synthetic data in follow-up work (Li et al., 2023b;Abdin et al., 2023;2024). However, their datasets and prompts are not publicly available. (Maini et al., 2024) prompt an LM to rephrase documents for pretraining, improving training efficiency. Different from all above works, our focus is teaching a pretrained LLM the knowledge of a small corpus. Mecklenburg et al. (2024) consider task-specific finetuning and propose a fact-based synthetic QA generation procedure, but do not show improvement on generic instruction following tasks beyond simple QA. We instead focus on teaching a model generally useful knowledge about a small corpus, untied to a particular downstream task. Ovadia et al. (2024) continually pretrain Llama 2-based language models on synthetic paraphrases of Wikipedia articles, but do not observe consistent performance improvements."
                        ],
                        "paper": {
                            "corpus_id": 272593422,
                            "title": "Synthetic continued pretraining",
                            "authors": [
                                {
                                    "authorId": "2283514777",
                                    "name": "Zitong Yang"
                                },
                                {
                                    "authorId": "2294359410",
                                    "name": "Neil Band"
                                },
                                {
                                    "authorId": "2320941550",
                                    "name": "Shuangping Li"
                                },
                                {
                                    "authorId": "2283307289",
                                    "name": "Emmanuel J. Candes"
                                },
                                {
                                    "authorId": "2294362683",
                                    "name": "Tatsunori Hashimoto"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 16
                        },
                        "score": 0.912109375
                    },
                    {
                        "id": "(Ajmal et al., 2024)",
                        "snippets": [
                            "Integral to Intellecta is the incorporation of advanced synthetic generation techniques, which fabricate a dual-composed content: one that simulates complex thought processes and another that yields textbookstyle elucidations laden with core concepts and pragmatic examples.We took available open source instruction data as the seed data to generate the synthetic data.From the instructions, we first create a textbook style Gunasekar et al. [2023] text explaining the concept required for answering the instruction.The second step is to enrich the response from the seed data with thought process how the model arrive at that result.Combining these 2 will provide a textbook style concept explanation followed by exercises and the thought process to resolve the same.This correlate to the same a student learn a chapter starting with textbook followed by exercises and thought process behind it."
                        ],
                        "paper": {
                            "corpus_id": 269294098,
                            "title": "Intellecta Cognitiva: A Comprehensive Dataset for Advancing Academic Knowledge and Machine Reasoning",
                            "authors": [
                                {
                                    "authorId": "2297772548",
                                    "name": "PS Ajmal"
                                },
                                {
                                    "authorId": "2297772495",
                                    "name": "PS Ditto"
                                },
                                {
                                    "authorId": "2297773046",
                                    "name": "VG Jithin"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.92626953125
                    },
                    {
                        "id": "(Akter et al., 2024)",
                        "snippets": [
                            "To generate high-quality data at scale, current synthetic data generation approach explores rephrasing texts using LLMs in varied syntax while preserving the core content (Maini et al., 2024). However, their proposed approach limits up-sampling high-quality data in a way that does not go beyond grammatical styles or surface form transformations-leading little to no improvement when it comes to performance across complex and logical reasoning tasks. We hypothesize that simple rephrasing does not leverage the full potential of the synthetic data to improve the mathematical and complex multi-hop reasoning ability of LLM. Therefore, we propose, MIND, a conversational synthetic data generation approach that adds semantic variations and structured complexity to the raw text which is required to improve complex reasoning ability of the LLMs. In addition, multi-turn conversations can break down the original context step-by-step while each step addresses a sub-context at a time by often injecting complimentary reasoning or explanations. This resonates with how human solves a complex problem using consecutive chain-of-thought reasoning."
                        ],
                        "paper": {
                            "corpus_id": 273403575,
                            "title": "MIND: Math Informed syNthetic Dialogues for Pretraining LLMs",
                            "authors": [
                                {
                                    "authorId": "1900302322",
                                    "name": "Syeda Nahida Akter"
                                },
                                {
                                    "authorId": "9358910",
                                    "name": "Shrimai Prabhumoye"
                                },
                                {
                                    "authorId": "51028721",
                                    "name": "John Kamalu"
                                },
                                {
                                    "authorId": "145031342",
                                    "name": "S. Satheesh"
                                },
                                {
                                    "authorId": "2279547669",
                                    "name": "Eric Nyberg"
                                },
                                {
                                    "authorId": "66870756",
                                    "name": "M. Patwary"
                                },
                                {
                                    "authorId": "1911755",
                                    "name": "M. Shoeybi"
                                },
                                {
                                    "authorId": "2264406909",
                                    "name": "Bryan Catanzaro"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 2
                        },
                        "score": 0.9326171875
                    },
                    {
                        "id": "(Maini et al., 2024)",
                        "snippets": [
                            "Large language models are trained on massive scrapes of the web, which are often unstructured, noisy, and poorly phrased. Current scaling laws show that learning from such data requires an abundance of both compute and data, which grows with the size of the model being trained. This is infeasible both because of the large compute costs and duration associated with pre-training, and the impending scarcity of high-quality data on the web. In this work, we propose Web Rephrase Augmented Pre-training ($\\textbf{WRAP}$) that uses an off-the-shelf instruction-tuned model prompted to paraphrase documents on the web in specific styles such as\"like Wikipedia\"or in\"question-answer format\"to jointly pre-train LLMs on real and synthetic rephrases. First, we show that using WRAP on the C4 dataset, which is naturally noisy, speeds up pre-training by $\\sim3x$. At the same pre-training compute budget, it improves perplexity by more than 10% on average across different subsets of the Pile, and improves zero-shot question answer accuracy across 13 tasks by more than 2%. Second, we investigate the impact of the re-phrasing style on the performance of the model, offering insights into how the composition of the training data can impact the performance of LLMs in OOD settings. Our gains are attributed to the fact that re-phrased synthetic data has higher utility than just real data because it (i) incorporates style diversity that closely reflects downstream evaluation style, and (ii) has higher 'quality' than web-scraped data."
                        ],
                        "paper": {
                            "corpus_id": 267312030,
                            "title": "Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling",
                            "authors": [
                                {
                                    "authorId": "153742303",
                                    "name": "Pratyush Maini"
                                },
                                {
                                    "authorId": "31855650",
                                    "name": "Skyler Seto"
                                },
                                {
                                    "authorId": "37374479",
                                    "name": "Richard He Bai"
                                },
                                {
                                    "authorId": "2529182",
                                    "name": "David Grangier"
                                },
                                {
                                    "authorId": "2254045488",
                                    "name": "Yizhe Zhang"
                                },
                                {
                                    "authorId": "3111912",
                                    "name": "N. Jaitly"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 66
                        },
                        "score": 0
                    },
                    {
                        "id": "(Liu et al., 2024)",
                        "snippets": [
                            "Synthetic Data Generation for Model Fine-tuning. The performance of large language models (LLMs) hinge on the quality and diversity of their training data. To address the limitations of manual datasets, synthetic data generation methods (Wang et al., 2022;Xu et al., 2023) have been developed to automatically create instruction-following examples from LLMs, reducing reliance on human annotations. Various techniques enhance data quality: Wang et al. (2022) generates multiple reasoning traces and selects the most frequent output to improve robustness, while other approaches (Lightman et al., 2023;Zhang et al., 2024b) assess response quality based on these traces. Self-training methods utilize synthetic data for iterative fine-tuning, boosting reasoning capabilities (Singh et al., 2023;Feng et al., 2023). These advancements show how synthetic data can effectively scale and optimize models through iterative feedback."
                        ],
                        "paper": {
                            "corpus_id": 272770433,
                            "title": "CraftRTL: High-quality Synthetic Data Generation for Verilog Code Models with Correct-by-Construction Non-Textual Representations and Targeted Code Repair",
                            "authors": [
                                {
                                    "authorId": "2264029479",
                                    "name": "Mingjie Liu"
                                },
                                {
                                    "authorId": "3328096",
                                    "name": "Yun-Da Tsai"
                                },
                                {
                                    "authorId": "2322144595",
                                    "name": "Wenfei Zhou"
                                },
                                {
                                    "authorId": "2268825069",
                                    "name": "Haoxing Ren"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 17
                        },
                        "score": 0.95361328125
                    },
                    {
                        "id": "(Zelikman et al., 2022)",
                        "snippets": [
                            "Generating step-by-step\"chain-of-thought\"rationales improves language model performance on complex reasoning tasks like mathematics or commonsense question-answering. However, inducing language model rationale generation currently requires either constructing massive rationale datasets or sacrificing accuracy by using only few-shot inference. We propose a technique to iteratively leverage a small number of rationale examples and a large dataset without rationales, to bootstrap the ability to perform successively more complex reasoning. This technique, the\"Self-Taught Reasoner\"(STaR), relies on a simple loop: generate rationales to answer many questions, prompted with a few rationale examples; if the generated answers are wrong, try again to generate a rationale given the correct answer; fine-tune on all the rationales that ultimately yielded correct answers; repeat. We show that STaR significantly improves performance on multiple datasets compared to a model fine-tuned to directly predict final answers, and performs comparably to fine-tuning a 30$\\times$ larger state-of-the-art language model on CommensenseQA. Thus, STaR lets a model improve itself by learning from its own generated reasoning."
                        ],
                        "paper": {
                            "corpus_id": 247762790,
                            "title": "STaR: Bootstrapping Reasoning With Reasoning",
                            "authors": [
                                {
                                    "authorId": "49456763",
                                    "name": "E. Zelikman"
                                },
                                {
                                    "authorId": "3374063",
                                    "name": "Yuhuai Wu"
                                },
                                {
                                    "authorId": "144002017",
                                    "name": "Noah D. Goodman"
                                }
                            ],
                            "year": 2022,
                            "venue": "",
                            "n_citations": 510
                        },
                        "score": 0
                    },
                    {
                        "id": "(Li et al._1, 2023)",
                        "snippets": [
                            "Chain-of-thought prompting (e.g., \"Let\u2019s think step-by-ste\") primes large language models to verbalize rationalization for their predictions. While chain-of-thought can lead to dramatic performance gains, benefits appear to emerge only for sufficiently large models (beyond 50B parameters). We show that orders-of-magnitude smaller models (125M\u20141.3B parameters) can still benefit from chain-of-thought prompting. To achieve this, we introduce Symbolic Chain-of-Thought Distillation (SCoTD), a method to train a smaller student model on rationalizations sampled from a significantly larger teacher model. Experiments across several commonsense benchmarks show that: 1) SCoTD enhances the performance of the student model in both supervised and few-shot settings, and especially for challenge sets; 2) sampling many reasoning chains per instance from the teacher is paramount; and 3) after distillation, student chain-of-thoughts are judged by humans as comparable to the teacher, despite orders of magnitude fewer parameters. We test several hypotheses regarding what properties of chain-of-thought samples are important, e.g., diversity vs. teacher likelihood vs. open-endedness. We release our corpus of chain-of-thought samples and code."
                        ],
                        "paper": {
                            "corpus_id": 259251773,
                            "title": "Symbolic Chain-of-Thought Distillation: Small Models Can Also \u201cThink\u201d Step-by-Step",
                            "authors": [
                                {
                                    "authorId": "32562635",
                                    "name": "Liunian Harold Li"
                                },
                                {
                                    "authorId": "2689239",
                                    "name": "Jack Hessel"
                                },
                                {
                                    "authorId": "2111510510",
                                    "name": "Youngjae Yu"
                                },
                                {
                                    "authorId": "2115257544",
                                    "name": "Xiang Ren"
                                },
                                {
                                    "authorId": "2782886",
                                    "name": "Kai-Wei Chang"
                                },
                                {
                                    "authorId": "1699545",
                                    "name": "Yejin Choi"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 143
                        },
                        "score": 0
                    },
                    {
                        "id": "(Yang et al., 2025)",
                        "snippets": [
                            "Modern foundation models often undergo iterative ``bootstrapping'' in their post-training phase: a model generates synthetic data, an external verifier filters out low-quality samples, and the high-quality subset is used for further fine-tuning. Over multiple iterations, the model's performance improves--raising a crucial question: how should the total budget on generation and training be allocated across iterations to maximize final performance?"
                        ],
                        "paper": {
                            "corpus_id": 276079713,
                            "title": "Spend Wisely: Maximizing Post-Training Gains in Iterative Synthetic Data Boostrapping",
                            "authors": [
                                {
                                    "authorId": "2284644270",
                                    "name": "Pu Yang"
                                },
                                {
                                    "authorId": "2343647731",
                                    "name": "Yunzhen Feng"
                                },
                                {
                                    "authorId": "2343512547",
                                    "name": "Ziyuan Chen"
                                },
                                {
                                    "authorId": "2343592601",
                                    "name": "Yuhang Wu"
                                },
                                {
                                    "authorId": "2344832360",
                                    "name": "Zhuoyuan Li"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.93017578125
                    },
                    {
                        "id": "(Nadas et al., 2025)",
                        "snippets": [
                            "In the text domain, LLM-generated data has proven especially useful for low-resource scenarios, delivering substantial improvements in tasks like classification and QA when human-labeled data is limited [33]. Techniques such as promptbased augmentation (zero-shot, few-shot, etc.), retrieval augmentation for grounding facts [3], and iterative refinement have pushed synthetic text data closer in effectiveness to real data.\n\nIn the code domain, LLMs have unlocked new possibilities by generating code snippets, programming instructions, and even whole problem solutions, facilitating better training of code models. The ability to verify code correctness via execution [37] provides a strong advantage, allowing the curation of large-scale, correct-by-construction synthetic code datasets (e.g., Code Alpaca, WizardCoder) that have propelled open-source code models to approach the competency of their proprietary counterparts."
                        ],
                        "paper": {
                            "corpus_id": 277104955,
                            "title": "Synthetic Data Generation Using Large Language Models: Advances in Text and Code",
                            "authors": [
                                {
                                    "authorId": "2350754914",
                                    "name": "Mihai Nadas"
                                },
                                {
                                    "authorId": "2306585020",
                                    "name": "Laura Dio\u015fan"
                                },
                                {
                                    "authorId": "2350756350",
                                    "name": "Andreea Tomescu"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.97607421875
                    },
                    {
                        "id": "(Raghavan et al., 2024)",
                        "snippets": [
                            "Recent works have shown significant progress in synthetic data generation in the image space in (Liang et al., 2023) (Ramesh et al., 2021)) (Sun et al. 2024) (Bae et al., 2022) etc. However, similar progress has not been seen in the audio space. Zeroshot text-to-image generation approaches have expanded the scope of synthetic data applications by enabling the generation of novel image data from unseen textual prompts, highlighting the model's ability to generalize from limited examples (Ramesh et al., 2021). Digiface-1m (Bae et al., 2022)) dataset exemplifies the practical applications of these technologies, providing a robust framework for testing and improving face recognition algorithms through access to one million digital face images. (Ye et al. 2022) outlines a method to leverage LLMs to create synthetic datasets produced entirely using pre-trained language models (PLMs) without human interference while emphasizing the efficiency and flexibility of using synthetic datasets to train task-specific models. (Yu et al., 2023)) explores generation of training data that not only focuses on diversity, but also addresses inherent biases within the data generated by LLMs. It highlights the critical role of using diversely attributed prompts that enhance quality and utility of synthetic datasets improving model performance across NLP tasks. (Patel, Raffel, and Callison-Burch 2024) presents a tool designed to streamline synthetic data generation using LLMs providing a platform to generate, train, and share data sets and models."
                        ],
                        "paper": {
                            "corpus_id": 273185896,
                            "title": "Did You Hear That? Introducing AADG: A Framework for Generating Benchmark Data in Audio Anomaly Detection",
                            "authors": [
                                {
                                    "authorId": "2093263031",
                                    "name": "Ksheeraja Raghavan"
                                },
                                {
                                    "authorId": "2199184565",
                                    "name": "Samiran Gode"
                                },
                                {
                                    "authorId": "2257395734",
                                    "name": "Ankit Shah"
                                },
                                {
                                    "authorId": "2324783136",
                                    "name": "Surabhi Raghavan"
                                },
                                {
                                    "authorId": "2322439428",
                                    "name": "Wolfram Burgard"
                                },
                                {
                                    "authorId": "2288787089",
                                    "name": "Bhiksha Raj"
                                },
                                {
                                    "authorId": "2289375291",
                                    "name": "Rita Singh"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.931640625
                    },
                    {
                        "id": "(Ramesh et al., 2021)",
                        "snippets": [
                            "Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion."
                        ],
                        "paper": {
                            "corpus_id": 232035663,
                            "title": "Zero-Shot Text-to-Image Generation",
                            "authors": [
                                {
                                    "authorId": "1992922591",
                                    "name": "A. Ramesh"
                                },
                                {
                                    "authorId": "2068123790",
                                    "name": "Mikhail Pavlov"
                                },
                                {
                                    "authorId": "40087786",
                                    "name": "Gabriel Goh"
                                },
                                {
                                    "authorId": "145565184",
                                    "name": "Scott Gray"
                                },
                                {
                                    "authorId": "153387869",
                                    "name": "Chelsea Voss"
                                },
                                {
                                    "authorId": "38909097",
                                    "name": "Alec Radford"
                                },
                                {
                                    "authorId": "2108828435",
                                    "name": "Mark Chen"
                                },
                                {
                                    "authorId": "1701686",
                                    "name": "I. Sutskever"
                                }
                            ],
                            "year": 2021,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 5000
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Graph-based Approaches for Reasoning Tasks",
                "tldr": "Graph-based approaches leverage structured knowledge representations to generate high-quality synthetic data for complex reasoning tasks. These methods use graph structures to encode relationships between concepts, enabling scalable data generation with controlled reasoning paths and improved semantic coherence. (4 sources)",
                "text": "\nGraph structures provide an effective framework for generating synthetic reasoning data by explicitly representing relationships between concepts. This approach has gained traction for addressing challenges in commonsense reasoning, where manual annotation of datasets is expensive and not scalable. Maharana et al. introduced GraDA, a graph-generative data augmentation framework that synthesizes factual samples from knowledge graphs for commonsense reasoning datasets. Their system trains a graph-to-text model for generating questions from graph entities and relations, then employs a GAN-based approach to create plausible distractors for these synthetic questions <Paper corpusId=\"250390686\" paperTitle=\"(Maharana et al., 2022)\" isShortName></Paper>.\n\nRecent research has explored how graph-based synthetic data can enhance large language models' capabilities on complex reasoning tasks that involve long reasoning chains. Zhou et al. investigated this approach by designing a random walk sampling algorithm on graphs combined with a novel prompting strategy. Their method first extracts a reasoning chain from the graph structure and then derives the answer, providing a more controlled way to generate reasoning data <Paper corpusId=\"272753147\" paperTitle=\"(Zhou et al._1, 2024)\" isShortName></Paper>.\n\nBuilding on these ideas, Wang et al. proposed the Graph-based Synthetic Data Pipeline (GSDP), which addresses the dual challenges of quality and scalability. Their framework extracts knowledge points from seed data and constructs a knowledge point relationships graph to map interconnections between concepts. By exploring these implicit relationships, GSDP achieved remarkable data expansion (255 times the original dataset size) while maintaining high quality. Notably, their approach produced synthetic data comparable to that generated by GPT-4-0613 but at approximately 100 times lower cost <Paper corpusId=\"274656618\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\nThe integration of neuro-symbolic (NeSy) architectures with graph-based approaches represents another promising direction. Savazzi et al. explored using scene graphs\u2014structured representations where nodes represent objects and edges represent relations\u2014as a symbolic framework for conditioning image generation. Their work suggests that scene graphs encode valuable background knowledge that can guide the synthetic data generation process, ensuring adherence to structural and semantic constraints particularly for tasks like Scene Graph Generation from images <Paper corpusId=\"277244110\" paperTitle=\"(Savazzi et al., 2025)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Maharana et al., 2022)",
                        "snippets": [
                            "Recent advances in commonsense reasoning have been fueled by the availability of large-scale human annotated datasets. Manual annotation of such datasets, many of which are based on existing knowledge bases, is expensive and not scalable. Moreover, it is challenging to build augmentation data for commonsense reasoning because the synthetic questions need to adhere to real-world scenarios. Hence, we present GraDA, a graph-generative data augmentation framework to synthesize factual data samples from knowledge graphs for commonsense reasoning datasets. First, we train a graph-to-text model for conditional generation of questions from graph entities and relations. Then, we train a generator with GAN loss to generate distractors for synthetic questions."
                        ],
                        "paper": {
                            "corpus_id": 250390686,
                            "title": "GraDA: Graph Generative Data Augmentation for Commonsense Reasoning",
                            "authors": [
                                {
                                    "authorId": "8785371",
                                    "name": "Adyasha Maharana"
                                },
                                {
                                    "authorId": "143977268",
                                    "name": "Mohit Bansal"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Conference on Computational Linguistics",
                            "n_citations": 96
                        },
                        "score": 0.91064453125
                    },
                    {
                        "id": "(Zhou et al._1, 2024)",
                        "snippets": [
                            "Despite recent advances in training and prompting strategies for Large Language Models (LLMs), these models continue to face challenges with complex logical reasoning tasks that involve long reasoning chains. In this work, we explore the potential and limitations of using graph-based synthetic reasoning data as training signals to enhance LLMs' reasoning capabilities.\n\nRecently, several works (Xu et al., 2024;Abdin et al., 2024;Anil et al., 2023) have demonstrated the efficacy of boosting the LLMs' reasoning capacity via fine-tuning on synthetic data generated by stronger LLMs. However, how to make such synthetic data generation effective and controllable for specific applications remains an open question.\n\nMotivated by the fact that natural language reasoning tasks can be represented as structured data with finite nodes and edges (Jin et al., 2024), and inspired by existing works on constructing reasoning benchmarks (Fatemi et al., 2024;Kazemi et al., 2023;Agrawal et al., 2024), we propose to leverage synthetic graph-based data for task-specific posttraining adaptation to improve the correctness of the generated reasoning questions and labels.\n\nIn this paper, we carefully design a random walk sampling algorithm on graphs and introduce a new prompting strategy that first extracts a reasoning chain and then derives the answer."
                        ],
                        "paper": {
                            "corpus_id": 272753147,
                            "title": "Enhancing Logical Reasoning in Large Language Models through Graph-based Synthetic Data",
                            "authors": [
                                {
                                    "authorId": "2297828274",
                                    "name": "Jiaming Zhou"
                                },
                                {
                                    "authorId": "2321867953",
                                    "name": "Abbas Ghaddar"
                                },
                                {
                                    "authorId": "2321875142",
                                    "name": "Ge Zhang"
                                },
                                {
                                    "authorId": "1892081076",
                                    "name": "Liheng Ma"
                                },
                                {
                                    "authorId": "2288403553",
                                    "name": "Yaochen Hu"
                                },
                                {
                                    "authorId": "38939190",
                                    "name": "Soumyasundar Pal"
                                },
                                {
                                    "authorId": "2287938359",
                                    "name": "Mark Coates"
                                },
                                {
                                    "authorId": "2321911323",
                                    "name": "Bin Wang"
                                },
                                {
                                    "authorId": "2275529643",
                                    "name": "Yingxue Zhang"
                                },
                                {
                                    "authorId": "2307072418",
                                    "name": "Jianye Hao"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 4
                        },
                        "score": 0.951171875
                    },
                    {
                        "id": "(Wang et al., 2024)",
                        "snippets": [
                            "Synthesizing high-quality reasoning data for continual training has been proven to be effective in enhancing the performance of Large Language Models (LLMs). However, previous synthetic approaches struggle to easily scale up data and incur high costs in the pursuit of high quality. In this paper, we propose the Graph-based Synthetic Data Pipeline (GSDP), an economical and scalable framework for high-quality reasoning data synthesis. Inspired by knowledge graphs, we extracted knowledge points from seed data and constructed a knowledge point relationships graph to explore their interconnections. By exploring the implicit relationships among knowledge, our method achieves $\\times$255 data expansion. Furthermore, GSDP led by open-source models, achieves synthesis quality comparable to GPT-4-0613 while maintaining $\\times$100 lower costs."
                        ],
                        "paper": {
                            "corpus_id": 274656618,
                            "title": "A Graph-Based Synthetic Data Pipeline for Scaling High-Quality Reasoning Instructions",
                            "authors": [
                                {
                                    "authorId": "2335083125",
                                    "name": "Jiankang Wang"
                                },
                                {
                                    "authorId": "2187441024",
                                    "name": "Jianjun Xu"
                                },
                                {
                                    "authorId": "2334884780",
                                    "name": "Xiaorui Wang"
                                },
                                {
                                    "authorId": "2143529316",
                                    "name": "Yuxin Wang"
                                },
                                {
                                    "authorId": "2067080682",
                                    "name": "Mengting Xing"
                                },
                                {
                                    "authorId": "13808397",
                                    "name": "Shancheng Fang"
                                },
                                {
                                    "authorId": "2334916976",
                                    "name": "Zhineng Chen"
                                },
                                {
                                    "authorId": "2323899630",
                                    "name": "Hongtao Xie"
                                },
                                {
                                    "authorId": "2290965957",
                                    "name": "Yongdong Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.95361328125
                    },
                    {
                        "id": "(Savazzi et al., 2025)",
                        "snippets": [
                            "While NeSy architectures have demonstrated enhanced generalization in reasoning tasks, their potential for improving synthetic data generation remains largely unexplored. Scene graphs-structured representations where nodes represent objects and edges represent relations-offer a promising symbolic framework for conditioning image generation. This work explores the integration of such NeSy approaches into dataset generation to improve performance on complex tasks, particularly Scene Graph Generation (SGG) from images. The hypothesis posits that scene graphs encode useful background knowledge that can guide the generation process, ensuring synthetic data adhere to structural and semantic constraints."
                        ],
                        "paper": {
                            "corpus_id": 277244110,
                            "title": "Neuro-Symbolic Scene Graph Conditioning for Synthetic Image Dataset Generation",
                            "authors": [
                                {
                                    "authorId": "2351599231",
                                    "name": "Giacomo Savazzi"
                                },
                                {
                                    "authorId": "2221012084",
                                    "name": "Eugenio Lomurno"
                                },
                                {
                                    "authorId": "2172115265",
                                    "name": "Cristian Sbrolli"
                                },
                                {
                                    "authorId": "2607532",
                                    "name": "Agnese Chiatti"
                                },
                                {
                                    "authorId": "2304551810",
                                    "name": "Matteo Matteucci"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.984375
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Mathematical Reasoning Data Synthesis",
                "tldr": "Mathematical reasoning data synthesis leverages techniques like key-point extraction, concept graphs, and instruction evolution to create high-quality synthetic datasets that enhance LLMs' problem-solving capabilities. Recent advances focus on scalability, diversity, and verification methods to generate millions of math problems across difficulty levels, resulting in significant performance improvements for fine-tuned models. (12 sources)",
                "text": "\nMathematical reasoning has emerged as a particularly fruitful domain for synthetic data generation due to its inherent structure and verifiability. Researchers have developed specialized approaches to address the scarcity of high-quality, reasoning-focused mathematical training data. A notable advancement in this area is the Key-Point-Driven Data Synthesis (KPDDS) framework, which leverages key points and exemplar practices from authentic data sources to synthesize novel question-answer pairs. This approach has led to the creation of KPMath, an extensive dataset comprising over 800,000 mathematical reasoning problems <Paper corpusId=\"268247488\" paperTitle=\"(Huang et al., 2024)\" isShortName></Paper>.\n\nBuilding on similar principles, the MathScale method creates high-quality mathematical reasoning data by first extracting topics and knowledge points from seed math questions and then constructing a concept graph, which is subsequently used to generate new math problems. This approach has demonstrated impressive scalability, resulting in MathScaleQA, a dataset containing two million math question-answer pairs <Paper corpusId=\"268247902\" paperTitle=\"(Tang et al., 2024)\" isShortName></Paper>.\n\nAnother innovative approach is ScaleQuest, which introduces a two-stage question-tuning process comprising Question Fine-Tuning (QFT) and Question Preference Optimization (QPO). This method enables the generation of large-scale mathematical reasoning datasets using lightweight 7B-scale models, producing one million problem-solution pairs without relying on powerful proprietary models or seed data <Paper corpusId=\"273549775\" paperTitle=\"(Ding et al., 2024)\" isShortName></Paper>.\n\nData augmentation based on existing annotated training sets has been widely explored, with methods ranging from self-evolving instructions <Paper corpusId=\"270213007\" paperTitle=\"(Zeng et al., 2024)\" isShortName></Paper> and question paraphrasing <Paper corpusId=\"262084051\" paperTitle=\"(Yu et al., 2023)\" isShortName></Paper> to solution augmentation. However, these approaches are often limited by the available training data, constraining synthesis diversity <Paper corpusId=\"278171321\" paperTitle=\"(Wang et al., 2025)\" isShortName></Paper>.\n\nMathematical reasoning synthetic data has been particularly effective for fine-tuning smaller language models. The WizardMath approach, which applies Reinforcement Learning from Evol-Instruct Feedback (RLEIF) to the domain of mathematics, has demonstrated remarkable capabilities. The resulting WizardMath-Mistral 7B model surpasses top-tier open-source LLMs by a substantial margin with higher data efficiency <Paper corpusId=\"261030818\" paperTitle=\"(Luo et al., 2023)\" isShortName></Paper>. Similarly, MetaMath, which bootstraps mathematical questions by rewriting them from multiple perspectives, has achieved significant improvements on benchmarks like GSM8K and MATH <Paper corpusId=\"262084051\" paperTitle=\"(Yu et al., 2023)\" isShortName></Paper>.\n\nRecent approaches have also explored creating targeted, instance-specific data rather than broad-coverage datasets. This direction enables data generation agents to produce specific data based on a particular model's weaknesses <Paper corpusId=\"273228651\" paperTitle=\"(Khan et al., 2024)\" isShortName></Paper>. Unlike methods that modify problems in surface form, these approaches first infer a latent structure and then create problems by sampling from this structure <Paper corpusId=\"277780529\" paperTitle=\"(Khan et al., 2025)\" isShortName></Paper>.\n\nIn the realm of advanced mathematical reasoning, AlphaGeometry represents a milestone in theorem proving for Euclidean plane geometry. This neuro-symbolic system uses a neural language model trained from scratch on large-scale synthetic data to guide a symbolic deduction engine through challenging problems. On a test set of 30 olympiad-level problems, AlphaGeometry solved 25, approaching the performance of an average International Mathematical Olympiad gold medalist <Paper corpusId=\"267032902\" paperTitle=\"(Trinh et al., 2024)\" isShortName></Paper>.\n\nAnother noteworthy approach is Additional Logic Training (ALT), which enhances LLMs' reasoning capabilities through program-generated logical reasoning samples. Based on principles from symbolic logic theory, researchers constructed a synthetic corpus called Formal Logic Deduction Diverse (FLD\u00d72), comprising numerous samples of multi-step deduction with diverse reasoning rules. This training has shown substantial improvements in reasoning capabilities for models like LLaMA-3.1-70B, with gains of up to 30 points on logical reasoning benchmarks and up to 10 points on math benchmarks <Paper corpusId=\"274141027\" paperTitle=\"(Morishita et al., 2024)\" isShortName></Paper>.\n\nThe field has also benefited from approaches like EntailmentBank, which generates explanations in the form of entailment trees\u2014showing a line of reasoning from known facts, through intermediate conclusions, to the answer. This structured approach to synthetic data generation has shown promising results, particularly when relevant sentences are included in the input <Paper corpusId=\"233297051\" paperTitle=\"(Dalvi et al., 2021)\" isShortName></Paper>.\n\nThese diverse approaches to mathematical reasoning data synthesis highlight the field's rapid evolution, with increasingly sophisticated methods enabling the creation of high-quality, scalable datasets that significantly improve model performance on complex reasoning tasks.",
                "citations": [
                    {
                        "id": "(Huang et al., 2024)",
                        "snippets": [
                            "Large language models have shown great potential in complex reasoning tasks, yet their performance is often hampered by the scarcity of high-quality and reasoning-focused training datasets. Addressing this challenge, we propose Key-PointDriven Data Synthesis (KPDDS), a novel data synthesis framework that synthesizes question-answer pairs by leveraging key points and exemplar practices from authentic data sources. KPDDS ensures the generation of novel questions with rigorous quality control and substantial scalability. As a result, we present KPMath, an extensive synthetic dataset tailored for mathematical reasoning, comprising over 800K questionanswer pairs."
                        ],
                        "paper": {
                            "corpus_id": 268247488,
                            "title": "Key-Point-Driven Data Synthesis with its Enhancement on Mathematical Reasoning",
                            "authors": [
                                {
                                    "authorId": "2261394934",
                                    "name": "Yiming Huang"
                                },
                                {
                                    "authorId": "49544272",
                                    "name": "Xiao Liu"
                                },
                                {
                                    "authorId": "2254121650",
                                    "name": "Yeyun Gong"
                                },
                                {
                                    "authorId": "1797090",
                                    "name": "Zhibin Gou"
                                },
                                {
                                    "authorId": "2237948786",
                                    "name": "Yelong Shen"
                                },
                                {
                                    "authorId": "2269471632",
                                    "name": "Nan Duan"
                                },
                                {
                                    "authorId": "2249538838",
                                    "name": "Weizhu Chen"
                                }
                            ],
                            "year": 2024,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 43
                        },
                        "score": 0.90380859375
                    },
                    {
                        "id": "(Tang et al., 2024)",
                        "snippets": [
                            "Large language models (LLMs) have demonstrated remarkable capabilities in problem-solving. However, their proficiency in solving mathematical problems remains inadequate. We propose MathScale, a simple and scalable method to create high-quality mathematical reasoning data using frontier LLMs (e.g., {\\tt GPT-3.5}). Inspired by the cognitive mechanism in human mathematical learning, it first extracts topics and knowledge points from seed math questions and then build a concept graph, which is subsequently used to generate new math questions. MathScale exhibits effective scalability along the size axis of the math dataset that we generate. As a result, we create a mathematical reasoning dataset (MathScaleQA) containing two million math question-answer pairs. To evaluate mathematical reasoning abilities of LLMs comprehensively, we construct {\\sc MwpBench}, a benchmark of Math Word Problems, which is a collection of ten datasets (including GSM8K and MATH) covering K-12, college, and competition level math problems. We apply MathScaleQA to fine-tune open-source LLMs (e.g., LLaMA-2 and Mistral), resulting in significantly improved capabilities in mathematical reasoning. Evaluated on {\\sc MwpBench}, MathScale-7B achieves state-of-the-art performance across all datasets, surpassing its best peers of equivalent size by 42.9\\% in micro average accuracy and 43.7\\% in macro average accuracy, respectively."
                        ],
                        "paper": {
                            "corpus_id": 268247902,
                            "title": "MathScale: Scaling Instruction Tuning for Mathematical Reasoning",
                            "authors": [
                                {
                                    "authorId": "2284834012",
                                    "name": "Zhengyang Tang"
                                },
                                {
                                    "authorId": "2284863493",
                                    "name": "Xingxing Zhang"
                                },
                                {
                                    "authorId": "2284827140",
                                    "name": "Benyou Wang"
                                },
                                {
                                    "authorId": "2290016262",
                                    "name": "Furu Wei"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 82
                        },
                        "score": 0
                    },
                    {
                        "id": "(Ding et al., 2024)",
                        "snippets": [
                            "In this paper, we propose ScaleQuest, a novel, scalable, and cost-effective data synthesis method that enables the generation of large-scale mathematical reasoning datasets using lightweight 7B-scale models. ScaleQuest introduces a two-stage question-tuning process comprising Question Fine-Tuning (QFT) and Question Preference Optimization (QPO) to unlock the question generation capabilities of problem-solving models. By generating diverse questions from scratch -- without relying on powerful proprietary models or seed data -- we produce a dataset of 1 million problem-solution pairs."
                        ],
                        "paper": {
                            "corpus_id": 273549775,
                            "title": "Unleashing LLM Reasoning Capability via Scalable Question Synthesis from Scratch",
                            "authors": [
                                {
                                    "authorId": "2243263723",
                                    "name": "Yuyang Ding"
                                },
                                {
                                    "authorId": "2327658780",
                                    "name": "Xinyu Shi"
                                },
                                {
                                    "authorId": "48083523",
                                    "name": "Xiaobo Liang"
                                },
                                {
                                    "authorId": "2257093356",
                                    "name": "Juntao Li"
                                },
                                {
                                    "authorId": "2363558591",
                                    "name": "Zhaopeng Tu"
                                },
                                {
                                    "authorId": "2243490219",
                                    "name": "Qiaoming Zhu"
                                },
                                {
                                    "authorId": "2258690233",
                                    "name": "Min Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "",
                            "n_citations": 8
                        },
                        "score": 0.966796875
                    },
                    {
                        "id": "(Zeng et al., 2024)",
                        "snippets": [
                            "Fine-tuning large pre-trained language models with Evol-Instruct has achieved encouraging results across a wide range of tasks. However, designing effective evolving methods for instruction evolution requires substantial human expertise. This paper proposes Auto Evol-Instruct, an end-to-end framework that evolves instruction datasets using large language models without any human effort. The framework automatically analyzes and summarizes suitable evolutionary strategies for the given instruction data and iteratively improves the evolving method based on issues exposed during the instruction evolution process. Our extensive experiments demonstrate that the best method optimized by Auto Evol-Instruct outperforms human-designed methods on various benchmarks, including MT-Bench, AlpacaEval, GSM8K, and HumanEval."
                        ],
                        "paper": {
                            "corpus_id": 270213007,
                            "title": "Automatic Instruction Evolving for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2069640466",
                                    "name": "Weihao Zeng"
                                },
                                {
                                    "authorId": "2304522809",
                                    "name": "Can Xu"
                                },
                                {
                                    "authorId": "2290476555",
                                    "name": "Yingxiu Zhao"
                                },
                                {
                                    "authorId": "2304469600",
                                    "name": "Jian-Guang Lou"
                                },
                                {
                                    "authorId": "2264439430",
                                    "name": "Weizhu Chen"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 10
                        },
                        "score": 0
                    },
                    {
                        "id": "(Yu et al., 2023)",
                        "snippets": [
                            "Large language models (LLMs) have pushed the limits of natural language understanding and exhibited excellent problem-solving ability. Despite the great success, most existing open-source LLMs (e.g., LLaMA-2) are still far away from satisfactory for solving mathematical problem due to the complex reasoning procedures. To bridge this gap, we propose MetaMath, a fine-tuned language model that specializes in mathematical reasoning. Specifically, we start by bootstrapping mathematical questions by rewriting the question from multiple perspectives without extra knowledge, which results in a new dataset called MetaMathQA. Then we fine-tune the LLaMA-2 models on MetaMathQA. Experimental results on two popular benchmarks (i.e., GSM8K and MATH) for mathematical reasoning demonstrate that MetaMath outperforms a suite of open-source LLMs by a significant margin. Our MetaMath-7B model achieves 66.4% on GSM8K and 19.4% on MATH, exceeding the state-of-the-art models of the same size by 11.5% and 8.7%. Particularly, MetaMath-70B achieves an accuracy of 82.3% on GSM8K, slightly better than GPT-3.5-Turbo. We release all the MetaMathQA dataset, the MetaMath models with different model sizes and the training code for public use."
                        ],
                        "paper": {
                            "corpus_id": 262084051,
                            "title": "MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2112584251",
                                    "name": "L. Yu"
                                },
                                {
                                    "authorId": "2152123946",
                                    "name": "Weisen Jiang"
                                },
                                {
                                    "authorId": "152751416",
                                    "name": "Han Shi"
                                },
                                {
                                    "authorId": "2193887687",
                                    "name": "Jincheng Yu"
                                },
                                {
                                    "authorId": "2239065052",
                                    "name": "Zhengying Liu"
                                },
                                {
                                    "authorId": "2153638098",
                                    "name": "Yu Zhang"
                                },
                                {
                                    "authorId": "2243335442",
                                    "name": "James T. Kwok"
                                },
                                {
                                    "authorId": "121544682",
                                    "name": "Zheng Li"
                                },
                                {
                                    "authorId": "145689461",
                                    "name": "Adrian Weller"
                                },
                                {
                                    "authorId": "2243412679",
                                    "name": "Weiyang Liu"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 395
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wang et al., 2025)",
                        "snippets": [
                            "To address this scarcity, researchers have explored various synthesizing methods, particularly in the mathematics domain. The mainstream methods involve data augmentation based on existing annotated training sets, such as GSM8K (Cobbe et al., 2021) and MATH (Huang et al., 2024), ranging from self-evolving instructions (Xu et al., 2024;(Zeng et al., 2024) and question paraphrasing (Yu et al., 2024), to solution augmentation (Lu et al., 2024). However, these methods are limited by the available training data, constraining the synthesis diversity (Li et al., 2024c(Li et al., , 2023)). To enhance diversity, recent approaches enable LLMs to generate a large scale of questions from various mathematicsrelated sources, including web pages (Yue et al., 2024) and knowledge points (Tang et al., 2024) from web corpora or textbooks."
                        ],
                        "paper": {
                            "corpus_id": 278171321,
                            "title": "RV-Syn: Rational and Verifiable Mathematical Reasoning Data Synthesis based on Structured Function Library",
                            "authors": [
                                {
                                    "authorId": "2302813110",
                                    "name": "Jiapeng Wang"
                                },
                                {
                                    "authorId": "2118240359",
                                    "name": "Jinhao Jiang"
                                },
                                {
                                    "authorId": "2358106383",
                                    "name": "Zhiqiang Zhang"
                                },
                                {
                                    "authorId": "2279870653",
                                    "name": "Jun Zhou"
                                },
                                {
                                    "authorId": "2294811281",
                                    "name": "Wayne Xin Zhao"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.93798828125
                    },
                    {
                        "id": "(Luo et al., 2023)",
                        "snippets": [
                            "Large language models (LLMs), such as GPT-4, have shown remarkable performance in natural language processing (NLP) tasks, including challenging mathematical reasoning. However, most existing open-source models are only pre-trained on large-scale internet data and without math-related optimization. In this paper, we present WizardMath, which enhances the mathematical CoT reasoning abilities of LLMs without using external python tools, by applying our proposed Reinforcement Learning from Evol-Instruct Feedback (RLEIF) method to the domain of math. Through extensive experiments on two mathematical reasoning benchmarks, namely GSM8k and MATH, we reveal the extraordinary capabilities of our model. Remarkably, WizardMath-Mistral 7B surpasses top-tier open-source LLMs by a substantial margin with higher data efficiency. Furthermore, WizardMath 70B even outperforms GPT-3.5-Turbo, Claude 2, Gemini Pro and GPT-4-early-version. Additionally, our preliminary exploration highlights the pivotal role of instruction evolution and process supervision in achieving exceptional math performance. For more details refer to https://github.com/nlpxucan/WizardLM"
                        ],
                        "paper": {
                            "corpus_id": 261030818,
                            "title": "WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct",
                            "authors": [
                                {
                                    "authorId": "2131127",
                                    "name": "Haipeng Luo"
                                },
                                {
                                    "authorId": "2112549330",
                                    "name": "Qingfeng Sun"
                                },
                                {
                                    "authorId": "46747953",
                                    "name": "Can Xu"
                                },
                                {
                                    "authorId": "2007757792",
                                    "name": "Pu Zhao"
                                },
                                {
                                    "authorId": "4648762",
                                    "name": "Jian-Guang Lou"
                                },
                                {
                                    "authorId": "8801869",
                                    "name": "Chongyang Tao"
                                },
                                {
                                    "authorId": "2442662",
                                    "name": "Xiubo Geng"
                                },
                                {
                                    "authorId": "2793487",
                                    "name": "Qingwei Lin"
                                },
                                {
                                    "authorId": "2232739272",
                                    "name": "Shifeng Chen"
                                },
                                {
                                    "authorId": "2109581369",
                                    "name": "Dongmei Zhang"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 467
                        },
                        "score": 0
                    },
                    {
                        "id": "(Khan et al., 2024)",
                        "snippets": [
                            "The process of creating training data to teach models is currently driven by humans, who manually analyze model weaknesses and plan how to create data that improves a student model. Approaches using LLMs as annotators reduce human effort, but still require humans to interpret feedback from evaluations and control the LLM to produce data the student needs. Automating this labor-intensive process by creating autonomous data generation agents - or teachers - is desirable, but requires environments that can simulate the feedback-driven, iterative, closed loop of data creation. To enable rapid, scalable testing for such agents and their modules, we introduce DataEnvGym, a testbed of teacher environments for data generation agents. DataEnvGym frames data generation as a sequential decision-making task, involving an agent consisting of a data generation policy (which generates a plan for creating training data) and a data generation engine (which transforms the plan into data), inside an environment that provides student feedback. The agent's goal is to improve student performance. Students are iteratively trained and evaluated on generated data, and their feedback (in the form of errors or weak skills) is reported to the agent after each iteration. DataEnvGym includes multiple teacher environment instantiations across 3 levels of structure in the state representation and action space. More structured environments are based on inferred skills and offer more interpretability and curriculum control. We support 4 domains (math, code, VQA, and tool-use) and test multiple students and teachers. Example agents in our teaching environments can iteratively improve students across tasks and settings. Moreover, we show that environments teach different skill levels and test variants of key modules, pointing to future work in improving data generation agents, engines, and feedback mechanisms."
                        ],
                        "paper": {
                            "corpus_id": 273228651,
                            "title": "DataEnvGym: Data Generation Agents in Teacher Environments with Student Feedback",
                            "authors": [
                                {
                                    "authorId": "2324992429",
                                    "name": "Zaid Khan"
                                },
                                {
                                    "authorId": "2281825070",
                                    "name": "Elias Stengel-Eskin"
                                },
                                {
                                    "authorId": "2706729",
                                    "name": "Jaemin Cho"
                                },
                                {
                                    "authorId": "2281826842",
                                    "name": "Mohit Bansal"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 3
                        },
                        "score": 0
                    },
                    {
                        "id": "(Khan et al., 2025)",
                        "snippets": [
                            "Past work has generally approached improving models on reasoning tasks like math by generating large amounts of broad-coverage training data. This trend builds on work in generating instruction-tuning data (Wang et al., 2022), where model-generated instructions have been used to teach models to follow prompts. Luo et al. (2023) introduced generation method based on Evol-Instruct (Xu et al., 2023), which augmented a seed dataset of math problems by generating easier and harder problems. Related lines of work have sought to expand datasets by augmenting existing math datasets (Yu et al., 2023), adding multiple reasoning strategies (Yang et al., 2023), covering challenging competition problems (Li et al., 2024), or curating responses (Liu et al., 2024). The data generated in these settings differs from our data in a number of respects: first, it is generally broad-coverage, focusing on large-scale diverse data, as opposed to targeted, instancespecific data. This direction was also explored by (Khan et al., 2024), who define data generation agents that can generate specific data based on a particular model's weaknesses, covering math and several other domains. Finally, past work that has augmented a seed dataset (e.g., (Yu et al., 2023); (Yang et al., 2023)) has done so by modifying problems in the surface form, whereas our method first infers a latent structure and then creates problems by sampling from the structure."
                        ],
                        "paper": {
                            "corpus_id": 277780529,
                            "title": "Executable Functional Abstractions: Inferring Generative Programs for Advanced Math Problems",
                            "authors": [
                                {
                                    "authorId": "2324992429",
                                    "name": "Zaid Khan"
                                },
                                {
                                    "authorId": "2281825070",
                                    "name": "Elias Stengel-Eskin"
                                },
                                {
                                    "authorId": "1677896557",
                                    "name": "Archiki Prasad"
                                },
                                {
                                    "authorId": "2706729",
                                    "name": "Jaemin Cho"
                                },
                                {
                                    "authorId": "2281826842",
                                    "name": "Mohit Bansal"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.9248046875
                    },
                    {
                        "id": "(Trinh et al., 2024)",
                        "snippets": [
                            "Proving mathematical theorems at the olympiad level represents a notable milestone in human-level automated reasoning1\u20134, owing to their reputed difficulty among the world\u2019s best talents in pre-university mathematics. Current machine-learning approaches, however, are not applicable to most mathematical domains owing to the high cost of translating human proofs into machine-verifiable format. The problem is even worse for geometry because of its unique translation challenges1,5, resulting in severe scarcity of training data. We propose AlphaGeometry, a theorem prover for Euclidean plane geometry that sidesteps the need for human demonstrations by synthesizing millions of theorems and proofs across different levels of complexity. AlphaGeometry is a neuro-symbolic system that uses a neural language model, trained from scratch on our large-scale synthetic data, to guide a symbolic deduction engine through infinite branching points in challenging problems. On a test set of 30 latest olympiad-level problems, AlphaGeometry solves 25, outperforming the previous best method that only solves ten problems and approaching the performance of an average International Mathematical Olympiad (IMO) gold medallist. Notably, AlphaGeometry produces human-readable proofs, solves all geometry problems in the IMO 2000 and 2015 under human expert evaluation and discovers a generalized version of a translated IMO theorem in 2004."
                        ],
                        "paper": {
                            "corpus_id": 267032902,
                            "title": "Solving olympiad geometry without human demonstrations",
                            "authors": [
                                {
                                    "authorId": "40895509",
                                    "name": "Trieu H. Trinh"
                                },
                                {
                                    "authorId": "2279769334",
                                    "name": "Yuhuai Wu"
                                },
                                {
                                    "authorId": "2279797756",
                                    "name": "Quoc V. Le"
                                },
                                {
                                    "authorId": "2279870483",
                                    "name": "He He"
                                },
                                {
                                    "authorId": "2279807154",
                                    "name": "Thang Luong"
                                }
                            ],
                            "year": 2024,
                            "venue": "The Naturalist",
                            "n_citations": 390
                        },
                        "score": 0
                    },
                    {
                        "id": "(Morishita et al., 2024)",
                        "snippets": [
                            "Large language models (LLMs) are capable of solving a wide range of tasks, yet they have struggled with reasoning. To address this, we propose $\\textbf{Additional Logic Training (ALT)}$, which aims to enhance LLMs' reasoning capabilities by program-generated logical reasoning samples. We first establish principles for designing high-quality samples by integrating symbolic logic theory and previous empirical insights. Then, based on these principles, we construct a synthetic corpus named $\\textbf{Formal Logic Deduction Diverse}$ ($\\textbf{FLD}$$_{\\times 2}$), comprising numerous samples of multi-step deduction with unknown facts, diverse reasoning rules, diverse linguistic expressions, and challenging distractors. Finally, we empirically show that ALT on FLD$_{\\times2}$ substantially enhances the reasoning capabilities of state-of-the-art LLMs, including LLaMA-3.1-70B. Improvements include gains of up to 30 points on logical reasoning benchmarks, up to 10 points on math and coding benchmarks, and 5 points on the benchmark suite BBH."
                        ],
                        "paper": {
                            "corpus_id": 274141027,
                            "title": "Enhancing Reasoning Capabilities of LLMs via Principled Synthetic Logic Corpus",
                            "authors": [
                                {
                                    "authorId": "1379579811",
                                    "name": "Terufumi Morishita"
                                },
                                {
                                    "authorId": "29347584",
                                    "name": "Gaku Morio"
                                },
                                {
                                    "authorId": "145412147",
                                    "name": "Atsuki Yamaguchi"
                                },
                                {
                                    "authorId": "2106369",
                                    "name": "Yasuhiro Sogawa"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 15
                        },
                        "score": 0
                    },
                    {
                        "id": "(Dalvi et al., 2021)",
                        "snippets": [
                            "Our goal, in the context of open-domain textual question-answering (QA), is to explain answers by showing the line of reasoning from what is known to the answer, rather than simply showing a fragment of textual evidence (a \"rationale\"). If this could be done, new opportunities for understanding and debugging the system\u2019s reasoning become possible. Our approach is to generate explanations in the form of entailment trees, namely a tree of multipremise entailment steps from facts that are known, through intermediate conclusions, to the hypothesis of interest (namely the question + answer). To train a model with this skill, we created ENTAILMENTBANK, the first dataset to contain multistep entailment trees. Given a hypothesis (question + answer), we define three increasingly difficult explanation tasks: generate a valid entailment tree given (a) all relevant sentences (b) all relevant and some irrelevant sentences, or (c) a corpus. We show that a strong language model can partially solve these tasks, in particular when the relevant sentences are included in the input (e.g., 35% of trees for (a) are perfect), and with indications of generalization to other domains. This work is significant as it provides a new type of dataset (multistep entailments) and baselines, offering a new avenue for the community to generate richer, more systematic explanations."
                        ],
                        "paper": {
                            "corpus_id": 233297051,
                            "title": "Explaining Answers with Entailment Trees",
                            "authors": [
                                {
                                    "authorId": "40135250",
                                    "name": "Bhavana Dalvi"
                                },
                                {
                                    "authorId": "144949918",
                                    "name": "Peter Alexander Jansen"
                                },
                                {
                                    "authorId": "3385516",
                                    "name": "Oyvind Tafjord"
                                },
                                {
                                    "authorId": "9746804",
                                    "name": "Zhengnan Xie"
                                },
                                {
                                    "authorId": "2110645880",
                                    "name": "Hannah Smith"
                                },
                                {
                                    "authorId": "2078502192",
                                    "name": "Leighanna Pipatanangkura"
                                },
                                {
                                    "authorId": "48323507",
                                    "name": "Peter Clark"
                                }
                            ],
                            "year": 2021,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 185
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Code Reasoning Data Synthesis",
                "tldr": "Synthetic data generation for code reasoning has evolved from basic instruction-following examples to sophisticated approaches that leverage open-source code repositories and execution-based verification. These techniques have significantly improved smaller language models' code generation capabilities, in some cases enabling them to match or exceed proprietary models on standard benchmarks. (7 sources)",
                "text": "\nCode reasoning represents another fertile domain for synthetic data generation, with approaches that leverage the unique advantage of code's executability for verification. Early efforts like Code Alpaca used self-instruct methodology to generate 20,000 instruction-tuning examples based on seed tasks <Paper corpusId=\"272753174\" paperTitle=\"(Jiang et al., 2024)\" isShortName></Paper>. This approach has since evolved significantly with frameworks like WizardCoder, which adapted the Evol-Instruct method to the code domain, creating increasingly complex instruction-tuning data <Paper corpusId=\"272753174\" paperTitle=\"(Jiang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"259164815\" paperTitle=\"(Luo et al._1, 2023)\" isShortName></Paper>.\n\nA particularly influential innovation in code data synthesis is OSS-Instruct, implemented in the Magicoder framework. This approach addresses the inherent bias in LLM-generated synthetic data by leveraging open-source code snippets as references. By generating 75,000 diverse instruction-solution pairs, Magicoder produced models that substantially outperformed state-of-the-art code models of similar or even larger sizes. Notably, the MagicoderS-CL-7B model, built on CodeLlama, achieved results surpassing ChatGPT on the HumanEval+ benchmark <Paper corpusId=\"272988100\" paperTitle=\"(Chan et al., 2024)\" isShortName></Paper> <Paper corpusId=\"270358041\" paperTitle=\"(Wei et al., 2023)\" isShortName></Paper>.\n\nThe ability to verify the correctness of generated code provides a crucial advantage for synthetic data generation in this domain. By running test cases or directly executing the code, researchers can filter out incorrect solutions and create high-quality datasets <Paper corpusId=\"272988100\" paperTitle=\"(Chan et al., 2024)\" isShortName></Paper>. This verification capability has helped narrow the performance gap between smaller open-source models and larger proprietary alternatives.\n\nRecent frameworks have expanded beyond traditional code generation to address multiple code-related tasks. WaveCoder, for example, focuses on generating diverse, high-quality instruction data for multi-task scenarios, resulting in the CodeSeaXDataset with nearly 20,000 instruction instances spanning four code-related tasks <Paper corpusId=\"277502040\" paperTitle=\"(Ahmad et al., 2025)\" isShortName></Paper> <Paper corpusId=\"270258158\" paperTitle=\"(Yu et al._1, 2023)\" isShortName></Paper>.\n\nA novel approach called SoftSRV takes synthetic data generation for code reasoning a step further by using a data-driven loss minimization strategy. Instead of relying on human-engineered prompt templates, which can be labor-intensive and domain-specific, SoftSRV steers a frozen large language model to generate synthetic sequences similar to those from a target distribution. This provides a more systematic and generalizable framework for generating task-specific synthetic data <Paper corpusId=\"273507197\" paperTitle=\"(DeSalvo et al., 2024)\" isShortName></Paper>.\n\nThe rapid advancement in code reasoning data synthesis techniques has led to impressive results, with recent works generating diverse instruction-solution pairs that, when used for fine-tuning base models, achieve top performance on standard benchmarks like HumanEval, MBPP, and BigCodeBench <Paper corpusId=\"277502040\" paperTitle=\"(Ahmad et al., 2025)\" isShortName></Paper>. These approaches have consistently demonstrated that synthetic data can effectively enhance the code reasoning capabilities of language models, enabling smaller, more accessible models to achieve performance comparable to much larger, proprietary alternatives.",
                "citations": [
                    {
                        "id": "(Jiang et al., 2024)",
                        "snippets": [
                            "Synthetic data has played an important role in enhancing LLMs' reasoning capabilities (Dubey et al., 2024), especially in mathematics and coding domains. \n\nFor mathematical reasoning, synthetic data generation includes problem-driven methods such as evol-instruct (Luo et al., 2023a;Zeng et al., 2024), problem restatement (Yu et al., 2023), backtranslation (Lu et al., 2024), and few-shot examples (Dalvi et al., 2021), as well as knowledge-driven methods that rely on knowledge bases (Dalvi et al., 2021) or concept graphs (Tang et al., 2024) to generate new problems by sampling key reasoning points (Huang et al., 2024). \n\nIn terms of code reasoning, from Code Alpaca's (Wang et al., 2023) use of self-instruct to generate 20K instruction data based on seed tasks, to Wiz-ardCoder's (Luo et al., 2023b) use of Code evolinstruct to generate more complex tasks, to Magicoder's use of oss-instruct (Wei et al., 2024) to extract 75K instructions from open source code. Synthesizing data continuously improves the model's code reasoning capabilities."
                        ],
                        "paper": {
                            "corpus_id": 272753174,
                            "title": "LogicPro: Improving Complex Logical Reasoning via Program-Guided Learning",
                            "authors": [
                                {
                                    "authorId": "2319385003",
                                    "name": "Jin Jiang"
                                },
                                {
                                    "authorId": "2284984220",
                                    "name": "Yuchen Yan"
                                },
                                {
                                    "authorId": "2316670312",
                                    "name": "Yang Liu"
                                },
                                {
                                    "authorId": "2321879179",
                                    "name": "Yonggang Jin"
                                },
                                {
                                    "authorId": "2072715089",
                                    "name": "Shuai Peng"
                                },
                                {
                                    "authorId": "2284132141",
                                    "name": "Mengdi Zhang"
                                },
                                {
                                    "authorId": "2317059694",
                                    "name": "Xunliang Cai"
                                },
                                {
                                    "authorId": "2319459835",
                                    "name": "Yixin Cao"
                                },
                                {
                                    "authorId": "2165982472",
                                    "name": "Liangcai Gao"
                                },
                                {
                                    "authorId": "2268046262",
                                    "name": "Zhi Tang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 7
                        },
                        "score": 0.93896484375
                    },
                    {
                        "id": "(Luo et al._1, 2023)",
                        "snippets": [
                            "Code Large Language Models (Code LLMs), such as StarCoder, have demonstrated exceptional performance in code-related tasks. However, most existing models are solely pre-trained on extensive raw code data without instruction fine-tuning. In this paper, we introduce WizardCoder, which empowers Code LLMs with complex instruction fine-tuning, by adapting the Evol-Instruct method to the domain of code. Through comprehensive experiments on four prominent code generation benchmarks, namely HumanEval, HumanEval+, MBPP, and DS-1000, we unveil the exceptional capabilities of our model. It surpasses all other open-source Code LLMs by a substantial margin. Moreover, our model even outperforms the largest closed LLMs, Anthropic's Claude and Google's Bard, on HumanEval and HumanEval+. Our code, model weights, and data are public at https://github.com/nlpxucan/WizardLM"
                        ],
                        "paper": {
                            "corpus_id": 259164815,
                            "title": "WizardCoder: Empowering Code Large Language Models with Evol-Instruct",
                            "authors": [
                                {
                                    "authorId": "23523733",
                                    "name": "Ziyang Luo"
                                },
                                {
                                    "authorId": "46747953",
                                    "name": "Can Xu"
                                },
                                {
                                    "authorId": "2007757792",
                                    "name": "Pu Zhao"
                                },
                                {
                                    "authorId": "2112549330",
                                    "name": "Qingfeng Sun"
                                },
                                {
                                    "authorId": "2442662",
                                    "name": "Xiubo Geng"
                                },
                                {
                                    "authorId": "50105419",
                                    "name": "Wenxiang Hu"
                                },
                                {
                                    "authorId": "8801869",
                                    "name": "Chongyang Tao"
                                },
                                {
                                    "authorId": "2157405974",
                                    "name": "Jing Ma"
                                },
                                {
                                    "authorId": "2793487",
                                    "name": "Qingwei Lin"
                                },
                                {
                                    "authorId": "2086994543",
                                    "name": "Daxin Jiang"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 689
                        },
                        "score": 0
                    },
                    {
                        "id": "(Chan et al., 2024)",
                        "snippets": [
                            "In the domain of mathematical reasoning, highquality instructions are scarce, so many works leverage LLM-generated synthetic data to significantly improved the math reasoning ability of small LLMs (Yu et al., 2023;(Qin et al., 2024)Setlur et al., 2024;(Luo et al., 2023). In code generation, synthetic data from LLMs can be further verified by running test cases or the code directly, which helps close the gap between closed-source LLMs and smaller LLMs (Wei et al., 2023)Yang et al., 2024)."
                        ],
                        "paper": {
                            "corpus_id": 272988100,
                            "title": "Balancing Cost and Effectiveness of Synthetic Data Generation Strategies for LLMs",
                            "authors": [
                                {
                                    "authorId": "2324074843",
                                    "name": "Yung-Chieh Chan"
                                },
                                {
                                    "authorId": "2323513733",
                                    "name": "George Pu"
                                },
                                {
                                    "authorId": "1390689296",
                                    "name": "Apaar Shanker"
                                },
                                {
                                    "authorId": "2268781105",
                                    "name": "Parth Suresh"
                                },
                                {
                                    "authorId": "2323512524",
                                    "name": "Penn Jenks"
                                },
                                {
                                    "authorId": "2323512547",
                                    "name": "John Heyer"
                                },
                                {
                                    "authorId": "2323515242",
                                    "name": "Sam Denton"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 10
                        },
                        "score": 0.90283203125
                    },
                    {
                        "id": "(Wei et al., 2023)",
                        "snippets": [
                            "We introduce Magicoder, a series of fully open-source (code, weights, and data) Large Language Models (LLMs) for code that significantly closes the gap with top code models while having no more than 7B parameters. Magicoder models are trained on 75K synthetic instruction data using OSS-Instruct, a novel approach to enlightening LLMs with open-source code snippets to generate diverse instruction data for code. Our main motivation is to mitigate the inherent bias of the synthetic data generated by LLMs through the wealth of open-source references for the production of more realistic and controllable data. The orthogonality of OSS-Instruct and other data generation methods like Evol-Instruct further enables us to build an enhanced MagicoderS. Both Magicoder and MagicoderS substantially outperform state-of-the-art code models with similar or even larger sizes on a wide range of coding benchmarks. Notably, MagicoderS-CL-7B based on CodeLlama even surpasses the prominent ChatGPT on HumanEval+ (66.5 vs. 65.9 in pass@1 ). Overall, OSS-Instruct opens a new direction for crafting diverse synthetic instruction data for code using abundant open-source references."
                        ],
                        "paper": {
                            "corpus_id": 270358041,
                            "title": "Magicoder: Empowering Code Generation with OSS-Instruct",
                            "authors": [
                                {
                                    "authorId": "2237736409",
                                    "name": "Yuxiang Wei"
                                },
                                {
                                    "authorId": "2269532027",
                                    "name": "Zhe Wang"
                                },
                                {
                                    "authorId": "2296736695",
                                    "name": "Jiawei Liu"
                                },
                                {
                                    "authorId": "2111235780",
                                    "name": "Yifeng Ding"
                                },
                                {
                                    "authorId": "2289125201",
                                    "name": "Lingming Zhang"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 118
                        },
                        "score": 0
                    },
                    {
                        "id": "(Ahmad et al., 2025)",
                        "snippets": [
                            "To overcome this bottleneck, many have successfully leveraged LLMs to generate high-quality synthetic code data (Luo et al., 2023)(Yu et al., 2023). Notably, works like Wei et al. (2024a) and Huang et al. (2025) have generated diverse instruction-solution pairs, subsequently fine-tuning base models to achieve top results in HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021) and BigCodeBench (Zhuo et al., 2025) benchmarks."
                        ],
                        "paper": {
                            "corpus_id": 277502040,
                            "title": "OpenCodeReasoning: Advancing Data Distillation for Competitive Coding",
                            "authors": [
                                {
                                    "authorId": "38123220",
                                    "name": "Wasi Uddin Ahmad"
                                },
                                {
                                    "authorId": "2135333375",
                                    "name": "Sean Narenthiran"
                                },
                                {
                                    "authorId": "9099952",
                                    "name": "Somshubra Majumdar"
                                },
                                {
                                    "authorId": "2186740325",
                                    "name": "Aleksander Ficek"
                                },
                                {
                                    "authorId": "2353320260",
                                    "name": "Siddhartha Jain"
                                },
                                {
                                    "authorId": "1390669078",
                                    "name": "Jocelyn Huang"
                                },
                                {
                                    "authorId": "2353998",
                                    "name": "V. Noroozi"
                                },
                                {
                                    "authorId": "2353271013",
                                    "name": "Boris Ginsburg"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 13
                        },
                        "score": 0.900390625
                    },
                    {
                        "id": "(Yu et al._1, 2023)",
                        "snippets": [
                            "Recent work demonstrates that, after instruction tuning, Code Large Language Models (Code LLMs) can obtain impressive capabilities to address a wide range of code-related tasks. However, current instruction tuning methods for Code LLMs mainly focus on the traditional code generation task, resulting in poor performance in complex multi-task scenarios. In this paper, we concentrate on multiple code-related tasks and present WaveCoder, a series of Code LLMs trained with Widespread And Versatile Enhanced instruction data. To enable the models to tackle complex code-related tasks, we propose a method to stably generate diverse, high-quality instruction data from open source code dataset in multi-task scenarios and obtain CodeSeaXDataset, a dataset comprising 19,915 instruction instances across 4 code-related tasks, which is aimed at improving the generalization ability of Code LLM. Our experiments demonstrate that WaveCoder models significantly outperform other open-source models in terms of the generalization ability across different code-related tasks. Moreover, WaveCoder-Ultra-6.7B presents the state-of-the-art generalization abilities on a wide range of code-related tasks."
                        ],
                        "paper": {
                            "corpus_id": 270258158,
                            "title": "WaveCoder: Widespread And Versatile Enhancement For Code Large Language Models By Instruction Tuning",
                            "authors": [
                                {
                                    "authorId": "2276315776",
                                    "name": "Zhaojian Yu"
                                },
                                {
                                    "authorId": "2276235606",
                                    "name": "Xin Zhang"
                                },
                                {
                                    "authorId": "2276207911",
                                    "name": "Ning Shang"
                                },
                                {
                                    "authorId": "2276731918",
                                    "name": "Yangyu Huang"
                                },
                                {
                                    "authorId": "2276263216",
                                    "name": "Can Xu"
                                },
                                {
                                    "authorId": "2276509780",
                                    "name": "Yishujie Zhao"
                                },
                                {
                                    "authorId": "2276366691",
                                    "name": "Wenxiang Hu"
                                },
                                {
                                    "authorId": "2276205940",
                                    "name": "Qiufeng Yin"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 28
                        },
                        "score": 0
                    },
                    {
                        "id": "(DeSalvo et al., 2024)",
                        "snippets": [
                            "We present a novel framework, SoftSRV, that is used to generate targeted synthetic fine-tuning data for improving task-specific model performance. Given a sample from a target distribution, our proposed framework uses a data-driven loss minimization approach to steer a frozen large language model (LLM) to generate synthetic sequences that are similar to those from the target distribution. SoftSRV provides a practical improvement over common prompt engineering approaches that rely on human-engineered prompt-templates, which can be idiosyncratic, labor-intensive to craft, and may need to be specialized per domain. We empirically evaluate our method against standard baselines guiding a large LLM to generate synthetic data to fine-tune a smaller language model on three different domains (coding, math, reasoning)."
                        ],
                        "paper": {
                            "corpus_id": 273507197,
                            "title": "SoftSRV: Learn to Generate Targeted Synthetic Data",
                            "authors": [
                                {
                                    "authorId": "2280911353",
                                    "name": "Giulia DeSalvo"
                                },
                                {
                                    "authorId": "2327046826",
                                    "name": "Jean-Fracois Kagy"
                                },
                                {
                                    "authorId": "2283008",
                                    "name": "Lazaros Karydas"
                                },
                                {
                                    "authorId": "2435268",
                                    "name": "Afshin Rostamizadeh"
                                },
                                {
                                    "authorId": "2275226495",
                                    "name": "Sanjiv Kumar"
                                }
                            ],
                            "year": 2024,
                            "venue": "",
                            "n_citations": 0
                        },
                        "score": 0.94921875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Key Techniques and Frameworks",
                "tldr": "Synthetic data generation leverages diverse technical approaches including differential privacy mechanisms, generative models, and specialized frameworks for domain-specific tasks. These techniques range from privacy-preserving methods like DP-MERF to task-specific frameworks like CLIPPER for narrative verification and comprehensive systems like MagiCoder for code generation. (22 sources)",
                "text": "\n## Differential Privacy Approaches\n\nDifferential privacy (DP) has become fundamental in synthetic data generation, offering rigorous privacy guarantees while maintaining utility. Recent approaches fall into several categories:\n\n1. **Query-based methods** release differentially private answers to counting queries and use them as the basis for synthetic data generation. Notable examples include AIM <Paper corpusId=\"246430835\" paperTitle=\"(McKenna et al., 2022)\" isShortName></Paper>, Private-PGM <Paper corpusId=\"236976348\" paperTitle=\"(McKenna et al., 2021)\" isShortName></Paper>, and HDMM <Paper corpusId=\"51869463\" paperTitle=\"(McKenna et al., 2018)\" isShortName></Paper>.\n\n2. **Generative model approaches** train models like GANs on real datasets under differential privacy constraints. Examples include GS-WGAN <Paper corpusId=\"219687936\" paperTitle=\"(Chen et al., 2020)\" isShortName></Paper> and G-PATE <Paper corpusId=\"245634703\" paperTitle=\"(Long et al., 2019)\" isShortName></Paper>, which leverages private aggregation mechanisms to ensure strong privacy guarantees.\n\n3. **Hybrid frameworks** combine aspects of both approaches, as seen in DP-MERF <Paper corpusId=\"225077562\" paperTitle=\"(Harder et al., 2020)\" isShortName></Paper>, which uses random feature representations of kernel mean embeddings when comparing distributions, providing minimal privacy cost for training deep generative models.\n\n## Domain-Specific Frameworks\n\nSeveral frameworks have been developed to address specific domains and reasoning tasks:\n\n1. **Code Generation Frameworks**\n - **MagiCoder** implements OSS-Instruct to generate diverse instruction-solution pairs for code tasks by leveraging open-source code snippets as references <Paper corpusId=\"258461502\" paperTitle=\"(Rose et al., 2023)\" isShortName></Paper>.\n - **WaveCoder** focuses on multi-task scenarios, generating diverse instruction data across multiple code-related tasks <Paper corpusId=\"268230871\" paperTitle=\"(Sudalairaj et al., 2024)\" isShortName></Paper>.\n\n2. **Mathematical Reasoning Frameworks**\n - **Evol-Instruct** synthesizes iteratively more complex instructions to overcome limitations of previous methods <Paper corpusId=\"268230871\" paperTitle=\"(Sudalairaj et al., 2024)\" isShortName></Paper>.\n - **UCTR-ST** addresses complex tabular reasoning by incorporating program management, transformation, and table-text manipulation components <Paper corpusId=\"254877131\" paperTitle=\"(Li et al., 2022)\" isShortName></Paper>.\n\n3. **Multimodal Frameworks**\n - **Visual Chain-of-Thought (VCOT)** combines the efficiency of chain-of-thought reasoning with multimodal capabilities, generating synthetic data by recursively creating multimodal infillings <Paper corpusId=\"258461502\" paperTitle=\"(Rose et al., 2023)\" isShortName></Paper>.\n - **Hybrid-Synth** introduces background complexity to synthetic images along with programmatic variation of rotation, lighting, and scale <Paper corpusId=\"257607791\" paperTitle=\"(Natarajan et al., 2023)\" isShortName></Paper>.\n\n## Generation Techniques\n\n1. **GAN-based techniques** remain popular for tabular data generation, with implementations like TableGAN <Paper corpusId=\"47017667\" paperTitle=\"(Park et al., 2018)\" isShortName></Paper> and medBGAN <Paper corpusId=\"54479855\" paperTitle=\"(Baowaly et al., 2018)\" isShortName></Paper> demonstrating effectiveness in generating realistic synthetic records while preserving privacy.\n\n2. **Diffusion models** are increasingly applied to tabular data generation, showing superior performance over GANs and VAEs in certain contexts <Paper corpusId=\"249847841\" paperTitle=\"(Kim et al., 2022)\" isShortName></Paper> <Paper corpusId=\"264439324\" paperTitle=\"(Xu et al., 2023)\" isShortName></Paper>.\n\n3. **Instruction backtranslation** automatically labels human-written text with corresponding instructions, enabling construction of high-quality instruction-following models <Paper corpusId=\"260866107\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper>.\n\n4. **Bootstrapping techniques** involve iterative processes where models generate synthetic data, which is filtered and used for further fine-tuning <Paper corpusId=\"268553593\" paperTitle=\"(Peyrard et al., 2024)\" isShortName></Paper>.\n\n5. **Graph-based generation** techniques like GraDA synthesize factual samples from knowledge graphs for commonsense reasoning datasets <Paper corpusId=\"250390686\" paperTitle=\"(Maharana et al., 2022)\" isShortName></Paper>.\n\n6. **Compression-based approaches** like CLIPPER generate synthetic data for narrative claim verification by first compressing books into outlines and summaries before generating claims <Paper corpusId=\"276482747\" paperTitle=\"(Pham et al., 2025)\" isShortName></Paper>.\n\n7. **Repeated sampling strategies** enable smaller language models to achieve comparable performance to larger models for certain tasks when operating within fixed computational budgets <Paper corpusId=\"272146630\" paperTitle=\"(Bansal et al., 2024)\" isShortName></Paper>.\n\n8. **Pre-training data utilization** approaches like SYNTHLLM leverage existing web documents as an underutilized resource for scalable post-training data generation <Paper corpusId=\"277313659\" paperTitle=\"(Qin et al., 2025)\" isShortName></Paper> <Paper corpusId=\"269605607\" paperTitle=\"(Yue et al., 2024)\" isShortName></Paper> <Paper corpusId=\"269981934\" paperTitle=\"(Zhou et al., 2024)\" isShortName></Paper>.\n\nThese diverse frameworks and techniques illustrate the rapid evolution of synthetic data generation approaches across different domains, with increasing focus on task-specific optimizations, privacy preservation, and computational efficiency.",
                "citations": [
                    {
                        "id": "(McKenna et al., 2022)",
                        "snippets": [
                            "We propose AIM, a new algorithm for differentially private synthetic data generation. AIM is a workload-adaptive algorithm within the paradigm of algorithms that first selects a set of queries, then privately measures those queries, and finally generates synthetic data from the noisy measurements. It uses a set of innovative features to iteratively select the most useful measurements, reflecting both their relevance to the workload and their value in approximating the input data. We also provide analytic expressions to bound per-query error with high probability which can be used to construct confidence intervals and inform users about the accuracy of generated data. We show empirically that AIM consistently outperforms a wide variety of existing mechanisms across a variety of experimental settings."
                        ],
                        "paper": {
                            "corpus_id": 246430835,
                            "title": "AIM: An Adaptive and Iterative Mechanism for Differentially Private Synthetic Data",
                            "authors": [
                                {
                                    "authorId": "35836504",
                                    "name": "Ryan McKenna"
                                },
                                {
                                    "authorId": "122380107",
                                    "name": "Brett Mullins"
                                },
                                {
                                    "authorId": "144799908",
                                    "name": "D. Sheldon"
                                },
                                {
                                    "authorId": "1729605",
                                    "name": "G. Miklau"
                                }
                            ],
                            "year": 2022,
                            "venue": "Proceedings of the VLDB Endowment",
                            "n_citations": 86
                        },
                        "score": 0
                    },
                    {
                        "id": "(McKenna et al., 2021)",
                        "snippets": [
                            "We propose a general approach for differentially private synthetic data generation, that consists of three steps: (1) select a collection of low-dimensional marginals, (2) measure those marginals with a noise addition mechanism, and (3)\u00a0generate synthetic data that preserves the measured marginals well. Central to this approach is Private-PGM, a post-processing method that is used to estimate a high-dimensional data distribution from noisy measurements of its marginals. We present two mechanisms, NIST-MST and MST, that are instances of this general approach. NIST-MST was the winning mechanism in the 2018 NIST differential privacy synthetic data competition, and MST is a new mechanism that can work in more general settings, while still performing comparably to NIST-MST. We believe our general approach should be of broad interest, and can be adopted in future mechanisms for synthetic data generation."
                        ],
                        "paper": {
                            "corpus_id": 236976348,
                            "title": "Winning the NIST Contest: A scalable and general approach to differentially private synthetic data",
                            "authors": [
                                {
                                    "authorId": "35836504",
                                    "name": "Ryan McKenna"
                                },
                                {
                                    "authorId": "1729605",
                                    "name": "G. Miklau"
                                },
                                {
                                    "authorId": "144799908",
                                    "name": "D. Sheldon"
                                }
                            ],
                            "year": 2021,
                            "venue": "Journal of Privacy and Confidentiality",
                            "n_citations": 126
                        },
                        "score": 0
                    },
                    {
                        "id": "(McKenna et al., 2018)",
                        "snippets": [
                            "Differentially private algorithms for answering sets of predicate counting queries on a sensitive database have many applications. Organizations that collect individual-level data, such as statistical agencies and medical institutions, use them to safely release summary tabulations. However, existing techniques are accurate only on a narrow class of query workloads, or are extremely slow, especially when analyzing more than one or two dimensions of the data.\n In this work we propose HDMM, a new differentially private algorithm for answering a workload of predicate counting queries, that is especially effective for higher-dimensional datasets. HDMM represents query workloads using an implicit matrix representation and exploits this compact representation to efficiently search (a subset of) the space of differentially private algorithms for one that answers the input query workload with high accuracy. We empirically show that HDMM can efficiently answer queries with lower error than state-of-the-art techniques on a variety of low and high dimensional datasets."
                        ],
                        "paper": {
                            "corpus_id": 51869463,
                            "title": "Optimizing error of high-dimensional statistical queries under differential privacy",
                            "authors": [
                                {
                                    "authorId": "35836504",
                                    "name": "Ryan McKenna"
                                },
                                {
                                    "authorId": "1729605",
                                    "name": "G. Miklau"
                                },
                                {
                                    "authorId": "145256244",
                                    "name": "Michael Hay"
                                },
                                {
                                    "authorId": "2357165",
                                    "name": "Ashwin Machanavajjhala"
                                }
                            ],
                            "year": 2018,
                            "venue": "Proceedings of the VLDB Endowment",
                            "n_citations": 114
                        },
                        "score": 0
                    },
                    {
                        "id": "(Chen et al., 2020)",
                        "snippets": [
                            "The wide-spread availability of rich data has fueled the growth of machine learning applications in numerous domains. However, growth in domains with highly-sensitive data (e.g., medical) is largely hindered as the private nature of data prohibits it from being shared. To this end, we propose Gradient-sanitized Wasserstein Generative Adversarial Networks (GS-WGAN), which allows releasing a sanitized form of the sensitive data with rigorous privacy guarantees. In contrast to prior work, our approach is able to distort gradient information more precisely, and thereby enabling training deeper models which generate more informative samples. Moreover, our formulation naturally allows for training GANs in both centralized and federated (i.e., decentralized) data scenarios. Through extensive experiments, we find our approach consistently outperforms state-of-the-art approaches across multiple metrics (e.g., sample quality) and datasets."
                        ],
                        "paper": {
                            "corpus_id": 219687936,
                            "title": "GS-WGAN: A Gradient-Sanitized Approach for Learning Differentially Private Generators",
                            "authors": [
                                {
                                    "authorId": "153642281",
                                    "name": "Dingfan Chen"
                                },
                                {
                                    "authorId": "9517443",
                                    "name": "Tribhuvanesh Orekondy"
                                },
                                {
                                    "authorId": "1739548",
                                    "name": "Mario Fritz"
                                }
                            ],
                            "year": 2020,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 185
                        },
                        "score": 0
                    },
                    {
                        "id": "(Long et al., 2019)",
                        "snippets": [
                            "Recent advances in machine learning have largely benefited from the massive accessible training data. However, large-scale data sharing has raised great privacy concerns. In this work, we propose a novel privacy-preserving data Generative model based on the PATE framework (G-PATE), aiming to train a scalable differentially private data generator that preserves high generated data utility. Our approach leverages generative adversarial nets to generate data, combined with private aggregation among different discriminators to ensure strong privacy guarantees. Compared to existing approaches, G-PATE significantly improves the use of privacy budgets. In particular, we train a student data generator with an ensemble of teacher discriminators and propose a novel private gradient aggregation mechanism to ensure differential privacy on all information that flows from teacher discriminators to the student generator. In addition, with random projection and gradient discretization, the proposed gradient aggregation mechanism is able to effectively deal with high-dimensional gradient vectors. Theoretically, we prove that G-PATE ensures differential privacy for the data generator. Empirically, we demonstrate the superiority of G-PATE over prior work through extensive experiments. We show that G-PATE is the first work being able to generate high-dimensional image data with high data utility under limited privacy budgets ($\\epsilon \\le 1$). Our code is available at https://github.com/AI-secure/G-PATE."
                        ],
                        "paper": {
                            "corpus_id": 245634703,
                            "title": "G-PATE: Scalable Differentially Private Data Generator via Private Aggregation of Teacher Discriminators",
                            "authors": [
                                {
                                    "authorId": "3147214",
                                    "name": "Yunhui Long"
                                },
                                {
                                    "authorId": "2153207717",
                                    "name": "Boxin Wang"
                                },
                                {
                                    "authorId": "2119399524",
                                    "name": "Zhuolin Yang"
                                },
                                {
                                    "authorId": "1749353",
                                    "name": "B. Kailkhura"
                                },
                                {
                                    "authorId": "2085709",
                                    "name": "Aston Zhang"
                                },
                                {
                                    "authorId": "119755266",
                                    "name": "C.A. Gunter"
                                },
                                {
                                    "authorId": "2155883218",
                                    "name": "Bo Li"
                                }
                            ],
                            "year": 2019,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 74
                        },
                        "score": 0
                    },
                    {
                        "id": "(Harder et al., 2020)",
                        "snippets": [
                            "We propose a differentially private data generation paradigm using random feature representations of kernel mean embeddings when comparing the distribution of true data with that of synthetic data. We exploit the random feature representations for two important benefits. First, we require a minimal privacy cost for training deep generative models. This is because unlike kernel-based distance metrics that require computing the kernel matrix on all pairs of true and synthetic data points, we can detach the data-dependent term from the term solely dependent on synthetic data. Hence, we need to perturb the data-dependent term once and for all and then use it repeatedly during the generator training. Second, we can obtain an analytic sensitivity of the kernel mean embedding as the random features are norm bounded by construction. This removes the necessity of hyper-parameter search for a clipping norm to handle the unknown sensitivity of a generator network. We provide several variants of our algorithm, differentially private mean embeddings with random features (DP-MERF) to jointly generate labels and input features for datasets such as heterogeneous tabular data and image data. Our algorithm achieves drastically better privacy-utility trade-offs than existing methods when tested on several datasets."
                        ],
                        "paper": {
                            "corpus_id": 225077562,
                            "title": "DP-MERF: Differentially Private Mean Embeddings with RandomFeatures for Practical Privacy-preserving Data Generation",
                            "authors": [
                                {
                                    "authorId": "153653986",
                                    "name": "Frederik Harder"
                                },
                                {
                                    "authorId": "2220045",
                                    "name": "Kamil Adamczewski"
                                },
                                {
                                    "authorId": "50811522",
                                    "name": "Mijung Park"
                                }
                            ],
                            "year": 2020,
                            "venue": "International Conference on Artificial Intelligence and Statistics",
                            "n_citations": 101
                        },
                        "score": 0
                    },
                    {
                        "id": "(Rose et al., 2023)",
                        "snippets": [
                            "We propose Visual Chain-of-Thought (VCOT), which combines the efficiency, robustness, and multi-step reasoning of COT with the multimodal capabilities of vision-language models. VCOT synthetically augments sequential datasets and bridges logical gaps by recursively generating multimodal infillings and using the synthetic data to improve downstream task performance. These synthetic generations also serve as human-interpretable insights into AI systems' ability of multi-step reasoning. We demonstrate that VCOT creates consistent and novel synthetic data that enhances downstream performance on the VIST (Huang et al., 2016) and WIKIHOW (Koupaee & Wang, 2018) datasets."
                        ],
                        "paper": {
                            "corpus_id": 258461502,
                            "title": "Visual Chain of Thought: Bridging Logical Gaps with Multimodal Infillings",
                            "authors": [
                                {
                                    "authorId": "2215927057",
                                    "name": "Daniel Philip Rose"
                                },
                                {
                                    "authorId": "2215926792",
                                    "name": "Vaishnavi Himakunthala"
                                },
                                {
                                    "authorId": "2215914015",
                                    "name": "Andy Ouyang"
                                },
                                {
                                    "authorId": "2215915076",
                                    "name": "Ryan He"
                                },
                                {
                                    "authorId": "2185480449",
                                    "name": "Alex Mei"
                                },
                                {
                                    "authorId": "47006228",
                                    "name": "Yujie Lu"
                                },
                                {
                                    "authorId": "48227633",
                                    "name": "Michael Stephen Saxon"
                                },
                                {
                                    "authorId": "26961225",
                                    "name": "Chinmay Sonar"
                                },
                                {
                                    "authorId": "1705929",
                                    "name": "Diba Mirza"
                                },
                                {
                                    "authorId": "1682479",
                                    "name": "William Yang Wang"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 47
                        },
                        "score": 0.9150390625
                    },
                    {
                        "id": "(Sudalairaj et al., 2024)",
                        "snippets": [
                            "Xu et al. (2023) introduces Evol-Instruct, another variant of Self-Instruct, that synthesizes iteratively more complex instruction to overcome shortcomings of previous methods. Mukherjee et al. (2023), Mitra et al. (2023) present a synthetic data generation approach to enhance task diversity and scalability, alongside a progressive training framework aimed at improving the model's reasoning ability and response style to match teacher models. This is achieved by generating rich reasoning signals in the generated answer and progressively training on datasets of varying difficulty in incremental phases."
                        ],
                        "paper": {
                            "corpus_id": 268230871,
                            "title": "LAB: Large-Scale Alignment for ChatBots",
                            "authors": [
                                {
                                    "authorId": "2114819100",
                                    "name": "Shivchander Sudalairaj"
                                },
                                {
                                    "authorId": "150894502",
                                    "name": "Abhishek Bhandwaldar"
                                },
                                {
                                    "authorId": "2288530668",
                                    "name": "Aldo Pareja"
                                },
                                {
                                    "authorId": "2261102120",
                                    "name": "Kai Xu"
                                },
                                {
                                    "authorId": "2289845472",
                                    "name": "David D. Cox"
                                },
                                {
                                    "authorId": "2243025154",
                                    "name": "Akash Srivastava"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 34
                        },
                        "score": 0.951171875
                    },
                    {
                        "id": "(Li et al., 2022)",
                        "snippets": [
                            "To address the insufficient annotation challenge, we present a self-training framework for unsupervised complex tabular reasoning (UCTR-ST) by generating diverse synthetic data with complex logic. Specifically, UCTR-ST incorporates several essential techniques: we aggregate diverse programs and execute them on tables based on a \"Program-Management\" component, and we bridge the gap between programs and text with a powerful \"Program-Transformation\" module that generates natural language sentences with complex logic. Furthermore, we optimize the procedure using \"Table-Text Manipulator\" to handle joint table-text reasoning scenarios."
                        ],
                        "paper": {
                            "corpus_id": 254877131,
                            "title": "Optimization Techniques for Unsupervised Complex Table Reasoning via Self-Training Framework",
                            "authors": [
                                {
                                    "authorId": "2155354022",
                                    "name": "Zhenyu Li"
                                },
                                {
                                    "authorId": "2116521868",
                                    "name": "Xiuxing Li"
                                },
                                {
                                    "authorId": "2272781120",
                                    "name": "Sunqi Fan"
                                },
                                {
                                    "authorId": "2115642141",
                                    "name": "Jianyong Wang"
                                }
                            ],
                            "year": 2022,
                            "venue": "IEEE Transactions on Knowledge and Data Engineering",
                            "n_citations": 6
                        },
                        "score": 0.919921875
                    },
                    {
                        "id": "(Natarajan et al., 2023)",
                        "snippets": [
                            "We therefore exploit advances in open-source plugins and game engines, combined with unique techniques in synthetic data generation to perform state-of-the-art synthetic to real transfer learning, and make the following contributions. \n\n1. We present a synthetic data generation framework with an approach of introducing background complexity to synthetic images, in addition to the ability to programmatically vary rotation, lighting, backgrounds, and scale, making the resulting classifier very robust. We have made our framework publicly available (https://github.com/saiabinesh/hybrid-synth), which can be used to generate a dataset with any number of arbitrary classes."
                        ],
                        "paper": {
                            "corpus_id": 257607791,
                            "title": "Hybrid synthetic data generation pipeline that outperforms real data",
                            "authors": [
                                {
                                    "authorId": "143927102",
                                    "name": "S. Natarajan"
                                },
                                {
                                    "authorId": "32418087",
                                    "name": "M. G. Madden"
                                }
                            ],
                            "year": 2023,
                            "venue": "J. Electronic Imaging",
                            "n_citations": 2
                        },
                        "score": 0.90283203125
                    },
                    {
                        "id": "(Park et al., 2018)",
                        "snippets": [
                            "Privacy is an important concern for our society where sharing data with partners or releasing data to the public is a frequent occurrence. Some of the techniques that are being used to achieve privacy are to remove identifiers, alter quasi-identifiers, and perturb values. Unfortunately, these approaches suffer from two limitations. First, it has been shown that private information can still be leaked if attackers possess some background knowledge or other information sources. Second, they do not take into account the adverse impact these methods will have on the utility of the released data. In this paper, we propose a method that meets both requirements. Our method, called\n table-GAN\n , uses generative adversarial networks (GANs) to synthesize fake tables that are statistically similar to the original table yet do not incur information leakage. We show that the machine learning models trained using our synthetic tables exhibit performance that is similar to that of models trained using the original table for unknown testing cases. We call this property\n model compatibility\n . We believe that anonymization/perturbation/synthesis methods without model compatibility are of little value. We used four real-world datasets from four different domains for our experiments and conducted indepth comparisons with state-of-the-art anonymization, perturbation, and generation techniques. Throughout our experiments, only our method consistently shows balance between privacy level and model compatibility."
                        ],
                        "paper": {
                            "corpus_id": 47017667,
                            "title": "Data Synthesis based on Generative Adversarial Networks",
                            "authors": [
                                {
                                    "authorId": "5166698",
                                    "name": "Noseong Park"
                                },
                                {
                                    "authorId": "48794484",
                                    "name": "Mahmoud Mohammadi"
                                },
                                {
                                    "authorId": "50978924",
                                    "name": "Kshitij Gorde"
                                },
                                {
                                    "authorId": "1699357",
                                    "name": "S. Jajodia"
                                },
                                {
                                    "authorId": "2110627459",
                                    "name": "Hongkyu Park"
                                },
                                {
                                    "authorId": null,
                                    "name": "Youngmin Kim"
                                }
                            ],
                            "year": 2018,
                            "venue": "Proceedings of the VLDB Endowment",
                            "n_citations": 484
                        },
                        "score": 0
                    },
                    {
                        "id": "(Baowaly et al., 2018)",
                        "snippets": [
                            "Objective\nThe aim of this study was to generate synthetic electronic health records (EHRs). The generated EHR data will be more realistic than those generated using the existing medical Generative Adversarial Network (medGAN) method.\n\n\nMaterials and Methods\nWe modified medGAN to obtain two synthetic data generation models-designated as medical Wasserstein GAN with gradient penalty (medWGAN) and medical boundary-seeking GAN (medBGAN)-and compared the results obtained using the three models. We used 2 databases: MIMIC-III and National Health Insurance Research Database (NHIRD), Taiwan. First, we trained the models and generated synthetic EHRs by using these three 3 models. We then analyzed and compared the models' performance by using a few statistical methods (Kolmogorov-Smirnov test, dimension-wise probability for binary data, and dimension-wise average count for count data) and 2 machine learning tasks (association rule mining and prediction).\n\n\nResults\nWe conducted a comprehensive analysis and found our models were adequately efficient for generating synthetic EHR data. The proposed models outperformed medGAN in all cases, and among the 3 models, boundary-seeking GAN (medBGAN) performed the best.\n\n\nDiscussion\nTo generate realistic synthetic EHR data, the proposed models will be effective in the medical industry and related research from the viewpoint of providing better services. Moreover, they will eliminate barriers including limited access to EHR data and thus accelerate research on medical informatics.\n\n\nConclusion\nThe proposed models can adequately learn the data distribution of real EHRs and efficiently generate realistic synthetic EHRs. The results show the superiority of our models over the existing model."
                        ],
                        "paper": {
                            "corpus_id": 54479855,
                            "title": "Synthesizing electronic health records using improved generative adversarial networks",
                            "authors": [
                                {
                                    "authorId": "29805373",
                                    "name": "M. K. Baowaly"
                                },
                                {
                                    "authorId": "2116503720",
                                    "name": "Chia-Ching Lin"
                                },
                                {
                                    "authorId": "39986827",
                                    "name": "Chao-Lin Liu"
                                },
                                {
                                    "authorId": "6270307",
                                    "name": "Kuan-Ta Chen"
                                }
                            ],
                            "year": 2018,
                            "venue": "J. Am. Medical Informatics Assoc.",
                            "n_citations": 197
                        },
                        "score": 0
                    },
                    {
                        "id": "(Kim et al., 2022)",
                        "snippets": [
                            "Score-based generative models (SGMs) are a recent breakthrough in generating fake images. SGMs are known to surpass other generative models, e.g., generative adversarial networks (GANs) and variational autoencoders (VAEs). Being inspired by their big success, in this work, we fully customize them for generating fake tabular data. In particular, we are interested in oversampling minor classes since imbalanced classes frequently lead to sub-optimal training outcomes. To our knowledge, we are the first presenting a score-based tabular data oversampling method. Firstly, we re-design our own score network since we have to process tabular data. Secondly, we propose two options for our generation method: the former is equivalent to a style transfer for tabular data and the latter uses the standard generative policy of SGMs. Lastly, we define a fine-tuning method, which further enhances the oversampling quality. In our experiments with 6 datasets and 10 baselines, our method outperforms other oversampling methods in all cases."
                        ],
                        "paper": {
                            "corpus_id": 249847841,
                            "title": "SOS: Score-based Oversampling for Tabular Data",
                            "authors": [
                                {
                                    "authorId": "2109187265",
                                    "name": "Jayoung Kim"
                                },
                                {
                                    "authorId": "2109513298",
                                    "name": "Chae-Eun Lee"
                                },
                                {
                                    "authorId": "2171121134",
                                    "name": "Yehjin Shin"
                                },
                                {
                                    "authorId": "2107883259",
                                    "name": "Sewon Park"
                                },
                                {
                                    "authorId": "2117955473",
                                    "name": "Minjung Kim"
                                },
                                {
                                    "authorId": "5166698",
                                    "name": "Noseong Park"
                                },
                                {
                                    "authorId": "2115699011",
                                    "name": "Jihoon Cho"
                                }
                            ],
                            "year": 2022,
                            "venue": "Knowledge Discovery and Data Mining",
                            "n_citations": 38
                        },
                        "score": 0
                    },
                    {
                        "id": "(Xu et al., 2023)",
                        "snippets": [
                            "In the burgeoning field of natural language processing (NLP), Neural Topic Models (NTMs) , Large Language Models (LLMs) and Diffusion model have emerged as areas of significant research interest. Despite this, NTMs primarily utilize contextual embeddings from LLMs, which are not optimal for clustering or capable for topic based text generation. NTMs have never been combined with diffusion model for text generation. Our study addresses these gaps by introducing a novel framework named Diffusion-Enhanced Topic Modeling using Encoder-Decoder-based LLMs (DeTiME). DeTiME leverages Encoder-Decoder-based LLMs to produce highly clusterable embeddings that could generate topics that exhibit both superior clusterability and enhanced semantic coherence compared to existing methods. Additionally, by exploiting the power of diffusion model, our framework also provides the capability to do topic based text generation. This dual functionality allows users to efficiently produce highly clustered topics and topic based text generation simultaneously. DeTiME's potential extends to generating clustered embeddings as well. Notably, our proposed framework(both encoder-decoder based LLM and diffusion model) proves to be efficient to train and exhibits high adaptability to other LLMs and diffusion model, demonstrating its potential for a wide array of applications."
                        ],
                        "paper": {
                            "corpus_id": 264439324,
                            "title": "DeTiME: Diffusion-Enhanced Topic Modeling using Encoder-decoder based LLM",
                            "authors": [
                                {
                                    "authorId": "2110546424",
                                    "name": "Weijie Xu"
                                },
                                {
                                    "authorId": "2261645232",
                                    "name": "Wenxiang Hu"
                                },
                                {
                                    "authorId": "2261413304",
                                    "name": "Fanyou Wu"
                                },
                                {
                                    "authorId": "1757518",
                                    "name": "Srinivasan H. Sengamedu"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 18
                        },
                        "score": 0
                    },
                    {
                        "id": "(Li et al., 2023)",
                        "snippets": [
                            "We present a scalable method to build a high quality instruction following language model by automatically labelling human-written text with corresponding instructions. Our approach, named instruction backtranslation, starts with a language model finetuned on a small amount of seed data, and a given web corpus. The seed model is used to construct training examples by generating instruction prompts for web documents (self-augmentation), and then selecting high quality examples from among these candidates (self-curation). This data is then used to finetune a stronger model. Finetuning LLaMa on two iterations of our approach yields a model that outperforms all other LLaMa-based models on the Alpaca leaderboard not relying on distillation data, demonstrating highly effective self-alignment."
                        ],
                        "paper": {
                            "corpus_id": 260866107,
                            "title": "Self-Alignment with Instruction Backtranslation",
                            "authors": [
                                {
                                    "authorId": "2116235416",
                                    "name": "Xian Li"
                                },
                                {
                                    "authorId": "2114104308",
                                    "name": "Ping Yu"
                                },
                                {
                                    "authorId": "2384711",
                                    "name": "Chunting Zhou"
                                },
                                {
                                    "authorId": "32246932",
                                    "name": "Timo Schick"
                                },
                                {
                                    "authorId": "1982950",
                                    "name": "Luke Zettlemoyer"
                                },
                                {
                                    "authorId": "39455775",
                                    "name": "Omer Levy"
                                },
                                {
                                    "authorId": "145183709",
                                    "name": "J. Weston"
                                },
                                {
                                    "authorId": "35084211",
                                    "name": "M. Lewis"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 134
                        },
                        "score": 0
                    },
                    {
                        "id": "(Peyrard et al., 2024)",
                        "snippets": [
                            "The synthetic data generation Flows can leverage domain knowledge (Tang et al., 2023), task properties (Lu et al., 2024;Veselovsky et al., 2023;(Josifoski et al., 2023), or collaboration (Abdullin et al., 2024), and synthesize data of notably higher quality than what a single model or simple heuristics can achieve. This sets the stage for effective self-improvement loops where a language model participates in a semantic decoding algorithm producing high-quality synthetic data. Then, the language model improves itself through fine-tuning, thereby improving the Flow's capacity to generate even better synthetic data in a virtuous cycle (Silver et al., 2017;Burns et al., 2023;Singh et al., 2023;Chen et al., 2024b). An example of such a Flow is MAGDi (Chen et al., 2024a), a framework designed to distill reasoning interactions among multiple LLMs into smaller ones. This approach surpasses single-teacher distillation (Li et al., 2023)(Magister et al., 2022) and finetuning based on reasoning trajectories sampled from GPT-4 (Chen et al., 2023a)."
                        ],
                        "paper": {
                            "corpus_id": 268553593,
                            "title": "Agentic AI: The Era of Semantic Decoding",
                            "authors": [
                                {
                                    "authorId": "35512303",
                                    "name": "Maxime Peyrard"
                                },
                                {
                                    "authorId": "65826567",
                                    "name": "Martin Josifoski"
                                },
                                {
                                    "authorId": "2269473532",
                                    "name": "Robert West"
                                }
                            ],
                            "year": 2024,
                            "venue": "",
                            "n_citations": 0
                        },
                        "score": 0.95166015625
                    },
                    {
                        "id": "(Maharana et al., 2022)",
                        "snippets": [
                            "Recent advances in commonsense reasoning have been fueled by the availability of large-scale human annotated datasets. Manual annotation of such datasets, many of which are based on existing knowledge bases, is expensive and not scalable. Moreover, it is challenging to build augmentation data for commonsense reasoning because the synthetic questions need to adhere to real-world scenarios. Hence, we present GraDA, a graph-generative data augmentation framework to synthesize factual data samples from knowledge graphs for commonsense reasoning datasets. First, we train a graph-to-text model for conditional generation of questions from graph entities and relations. Then, we train a generator with GAN loss to generate distractors for synthetic questions."
                        ],
                        "paper": {
                            "corpus_id": 250390686,
                            "title": "GraDA: Graph Generative Data Augmentation for Commonsense Reasoning",
                            "authors": [
                                {
                                    "authorId": "8785371",
                                    "name": "Adyasha Maharana"
                                },
                                {
                                    "authorId": "143977268",
                                    "name": "Mohit Bansal"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Conference on Computational Linguistics",
                            "n_citations": 96
                        },
                        "score": 0.91064453125
                    },
                    {
                        "id": "(Pham et al., 2025)",
                        "snippets": [
                            "We introduce CLIPPER, a compression-based approach for generating synthetic data tailored to narrative claim verification - a task that requires reasoning over a book to verify a given claim. Instead of generating claims directly from the raw text of the book, which results in artifact-riddled claims, CLIPPER first compresses the book into chapter outlines and book summaries and then uses these intermediate representations to generate complex claims and corresponding chain-of-thoughts. Compared to naive approaches, CLIPPER produces claims that are more valid, grounded, and complex."
                        ],
                        "paper": {
                            "corpus_id": 276482747,
                            "title": "CLIPPER: Compression enables long-context synthetic data generation",
                            "authors": [
                                {
                                    "authorId": "2264486147",
                                    "name": "Chau Minh Pham"
                                },
                                {
                                    "authorId": "144455052",
                                    "name": "Yapei Chang"
                                },
                                {
                                    "authorId": "2136562",
                                    "name": "Mohit Iyyer"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.958984375
                    },
                    {
                        "id": "(Bansal et al., 2024)",
                        "snippets": [
                            "Several recent works aim to understand the utility of the weak but cheaper LMs in comparison to the strong but expensive LMs for reasoning. Specifically, Brown et al. (2024); Snell et al. (2024); Song et al. (2024) show that the solve rate of the small LMs can increase significantly with repeated sampling. In addition, Hassid et al. (2024) demonstrate that repeated generations from smaller LMs can outperform the data generated by larger LMs at a fixed sampling computational budget during inference for coding tasks."
                        ],
                        "paper": {
                            "corpus_id": 272146630,
                            "title": "Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal Sampling",
                            "authors": [
                                {
                                    "authorId": "2317010356",
                                    "name": "Hritik Bansal"
                                },
                                {
                                    "authorId": "2090537547",
                                    "name": "Arian Hosseini"
                                },
                                {
                                    "authorId": "2317013277",
                                    "name": "Rishabh Agarwal"
                                },
                                {
                                    "authorId": "2317984275",
                                    "name": "Vinh Q. Tran"
                                },
                                {
                                    "authorId": "2317010095",
                                    "name": "Mehran Kazemi"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 49
                        },
                        "score": 0.90234375
                    },
                    {
                        "id": "(Qin et al., 2025)",
                        "snippets": [
                            "Synthetic data has emerged as a promising approach to augment real data, addressing challenges related to data scarcity and the high cost of data collection and annotation. It has been widely applied in pre-training, instruction following, and reasoning tasks, demonstrating notable improvements in model performance. In this work, we focus on synthetic data generation for the post-training phase. Existing synthetic data generation methods typically rely on a small set of high-quality human-annotated seed samples, leveraging LLMs to generate diverse augmentations through various techniques: 1) sampling seed instructions as few-shot examples to elicit LLMs to generate new instructions [19,23,(Toshniwal et al., 2024)[50]; 2) leveraging LLMs to rephrase seed samples [12,21,34,35,(Yu et al., 2023). However, the scarcity of high-quality seed examples limits the scalability and diversity of generated data. To address this, Xu et al. [51] and Li et al. [24] propose generating synthetic data from scratch, leveraging either the inherent uncertainty of LLMs or a predefined knowledge taxonomy to guide the generation process. In contrast, pre-training data-being both vast and highly diverse-still remains an underutilized resource for scalable post-training data generation. Recent methods have explored extracting high-quality samples directly from web documents (similar to our Level-1 method) [54](Yue et al., 2024)(Zhou et al., 2024) or employing document backtranslation to generate questions (Li et al., 2023). Our SYNTHLLM aligns with this line of research and offers a more efficient approach to leveraging web documents for scaling diverse sample generation compared to aforementioned methods."
                        ],
                        "paper": {
                            "corpus_id": 277313659,
                            "title": "Scaling Laws of Synthetic Data for Language Models",
                            "authors": [
                                {
                                    "authorId": "2118242824",
                                    "name": "Zeyu Qin"
                                },
                                {
                                    "authorId": "2287927238",
                                    "name": "Qingxiu Dong"
                                },
                                {
                                    "authorId": "2284863493",
                                    "name": "Xingxing Zhang"
                                },
                                {
                                    "authorId": "2294850817",
                                    "name": "Li Dong"
                                },
                                {
                                    "authorId": "2116768132",
                                    "name": "Xiaolong Huang"
                                },
                                {
                                    "authorId": "2291073936",
                                    "name": "Ziyi Yang"
                                },
                                {
                                    "authorId": "2268760479",
                                    "name": "Mahmoud Khademi"
                                },
                                {
                                    "authorId": "2273919921",
                                    "name": "Dongdong Zhang"
                                },
                                {
                                    "authorId": "3032929",
                                    "name": "H. Awadalla"
                                },
                                {
                                    "authorId": "2352012716",
                                    "name": "Yi R. Fung"
                                },
                                {
                                    "authorId": "2347682196",
                                    "name": "Weizhu Chen"
                                },
                                {
                                    "authorId": "2258337019",
                                    "name": "Minhao Cheng"
                                },
                                {
                                    "authorId": "2323870436",
                                    "name": "Furu Wei"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 7
                        },
                        "score": 0.96337890625
                    },
                    {
                        "id": "(Yue et al., 2024)",
                        "snippets": [
                            "Instruction tuning improves the reasoning abilities of large language models (LLMs), with data quality and scalability being the crucial factors. Most instruction tuning data come from human crowd-sourcing or GPT-4 distillation. We propose a paradigm to efficiently harvest 10 million naturally existing instruction data from the pre-training web corpus to enhance LLM reasoning. Our approach involves (1) recalling relevant documents, (2) extracting instruction-response pairs, and (3) refining the extracted pairs using open-source LLMs. Fine-tuning base LLMs on this dataset, we build MAmmoTH2 models, which significantly boost performance on reasoning benchmarks. Notably, MAmmoTH2-7B's (Mistral) performance increases from 11% to 36.7% on MATH and from 36% to 68.4% on GSM8K without training on any in-domain data. Further training MAmmoTH2 on public instruction tuning datasets yields MAmmoTH2-Plus, achieving state-of-the-art performance on several reasoning and chatbot benchmarks. Our work demonstrates how to harvest large-scale, high-quality instruction data without costly human annotation or GPT-4 distillation, providing a new paradigm for building better instruction tuning data."
                        ],
                        "paper": {
                            "corpus_id": 269605607,
                            "title": "MAmmoTH2: Scaling Instructions from the Web",
                            "authors": [
                                {
                                    "authorId": "2284988933",
                                    "name": "Xiang Yue"
                                },
                                {
                                    "authorId": "2300091474",
                                    "name": "Tuney Zheng"
                                },
                                {
                                    "authorId": "2143853895",
                                    "name": "Ge Zhang"
                                },
                                {
                                    "authorId": "2249847177",
                                    "name": "Wenhu Chen"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 101
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhou et al., 2024)",
                        "snippets": [
                            "Mathematical reasoning is an important capability of large language models~(LLMs) for real-world applications. To enhance this capability, existing work either collects large-scale math-related texts for pre-training, or relies on stronger LLMs (\\eg GPT-4) to synthesize massive math problems. Both types of work generally lead to large costs in training or synthesis. To reduce the cost, based on open-source available texts, we propose an efficient way that trains a small LLM for math problem synthesis, to efficiently generate sufficient high-quality pre-training data. To achieve it, we create a dataset using GPT-4 to distill its data synthesis capability into the small LLM. Concretely, we craft a set of prompts based on human education stages to guide GPT-4, to synthesize problems covering diverse math knowledge and difficulty levels. Besides, we adopt the gradient-based influence estimation method to select the most valuable math-related texts. The both are fed into GPT-4 for creating the knowledge distillation dataset to train the small LLM. We leverage it to synthesize 6 million math problems for pre-training our JiuZhang3.0 model, which only needs to invoke GPT-4 API 9.3k times and pre-train on 4.6B data. Experimental results have shown that JiuZhang3.0 achieves state-of-the-art performance on several mathematical reasoning datasets, under both natural language reasoning and tool manipulation settings. Our code and data will be publicly released in \\url{https://github.com/RUCAIBox/JiuZhang3.0}."
                        ],
                        "paper": {
                            "corpus_id": 269981934,
                            "title": "JiuZhang3.0: Efficiently Improving Mathematical Reasoning by Training Small Data Synthesis Models",
                            "authors": [
                                {
                                    "authorId": "2265383494",
                                    "name": "Kun Zhou"
                                },
                                {
                                    "authorId": "2107926615",
                                    "name": "Beichen Zhang"
                                },
                                {
                                    "authorId": "2302813110",
                                    "name": "Jiapeng Wang"
                                },
                                {
                                    "authorId": "2111335050",
                                    "name": "Zhipeng Chen"
                                },
                                {
                                    "authorId": "2257376413",
                                    "name": "Wayne Xin Zhao"
                                },
                                {
                                    "authorId": "2165225571",
                                    "name": "Jing Sha"
                                },
                                {
                                    "authorId": "2125340023",
                                    "name": "Zhichao Sheng"
                                },
                                {
                                    "authorId": "2302793582",
                                    "name": "Shijin Wang"
                                },
                                {
                                    "authorId": "2274218622",
                                    "name": "Ji-Rong Wen"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 34
                        },
                        "score": 0
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Evaluation and Optimization Methods",
                "tldr": "Evaluating and optimizing synthetic data generation involves both quality assessment through external validators and strategic resource allocation across bootstrapping iterations. Recent frameworks have introduced data-driven approaches that replace traditional human-engineered prompts with systematic optimization techniques to generate more effective task-specific training data. (2 sources)",
                "text": "\nEvaluating the quality and effectiveness of synthetically generated data presents unique challenges that have prompted the development of specialized methodologies. A critical component in many recent synthetic data pipelines is the use of external verification mechanisms to filter out low-quality samples. This verification step has proven particularly valuable in bootstrapping approaches, where models generate synthetic data iteratively for fine-tuning <Paper corpusId=\"276079713\" paperTitle=\"(Yang et al., 2025)\" isShortName></Paper>.\n\nThe bootstrapping process raises important questions about resource optimization. Yang et al. explored how total budgets for generation and training should be allocated across multiple iterations to maximize final performance. Their research highlights the tradeoff between generating more data in each iteration versus conducting more iterations with smaller datasets <Paper corpusId=\"276079713\" paperTitle=\"(Yang et al., 2025)\" isShortName></Paper>. This optimization challenge becomes increasingly relevant as synthetic data generation scales up, particularly for resource-intensive tasks like mathematical reasoning and code generation.\n\nBeyond filtering approaches, recent work has introduced more sophisticated frameworks for optimizing synthetic data quality. The SoftSRV framework represents a significant advancement by using a data-driven loss minimization approach to steer large language models toward generating high-quality synthetic sequences for specific tasks. This approach provides a practical improvement over traditional prompt engineering methods that rely on human-crafted templates, which tend to be labor-intensive and domain-specific <Paper corpusId=\"273507197\" paperTitle=\"(DeSalvo et al., 2024)\" isShortName></Paper>.\n\nSoftSRV has demonstrated effectiveness across multiple domains including coding, mathematical reasoning, and general reasoning tasks. By systematically optimizing the generation process based on target distributions, this framework creates more effective fine-tuning data that can significantly improve the performance of smaller language models <Paper corpusId=\"273507197\" paperTitle=\"(DeSalvo et al., 2024)\" isShortName></Paper>.\n\nThe move toward data-driven optimization represents an important shift in the field. Rather than relying on human intuition to craft prompts that might elicit useful synthetic data, these newer approaches employ statistical measures and learning-based techniques to systematically guide the generation process. This trend aligns with the broader movement in AI toward more automated and principled methods for model improvement <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.\n\nAs synthetic data generation techniques continue to evolve, we can expect further refinements in evaluation metrics and optimization strategies. Developing robust, domain-agnostic methods for assessing synthetic data quality remains an active area of research, with particular emphasis on measuring how well synthetic datasets transfer to downstream tasks and generalize beyond their training distribution <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.",
                "citations": [
                    {
                        "id": "(Yang et al., 2025)",
                        "snippets": [
                            "Modern foundation models often undergo iterative ``bootstrapping'' in their post-training phase: a model generates synthetic data, an external verifier filters out low-quality samples, and the high-quality subset is used for further fine-tuning. Over multiple iterations, the model's performance improves--raising a crucial question: how should the total budget on generation and training be allocated across iterations to maximize final performance?"
                        ],
                        "paper": {
                            "corpus_id": 276079713,
                            "title": "Spend Wisely: Maximizing Post-Training Gains in Iterative Synthetic Data Boostrapping",
                            "authors": [
                                {
                                    "authorId": "2284644270",
                                    "name": "Pu Yang"
                                },
                                {
                                    "authorId": "2343647731",
                                    "name": "Yunzhen Feng"
                                },
                                {
                                    "authorId": "2343512547",
                                    "name": "Ziyuan Chen"
                                },
                                {
                                    "authorId": "2343592601",
                                    "name": "Yuhang Wu"
                                },
                                {
                                    "authorId": "2344832360",
                                    "name": "Zhuoyuan Li"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.93017578125
                    },
                    {
                        "id": "(DeSalvo et al., 2024)",
                        "snippets": [
                            "We present a novel framework, SoftSRV, that is used to generate targeted synthetic fine-tuning data for improving task-specific model performance. Given a sample from a target distribution, our proposed framework uses a data-driven loss minimization approach to steer a frozen large language model (LLM) to generate synthetic sequences that are similar to those from the target distribution. SoftSRV provides a practical improvement over common prompt engineering approaches that rely on human-engineered prompt-templates, which can be idiosyncratic, labor-intensive to craft, and may need to be specialized per domain. We empirically evaluate our method against standard baselines guiding a large LLM to generate synthetic data to fine-tune a smaller language model on three different domains (coding, math, reasoning)."
                        ],
                        "paper": {
                            "corpus_id": 273507197,
                            "title": "SoftSRV: Learn to Generate Targeted Synthetic Data",
                            "authors": [
                                {
                                    "authorId": "2280911353",
                                    "name": "Giulia DeSalvo"
                                },
                                {
                                    "authorId": "2327046826",
                                    "name": "Jean-Fracois Kagy"
                                },
                                {
                                    "authorId": "2283008",
                                    "name": "Lazaros Karydas"
                                },
                                {
                                    "authorId": "2435268",
                                    "name": "Afshin Rostamizadeh"
                                },
                                {
                                    "authorId": "2275226495",
                                    "name": "Sanjiv Kumar"
                                }
                            ],
                            "year": 2024,
                            "venue": "",
                            "n_citations": 0
                        },
                        "score": 0.94921875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Future Directions",
                "tldr": "Future research in synthetic data generation will likely focus on creating larger, more diverse datasets across complex data types while developing innovative architectures and privacy-preserving mechanisms. These advancements aim to democratize AI development and expand the application of synthetic data to new domains without requiring extensive computational resources. (1 source)",
                "text": "\nDespite significant progress in synthetic data generation for reasoning tasks, several promising directions remain for future exploration. Expanding the scale and diversity of synthetic datasets represents a primary goal for researchers. The ability to generate extensive datasets with high variability would address persistent data scarcity challenges in specialized domains and enhance the applicability of synthetic data in complex machine learning tasks <Paper corpusId=\"273811804\" paperTitle=\"(Zbeeb et al., 2024)\" isShortName></Paper>.\n\nThe development of novel architectures beyond current generative models presents another important avenue for advancement. While GANs, VAEs, and diffusion models have demonstrated impressive capabilities, exploring innovative architectures could further improve synthetic data quality and diversity. Importantly, future research should focus on making these advancements accessible with limited computational resources, democratizing access to cutting-edge synthetic data generation techniques without requiring extensive infrastructure <Paper corpusId=\"273811804\" paperTitle=\"(Zbeeb et al., 2024)\" isShortName></Paper>.\n\nPrivacy preservation remains a critical area for future development. As synthetic data generation expands into more sensitive domains like healthcare and finance, integrating robust privacy-preserving techniques becomes essential. Future work will likely focus on combining differential privacy mechanisms with generative models to provide stronger guarantees while maintaining data utility <Paper corpusId=\"273811804\" paperTitle=\"(Zbeeb et al., 2024)\" isShortName></Paper>.\n\nFinally, expanding synthetic data generation to more complex data types represents a significant opportunity. While current approaches have shown promise for text, code, and certain types of images, applying these techniques to time-series data, graph structures, and multimodal datasets would substantially broaden their impact. Tailoring generative models to effectively handle these complex data structures could unlock new applications across various fields and further enhance reasoning capabilities in AI systems <Paper corpusId=\"273811804\" paperTitle=\"(Zbeeb et al., 2024)\" isShortName></Paper>.\n\nAs the field continues to mature, these future directions will likely converge with ongoing research in verification methods, domain-specific optimization techniques, and automated data generation workflows to create increasingly powerful tools for addressing complex reasoning tasks <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.",
                "citations": [
                    {
                        "id": "(Zbeeb et al., 2024)",
                        "snippets": [
                            "To further advance the field of synthetic data generation, several key areas warrant additional exploration and development. One significant avenue is the capability to generate larger and more diverse datasets. Expanding the capacity to synthesize extensive datasets with high variability would greatly enhance the applicability of synthetic data in machine learning tasks, especially in domains where data scarcity remains a challenge. \n\nMoreover, exploring innovative architectures beyond the current models can lead to substantial advancements. Investigating new generative models or enhancing existing ones could improve the quality and diversity of synthetic data. Importantly, demonstrating that these advancements can be achieved using accessible computational resources, such as a personal computer with a well-coded pipeline, would underscore the feasibility of cutting-edge AI developments without the need for extensive infrastructure. This democratization of technology could encourage broader participation in the field and accelerate innovation. \n\nAdditionally, integrating more robust privacy-preserving techniques into the data generation process remains a critical area for future work. As privacy concerns continue to grow, developing methods that ensure data utility while rigorously protecting sensitive information is essential. Combining differential privacy mechanisms with generative models could provide stronger guarantees and expand the adoption of synthetic data in sensitive domains. \n\nFinally, applying synthetic data generation techniques to a wider range of applications, including those with complex data types such as time-series, graphs, and multimodal data, would significantly broaden the impact of this research. Tailoring generative models to handle these complex data structures effectively could open new opportunities in various fields, from healthcare to finance, where such data types are prevalent."
                        ],
                        "paper": {
                            "corpus_id": 273811804,
                            "title": "Exploring the Landscape for Generative Sequence Models for Specialized Data Synthesis",
                            "authors": [
                                {
                                    "authorId": "2329103285",
                                    "name": "Mohammad Zbeeb"
                                },
                                {
                                    "authorId": "2329102879",
                                    "name": "Mohammad Ghorayeb"
                                },
                                {
                                    "authorId": "2329103319",
                                    "name": "Mariam Salman"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.943359375
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.288618
    }
}