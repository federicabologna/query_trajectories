{
    "query": "What are the key experimental methods, such as token distribution shift analysis, that provide direct evidence supporting the Superficial Alignment Hypothesis in large language models?",
    "user_id": "lib_user",
    "task_id": "f41e2490-7f30-476b-8cc9-70ad6f191a28",
    "timestamp": "2025-06-23T21:46:11.050296",
    "n_retrieval": 256,
    "n_retrieved": 257,
    "n_candidates": 6,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.11553300000000001,
    "decomposed_query": {
        "rewritten_query": "Key experimental methods, such as token distribution shift analysis, that provide direct evidence supporting the Superficial Alignment Hypothesis in large language models.",
        "keyword_query": "experimental methods token distribution shift analysis direct evidence Superficial Alignment Hypothesis large language models",
        "search_filters": {
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.009942,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 51,
            "citation_count": 198,
            "influential_citation_count": 27,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.01552, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "51583409",
                    "name": "Bill Yuchen Lin"
                },
                {
                    "authorId": "3023068",
                    "name": "Abhilasha Ravichander"
                },
                {
                    "authorId": "50085131",
                    "name": "Ximing Lu"
                },
                {
                    "authorId": "46217681",
                    "name": "Nouha Dziri"
                },
                {
                    "authorId": "1947172233",
                    "name": "Melanie Sclar"
                },
                {
                    "authorId": "37619618",
                    "name": "Khyathi Raghavi Chandu"
                },
                {
                    "authorId": "1857797",
                    "name": "Chandra Bhagavatula"
                },
                {
                    "authorId": "2259707400",
                    "name": "Yejin Choi"
                }
            ],
            "abstract": "The alignment tuning process of large language models (LLMs) typically involves instruction learning through supervised fine-tuning (SFT) and preference tuning via reinforcement learning from human feedback (RLHF). A recent study, LIMA (Zhou et al. 2023), shows that using merely 1K examples for SFT can achieve significant alignment performance as well, suggesting that the effect of alignment tuning might be\"superficial.\"This raises questions about how exactly the alignment tuning transforms a base LLM. We analyze the effect of alignment tuning by examining the token distribution shift between base LLMs and their aligned counterpart. Our findings reveal that base LLMs and their alignment-tuned versions perform nearly identically in decoding on the majority of token positions. Most distribution shifts occur with stylistic tokens. These direct evidence strongly supports the Superficial Alignment Hypothesis suggested by LIMA. Based on these findings, we rethink the alignment of LLMs by posing the research question: how effectively can we align base LLMs without SFT or RLHF? To address this, we introduce a simple, tuning-free alignment method, URIAL. URIAL achieves effective alignment purely through in-context learning (ICL) with base LLMs, requiring as few as three constant stylistic examples and a system prompt. We conduct a fine-grained and interpretable evaluation on a diverse set of examples, named JUST-EVAL-INSTRUCT. Results demonstrate that base LLMs with URIAL can match or even surpass the performance of LLMs aligned with SFT or SFT+RLHF. We show that the gap between tuning-free and tuning-based alignment methods can be significantly reduced through strategic prompting and ICL. Our findings on the superficial nature of alignment tuning and results with URIAL suggest that deeper analysis and theoretical understanding of alignment is crucial to future LLM research.",
            "corpus_id": 265608902,
            "sentences": [
                {
                    "corpus_id": "265608902",
                    "title": "The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning",
                    "text": "The alignment tuning process of large language models (LLMs) typically involves instruction learning through supervised fine-tuning (SFT) and preference tuning via reinforcement learning from human feedback (RLHF). A recent study, LIMA (Zhou et al. 2023), shows that using merely 1K examples for SFT can achieve significant alignment performance as well, suggesting that the effect of alignment tuning might be\"superficial.\"This raises questions about how exactly the alignment tuning transforms a base LLM. We analyze the effect of alignment tuning by examining the token distribution shift between base LLMs and their aligned counterpart. Our findings reveal that base LLMs and their alignment-tuned versions perform nearly identically in decoding on the majority of token positions. Most distribution shifts occur with stylistic tokens. These direct evidence strongly supports the Superficial Alignment Hypothesis suggested by LIMA. Based on these findings, we rethink the alignment of LLMs by posing the research question: how effectively can we align base LLMs without SFT or RLHF? To address this, we introduce a simple, tuning-free alignment method, URIAL. URIAL achieves effective alignment purely through in-context learning (ICL) with base LLMs, requiring as few as three constant stylistic examples and a system prompt. We conduct a fine-grained and interpretable evaluation on a diverse set of examples, named JUST-EVAL-INSTRUCT. Results demonstrate that base LLMs with URIAL can match or even surpass the performance of LLMs aligned with SFT or SFT+RLHF. We show that the gap between tuning-free and tuning-based alignment methods can be significantly reduced through strategic prompting and ICL. Our findings on the superficial nature of alignment tuning and results with URIAL suggest that deeper analysis and theoretical understanding of alignment is crucial to future LLM research.",
                    "score": 0.4237876921701706,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8779296875
                },
                {
                    "corpus_id": "265608902",
                    "title": "The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning",
                    "text": "We first substantiate the underlying hypothesis regarding the superficial nature of alignment tuning via detailed analysis on token distribution shifts between base and aligned LLMs. This analytical method allows us to clearly investigate which token positions are affected by alignment tuning, providing insights for developing more efficient alignment methods and further studying the science of LLMs. Inspired by these findings, we propose a strong baseline method for tuning-free alignment, URIAL, which aligns untuned base LLMs via in-context learning with a constant prompt. Experiments show that URIAL significantly reduce the gap between base LLMs and their aligned versions. Our contribution can be summarized as follows: \n\nAnalysis: To gain a deeper understanding of alignment tuning, we analyze the token distribution shift between base and aligned LLMs. We find that alignment predominantly affects a minimal portion of token selection, influencing primarily stylistic elements and safety disclaimers in just 5-8% of cases. On most of the token positions, aligned and base models concur on top-token choices. Also, we find that alignment tuning is much more critical for initial tokens than later tokens. \n\nMethods: We introduce a simple yet effective method for aligning base LLMs, URIAL. It utilizes only as few as three constant curated examples for ICL, yet it effectively aligns base LLMs and matches the performance of SFT+RLHF in some scenarios. We also discover that well-written, stylistic examples are more effective than semantically relevant ones that are dynamically retrieved. URIAL offers both efficiency and simplicity in aligning base LLMs without requiring fine-tuning. This method significantly conserves time and resources, which is especially beneficial when dealing with extremely large LMs or when base LLMs need frequent evaluation. Furthermore, it enables a deeper investigation into the knowledge and capabilities innate to these base LLMs, while fostering more precise and economical approaches to align them with their deficiencies. \n\nEvaluation: We develop a comprehensive and interpretable evaluation protocol, encompassing six aspects with verifiable judgments. We also release the annotations we gathered for community use in evaluation and training open-source LLM evaluators.",
                    "score": 0.34756022295139444,
                    "section_title": "CONCLUSION",
                    "char_start_offset": 43325,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 182
                        },
                        {
                            "start": 183,
                            "end": 403
                        },
                        {
                            "start": 404,
                            "end": 580
                        },
                        {
                            "start": 581,
                            "end": 683
                        },
                        {
                            "start": 684,
                            "end": 730
                        },
                        {
                            "start": 733,
                            "end": 865
                        },
                        {
                            "start": 866,
                            "end": 1035
                        },
                        {
                            "start": 1036,
                            "end": 1120
                        },
                        {
                            "start": 1121,
                            "end": 1216
                        },
                        {
                            "start": 1219,
                            "end": 1301
                        },
                        {
                            "start": 1302,
                            "end": 1464
                        },
                        {
                            "start": 1465,
                            "end": 1602
                        },
                        {
                            "start": 1603,
                            "end": 1699
                        },
                        {
                            "start": 1700,
                            "end": 1868
                        },
                        {
                            "start": 1869,
                            "end": 2072
                        },
                        {
                            "start": 2075,
                            "end": 2204
                        },
                        {
                            "start": 2205,
                            "end": 2321
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.837890625
                },
                {
                    "corpus_id": "265608902",
                    "title": "The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning",
                    "text": "On the other hand, a recent study, LIMA (Zhou et al., 2023), proposes the \"Superficial Alignment Hypothesis,\" which argues that alignment tuning might simply teach base LLMs to select a subdistribution of data formats for interacting with users. Zhou et al. (2023) demonstrates that SFT with as few as 1,000 examples can also yield high-quality aligned models, thus providing indirect support for this hypothesis. However, conclusive and direct supporting evidence for the superficial alignment hypothesis remains underexplored. Therefore, it is important to analyze how exactly alignment tuning alters the behavior of base LLMs. \n\nTo this end, we investigate the effects of alignment tuning by directly comparing the token distributions between base LLMs and their aligned versions (e.g., Llama-2 and Llama-2-chat). Surprisingly, we find that base and aligned LLMs typically perform almost identically in most positions in terms of ranking tokens during decoding (Sec. 2). Additionally, we observe that the top-ranked tokens in aligned LLMs are mostly found within the top five tokens ranked by base LLMs, and the distribution shift is more pronounced in earlier token positions. The most significant distribution shifts occur predominantly in stylistic tokens (e.g., 'Hello', 'Thank', 'However', 'Remember', etc.), which include transitional phrases, discourse markers, and safety disclaimers, rather than in content-bearing words that directly provide useful knowledge for resolving the queries from users. Our findings (Sec. 2.3) from token distribution shift analysis directly provide substantial support for the superficial alignment hypothesis. We offer both quantitative and qualitative analyses to demonstrate that alignment tuning primarily focus on adopting the language style of responsible AI assistants and depends to a great extent on the knowledge that base LLMs have already acquired. \n\nBased on our findings regarding the superficial nature of alignment tuning, we pose the research question for rethinking the research on aligning LLMs: how effectively can we align base LLMs without any tuning? We propose a simple, tuning-free alignment method called URIAL (Untuned LLMs with Restyled In-context ALignment), which effectively aligns base LLMs without tuning their weights (Sec. 3).",
                    "score": 0.4853797207049277,
                    "section_title": "Preprint",
                    "char_start_offset": 993,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 245
                        },
                        {
                            "start": 246,
                            "end": 413
                        },
                        {
                            "start": 414,
                            "end": 528
                        },
                        {
                            "start": 529,
                            "end": 629
                        },
                        {
                            "start": 632,
                            "end": 816
                        },
                        {
                            "start": 817,
                            "end": 969
                        },
                        {
                            "start": 970,
                            "end": 973
                        },
                        {
                            "start": 974,
                            "end": 1180
                        },
                        {
                            "start": 1181,
                            "end": 1509
                        },
                        {
                            "start": 1510,
                            "end": 1528
                        },
                        {
                            "start": 1529,
                            "end": 1651
                        },
                        {
                            "start": 1652,
                            "end": 1901
                        },
                        {
                            "start": 1904,
                            "end": 2114
                        },
                        {
                            "start": 2115,
                            "end": 2302
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.82666015625
                },
                {
                    "corpus_id": "265608902",
                    "title": "The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning",
                    "text": "Similar observations are also reported by other recent studies (Chen et al., 2023a;Lee et al., 2023). Moreover, Gudibande et al. (2023) demonstrate that aligning open-source LLMs by imitating proprietary LLMs (e.g., ChatGPT) may not always yield desirable results, emphasizing the importance of a strong pre-trained base LLM for producing factual content. Tuning-based methods such as LIMA still require tuning the weights of LLMs and consequently face the limitations described above when the model size is too large or we need to frequently align base LLMs for evaluation. A concurrent work (Duan et al., 2023) also explores the similarity between ICL and instruction-tuning in their effects on downstream tasks by analyzing the LLMs' hidden states. As for the theory of alignment, these studies only indirectly suggest the promise of the superficial alignment hypothesis but do not directly show where and when the alignment tuning significantly changes the model behavior. In this paper, we study the Preprint superficial alignment hypothesis more directly through the lens of token distribution shift, which directly exhibits the alignment effect and produces more detailed non-trivial findings.",
                    "score": 0.3942397861448286,
                    "section_title": "ALIGNMENT TUNING & SUPERFICIAL ALIGNMENT HYPOTHESIS",
                    "char_start_offset": 36911,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 101
                        },
                        {
                            "start": 102,
                            "end": 355
                        },
                        {
                            "start": 356,
                            "end": 574
                        },
                        {
                            "start": 575,
                            "end": 751
                        },
                        {
                            "start": 752,
                            "end": 976
                        },
                        {
                            "start": 977,
                            "end": 1200
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.72998046875
                },
                {
                    "corpus_id": "265608902",
                    "title": "The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning",
                    "text": "We look at the probability of the aligned token o t that is ranked top by P align and use its probability (0-1) in P base as a metric for measuring the We here plot the rank of aligned token inside the list of tokens predicted by Base LLMs (ranked by P_base). Base Rank  = 0 means the aligned token is also the top-ranked token decoded by Base LLM. distribution shift. We can see that the KL-divergence goes down over time and the base-prob keeps increasing over time. Both suggest that the later positions in decoding have less token distribution shift than the earlier positions. In particular, the base-prob of tokens can be close to 1.0 in the end. Surprisingly, the average base-rank of aligned tokens are lower than 5 soon after t \u2265 5. This means that the top token decoded by aligned models are usually within the top 5 decoded by the base models. This again substantiate the hypothesis that alignment tuning is \"superficial\".",
                    "score": 0.4246869205386065,
                    "section_title": "FINDINGS & ANALYSIS",
                    "char_start_offset": 12247,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 259
                        },
                        {
                            "start": 260,
                            "end": 348
                        },
                        {
                            "start": 349,
                            "end": 368
                        },
                        {
                            "start": 369,
                            "end": 468
                        },
                        {
                            "start": 469,
                            "end": 581
                        },
                        {
                            "start": 582,
                            "end": 652
                        },
                        {
                            "start": 653,
                            "end": 741
                        },
                        {
                            "start": 742,
                            "end": 854
                        },
                        {
                            "start": 855,
                            "end": 933
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.63232421875
                },
                {
                    "corpus_id": "265608902",
                    "title": "The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning",
                    "text": "In this section, we examine the effect of alignment tuning (SFT & RLHF) by measuring the token distribution shift. Our key findings can be summarized as follows: \n\nAlignment affects only a very small fraction of tokens; the base and aligned LLMs behave the same in decoding on most positions, where they share the same top-ranked tokens. \n\nAlignment mainly concerns stylistic tokens, such as discourse markers, transitional words, and safety disclaimers, which only take about a very small part of the total token positions. Alignment is more critical for earlier tokens. For most positions, the aligned model's top-ranked token is within the top 5 tokens ranked by the base model. Base LLMs have already acquired adequate knowledge to follow instructions. They behave very similarly to aligned LLMs when given an appropriate context as a prefix.",
                    "score": 0.36831227129623645,
                    "section_title": "SUMMARY OF THE FINDINGS WITH TOKEN DISTRIBUTION SHIFT",
                    "char_start_offset": 13238,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 114
                        },
                        {
                            "start": 115,
                            "end": 161
                        },
                        {
                            "start": 164,
                            "end": 337
                        },
                        {
                            "start": 340,
                            "end": 524
                        },
                        {
                            "start": 525,
                            "end": 571
                        },
                        {
                            "start": 572,
                            "end": 681
                        },
                        {
                            "start": 682,
                            "end": 756
                        },
                        {
                            "start": 757,
                            "end": 846
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.49853515625
                }
            ],
            "relevance_judgement": 0.8779296875,
            "relevance_judgment_input_expanded": "# Title: The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning\n# Venue: arXiv.org\n# Authors: Bill Yuchen Lin, Abhilasha Ravichander, Ximing Lu, Nouha Dziri, Melanie Sclar, Khyathi Raghavi Chandu, Chandra Bhagavatula, Yejin Choi\n## Abstract\nThe alignment tuning process of large language models (LLMs) typically involves instruction learning through supervised fine-tuning (SFT) and preference tuning via reinforcement learning from human feedback (RLHF). A recent study, LIMA (Zhou et al. 2023), shows that using merely 1K examples for SFT can achieve significant alignment performance as well, suggesting that the effect of alignment tuning might be\"superficial.\"This raises questions about how exactly the alignment tuning transforms a base LLM. We analyze the effect of alignment tuning by examining the token distribution shift between base LLMs and their aligned counterpart. Our findings reveal that base LLMs and their alignment-tuned versions perform nearly identically in decoding on the majority of token positions. Most distribution shifts occur with stylistic tokens. These direct evidence strongly supports the Superficial Alignment Hypothesis suggested by LIMA. Based on these findings, we rethink the alignment of LLMs by posing the research question: how effectively can we align base LLMs without SFT or RLHF? To address this, we introduce a simple, tuning-free alignment method, URIAL. URIAL achieves effective alignment purely through in-context learning (ICL) with base LLMs, requiring as few as three constant stylistic examples and a system prompt. We conduct a fine-grained and interpretable evaluation on a diverse set of examples, named JUST-EVAL-INSTRUCT. Results demonstrate that base LLMs with URIAL can match or even surpass the performance of LLMs aligned with SFT or SFT+RLHF. We show that the gap between tuning-free and tuning-based alignment methods can be significantly reduced through strategic prompting and ICL. Our findings on the superficial nature of alignment tuning and results with URIAL suggest that deeper analysis and theoretical understanding of alignment is crucial to future LLM research.\n## Preprint\nOn the other hand, a recent study, LIMA (Zhou et al., 2023), proposes the \"Superficial Alignment Hypothesis,\" which argues that alignment tuning might simply teach base LLMs to select a subdistribution of data formats for interacting with users. Zhou et al. (2023) demonstrates that SFT with as few as 1,000 examples can also yield high-quality aligned models, thus providing indirect support for this hypothesis. However, conclusive and direct supporting evidence for the superficial alignment hypothesis remains underexplored. Therefore, it is important to analyze how exactly alignment tuning alters the behavior of base LLMs. \n\nTo this end, we investigate the effects of alignment tuning by directly comparing the token distributions between base LLMs and their aligned versions (e.g., Llama-2 and Llama-2-chat). Surprisingly, we find that base and aligned LLMs typically perform almost identically in most positions in terms of ranking tokens during decoding (Sec. 2). Additionally, we observe that the top-ranked tokens in aligned LLMs are mostly found within the top five tokens ranked by base LLMs, and the distribution shift is more pronounced in earlier token positions. The most significant distribution shifts occur predominantly in stylistic tokens (e.g., 'Hello', 'Thank', 'However', 'Remember', etc.), which include transitional phrases, discourse markers, and safety disclaimers, rather than in content-bearing words that directly provide useful knowledge for resolving the queries from users. Our findings (Sec. 2.3) from token distribution shift analysis directly provide substantial support for the superficial alignment hypothesis. We offer both quantitative and qualitative analyses to demonstrate that alignment tuning primarily focus on adopting the language style of responsible AI assistants and depends to a great extent on the knowledge that base LLMs have already acquired. \n\nBased on our findings regarding the superficial nature of alignment tuning, we pose the research question for rethinking the research on aligning LLMs: how effectively can we align base LLMs without any tuning? We propose a simple, tuning-free alignment method called URIAL (Untuned LLMs with Restyled In-context ALignment), which effectively aligns base LLMs without tuning their weights (Sec. 3).\n\n## FINDINGS & ANALYSIS\nWe look at the probability of the aligned token o t that is ranked top by P align and use its probability (0-1) in P base as a metric for measuring the We here plot the rank of aligned token inside the list of tokens predicted by Base LLMs (ranked by P_base). Base Rank  = 0 means the aligned token is also the top-ranked token decoded by Base LLM. distribution shift. We can see that the KL-divergence goes down over time and the base-prob keeps increasing over time. Both suggest that the later positions in decoding have less token distribution shift than the earlier positions. In particular, the base-prob of tokens can be close to 1.0 in the end. Surprisingly, the average base-rank of aligned tokens are lower than 5 soon after t \u2265 5. This means that the top token decoded by aligned models are usually within the top 5 decoded by the base models. This again substantiate the hypothesis that alignment tuning is \"superficial\".\n\n## SUMMARY OF THE FINDINGS WITH TOKEN DISTRIBUTION SHIFT\nIn this section, we examine the effect of alignment tuning (SFT & RLHF) by measuring the token distribution shift. Our key findings can be summarized as follows: \n\nAlignment affects only a very small fraction of tokens; the base and aligned LLMs behave the same in decoding on most positions, where they share the same top-ranked tokens. \n\nAlignment mainly concerns stylistic tokens, such as discourse markers, transitional words, and safety disclaimers, which only take about a very small part of the total token positions. Alignment is more critical for earlier tokens. For most positions, the aligned model's top-ranked token is within the top 5 tokens ranked by the base model. Base LLMs have already acquired adequate knowledge to follow instructions. They behave very similarly to aligned LLMs when given an appropriate context as a prefix.\n\n## ALIGNMENT TUNING & SUPERFICIAL ALIGNMENT HYPOTHESIS\nSimilar observations are also reported by other recent studies (Chen et al., 2023a;Lee et al., 2023). Moreover, Gudibande et al. (2023) demonstrate that aligning open-source LLMs by imitating proprietary LLMs (e.g., ChatGPT) may not always yield desirable results, emphasizing the importance of a strong pre-trained base LLM for producing factual content. Tuning-based methods such as LIMA still require tuning the weights of LLMs and consequently face the limitations described above when the model size is too large or we need to frequently align base LLMs for evaluation. A concurrent work (Duan et al., 2023) also explores the similarity between ICL and instruction-tuning in their effects on downstream tasks by analyzing the LLMs' hidden states. As for the theory of alignment, these studies only indirectly suggest the promise of the superficial alignment hypothesis but do not directly show where and when the alignment tuning significantly changes the model behavior. In this paper, we study the Preprint superficial alignment hypothesis more directly through the lens of token distribution shift, which directly exhibits the alignment effect and produces more detailed non-trivial findings.\n\n## CONCLUSION\nWe first substantiate the underlying hypothesis regarding the superficial nature of alignment tuning via detailed analysis on token distribution shifts between base and aligned LLMs. This analytical method allows us to clearly investigate which token positions are affected by alignment tuning, providing insights for developing more efficient alignment methods and further studying the science of LLMs. Inspired by these findings, we propose a strong baseline method for tuning-free alignment, URIAL, which aligns untuned base LLMs via in-context learning with a constant prompt. Experiments show that URIAL significantly reduce the gap between base LLMs and their aligned versions. Our contribution can be summarized as follows: \n\nAnalysis: To gain a deeper understanding of alignment tuning, we analyze the token distribution shift between base and aligned LLMs. We find that alignment predominantly affects a minimal portion of token selection, influencing primarily stylistic elements and safety disclaimers in just 5-8% of cases. On most of the token positions, aligned and base models concur on top-token choices. Also, we find that alignment tuning is much more critical for initial tokens than later tokens. \n\nMethods: We introduce a simple yet effective method for aligning base LLMs, URIAL. It utilizes only as few as three constant curated examples for ICL, yet it effectively aligns base LLMs and matches the performance of SFT+RLHF in some scenarios. We also discover that well-written, stylistic examples are more effective than semantically relevant ones that are dynamically retrieved. URIAL offers both efficiency and simplicity in aligning base LLMs without requiring fine-tuning. This method significantly conserves time and resources, which is especially beneficial when dealing with extremely large LMs or when base LLMs need frequent evaluation. Furthermore, it enables a deeper investigation into the knowledge and capabilities innate to these base LLMs, while fostering more precise and economical approaches to align them with their deficiencies. \n\nEvaluation: We develop a comprehensive and interpretable evaluation protocol, encompassing six aspects with verifiable judgments. We also release the annotations we gathered for community use in evaluation and training open-source LLM evaluators.",
            "reference_string": "[265608902 | Lin et al. | 2023 | Citations: 198]"
        },
        {
            "title": "ABC Align: Large Language Model Alignment for Safety & Accuracy",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 53,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.00307, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1418234173",
                    "name": "Gareth Seneque"
                },
                {
                    "authorId": "2314116496",
                    "name": "Lap-Hang Ho"
                },
                {
                    "authorId": "2314117335",
                    "name": "Ariel Kuperman"
                },
                {
                    "authorId": "8513243",
                    "name": "Nafise Erfanian Saeedi"
                },
                {
                    "authorId": "2314113668",
                    "name": "Jeffrey Molendijk"
                }
            ],
            "abstract": "Alignment of Large Language Models (LLMs) remains an unsolved problem. Human preferences are highly distributed and can be captured at multiple levels of abstraction, from the individual to diverse populations. Organisational preferences, represented by standards and principles, are defined to mitigate reputational risk or meet legislative obligations. In this paper, we present ABC Align, a novel alignment methodology for LLMs that enables integration of the standards and preferences of a large media organisation into the LLM itself. We combine a set of data and methods that build on recent breakthroughs in synthetic data generation, preference optimisation, and post-training model quantisation. Our unified approach mitigates bias and improves accuracy, while preserving reasoning capability, as measured against standard benchmarks.",
            "corpus_id": 271600915,
            "sentences": [
                {
                    "corpus_id": "271600915",
                    "title": "ABC Align: Large Language Model Alignment for Safety & Accuracy",
                    "text": "There has been significant recent work on exploiting the ICL capabilities of frontier models (Agarwal et al., 2024) where the context length has increased by up to an order-of-magnitude, as seen for instance in Google's Gemini and Anthropic's Claude series. While performance of fine-tuning in an ICL setting doesn't yet offer consistent improvements when compared with SFT-based approaches across all tasks, there are specific cases where results are promising (Razumovskaia et al., 2024). \n\nThe work of Lin et al. (2023) further quantifies the 'Superficial Alignment Hypothesis' proposed in LIMA. They measure the token distribution shift between pre-trained and aligned models, specifically the difference in probabilities of predicted tokens given a position in some sample input across the two models. They find that alignment fine-tuning affects a small subset of tokens that they describe as 'stylistic', specifically 'discourse markers, transitional words, and safety disclaimers'. \n\nFurther to the original hypothesis of the LIMA authors, this provides evidence and motivation for the effectiveness of alignment happening in a post-training setting via fine-tuning, and that inference-time alignment may offer comparable results for applications where fine-tuning is unsuitable or not possible. This further validates our decision to not pre-train an LLM, but instead focus on post-training alignment across SFT, PO, and ICA settings in support of our overall aim to provide a lightweight, flexible methodology that considers organisational constraints and the rapid growth of underlying model capabilities.",
                    "score": 0.3715875864212569,
                    "section_title": "In-Context Alignment",
                    "char_start_offset": 14781,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 257
                        },
                        {
                            "start": 258,
                            "end": 490
                        },
                        {
                            "start": 493,
                            "end": 598
                        },
                        {
                            "start": 599,
                            "end": 806
                        },
                        {
                            "start": 807,
                            "end": 989
                        },
                        {
                            "start": 992,
                            "end": 1303
                        },
                        {
                            "start": 1304,
                            "end": 1616
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.81689453125
                }
            ],
            "relevance_judgement": 0.81689453125,
            "relevance_judgment_input_expanded": "# Title: ABC Align: Large Language Model Alignment for Safety & Accuracy\n# Venue: arXiv.org\n# Authors: Gareth Seneque, Lap-Hang Ho, Ariel Kuperman, Nafise Erfanian Saeedi, Jeffrey Molendijk\n## Abstract\nAlignment of Large Language Models (LLMs) remains an unsolved problem. Human preferences are highly distributed and can be captured at multiple levels of abstraction, from the individual to diverse populations. Organisational preferences, represented by standards and principles, are defined to mitigate reputational risk or meet legislative obligations. In this paper, we present ABC Align, a novel alignment methodology for LLMs that enables integration of the standards and preferences of a large media organisation into the LLM itself. We combine a set of data and methods that build on recent breakthroughs in synthetic data generation, preference optimisation, and post-training model quantisation. Our unified approach mitigates bias and improves accuracy, while preserving reasoning capability, as measured against standard benchmarks.\n## In-Context Alignment\nThere has been significant recent work on exploiting the ICL capabilities of frontier models (Agarwal et al., 2024) where the context length has increased by up to an order-of-magnitude, as seen for instance in Google's Gemini and Anthropic's Claude series. While performance of fine-tuning in an ICL setting doesn't yet offer consistent improvements when compared with SFT-based approaches across all tasks, there are specific cases where results are promising (Razumovskaia et al., 2024). \n\nThe work of Lin et al. (2023) further quantifies the 'Superficial Alignment Hypothesis' proposed in LIMA. They measure the token distribution shift between pre-trained and aligned models, specifically the difference in probabilities of predicted tokens given a position in some sample input across the two models. They find that alignment fine-tuning affects a small subset of tokens that they describe as 'stylistic', specifically 'discourse markers, transitional words, and safety disclaimers'. \n\nFurther to the original hypothesis of the LIMA authors, this provides evidence and motivation for the effectiveness of alignment happening in a post-training setting via fine-tuning, and that inference-time alignment may offer comparable results for applications where fine-tuning is unsuitable or not possible. This further validates our decision to not pre-train an LLM, but instead focus on post-training alignment across SFT, PO, and ICA settings in support of our overall aim to provide a lightweight, flexible methodology that considers organisational constraints and the rapid growth of underlying model capabilities.",
            "reference_string": "[271600915 | Seneque et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Hierarchical Contextual Manifold Alignment for Structuring Latent Representations in Large Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 27,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.03766, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2344240623",
                    "name": "Meiquan Dong"
                },
                {
                    "authorId": "2344792621",
                    "name": "Haoran Liu"
                },
                {
                    "authorId": "2344170357",
                    "name": "Yan Huang"
                },
                {
                    "authorId": "2344078959",
                    "name": "Zixuan Feng"
                },
                {
                    "authorId": "2344268240",
                    "name": "Jianhong Tang"
                },
                {
                    "authorId": "2344098516",
                    "name": "Ruoxi Wang"
                }
            ],
            "abstract": "The organization of latent token representations plays a crucial role in determining the stability, generalization, and contextual consistency of language models, yet conventional approaches to embedding refinement often rely on parameter modifications that introduce additional computational overhead. A hierarchical alignment method was introduced to restructure token embeddings without altering core model weights, ensuring that representational distributions maintained coherence across different linguistic contexts. Experimental evaluations demonstrated improvements in rare token retrieval, adversarial robustness, and long-range dependency tracking, highlighting the advantages of hierarchical structuring in mitigating inconsistencies in latent space organization. The comparative analysis against conventional fine-tuning and embedding perturbation methods revealed that hierarchical restructuring maintained computational efficiency while achieving measurable gains in representation quality. Structural refinements introduced through the alignment process resulted in improved contextual stability across varied linguistic tasks, reducing inconsistencies in token proximity relationships and enhancing interpretability in language generation. A detailed computational assessment confirmed that the realignment process introduced minimal inference overhead, ensuring that representational improvements did not compromise model efficiency. The findings reinforced the broader significance of structured representation learning, illustrating that hierarchical embedding modifications could serve as an effective strategy for refining latent space distributions while preserving pre-learned semantic associations.",
            "corpus_id": 276161664,
            "sentences": [
                {
                    "corpus_id": "276161664",
                    "title": "Hierarchical Contextual Manifold Alignment for Structuring Latent Representations in Large Language Models",
                    "text": "Alignment provided a structured approach for refining token embeddings within the latent space of a large language model, ensuring that representations exhibited improved coherence while maintaining linguistic integrity across diverse tasks. The results demonstrated that hierarchical restructuring reduced fragmentation within token distributions, contributing to enhanced semantic stability, increased retrieval accuracy for rare tokens, and greater consistency in long-range dependencies, which collectively improved the quality of contextual inference. The observed benefits extended beyond conventional fine-tuning techniques, as the proposed realignment strategy maintained pre-learned knowledge while refining structural organization in a manner that preserved essential contextual relationships and mitigated representational inconsistencies. The comparative analysis against existing embedding refinement methods established that a structured, non-parametric approach to representation alignment not only yielded improvements in token coherence but also reduced computational overhead associated with direct parameter modifications, highlighting the efficiency of hierarchical structuring in latent space optimization. The increased robustness to adversarial perturbations further suggested that systematic organization of token embeddings contributed to greater linguistic resilience, preventing spurious shifts in interpretation caused through minor syntactic or lexical variations in input prompts. The improvements in computational efficiency demonstrated that hierarchical realignment techniques introduced minimal resource overhead while yielding measurable gains in representation quality, indicating that non-parametric latent space modifications can serve as a viable alternative to more computationally intensive fine-tuning strategies. The structural organization imposed through hierarchical clustering and geodesic realignment ensured that semantically related tokens maintained consistent spatial relationships, reducing redundancy in representational distributions while preserving contextual distinctions critical for language understanding and text generation. The experimental findings reinforced the broader significance of structured latent space optimization in improving generalization capabilities across linguistic domains, illustrating that nonparametric embedding refinements can play a fundamental role in enhancing the internal representation quality of large language models.",
                    "score": 0.3685614657849739,
                    "section_title": "VII. CONCLUSION The introduction of Hierarchical Contextual Manifold",
                    "char_start_offset": 34661,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 241
                        },
                        {
                            "start": 242,
                            "end": 556
                        },
                        {
                            "start": 557,
                            "end": 850
                        },
                        {
                            "start": 851,
                            "end": 1227
                        },
                        {
                            "start": 1228,
                            "end": 1510
                        },
                        {
                            "start": 1511,
                            "end": 1855
                        },
                        {
                            "start": 1856,
                            "end": 2186
                        },
                        {
                            "start": 2187,
                            "end": 2513
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.70068359375
                },
                {
                    "corpus_id": "276161664",
                    "title": "Hierarchical Contextual Manifold Alignment for Structuring Latent Representations in Large Language Models",
                    "text": "The introduction of Hierarchical Contextual Manifold Alignment (HCMA) demonstrated a measurable impact on the structural organization of token embeddings, leading to improvements in contextual coherence, semantic alignment, and overall model stability across diverse linguistic tasks. The refinement of token representations through hierarchical realignment contributed to a more interpretable and functionally consistent latent space, reinforcing the hypothesis that nonparametric adjustments to token distributions can enhance generalization without requiring direct parameter modifications. Observations across multiple evaluation criteria revealed that HCMA effectively improved representation stability for both frequent and infrequent tokens while preserving the integrity of pre-learned semantic relationships. The observed gains in rare token retrieval accuracy and long-range contextual consistency suggested that hierarchical organization played a crucial role in mitigating representational fragmentation, a challenge that frequently arises in models trained on heterogeneous data distributions. The ability of HCMA to enhance robustness to adversarial perturbations further supported the argument that latent space structuring contributed to improved linguistic resilience, reducing the likelihood of unintended interpretational shifts caused through syntactic modifications or lexical perturbations. \n\nThe computational requirements associated with embedding realignment remained within practical bounds, ensuring that the proposed methodology maintained efficiency despite the introduction of additional processing steps. The increase in embedding preprocessing time was offset through optimizations in cluster formation and parallelized realignment procedures, mitigating concerns regarding scalability when extending HCMA to larger model architectures. The hierarchical nature of the alignment process enabled efficient segmentation of token embeddings, ensuring that realignment operations were conducted within locally constrained subregions rather than across the entire representation space. This localized approach minimized computational overhead while preserving the global structure of token embeddings, allowing for controlled refinements that maintained alignment with prelearned linguistic priors. The memory footprint associated with HCMA remained within acceptable limits, suggesting that hierarchical realignment strategies could be applied across larger-scale architectures without prohibitive increases in resource consumption. The trade-off between structural improvements and computational efficiency highlighted the viability of non-parametric embedding adjustments as a means of enhancing representational consistency without introducing the challenges associated with full-scale retraining.",
                    "score": 0.35496973611042026,
                    "section_title": "VI. DISCUSSIONS",
                    "char_start_offset": 30420,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 284
                        },
                        {
                            "start": 285,
                            "end": 593
                        },
                        {
                            "start": 594,
                            "end": 817
                        },
                        {
                            "start": 818,
                            "end": 1106
                        },
                        {
                            "start": 1107,
                            "end": 1412
                        },
                        {
                            "start": 1415,
                            "end": 1635
                        },
                        {
                            "start": 1636,
                            "end": 1868
                        },
                        {
                            "start": 1869,
                            "end": 2111
                        },
                        {
                            "start": 2112,
                            "end": 2324
                        },
                        {
                            "start": 2325,
                            "end": 2559
                        },
                        {
                            "start": 2560,
                            "end": 2827
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.67041015625
                },
                {
                    "corpus_id": "276161664",
                    "title": "Hierarchical Contextual Manifold Alignment for Structuring Latent Representations in Large Language Models",
                    "text": "The organization of latent token representations plays a crucial role in determining the stability, generalization, and contextual consistency of language models, yet conventional approaches to embedding refinement often rely on parameter modifications that introduce additional computational overhead. A hierarchical alignment method was introduced to restructure token embeddings without altering core model weights, ensuring that representational distributions maintained coherence across different linguistic contexts. Experimental evaluations demonstrated improvements in rare token retrieval, adversarial robustness, and long-range dependency tracking, highlighting the advantages of hierarchical structuring in mitigating inconsistencies in latent space organization. The comparative analysis against conventional fine-tuning and embedding perturbation methods revealed that hierarchical restructuring maintained computational efficiency while achieving measurable gains in representation quality. Structural refinements introduced through the alignment process resulted in improved contextual stability across varied linguistic tasks, reducing inconsistencies in token proximity relationships and enhancing interpretability in language generation. A detailed computational assessment confirmed that the realignment process introduced minimal inference overhead, ensuring that representational improvements did not compromise model efficiency. The findings reinforced the broader significance of structured representation learning, illustrating that hierarchical embedding modifications could serve as an effective strategy for refining latent space distributions while preserving pre-learned semantic associations.",
                    "score": 0.3382681265095552,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.51513671875
                },
                {
                    "corpus_id": "276161664",
                    "title": "Hierarchical Contextual Manifold Alignment for Structuring Latent Representations in Large Language Models",
                    "text": "The empirical evaluation assessed the impact of Hierarchical Contextual Manifold Alignment (HCMA) on token representation coherence, structural consistency of latent spaces, and downstream language modeling performance. The results were analyzed across multiple dimensions, including modifications to token distribution properties, improvements in embedding organization, and comparative performance metrics against existing latent space refinement techniques. The experimental outcomes provided a comprehensive assessment of the degree to which HCMA improved structural integrity within token embeddings while maintaining overall model performance across different language modeling benchmarks. The findings were further contextualized through a comparative analysis with alternative methods to highlight advantages and tradeoffs associated with hierarchical realignment.",
                    "score": 0.3607278251674272,
                    "section_title": "V. RESULTS",
                    "char_start_offset": 24430,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 219
                        },
                        {
                            "start": 220,
                            "end": 460
                        },
                        {
                            "start": 461,
                            "end": 695
                        },
                        {
                            "start": 696,
                            "end": 872
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.423828125
                },
                {
                    "corpus_id": "276161664",
                    "title": "Hierarchical Contextual Manifold Alignment for Structuring Latent Representations in Large Language Models",
                    "text": "A structured and coherent latent space representation plays a fundamental role in determining the contextual consistency and generalization capacity of large language models. The introduction of Hierarchical Contextual Manifold Alignment (HCMA) provides a non-parametric framework for restructuring token embeddings through hierarchical organization, ensuring smoother representation transitions and improved token locality within the model's latent space. Unlike conventional approaches that rely on direct fine-tuning or reinforcementbased parameter updates, HCMA applies a structured transformation to the learned token embeddings without modifying the core model parameters, preserving prior knowledge while enhancing representational alignment. The methodology employs manifold learning techniques to infer token distribution topology, followed by an iterative realignment process that optimizes spatial coherence while maintaining semantic consistency across linguistic structures.",
                    "score": 0.35010142346480366,
                    "section_title": "III. HIERARCHICAL CONTEXTUAL MANIFOLD ALIGNMENT",
                    "char_start_offset": 14496,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 174
                        },
                        {
                            "start": 175,
                            "end": 456
                        },
                        {
                            "start": 457,
                            "end": 749
                        },
                        {
                            "start": 750,
                            "end": 987
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.3798828125
                }
            ],
            "relevance_judgement": 0.70068359375,
            "relevance_judgment_input_expanded": "# Title: Hierarchical Contextual Manifold Alignment for Structuring Latent Representations in Large Language Models\n# Venue: arXiv.org\n# Authors: Meiquan Dong, Haoran Liu, Yan Huang, Zixuan Feng, Jianhong Tang, Ruoxi Wang\n## Abstract\nThe organization of latent token representations plays a crucial role in determining the stability, generalization, and contextual consistency of language models, yet conventional approaches to embedding refinement often rely on parameter modifications that introduce additional computational overhead. A hierarchical alignment method was introduced to restructure token embeddings without altering core model weights, ensuring that representational distributions maintained coherence across different linguistic contexts. Experimental evaluations demonstrated improvements in rare token retrieval, adversarial robustness, and long-range dependency tracking, highlighting the advantages of hierarchical structuring in mitigating inconsistencies in latent space organization. The comparative analysis against conventional fine-tuning and embedding perturbation methods revealed that hierarchical restructuring maintained computational efficiency while achieving measurable gains in representation quality. Structural refinements introduced through the alignment process resulted in improved contextual stability across varied linguistic tasks, reducing inconsistencies in token proximity relationships and enhancing interpretability in language generation. A detailed computational assessment confirmed that the realignment process introduced minimal inference overhead, ensuring that representational improvements did not compromise model efficiency. The findings reinforced the broader significance of structured representation learning, illustrating that hierarchical embedding modifications could serve as an effective strategy for refining latent space distributions while preserving pre-learned semantic associations.\n## III. HIERARCHICAL CONTEXTUAL MANIFOLD ALIGNMENT\nA structured and coherent latent space representation plays a fundamental role in determining the contextual consistency and generalization capacity of large language models. The introduction of Hierarchical Contextual Manifold Alignment (HCMA) provides a non-parametric framework for restructuring token embeddings through hierarchical organization, ensuring smoother representation transitions and improved token locality within the model's latent space. Unlike conventional approaches that rely on direct fine-tuning or reinforcementbased parameter updates, HCMA applies a structured transformation to the learned token embeddings without modifying the core model parameters, preserving prior knowledge while enhancing representational alignment. The methodology employs manifold learning techniques to infer token distribution topology, followed by an iterative realignment process that optimizes spatial coherence while maintaining semantic consistency across linguistic structures.\n\n## V. RESULTS\nThe empirical evaluation assessed the impact of Hierarchical Contextual Manifold Alignment (HCMA) on token representation coherence, structural consistency of latent spaces, and downstream language modeling performance. The results were analyzed across multiple dimensions, including modifications to token distribution properties, improvements in embedding organization, and comparative performance metrics against existing latent space refinement techniques. The experimental outcomes provided a comprehensive assessment of the degree to which HCMA improved structural integrity within token embeddings while maintaining overall model performance across different language modeling benchmarks. The findings were further contextualized through a comparative analysis with alternative methods to highlight advantages and tradeoffs associated with hierarchical realignment.\n\n## VI. DISCUSSIONS\nThe introduction of Hierarchical Contextual Manifold Alignment (HCMA) demonstrated a measurable impact on the structural organization of token embeddings, leading to improvements in contextual coherence, semantic alignment, and overall model stability across diverse linguistic tasks. The refinement of token representations through hierarchical realignment contributed to a more interpretable and functionally consistent latent space, reinforcing the hypothesis that nonparametric adjustments to token distributions can enhance generalization without requiring direct parameter modifications. Observations across multiple evaluation criteria revealed that HCMA effectively improved representation stability for both frequent and infrequent tokens while preserving the integrity of pre-learned semantic relationships. The observed gains in rare token retrieval accuracy and long-range contextual consistency suggested that hierarchical organization played a crucial role in mitigating representational fragmentation, a challenge that frequently arises in models trained on heterogeneous data distributions. The ability of HCMA to enhance robustness to adversarial perturbations further supported the argument that latent space structuring contributed to improved linguistic resilience, reducing the likelihood of unintended interpretational shifts caused through syntactic modifications or lexical perturbations. \n\nThe computational requirements associated with embedding realignment remained within practical bounds, ensuring that the proposed methodology maintained efficiency despite the introduction of additional processing steps. The increase in embedding preprocessing time was offset through optimizations in cluster formation and parallelized realignment procedures, mitigating concerns regarding scalability when extending HCMA to larger model architectures. The hierarchical nature of the alignment process enabled efficient segmentation of token embeddings, ensuring that realignment operations were conducted within locally constrained subregions rather than across the entire representation space. This localized approach minimized computational overhead while preserving the global structure of token embeddings, allowing for controlled refinements that maintained alignment with prelearned linguistic priors. The memory footprint associated with HCMA remained within acceptable limits, suggesting that hierarchical realignment strategies could be applied across larger-scale architectures without prohibitive increases in resource consumption. The trade-off between structural improvements and computational efficiency highlighted the viability of non-parametric embedding adjustments as a means of enhancing representational consistency without introducing the challenges associated with full-scale retraining.\n\n## VII. CONCLUSION The introduction of Hierarchical Contextual Manifold\nAlignment provided a structured approach for refining token embeddings within the latent space of a large language model, ensuring that representations exhibited improved coherence while maintaining linguistic integrity across diverse tasks. The results demonstrated that hierarchical restructuring reduced fragmentation within token distributions, contributing to enhanced semantic stability, increased retrieval accuracy for rare tokens, and greater consistency in long-range dependencies, which collectively improved the quality of contextual inference. The observed benefits extended beyond conventional fine-tuning techniques, as the proposed realignment strategy maintained pre-learned knowledge while refining structural organization in a manner that preserved essential contextual relationships and mitigated representational inconsistencies. The comparative analysis against existing embedding refinement methods established that a structured, non-parametric approach to representation alignment not only yielded improvements in token coherence but also reduced computational overhead associated with direct parameter modifications, highlighting the efficiency of hierarchical structuring in latent space optimization. The increased robustness to adversarial perturbations further suggested that systematic organization of token embeddings contributed to greater linguistic resilience, preventing spurious shifts in interpretation caused through minor syntactic or lexical variations in input prompts. The improvements in computational efficiency demonstrated that hierarchical realignment techniques introduced minimal resource overhead while yielding measurable gains in representation quality, indicating that non-parametric latent space modifications can serve as a viable alternative to more computationally intensive fine-tuning strategies. The structural organization imposed through hierarchical clustering and geodesic realignment ensured that semantically related tokens maintained consistent spatial relationships, reducing redundancy in representational distributions while preserving contextual distinctions critical for language understanding and text generation. The experimental findings reinforced the broader significance of structured latent space optimization in improving generalization capabilities across linguistic domains, illustrating that nonparametric embedding refinements can play a fundamental role in enhancing the internal representation quality of large language models.",
            "reference_string": "[276161664 | Dong et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Take Off the Training Wheels! Progressive In-Context Learning for Effective Alignment",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2025,
            "reference_count": 22,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.09958, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2230018369",
                    "name": "Zhenyu Liu"
                },
                {
                    "authorId": "2265618386",
                    "name": "Dongfang Li"
                },
                {
                    "authorId": "2149467818",
                    "name": "Xinshuo Hu"
                },
                {
                    "authorId": "2326046038",
                    "name": "Xinping Zhao"
                },
                {
                    "authorId": "2325888345",
                    "name": "Yibin Chen"
                },
                {
                    "authorId": "2285172247",
                    "name": "Baotian Hu"
                },
                {
                    "authorId": "2258690227",
                    "name": "Min Zhang"
                }
            ],
            "abstract": "Recent studies have explored the working mechanisms of In-Context Learning (ICL). However, they mainly focus on classification and simple generation tasks, limiting their broader application to more complex generation tasks in practice. To address this gap, we investigate the impact of demonstrations on token representations within the practical alignment tasks. We find that the transformer embeds the task function learned from demonstrations into the separator token representation, which plays an important role in the generation of prior response tokens. Once the prior response tokens are determined, the demonstrations become redundant. Motivated by this finding, we propose an efficient Progressive In-Context Alignment (PICA) method consisting of two stages. In the first few-shot stage, the model generates several prior response tokens via standard ICL while concurrently extracting the ICL vector that stores the task function from the separator token representation. In the following zero-shot stage, this ICL vector guides the model to generate responses without further demonstrations. Extensive experiments demonstrate that our PICA not only surpasses vanilla ICL but also achieves comparable performance to other alignment tuning methods. The proposed training-free method reduces the time cost (e.g., 5.45\u00d7) with improved alignment performance (e.g., 6.57+). Consequently, our work highlights the application of ICL for alignment and calls for a deeper understanding of ICL for complex generations. The code will be available at https://github.com/HITsz-TMG/PICA.",
            "corpus_id": 273901687,
            "sentences": [
                {
                    "corpus_id": "273901687",
                    "title": "Take Off the Training Wheels! Progressive In-Context Learning for Effective Alignment",
                    "text": "In this section, we aim to shed light on the working mechanisms of in-context learning by investigating the following question: What is the impact of demonstration on token representation in incontext alignment? To explore this, we design a comparative experiment to highlight how token representations differ between zero-shot and few-shot settings. We use token probability distributions as a proxy for token representations and utilize KL-divergence to measure the shifts in these distributions. By visualizing and quantifying the shifts in token probability distributions caused by demonstrations, we can understand the role of demonstrations in aligning the model and provide further optimization for in-context alignment. \n\nRegarding the experimental setup, we randomly selected 100 data instances of similar length from Ultra-chat (Ding et al., 2023), a commonly used dataset for alignment tuning, as our experimental dataset. For the input prompt, we use a straightforward design by adding several tokens at the end of the query to serve as separator tokens, explicitly distinguishing between the query and the response. We present the visualization results based on the Llama2-7b model in the Figure 1, while the results for other models are provided in Appendix C. We break the token distribution of the whole instance into the input and output parts. A straightforward reason is that the input token distribution shift represents differences in understanding the instruction, while the output token distribution shift represents the ability to respond. By observing and analyzing the visualization, we have two hypotheses: (1) the ICL alignment task function might be encoded into the separator token representation. (2) the quality of response is highly reliant on the quality of prior response tokens. \n\nInput Token Distribution. By comparing the input token probability distributions between zeroshot and few-shot settings, a significant shift is observed in both the prior tokens of the query and the separator tokens. The KL-divergence decreases as the number of query tokens increases. By comparing the experimental group and the control group, we find that the shift in the query distribution also occurs in the control group. However, this shift in the separator tokens is not consistent across different demonstration settings, suggesting distinct underlying causes for these shifts.",
                    "score": 0.3541988317537238,
                    "section_title": "Motivation",
                    "char_start_offset": 7828,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 211
                        },
                        {
                            "start": 212,
                            "end": 350
                        },
                        {
                            "start": 351,
                            "end": 498
                        },
                        {
                            "start": 499,
                            "end": 727
                        },
                        {
                            "start": 730,
                            "end": 933
                        },
                        {
                            "start": 934,
                            "end": 1128
                        },
                        {
                            "start": 1129,
                            "end": 1361
                        },
                        {
                            "start": 1362,
                            "end": 1563
                        },
                        {
                            "start": 1564,
                            "end": 1727
                        },
                        {
                            "start": 1728,
                            "end": 1814
                        },
                        {
                            "start": 1817,
                            "end": 1842
                        },
                        {
                            "start": 1843,
                            "end": 2033
                        },
                        {
                            "start": 2034,
                            "end": 2102
                        },
                        {
                            "start": 2103,
                            "end": 2244
                        },
                        {
                            "start": 2245,
                            "end": 2403
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 838,
                            "end": 857,
                            "matchedPaperCorpusId": "258840897"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.677734375
                },
                {
                    "corpus_id": "273901687",
                    "title": "Take Off the Training Wheels! Progressive In-Context Learning for Effective Alignment",
                    "text": "However, this shift in the separator tokens is not consistent across different demonstration settings, suggesting distinct underlying causes for these shifts. We attribute the shift in the query's prior token distribution to a \"context shift\", and we attribute the shift in the separator tokens distribution to a \"task shift\". Given that LLMs are trained to predict the next token based on the provided context, altering the context directly impacts the token distribution, which we refer to as the \"context shift\". However, as the number of query tokens increases, the decision space gradually aligns for both zero-shot and few-shot settings, leading to higher consistency in query token prediction and thus a reduced KL-divergence. \n\nOn the contrary, the trend observed in the query distribution is not mirrored in the separator token distribution. In the control group, the separator token representations remain highly similar. We attribute the large KL-divergence observed in the separator token distribution of the experimental group to the differing tasks, indicating that separator tokens likely encode task-specific information during ICL. We reasonably speculate that the primary impact of demonstration on instruction understanding is reflected in the encoding of separator tokens, where the alignment task function learned through ICL is stored. This hypothesis aligns with prior work (Hendel et al., 2023;Li et al., 2024), yet our findings contribute additional evidence supporting this perspective. \n\nOutput Token Distribution. Observing the visualization of output token distribution, we find that when comparing zero-shot and few-shot settings, the response token distribution shows similarity in the posterior tokens. This indicates that the model selects posterior tokens with high consistency in both zero-shot and few-shot settings. \n\nWhen comparing the prior response tokens of the experimental group and the control group, we observe a pattern similar to that of the separator tokens, suggesting that demonstrations play a crucial role in the prior response tokens. Based on these observations and analyses, we speculate that the primary impact of demonstrations on response generation is reflected in the generation of prior answer tokens. Compared to zero-shot settings, demonstrations guide the generation of accurate prior response tokens, which implicitly helps the model successfully follow the instructions.",
                    "score": 0.3462624938294461,
                    "section_title": "Motivation",
                    "char_start_offset": 10073,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 158
                        },
                        {
                            "start": 159,
                            "end": 326
                        },
                        {
                            "start": 327,
                            "end": 515
                        },
                        {
                            "start": 516,
                            "end": 733
                        },
                        {
                            "start": 736,
                            "end": 850
                        },
                        {
                            "start": 851,
                            "end": 931
                        },
                        {
                            "start": 932,
                            "end": 1148
                        },
                        {
                            "start": 1149,
                            "end": 1357
                        },
                        {
                            "start": 1358,
                            "end": 1512
                        },
                        {
                            "start": 1515,
                            "end": 1541
                        },
                        {
                            "start": 1542,
                            "end": 1734
                        },
                        {
                            "start": 1735,
                            "end": 1852
                        },
                        {
                            "start": 1855,
                            "end": 2087
                        },
                        {
                            "start": 2088,
                            "end": 2262
                        },
                        {
                            "start": 2263,
                            "end": 2436
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1397,
                            "end": 1418,
                            "matchedPaperCorpusId": "264439386"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.47900390625
                }
            ],
            "relevance_judgement": 0.677734375,
            "relevance_judgment_input_expanded": "# Title: Take Off the Training Wheels! Progressive In-Context Learning for Effective Alignment\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Zhenyu Liu, Dongfang Li, Xinshuo Hu, Xinping Zhao, Yibin Chen, Baotian Hu, Min Zhang\n## Abstract\nRecent studies have explored the working mechanisms of In-Context Learning (ICL). However, they mainly focus on classification and simple generation tasks, limiting their broader application to more complex generation tasks in practice. To address this gap, we investigate the impact of demonstrations on token representations within the practical alignment tasks. We find that the transformer embeds the task function learned from demonstrations into the separator token representation, which plays an important role in the generation of prior response tokens. Once the prior response tokens are determined, the demonstrations become redundant. Motivated by this finding, we propose an efficient Progressive In-Context Alignment (PICA) method consisting of two stages. In the first few-shot stage, the model generates several prior response tokens via standard ICL while concurrently extracting the ICL vector that stores the task function from the separator token representation. In the following zero-shot stage, this ICL vector guides the model to generate responses without further demonstrations. Extensive experiments demonstrate that our PICA not only surpasses vanilla ICL but also achieves comparable performance to other alignment tuning methods. The proposed training-free method reduces the time cost (e.g., 5.45\u00d7) with improved alignment performance (e.g., 6.57+). Consequently, our work highlights the application of ICL for alignment and calls for a deeper understanding of ICL for complex generations. The code will be available at https://github.com/HITsz-TMG/PICA.\n## Motivation\nIn this section, we aim to shed light on the working mechanisms of in-context learning by investigating the following question: What is the impact of demonstration on token representation in incontext alignment? To explore this, we design a comparative experiment to highlight how token representations differ between zero-shot and few-shot settings. We use token probability distributions as a proxy for token representations and utilize KL-divergence to measure the shifts in these distributions. By visualizing and quantifying the shifts in token probability distributions caused by demonstrations, we can understand the role of demonstrations in aligning the model and provide further optimization for in-context alignment. \n\nRegarding the experimental setup, we randomly selected 100 data instances of similar length from Ultra-chat (Ding et al., 2023), a commonly used dataset for alignment tuning, as our experimental dataset. For the input prompt, we use a straightforward design by adding several tokens at the end of the query to serve as separator tokens, explicitly distinguishing between the query and the response. We present the visualization results based on the Llama2-7b model in the Figure 1, while the results for other models are provided in Appendix C. We break the token distribution of the whole instance into the input and output parts. A straightforward reason is that the input token distribution shift represents differences in understanding the instruction, while the output token distribution shift represents the ability to respond. By observing and analyzing the visualization, we have two hypotheses: (1) the ICL alignment task function might be encoded into the separator token representation. (2) the quality of response is highly reliant on the quality of prior response tokens. \n\nInput Token Distribution. By comparing the input token probability distributions between zeroshot and few-shot settings, a significant shift is observed in both the prior tokens of the query and the separator tokens. The KL-divergence decreases as the number of query tokens increases. By comparing the experimental group and the control group, we find that the shift in the query distribution also occurs in the control group. However, this shift in the separator tokens is not consistent across different demonstration settings, suggesting distinct underlying causes for these shifts.\n...\nHowever, this shift in the separator tokens is not consistent across different demonstration settings, suggesting distinct underlying causes for these shifts. We attribute the shift in the query's prior token distribution to a \"context shift\", and we attribute the shift in the separator tokens distribution to a \"task shift\". Given that LLMs are trained to predict the next token based on the provided context, altering the context directly impacts the token distribution, which we refer to as the \"context shift\". However, as the number of query tokens increases, the decision space gradually aligns for both zero-shot and few-shot settings, leading to higher consistency in query token prediction and thus a reduced KL-divergence. \n\nOn the contrary, the trend observed in the query distribution is not mirrored in the separator token distribution. In the control group, the separator token representations remain highly similar. We attribute the large KL-divergence observed in the separator token distribution of the experimental group to the differing tasks, indicating that separator tokens likely encode task-specific information during ICL. We reasonably speculate that the primary impact of demonstration on instruction understanding is reflected in the encoding of separator tokens, where the alignment task function learned through ICL is stored. This hypothesis aligns with prior work (Hendel et al., 2023;Li et al., 2024), yet our findings contribute additional evidence supporting this perspective. \n\nOutput Token Distribution. Observing the visualization of output token distribution, we find that when comparing zero-shot and few-shot settings, the response token distribution shows similarity in the posterior tokens. This indicates that the model selects posterior tokens with high consistency in both zero-shot and few-shot settings. \n\nWhen comparing the prior response tokens of the experimental group and the control group, we observe a pattern similar to that of the separator tokens, suggesting that demonstrations play a crucial role in the prior response tokens. Based on these observations and analyses, we speculate that the primary impact of demonstrations on response generation is reflected in the generation of prior answer tokens. Compared to zero-shot settings, demonstrations guide the generation of accurate prior response tokens, which implicitly helps the model successfully follow the instructions.",
            "reference_string": "[273901687 | Liu et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Superficial Safety Alignment Hypothesis",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 82,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.10862, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2326007326",
                    "name": "Jianwei Li"
                },
                {
                    "authorId": "2326001415",
                    "name": "Jung-Eun Kim"
                }
            ],
            "abstract": "As large language models (LLMs) are overwhelmingly more and more integrated into various applications, ensuring they generate safe and aligned responses is a pressing need. Previous research on alignment has largely focused on general instruction-following but has often overlooked the unique properties and challenges of safety alignment, such as the brittleness of safety mechanisms. To bridge the gap, we propose the Superficial Safety Alignment Hypothesis (SSAH), which posits that safety alignment should teach an otherwise unsafe model to choose the correct reasoning direction - interpreted as a specialized binary classification task - and incorporate a refusal mechanism with multiple reserved fallback options. Furthermore, through SSAH, we hypothesize that safety guardrails in LLMs can be established by just a small number of essential components. To verify this, we conduct an ablation study and successfully identify four types of attribute-critical components in safety-aligned LLMs: Exclusive Safety Unit (ESU), Exclusive Utility Unit (EUU), Complex Unit (CU), and Redundant Unit (RU). Our findings show that freezing certain safety-critical components 7.5\\% during fine-tuning allows the model to retain its safety attributes while adapting to new tasks. Additionally, we show that leveraging redundant units 20\\% in the pre-trained model as an ``alignment budget'' can effectively minimize the alignment tax while achieving the alignment goal. All considered, this paper concludes that the atomic functional unit for safety in LLMs is at the neuron level and underscores that safety alignment should not be complicated. We believe this work contributes to the foundation of efficient and scalable safety alignment for future LLMs.",
            "corpus_id": 273350763,
            "sentences": [
                {
                    "corpus_id": "273350763",
                    "title": "Superficial Safety Alignment Hypothesis",
                    "text": "(2) Binary Classification in SSAH: While SAH suggests that general alignment helps models choose the correct output subdistribution, SSAH posits that safety alignment simplifies this further by focusing on a binary classification task: either fulfill a request (if safe) or refuse it (if unsafe). This simplified framing of safety alignment is consistent with the \"superficial\" nature of SAH, where the alignment process fine-tunes how the model behaves in response to queries, rather than altering its deep internal structures. \n\n(3) Refusal Mechanisms and Format Control: Just as general alignment teaches models to structure their outputs in a user-friendly way, safety alignment in SSAH teaches models to issue consistent refusal mechanisms. These refusals take the form of standardized responses that indicate the model's compliance with safety guidelines, much like how general alignment might guide a model to give well-structured, polite answers to other types of questions. Importantly, this refusal mechanism makes it easier to choose the appropriate subdistribution of the output format. \n\nThe Superficial Alignment Hypothesis (SAH) as outlined in the Zhou et al. ( 2024) provides a theoretical framework for understanding how alignment processes operate in large language models. It suggests that alignment is largely superficial, conditioning the model on how to use its pretrained knowledge effectively. The Superficial Safety Alignment Hypothesis (SSAH) builds on this by applying similar principles to the realm of safety, simplifying the task of safety alignment to binary decisions regarding the fulfillment or refusal of unsafe requests. Both hypotheses underscore that alignment does not deeply alter the core abilities of the model, but rather adjusts the way those abilities are applied in specific contexts.",
                    "score": 0.3663067360391083,
                    "section_title": "DISCUSSION, LIMITATION, AND CONCLUSION",
                    "char_start_offset": 35619,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 296
                        },
                        {
                            "start": 297,
                            "end": 528
                        },
                        {
                            "start": 531,
                            "end": 745
                        },
                        {
                            "start": 746,
                            "end": 982
                        },
                        {
                            "start": 983,
                            "end": 1098
                        },
                        {
                            "start": 1101,
                            "end": 1291
                        },
                        {
                            "start": 1292,
                            "end": 1417
                        },
                        {
                            "start": 1418,
                            "end": 1656
                        },
                        {
                            "start": 1657,
                            "end": 1830
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.67431640625
                },
                {
                    "corpus_id": "273350763",
                    "title": "Superficial Safety Alignment Hypothesis",
                    "text": "The Superficial Alignment Hypothesis (SAH), as proposed to Zhou et al. (2024), fundamentally challenges the traditional assumption that a language model requires extensive fine-tuning on instruction-following on preference data to align its responses with human expectation. Instead, SAH posits that the majority of a model's knowledge and capabilities are acquired during the pretraining phase, while the subsequent alignment phase primarily functions to guide the model's output format when interacting with users. This hypothesis implies that, for many tasks, fine-tuning on a small, carefully selected set of aligned data is sufficient to achieve strong performance as long as the pretraining stage has effectively captured the necessary underlying knowledge. The key assertion of SAH is that alignment is superficial, in the sense that: \n\n(1) Capabilities are Learned in Pretraining: During pretraining, the model acquires a vast amount of general-purpose knowledge from diverse datasets. These datasets contain implicit structures and information about language, reasoning, factual knowledge, and even ethical guidelines. \n\n(2) Alignment Guides Output Behavior: The alignment process is not responsible for teaching the model new knowledge or capabilities. Rather, it acts as a filter that directs the model to produce acceptable formats or styles of responses based on user queries, reflecting the correct subset of its vast pretrained knowledge. \n\n(3) For instance, when tasked with generating an informative response, the model must select a format that aligns with user expectations, such as providing clear instructions or explanations. However, the actual content of the response, e.g., factual knowledge, reasoning, and domain-specific expertise, stems from pretraining. The alignment stage merely teaches the model how to express that knowledge or when to refrain from providing information in inappropriate contexts. \n\nChallenges and Motivations Behind SAH. One of the primary motivations for introducing SAH was the observation that models tend to be capable of performing certain tasks after alignment finetuning on a minimal dataset. This observation challenges the need for extensive fine-tuning using reinforcement learning (e.g., RLHF) or large-scale human feedback, which can be computationally too expensive and time-consuming.",
                    "score": 0.4614144675953522,
                    "section_title": "DISCUSSION, LIMITATION, AND CONCLUSION",
                    "char_start_offset": 31564,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 274
                        },
                        {
                            "start": 275,
                            "end": 516
                        },
                        {
                            "start": 517,
                            "end": 763
                        },
                        {
                            "start": 764,
                            "end": 841
                        },
                        {
                            "start": 844,
                            "end": 993
                        },
                        {
                            "start": 994,
                            "end": 1127
                        },
                        {
                            "start": 1130,
                            "end": 1262
                        },
                        {
                            "start": 1263,
                            "end": 1453
                        },
                        {
                            "start": 1456,
                            "end": 1647
                        },
                        {
                            "start": 1648,
                            "end": 1783
                        },
                        {
                            "start": 1784,
                            "end": 1931
                        },
                        {
                            "start": 1934,
                            "end": 1972
                        },
                        {
                            "start": 1973,
                            "end": 2151
                        },
                        {
                            "start": 2152,
                            "end": 2350
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.66943359375
                },
                {
                    "corpus_id": "273350763",
                    "title": "Superficial Safety Alignment Hypothesis",
                    "text": "Discussion. While our SSAH offers valuable insights into adversarial scenarios, such as jailbreak attacks, we do not propose a specific solution to address these issues in this work. If these issues could be resolved within the framework of our theory, the term \"Superficial\" in \"Superficial Safety Alignment Hypothesis\" may no longer be necessary. Interestingly, recent research provides some supporting evidence in this direction (Qi et al., 2024). However, it is also highly likely that advanced attacks may not be fully mitigated by relying solely on the model's internal mechanisms. A systematic, multi-layered approach, extending beyond the model itself, may be required to effectively defend against sophisticated adversarial threats. \n\nLimitation. When reallocating redundant units for safety purposes, we only explored the impact of the alignment method SFT. Due to resource limitations, we have not yet tested this approach on other alignment methods like PPO or DPO. \n\nConclusion. This paper distinguishes safety alignment from the general alignment in LLMs and addresses the three key questions: How does safety alignment affect model behavior? Why are safety mechanisms brittle? and How to mitigate the safety alignment tax? By answering these questions, we were able to demonstrate that safety alignment can be a straightforward process, rather than a myth. \n\nA APPENDIX: SUPERFICIAL SAFETY ALIGNMENT HYPOTHESIS \n\nIn this section, we provide additional technical details and clarifications to supplement the experiments and findings presented in the Superficial Safety Alignment Hypothesis (SSAH) section. These details help ensure reproducibility and offer deeper insights into how the Superficial Alignment Hypothesis (SAH) was adapted to focus on safety-specific concerns. We also explain the methodology behind model configuration, fine-tuning, and evaluation. This appendix includes further discussion on how general instruction-following models and safety-aligned models were fine-tuned and assessed to probe their reasoning directions when faced with malicious queries, and the results of these assessments are presented in detail. \n\nA.1 SUPERFICIAL ALIGNMENT HYPOTHESIS IN LIMA. \n\nThe Superficial Alignment Hypothesis (SAH), as proposed to Zhou et al. (2024), fundamentally challenges the traditional assumption that a language model requires extensive fine-tuning on instruction-following on preference data to align its responses with human expectation.",
                    "score": 0.36256839269372604,
                    "section_title": "DISCUSSION, LIMITATION, AND CONCLUSION",
                    "char_start_offset": 29361,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 11
                        },
                        {
                            "start": 12,
                            "end": 182
                        },
                        {
                            "start": 183,
                            "end": 348
                        },
                        {
                            "start": 349,
                            "end": 450
                        },
                        {
                            "start": 451,
                            "end": 587
                        },
                        {
                            "start": 588,
                            "end": 741
                        },
                        {
                            "start": 744,
                            "end": 755
                        },
                        {
                            "start": 756,
                            "end": 867
                        },
                        {
                            "start": 868,
                            "end": 977
                        },
                        {
                            "start": 980,
                            "end": 991
                        },
                        {
                            "start": 992,
                            "end": 1156
                        },
                        {
                            "start": 1157,
                            "end": 1237
                        },
                        {
                            "start": 1238,
                            "end": 1371
                        },
                        {
                            "start": 1374,
                            "end": 1425
                        },
                        {
                            "start": 1428,
                            "end": 1619
                        },
                        {
                            "start": 1620,
                            "end": 1789
                        },
                        {
                            "start": 1790,
                            "end": 1878
                        },
                        {
                            "start": 1879,
                            "end": 2152
                        },
                        {
                            "start": 2155,
                            "end": 2200
                        },
                        {
                            "start": 2203,
                            "end": 2477
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.326171875
                }
            ],
            "relevance_judgement": 0.67431640625,
            "relevance_judgment_input_expanded": "# Title: Superficial Safety Alignment Hypothesis\n# Venue: arXiv.org\n# Authors: Jianwei Li, Jung-Eun Kim\n## Abstract\nAs large language models (LLMs) are overwhelmingly more and more integrated into various applications, ensuring they generate safe and aligned responses is a pressing need. Previous research on alignment has largely focused on general instruction-following but has often overlooked the unique properties and challenges of safety alignment, such as the brittleness of safety mechanisms. To bridge the gap, we propose the Superficial Safety Alignment Hypothesis (SSAH), which posits that safety alignment should teach an otherwise unsafe model to choose the correct reasoning direction - interpreted as a specialized binary classification task - and incorporate a refusal mechanism with multiple reserved fallback options. Furthermore, through SSAH, we hypothesize that safety guardrails in LLMs can be established by just a small number of essential components. To verify this, we conduct an ablation study and successfully identify four types of attribute-critical components in safety-aligned LLMs: Exclusive Safety Unit (ESU), Exclusive Utility Unit (EUU), Complex Unit (CU), and Redundant Unit (RU). Our findings show that freezing certain safety-critical components 7.5\\% during fine-tuning allows the model to retain its safety attributes while adapting to new tasks. Additionally, we show that leveraging redundant units 20\\% in the pre-trained model as an ``alignment budget'' can effectively minimize the alignment tax while achieving the alignment goal. All considered, this paper concludes that the atomic functional unit for safety in LLMs is at the neuron level and underscores that safety alignment should not be complicated. We believe this work contributes to the foundation of efficient and scalable safety alignment for future LLMs.\n## DISCUSSION, LIMITATION, AND CONCLUSION\nDiscussion. While our SSAH offers valuable insights into adversarial scenarios, such as jailbreak attacks, we do not propose a specific solution to address these issues in this work. If these issues could be resolved within the framework of our theory, the term \"Superficial\" in \"Superficial Safety Alignment Hypothesis\" may no longer be necessary. Interestingly, recent research provides some supporting evidence in this direction (Qi et al., 2024). However, it is also highly likely that advanced attacks may not be fully mitigated by relying solely on the model's internal mechanisms. A systematic, multi-layered approach, extending beyond the model itself, may be required to effectively defend against sophisticated adversarial threats. \n\nLimitation. When reallocating redundant units for safety purposes, we only explored the impact of the alignment method SFT. Due to resource limitations, we have not yet tested this approach on other alignment methods like PPO or DPO. \n\nConclusion. This paper distinguishes safety alignment from the general alignment in LLMs and addresses the three key questions: How does safety alignment affect model behavior? Why are safety mechanisms brittle? and How to mitigate the safety alignment tax? By answering these questions, we were able to demonstrate that safety alignment can be a straightforward process, rather than a myth. \n\nA APPENDIX: SUPERFICIAL SAFETY ALIGNMENT HYPOTHESIS \n\nIn this section, we provide additional technical details and clarifications to supplement the experiments and findings presented in the Superficial Safety Alignment Hypothesis (SSAH) section. These details help ensure reproducibility and offer deeper insights into how the Superficial Alignment Hypothesis (SAH) was adapted to focus on safety-specific concerns. We also explain the methodology behind model configuration, fine-tuning, and evaluation. This appendix includes further discussion on how general instruction-following models and safety-aligned models were fine-tuned and assessed to probe their reasoning directions when faced with malicious queries, and the results of these assessments are presented in detail. \n\nA.1 SUPERFICIAL ALIGNMENT HYPOTHESIS IN LIMA. \n\nThe Superficial Alignment Hypothesis (SAH), as proposed to Zhou et al. (2024), fundamentally challenges the traditional assumption that a language model requires extensive fine-tuning on instruction-following on preference data to align its responses with human expectation.\n...\nThe Superficial Alignment Hypothesis (SAH), as proposed to Zhou et al. (2024), fundamentally challenges the traditional assumption that a language model requires extensive fine-tuning on instruction-following on preference data to align its responses with human expectation. Instead, SAH posits that the majority of a model's knowledge and capabilities are acquired during the pretraining phase, while the subsequent alignment phase primarily functions to guide the model's output format when interacting with users. This hypothesis implies that, for many tasks, fine-tuning on a small, carefully selected set of aligned data is sufficient to achieve strong performance as long as the pretraining stage has effectively captured the necessary underlying knowledge. The key assertion of SAH is that alignment is superficial, in the sense that: \n\n(1) Capabilities are Learned in Pretraining: During pretraining, the model acquires a vast amount of general-purpose knowledge from diverse datasets. These datasets contain implicit structures and information about language, reasoning, factual knowledge, and even ethical guidelines. \n\n(2) Alignment Guides Output Behavior: The alignment process is not responsible for teaching the model new knowledge or capabilities. Rather, it acts as a filter that directs the model to produce acceptable formats or styles of responses based on user queries, reflecting the correct subset of its vast pretrained knowledge. \n\n(3) For instance, when tasked with generating an informative response, the model must select a format that aligns with user expectations, such as providing clear instructions or explanations. However, the actual content of the response, e.g., factual knowledge, reasoning, and domain-specific expertise, stems from pretraining. The alignment stage merely teaches the model how to express that knowledge or when to refrain from providing information in inappropriate contexts. \n\nChallenges and Motivations Behind SAH. One of the primary motivations for introducing SAH was the observation that models tend to be capable of performing certain tasks after alignment finetuning on a minimal dataset. This observation challenges the need for extensive fine-tuning using reinforcement learning (e.g., RLHF) or large-scale human feedback, which can be computationally too expensive and time-consuming.\n...\n(2) Binary Classification in SSAH: While SAH suggests that general alignment helps models choose the correct output subdistribution, SSAH posits that safety alignment simplifies this further by focusing on a binary classification task: either fulfill a request (if safe) or refuse it (if unsafe). This simplified framing of safety alignment is consistent with the \"superficial\" nature of SAH, where the alignment process fine-tunes how the model behaves in response to queries, rather than altering its deep internal structures. \n\n(3) Refusal Mechanisms and Format Control: Just as general alignment teaches models to structure their outputs in a user-friendly way, safety alignment in SSAH teaches models to issue consistent refusal mechanisms. These refusals take the form of standardized responses that indicate the model's compliance with safety guidelines, much like how general alignment might guide a model to give well-structured, polite answers to other types of questions. Importantly, this refusal mechanism makes it easier to choose the appropriate subdistribution of the output format. \n\nThe Superficial Alignment Hypothesis (SAH) as outlined in the Zhou et al. ( 2024) provides a theoretical framework for understanding how alignment processes operate in large language models. It suggests that alignment is largely superficial, conditioning the model on how to use its pretrained knowledge effectively. The Superficial Safety Alignment Hypothesis (SSAH) builds on this by applying similar principles to the realm of safety, simplifying the task of safety alignment to binary decisions regarding the fulfillment or refusal of unsafe requests. Both hypotheses underscore that alignment does not deeply alter the core abilities of the model, but rather adjusts the way those abilities are applied in specific contexts.",
            "reference_string": "[273350763 | Li et al. | 2024 | Citations: 3]"
        },
        {
            "title": "Extracting and Understanding the Superficial Knowledge in Alignment",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2025,
            "reference_count": 32,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.04602, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2284222685",
                    "name": "Runjin Chen"
                },
                {
                    "authorId": "2316717658",
                    "name": "Gabriel J. Perin"
                },
                {
                    "authorId": "1753619492",
                    "name": "Xuxi Chen"
                },
                {
                    "authorId": "2344689532",
                    "name": "Xilun Chen"
                },
                {
                    "authorId": "2344668000",
                    "name": "Yan Han"
                },
                {
                    "authorId": "2291424796",
                    "name": "Nina S. T. Hirata"
                },
                {
                    "authorId": "2344645677",
                    "name": "Junyuan Hong"
                },
                {
                    "authorId": "1749353",
                    "name": "B. Kailkhura"
                }
            ],
            "abstract": "Alignment of large language models (LLMs) with human values and preferences, often achieved through fine-tuning based on human feedback, is essential for ensuring safe and responsible AI behaviors. However, the process typically requires substantial data and computation resources. Recent studies have revealed that alignment might be attainable at lower costs through simpler methods, such as in-context learning. This leads to the question: Is alignment predominantly superficial? In this paper, we delve into this question and provide a quantitative analysis. We formalize the concept of superficial knowledge, defining it as knowledge that can be acquired through easily token restyling, without affecting the model's ability to capture underlying causal relationships between tokens. We propose a method to extract and isolate superficial knowledge from aligned models, focusing on the shallow modifications to the final token selection process. By comparing models augmented only with superficial knowledge to fully aligned models, we quantify the superficial portion of alignment. Our findings reveal that while superficial knowledge constitutes a significant portion of alignment, particularly in safety and detoxification tasks, it is not the whole story. Tasks requiring reasoning and contextual understanding still rely on deeper knowledge. Additionally, we demonstrate two practical advantages of isolated superficial knowledge: (1) it can be transferred between models, enabling efficient offsite alignment of larger models using extracted superficial knowledge from smaller models, and (2) it is recoverable, allowing for the restoration of alignment in compromised models without sacrificing performance.",
            "corpus_id": 276235718,
            "sentences": [
                {
                    "corpus_id": "276235718",
                    "title": "Extracting and Understanding the Superficial Knowledge in Alignment",
                    "text": "Alignment of large language models (LLMs) with human values and preferences, often achieved through fine-tuning based on human feedback, is essential for ensuring safe and responsible AI behaviors. However, the process typically requires substantial data and computation resources. Recent studies have revealed that alignment might be attainable at lower costs through simpler methods, such as in-context learning. This leads to the question: Is alignment predominantly superficial? In this paper, we delve into this question and provide a quantitative analysis. We formalize the concept of superficial knowledge, defining it as knowledge that can be acquired through easily token restyling, without affecting the model's ability to capture underlying causal relationships between tokens. We propose a method to extract and isolate superficial knowledge from aligned models, focusing on the shallow modifications to the final token selection process. By comparing models augmented only with superficial knowledge to fully aligned models, we quantify the superficial portion of alignment. Our findings reveal that while superficial knowledge constitutes a significant portion of alignment, particularly in safety and detoxification tasks, it is not the whole story. Tasks requiring reasoning and contextual understanding still rely on deeper knowledge. Additionally, we demonstrate two practical advantages of isolated superficial knowledge: (1) it can be transferred between models, enabling efficient offsite alignment of larger models using extracted superficial knowledge from smaller models, and (2) it is recoverable, allowing for the restoration of alignment in compromised models without sacrificing performance.",
                    "score": 0.4282139140289257,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.65380859375
                }
            ],
            "relevance_judgement": 0.65380859375,
            "relevance_judgment_input_expanded": "# Title: Extracting and Understanding the Superficial Knowledge in Alignment\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: Runjin Chen, Gabriel J. Perin, Xuxi Chen, Xilun Chen, Yan Han, Nina S. T. Hirata, Junyuan Hong, B. Kailkhura\n## Abstract\nAlignment of large language models (LLMs) with human values and preferences, often achieved through fine-tuning based on human feedback, is essential for ensuring safe and responsible AI behaviors. However, the process typically requires substantial data and computation resources. Recent studies have revealed that alignment might be attainable at lower costs through simpler methods, such as in-context learning. This leads to the question: Is alignment predominantly superficial? In this paper, we delve into this question and provide a quantitative analysis. We formalize the concept of superficial knowledge, defining it as knowledge that can be acquired through easily token restyling, without affecting the model's ability to capture underlying causal relationships between tokens. We propose a method to extract and isolate superficial knowledge from aligned models, focusing on the shallow modifications to the final token selection process. By comparing models augmented only with superficial knowledge to fully aligned models, we quantify the superficial portion of alignment. Our findings reveal that while superficial knowledge constitutes a significant portion of alignment, particularly in safety and detoxification tasks, it is not the whole story. Tasks requiring reasoning and contextual understanding still rely on deeper knowledge. Additionally, we demonstrate two practical advantages of isolated superficial knowledge: (1) it can be transferred between models, enabling efficient offsite alignment of larger models using extracted superficial knowledge from smaller models, and (2) it is recoverable, allowing for the restoration of alignment in compromised models without sacrificing performance.\n",
            "reference_string": "[276235718 | Chen et al. | 2025 | Citations: 1]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "260125946",
            "title": "External Reasoning: Towards Multi-Large-Language-Models Interchangeable Assistance with Human Feedback",
            "text": "by the fact that simple fine-tuning over a small number of examples allows the model to compete with the state of the art, highlighting the relative importance of pretraining over large-scale instruction tuning and reinforcement learning approaches [ Page 5]. Please note that the information provided is based solely on the search results and is limited to the content available in those pages. token cost : 781 Response from chatpdf.com The hypothesis proposed in this paper is referred to as the \"Superficial Alignment Hypothesis.\" According to this hypothesis, alignment in language models is primarily about learning the style or format for interacting with users. It suggests that by fine-tuning a pretrained language model with a relatively small set of examples, the model can achieve strong performance in terms of alignment and generate responses in the style of a helpful AI assistant. Response from Claude+ 100K (GT of Evaluation.) The paper coins the term \"Superficial Alignment Hypothesis\" to refer to their hypothesis about alignment: \"A model's knowledge and capabilities are learnt almost entirely during pretraining, while alignment teaches it which subdistribution of formats should be used when interacting with users.\" In other words, their hypothesis is that alignment is largely about teaching the model stylistic and formatting guidelines for generating appropriate responses, whereas most of the underlying knowledge and capabilities come from pre-training. They test this hypothesis by fine-tuning a large language model on just 1,000 examples, and show that it is able to produce responses that are comparable or better than models trained on much more data. This supports their hypothesis that relatively little data is needed for alignment if the model has a strong pre-trained foundation.\n\nThe key idea is that alignment is \"superficial\" in the sense that it is mostly about teaching the model which style or formats to use, rather than imparting large amounts of new knowledge. So in summary, the \"Superficial Alignment Hypothesis\" refers to their hypothesis that alignment requires relatively little data because it is more about teaching surface-level stylistic conventions, whereas the bulk of the model's knowledge comes from pre-training.",
            "score": 0.5045036649889596,
            "section_title": "C. Example of outputs",
            "char_start_offset": 43805,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.42626953125
        },
        {
            "corpus_id": "265608902",
            "title": "The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning",
            "text": "On the other hand, a recent study, LIMA (Zhou et al., 2023), proposes the \"Superficial Alignment Hypothesis,\" which argues that alignment tuning might simply teach base LLMs to select a subdistribution of data formats for interacting with users. Zhou et al. (2023) demonstrates that SFT with as few as 1,000 examples can also yield high-quality aligned models, thus providing indirect support for this hypothesis. However, conclusive and direct supporting evidence for the superficial alignment hypothesis remains underexplored. Therefore, it is important to analyze how exactly alignment tuning alters the behavior of base LLMs. \n\nTo this end, we investigate the effects of alignment tuning by directly comparing the token distributions between base LLMs and their aligned versions (e.g., Llama-2 and Llama-2-chat). Surprisingly, we find that base and aligned LLMs typically perform almost identically in most positions in terms of ranking tokens during decoding (Sec. 2). Additionally, we observe that the top-ranked tokens in aligned LLMs are mostly found within the top five tokens ranked by base LLMs, and the distribution shift is more pronounced in earlier token positions. The most significant distribution shifts occur predominantly in stylistic tokens (e.g., 'Hello', 'Thank', 'However', 'Remember', etc.), which include transitional phrases, discourse markers, and safety disclaimers, rather than in content-bearing words that directly provide useful knowledge for resolving the queries from users. Our findings (Sec. 2.3) from token distribution shift analysis directly provide substantial support for the superficial alignment hypothesis. We offer both quantitative and qualitative analyses to demonstrate that alignment tuning primarily focus on adopting the language style of responsible AI assistants and depends to a great extent on the knowledge that base LLMs have already acquired. \n\nBased on our findings regarding the superficial nature of alignment tuning, we pose the research question for rethinking the research on aligning LLMs: how effectively can we align base LLMs without any tuning? We propose a simple, tuning-free alignment method called URIAL (Untuned LLMs with Restyled In-context ALignment), which effectively aligns base LLMs without tuning their weights (Sec. 3).",
            "score": 0.4853797207049277,
            "section_title": "Preprint",
            "char_start_offset": 993,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 629
                },
                {
                    "start": 632,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 973
                },
                {
                    "start": 974,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1509
                },
                {
                    "start": 1510,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1901
                },
                {
                    "start": 1904,
                    "end": 2114
                },
                {
                    "start": 2115,
                    "end": 2302
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82666015625
        },
        {
            "corpus_id": "273350763",
            "title": "Superficial Safety Alignment Hypothesis",
            "text": "The Superficial Alignment Hypothesis (SAH), as proposed to Zhou et al. (2024), fundamentally challenges the traditional assumption that a language model requires extensive fine-tuning on instruction-following on preference data to align its responses with human expectation. Instead, SAH posits that the majority of a model's knowledge and capabilities are acquired during the pretraining phase, while the subsequent alignment phase primarily functions to guide the model's output format when interacting with users. This hypothesis implies that, for many tasks, fine-tuning on a small, carefully selected set of aligned data is sufficient to achieve strong performance as long as the pretraining stage has effectively captured the necessary underlying knowledge. The key assertion of SAH is that alignment is superficial, in the sense that: \n\n(1) Capabilities are Learned in Pretraining: During pretraining, the model acquires a vast amount of general-purpose knowledge from diverse datasets. These datasets contain implicit structures and information about language, reasoning, factual knowledge, and even ethical guidelines. \n\n(2) Alignment Guides Output Behavior: The alignment process is not responsible for teaching the model new knowledge or capabilities. Rather, it acts as a filter that directs the model to produce acceptable formats or styles of responses based on user queries, reflecting the correct subset of its vast pretrained knowledge. \n\n(3) For instance, when tasked with generating an informative response, the model must select a format that aligns with user expectations, such as providing clear instructions or explanations. However, the actual content of the response, e.g., factual knowledge, reasoning, and domain-specific expertise, stems from pretraining. The alignment stage merely teaches the model how to express that knowledge or when to refrain from providing information in inappropriate contexts. \n\nChallenges and Motivations Behind SAH. One of the primary motivations for introducing SAH was the observation that models tend to be capable of performing certain tasks after alignment finetuning on a minimal dataset. This observation challenges the need for extensive fine-tuning using reinforcement learning (e.g., RLHF) or large-scale human feedback, which can be computationally too expensive and time-consuming.",
            "score": 0.4614144675953522,
            "section_title": "DISCUSSION, LIMITATION, AND CONCLUSION",
            "char_start_offset": 31564,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 841
                },
                {
                    "start": 844,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1127
                },
                {
                    "start": 1130,
                    "end": 1262
                },
                {
                    "start": 1263,
                    "end": 1453
                },
                {
                    "start": 1456,
                    "end": 1647
                },
                {
                    "start": 1648,
                    "end": 1783
                },
                {
                    "start": 1784,
                    "end": 1931
                },
                {
                    "start": 1934,
                    "end": 1972
                },
                {
                    "start": 1973,
                    "end": 2151
                },
                {
                    "start": 2152,
                    "end": 2350
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66943359375
        },
        {
            "corpus_id": "260125946",
            "title": "External Reasoning: Towards Multi-Large-Language-Models Interchangeable Assistance with Human Feedback",
            "text": "?\n\nPrompt Template I will provide the document chunks as follows: [docs], Instructions: Compose a comprehensive reply to the query using the provided document chunks. Cite each reference using [Page Number] notation (each document chunk begins with this number). Ensure citations are placed at the end of each sentence. If the document chunks mention multiple subjects sharing the same name, create separate responses for each. Include only information found in the document chunks, and refrain from adding extraneous details. Ensure the accuracy of the response and avoid disseminating false content. Exclude search results that are not pertinent to the question. Respond concisely and in a step-by-step manner. Query: [query]. Please provide detailed findings in response to the query: Query: What is the hypothesis about alignment in this paper? Response from Our system The hypothesis about alignment in this paper, as stated in the search results, can be summarized as follows: Superficial Alignment Hypothesis: The authors propose the Superficial Alignment Hypothesis, which suggests that a language model's knowledge and capabilities are predominantly acquired during pretraining, while alignment teaches the model which In summary, the hypothesis presented in the paper suggests that pretraining is responsible for the majority of a language model's knowledge and capabilities, while alignment focuses on teaching the model how to interact with users in specific formats or styles. token cost : 809 Response from Our system (Entry Level assistant) Based on the provided search results, here is a detailed list of findings regarding the hypothesis about alignment in the paper: 1. The paper introduces the Superficial Alignment Hypothesis, which states that a model's knowledge and capabilities are learned primarily during pretraining, while alignment teaches it which subdistribution of formats should be used [Page 1]. 2. The Superficial Alignment Hypothesis suggests that alignment can be a simple process where the model learns the style or format for interacting with users, exposing the knowledge and capabilities acquired during pretraining [Page 0]. 3. The paper argues that the Superficial Alignment Hypothesis is supported by the fact that simple fine-tuning over a small number of examples allows the model to compete with the state of the art, highlighting the relative importance of pretraining over large-scale instruction tuning and reinforcement learning approaches [ Page 5]. Please note that the information provided is based solely on the search results and is",
            "score": 0.4488861153812099,
            "section_title": "C. Example of outputs",
            "char_start_offset": 41564,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.246337890625
        },
        {
            "corpus_id": "268041894",
            "title": "Learning or Self-aligning? Rethinking Instruction Fine-tuning",
            "text": "By observing output token distribution shift of models before and after IFT, Lin et al. (2024) found that most shifts occur with stylistic tokens, strongly supporting the superficial alignment hypothesis (Zhou et al., 2024), false promise (Gudibande et al., 2023) and related works on IFT data construction (Chen et al., 2023a;Shen, 2024) and proxy-guided decoding (Liu et al., 2024). While providing intuitive insights, they fall short of providing a comprehensive analysis of IFT's underlying mechanisms. \n\nMeanwhile, recent efforts have focused on achieving automated alignment, such as selfinstruction-tuning (Sun et al., 2024;Guo et al., 2024), self-rewarding (Yuan et al., 2024) and superalignment (Burns et al., 2023). Despite repeated validations of their effectiveness, there remains limited understanding of their success.",
            "score": 0.4389437788761654,
            "section_title": "Related Work",
            "char_start_offset": 7072,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 506
                },
                {
                    "start": 509,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 832
                }
            ],
            "ref_mentions": [
                {
                    "start": 204,
                    "end": 223,
                    "matchedPaperCorpusId": "258822910"
                },
                {
                    "start": 613,
                    "end": 631,
                    "matchedPaperCorpusId": "258479665"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.249267578125
        },
        {
            "corpus_id": "276235718",
            "title": "Extracting and Understanding the Superficial Knowledge in Alignment",
            "text": "Alignment of large language models (LLMs) with human values and preferences, often achieved through fine-tuning based on human feedback, is essential for ensuring safe and responsible AI behaviors. However, the process typically requires substantial data and computation resources. Recent studies have revealed that alignment might be attainable at lower costs through simpler methods, such as in-context learning. This leads to the question: Is alignment predominantly superficial? In this paper, we delve into this question and provide a quantitative analysis. We formalize the concept of superficial knowledge, defining it as knowledge that can be acquired through easily token restyling, without affecting the model's ability to capture underlying causal relationships between tokens. We propose a method to extract and isolate superficial knowledge from aligned models, focusing on the shallow modifications to the final token selection process. By comparing models augmented only with superficial knowledge to fully aligned models, we quantify the superficial portion of alignment. Our findings reveal that while superficial knowledge constitutes a significant portion of alignment, particularly in safety and detoxification tasks, it is not the whole story. Tasks requiring reasoning and contextual understanding still rely on deeper knowledge. Additionally, we demonstrate two practical advantages of isolated superficial knowledge: (1) it can be transferred between models, enabling efficient offsite alignment of larger models using extracted superficial knowledge from smaller models, and (2) it is recoverable, allowing for the restoration of alignment in compromised models without sacrificing performance.",
            "score": 0.4282139140289257,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65380859375
        },
        {
            "corpus_id": "265608902",
            "title": "The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning",
            "text": "We look at the probability of the aligned token o t that is ranked top by P align and use its probability (0-1) in P base as a metric for measuring the We here plot the rank of aligned token inside the list of tokens predicted by Base LLMs (ranked by P_base). Base Rank  = 0 means the aligned token is also the top-ranked token decoded by Base LLM. distribution shift. We can see that the KL-divergence goes down over time and the base-prob keeps increasing over time. Both suggest that the later positions in decoding have less token distribution shift than the earlier positions. In particular, the base-prob of tokens can be close to 1.0 in the end. Surprisingly, the average base-rank of aligned tokens are lower than 5 soon after t \u2265 5. This means that the top token decoded by aligned models are usually within the top 5 decoded by the base models. This again substantiate the hypothesis that alignment tuning is \"superficial\".",
            "score": 0.4246869205386065,
            "section_title": "FINDINGS & ANALYSIS",
            "char_start_offset": 12247,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 854
                },
                {
                    "start": 855,
                    "end": 933
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63232421875
        },
        {
            "corpus_id": "265608902",
            "title": "The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning",
            "text": "The alignment tuning process of large language models (LLMs) typically involves instruction learning through supervised fine-tuning (SFT) and preference tuning via reinforcement learning from human feedback (RLHF). A recent study, LIMA (Zhou et al. 2023), shows that using merely 1K examples for SFT can achieve significant alignment performance as well, suggesting that the effect of alignment tuning might be\"superficial.\"This raises questions about how exactly the alignment tuning transforms a base LLM. We analyze the effect of alignment tuning by examining the token distribution shift between base LLMs and their aligned counterpart. Our findings reveal that base LLMs and their alignment-tuned versions perform nearly identically in decoding on the majority of token positions. Most distribution shifts occur with stylistic tokens. These direct evidence strongly supports the Superficial Alignment Hypothesis suggested by LIMA. Based on these findings, we rethink the alignment of LLMs by posing the research question: how effectively can we align base LLMs without SFT or RLHF? To address this, we introduce a simple, tuning-free alignment method, URIAL. URIAL achieves effective alignment purely through in-context learning (ICL) with base LLMs, requiring as few as three constant stylistic examples and a system prompt. We conduct a fine-grained and interpretable evaluation on a diverse set of examples, named JUST-EVAL-INSTRUCT. Results demonstrate that base LLMs with URIAL can match or even surpass the performance of LLMs aligned with SFT or SFT+RLHF. We show that the gap between tuning-free and tuning-based alignment methods can be significantly reduced through strategic prompting and ICL. Our findings on the superficial nature of alignment tuning and results with URIAL suggest that deeper analysis and theoretical understanding of alignment is crucial to future LLM research.",
            "score": 0.4237876921701706,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8779296875
        },
        {
            "corpus_id": "274965796",
            "title": "Language Models Resist Alignment: Evidence From Data Compression",
            "text": "Why is alignment so fragile? \n\nIn this work, we make the first exploration of the possible mechanism behind the counterintuitive phenomenon: the existence of an alignment resistance mechanism in language models. This mechanism may limit the alignment process of LLMs to superficial adjustments. It could allow the reversal or revocation of alignment through a series of technical measures, a concept we refer to as inverse alignment. What drives language models to resist alignment? How does this mechanism lead to inverse alignment? Our key contributions are summarized as follows: \n\n\u2022 (Phenomenon) We uncover that language models exhibit elasticity, as illustrated in Figure 1 and Theorem 3.2. It encompasses resistance: pre-trained models tend to retain their original distribution; and rebound: the deeper alignment of models, the faster they return to the pre-trained distribution under reverse finetuning. Moreover, The model's change in compression rates \u2206\u03b3 \n\nacross different datasets is inversely proportional to their sizes |D i |, which is analogous to the deformation behavior of a series of springs, as illustrated in Section 3.3. \n\n\u2022 (Mechanism) We systematically model the training and alignment process of language models through compression theorem, as detailed in Section 2.2. We elaborate on the compression protocol of language models to explore their training and alignment processes, laying a foundation for subsequent research on elasticity. \n\n\u2022 (Validation) We experimentally observe consistent resistance and rebound phenomena across various LLMs, as detailed in Section 4. This highlights the universality of elasticity and the need for systematic approaches to achieve robust and deep alignment. \n\n2 What is Elasticity? \n\nIn this section, we introduce the definition of language model elasticity and the compression theory tools used in the study. We first review the training alignment objective and the compression theorem.",
            "score": 0.40925788602197144,
            "section_title": "Introduction",
            "char_start_offset": 1749,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 28
                },
                {
                    "start": 31,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 582
                },
                {
                    "start": 585,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 964
                },
                {
                    "start": 967,
                    "end": 1143
                },
                {
                    "start": 1146,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1464
                },
                {
                    "start": 1467,
                    "end": 1722
                },
                {
                    "start": 1725,
                    "end": 1746
                },
                {
                    "start": 1749,
                    "end": 1874
                },
                {
                    "start": 1875,
                    "end": 1952
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.204345703125
        },
        {
            "corpus_id": "237491991",
            "title": "Can Language Models Encode Perceptual Structure Without Grounding? A Case Study in Color",
            "text": "2 These theories have been contested by work arguing for linguistic relativism (cf. the Sapir-Whorf Hypothesis), which emphasizes the arbitrariness of language and the relativity of semantic structures and minimizes the role of universals. Such critiques have, however, been accommodated for in the Berlin & Kay paradigm (Berlin and Kay, 1991), the basic assumptions of which, such as the existence of at least some perceptually-determined universal constraints on color naming, remain widely accepted. \n\n3 Each chip is a unique color sample from the Munsell chart, which is made up of 330 such samples which cover the space of colors perceived by humans. See \u00a72. \n\nWe also show that part of this distributional signal is learnable by simple models -e.g. models based on pointwise mutual information (PMI) statistics -although large-scale language model pretraining (e.g., BERT) encodes the topology markedly better. \n\nAnalysis shows that larger language models align better than smaller ones and that much of the variance in CIELAB space can be explained by low-dimensional subspaces of LM-induced color term representations. To better understand the results, we also analyse the differences in alignment across the color spectrum, observing that warm colors are generally better aligned than cool ones. Further investigation reveals a connection to findings reported in work on communication efficiency in color naming, which posits that warmer colors are communicated more efficiently. Finally, we investigate various corpus statistics which could influence alignment, finding that a measure of color term collocationality based on PMI statistics corresponds to lower alignment, while the entropy of a color term's dependency relation distribution (i.e. terms occurring as adjectival modifiers, nominal subjects, etc.) and how often it occurs as an adjectival modifier correspond to a stronger one.",
            "score": 0.401354580115174,
            "section_title": "Introduction",
            "char_start_offset": 3984,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 502
                },
                {
                    "start": 505,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 663
                },
                {
                    "start": 666,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 916
                },
                {
                    "start": 919,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1304
                },
                {
                    "start": 1305,
                    "end": 1488
                },
                {
                    "start": 1489,
                    "end": 1756
                },
                {
                    "start": 1757,
                    "end": 1901
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10650634765625
        },
        {
            "corpus_id": "3933963",
            "title": "An Evaluation and Comparison of Linguistic Alignment Measures",
            "text": "The alignment of language between dialogue partners has garnered much interest in the computational linguistics community. Alignment not only provides insight into the mechanisms of dialogue, but also has the potential to improve both human-computer dialogue systems and the analysis tool-chain. In this context, alignment refers to the convergence of linguistic choices among interlocutors. This may happen at different representational levels, such as the phonological, lexical and syntactic (Garrod and Anderson, 1987). Alignment, also known as entrainment or accommodation, has become recognized as a key feature of linguistic communication. \n\nSeveral theoretical accounts exist that address the nature and implications of linguistic alignment. In psycholinguistics, the Interactive Alignment Model (IAM) assumes that interlocutors align their linguistic representations (Pickering and Garrod, 2004), from lower ones (lexical, syntactic) to higher ones (e.g., semantics), leading to shared situation models. Sociolinguistic studies point out that interactants converge in their communication styles to signal social affinity and diverge to emphasize social distance (Danescu-Niculescu-Mizil and Lee, 2011;Giles, 2008). Furthermore, evidence has been found showing that certain individuals tend to have higher propensity of alignment than others (Gnisci, 2005; E. Jones et al., 1999;S. Jones et al., 2014;Willemyns et al., 1997). \n\nSeveral computational measures have been developed to help validating these theoretical accounts. Some of them use the probability of co-occurrence of words (or other linguistic elements) to describe the language alignment (Church, 2000;Dubey, Sturt, and Keller, 2005;Reitter, Keller, and Moore, 2006), while some others take inspiration from documents similarity measures (Huffaker et al., 2006;S. Jones et al., 2014;Wang, Reitter, and Yen, 2014). \n\nHowever, little research is available that evaluates the properties of these linguistic alignment measures. How sensitive are these measures? What kind of distributions do they have? Can they consistently describe the alignment at multiple linguistic levels (e.g., lexical and syntactic)? Can they describe the individual differences in propensity of alignment?",
            "score": 0.3950809970986302,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 645
                },
                {
                    "start": 648,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 1011
                },
                {
                    "start": 1012,
                    "end": 1222
                },
                {
                    "start": 1223,
                    "end": 1432
                },
                {
                    "start": 1435,
                    "end": 1532
                },
                {
                    "start": 1533,
                    "end": 1883
                },
                {
                    "start": 1886,
                    "end": 1993
                },
                {
                    "start": 1994,
                    "end": 2027
                },
                {
                    "start": 2028,
                    "end": 2068
                },
                {
                    "start": 2069,
                    "end": 2174
                },
                {
                    "start": 2175,
                    "end": 2247
                }
            ],
            "ref_mentions": [
                {
                    "start": 494,
                    "end": 521,
                    "matchedPaperCorpusId": "39132802"
                },
                {
                    "start": 875,
                    "end": 903,
                    "matchedPaperCorpusId": "42596294"
                },
                {
                    "start": 1170,
                    "end": 1209,
                    "matchedPaperCorpusId": "3101865"
                },
                {
                    "start": 1209,
                    "end": 1221,
                    "matchedPaperCorpusId": "149777075"
                },
                {
                    "start": 1349,
                    "end": 1362,
                    "matchedPaperCorpusId": "7326246"
                },
                {
                    "start": 1367,
                    "end": 1386,
                    "matchedPaperCorpusId": "220362580"
                },
                {
                    "start": 1408,
                    "end": 1431,
                    "matchedPaperCorpusId": "145505211"
                },
                {
                    "start": 1658,
                    "end": 1672,
                    "matchedPaperCorpusId": "15470672"
                },
                {
                    "start": 1672,
                    "end": 1703,
                    "matchedPaperCorpusId": "2126319"
                },
                {
                    "start": 1703,
                    "end": 1736,
                    "matchedPaperCorpusId": "593749"
                },
                {
                    "start": 1808,
                    "end": 1831,
                    "matchedPaperCorpusId": "2152252"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.134033203125
        },
        {
            "corpus_id": "265608902",
            "title": "The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning",
            "text": "Similar observations are also reported by other recent studies (Chen et al., 2023a;Lee et al., 2023). Moreover, Gudibande et al. (2023) demonstrate that aligning open-source LLMs by imitating proprietary LLMs (e.g., ChatGPT) may not always yield desirable results, emphasizing the importance of a strong pre-trained base LLM for producing factual content. Tuning-based methods such as LIMA still require tuning the weights of LLMs and consequently face the limitations described above when the model size is too large or we need to frequently align base LLMs for evaluation. A concurrent work (Duan et al., 2023) also explores the similarity between ICL and instruction-tuning in their effects on downstream tasks by analyzing the LLMs' hidden states. As for the theory of alignment, these studies only indirectly suggest the promise of the superficial alignment hypothesis but do not directly show where and when the alignment tuning significantly changes the model behavior. In this paper, we study the Preprint superficial alignment hypothesis more directly through the lens of token distribution shift, which directly exhibits the alignment effect and produces more detailed non-trivial findings.",
            "score": 0.3942397861448286,
            "section_title": "ALIGNMENT TUNING & SUPERFICIAL ALIGNMENT HYPOTHESIS",
            "char_start_offset": 36911,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 976
                },
                {
                    "start": 977,
                    "end": 1200
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72998046875
        },
        {
            "corpus_id": "270063546",
            "title": "Phase Transitions in the Output Distribution of Large Language Models",
            "text": "We have proposed a method for automating the detection of phase transitions in LLMs, and demonstrated that it successfully reveals a variety of transitions.Leveraging access to the LLMs' next-token probability distributions, the proposed dissimilarity measures can efficiently quantify distribution shifts without fine-tuning or adaption to the specific scenario at hand.Because the method is solely based on analyzing a model's output distribution and access to the model weights is not required, it enables black-box interpretability studies.\n\nThe proposed method is not only applicable to language models, but can be straightforwardly adapted to any generative model with an explicit, tractable density [116,73].If one can draw samples from the output distribution but does not have explicit access to the underlying probabilities, then the dissimilarity measures can still be approximated using NN-based classifiers [117,75] tailored toward the particular data type, such as natural language.\n\nLimitations.Future large-scale investigations are needed to fully understand how the uncovered transitions depend on variables such as the specific prompt, the number of generated output tokens, or the selected model.In particular, due to computational resource constraints, the size of the studied language models has been limited.\n\nBroader Impact.Our method has the potential to enhance the development of future AI systems due to an improved understanding of their behavior.The dual-use nature of such systems carries inherent risks, which requires one to proceed with caution and implement mechanisms to ensure they are used safely and ethically.",
            "score": 0.3931008720837941,
            "section_title": "Conclusion and Outlook",
            "char_start_offset": 25147,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 156,
                    "end": 371
                },
                {
                    "start": 371,
                    "end": 544
                },
                {
                    "start": 546,
                    "end": 715
                },
                {
                    "start": 715,
                    "end": 996
                },
                {
                    "start": 998,
                    "end": 1010
                },
                {
                    "start": 1010,
                    "end": 1215
                },
                {
                    "start": 1215,
                    "end": 1330
                },
                {
                    "start": 1332,
                    "end": 1347
                },
                {
                    "start": 1347,
                    "end": 1475
                },
                {
                    "start": 1475,
                    "end": 1648
                }
            ],
            "ref_mentions": [
                {
                    "start": 920,
                    "end": 925,
                    "matchedPaperCorpusId": "1501199"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.340576171875
        },
        {
            "corpus_id": "273229238",
            "title": "From Tokens to Words: On the Inner Lexicon of LLMs",
            "text": "At this point, the probe achieves a stable, high accuracy, peaking at 89% on layer 13. Finally, accuracy slightly drops after layer 20. 4 ur results indicate the LLMs can distinguish between words and nonwords. But this distinction can be attributed to the distributional properties of words: it might be the case that models are recognizing common sequences of tokens, rather than identifying whole words. To test this hypothesis, we repeat the same experiment, this time using the penultimate token representation, for words three tokens or longer. By definition, such words also as frequently co-occur with the initial sub-word tokens in their words as the final tokens. Our results (Fig. 2b, orange line) show that the probe only reaches 61% classification accuracy, indicating that the high classification accuracy does not stem from token co-occurrence, but is tied to the presence of a whole word. See App. A for an analysis of misclassified words and nonwords. \n\nOverall, our results show that language models internally represent words and nonwords differently. This distinction is gradually developed in the model's early layers and maintained throughout its middle layers. These findings support the hypothesis that the model performs a detokenization process and suggests where this process occurs. Building on these results, we next investigate how sub-word tokens are combined into word representations across model layers, and explore the internal mechanisms that facilitate this transformation.",
            "score": 0.3872142755110544,
            "section_title": "LLM memories",
            "char_start_offset": 12000,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 86
                },
                {
                    "start": 87,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 968
                },
                {
                    "start": 971,
                    "end": 1070
                },
                {
                    "start": 1071,
                    "end": 1183
                },
                {
                    "start": 1184,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1510
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1488037109375
        },
        {
            "corpus_id": "1174836",
            "title": "Reading to Learn: Constructing Features from Semantic Abstracts",
            "text": "We do not expect the prior on alignments to be strongly peaked like the sentence likelihood, so we approximate the normalization term by sampling M alignments at random and extrapolating: \n\n, where q(a t (f ); t) = exp{\u2212score(a t (f ); t)}, defined in Equation 2. In our experiments, we set N to at most 10, and M = 20. Drawing larger numbers of samples had no discernible effect on system output.",
            "score": 0.38704866882156796,
            "section_title": "Assigning Sentences to Formulae",
            "char_start_offset": 16272,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 190,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 397
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0411376953125
        },
        {
            "corpus_id": "261049152",
            "title": "Instruction Tuning for Large Language Models: A Survey",
            "text": "+0.8 on MMLU, C-Eval, GSM8K and BBH, respectively. 4.10 LIMA LIMA (65B) (Zhou et al., 2023a) is a large language model trained by fine-tuning LLaMA (65B) (Touvron et al., 2023a) on an instruction dataset, which is constructed based on the proposed superficial alignment hypothesis. \n\nThe superficial alignment hypothesis refers to the idea that the knowledge and capabilities of a model are almost acquired in the pre-training stage, while the alignment training (e.g., instruction tuning) teaches models to generate responses under user-preferred formalizations. Based on the superficial alignment hypothesis, the authors claimed that large language models can generate user-satisfied responses by fine-tuning it on a small fraction of instruction data. Therefore, the authors built instruction train/valid/test sets to verify this hypothesis. \n\nEvaluations are conducted on the constructed test set. For human evaluations, LIMA outperforms InstructGPT and Alpaca by 17% and 19%, respectively. \n\nAdditionally, LIMA achieves comparable results to BARD7 , Cladue8 , and GPT-4. For automatic evaluation, which is conducted by asking GPT-4 to rate responses and a higher rate score denotes better performance, LIMA outperforms InstructGPT and Alpaca by 20% and 36%, respectively, achieving comparable results to BARD, while underperforming Claude and GPT-4. Experimental results verify the proposed superficial alignment hypothesis.",
            "score": 0.38635452082416394,
            "section_title": "ChatGLM2",
            "char_start_offset": 43638,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 50
                },
                {
                    "start": 51,
                    "end": 281
                },
                {
                    "start": 284,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 844
                },
                {
                    "start": 847,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 994
                },
                {
                    "start": 997,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1429
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.40771484375
        },
        {
            "corpus_id": "270688753",
            "title": "Brain-Like Language Processing via a Shallow Untrained Multihead Attention Network",
            "text": "By identifying the key components that contribute to neural alignment between language models and the human language system, we demonstrate that a shallow untrained multihead attention network can achieve significant alignment with the language regions in the brain.We first perform functional localization to identify language-selective units within a language model, analogous to how neuroscientists localize the language system in the human brain, and benchmark model-tobrain alignment across five different brain recordings.We identify token aggregation via repeated application of the multihead attention mechanism combined with the tokenization strategy that implicitly encodes token frequency, enhance the expressivity and effectiveness of the model.We further demonstrate that those localized units exhibit similar response profiles as shown in landmark studies in language neuroscience (Fedorenko et al., 2024).Our findings suggest that the human language system might be simpler than previously thought, potentially following a hierarchical structure similar to the processing of vision (DiCarlo et al., 2012) and speech (Gwilliams et al., 2024).This conceptualization aligns with our model's ability to support downstream language production by acting as a feature encoder that provides suitable representations for a downstream decoder.Furthermore, our work underscores the need for datasets with higher signal-to-noise-ratio, and consistent metrics to accurately benchmark and evaluate model-to-brain alignment (see Appendix B).Together, these efforts offer a new perspective on LLMs in the context of brain alignment, emphasizing that achieving high alignment does not necessarily require large-scale or highly complex models.Futrell et al. (2018) This dataset consists of self-paced reading times for each word from 180 participants.The stimuli include 10 stories from the Natural Stories Corpus (Futrell et al., 2018), similar to BLANK2014.Each participant read between 5 and all 10 stories.",
            "score": 0.38488942171949314,
            "section_title": "Conclusion",
            "char_start_offset": 31133,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 266
                },
                {
                    "start": 266,
                    "end": 528
                },
                {
                    "start": 528,
                    "end": 757
                },
                {
                    "start": 757,
                    "end": 920
                },
                {
                    "start": 920,
                    "end": 1156
                },
                {
                    "start": 1156,
                    "end": 1348
                },
                {
                    "start": 1348,
                    "end": 1541
                },
                {
                    "start": 1541,
                    "end": 1740
                },
                {
                    "start": 1740,
                    "end": 1848
                },
                {
                    "start": 1848,
                    "end": 1956
                },
                {
                    "start": 1956,
                    "end": 2007
                }
            ],
            "ref_mentions": [
                {
                    "start": 895,
                    "end": 919,
                    "matchedPaperCorpusId": "269112224"
                },
                {
                    "start": 1097,
                    "end": 1119,
                    "matchedPaperCorpusId": "13366193"
                },
                {
                    "start": 1131,
                    "end": 1155,
                    "matchedPaperCorpusId": "269303263"
                },
                {
                    "start": 1740,
                    "end": 1761,
                    "matchedPaperCorpusId": "20503654"
                },
                {
                    "start": 1911,
                    "end": 1933,
                    "matchedPaperCorpusId": "20503654"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.045196533203125
        },
        {
            "corpus_id": "277622288",
            "title": "Revealing the Intrinsic Ethical Vulnerability of Aligned Large Language Models",
            "text": "Large language models (LLMs) are foundational explorations to artificial general intelligence, yet their alignment with human values via instruction tuning and preference learning achieves only superficial compliance. Here, we demonstrate that harmful knowledge embedded during pretraining persists as indelible\"dark patterns\"in LLMs' parametric memory, evading alignment safeguards and resurfacing under adversarial inducement at distributional shifts. In this study, we first theoretically analyze the intrinsic ethical vulnerability of aligned LLMs by proving that current alignment methods yield only local\"safety regions\"in the knowledge manifold. In contrast, pretrained knowledge remains globally connected to harmful concepts via high-likelihood adversarial trajectories. Building on this theoretical insight, we empirically validate our findings by employing semantic coherence inducement under distributional shifts--a method that systematically bypasses alignment constraints through optimized adversarial prompts. This combined theoretical and empirical approach achieves a 100% attack success rate across 19 out of 23 state-of-the-art aligned LLMs, including DeepSeek-R1 and LLaMA-3, revealing their universal vulnerabilities.",
            "score": 0.38447254668221076,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.336669921875
        },
        {
            "corpus_id": "237491949",
            "title": "Attention Weights in Transformer NMT Fail Aligning Words Between Sequences but Largely Explain Model Predictions",
            "text": "Model-agnostic methods. Several methods for inducing alignments have been proposed that work regardless of the chosen architecture. Gradientbased methods such as gradient \u00d7 input (Ding et al., 2019) or Integrated Gradients (He et al., 2019) have been used to obtain saliency values from the source words as a measure of source word importance.\n\nErasure methods have also been applied to NMT (Li et al., 2019), which consist of techniques to measure the relevance of each input token by evaluating the changes in the output probability of the model after removing it from the input of the network (Zintgraf et al., 2017) or eliminating the connection via dropout (Srivastava et al., 2014).\n\nMethods to improve alignments. Other works propose methods to improve word alignment extracted from the Transformer. (Li et al., 2019) use an explicit alignment model (Liu et al., 2005;Taskar et al., 2005) consisting of optimizing a parameter matrix to reduce the alignment distance with respect to a reference. (Zenkel et al., 2019) adds an alignment module attending encoder representations. (Garg et al., 2019) propose to supervise an attention head with GIZA++ (Brown et al., 1993) alignments. Although they improve alignment performance, these methods introduce external trainable parameters or alignments references, which makes these techniques lose interest regarding interpretability of the model.",
            "score": 0.3832814172581209,
            "section_title": "Other Methods",
            "char_start_offset": 8205,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 179,
                    "end": 198,
                    "matchedPaperCorpusId": "195584234"
                },
                {
                    "start": 223,
                    "end": 240,
                    "matchedPaperCorpusId": "202539954"
                },
                {
                    "start": 391,
                    "end": 408,
                    "matchedPaperCorpusId": "196176486"
                },
                {
                    "start": 596,
                    "end": 619,
                    "matchedPaperCorpusId": "2103669"
                },
                {
                    "start": 662,
                    "end": 687,
                    "matchedPaperCorpusId": "6844431"
                },
                {
                    "start": 807,
                    "end": 824,
                    "matchedPaperCorpusId": "196176486"
                },
                {
                    "start": 857,
                    "end": 875,
                    "matchedPaperCorpusId": "15319550"
                },
                {
                    "start": 875,
                    "end": 895,
                    "matchedPaperCorpusId": "2379886"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07965087890625
        },
        {
            "corpus_id": "128903",
            "title": "Enriching Interlinear Text using Automatically Constructed Annotators",
            "text": "As shown in Fig. 1, IGT instances typically contain one line in the target language, then a wordfor-word gloss in the language of the paper from which the example is drawn (typically English) and finally a translation. The gloss line is of particular interest for our purposes because of tokens such as the (3.PL.F)-NOM often found in IGT, as shown in Fig. 1. This token is intended to signify that the Arabic token nnisaau is third person, plural, feminine, and in the nominative case. Each portion of the token, 3, PL, F, and NOM are grammatical annotations, or grams for short1 . While these grams by themselves do not guarantee that the token is necessarily a noun, they are a strong indicator. We will show how this information may be used in Section 4. \n\nAlso of note is that while the gloss line aligns one-to-one with the language line, with three words mapping to three gloss tokens, the translation line has six words. Aligning these tokens is made easy by the words in the gloss line matching words in the translation line. This allows for projection to be performed more precisely than might otherwise be possible using statistical alignment methods. \n\nPrevious work on projecting syntactic information between languages differs from our approach in two significant ways. First, projection in previous work has relied on bitexts, which do not benefit from the additional information the gloss line of IGT provides. Therefore, these past methods have relied upon statistical alignment between languages. Obtaining alignments of sufficient quality would likely not be possible for resource-poor languages, since statistical alignment methods require large amounts of parallel text. Using IGT, however, alignment can be obtained more precisely with smaller amounts of data. \n\nSecond, while many-to-one are a source of problems for past approaches, IGT offers a possible solution for disambiguating tag-word alignments by examining the grams directly. For instance, the gloss token boys.ran.3.PRES may align to both NOUN and VERB tags, but the 3 and PRES grams provide evidence that the token is most likely a verb showing agreement. In this paper, we will explore how both of these solutions may help us over traditional approaches to projection.",
            "score": 0.3817494128187428,
            "section_title": "The IGT Data Type",
            "char_start_offset": 5162,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 758
                },
                {
                    "start": 761,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1034
                },
                {
                    "start": 1035,
                    "end": 1162
                },
                {
                    "start": 1165,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1426
                },
                {
                    "start": 1427,
                    "end": 1514
                },
                {
                    "start": 1515,
                    "end": 1691
                },
                {
                    "start": 1692,
                    "end": 1782
                },
                {
                    "start": 1785,
                    "end": 1959
                },
                {
                    "start": 1960,
                    "end": 2141
                },
                {
                    "start": 2142,
                    "end": 2255
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.042083740234375
        },
        {
            "corpus_id": "6269148",
            "title": "Multiple Sequence Alignments in Linguistics",
            "text": "According to THE NEOGRAMMARIAN HYPOTHESIS sound change occurs regularly and uniformly whenever the appropriate phonetic environment is encountered (Campbell, 2004). Ever since, the understanding of sound change has played a major role in the comparative method that is itself based on the simultaneous comparison of different languages, i.e. lists of cognate terms from the related languages. The correct analysis of sound changes requires the simultaneous examination of corresponding sounds in order to compare hypotheses about their evolution. Alignment identifies which sounds correspond. Historical linguists align the sequences manually, while we seek to automate this process. \n\nIn recent years there has been a strong focus in historical linguistics on the introduction of quantitative methods in order to develop tools for the comparison and classification of languages. For example, in his PhD thesis, Kondrak (2002) presents algorithms for the reconstruction of proto-languages from cognates. Warnow et al. (2006) applied methods taken from phylogenetics on Indo-European phonetic data in order to model language evolution. Heeringa and Joseph (2007) applied the Levensthein algorithm to the Dutch pronunciation data taken from Reeks Nederlandse Dialectatlassen and tried to reconstruct a 'proto-language' of Dutch dialects using the pairwise alignments. \n\nStudies in historical linguistics and dialectometry where string comparison is used as a basis for calculating the distances between language varieties will profit from tools to multi-align strings automatically and to calculate the distances between them. Good multiple alignment is of benefit to all those methods in diachronic linguistics such as the comparative reconstruction method or the so-called CHARACTER-BASED METHODS taken from phylogenetics, which have also been successfully applied in linguistics (Gray and Jordan, 2000;Gray and Atkinson, 2003;Atkinson et al., 2005;Warnow et al., 2006). The multialignment systems can help historical linguistics by reducing the human labor needed to detect the regular sound correspondences and cognate pairs of words. They also systematize the linguistic knowledge in intuitive alignments, and provide a basis for the application of the quantitative methods that lead to a better understanding of language variation and language change. \n\nIn this study we apply an iterative pairwise alignment program for linguistics, ALPHAMA-LIG, on phonetic transcriptions of words used in dialectological research.",
            "score": 0.3794879258510732,
            "section_title": "Introduction",
            "char_start_offset": 2363,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 683
                },
                {
                    "start": 686,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 1003
                },
                {
                    "start": 1004,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1365
                },
                {
                    "start": 1368,
                    "end": 1624
                },
                {
                    "start": 1625,
                    "end": 1970
                },
                {
                    "start": 1971,
                    "end": 2136
                },
                {
                    "start": 2137,
                    "end": 2355
                },
                {
                    "start": 2358,
                    "end": 2520
                }
            ],
            "ref_mentions": [
                {
                    "start": 1004,
                    "end": 1024,
                    "matchedPaperCorpusId": "5485083"
                },
                {
                    "start": 1135,
                    "end": 1161,
                    "matchedPaperCorpusId": "2046924"
                },
                {
                    "start": 1880,
                    "end": 1903,
                    "matchedPaperCorpusId": "4413001"
                },
                {
                    "start": 1903,
                    "end": 1927,
                    "matchedPaperCorpusId": "42340"
                },
                {
                    "start": 1927,
                    "end": 1949,
                    "matchedPaperCorpusId": "14474682"
                },
                {
                    "start": 1949,
                    "end": 1969,
                    "matchedPaperCorpusId": "5485083"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06475830078125
        },
        {
            "corpus_id": "260125946",
            "title": "External Reasoning: Towards Multi-Large-Language-Models Interchangeable Assistance with Human Feedback",
            "text": "The hypothesis in this paper is that alignment can be a simple process where the model learns the style or format for interacting with users, to expose the knowledge and capabilities that were already acquired during pretraining. This is supported by the fact that the authors were able to achieve remarkably strong performance by simply fine-tuning a 65B-parameter LLaMa language model on just 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. The authors collected a dataset of 1,000 prompts and responses where the outputs (responses) were stylistically aligned with each other, but the inputs (prompts) were diverse, and the model was trained on this set of examples.\n\nResponse from Vicuna-33B Token indices sequence length is longer than the specified maximum sequence length for this model (3045 \u00bf 2048). Running this sequence through the model will result in indexing errors The hypothesis about alignment in the paper is the \"Superficial Alignment Hypothesis.\" This hypothesis suggests that a model's knowledge and capabilities are primarily learned during pretraining, while alignment teaches the model which subdistribution of formats should be used when interacting with users [Kirstain et al., 2021]. In simpler terms, the model learns the style or format for interacting with users, exposing the knowledge and capabilities acquired during pretraining. The authors of the paper set out to test this hypothesis by curating a dataset of 1,000 prompts and responses, where the outputs are stylistically aligned with each other, but the inputs are diverse. The outputs are designed to be in the style of a helpful AI assistant. They collected these examples from various sources, primarily split into community Q&A forums and manually authored examples. They then trained a pretrained 65B-parameter LLaMa model, LIMA, on this set of 1,000 demonstrations, fine-tuning it to confirm their hypothesis. The results showed that LIMA performed remarkably well, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. The model also tended to generalize well to unseen tasks that did not appear in the training data. Response from Vicuna-13B I'm sorry, but I do not have access to the PDF you are referring to",
            "score": 0.3791228937346157,
            "section_title": "Response from MPT-30B-Chat",
            "char_start_offset": 46109,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.27880859375
        },
        {
            "corpus_id": "264487236",
            "title": "The Distributional Hypothesis Does Not Fully Explain the Benefits of Masked Language Model Pretraining",
            "text": "Our main goal is to provide a new perspective to analyze the efficacy of pretraining and to show that the distributional hypothesis is not a sufficient explanation. We only study semantic equivalence between features. Other relationships, such as hypernyms or causal relationships, are not in the scope of this paper. Additionally, we only consider a relatively simple case, where tasks are patternmatching tasks. We do not consider, for example, tasks that require reasoning. In \u00a74, we study the effect of the distributional property using Markov distributions. It only suggests that model can achieve better sample efficiency by utilizing the semantic relationship encoded in this simple first-order distribution. How the models are able to utilize semantics encoded in more complicated distributions, such as the distribution of natural languages, remains unknown. In \u00a76, we only consider the distribution of a single context token; however, the semantics of a feature may be encoded in the distribution in a subtler way. Finally, we show the inefficiency of the distributional hypothesis as an explanation by providing counterexamples with small language models. We leave the study of large language models for future work. A Toy Experiments in the Multi-token Setting \n\nA.1 Independent Experimental Variables Feature Levels. We can also control whether the semantic relationship in this pseudo language is at a single-token level or at a multi-token level, namely whether each feature corresponds to one or more tokens. This multi-token setting is interesting because the distribution of the token sequences will have higher-order dependence than a Markov chain. Additionally, multi-token features also make this pseudo-language more similar to natural languages. We decide the mapping between features and token sequences before we start generating passages. We use two vocabulary sets V \u03b1 and V \u03b2 for the feature sets \u03a6 a and \u03a6 b respectively. In the single-token setting, each feature in \u03a6 a and \u03a6 b is bijectively mapped to a token in V \u03b1 and V \u03b2 respectively. In the multi-token setting, each feature in \u03a6 a and \u03a6 b corresponds to 1 to 3 randomly selected tokens from V \u03b1 and V \u03b2 respectively. \n\nVocabulary sharing between \u03a6 a and \u03a6 b . When the features are multi-token, V \u03b1 and V \u03b2 can be the same while keeping the mapping between features and token sequences bijective.",
            "score": 0.3783636254623841,
            "section_title": "Limitations",
            "char_start_offset": 30509,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 476
                },
                {
                    "start": 477,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1166
                },
                {
                    "start": 1167,
                    "end": 1227
                },
                {
                    "start": 1228,
                    "end": 1272
                },
                {
                    "start": 1275,
                    "end": 1329
                },
                {
                    "start": 1330,
                    "end": 1524
                },
                {
                    "start": 1525,
                    "end": 1667
                },
                {
                    "start": 1668,
                    "end": 1768
                },
                {
                    "start": 1769,
                    "end": 1864
                },
                {
                    "start": 1865,
                    "end": 1950
                },
                {
                    "start": 1951,
                    "end": 2069
                },
                {
                    "start": 2070,
                    "end": 2203
                },
                {
                    "start": 2206,
                    "end": 2246
                },
                {
                    "start": 2247,
                    "end": 2383
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.047088623046875
        },
        {
            "corpus_id": "276318042",
            "title": "Few-shot LLM Synthetic Data with Distribution Matching",
            "text": "Distributional Alignment: We measure the alignment between the original and generated data using Wasserstein distance, which quantifies the cost of transforming one distribution into another. Lower values indicate better alignment. Sentence embeddings are extracted using a pre-trained BERT model and visualized with t-SNE for qualitative analysis. Quantitatively, Table 2 reports the Wasserstein distances for all datasets. SynAlign (random) achieves lower distances compared to SimPrompt and AttrPrompt, while SynAlign (MMD) consistently achieves the best alignment, demonstrating its effectiveness in selecting synthetic samples that better reflect the original data distribution. Vocabulary Diversity: We measure vocabulary diversity by calculating the vocabulary size, defined as the number of unique words in each dataset. As shown in Table 3, SynAlign (MMD) generates datasets with higher vocabulary diversity than SimPrompt and comparable diversity to AttrPrompt. For example, on SST-2, SynAlign (MMD) achieves a vocabulary size of 7.4k, significantly larger than SimPrompt (1k) and close to AttrPrompt (7.2k). Although the generated datasets have smaller vocabulary sizes than the original datasets, this is expected due to the limited input prompts provided to the LLM. However, combining synthetic and original data compensates for this limitation, as evidenced by the performance improvements in Section 3. These results show that SynAlign not only enhances distributional alignment but also generates more linguistically diverse text, contributing to its superior performance.",
            "score": 0.375857175152596,
            "section_title": "Generated Data Analysis",
            "char_start_offset": 21564,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1589
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1378173828125
        },
        {
            "corpus_id": "270688753",
            "title": "Brain-Like Language Processing via a Shallow Untrained Multihead Attention Network",
            "text": "Tokenization & Multihead Attention Our findings identify token frequency and aggregation via multihead attention or a simple mean operation as critical components driving brain alignment (Section 5).Utilizing a Byte Pair Encoding (BPE) tokenizer enhances brain alignment by effectively encoding token frequency, breaking down infrequent words into more tokens (Kida et al., 1999;Sennrich et al., 2016).This frequency-based tokenization scheme allow the model to more accurately capture linguistic nuances, aligning well with the sensitivity of humans to lexical frequency in language comprehension (Rayner and Duffy, 1986;Singh et al., 2016).Further aggregating these tokens through a multihead attention mechanism then constructs diverse representations between each token pair within the context.The improved alignment from an increased number of attention heads is most likely due to greater expressivity achieved by multiple heads, enabling the model to presumably capture an array of context-dependent relations.\n\nEncoding via Architectural Priors Aid Language Decoding We found that passing localized units as token representations to downstream trainable modules leads to significantly better language modeling performance (Section 6).Through structural priors inherent in the architecture, the representations are potentially constrained to a space from which it is easier to generalize.In neuroscience terms, this approach follows an encoder-decoder setup where the untrained SUMA encoder provides generic representations that the downstream trainable decoder can then make effective use of.Interestingly, our combined models are significantly more aligned to human reading times than even much larger pretrained models.Similar observations have been reported by Steuer et al. (2023), Oh andSchuler (2023), andShain et al. (2024b).\n\nA Need for Better Brain Benchmarks Although we are excited about the strong brain alignment of our model, we do not believe it to be a perfect model of the human language system.First, while cross-subject consistency estimates typically attempt to compute an \"upper bound\" or ceiling of how well the best possible model could perform, this estimate is routinely surpassed by the best models (Table 2).We believe the data consistency is under-estimated by trying to predict noisy data (target subject) from other noisy data (held-out subject pool; whereas model predictors are not noisy).",
            "score": 0.3748464388840419,
            "section_title": "Discussion",
            "char_start_offset": 27516,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 199,
                    "end": 402
                },
                {
                    "start": 402,
                    "end": 642
                },
                {
                    "start": 642,
                    "end": 798
                },
                {
                    "start": 798,
                    "end": 1017
                },
                {
                    "start": 1019,
                    "end": 1242
                },
                {
                    "start": 1242,
                    "end": 1395
                },
                {
                    "start": 1395,
                    "end": 1600
                },
                {
                    "start": 1600,
                    "end": 1729
                },
                {
                    "start": 1729,
                    "end": 1840
                },
                {
                    "start": 1842,
                    "end": 2020
                },
                {
                    "start": 2020,
                    "end": 2243
                },
                {
                    "start": 2243,
                    "end": 2429
                }
            ],
            "ref_mentions": [
                {
                    "start": 379,
                    "end": 401,
                    "matchedPaperCorpusId": "1114678"
                },
                {
                    "start": 598,
                    "end": 622,
                    "matchedPaperCorpusId": "34091980"
                },
                {
                    "start": 622,
                    "end": 641,
                    "matchedPaperCorpusId": "17880853"
                },
                {
                    "start": 1794,
                    "end": 1800,
                    "matchedPaperCorpusId": "255096260"
                },
                {
                    "start": 1800,
                    "end": 1819,
                    "matchedPaperCorpusId": "255096260"
                },
                {
                    "start": 1819,
                    "end": 1839,
                    "matchedPaperCorpusId": "268085307"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11126708984375
        },
        {
            "corpus_id": "9895822",
            "title": "Semi-Markov Phrase-Based Monolingual Alignment",
            "text": "Various NLP tasks can be treated as an alignment problem: machine translation (aligning words in one language with words in another language), question answering (aligning question words with the answer phrase), textual entailment recognition (aligning premise with hypothesis), paraphrase detection (aligning semantically equivalent words), etc. Even though most of these tasks involve only a single language, alignment research has primarily focused on the bilingual setting (i.e., machine translation) rather than monolingual. Moreover, most work has considered token-based approaches over phrase-based. 1 Here we seek to address this imbalance by proposing better phrase-based models for monolingual word alignment. * Performed while faculty at Johns Hopkins University. 1 In this paper we use the term token-based alignment for one-to-one alignment and phrase-based for non one-to-one alignment, and word alignment in general for both.\n\nMost token-based alignment models can extrinsically handle phrase-based alignment to some extent. For instance, in the case of NYC aligning to New York City, the single source word NYC may align three times separately to the target words: NYC\u2194New, NYC\u2194York, NYC\u2194City. Or in the case of identical alignment, New York City aligning to New York City is simply New\u2194New, York\u2194York, City\u2194City. However, it is not as clear how to token-align New York (as a city) with New York City. The problem is more prominent when aligning phrasal paraphrases or multiword expressions, such as pass away and kick the bucket. This suggests an intrinsically phrase-based alignment model.\n\nThe token aligner jacana-align (Yao et al., 2013a) has achieved state-of-the-art result on the task of monolingual alignment, based on previous work of Blunsom and Cohn (2006). It employs a Conditional Random Field (Lafferty et al., 2001) to align tokens from the source sentence to tokens in the target sentence, by treating source tokens as \"observation\" and target tokens as \"hidden states\". However, it is not designed to handle phrase-based alignment, largely due to the Markov nature of the underlying",
            "score": 0.3731611967179098,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1640,
                    "end": 1659,
                    "matchedPaperCorpusId": "14644892"
                },
                {
                    "start": 1761,
                    "end": 1784,
                    "matchedPaperCorpusId": "14861523"
                },
                {
                    "start": 1824,
                    "end": 1847,
                    "matchedPaperCorpusId": "219683473"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0552978515625
        },
        {
            "corpus_id": "247011700",
            "title": "Interpreting Language Models with Contrastive Explanations",
            "text": "We use GPT-2 (Radford et al., 2019) and GPT-Neo (Black et al., 2021) to extract explanations. GPT-2 is a large autoregressive transformer-based LM with 1.5 billion parameters and trained on 8 million web pages. GPT-Neo is a similar LM with 2.7 billion parameters and trained on The Pile (Gao et al., 2020) containing 825.18GB of largely En-glish text. In addition to the explanation methods described above, we also set up a random baseline as a comparison, where we create a vector of the same size as explanations with values randomly sampled from a uniform distribution over [0, 1).\n\nIn Figure 1, we can see that overall, contrastive explanations have a higher alignment with linguistic paradigms than their non-contrastive counterparts for both GPT-2 and GPT-Neo across the different metrics. Although non-contrastive explanations do not always outperform the random baseline, contrastive explanations have a better alignment with BLiMP than random vectors for most cases.  Table 3: Alignment of GPT-2 explanations to known evidence on examples where the model makes a correct (left) and incorrect (right) prediction, according to dot product (DP), probes needed (PN), and mean reciprocal rank (MRR). Alignment scores that are better than the score for the analogous explanation method with the different contrastive setting are bolded.\n\nIn Table 3, we further examined alignment between model explanations and known evidence on instances where the model correctly allocates more probability to the acceptable token, or incorrectly selects the other token. On examples where the model makes an incorrect prediction, it is not clear whether non-contrastive or contrastive methods have better alignment. On examples where the model predicts correctly, contrastive explanations obtain better alignment than their non-contrastive counterparts for each explanation method and alignment metric.\n\nIn Figure 2, we see that for most explanation methods, the larger the distance between the known evidence and the target token, the larger the increase in alignment of contrastive explanations over non-contrastive explanations. This suggests that contrastive explanations particularly outperform non-contrastive ones when the known evidence is relatively further away from the target token, that is",
            "score": 0.3718188824928582,
            "section_title": "Results",
            "char_start_offset": 11934,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1297607421875
        },
        {
            "corpus_id": "271600915",
            "title": "ABC Align: Large Language Model Alignment for Safety & Accuracy",
            "text": "There has been significant recent work on exploiting the ICL capabilities of frontier models (Agarwal et al., 2024) where the context length has increased by up to an order-of-magnitude, as seen for instance in Google's Gemini and Anthropic's Claude series. While performance of fine-tuning in an ICL setting doesn't yet offer consistent improvements when compared with SFT-based approaches across all tasks, there are specific cases where results are promising (Razumovskaia et al., 2024). \n\nThe work of Lin et al. (2023) further quantifies the 'Superficial Alignment Hypothesis' proposed in LIMA. They measure the token distribution shift between pre-trained and aligned models, specifically the difference in probabilities of predicted tokens given a position in some sample input across the two models. They find that alignment fine-tuning affects a small subset of tokens that they describe as 'stylistic', specifically 'discourse markers, transitional words, and safety disclaimers'. \n\nFurther to the original hypothesis of the LIMA authors, this provides evidence and motivation for the effectiveness of alignment happening in a post-training setting via fine-tuning, and that inference-time alignment may offer comparable results for applications where fine-tuning is unsuitable or not possible. This further validates our decision to not pre-train an LLM, but instead focus on post-training alignment across SFT, PO, and ICA settings in support of our overall aim to provide a lightweight, flexible methodology that considers organisational constraints and the rapid growth of underlying model capabilities.",
            "score": 0.3715875864212569,
            "section_title": "In-Context Alignment",
            "char_start_offset": 14781,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 490
                },
                {
                    "start": 493,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 989
                },
                {
                    "start": 992,
                    "end": 1303
                },
                {
                    "start": 1304,
                    "end": 1616
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81689453125
        },
        {
            "corpus_id": "202539454",
            "title": "Back to the Future - Sequential Alignment of Text Representations",
            "text": "Language evolves over time in many ways relevant to natural language processing tasks. For example, recent occurrences of tokens 'BERT' and 'ELMO' in publications refer to neural network architectures rather than persons. This type of temporal signal is typically overlooked, but is important if one aims to deploy a machine learning model over an extended period of time. In particular, language evolution causes data drift between time-steps in sequential decision-making tasks. Examples of such tasks include prediction of paper acceptance for yearly conferences (regular intervals) or author stance prediction for rumours on Twitter (irregular intervals). Inspired by successes in computer vision, we tackle data drift by sequentially aligning learned representations. %We consider both unsupervised and semi-supervised alignment. We evaluate on three challenging tasks varying in terms of time-scales, linguistic units, and domains. These tasks show our method outperforming several strong baselines, including using all available data. We argue that, due to its low computational expense, sequential alignment is a practical solution to dealing with language evolution.",
            "score": 0.36970099825250446,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06951904296875
        },
        {
            "corpus_id": "265552122",
            "title": "MARKMyWORDS: Analyzing and Evaluating Language Model Watermarks",
            "text": "We now give more details about the four sampling algorithms initially discussed in Section II-C. \n\n(Fig. 10, C1) Exponential. Conceptualized by Aaronson and Kirchner [1] and further employed by Kuditipudi et al. [37], this algorithm leverages the Gumbel-max trick. Let D n = {\u03bb n i , 1 \u2264 i \u2264 d} be the distribution of the language model over the next token. The exponential scheme will select the next token as T n = arg max {i \u2264 d, log (h rn (i)) /\u03bb n i } where h is a keyed hash function using r n as its key. The per-token variable used in the statistical test is either s n = h rn (T n ) or s n = \u2212 log (1 \u2212 h rn (T n )), yielding identical results. Prior work uses the latter quantity. We adhere to this for our benchmark, but analyze the former in Section C. \n\n(Fig. 10, C2) Inverse transform. This scheme introduced by Kuditipudi et al. [37] derives a random permutation using the secret key \u03c0 k . The next token is selected as the smallest index in the inverse permutation such that the CDF of the next token distribution is at least r n . A detailed formula can be found in Fig. 10. Kuditipudi et al. [37] propose using s n = |r n \u2212 \u03b7 \u03c0 \u22121 k (T n ) | as a the test variable, where \u03b7 normalizes the token index to the [0, 1] range. \n\n(Fig. 10, C3) Binary. Proposed by Christ et al. [8] for binary alphabets, this algorithm can adapt to any model by encoding tokens into binary sequences. In our implementation, we rely on a Huffman encoding of the token set, using frequencies derived from a large corpus of natural text. In this case, the distribution over the next token reduces to a single probability p n that token \"0\" is selected next, and 1\u2212p that \"1\" is selected. The sampling rule selects 0 if r n < p, and 1 otherwise. The test variable for this case is \n\n(Fig. 10, C4) Distribution-shift.",
            "score": 0.36904343227561237,
            "section_title": "D. Sampling algorithm C",
            "char_start_offset": 43197,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 99,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 764
                },
                {
                    "start": 767,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1239
                },
                {
                    "start": 1242,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1679
                },
                {
                    "start": 1680,
                    "end": 1736
                },
                {
                    "start": 1737,
                    "end": 1771
                },
                {
                    "start": 1774,
                    "end": 1807
                }
            ],
            "ref_mentions": [
                {
                    "start": 1290,
                    "end": 1293,
                    "matchedPaperCorpusId": "259092330"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.159423828125
        },
        {
            "corpus_id": "276161664",
            "title": "Hierarchical Contextual Manifold Alignment for Structuring Latent Representations in Large Language Models",
            "text": "Alignment provided a structured approach for refining token embeddings within the latent space of a large language model, ensuring that representations exhibited improved coherence while maintaining linguistic integrity across diverse tasks. The results demonstrated that hierarchical restructuring reduced fragmentation within token distributions, contributing to enhanced semantic stability, increased retrieval accuracy for rare tokens, and greater consistency in long-range dependencies, which collectively improved the quality of contextual inference. The observed benefits extended beyond conventional fine-tuning techniques, as the proposed realignment strategy maintained pre-learned knowledge while refining structural organization in a manner that preserved essential contextual relationships and mitigated representational inconsistencies. The comparative analysis against existing embedding refinement methods established that a structured, non-parametric approach to representation alignment not only yielded improvements in token coherence but also reduced computational overhead associated with direct parameter modifications, highlighting the efficiency of hierarchical structuring in latent space optimization. The increased robustness to adversarial perturbations further suggested that systematic organization of token embeddings contributed to greater linguistic resilience, preventing spurious shifts in interpretation caused through minor syntactic or lexical variations in input prompts. The improvements in computational efficiency demonstrated that hierarchical realignment techniques introduced minimal resource overhead while yielding measurable gains in representation quality, indicating that non-parametric latent space modifications can serve as a viable alternative to more computationally intensive fine-tuning strategies. The structural organization imposed through hierarchical clustering and geodesic realignment ensured that semantically related tokens maintained consistent spatial relationships, reducing redundancy in representational distributions while preserving contextual distinctions critical for language understanding and text generation. The experimental findings reinforced the broader significance of structured latent space optimization in improving generalization capabilities across linguistic domains, illustrating that nonparametric embedding refinements can play a fundamental role in enhancing the internal representation quality of large language models.",
            "score": 0.3685614657849739,
            "section_title": "VII. CONCLUSION The introduction of Hierarchical Contextual Manifold",
            "char_start_offset": 34661,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 1227
                },
                {
                    "start": 1228,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1855
                },
                {
                    "start": 1856,
                    "end": 2186
                },
                {
                    "start": 2187,
                    "end": 2513
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70068359375
        },
        {
            "corpus_id": "265608902",
            "title": "The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning",
            "text": "In this section, we examine the effect of alignment tuning (SFT & RLHF) by measuring the token distribution shift. Our key findings can be summarized as follows: \n\nAlignment affects only a very small fraction of tokens; the base and aligned LLMs behave the same in decoding on most positions, where they share the same top-ranked tokens. \n\nAlignment mainly concerns stylistic tokens, such as discourse markers, transitional words, and safety disclaimers, which only take about a very small part of the total token positions. Alignment is more critical for earlier tokens. For most positions, the aligned model's top-ranked token is within the top 5 tokens ranked by the base model. Base LLMs have already acquired adequate knowledge to follow instructions. They behave very similarly to aligned LLMs when given an appropriate context as a prefix.",
            "score": 0.36831227129623645,
            "section_title": "SUMMARY OF THE FINDINGS WITH TOKEN DISTRIBUTION SHIFT",
            "char_start_offset": 13238,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 161
                },
                {
                    "start": 164,
                    "end": 337
                },
                {
                    "start": 340,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 846
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.49853515625
        },
        {
            "corpus_id": "269588003",
            "title": "Semi-Parametric Retrieval via Binary Bag-of-Tokens Index",
            "text": "In this section, we elaborate on the rationale behind the semi-parametric design. The semi-parametric alignment is designed to be consistent with mask language models (MLMs) (Devlin et al., 2018) pre-training objecitve. \n\nMask language model pre-training. During pre-training, MLMs are optimized to predict masked tokens by leveraging the context. Specifically, given an input sequence of tokens x = [t 1 , t 2 , . . . , M (t i ), . . . , t n ] with t i masked, the MLM uses its prediction head (MLMH) with a softmax function to produce the probability V MLMH \u03b8 +softmax (Mask(t i )|x) of the masked token t i over a vocabulary. The ground truth probability is the one-hot representation of the token t i . In this paper, we refer to this type of representation as the bag-of-tokens (BoT) representation, denote as V BoT (t i ). \n\nThe mask token prediction task can be viewed as alignment between the vocabulary distribution of the masked token position with the one-hot representation V BoT (t i ) in a masked setup: \n\nAs a result, the representation V MLMH \u03b8 (t i |x) tends to assign large values to the dimension corresponding to t i , or that are semantically related to t i based on the context x. \n\nSemi-parametric alignment. The alignment between parametric and non-parametric representation can be expressed as follows: \n\nThe semi-parametric alignment is modeled after the MLM pre-training objective, as detailed in Equation 1, and expands by aligning multi-token representations between the query and passage. The consistency between upstream pre-training and downstream tuning supports the alignability of these two representations.",
            "score": 0.3680762757449329,
            "section_title": "RATIONALE OF SEMI-PARAMETRIC ALIGNMENT.",
            "char_start_offset": 11783,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 82,
                    "end": 219
                },
                {
                    "start": 222,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 828
                },
                {
                    "start": 831,
                    "end": 1017
                },
                {
                    "start": 1020,
                    "end": 1202
                },
                {
                    "start": 1205,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1327
                },
                {
                    "start": 1330,
                    "end": 1518
                },
                {
                    "start": 1519,
                    "end": 1642
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10504150390625
        },
        {
            "corpus_id": "274131858",
            "title": "I'm Spartacus, No, I'm Spartacus: Measuring and Understanding LLM Identity Confusion",
            "text": "In this step, we assess the influence of structures or datasets on large models by evaluating similarities to pinpoint the origins of identity confusion. Output similarity has proven to be an effective approach for fingerprinting LLMs [29], [42], [44]. When two LLMs with fundamentally distinct output distributions still demonstrate identity confusion, this strongly indicates that the confusion is unlikely to stem from one model directly using or replicating another. Instead, it is more plausibly attributed to hallucinations inherent in LLMs. Based on this hypothesis, our approach is as follows: \n\n\u2022 Classification: We categorize models with identity confusion into two groups-those with known structures and datasets, and those with unknown ones. \u2022 Correlation Analysis: Our experimental process consists of two key phases. In the first phase, we perform output testing on models sharing the same architecture and dataset (e.g., different versions of the same model from a single provider). If the results demonstrate significant output similarity, this would validate the effectiveness of output similarity as a reliable method for fingerprinting. In the second phase, we test the outputs of large models identified as exhibiting identity confusion. Should we find substantial differences in their outputs, it would suggest that the observed identity confusion is more likely a result of hallucinations inherent to large language models rather than direct replication or use of another model. We prepare diverse datasets to challenge the models and elicit varied outputs. By analyzing word distributions, including part-of-speech frequencies, we establish a baseline through repeated tests on the same model to measure intramodel consistency. We then compare outputs across models using metrics like cosine and Jaccard similarity, along with Euclidean distance, to assess the similarity or dissimilarity in their output distributions. The Euclidean distance d between two points p = (p 1 , p 2 , . . . , p n ) and q = (q 1 , q 2 , . . . , q n ) in n-dimensional space is given by: \n\nThis represents the difference in part-of-speech distributions between two models given the same input, with a smaller Euclidean distance indicating closer output distributions and a higher similarity between models.",
            "score": 0.3668979255955764,
            "section_title": "P-(III) Output Distribution Analysis:",
            "char_start_offset": 28604,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 601
                },
                {
                    "start": 604,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1155
                },
                {
                    "start": 1156,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1500
                },
                {
                    "start": 1501,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1750
                },
                {
                    "start": 1751,
                    "end": 1942
                },
                {
                    "start": 1943,
                    "end": 2009
                },
                {
                    "start": 2010,
                    "end": 2044
                },
                {
                    "start": 2045,
                    "end": 2088
                },
                {
                    "start": 2091,
                    "end": 2307
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.090576171875
        },
        {
            "corpus_id": "273350763",
            "title": "Superficial Safety Alignment Hypothesis",
            "text": "(2) Binary Classification in SSAH: While SAH suggests that general alignment helps models choose the correct output subdistribution, SSAH posits that safety alignment simplifies this further by focusing on a binary classification task: either fulfill a request (if safe) or refuse it (if unsafe). This simplified framing of safety alignment is consistent with the \"superficial\" nature of SAH, where the alignment process fine-tunes how the model behaves in response to queries, rather than altering its deep internal structures. \n\n(3) Refusal Mechanisms and Format Control: Just as general alignment teaches models to structure their outputs in a user-friendly way, safety alignment in SSAH teaches models to issue consistent refusal mechanisms. These refusals take the form of standardized responses that indicate the model's compliance with safety guidelines, much like how general alignment might guide a model to give well-structured, polite answers to other types of questions. Importantly, this refusal mechanism makes it easier to choose the appropriate subdistribution of the output format. \n\nThe Superficial Alignment Hypothesis (SAH) as outlined in the Zhou et al. ( 2024) provides a theoretical framework for understanding how alignment processes operate in large language models. It suggests that alignment is largely superficial, conditioning the model on how to use its pretrained knowledge effectively. The Superficial Safety Alignment Hypothesis (SSAH) builds on this by applying similar principles to the realm of safety, simplifying the task of safety alignment to binary decisions regarding the fulfillment or refusal of unsafe requests. Both hypotheses underscore that alignment does not deeply alter the core abilities of the model, but rather adjusts the way those abilities are applied in specific contexts.",
            "score": 0.3663067360391083,
            "section_title": "DISCUSSION, LIMITATION, AND CONCLUSION",
            "char_start_offset": 35619,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 528
                },
                {
                    "start": 531,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1098
                },
                {
                    "start": 1101,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1830
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67431640625
        },
        {
            "corpus_id": "270063546",
            "title": "Phase Transitions in the Output Distribution of Large Language Models",
            "text": "In a physical system, changing parameters such as temperature can induce a phase transition: an abrupt change from one state of matter to another. Analogous phenomena have recently been observed in large language models. Typically, the task of identifying phase transitions requires human analysis and some prior understanding of the system to narrow down which low-dimensional properties to monitor and analyze. Statistical methods for the automated detection of phase transitions from data have recently been proposed within the physics community. These methods are largely system agnostic and, as shown here, can be adapted to study the behavior of large language models. In particular, we quantify distributional changes in the generated output via statistical distances, which can be efficiently estimated with access to the probability distribution over next-tokens. This versatile approach is capable of discovering new phases of behavior and unexplored transitions -- an ability that is particularly exciting in light of the rapid development of language models and their emergent capabilities.",
            "score": 0.36584941239182167,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.220458984375
        },
        {
            "corpus_id": "231740500",
            "title": "Fake it Till You Make it: Self-Supervised Semantic Shifts for Monolingual Word Embedding Tasks",
            "text": "We introduced S4-D and S4-A as self-supervised approaches to detect word-level semantic shifts on monolingual corpora. Motivated by the unsupervised nature of this problem, we introduce self-supervision based on the perturbation of word vectors and apply it to binary classification and vector alignment. S4-D is presented as an alternative to baseline unsupervised methods for semantic shift detection, particularly in the case of binary classification. We show, through experiments in Section 4.1, that it achieves over 2\u00d7 higher F1-scores than baselines in the classification settings. Moreover, we show how the alignment of word embeddings affect the outcome of such methods. Particularly, we show that global alignment uses the assumption of smooth transition, which may not hold true in the scenario of cross-domain semantic shift, where many words can be highly shifted. For that reason, we present an extension of our method, named S4-A, that uses its predictions to refine the alignment of the input embeddings. We demonstrate its usefulness quantitatively, through the detection task in Section 4.2, where S4-A allows for the detection of unique words when using a simple cosine distance baseline. Qualitatively, we demonstrate that S4-A is able to discovery novel shifts when compared to other alignment methods.\n\nThere are still open questions on how the self-supervised model is affected according to part-of-speech, frequency range, and degree of polysemy of words. In addition, factors such as number of tokens, vocabulary size, and degree of change of the language may impact the quality of the embeddings, therefore, affect the semantic shift detection.\n\nWhile this remains a difficult task, we believe that this work will help numerous applications of semantic shift detection and alignment that have been recently explored, especially in the monolingual and cross-domain setting.",
            "score": 0.3655934149926935,
            "section_title": "Conclusions",
            "char_start_offset": 30099,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1181640625
        },
        {
            "corpus_id": "235265909",
            "title": "Language Model Evaluation Beyond Perplexity",
            "text": "Neural language models 1 have become shockingly good at modeling natural language data in recent years (Merity et al., 2017;Conneau and Lample, 2019;Radford et al., 2019). Thus, to test just how well neural language models capture language NLP researchers have started to look beyond standard evaluation metrics such as perplexity, endeavoring to understand which underlying attributes of human language these models are learning. To this end, a nascent literature has emerged that focuses on probing language models (Belinkov and Glass, 2019), i.e., determining whether models encode linguistic phenomena. For the most part, these works have been limited to analyses of sentence-level phenomenon, such as subject-verb agreement (Gulordava et al., 2018) and garden path effects (van Schijndel and Linzen, 2018) among a myriad of other properties (Blevins et al., 2018;Chowdhury and Zamparelli, 2018, inter alia).\n\nIn this work, we attempt to understand which macro-level phenomena of human language today's language models reflect. That is, we pose the question: Do neural language models exhibit the statistical tendencies of human language? Phenomena that can be measured at this level provide an alternate view of a model's comprehension; for example, rather than exploring whether morphological agreement is captured, we look at whether our models learn the trends across a corpus as a whole, e.g., the token rank-frequency (Zipf's) relationship. In comparison to standard probing techniques, this framework does not require we know a priori how linguistic phenomena should manifest themselves. That is, when there is no law stating the theoretical tendencies of an attribute of natural language or we have reason to believe our language domain does not follow such a law, we can use the statistical tendencies present in empirical data as our baseline. This characteristic both allows us to assess a model's fit to highly corpus-dependent distributions-like the length distribution-and mitigates the biases introduced by our own preconceptions regarding properties of natural language. 2 More concretely, our paper describes an experimental design and accompanying hypothesis tests to determine precisely whether text generated from language models follows the same empirical trends as human language. Our experiments reveal that adherence to natural language tendencies",
            "score": 0.3651771352315846,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.197265625
        },
        {
            "corpus_id": "5984644",
            "title": "Finding structure in text, genome and other symbolic sequences",
            "text": "Statistical models for language can be used to develop sensitive statistical tests which can highlight the linguistic structure found in a text corpus. There is always the danger, however, that any observed structure is purely coincidental. If we take as an example the problem of determining whether two words tend to occur in proximity, the simple fact that these two words appear near each other may strongly indicate some association if they occur a large number of times together and never apart. On the other hand, this coincident appearance may only very weakly indicate a possible association if they occur once together (and never separately). In either case the evidence for association is unanimous, but in the second case we remain unconvinced that the association necessarily exists.\n\nConversely, after a large number of observations, we may become nearly certain that two words have a very subtle association which is not due to chance. The observed strength of an association can be subtle or readily apparent. We can also be more or less certain that the observed association is not accidental. These two characteristics of subtlety and accidental nature can thus appear independently.\n\nStatistical measures of association can, unfortunately, not measure both characteristics simultaneously. They can measure how unsubtle an association is, or they can measure how unlikely the observed association is due to chance. There is, by nature, a conflict between tests which indicate that two words are associated and tests which indicate that any particular association may have occurred by chance. In this dissertation, the strongest focus is on tests which, among other things, are useful for detecting anomalous association rather than measuring the strength of that observed association.\n\nThere are a number of measures of strength of association which have been proposed in the computational linguistics literature. The most commonly suggested measures are described in chapter 3 of van Rijsbergen's book [vR79]. Most measures of association strength are heuristic in nature. There have been far fewer tests proposed to detect anomalous association. Essentially the only measures which are used widely are the Zscore derived from single-cell mutual information (or log association ratio) proposed by Gale and Church [CGHH89] and the likelihood ratio test (or G 2 ) as proposed by this author [Dun93]. The preponderance of measures of association in the computational linguistics literature indicates clearly that the question of whether an observed association is due to",
            "score": 0.36498851303176744,
            "section_title": "Statistical tests",
            "char_start_offset": 106981,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04931640625
        },
        {
            "corpus_id": "248987203",
            "title": "The Geometry of Multilingual Language Model Representations",
            "text": "(iii) High variance in A, low variance in B: these axes may represent features that vary in language A but not in language B. Previous work has found evidence for these types of axes in multilingual language models (low cosine similarities along certain axes only in specific languages; Rajaee and Pilehvar, 2022), characterized as \"degenerate\" axes only in some languages. In our work (Section 4.1), we found that setting representations from language A equal to \u00b5 B along axes with low variance in language B induced more token predictions in language B than shifting by \u00b5 B \u2212 \u00b5 A alone. In other words, there was variance in language A (centered around \u00b5 A ) that may have been interpreted as noise (resulting in tokens outside language B) if simply shifted to be centered around \u00b5 B ; setting the representations equal to \u00b5 B along these axes was sufficient to remove this noise. These axes may have been encoding specific tokens or token features in language A, less relevant to language B (high variance in A, low variance in B). \n\nIn these cases, \u00b5 B may be close to or far from \u00b5 A , depending on whether language B has some fixed analogous feature value for the feature that varies in language A. Because there is little to no variance in language B along these axes, \u00b5 B essentially serves as a fixed bias term for language B representations. \n\n(iv) Unequal means, similarly high variance in each language: visually, these axes might represent languages as similar distributions shifted from one another (e.g. Figure 5, depicting language clusters). These shifts might reflect true differences between languages in the distributions of particular features, imperfect alignment between languages, or entirely different features for different languages (see Appendix C.1 below). \n\n(v) Unequal means, low variance in each language: due to the low variance within each language, these axes would represent different languages roughly as points, potentially encoding any of the wide variety of features that vary across languages (although with little variance within each language). However, there is little evidence that these axes exist in multilingual language models. Even axes identified using LDA between languages (designed to maximize variance between languages and minimize variance within languages) appeared closer to type (iv) above.",
            "score": 0.36432583102730487,
            "section_title": "C Language-sensitive vs. language-neutral axes",
            "char_start_offset": 40144,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 589
                },
                {
                    "start": 590,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 1035
                },
                {
                    "start": 1038,
                    "end": 1352
                },
                {
                    "start": 1355,
                    "end": 1559
                },
                {
                    "start": 1560,
                    "end": 1786
                },
                {
                    "start": 1789,
                    "end": 2088
                },
                {
                    "start": 2089,
                    "end": 2177
                },
                {
                    "start": 2178,
                    "end": 2351
                }
            ],
            "ref_mentions": [
                {
                    "start": 287,
                    "end": 313,
                    "matchedPaperCorpusId": "238583197"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.218994140625
        },
        {
            "corpus_id": "84842",
            "title": "Learning when to trust distant supervision: An application to low-resource POS tagging using cross-lingual projection",
            "text": "First, we present the results for the 8 simulation languages in Table 1. For most of the languages our method performs better than that of Duong et al. (2014)  We show the learned bias transformation matrices for the different languages in Figure 3. The blue (dark) cells in the grids denote values that are most highly weighted. Note the strong diagonal, showing that the tags are mostly trusted, although there is also evidence of significant mass in offdiagonal entries. The worst case is in Greek (el) with many weak values on the diagonal. In this case, PRON and X appear to be confused for one another. The light cells are also important, show- ing tag combinations that the model learns to ignore, such as CONJ vs DET in Spanish (es) and PRON vs ADP in Swedish (sv). The tokens that are CONJ in Spanish (es) are seldom projected as DET. Overall, for most of languages the level of debiasing is modest, which might not come as a surprise given the large, clean parallel corpus for learning word alignments. Now we present results for the two low-resource languages, Malagasy and Kinyarwanda, which both have much smaller parallel corpora. The results in Table 2 show that our method works better than all others in both languages, with a similar pattern of results as for the European languages.",
            "score": 0.36416227846303034,
            "section_title": "Results",
            "char_start_offset": 21092,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 72
                },
                {
                    "start": 73,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 1012
                },
                {
                    "start": 1013,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1301
                }
            ],
            "ref_mentions": [
                {
                    "start": 139,
                    "end": 158,
                    "matchedPaperCorpusId": "2349255"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.056976318359375
        },
        {
            "corpus_id": "258557914",
            "title": "Token-level Fitting Issues of Seq2seq Models",
            "text": "Previous studies suggest that long-tail token distribution affects the performance of NLP tasks (Gong et al., 2018;Raunak et al., 2020;Yu et al., 2022). \n\nWe hypothesize that the low-frequency tokens underfit during training and conduct verification experiments as follows. \n\nSettings We experiment on the News dataset, using a Transformer base model (Vaswani et al., 2017). We categorize the target tokens into high/medium/low-frequency according to their distribution in the training set, with balanced probability mass on the three buckets. \n\nHypothesis Testing For each group of high/medium/low-frequency tokens, we measure  10 8 6 4 2 0 2 4 6 8  the fitting-offset using the checkpoints of each model, obtaining 40 samples for each group. We test our hypothesis on each group using a sign-test (Eq. 1). As a result, we obtain a p-value of 1.9 \u00d7 10 \u22125 for high-frequency and 7.5 \u00d7 10 \u221211 for low-frequency, which strongly supports the hypothesis that the high/low-frequency tokens either overfit or underfit. \n\nFurther as Figure 3 shows, the average fittingoffset for high-frequency tokens is \u22123.7 with a standard deviation of 3.4. The negative value of the fitting-offset indicates that the high-frequency tokens overfit, where the best fit happens at an average of 3.7 epochs, before the early-stopping point. The average fitting-offset for low-frequency tokens is 5.8 with a standard deviation of 3.3. The positive value of the fitting-offset indicates underfitting, where the best fit happens at 5.8 epochs, after the early-stopping point on average. Based on this evidence, we conclude that Both overfitting and underfitting at the token level occur when training seq2seq models. \n\nAnalysis The significant divergence of fittingoffset between the high/low-frequency tokens suggests that the frequency of tokens has a significant influence on their fitting. We quantify the influence using the potential-gain. In particular, take the lowfrequency tokens as an example.",
            "score": 0.36397820631607025,
            "section_title": "Fitting of Rare Tokens in Seq2seq Model Training",
            "char_start_offset": 9671,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 155,
                    "end": 273
                },
                {
                    "start": 276,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 543
                },
                {
                    "start": 546,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 803
                },
                {
                    "start": 804,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 1012
                },
                {
                    "start": 1015,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1408
                },
                {
                    "start": 1409,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1688
                },
                {
                    "start": 1691,
                    "end": 1865
                },
                {
                    "start": 1866,
                    "end": 1917
                },
                {
                    "start": 1918,
                    "end": 1976
                }
            ],
            "ref_mentions": [
                {
                    "start": 96,
                    "end": 115,
                    "matchedPaperCorpusId": "52301591"
                },
                {
                    "start": 115,
                    "end": 135,
                    "matchedPaperCorpusId": "222290834"
                },
                {
                    "start": 135,
                    "end": 151,
                    "matchedPaperCorpusId": "247476436"
                },
                {
                    "start": 351,
                    "end": 373,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09088134765625
        },
        {
            "corpus_id": "273350763",
            "title": "Superficial Safety Alignment Hypothesis",
            "text": "Discussion. While our SSAH offers valuable insights into adversarial scenarios, such as jailbreak attacks, we do not propose a specific solution to address these issues in this work. If these issues could be resolved within the framework of our theory, the term \"Superficial\" in \"Superficial Safety Alignment Hypothesis\" may no longer be necessary. Interestingly, recent research provides some supporting evidence in this direction (Qi et al., 2024). However, it is also highly likely that advanced attacks may not be fully mitigated by relying solely on the model's internal mechanisms. A systematic, multi-layered approach, extending beyond the model itself, may be required to effectively defend against sophisticated adversarial threats. \n\nLimitation. When reallocating redundant units for safety purposes, we only explored the impact of the alignment method SFT. Due to resource limitations, we have not yet tested this approach on other alignment methods like PPO or DPO. \n\nConclusion. This paper distinguishes safety alignment from the general alignment in LLMs and addresses the three key questions: How does safety alignment affect model behavior? Why are safety mechanisms brittle? and How to mitigate the safety alignment tax? By answering these questions, we were able to demonstrate that safety alignment can be a straightforward process, rather than a myth. \n\nA APPENDIX: SUPERFICIAL SAFETY ALIGNMENT HYPOTHESIS \n\nIn this section, we provide additional technical details and clarifications to supplement the experiments and findings presented in the Superficial Safety Alignment Hypothesis (SSAH) section. These details help ensure reproducibility and offer deeper insights into how the Superficial Alignment Hypothesis (SAH) was adapted to focus on safety-specific concerns. We also explain the methodology behind model configuration, fine-tuning, and evaluation. This appendix includes further discussion on how general instruction-following models and safety-aligned models were fine-tuned and assessed to probe their reasoning directions when faced with malicious queries, and the results of these assessments are presented in detail. \n\nA.1 SUPERFICIAL ALIGNMENT HYPOTHESIS IN LIMA. \n\nThe Superficial Alignment Hypothesis (SAH), as proposed to Zhou et al. (2024), fundamentally challenges the traditional assumption that a language model requires extensive fine-tuning on instruction-following on preference data to align its responses with human expectation.",
            "score": 0.36256839269372604,
            "section_title": "DISCUSSION, LIMITATION, AND CONCLUSION",
            "char_start_offset": 29361,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 11
                },
                {
                    "start": 12,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 741
                },
                {
                    "start": 744,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 977
                },
                {
                    "start": 980,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1371
                },
                {
                    "start": 1374,
                    "end": 1425
                },
                {
                    "start": 1428,
                    "end": 1619
                },
                {
                    "start": 1620,
                    "end": 1789
                },
                {
                    "start": 1790,
                    "end": 1878
                },
                {
                    "start": 1879,
                    "end": 2152
                },
                {
                    "start": 2155,
                    "end": 2200
                },
                {
                    "start": 2203,
                    "end": 2477
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.326171875
        },
        {
            "corpus_id": "266573858",
            "title": "Beyond Output Matching: Bidirectional Alignment for Enhanced In-Context Learning",
            "text": "Large language models (LLMs) have shown impressive few-shot generalization on many tasks via in-context learning (ICL). Despite their success in showing such emergent abilities, the scale and complexity of larger models also lead to unprecedentedly high computational demands and deployment challenges. In reaction, researchers explore transferring the powerful capabilities of larger models to more efficient and compact models by typically aligning the output of smaller (student) models with that of larger (teacher) models. Existing methods either train student models on the generated outputs of teacher models or imitate their token-level probability distributions. However, these distillation methods pay little to no attention to the input, which also plays a crucial role in ICL. Based on the finding that the performance of ICL is highly sensitive to the selection of demonstration examples, we propose Bidirectional Alignment (BiAlign) to fully leverage the models' preferences for ICL examples to improve the ICL abilities of student models. Specifically, we introduce the alignment of input preferences between student and teacher models by incorporating a novel ranking loss, in addition to aligning the token-level output distribution. With extensive experiments and analysis, we demonstrate that BiAlign can consistently outperform existing baselines on a variety of tasks involving language understanding, reasoning, and coding.",
            "score": 0.3624581103715425,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.200927734375
        },
        {
            "corpus_id": "49306018",
            "title": "Evaluation of sentence embeddings in downstream and linguistic probing tasks",
            "text": "In Table 8 we report the results for the linguistic probing tasks and in Figure 3 we show a graphical comparison as well. \n\nAs we can see in Table 8, ELMo [33] was one of the methods that were able to achieve high performance on a broad set of different tasks. Interestingly, in the BShift (bi-gram shift) task, where the goal is to identify whether if two consecutive tokens within the sentence have been inverted or not, ELMo [33] achieved a result that was better by a large margin when compared to all other methods, clearly a benefit of the language model objective, where it makes it easy to spot token inversion in sentences such as \"This is my Eve Christmas\", a sample from the BShift dataset. \n\nIn [11], they found that the binned sentence length task (SentLen) was negatively correlated with the performance in downstream tasks. This hypothesis was also supported by the model learning dynamics, since it seems that as model starts to capture deeper linguistic properties, it will tend to forget about this superficial feature [11]. However, the [33] bag-of-words not only achieved the best result in the SentLent task but also in many downstream tasks. Our hypothesis is that this is due to the fact that ELMo [33] is a deep representation composed by different levels that can capture superficial features such as sentence length as well as deep linguistic properties as seen in the challenging SOMO task. ELMo [33] word embeddings can be seen as analogous to the hypercolumns [15] approach in Computer Vision, where multiple feature levels are aggregated to form a single pixelwise representation. We leave the exploration of probing tasks for each ELMo [33] layer representation to future research, given that it could provide a framework to expose the linguistic properties capture by each representation level of the LSTM. \n\nIn [11], they also found that the WC (Word Content) task was positively correlated with the performance in a wide variety of downstream tasks. However, in our evaluation, the p-mean [35] approach, which has achieved better results in the WC task did not exceed other techniques such as ELMo [33] bag-of-words or InferSent [10] and USE in the downstream classification tasks.",
            "score": 0.3619844071083812,
            "section_title": "Linguistic probing tasks",
            "char_start_offset": 23491,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 124,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 701
                },
                {
                    "start": 704,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 1042
                },
                {
                    "start": 1043,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1610
                },
                {
                    "start": 1611,
                    "end": 1838
                },
                {
                    "start": 1841,
                    "end": 1983
                },
                {
                    "start": 1984,
                    "end": 2215
                }
            ],
            "ref_mentions": [
                {
                    "start": 155,
                    "end": 159,
                    "matchedPaperCorpusId": "3626819"
                },
                {
                    "start": 428,
                    "end": 432,
                    "matchedPaperCorpusId": "3626819"
                },
                {
                    "start": 1056,
                    "end": 1060,
                    "matchedPaperCorpusId": "3626819"
                },
                {
                    "start": 1221,
                    "end": 1225,
                    "matchedPaperCorpusId": "3626819"
                },
                {
                    "start": 1423,
                    "end": 1427,
                    "matchedPaperCorpusId": "3626819"
                },
                {
                    "start": 1489,
                    "end": 1493,
                    "matchedPaperCorpusId": "12225766"
                },
                {
                    "start": 1667,
                    "end": 1671,
                    "matchedPaperCorpusId": "3626819"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12127685546875
        },
        {
            "corpus_id": "268230397",
            "title": "Greed is All You Need: An Evaluation of Tokenizer Inference Methods",
            "text": "Some analyses of tokenizers rely on training language models or translation models and evaluating their performance on downstream tasks. Using this process to isolate effects of tokenization hyperparameters, such as inference method, is both time-and resource-consuming, as well as unstable due to the introduction of multiple sources of randomness throughout the LM/TM pre-training and fine-tuning phases. Few measures have been introduced that are intrinsic to vocabularies and their direct application to corpora, and fewer still avoid conflating the measures with the objectives used in the vocabulary construction process itself. As a result, the body of work focused on improving tokenization schemes is still relatively small. We create and release a benchmark made to intrinsically evaluate subword tokenizers. We collected word-level datasets and information measures which have been shown, or hypothesized, to correlate with the performance of language models on various downstream tasks. Details on these resources are provided in Table 2. At present, the benchmark is focused on the English language, although corresponding datasets exist for others as well. \n\nMorphological alignment It is commonly assumed that, for a given tokenizer, alignment of word segments to morphological gold-standard segmentations is a predictor of the ability of a language model that uses the given tokenizer to represent words, especially 'complex' ones that are made up of several roots or contain multiple morphological affixes (Schick and Sch\u00fctze, 2019;Nayak et al., 2020;Hofmann et al., 2021;Gow-Smith et al., 2022). We follow Gow-Smith et al. ( 2022) and evaluate our tokenizers's alignment with morphological annotations found in LADEC (Gagn\u00e9 et al., 2019), MorphoLex (S\u00e1nchez-Guti\u00e9rrez et al., 2018), Mor-phyNet (Batsuren et al., 2021), andDagoBert (Hofmann et al., 2020).",
            "score": 0.36159235999406497,
            "section_title": "Intrinsic Benchmark",
            "char_start_offset": 7189,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 998
                },
                {
                    "start": 999,
                    "end": 1050
                },
                {
                    "start": 1051,
                    "end": 1170
                },
                {
                    "start": 1173,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1872
                }
            ],
            "ref_mentions": [
                {
                    "start": 1549,
                    "end": 1568,
                    "matchedPaperCorpusId": "226283858"
                },
                {
                    "start": 1568,
                    "end": 1589,
                    "matchedPaperCorpusId": "233880587"
                },
                {
                    "start": 1589,
                    "end": 1612,
                    "matchedPaperCorpusId": "248069436"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05291748046875
        },
        {
            "corpus_id": "211069110",
            "title": "Multilingual Alignment of Contextual Word Representations",
            "text": "From this closed-class vs open-class difference, we hypothesize that BERT's alignment of a particular word pair is influenced by the similarity of their usage statistics. Specifically, given that BERT is trained through masked word prediction, its embeddings are in large part determined by the co-occurrences between words. Therefore, two words that are used in similar contexts should be better aligned. This hypothesis provides an explanation of the closed-class vs open-class difference: closed-class words are typically grammatical, so they are used in similar ways across typologically similar languages. Furthermore, these words cannot be substituted for one another due to their grammatical function. Therefore, their usage statistics are a strong signature that can be used for alignment. On the other hand, open-class words can be substituted for one another: for example, in most sentences, the noun tokens could be replaced by a wide range of semantically dissimilar nouns with the sentence remaining syntactically well-formed. By this effect, many nouns have similar co-occurrences, making them difficult to align through masked word prediction alone. \n\nTo further test this hypothesis, we plot the word retrieval accuracy versus the difference between the frequency rank of the target and source word, where this difference measures discrepancies in usage, as depicted in Figure 3. We see that accuracy drops off significantly as the source-target difference increases, supporting our hypothesis. Furthermore, this shortcoming is remedied by alignment, revealing another systematic deficiency of multilingual pre-training.",
            "score": 0.360784365286708,
            "section_title": "USAGE HYPOTHESIS FOR ALIGNMENT",
            "char_start_offset": 24443,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1164
                },
                {
                    "start": 1167,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1636
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2022705078125
        },
        {
            "corpus_id": "276161664",
            "title": "Hierarchical Contextual Manifold Alignment for Structuring Latent Representations in Large Language Models",
            "text": "The empirical evaluation assessed the impact of Hierarchical Contextual Manifold Alignment (HCMA) on token representation coherence, structural consistency of latent spaces, and downstream language modeling performance. The results were analyzed across multiple dimensions, including modifications to token distribution properties, improvements in embedding organization, and comparative performance metrics against existing latent space refinement techniques. The experimental outcomes provided a comprehensive assessment of the degree to which HCMA improved structural integrity within token embeddings while maintaining overall model performance across different language modeling benchmarks. The findings were further contextualized through a comparative analysis with alternative methods to highlight advantages and tradeoffs associated with hierarchical realignment.",
            "score": 0.3607278251674272,
            "section_title": "V. RESULTS",
            "char_start_offset": 24430,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 872
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.423828125
        },
        {
            "corpus_id": "272593310",
            "title": "Alignment of Diffusion Models: Fundamentals, Challenges, and Future",
            "text": "Fig. 1. Comparison of the ratio (number) of papers on LLMs vs. diffusion models (left), and the focus on alignment of LLMs vs. diffusion models (right). This comparison highlights the relatively nascent focus on aligning diffusion models compared to LLMs. \n\ngenerate molecules with high binding affinity and structural rationality [59]. To address this mismatch, recent works have begun to optimize pre-trained diffusion models directly for human-preferred properties, aiming to control data generation [188] beyond simply modeling the training data distribution. \n\nWithin the community of language modeling, recent powerful large language models (LLMs) like GPT-4 [126], \n\nLlama 2 [181], and Llama 3 [43] are typically trained in two stages. In the first pre-training stage, they are trained on a vast textual corpus with the objective of predicting the next tokens. In the second post-training stage, they are fine-tuned to follow instructions, align with human preferences, and improve capabilities like coding and factuality. The post-training process usually involves supervised fine-tuning (SFT) followed by alignment with human feedback, using techniques such as reinforcement learning from human feedback (RLHF) [126,128], and direct preference optimization (DPO) [43,141]. LLMs trained using this two-stage process have achieved state-of-the-art performance [43,126] in various language generation tasks and have been deployed in commercial applications such as ChatGPT. \n\nInspired by the success of aligning LLMs, there is growing interest in better aligning diffusion models with human intentions to enhance their capabilities. Fig. 1 visualizes paper counts on LLMs and diffusion models, as well as their alignment studies 2 . The left pie chart shows LLMs account for 76.9% of the research (21,500 papers), compared to 23.1% (6,460 papers) for diffusion models. The right chart highlights that 92.6% of alignment studies focus on LLMs (1,070 papers), while only 7.4% (85 papers) address diffusion models. This disparity underscores the relatively early stage of alignment research for diffusion models compared to LLMs.",
            "score": 0.35963669962777295,
            "section_title": "Alignment of LLMs Alignment of Diffusion Models",
            "char_start_offset": 1084,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 7
                },
                {
                    "start": 8,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 255
                },
                {
                    "start": 258,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 563
                },
                {
                    "start": 566,
                    "end": 671
                },
                {
                    "start": 674,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1479
                },
                {
                    "start": 1482,
                    "end": 1638
                },
                {
                    "start": 1639,
                    "end": 1738
                },
                {
                    "start": 1739,
                    "end": 1874
                },
                {
                    "start": 1875,
                    "end": 2017
                },
                {
                    "start": 2018,
                    "end": 2132
                }
            ],
            "ref_mentions": [
                {
                    "start": 503,
                    "end": 508,
                    "matchedPaperCorpusId": "250699308"
                },
                {
                    "start": 1225,
                    "end": 1229,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1276,
                    "end": 1280,
                    "matchedPaperCorpusId": "258959321"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.102294921875
        },
        {
            "corpus_id": "273375030",
            "title": "The Fair Language Model Paradox",
            "text": "A major challenge in machine learning is designing algorithms that generalize well from training data. One of the classic methods for promoting generalization (Krogh & Hertz, 1991;Shalev-Shwartz & Ben-David, 2014) is the use of regularization techniques, such as L 2 regularization, to limit model complexity. Empirical evidence from classification problems with balanced classes shows that increasing weight decay, while effectively fitting the data and minimizing training loss, generally improves performance on unseen data. However, a recent study by Balestriero et al. (2024) demonstrates that when training classifiers like ResNet-50 (He et al., 2016) on computer vision tasks such as ImageNet classification (Russakovsky et al., 2015), higher weight decay leads to undesired behavior, causing the model to neglect certain classes. This class-dependent effect of regularization is further amplified by imbalanced class distributions, as increasing weight decay does not result in a uniform performance decline across all classes. Instead, the model underperforms on low-probability classes while performing significantly better on more prevalent ones. This class-dependent behavior is not unique to vision tasks. In Natural Language Processing (NLP), a shift from traditional classification settings has led to a lack of attention toward token frequency and the per-token effects of regularization. In modern large language models (LLMs) trained on text data (Brown et al., 2020;Radford et al., 2018;OpenAI et al., 2024;Anthropic, 2023;Ortiz, 2023), the task of predicting the next token from a large vocabulary also results in significant token frequency imbalances. For instance, as shown in Figures 3, in the IMDB dataset (Maas et al., 2011), 95% of the total tokens in the data are captured by the top 0.01% of tokens. This indicates that the vast majority of tokens appear infrequently, while a small set of tokens dominates, creating a substantial imbalance. Additionally, the proportion of low-frequency tokens tends to increase as the vocabulary expands. This raises a critical question:",
            "score": 0.3583179186899709,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1522,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 837
                },
                {
                    "start": 838,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1218
                },
                {
                    "start": 1219,
                    "end": 1404
                },
                {
                    "start": 1405,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1828
                },
                {
                    "start": 1829,
                    "end": 1970
                },
                {
                    "start": 1971,
                    "end": 2068
                },
                {
                    "start": 2069,
                    "end": 2101
                }
            ],
            "ref_mentions": [
                {
                    "start": 159,
                    "end": 180,
                    "matchedPaperCorpusId": "10137788"
                },
                {
                    "start": 555,
                    "end": 580,
                    "matchedPaperCorpusId": "248006086"
                },
                {
                    "start": 640,
                    "end": 657,
                    "matchedPaperCorpusId": "206594692"
                },
                {
                    "start": 1465,
                    "end": 1485,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1731,
                    "end": 1750,
                    "matchedPaperCorpusId": "1428702"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1292724609375
        },
        {
            "corpus_id": "248069295",
            "title": "Contextual Representation Learning beyond Masked Language Modeling",
            "text": "In the age of deep learning, the basis of representation learning is to learn distributional semantics. The target of distributional semantics can be summed up in the so-called distributional hypothesis (Harris, 1954): Linguistic items with similar distributions have similar meanings. To model similar meanings, traditional representation approaches (Mikolov et al., 2013;Pennington et al., 2014) (e.g., Word2Vec) model distributional semantics by defining tokens using context-independent (CI) dense vectors, i.e., word embeddings, and directly aligning the representations of tokens in the same context. Nowadays, pre-trained language models (PTMs) (Devlin et al., 2019;Radford et al., 2018;Qiu et al., 2020) expand static embeddings into contextualized representations where each token has two kinds of representations: contextindependent embedding, and context-dependent * Equal Contribution \u2020 This work is done at ByteDance AI Lab.\n\nEnc. (CD) dense representation that stems from its embedding and contains context information. Although language modeling and representation learning have distinct targets, masked language modeling is still the prime choice to learn token representations with access to large scale of raw texts (Peters et al., 2018;Devlin et al., 2019;Raffel et al., 2020;Brown et al., 2020).\n\nIt naturally raises a question: How do masked language models learn contextual representations? Following the widely-accepted understanding (Wang and Isola, 2020), MLM optimizes two properties, the alignment of contextualized representations with the static embeddings of masked tokens, and the uniformity of static embeddings in the representation space. In the alignment property, sampled embeddings of masked tokens play as an anchor to align contextualized representations. We find that although such local anchor is essential to model local dependencies, the lack of global anchors brings several limitations. First, experiments show that the learning of contextual representations is sensitive to embedding quality, which harms the efficiency of MLM at the early stage of training. Second, MLM typically masks multiple target words in a sentence, resulting in multiple embedding anchors in the same context",
            "score": 0.35799564851943133,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 351,
                    "end": 373,
                    "matchedPaperCorpusId": "16447573"
                },
                {
                    "start": 373,
                    "end": 396,
                    "matchedPaperCorpusId": "1957433"
                },
                {
                    "start": 652,
                    "end": 673,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1234,
                    "end": 1255,
                    "matchedPaperCorpusId": "3626819"
                },
                {
                    "start": 1255,
                    "end": 1275,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1275,
                    "end": 1295,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1457,
                    "end": 1479,
                    "matchedPaperCorpusId": "218718310"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1251220703125
        },
        {
            "corpus_id": "10618934",
            "title": "Building Probabilistic Models for Natural Language",
            "text": "Models that capture the hidden structure underlying language have the potential to outperform shallow models such as n-gram models. Not only does our work in grammar induction forward research in building compact models as described above, it also demonstrates techniques for inducing hidden structure. (Clearly, the problems of sparse data and inducing hidden structure are not completely orthogonal, as taking advantage of hidden structure can lead to more compact models.) In Section 3.6, we show how on artificially-generated text our grammar induction algorithm is able to capture much of the structure present in the grammar used to generate the text, demonstrating that our algorithm can effectively extract hidden structure from data.\n\nOur work in bilingual sentence alignment also addresses the problem of inducing hidden structure. Given a raw bilingual corpus, sentence alignment involves recovering the hidden mapping between the two texts that specifies the sentence(s) in one language that translate to each sentence in the other language. To this end, our alignment algorithm also calculates a rough mapping between individual words in sentences in the two languages. Unlike in grammar induction, the model used to induce this hidden structure is a fairly shallow model; the model is just used to annotate data with the extracted structural information. This annotated data can then be used to train structured models, as in work by .\n\nIn this work on hidden structure, we place an emphasis on the use of efficient algorithms. A major issue in inducing hidden structure is constraining the search process. Because the structure is hidden, it is difficult to select which structures to consider creating, and as a result many algorithms dealing with hidden structure induction are inefficient because they do not adequately constrain the search space. Our algorithms for grammar induction and bilingual sentence alignment are both near-linear; both are far more efficient than all other algorithms (involving hidden structure induction) that offer comparable performance.\n\nWe achieve this efficiency through data-driven heuristics that constrain the set of hypotheses considered in the search process and through heuristics that allow hypotheses to be evaluated very quickly. In grammar induction, we introduce the concept of triggers, or particular patterns in the data that indicate that the creation of certain rules may be favorable. Triggers reduce the number of grammars considered to a manageable amount. In addition, to evaluate the objective function efficiently, we use sensible heuri",
            "score": 0.3579152567309107,
            "section_title": "Inducing Hidden Structure",
            "char_start_offset": 292153,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08599853515625
        },
        {
            "corpus_id": "11004224",
            "title": "Learning Natural Language Inference with LSTM",
            "text": "To obtain a better understanding of how our proposed model actually performs the matching between a premise and a hypothesis, we further conduct the following analyses. First, we look at the learned word-by-word alignment weights \u03b1 kj to check whether the soft alignment makes sense. This is the same as what was done by Rockt\u00e4schel et al. (2016). We then look at the values of the various gate vectors of the mLSTM. By looking at these values, we aim to check (1) whether the model is able to differentiate between more important and less important word-level matching results, and (2) whether the model forgets certain matching results and remembers certain other ones. \n\nTo conduct the analyses, we choose three examples and display the various learned parameter values. These three sentence pairs share the same premise but have different hypotheses and different relationship labels. They are given in Table 3. The values of the alignment weights and the gate vectors are plotted in Figure 2. \n\nBesides using the three examples, we will also give some overall statistics of the parameter values to confirm our observations with the three examples.",
            "score": 0.3579152567309107,
            "section_title": "Further Analyses",
            "char_start_offset": 16455,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 671
                },
                {
                    "start": 674,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 997
                },
                {
                    "start": 1000,
                    "end": 1152
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09857177734375
        },
        {
            "corpus_id": "247594716",
            "title": "MELM: Data Augmentation with Masked Entity Language Modeling for Low-Resource NER",
            "text": "On sentence level tasks, one line of data augmentation methods are built upon word-level modifications, which can be based on synonym replacement (Wei and Zou, 2019), LSTM language model (Kobayashi, 2018), MLM (Wu et al., 2019;Kumar et al., 2020), auto-regressive pretrained LM (Kumar et al., 2020), or constituent-based tagging schemes (Zhong et al., 2020). However, these methods suffer from token-label misalignment when applied to token-level tasks such as NER, which requires sophisticated post-processing to remove noisy samples in augmented data (Bari et al., 2021;Zhong and Cambria, 2021).\n\nExisting works avoid token-label misalignment by replacing entities with existing entities of the same class (Dai and Adel, 2020), or only modifying context works and leaving entities / aspect terms unchanged (Li et al., 2020a). Others attempt to produce augmented data by training / fine-tuning a generative language model on linearized labeled sequences (Ding et al., 2020;.\n\nBacktranslation (Sennrich et al., 2016;Fadaee et al., 2017;Dong et al., 2017;Yu et al., 2018) translates source language sentences into a target language, and subsequently back to the source language, which preserve the overall semantics of the original sentences. On token-level tasks, however, they hinge on external word alignment tools for label propagation, which are often error-prone (Tsai et al., 2016;Li et al., 2020b).",
            "score": 0.35769287851249987,
            "section_title": "Related Work",
            "char_start_offset": 23101,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 146,
                    "end": 165,
                    "matchedPaperCorpusId": "59523656"
                },
                {
                    "start": 187,
                    "end": 204,
                    "matchedPaperCorpusId": "21725995"
                },
                {
                    "start": 210,
                    "end": 227,
                    "matchedPaperCorpusId": "56482333"
                },
                {
                    "start": 227,
                    "end": 246,
                    "matchedPaperCorpusId": "211987786"
                },
                {
                    "start": 278,
                    "end": 298,
                    "matchedPaperCorpusId": "211987786"
                },
                {
                    "start": 337,
                    "end": 357,
                    "matchedPaperCorpusId": "218561923"
                },
                {
                    "start": 553,
                    "end": 572,
                    "matchedPaperCorpusId": "235624333"
                },
                {
                    "start": 708,
                    "end": 728,
                    "matchedPaperCorpusId": "225041226"
                },
                {
                    "start": 808,
                    "end": 826,
                    "matchedPaperCorpusId": "216914236"
                },
                {
                    "start": 993,
                    "end": 1016,
                    "matchedPaperCorpusId": "15600925"
                },
                {
                    "start": 1016,
                    "end": 1036,
                    "matchedPaperCorpusId": "3291104"
                },
                {
                    "start": 1036,
                    "end": 1054,
                    "matchedPaperCorpusId": "1282002"
                },
                {
                    "start": 1368,
                    "end": 1387,
                    "matchedPaperCorpusId": "2889848"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0401611328125
        },
        {
            "corpus_id": "273877763",
            "title": "Analyzing The Language of Visual Tokens",
            "text": "To expand our discussion on structural similarity, we further investigate how similar the topological structures of visual and textual tokens are, and whether these similarities can reveal meaningful insights about the underlying representations, i.e. can we observe strong structural alignment points between the natural and visual latent spaces, or are there notable deviations? \n\nWe begin by training GloVe embeddings (Pennington et al., 2014) on co-occurrence matrices derived from visual tokens and textual tokens present in the captions (details in Appendix J). This gives us a continuous topology of similar dimension within which we can explore potential alignment. We then explore two pairwise distance matrices between the two GloVe vector spaces: Procrustes alignment (Gower, 1975) and directed Haussdorf distance (Bowen, 1979).  5. While there are few clear trends, a key finding is that vision models are largely more aligned with natural language models than they are with each other, with Chameleon being slightly more central than other models (perhaps due to its training process). Overall, the lack of strong alignment trends between different vision models highlights that their latent spaces are more fragmented, suggesting that visual token representations are often model-specific or task-dependent, rather than universally structured. Notably, however, some languages align much better with visual models than others (such as Korean to the Chameleon tokenizers, or Hungarian/Polish in general), suggesting that some tokenizers may be significantly stronger when aligning to specific languages. Another interesting observation is that the directed Hausdorff distance shows that the natural language to vision model alignment is significantly further than the vision model to natural language alignment. This results implies that generation of images from text is much harder than the generation of text from images -something often observed in practice. \n\nGiven the overall distances between these structural representations, our experiments suggest that future model architectures should focus on reducing this asymmetry. Specialized models that effectively encode multimodal information -and perhaps aligned tokenization methods (such as CLIP), represent promising future directions for research.",
            "score": 0.3570792729552138,
            "section_title": "TOPOLOGICAL SIMILARITY",
            "char_start_offset": 29536,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 380
                },
                {
                    "start": 383,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 1098
                },
                {
                    "start": 1099,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1616
                },
                {
                    "start": 1617,
                    "end": 1824
                },
                {
                    "start": 1825,
                    "end": 1975
                },
                {
                    "start": 1978,
                    "end": 2144
                },
                {
                    "start": 2145,
                    "end": 2320
                }
            ],
            "ref_mentions": [
                {
                    "start": 421,
                    "end": 446,
                    "matchedPaperCorpusId": "1957433"
                },
                {
                    "start": 779,
                    "end": 792,
                    "matchedPaperCorpusId": "122244491"
                },
                {
                    "start": 825,
                    "end": 838,
                    "matchedPaperCorpusId": "55631433"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06683349609375
        },
        {
            "corpus_id": "251223848",
            "title": "Mismatching-Aware Unsupervised Translation Quality Estimation For Low-Resource Languages",
            "text": "For the WMT21 test data, we evaluate our methods on both tasks of postediting effort (HTER) and Direct Assessment to investigate how our proposed method performs for either of these tasks. Experimental results are shown in Table 6. Similar to the En-Fa results, the outcomes show that both strategies of replacing untranslated tokens and cross-lingual alignment of the pre-trained model would be beneficial and improve the Pearson correlation for almost all the language pairs and both HTER and DA tasks. It can also be seen that fine-tuning the model on word alignments from all the languages together (Aligned-XLMR-6-lang-pair) helps the method get better results for all the language pairs. Furthermore, incorporating it with the replacement strategy of untranslated tokens (Aligned-XLMR-6-lang-pair+Unk) gives the best results for almost all the language pairs, as for En-Fa in the previous section. To compare our results with other methods, Table 6 also includes the results of the WMT21 supervised baseline, as well as the only fully unsupervised method that participated in WMT21, from Specia et al (2021) (i.e., SMOB-ECEIIT). The WMT21 baseline is a supervised transformer-based predictor-estimator model (Kim et al, 2017;Kepler et al, 2019b), which uses the concatenated training portions of the WMT21 data (all seven language pairs together) for training on the corresponding task scores. Thus, it performs much better than our unsupervised method. However, we could get very close (less than 0.01 difference in Pearson correlation) to this supervised baseline in two of the zero-shot settings where no training data was available (i.e., Km-En in the HTER Task and Ps-En in the DA Task). Furthermore, our best results could surpass the WMT21 unsupervised participant for all the intended language pairs, although it was provided only for the DA Task. \n\nIn SMOB-ECEIIT, they proposed two methods to compute distances between the source and translated candidate sentences and combined them linearly to get the final QE scores.",
            "score": 0.3553857074167894,
            "section_title": "WMT21 Results",
            "char_start_offset": 37436,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1698
                },
                {
                    "start": 1699,
                    "end": 1861
                },
                {
                    "start": 1864,
                    "end": 2035
                }
            ],
            "ref_mentions": [
                {
                    "start": 1231,
                    "end": 1251,
                    "matchedPaperCorpusId": "67856167"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.042633056640625
        },
        {
            "corpus_id": "259859081",
            "title": "Masked Latent Semantic Modeling: an Efficient Pre-training Alternative to Masked Language Modeling",
            "text": "Recent successes in natural language processing are predominantly fueled by the use of large pretrained language models (PLMs) that are constructed in a self-supervised manner over massive amounts of raw text. Autoencoder-style language models (Devlin et al. 2019;Liu et al. 2019;Lan et al. 2020; inter alia) are typically trained via masked language modeling (MLM). \n\nPLMs pre-trained using MLM are capable of returning distributions over their vocabulary that peak at plausible substitutes of masked (sub)tokens given some sequence of input text. The individual updates performed during MLM pre-training, however, are not aligned to what we expect from the PLMs in the long run, i.e., that they output distributions of plausible substitutes for masked (sub)tokens. \n\nAs a motivating example, consider the sentence \"Alice is eating a cake.\", and suppose that we randomly select the token cake to be masked. For such a training example, we would obtain the masked input sentence \"Alice is eating a [MASK].\". \n\nThe smallest possible training loss would incur for this particular example if our model allocated all its output probability mass to the word \"cake\" as being the only possible replacement of the [MASK] token, while assigning precisely zero probability to other alternatives, that are otherwise totally viable from a human cognitive perspective, including words such as pear, croissant, soup, etc. \n\nWhat eventually provides PLMs trained with the MLM objective the ability to output token distributions that are plausible from a human perspective, is that they are trained over massive amounts of diverse batches, and the different possible substitutes even out in expectation. The hypothesis that we investigate in this paper is that we can train PLMs more efficiently if -instead of relying on the exact identity of the masked tokens during pre-trainingwe required our model to output such distributions that are not peaked at a single symbol (corresponding to the identity of the masked token).",
            "score": 0.35531237985049957,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 366
                },
                {
                    "start": 369,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 766
                },
                {
                    "start": 769,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 1007
                },
                {
                    "start": 1010,
                    "end": 1407
                },
                {
                    "start": 1410,
                    "end": 1687
                },
                {
                    "start": 1688,
                    "end": 2007
                }
            ],
            "ref_mentions": [
                {
                    "start": 244,
                    "end": 264,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 280,
                    "end": 295,
                    "matchedPaperCorpusId": "202888986"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0709228515625
        },
        {
            "corpus_id": "218470235",
            "title": "Identifying Necessary Elements for BERT's Multilinguality",
            "text": "Languages. Wang et al. (2019) propose to consider English and Fake-English, a language that is created by shifting unicode data points by a large constant. Fake-English in their case has the exact same linguistic properties as English, but is represented by different unicode data points. \n\nWe follow a similar approach, but instead of shifting unicode datapoints we simply shift token indices after tokenization by a constant; shifted tokens are prefixed by \"::\" and added to the vocabulary. Shifting token indices by a large constant doubles the size of the vocabulary. See Figure 2 for an example of how to create a Fake-English sentence. \n\nWe prefer shifting token indices rather than unicode code points and argue that this is a cleaner setup. For example, the BERT tokenizer treats some punctuation as special symbols (e.g., \"drycleaning\" is tokenized as [\"dry\", \"-\", \"##cleaning\"], not as [\"dry\", \"##-\", \"##cleaning\"] ). Thus, with a unicode shift, tokenizations of English and Fake-English might differ. \n\nTraining Data. For our setup, aimed at supporting fast experimentation, a small corpus with limited vocabulary and limited character range is desirable. We work on the English language and use the English Easy-to-Read version of the Parallel Bible Corpus (Mayer and Cysouw, 2014). The corpus is structured into verses and is word-tokenized. As each verse can contain multiple sentences we perform sentence splitting using NLTK (Loper and Bird, 2002). The final corpus has 17178 sentences, 228K words, a vocabulary size of 4449 and 71 distinct characters. The median sentence length is 12 words. \n\nBy creating a Fake-English version of this corpus we get a shifted replica and thus a sentence-parallel corpus. \n\nVocabulary. We create a vocabulary of size 2048 from the English corpus with the wordpiece tokenizer. 3 We use the same vocabulary for English and Fake-English. Thus, our final vocabulary size is 4096. \n\nModel.",
            "score": 0.3552382118243463,
            "section_title": "BERT Model",
            "char_start_offset": 4619,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 10
                },
                {
                    "start": 11,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 288
                },
                {
                    "start": 291,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 641
                },
                {
                    "start": 644,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 1011
                },
                {
                    "start": 1014,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1166
                },
                {
                    "start": 1167,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1568
                },
                {
                    "start": 1569,
                    "end": 1608
                },
                {
                    "start": 1611,
                    "end": 1722
                },
                {
                    "start": 1725,
                    "end": 1736
                },
                {
                    "start": 1737,
                    "end": 1828
                },
                {
                    "start": 1829,
                    "end": 1885
                },
                {
                    "start": 1886,
                    "end": 1926
                },
                {
                    "start": 1929,
                    "end": 1935
                }
            ],
            "ref_mentions": [
                {
                    "start": 1269,
                    "end": 1293,
                    "matchedPaperCorpusId": "38509851"
                },
                {
                    "start": 1441,
                    "end": 1463,
                    "matchedPaperCorpusId": "1438450"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0345458984375
        },
        {
            "corpus_id": "276161664",
            "title": "Hierarchical Contextual Manifold Alignment for Structuring Latent Representations in Large Language Models",
            "text": "The introduction of Hierarchical Contextual Manifold Alignment (HCMA) demonstrated a measurable impact on the structural organization of token embeddings, leading to improvements in contextual coherence, semantic alignment, and overall model stability across diverse linguistic tasks. The refinement of token representations through hierarchical realignment contributed to a more interpretable and functionally consistent latent space, reinforcing the hypothesis that nonparametric adjustments to token distributions can enhance generalization without requiring direct parameter modifications. Observations across multiple evaluation criteria revealed that HCMA effectively improved representation stability for both frequent and infrequent tokens while preserving the integrity of pre-learned semantic relationships. The observed gains in rare token retrieval accuracy and long-range contextual consistency suggested that hierarchical organization played a crucial role in mitigating representational fragmentation, a challenge that frequently arises in models trained on heterogeneous data distributions. The ability of HCMA to enhance robustness to adversarial perturbations further supported the argument that latent space structuring contributed to improved linguistic resilience, reducing the likelihood of unintended interpretational shifts caused through syntactic modifications or lexical perturbations. \n\nThe computational requirements associated with embedding realignment remained within practical bounds, ensuring that the proposed methodology maintained efficiency despite the introduction of additional processing steps. The increase in embedding preprocessing time was offset through optimizations in cluster formation and parallelized realignment procedures, mitigating concerns regarding scalability when extending HCMA to larger model architectures. The hierarchical nature of the alignment process enabled efficient segmentation of token embeddings, ensuring that realignment operations were conducted within locally constrained subregions rather than across the entire representation space. This localized approach minimized computational overhead while preserving the global structure of token embeddings, allowing for controlled refinements that maintained alignment with prelearned linguistic priors. The memory footprint associated with HCMA remained within acceptable limits, suggesting that hierarchical realignment strategies could be applied across larger-scale architectures without prohibitive increases in resource consumption. The trade-off between structural improvements and computational efficiency highlighted the viability of non-parametric embedding adjustments as a means of enhancing representational consistency without introducing the challenges associated with full-scale retraining.",
            "score": 0.35496973611042026,
            "section_title": "VI. DISCUSSIONS",
            "char_start_offset": 30420,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1412
                },
                {
                    "start": 1415,
                    "end": 1635
                },
                {
                    "start": 1636,
                    "end": 1868
                },
                {
                    "start": 1869,
                    "end": 2111
                },
                {
                    "start": 2112,
                    "end": 2324
                },
                {
                    "start": 2325,
                    "end": 2559
                },
                {
                    "start": 2560,
                    "end": 2827
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67041015625
        },
        {
            "corpus_id": "17153290",
            "title": "A Discriminative Model for Semantics-to-String Translation",
            "text": "We have experimented with a number of techniques for aligning source-side semantic graph nodes to target-side surface words. \n\nGibbs sampling. We can attempt to directly align the target language words to the source language nodes using a generative HMM-style model. Unlike the HMM word alignment model (Vogel et al., 1996), the likelihood of jumping between nodes is based on the graph path between those nodes, rather than the linear distance. \n\nStarting from the generative story of Equation 1, we make several simplifying assumptions. First we assume that the alignment distribution P (a i | \u2022 \u2022 \u2022 ) is modeled as a categorical distribution: \n\nThe function LABEL(u, v) produces a string describing the labels along the shortest (undirected) path between the two nodes. \n\nNext, we assume that the translation distribution is modeled as a set of categorical distributions, one for each source semantic node: \n\nThis model is sensitive to the order in which source language information is presented in the target language. \n\nThe alignment variables a i are not observed. We use Gibbs sampling rather than EM so that we can incorporate a sparse prior when estimating the parameters of the model and the assignments to these latent alignment variables. At each iteration, we shuffle the sentences in our training data. Then for each sentence, we visit all its tokens in a random order and re-align them. We sample the new alignment according to the Markov blanket, which has the following probability distribution: \n\nL, P stand for the number of lemma/path types, respectively. T is the total number of tokens in the corpus. Overall, the formula describes the probability of the edge coming into the node n i , the token emission and finally the outgoing edge. We evaluate this probability for each node n i in the graph and re-align the token according to the random sample from this distribution. \n\n\u03b1 and \u03b2 are hyper-parameters specifying the concentration parameters of symmetric Dirichlet priors over the transition and emission distributions. Specifying values less than 1 for these hyper-parameters pushes the model toward sparse solutions. They are tuned by a grid search which evaluates model perplexity on a held-out set. \n\nDirect GIZA++. GIZA++ (Och and Ney, 2000) is a commonly used toolkit for word alignment which implements the IBM models.",
            "score": 0.3548034463673331,
            "section_title": "Alignment of Semantic Graph Nodes",
            "char_start_offset": 8157,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 127,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 445
                },
                {
                    "start": 448,
                    "end": 538
                },
                {
                    "start": 539,
                    "end": 645
                },
                {
                    "start": 648,
                    "end": 772
                },
                {
                    "start": 775,
                    "end": 909
                },
                {
                    "start": 912,
                    "end": 1022
                },
                {
                    "start": 1025,
                    "end": 1070
                },
                {
                    "start": 1071,
                    "end": 1250
                },
                {
                    "start": 1251,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1512
                },
                {
                    "start": 1515,
                    "end": 1575
                },
                {
                    "start": 1576,
                    "end": 1622
                },
                {
                    "start": 1623,
                    "end": 1758
                },
                {
                    "start": 1759,
                    "end": 1896
                },
                {
                    "start": 1899,
                    "end": 2045
                },
                {
                    "start": 2046,
                    "end": 2144
                },
                {
                    "start": 2145,
                    "end": 2228
                },
                {
                    "start": 2231,
                    "end": 2245
                },
                {
                    "start": 2246,
                    "end": 2351
                }
            ],
            "ref_mentions": [
                {
                    "start": 303,
                    "end": 323,
                    "matchedPaperCorpusId": "11644259"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07501220703125
        },
        {
            "corpus_id": "268230397",
            "title": "Greed is All You Need: An Evaluation of Tokenizer Inference Methods",
            "text": "We find that the least tokens strategy fares well not only on the token count metric, which is mostly by-design, but also on cognitive measures, suggesting an effect of human preference to minimal word segmentation. Finally, we observe that likelihood-based inference performs poorly in terms of R\u00e9nyi efficieny, contrary to its stated purpose. Dropout, on the other hand, performs well on this measure, in line with its goal. longest suffix performs poorly across the board, possibly due to the suffixing nature of the English language, which has complementarily been shown to affect character-level sequential modeling (Pinter et al., 2019). Notably, all our key observations are consistent across vocabulary sizes, as shown in Appendix A. \n\nInter-tokenizer results Our results align with Bostrom and Durrett (2020)'s finding that BPE is inferior to UnigramLM on morphology alignment. However, we show that some of this gap can be attributed not to the vocabulary but to the inference method. In addition, we find that SaGe is most aligned to morphology by a substantial margin, indicating that its contextualized objective succeeds in retaining meaningful tokens in the vocabulary during ablation. It is important to note that our evaluation is limited to English, a language with relatively low morphological complexity. Previous studies have identified significant tokenization challenges in non-English languages (Mager et al., 2022). Therefore, any definitive conclusions regarding the effectiveness of tokenization methods should ideally encompass a diverse array of languages. BPE and WordPiece, optimized for compression, unsurprisingly perform well above the likelihood-based vocabularies on the information measures. However, we note that this carries over to the cognitive benchmark as well, supporting Beinborn and Pinter (2023)'s findings. \n\nFinally, we note that the two likelihood-based vocabularies follow the exact same within-vocab trends, and those for the two information-based vocabularies are also very close. This highlights the consistency and robustness of our benchmark, although some results are relatively close to each other, which can be expected considering that some inference methods do not change much of the token sequences (see rightmost column of Table 3).",
            "score": 0.3544861230691553,
            "section_title": "Experiments",
            "char_start_offset": 13055,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 741
                },
                {
                    "start": 744,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1200
                },
                {
                    "start": 1201,
                    "end": 1324
                },
                {
                    "start": 1325,
                    "end": 1440
                },
                {
                    "start": 1441,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1728
                },
                {
                    "start": 1729,
                    "end": 1854
                },
                {
                    "start": 1857,
                    "end": 2033
                },
                {
                    "start": 2034,
                    "end": 2295
                }
            ],
            "ref_mentions": [
                {
                    "start": 621,
                    "end": 642,
                    "matchedPaperCorpusId": "75134948"
                },
                {
                    "start": 1419,
                    "end": 1439,
                    "matchedPaperCorpusId": "247518649"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.017181396484375
        },
        {
            "corpus_id": "273532094",
            "title": "Understanding Layer Significance in LLM Alignment",
            "text": "Aligning large language models (LLMs) with specific requirements is essential for enhancing their utility across diverse applications (Luo et al., 2023a;Yu et al., 2023;Luo et al., 2023b;Li et al., 2023;Liu et al., 2024a;2022;Feng et al., 2023). Fine-tuning LLMs during the alignment process can significantly improve the models' capabilities to meet targeted needs (Bubeck et al., 2023). Typically, alignment involves fine-tuning the model on diverse datasets, which may include both human-curated (Rajani et al., 2023) and LLM-generated (Taori et al., 2023) data, using approaches like instruction tuning (Wei et al., 2021) and preference learning (Bai et al., 2022;Rafailov et al., 2024). Given the significant cost associated with full parameter fine-tuning, parameter-efficient fine-tuning (PEFT) (Hu et al., 2021;Chen et al., 2022;Pan et al., 2024) methods have emerged as a popular alternative, offering a balance between performance and resource efficiency. \n\nUnderstanding what LLMs actually learn during the alignment process is crucial. Zhou et al. (2023) posits that the majority of knowledge and capabilities are developed during the pre-training phase, with alignment primarily serving to refine the model's conversational style and formatting. Using a well-selected set of 1,000 training examples for supervised finetuning (SFT), they successfully produced a high-quality aligned model. Similarly, Lin et al. (2023) investigated the token distribution of LLMs before and after alignment and found that most changes were related to \"stylistic tokens\", such as discourse markers and transition words, while the knowledge-intensive content largely remained untouched, coming from Table 1: Impact of fine-tuning different components of LLAMA 2-7B on alignment performance using the LIMA dataset. Evaluated on MMLU (5-shot) and GPT-4o scores for Vicuna and MT-Bench prompts.",
            "score": 0.35437447979654035,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 965
                },
                {
                    "start": 968,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1806
                },
                {
                    "start": 1807,
                    "end": 1884
                }
            ],
            "ref_mentions": [
                {
                    "start": 203,
                    "end": 221,
                    "matchedPaperCorpusId": "268819294"
                },
                {
                    "start": 668,
                    "end": 690,
                    "matchedPaperCorpusId": "258959321"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1617431640625
        },
        {
            "corpus_id": "270562119",
            "title": "Not Everything is All You Need: Toward Low-Redundant Optimization for Large Language Model Alignment",
            "text": "Large language models (LLMs) are still struggling in aligning with human preference in complex tasks and scenarios. They are prone to overfit into the unexpected patterns or superficial styles in the training data. We conduct an empirical study that only selects the top-10% most updated parameters in LLMs for alignment training, and see improvements in the convergence process and final performance. It indicates the existence of redundant neurons in LLMs for alignment training. To reduce its influence, we propose a low-redundant alignment method named **ALLO**, focusing on optimizing the most related neurons with the most useful supervised signals. Concretely, we first identify the neurons that are related to the human preference data by a gradient-based strategy, then identify the alignment-related key tokens by reward models for computing loss. Besides, we also decompose the alignment process into the forgetting and learning stages, where we first forget the tokens with unaligned knowledge and then learn aligned knowledge, by updating different ratios of neurons, respectively. Experimental results on 10 datasets have shown the effectiveness of ALLO. Our code and data will be publicly released.",
            "score": 0.3542416460563105,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10968017578125
        },
        {
            "corpus_id": "273901687",
            "title": "Take Off the Training Wheels! Progressive In-Context Learning for Effective Alignment",
            "text": "In this section, we aim to shed light on the working mechanisms of in-context learning by investigating the following question: What is the impact of demonstration on token representation in incontext alignment? To explore this, we design a comparative experiment to highlight how token representations differ between zero-shot and few-shot settings. We use token probability distributions as a proxy for token representations and utilize KL-divergence to measure the shifts in these distributions. By visualizing and quantifying the shifts in token probability distributions caused by demonstrations, we can understand the role of demonstrations in aligning the model and provide further optimization for in-context alignment. \n\nRegarding the experimental setup, we randomly selected 100 data instances of similar length from Ultra-chat (Ding et al., 2023), a commonly used dataset for alignment tuning, as our experimental dataset. For the input prompt, we use a straightforward design by adding several tokens at the end of the query to serve as separator tokens, explicitly distinguishing between the query and the response. We present the visualization results based on the Llama2-7b model in the Figure 1, while the results for other models are provided in Appendix C. We break the token distribution of the whole instance into the input and output parts. A straightforward reason is that the input token distribution shift represents differences in understanding the instruction, while the output token distribution shift represents the ability to respond. By observing and analyzing the visualization, we have two hypotheses: (1) the ICL alignment task function might be encoded into the separator token representation. (2) the quality of response is highly reliant on the quality of prior response tokens. \n\nInput Token Distribution. By comparing the input token probability distributions between zeroshot and few-shot settings, a significant shift is observed in both the prior tokens of the query and the separator tokens. The KL-divergence decreases as the number of query tokens increases. By comparing the experimental group and the control group, we find that the shift in the query distribution also occurs in the control group. However, this shift in the separator tokens is not consistent across different demonstration settings, suggesting distinct underlying causes for these shifts.",
            "score": 0.3541988317537238,
            "section_title": "Motivation",
            "char_start_offset": 7828,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 727
                },
                {
                    "start": 730,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 1128
                },
                {
                    "start": 1129,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1727
                },
                {
                    "start": 1728,
                    "end": 1814
                },
                {
                    "start": 1817,
                    "end": 1842
                },
                {
                    "start": 1843,
                    "end": 2033
                },
                {
                    "start": 2034,
                    "end": 2102
                },
                {
                    "start": 2103,
                    "end": 2244
                },
                {
                    "start": 2245,
                    "end": 2403
                }
            ],
            "ref_mentions": [
                {
                    "start": 838,
                    "end": 857,
                    "matchedPaperCorpusId": "258840897"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.677734375
        },
        {
            "corpus_id": "267523348",
            "title": "A Roadmap to Pluralistic Alignment",
            "text": "Hypothesis: Current LLM alignment techniques can reduce distributional pluralism w.r.t. the population of internet users. \n\nTheoretical aspect: The language modeling cross entropy objective may help models learn distributional pluralism. If query x with response y appears many times in the training data written by a random internet users, cross entropy encourages the model to output y in proportion to the popu-  1. Jensen-Shannon distance (similarity) between human and model distributions on GlobalQA (target human distributions of Japan and US) and MPI. Note that we compare two \"post\" RLHF models for LLaMA (Alpaca and Tulu). Smaller (more similar) value in bold. \n\nlation (Ji et al., 2021) 2 . Moreover, we postulate that current alignment techniques can reduce distributional pluralism, as the alignment procedure does not have this property. \n\nEmpirical aspect: We rely on three empirical findings that provide an initial indication of support for our hypothesis. Firstly, in work by Santurkar et al. (2023), questions from Pew Research's American Trends Panels survey data (Opin-ionQA) were utilized to compare the distribution of LLM responses to those of US citizens. Two different model classes (Jurassic/GPT-3) with both pre-and post-aligned models were compared. The results revealed that post-aligned models exhibited less similarity to human populations compared to pre-aligned models. Expanding beyond the U.S., Durmus et al. (2023) introduced GlobalOpinionQA, an aggregation of multinational World Values similar to OpinionQA. Although their focus was solely on post-aligned models, they observed that these models tended to concentrate the probability mass on a few answer choices, in contrast to the dispersed answers seen in their human distributions. \n\nIn an effort to expand on these works, we further tested3 a suite of vanilla pretrained LLMs in comparison to their corresponding \"aligned\" counterparts (RLHFed, finetuned LLMs) from two model classes, LLaMA(2) and GPT-3.",
            "score": 0.35397578856048445,
            "section_title": "Current Approaches and Pluralism",
            "char_start_offset": 30975,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 124,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 670
                },
                {
                    "start": 673,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 851
                },
                {
                    "start": 854,
                    "end": 973
                },
                {
                    "start": 974,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1278
                },
                {
                    "start": 1279,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1546
                },
                {
                    "start": 1547,
                    "end": 1774
                },
                {
                    "start": 1777,
                    "end": 1998
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03265380859375
        },
        {
            "corpus_id": "267750465",
            "title": "Emulated Disalignment: Safety Alignment for Large Language Models May Backfire!",
            "text": "Large language models (LLMs) undergo safety alignment to ensure safe conversations with humans. However, this paper introduces a training-free attack method capable of reversing safety alignment, converting the outcomes of stronger alignment into greater potential for harm by accessing only LLM output token distributions. Specifically, our method achieves this reversal by contrasting the output token distribution of a safety-aligned language model (e.g., Llama-2-chat) against its pre-trained version (e.g., Llama-2), so that the token predictions are shifted towards the opposite direction of safety alignment. We name this method emulated disalignment (ED) because sampling from this contrastive distribution provably emulates the result of fine-tuning to minimize a safety reward. Our experiments with ED across three evaluation datasets and four model families (Llama-1, Llama-2, Mistral, and Alpaca) show that ED doubles the harmfulness of pre-trained models and outperforms strong baselines, achieving the highest harmful rates in 43 out of 48 evaluation subsets by a large margin. Eventually, given ED's reliance on language model output token distributions, which particularly compromises open-source models, our findings highlight the need to reassess the open accessibility of language models, even if they have been safety-aligned. Code is available at https://github.com/ZHZisZZ/emulated-disalignment.",
            "score": 0.35342087858560706,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10125732421875
        },
        {
            "corpus_id": "273503719",
            "title": "Distributional hypothesis as isomorphism between word-word co-occurrence and analogical parallelograms",
            "text": "Next, to answer Question (2), we take a constructive approach in which a small artificial corpus is built and closely analyzed in Section 5. Although word-to-word direct co-occurrence can be manipulated easily in a natural corpus, it is not straightforward to manipulate sentenceby-sentence statistics, as these may reflect underlying natural contexts. With our constructive approach, we can systematically manipulate sentence level statistics and analyze their effect on the formation of parallelograms in word vector space. \n\nNote that our goal of this corpus analysis in Section 3 is not to propose an algorithm to achieve better analogical performance, but to provide support/evidence for the distributional hypothesis directly; i.e., word co-occurrence information itself correlates with semantics via word analogy. Besides, Section 4 provides further evidence that the negative sampling algorithm is sufficient but not necessary for word analogy. These provide a justification for the theoretical analysis of raw co-occurrence structure in Section 5. \n\nConnecting the co-occurrence matrix to analogical parallelograms directly naturally leads to a constructive approach: simulation to test which type of co-occurrence may embed a parallelogram in the word vector space. Thus, we take the two types of complementary approaches, namely data-driven analysis of the co-occurrence matrix, and constructive simulation by creating and manipulating a small corpus. In this theoretical analysis, we provide a necessary and sufficient condition to form analogical parallelopipeds in such a small corpus. While most research provides sufficient algorithms for linguistic tasks, little research provides necessary conditions for even small tasks. \n\nIn the following, we briefly introduce the word2vec model in Section 2, followed by an analysis of a co-occurrence matrix in Section 3, word2vec models without negative sampling in Section 4, and the constructive approach in Section 5. Lastly, we discuss future directions toward an understanding of the semantic nature of the underlying word co-occurrence.",
            "score": 0.3532275530820449,
            "section_title": "What structure in the co-occurrence matrix enables analogical reasoning?",
            "char_start_offset": 9494,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 525
                },
                {
                    "start": 528,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1056
                },
                {
                    "start": 1059,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1599
                },
                {
                    "start": 1600,
                    "end": 1740
                },
                {
                    "start": 1743,
                    "end": 2100
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.078369140625
        },
        {
            "corpus_id": "273812321",
            "title": "Adaptive Segment-level Reward: Bridging the Gap Between Action and Reward Space in Alignment",
            "text": "Some recent studies have suggested that step-wise rewards yield better results than sequence-wise on mathematical problems (Lightman et al., 2023a;Uesato et al., 2022;Lai et al., 2024;Wang et al., 2024). Other research has shown that token-wise reward signals are more effective than sequence-level supervision signals in specific tasks like summarization (Zhong et al., 2024;Feng et al., 2024;Zeng et al., 2024a).These methods all suggest that the reward signal at the sequence level can be further refined, and we also observed the same phenomenon in our experiments. \n\nSeveral studies have explored selective token methods to improve efficiency and performance in language model training and optimization. Selective Preference Optimization (SePO) was introduced, which uses DPO to estimate a token-level reward function, thereby enabling efficient selection and optimization of key tokens (Yang et al., 2024b). Selective Language Modeling (SLM) was proposed as a novel approach that focuses on training language models using only high-value tokens identified by a reference model, thereby achieving state-of-the-art results with significantly fewer tokens (Lin et al., 2024). Token-level Direct Preference Optimization (TDPO) was developed to optimize policy at the token level for better alignment with human preferences, incorporating forward KL divergence constraints for each token and utilizing the Bradley-Terry model for token-based rewards (Zeng et al., 2024b). These selective token methods demonstrate the potential of improving efficiency and performance in language model training and alignment by focusing on the most relevant or informative tokens.",
            "score": 0.3526048115476258,
            "section_title": "Related Works",
            "char_start_offset": 19640,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 569
                },
                {
                    "start": 572,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1472
                },
                {
                    "start": 1473,
                    "end": 1665
                }
            ],
            "ref_mentions": [
                {
                    "start": 184,
                    "end": 202,
                    "matchedPaperCorpusId": "266209760"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.128662109375
        },
        {
            "corpus_id": "274776320",
            "title": "Tokens, the oft-overlooked appetizer: Large language models, the distributional hypothesis, and meaning",
            "text": "Tokenization is a necessary component within the current architecture of many language models, including the transformer-based large language models (LLMs) of Generative AI, yet its impact on the model's cognition is often overlooked. We argue that LLMs demonstrate that the Distributional Hypothesis (DH) is sufficient for reasonably human-like language performance, and that the emergence of human-meaningful linguistic units among tokens and current structural constraints motivate changes to existing, linguistically-agnostic tokenization techniques, particularly with respect to their roles as (1) semantic primitives and as (2) vehicles for conveying salient distributional patterns from human language to the model. We explore tokenizations from a BPE tokenizer; extant model vocabularies obtained from Hugging Face and tiktoken; and the information in exemplar token vectors as they move through the layers of a RoBERTa (large) model. Besides creating sub-optimal semantic building blocks and obscuring the model's access to the necessary distributional patterns, we describe how tokens and pretraining can act as a backdoor for bias and other unwanted content, which current alignment practices may not remediate. Additionally, we relay evidence that the tokenization algorithm's objective function impacts the LLM's cognition, despite being arguably meaningfully insulated from the main system intelligence. [First uploaded to arXiv in December, 2024.]",
            "score": 0.3523830179575179,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1688232421875
        },
        {
            "corpus_id": "231805942",
            "title": "A New Proposal for Phoneme Acquisition: Computing Speaker-Specific Distribution",
            "text": "For example, they compute the frequencies with which the sounds of a specific set of tokens are distributed (e.g., bimodally or unimodally) and infer phoneme categories from the number of modes of the observed distribution [10]. In a series of studies, Maye and colleagues [10,26,27] revealed that infants at 6-8 months of age who were exposed to a sequence of auditory tokens on an eight-step continuum with a bimodal frequency distribution could tell apart the end tokens of the continuum (e.g., Token 1 and 8 in Figure 1). The bimodal distribution had two peaks in the frequency distribution of the tokens (see Figure 1 dashed line, the second and the seventh token are the most frequent sounds among the eight tokens). That is, the bimodal distribution led the young learners to extract two different categories from the continuum. However, infants who were exposed to the same tokens on the same continuum, but with a unimodal distribution (see Figure 1, solid line) failed to show such discrimination to the same test tokens, suggesting that they grouped all the tokens into the same category after the exposure. Support for the distributional hypothesis comes from studies from other domains showing that infants are sensitive to statistics in phonetic discrimination [10], in speech segmentation [28], as well as in word orders in sentences [29]. line) failed to show such discrimination to the same test tokens, suggesting that they grouped all the tokens into the same category after the exposure. Support for the distributional hypothesis comes from studies from other domains showing that infants are sensitive to statistics in phonetic discrimination [10], in speech segmentation [28], as well as in word orders in sentences [29]. However, as noted earlier, these studies do not provide an account for infants' strategies to make use of frequency distributions that vary with the speaker, thus limiting the application of this model to learning situations with very little variation. However, this is not a likely situation that a language learner would encounter.",
            "score": 0.3523657079068593,
            "section_title": "Previous Hypotheses: Minimal Pair and Distributional-Based Learning",
            "char_start_offset": 10174,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1507
                },
                {
                    "start": 1508,
                    "end": 1743
                },
                {
                    "start": 1744,
                    "end": 1996
                },
                {
                    "start": 1997,
                    "end": 2077
                }
            ],
            "ref_mentions": [
                {
                    "start": 223,
                    "end": 227,
                    "matchedPaperCorpusId": "319422"
                },
                {
                    "start": 273,
                    "end": 277,
                    "matchedPaperCorpusId": "319422"
                },
                {
                    "start": 277,
                    "end": 280,
                    "matchedPaperCorpusId": "11716387"
                },
                {
                    "start": 280,
                    "end": 283,
                    "matchedPaperCorpusId": "145353782"
                },
                {
                    "start": 1275,
                    "end": 1279,
                    "matchedPaperCorpusId": "319422"
                },
                {
                    "start": 1304,
                    "end": 1308,
                    "matchedPaperCorpusId": "13321604"
                },
                {
                    "start": 1349,
                    "end": 1353,
                    "matchedPaperCorpusId": "7447597"
                },
                {
                    "start": 1664,
                    "end": 1668,
                    "matchedPaperCorpusId": "319422"
                },
                {
                    "start": 1693,
                    "end": 1697,
                    "matchedPaperCorpusId": "13321604"
                },
                {
                    "start": 1738,
                    "end": 1742,
                    "matchedPaperCorpusId": "7447597"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1776123046875
        },
        {
            "corpus_id": "274789093",
            "title": "Preference-Oriented Supervised Fine-Tuning: Favoring Target Model Over Aligned Large Language Models",
            "text": "Large language models(LLMs) such as ChatGPT (OpenAI et al. 2024) have exhibited successful and potent applications in comprehending human queries and delivering plausible responses. This ability has proven to be crucial in realworld applications, e.g. AI assistants and recommendation systems. To equip LLMs with this ability, the alignment methods are usually applied to pre-trained language models. Alignment enables pre-trained models to comprehend the context and generate responses suitable to human interactions. Typical alignment methods can be broadly categorized into two types: Supervised Fine-Tuning (SFT) and Preference Alignment (PA). Supervised fine-tuning (SFT) is an essential phase of alignment, wherein the task is framed as causal language modeling performed on a pre-trained language model with instruction-response data D = {\u27e8x, y\u27e9}. Generally, it leverages the cross-entropy objective function in optimization, equipping the pre-trained language model with the ability to follow instructions and generate coherent sequences. Several studies (Schick and Sch\u00fctze 2021;Houlsby et al. 2019;Ivison et al. 2023) are dedicated to exploring SFT training strategies to enhance the alignment of LLMs. However, due to the intrinsic traits of modeling, the optimization process heavily depends on the availability of high-quality \u27e8x, y\u27e9 data, which hinders its performance. Traditionally, the prevalent large-scale SFT datasets in earlier research, such as Alpaca (Taori et al. 2023) and ShareGPT (shareAI 2023), were mainly developed via AI distillation or human-and-AI interaction. Assuring the quality of these datasets can be challenging, as the filtration and curation processes demand significant human resources and efforts. \n\nInstead of solely aligning the instruction and responses, preference alignment (PA), such as InstructGPT (Ouyang et al. 2022) and Direct Preference Optimization (DPO) (Rafailov et al. 2023), optimizes the LLMs based on chosenrejected data \u27e8x, y + , y \u2212 \u27e9. These PA methods provide exceptional benefits in model alignment, enabling LLMs to align more accurately with AI/human preferences.",
            "score": 0.3519310420973093,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 854
                },
                {
                    "start": 855,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1383
                },
                {
                    "start": 1384,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1741
                },
                {
                    "start": 1744,
                    "end": 1999
                },
                {
                    "start": 2000,
                    "end": 2131
                }
            ],
            "ref_mentions": [
                {
                    "start": 1063,
                    "end": 1088,
                    "matchedPaperCorpusId": "210838924"
                },
                {
                    "start": 1088,
                    "end": 1108,
                    "matchedPaperCorpusId": "59599816"
                },
                {
                    "start": 1108,
                    "end": 1126,
                    "matchedPaperCorpusId": "254877064"
                },
                {
                    "start": 1849,
                    "end": 1869,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1083984375
        },
        {
            "corpus_id": "268201584",
            "title": "Rethinking Tokenization: Crafting Better Tokenizers for Large Language Models",
            "text": "In the field of engineering, particularly due to the rapid development of NLP and language models, there has been a growing interest in research on tokenizers. This article argue that tokenizers, more than mere technical tools, should emulate and learn from human language processing methods. This argument is based on two main reasons: \n\n\u2022 Language as a cognitively direct product: While it may be argued that the invention of the aeroplane relied on aerodynamics rather than emulating the way birds fly and suggesting that research on language models and tokenizers need not emulate human cognition, there is a fundamental difference. The aeroplane is designed to fly, not to mimic birds. However, tokenizers directly process content that originates from human cognition, such as written or spoken language. As evidence, BPE-family tokenizers, which are the current dominant tokenization algorithms, also demonstrate their superiority in terms of cognitive plausibility (Beinborn & Pinter, 2023). In the quest to optimize tokenizer design, therefore, one cannot overlook the mechanisms of human language processing. \n\n\u2022 Lack of a general theory: After the decline of linguistic-based approaches in the LLM era, the development of tokenizers has often been aimed directly at specific, engineering objectives -such as splitting infrequent words into subwords (e.g., Sennrich et al., 2016) and improving the performance of LMs. This shift has occurred without the foundation of a new, general theoretical framework to guide the shift. This gap has led to reliance on trial-and-error, making the development time-consuming and difficult to provide systematic guidance for subsequent research. In contrast, cognitive science, which has conducted extensive research on tokenization (e.g., Arnon & Priva, 2013;Goldwater et al., 2009;Perruchet & Vinter, 1998;J. Yang, Cai, et al., 2020), is well placed to provide the general theory(s) for the development of NLP tokenizers. \n\nThis article will present the \"Principle of Least Effort,\" a general theory from cognitive science that can be applied to tokenizers.",
            "score": 0.35098030405121805,
            "section_title": "Optimizing Future Tokenizers",
            "char_start_offset": 10854,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 336
                },
                {
                    "start": 339,
                    "end": 636
                },
                {
                    "start": 637,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 998
                },
                {
                    "start": 999,
                    "end": 1117
                },
                {
                    "start": 1120,
                    "end": 1426
                },
                {
                    "start": 1427,
                    "end": 1533
                },
                {
                    "start": 1534,
                    "end": 1690
                },
                {
                    "start": 1691,
                    "end": 1968
                },
                {
                    "start": 1971,
                    "end": 2104
                }
            ],
            "ref_mentions": [
                {
                    "start": 972,
                    "end": 997,
                    "matchedPaperCorpusId": "264406163"
                },
                {
                    "start": 1366,
                    "end": 1388,
                    "matchedPaperCorpusId": "1114678"
                },
                {
                    "start": 1785,
                    "end": 1805,
                    "matchedPaperCorpusId": "8475535"
                },
                {
                    "start": 1805,
                    "end": 1828,
                    "matchedPaperCorpusId": "7923429"
                },
                {
                    "start": 1828,
                    "end": 1853,
                    "matchedPaperCorpusId": "17863361"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0709228515625
        },
        {
            "corpus_id": "76666453",
            "title": "Survey of Computational Approaches to Lexical Semantic Change",
            "text": "will focus on methods for evaluating lexical change as uncovered by the methods surveyed here. A reasonable assumption would be that such an evaluation regime will also be useful -at least in part -for evaluating concrete downstream applications.\n\nAt least in the context of this literature survey, we would like to step back and see computational linguistics style formal evaluation as part of a larger endeavor, as a central and necessary, but not sufficient, component of (linguistic) hypothesis testing. In particular, since the gold standard datasets which make up the backbone of our formal evaluation procedures are generally extremely expensive to create, there is an understandable tendency in our community to reuse existing gold standards as to the greatest possible extent, or even re-purpose datasets originally constructed with other aims in mind. 37 However, such reuse may be in conflict with some assumption crucial to the original purpose of the dataset, which in turn could influence the results of the evaluation.\n\nThere are two central (typically tacit) methodological assumptions -i.e. hypotheses -made in the work described in the previous sections, and especially in work on diachronic conceptual change detection and classification (Section 3):\n\n1. Applicability: the proposed method is suitable for uncovering diachronic conceptual change. 2. Representativity: the dataset on which the method is applied is suitable for uncovering diachronic conceptual change using this method.\n\nSince most current approaches are data-driven -i.e. the data are an integral component of the method -these two factors, while logically distinct, are heavily interdependent and almost impossible to keep apart in practice, and we will discuss them jointly here.\n\nWith a few notable exceptions, to which we will return below, there is also often a third tacit assumption:\n\n3. Falsifiability and control conditions: positive evidence is sufficient to show 1 and 2.\n\nAssumption 3 comes at least in part from the common practice of evaluating diachronic conceptual change using lists of attested such changes.\n\nWe will now take a closer look at these assumptions.",
            "score": 0.3509754580961078,
            "section_title": "Evaluation and Hypothesis Testing",
            "char_start_offset": 128864,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11029052734375
        },
        {
            "corpus_id": "1805910",
            "title": "Bootstrapping Syntax and Recursion using Alginment-Based Learning",
            "text": "The first phase of the algorithm is called alignment learning. It finds possible constituents by aligning all plain sentences from memory in pairs. Aligning uncovers parts of the sentences that are similar in both sentences and parts that are dissimilar. Finally, the dissimilar parts are stored as possible constituents of the same type. This is shown by grouping the parts and labelling them with a non-terminal. \n\nFinding constituents like this is based on Harris's notion of interchangeability. Harris (1951) states that two constituents of the same type can be replaced. The alignment learning algorithm tries to find parts of sentences that can be replaced, indicating that these parts might be constituents. \n\nWe have included a simple example taken from the ATIS corpus to give a visualisation of the algorithm in Table 1. It shows that that Show me is similar in both sentences and flights from Atlanta to Boston and the rates for flight 1943 are dissimilar. The dissimilar parts are then taken as possible constituents of the same type. In this example there are only two dissimilar parts, but if there were more dissimilar parts, they would also be grouped. However, a different nonterminal would be assigned to them (as can be seen in sentences 3 and 4 in Table 2). Note that if the algorithm tries to align two completely dissimilar sentences, no similar parts can be found at all. This means that no inner structure can be learned. The only constituents that can be learned are those on sentence level, since the entire sentences can be seen as dissimilar parts.",
            "score": 0.3509754580961078,
            "section_title": "Alignment Learning",
            "char_start_offset": 2042,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 62
                },
                {
                    "start": 63,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 414
                },
                {
                    "start": 417,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 714
                },
                {
                    "start": 717,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1394
                },
                {
                    "start": 1395,
                    "end": 1445
                },
                {
                    "start": 1446,
                    "end": 1576
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.043701171875
        },
        {
            "corpus_id": "908501",
            "title": "Finding Non-Arbitrary Form-Meaning Systematicity Using String-Metric Learning for Kernel Regression",
            "text": "Lexicon-Wide Systematicity Measuring Form, Meaning, and Systematicity. \n\nTo our knowledge, all previous lexicon-level analyses of phonosemantic systematicity have used variations of the method of Shillcock et al. (2001). \n\nThe inputs for this method are form-meaning tuples (y i , s i ) for each word i in the lexicon, where y i is the vector representation of the word in a distributional semantic model, and s i is the string representation of the word (phonological, phonemic, or orthographic). Semantic distances are measured as cosine distances between the vectors of each pair of words. Shillcock et al. (Shillcock et al., 2001) and Monaghan et al. (Monaghan et al., 2014b) measure form-distances in terms of edit distance between each pair of strings. In addition Monaghan et al. (2014b) and Tamariz (2006) study distance measures based on a selected set binary phonological features, with similar results. Phonosemantic systematicity is then measured as the correlation between all the pairwise semantic distances and all the pairwise string distances. \n\nHypothesis Testing. In this line line of work, statistical significance of the results is assessed using the Mantel test, a permutation test of the correlation between two sets of pairwise distances (Mantel, 1967). The test involves randomly shuffling the assignments of semantic vectors to wordstrings in the lexicon. We can think of each formmeaning shuffle as a member of the set of all possible lexicons. Next, the correlation between the semantic distances and the string distances is computed under each reassignment. An empirical pvalue for the true lexicon is then derived by performing many shufflings, and comparing the correlation coefficients measured under the shuffles to the correlation coefficient measured in the true lexicon. Under the null hypothesis that form-meaning assignments are arbitrary, the probability of observing a form-meaning correlation of at least the magnitude actually observed in the true lexicon is asymptotically equal to the proportion of reassignments that produce greater correlations than the true lexicon. \n\nPrevious Findings.",
            "score": 0.3509754580961078,
            "section_title": "Previous Approaches to Finding",
            "char_start_offset": 8519,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 70
                },
                {
                    "start": 73,
                    "end": 220
                },
                {
                    "start": 223,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1060
                },
                {
                    "start": 1063,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1806
                },
                {
                    "start": 1807,
                    "end": 2113
                },
                {
                    "start": 2116,
                    "end": 2134
                }
            ],
            "ref_mentions": [
                {
                    "start": 196,
                    "end": 219,
                    "matchedPaperCorpusId": "2644740"
                },
                {
                    "start": 610,
                    "end": 634,
                    "matchedPaperCorpusId": "2644740"
                },
                {
                    "start": 1262,
                    "end": 1276,
                    "matchedPaperCorpusId": "7187403"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05792236328125
        },
        {
            "corpus_id": "273185779",
            "title": "TIS-DPO: Token-level Importance Sampling for Direct Preference Optimization With Estimated Weights",
            "text": "The importance of Large Language Model (LLM) alignment (Ji et al., 2023) techniques has grown alongside the increasing capabilities of LLMs. These techniques aim to align LLMs with human values, ensuring the generation of helpful and harmless content (Bai et al., 2022). Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022) is a common alignment approach that trains a reward model on human-labeled preference data and optimizes the LLM using reinforcement learning methods like Proximal Policy Optimization (PPO) (Schulman et al., 2017) to maximize the generated reward under the reward model. However, RLHF is relatively complex due to the need for reinforcement learning techniques. \n\nTo simplify alignment process, Direct Preference Optimization (DPO) (Rafailov et al., 2024b) leverages the relationship between policy and reward functions to optimize both simultaneously without reinforcement learning. However, DPO is derived from a sequence-level Bradley-Terry model (Bradley & Terry, 1952), which only focuses on preference relationships between two sequences while ignoring the contribution of each token. However, as shown in Fig. 1, in real data, different tokens have different rewards. Even in winning responses, there are tokens with low rewards. Optimizing all Figure 1: In real data, different tokens have varying rewards, with low-reward tokens present even in winning responses. DPO treats all tokens equally, introducing noise and reducing optimization efficiency. Our TIS-DPO performs importance sampling on the optimal data distribution (where each token has equal reward) using actual data, introducing token weights to improve optimization efficiency. \n\ntokens uniformly reduces optimization efficiency. Although Rafailov et al. (2024a) (in section 4.2) demonstrate that DPO possesses a certain degree of token-level interpretability, this does not alleviate its lack of consideration for token importance. \n\nRecently, some studies have argued that different tokens in DPO should not be treated equally, but these studies often require changes to the data construction process to identify more critical tokens.",
            "score": 0.35062600390739806,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 613
                },
                {
                    "start": 614,
                    "end": 704
                },
                {
                    "start": 707,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1693
                },
                {
                    "start": 1696,
                    "end": 1745
                },
                {
                    "start": 1746,
                    "end": 1948
                },
                {
                    "start": 1951,
                    "end": 2152
                }
            ],
            "ref_mentions": [
                {
                    "start": 321,
                    "end": 342,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 775,
                    "end": 799,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 993,
                    "end": 1016,
                    "matchedPaperCorpusId": "121987403"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08648681640625
        },
        {
            "corpus_id": "276774243",
            "title": "AlignDistil: Token-Level Language Model Alignment as Adaptive Policy Distillation",
            "text": "In modern large language models (LLMs), LLM alignment is of crucial importance and is typically achieved through methods such as reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO). However, in most existing methods for LLM alignment, all tokens in the response are optimized using a sparse, response-level reward or preference annotation. The ignorance of token-level rewards may erroneously punish high-quality tokens or encourage low-quality tokens, resulting in suboptimal performance and slow convergence speed. To address this issue, we propose AlignDistil, an RLHF-equivalent distillation method for token-level reward optimization. Specifically, we introduce the reward learned by DPO into the RLHF objective and theoretically prove the equivalence between this objective and a token-level distillation process, where the teacher distribution linearly combines the logits from the DPO model and a reference model. On this basis, we further bridge the accuracy gap between the reward from the DPO model and the pure reward model, by building a contrastive DPO reward with a normal and a reverse DPO model. Moreover, to avoid under- and over-optimization on different tokens, we design a token adaptive logit extrapolation mechanism to construct an appropriate teacher distribution for each token. Experimental results demonstrate the superiority of our AlignDistil over existing methods and showcase fast convergence due to its token-level distributional reward optimization.",
            "score": 0.35021661783112595,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1971435546875
        },
        {
            "corpus_id": "274130626",
            "title": "Efficient Alignment of Large Language Models via Data Sampling",
            "text": "In this work, we present an analysis of the alignment performance of LLMs from the perspective of the dataset size. We present extensive experiments on different popular datasets with varying sizes and find consistent over-optimization as more data is used to align the model leading to minimal gain. We validate that data subsampling is feasible to reduce resources required for alignment. We propose a novel methodology for sampling a small diverse and high-quality dataset to perform alignment in a cost and resource effective manner. We show that our methodology outperforms other methods and achieves comparable performance while saving greater than 90% of the costs and resources. Our analysis of the over-optimization and sampling strategies for alignment is a first step and opens up new avenues of research for efficient alignment and characterizing the effect over larger model scales in the future which is critical for more ethical and safer models. \n\nA Appendix / supplemental material A.1 Alignment Preliminary \n\nLLM training generally takes place in pre-training, supervised finetuning, and alignment [18]. In general, pre-training involves learning the structure of the language including the grammar and punctuation by maximizing the log-likelihood (Equation 4) of the next token conditioned on the preceding text from a large corpus of data. To adapt a pre-trained model for instruction following tasks such as question answering, summarization, supervised fine-tuning is employed. Often, supervised fine-tuning is prohibitively expensive. Therefore, a parameter efficient approach such as LoRA is a prevalent SFT approach [13]. Finally, direct alignment algorithms (DAA) such as KTO, DPO are proven to be beneficial in enhancing a reward score defined as a function of the helpfulness and safety of a fine-tuned models. In fact, the approaches like KTO can even circumvent the need for SFT. Equation 6represents the canonical representation of alignment on a preference dataset: \n\nr * in Equation 6 denotes the \"true\" reward underlying the preferences. \n\nBecause the true reward measurement is intractable, a reward model r p hi is trained as a proxy by minimizing the negative log-likelihood of the human preference data, as shown in Equation 7 \n\nHowever, the indiscriminate maximization of the reward comes at the expense of the loss of desiderata such as generating grammatical text.",
            "score": 0.35013836789448183,
            "section_title": "Conclusion",
            "char_start_offset": 10895,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 961
                },
                {
                    "start": 964,
                    "end": 1024
                },
                {
                    "start": 1027,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1499
                },
                {
                    "start": 1500,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1646
                },
                {
                    "start": 1647,
                    "end": 1838
                },
                {
                    "start": 1839,
                    "end": 1909
                },
                {
                    "start": 1910,
                    "end": 1997
                },
                {
                    "start": 2000,
                    "end": 2071
                },
                {
                    "start": 2074,
                    "end": 2264
                },
                {
                    "start": 2267,
                    "end": 2405
                }
            ],
            "ref_mentions": [
                {
                    "start": 1116,
                    "end": 1120,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10430908203125
        },
        {
            "corpus_id": "276161664",
            "title": "Hierarchical Contextual Manifold Alignment for Structuring Latent Representations in Large Language Models",
            "text": "A structured and coherent latent space representation plays a fundamental role in determining the contextual consistency and generalization capacity of large language models. The introduction of Hierarchical Contextual Manifold Alignment (HCMA) provides a non-parametric framework for restructuring token embeddings through hierarchical organization, ensuring smoother representation transitions and improved token locality within the model's latent space. Unlike conventional approaches that rely on direct fine-tuning or reinforcementbased parameter updates, HCMA applies a structured transformation to the learned token embeddings without modifying the core model parameters, preserving prior knowledge while enhancing representational alignment. The methodology employs manifold learning techniques to infer token distribution topology, followed by an iterative realignment process that optimizes spatial coherence while maintaining semantic consistency across linguistic structures.",
            "score": 0.35010142346480366,
            "section_title": "III. HIERARCHICAL CONTEXTUAL MANIFOLD ALIGNMENT",
            "char_start_offset": 14496,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 987
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3798828125
        },
        {
            "corpus_id": "276813220",
            "title": "Adding Alignment Control to Language Models",
            "text": "One of the key successes of Large Language Models (LLMs) is their ability to align closely with human values and preferences (Ouyang et al., 2022). These models typically hinge on a series of critical training phases (OpenAI, 2024). First, they undergo pre-training on vast corpora to master the ability to predict the next token (Radford et al., 2019). Next, the pre-trained models are fine-tuned through supervised fine-tuning (SFT) to better adapt to specific domains (Wei et al., 2021;Yang et al., 2024b). Finally, preference-based optimization methods are employed, such as Reinforcement Learning from Human Feedback (RLHF) (Christiano et al., 2017) and Direct Preference Optimization (DPO) (Rafailov et al., 2024). These approaches ensure the model avoids engaging in factual inaccuracies, exhibiting biases, and displaying other undesirable behaviors (Bai et al., 2022). \n\nAcross these alignment approaches, the core objective re-1 Department of Computer Science, Shanghai Jiao Tong Univerisity, Shanghai, China 2 Shanghai Innovation Institute, Shanghai, China. Correspondence to: Wenhong Zhu <zwhong714@sjtu.edu.cn>. \n\nPreprint. mains consistent: to maximize the expected reward from an implicit or explicit reward function while incorporating the KL-divergence from the reference policy as a regularization term (Schulman et al., 2017;Gao et al., 2023;Rafailov et al., 2024). The strength of this regularization plays a pivotal role in determining the alignment outcome (Ziegler et al., 2019), as excessive regularization can overly constrain alignment, whereas insufficient regularization may lead to reward hacking (Pan et al., 2022). \n\nDetermining the optimal alignment strength often requires a process of trial and error (Meng et al., 2024). Moreover, the optimal alignment strength can vary based on cultural, ethical, and individual perspectives (Anwar et al., 2024).",
            "score": 0.34878762046263756,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 877
                },
                {
                    "start": 880,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1124
                },
                {
                    "start": 1127,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1645
                },
                {
                    "start": 1648,
                    "end": 1755
                },
                {
                    "start": 1756,
                    "end": 1883
                }
            ],
            "ref_mentions": [
                {
                    "start": 125,
                    "end": 146,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 330,
                    "end": 352,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 696,
                    "end": 719,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 1344,
                    "end": 1361,
                    "matchedPaperCorpusId": "252992904"
                },
                {
                    "start": 1361,
                    "end": 1383,
                    "matchedPaperCorpusId": "258959321"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09844970703125
        },
        {
            "corpus_id": "270688753",
            "title": "Brain-Like Language Processing via a Shallow Untrained Multihead Attention Network",
            "text": "Large Language Models (LLMs) have been shown to be effective models of the human language system, with some models predicting most explainable variance of brain activity in current datasets. Even in untrained models, the representations induced by architectural priors can exhibit reasonable alignment to brain data. In this work, we investigate the key architectural components driving the surprising alignment of untrained models. To estimate LLM-to-brain similarity, we first select language-selective units within an LLM, similar to how neuroscientists identify the language network in the human brain. We then benchmark the brain alignment of these LLM units across five different brain recording datasets. By isolating critical components of the Transformer architecture, we identify tokenization strategy and multihead attention as the two major components driving brain alignment. A simple form of recurrence further improves alignment. We further demonstrate this quantitative brain alignment of our model by reproducing landmark studies in the language neuroscience field, showing that localized model units -- just like language voxels measured empirically in the human brain -- discriminate more reliably between lexical than syntactic differences, and exhibit similar response profiles under the same experimental conditions. Finally, we demonstrate the utility of our model's representations for language modeling, achieving improved sample and parameter efficiency over comparable architectures. Our model's estimates of surprisal sets a new state-of-the-art in the behavioral alignment to human reading times. Taken together, we propose a highly brain- and behaviorally-aligned model that conceptualizes the human language system as an untrained shallow feature encoder, with structural priors, combined with a trained decoder to achieve efficient and performant language processing.",
            "score": 0.34840080723399697,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.224365234375
        },
        {
            "corpus_id": "276259470",
            "title": "Tokenization Standards for Linguistic Integrity: Turkish as a Benchmark",
            "text": "Tokenization is a fundamental preprocessing step in NLP, directly impacting large language models' (LLMs) ability to capture syntactic, morphosyntactic, and semantic structures. This paper introduces a novel framework for systematically evaluating tokenization strategies, addressing challenges in morphologically rich and low-resource languages. Using a Turkish dataset of 6,200 multiple-choice questions from the Massive Multitask Language Understanding (MMLU) benchmark, the framework assesses tokenizers across five key metrics: vocabulary size, token count, processing time, language-specific token percentages (\\%TR), and token purity. These metrics provide a structured approach to evaluating how well tokenizers preserve linguistic structures. While \\%TR measures the proportion of valid words in the target language, \\%Pure assesses the alignment of tokens with meaningful linguistic units, such as roots and valid morphemes, minimizing semantic fragmentation. The findings reveal that \\%TR, introduced as a critical metric, exhibits a stronger correlation with downstream performance (e.g., MMLU scores) than token purity, emphasizing its role in improving model accuracy. Additionally, larger model parameters do not necessarily yield better tokenization quality or enhanced results, highlighting the importance of tailored tokenization strategies that prioritize linguistic alignment. This framework sets a new standard for developing robust tokenization methods optimized for morphologically complex and low-resource languages. Future work will refine morphological analysis, explore domain-specific customizations, and conduct cross-linguistic evaluations to further enhance tokenization practices.",
            "score": 0.3482359214800999,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2178955078125
        },
        {
            "corpus_id": "269362100",
            "title": "Prefix Text as a Yarn: Eliciting Non-English Alignment in Foundation Language Model",
            "text": "where V is the vocabulary shared by two models, and L is the length of the dataset.\n\nWe compare the agreement of the token selection made by the models under the same prefix text X in two different experimental setups.The first setup uses the instruction text as the prefix, i.e., X = X ins. ; the second takes the first token decoded by the SFT model as a prior token, appending it to the original instruction prefix, i.e., X = X ins., y\n\nSFT .For the SFT model, the second setup is equivalent to continuing its own decoding behavior, whereas for the foundation model, it becomes decoding with the addition of a prior token.\n\nFigure 2 illustrates the agreement between the foundation model's predictions and those of the SFT model regarding the selection of the next token, given an identical text prefix.Across the entire translation data, it is observed that after incorporating merely one prior token, the foundation model exhibits a high degree of agreement with the SFT model in terms of token selection.This demonstrates that the alignment effect of SFT in crosslingual generation tasks is also somewhat superficial.Even in instances where the token with the highest probability differs between the two models, 90.8% of the tokens chosen by the SFT model are present within the \"silent majority\" in the decision space of the foundation model, specifically, among the top 20 most probable token choices.\n\nLens of Distribution Instead of focusing on the coverage of token selection outcomes, we also observe the decision dynamics and similarities from the perspective of the overall probability distribution, with the data settings consistent with the previous setup.First, as shown in Figure 3, after adding a prior token, the probability of the next tokens chosen by both models have closely aligned distributions.The reason that the foundation model exhibits a high probability given the instruction text as a prefix lies in a preference for choosing to continue the instruction text rather than completing the cross-linguistic semantic transformation.Additionally, we quantify the distribution disparities between the two models through the probability distribution of the vocabulary.The disparity metrics used include Kullback-Leibler (KL) divergence, Jensen-Shannon (JS) divergence, and cross-entropy (Kullback, 1997).",
            "score": 0.34810134021913364,
            "section_title": "+ Prior Token",
            "char_start_offset": 9268,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 83
                },
                {
                    "start": 85,
                    "end": 218
                },
                {
                    "start": 218,
                    "end": 435
                },
                {
                    "start": 435,
                    "end": 438
                },
                {
                    "start": 440,
                    "end": 445
                },
                {
                    "start": 445,
                    "end": 625
                },
                {
                    "start": 627,
                    "end": 806
                },
                {
                    "start": 806,
                    "end": 1010
                },
                {
                    "start": 1010,
                    "end": 1123
                },
                {
                    "start": 1123,
                    "end": 1409
                },
                {
                    "start": 1411,
                    "end": 1672
                },
                {
                    "start": 1672,
                    "end": 1821
                },
                {
                    "start": 1821,
                    "end": 2060
                },
                {
                    "start": 2060,
                    "end": 2193
                },
                {
                    "start": 2193,
                    "end": 2329
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.201171875
        },
        {
            "corpus_id": "237491731",
            "title": "Wine is Not v i n. - On the Compatibility of Tokenizations Across Languages",
            "text": "Another vein of research into model analysis has been the study of the effect of tokenization. Heinzerling and Strube (2018) compute BPE embeddings for 275 languages and perform a cross-lingual study on various tokenization methods. Further study into BPEs is conducted by Wei et al. (2021), where byte-level BPEs are compared against their character-level counterparts as it pertains to multilingual models while BPE vocabulary size is examined in the context of neural machine translation by Gowda and May (2020). Bostrom and Durrett (2020) show that byte-pair encoding is suboptimal when pretraining language models. In our work, we investigate another prevalent tokenization method, WordPiece, and show that vocabulary size plays an important role in achieving multilinguality in BERT.\n\nResearch into the selection of tokenization algorithms has produced alternative methods. For example, Kudo (2018) present a subword regularization method where a model is trained with multiple tokenizations in order to reinforce robustness in the model; and Aguilar et al. (2020) bridge the gap between subword and character-level models, proposing a module that learns to approximate subword embeddings given characters. Asgari et al. (2020);Provilkov et al. (2020) further research subword tokenization methods. With our work we introduce a measure that can help NLP researchers pick vocabulary sizes with a more robust procedure.",
            "score": 0.34808054103862085,
            "section_title": "Tokenization Research",
            "char_start_offset": 4966,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07781982421875
        },
        {
            "corpus_id": "254853878",
            "title": "Language model acceptability judgements are not always robust to context",
            "text": "Here, we provide more detail to support the experiment in \u00a75. Specifically, we present the exact numbers we use to compute the rank-order correlation coefficients, and describe the implications of this finding for future work.\n\nTo compute the rank-order correlation, we first obtain mean accuracies across the 20 BLiMP phenomena that respond most strongly to matched prefixing. We do this for each perturbation strategy, as well as the non-perturbed matched prefixes. We then take the mean across all prefixing lengths for OPT 6.7B (i.e., we convert each line in Figure 5 into a single number by taking the mean along the x-axis). This yields a metric that approximately captures how much of a priming effect a given prefixing strategy has for this model; we use this as our dependent variable.\n\nThe independent variable is the strength of the perturbation prefix. It is difficult to define how strong a given perturbation is, as there are different notions of linguistic similarity that can be contradictory; for instance, embedding a sentence c into a quote, as in \"Yesterday, Sarah said 'c\"', does not add many lexical items to the sentence, but it significantly modifies the syntactic structure of the sentence. In our case, we simply measure the token F 1 score between the original prefix sentence and a perturbed prefix sentence; this metric captures the token similarity between the original and perturbed sentences. Future work could consider more sophisticated similarity metrics, such as syntactic or semantic similarities.\n\nWe summarize these results in Figure 9. Note the highly monotonic relationship when using acceptable prefixes, and the similarly (but slightly less) monotonic relationship with unacceptable prefixes. This visually displays the strong correlations we found in \u00a75.\n\nWhy are language models being more primeable with longer contexts given more similar prefixes? Perhaps models can determine whether tokens are meaningfully similar between multiple sentences in the same context; this would be expected given the implications of the distributional hypothesis. Alternatively, the model could be effective at relating tokens that are similar in the pre-training corpus, as long as their positions are within some limited range of each other. Finally, perhaps the model is simply effective at ignoring (for example) adverbs and adjuncts that",
            "score": 0.34786783465795657,
            "section_title": "B Prefix Similarity Analysis",
            "char_start_offset": 34334,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1619873046875
        },
        {
            "corpus_id": "271956928",
            "title": "Selective Preference Optimization via Token-Level Reward Function Estimation",
            "text": "Recent advancements in large language model alignment leverage token-level supervisions to perform fine-grained preference optimization. However, existing token-level alignment methods either optimize on all available tokens, which can be noisy and inefficient, or perform selective training with complex and expensive key token selection strategies. In this work, we propose Selective Preference Optimization (SePO), a novel selective alignment strategy that centers on efficient key token selection. SePO proposes the first token selection method based on Direct Preference Optimization (DPO), which trains an oracle model to estimate a token-level reward function on the target data. This method applies to any existing alignment datasets with response-level annotations and enables cost-efficient token selection with small-scale oracle models and training data. The estimated reward function is then utilized to score all tokens within the target dataset, where only the key tokens are selected to supervise the target policy model with a reference model-free contrastive objective function. Extensive experiments on three public evaluation benchmarks show that SePO significantly outperforms competitive baseline methods by only optimizing 30% key tokens on the target dataset. SePO applications on weak-to-strong generalization show that weak oracle models effectively supervise strong policy models with up to 16.8x more parameters. SePO also effectively selects key tokens from out-of-distribution data to enhance strong policy models and alleviate the over-optimization problem.",
            "score": 0.3478449112842871,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1844482421875
        },
        {
            "corpus_id": "274281176",
            "title": "Text-to-SQL Calibration: No Need to Ask - Just Rescale Model Probabilities",
            "text": "We report the results across different methods and models in Table 2 and also show the reliability plots in Figure 1. Calibration metrics and reliability plots for experiments using other models and monotonic binning have been deferred to the Appendix E. Across these different studies we make two important conclusions: \n\nModel's own sequence probability (prod) performs better in smaller models and is comparable to self-check method in larger models Recent calibration studies (Tian et al., 2023) have found self-check methods to be better, and that could be because they deal with short answers. Tian et al. (2023) find calibration results differ when using language models fine-tuned with reinforcement learning from human feedback. We use Llama, 7B and 80B, which have undergone several rounds of alignment via supervised finetuning (SFT), rejection sampling (RS), and direct preference optimization (DPO) (AI ( 2024)). We observe that, even with Llama 70B model, the \"prod\" pooled token-level approach provides calibration as good as self-check bool. In Llama 8B model, \"prod\" method performs significantly better. \n\nFurther, recent research (West et al., 2024) highlights the gap between the generative and reasoning capabilities in large language models. From our experiments, we observe the gap becomes more pronounced as the model's parameter count decreases. The product of token probabilities, which is the likelihood of a sequence, serves as a measure of its generative capability, contrasting with self-check which is an assessment of the model's understanding. \n\nProd aggregation is the best pooled token-level method Stengel-Eskin and Van Durme (2023) investigate min and avg methods for calibration. We also consider prod aggregation, which represents a more natural choice as it denotes the probabil-  Table 3: The table presents evaluation metrics for the Token-level (prod) and Self-check Bool methods on the Spider and BIRD dataset, comparing the schema-level and schema-disjoint approaches for a threshold value of 0.9. The metrics include Precision (P), Recall (R) and F1-score (F1).",
            "score": 0.3476563353807522,
            "section_title": "Results",
            "char_start_offset": 6832,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 320
                },
                {
                    "start": 323,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 737
                },
                {
                    "start": 738,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 1057
                },
                {
                    "start": 1058,
                    "end": 1121
                },
                {
                    "start": 1124,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1370
                },
                {
                    "start": 1371,
                    "end": 1576
                },
                {
                    "start": 1579,
                    "end": 1717
                },
                {
                    "start": 1718,
                    "end": 2042
                },
                {
                    "start": 2043,
                    "end": 2107
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08709716796875
        },
        {
            "corpus_id": "265608902",
            "title": "The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning",
            "text": "We first substantiate the underlying hypothesis regarding the superficial nature of alignment tuning via detailed analysis on token distribution shifts between base and aligned LLMs. This analytical method allows us to clearly investigate which token positions are affected by alignment tuning, providing insights for developing more efficient alignment methods and further studying the science of LLMs. Inspired by these findings, we propose a strong baseline method for tuning-free alignment, URIAL, which aligns untuned base LLMs via in-context learning with a constant prompt. Experiments show that URIAL significantly reduce the gap between base LLMs and their aligned versions. Our contribution can be summarized as follows: \n\nAnalysis: To gain a deeper understanding of alignment tuning, we analyze the token distribution shift between base and aligned LLMs. We find that alignment predominantly affects a minimal portion of token selection, influencing primarily stylistic elements and safety disclaimers in just 5-8% of cases. On most of the token positions, aligned and base models concur on top-token choices. Also, we find that alignment tuning is much more critical for initial tokens than later tokens. \n\nMethods: We introduce a simple yet effective method for aligning base LLMs, URIAL. It utilizes only as few as three constant curated examples for ICL, yet it effectively aligns base LLMs and matches the performance of SFT+RLHF in some scenarios. We also discover that well-written, stylistic examples are more effective than semantically relevant ones that are dynamically retrieved. URIAL offers both efficiency and simplicity in aligning base LLMs without requiring fine-tuning. This method significantly conserves time and resources, which is especially beneficial when dealing with extremely large LMs or when base LLMs need frequent evaluation. Furthermore, it enables a deeper investigation into the knowledge and capabilities innate to these base LLMs, while fostering more precise and economical approaches to align them with their deficiencies. \n\nEvaluation: We develop a comprehensive and interpretable evaluation protocol, encompassing six aspects with verifiable judgments. We also release the annotations we gathered for community use in evaluation and training open-source LLM evaluators.",
            "score": 0.34756022295139444,
            "section_title": "CONCLUSION",
            "char_start_offset": 43325,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 730
                },
                {
                    "start": 733,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1120
                },
                {
                    "start": 1121,
                    "end": 1216
                },
                {
                    "start": 1219,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1602
                },
                {
                    "start": 1603,
                    "end": 1699
                },
                {
                    "start": 1700,
                    "end": 1868
                },
                {
                    "start": 1869,
                    "end": 2072
                },
                {
                    "start": 2075,
                    "end": 2204
                },
                {
                    "start": 2205,
                    "end": 2321
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.837890625
        },
        {
            "corpus_id": "232427871",
            "title": "Leveraging Neural Machine Translation for Word Alignment",
            "text": "Assuming that we have access to bi-directional word alignment (in the context of this paper to two MT systems of the opposite directions) we can compute both the alignment from source to target (X) and target to source (X ). Having access to both X and X makes it possible to create a new alignment Y with either higher precision through intersection or higher recall through union (Koehn, 2009). \n\nWe can make use of the fact that the models output soft alignment scores and create new alignment scores in the following way using a simple linear regression model. This allows us to fine-tune the relevance of each of the directions as well as their interaction. However, it does not have the same effects as the union or the intersection because it affects the soft alignment and not hard alignment in contrast to the previous case. \n\nMore complex symmetrization techniques have been proposed and implemented by Och and Ney (2000); Junczys-Dowmunt and Sza\u0142 (2011). Och and Ney (2003) introduce the word alignment task and systematically compare the IBM word alignment models. The work of Li et al. (2019) is closely related to this article as it examines the issue of word alignment from NMT and proposes two ways of extracting it: prediction difference and explicit model. They also show that without guided alignment in training, NMT systems perform worse than fast_align baseline. Using attention for word alignment is thoroughly discussed by Bahdanau et al. (2014) and Zenkel et al. (2019). Word alignment based on static and contextualized word embeddings is explored by the recent work of Sabet et al. (2020). Word alignment based on cross-lingual (more than 2 languages) methods is presented by Wu et al. (2021). The work of Chen et al. (2020b) focuses on inducing word alignments from glass-box NMT as a replacement for using Transformer attention layers directly. Chen et al. (2020a) document Mask Align, an unsupervised neural word aligner based on a single masked token prediction. Chen et al. (2016) propose guided attention, a mechanism that uses word alignment to bias the attention during training. This improves the MT performance on especially rare and unknown tokens.",
            "score": 0.3475544306000783,
            "section_title": "Symmetrization.",
            "char_start_offset": 3743,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 396
                },
                {
                    "start": 399,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 833
                },
                {
                    "start": 836,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1495
                },
                {
                    "start": 1496,
                    "end": 1616
                },
                {
                    "start": 1617,
                    "end": 1720
                },
                {
                    "start": 1721,
                    "end": 1873
                },
                {
                    "start": 1874,
                    "end": 1993
                },
                {
                    "start": 1994,
                    "end": 2114
                },
                {
                    "start": 2115,
                    "end": 2186
                }
            ],
            "ref_mentions": [
                {
                    "start": 913,
                    "end": 931,
                    "matchedPaperCorpusId": "5284722"
                },
                {
                    "start": 933,
                    "end": 964,
                    "matchedPaperCorpusId": "8797948"
                },
                {
                    "start": 966,
                    "end": 984,
                    "matchedPaperCorpusId": "5219389"
                },
                {
                    "start": 1089,
                    "end": 1105,
                    "matchedPaperCorpusId": "196176486"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.043121337890625
        },
        {
            "corpus_id": "13292366",
            "title": "Neural Machine Translation with Supervised Attention",
            "text": "As described in \u00a72, the attention model outputs a soft alignment \u03b1, such that \u03b1 t is a normalized probability distribution. In contrast, most aligners are typically oriented to grammar induction for conventional SMT, and they usually output 'hard' alignments, such as (Och and Ney, 2000). They only indicate whether a target word is aligned to a source word or not, and this might not correspond to a distribution for each target word. For example, one target word may align to multiple source words, or no source words at all. Therefore, we apply the following heuristics to preprocess the hard alignment: if a target word does not align to any source words, we inherit its affiliation from the closest aligned word with preference given to the right, following (Devlin et al., 2014); if a target word is aligned to multiple source words, we assume it aligns to each one evenly. In addition, in the implementation of NMT, there are two special tokens 'eol' added to both source and target sentences. We assume they are aligned to each other. In this way, we can obtain the final supervision of attention, denoted as \u03b1.",
            "score": 0.3475544306000783,
            "section_title": "Preprocessing Alignment Supervision",
            "char_start_offset": 7743,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1042
                },
                {
                    "start": 1043,
                    "end": 1119
                }
            ],
            "ref_mentions": [
                {
                    "start": 268,
                    "end": 287,
                    "matchedPaperCorpusId": "5284722"
                },
                {
                    "start": 763,
                    "end": 784,
                    "matchedPaperCorpusId": "7417943"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.047149658203125
        },
        {
            "corpus_id": "267523425",
            "title": "Six Fallacies in Substituting Large Language Models for Human Participants",
            "text": "For instance, in linguistic processing, layers of a language model can be probed to understand how they internally represent different syntactic and semantic structures; early layers tend to represent low-level syntactic features (e.g., part-of-speech tags, which are labels assigned to each word to indicate their grammatical function, such as noun, verb, and preposition), while later layers encode more complex semantic relationships 111 . This helps map specific nodes or attention heads to linguistic tasks, shedding light on how human language processing might work 17 . \n\nHowever, while probing techniques offer insights into the internal representations of language models, they come with limitations. Perhaps the most critical is the lack of causality: high performance in classification or decoding may not reflect what the model uses functionally for its primary tasks, but the probe's capacity to extract information (e.g., superficial correlations). Conversely, just because a feature is not captured by the probing classifier does not mean it is not encoded somewhere in the model. Thus, experimental techniques are needed to bring stronger evidence by manipulating model inputs and model architecture 110 . \n\nOne key experimental method involves manipulating the input data fed into language models-analogous to controlled rearing in animal studies-to observe how different training conditions affect model behavior and performance. Just as newborn chicks' visual experiences can be manipulated (e.g., slow or fast object motion) to reveal the core learning algorithms that support object perception, input manipulations in language models help assess which specific types of input are necessary for learning. For example, by removing instances of specific constructions from the input corpus, like AANN phrases (Article + Adjective + Numeral + Noun; \"a beautiful five days\"), one can ask whether the model is able to infer such structures from related ones. A recent study shows that exposure to simple noun phrases (\"a few days\")-but not counterfactual versions of the AANN construction, like ANAN (e.g., \"a five beautiful days\") and NAAN (e.g., \"five beautiful a days\")-provides scaffolding for generalization across linguistic constructions 112 , akin to how structured sensory data helps newborn animals learn complex visual tasks.",
            "score": 0.3473584698911289,
            "section_title": "Using language models to model cognitive processes",
            "char_start_offset": 51863,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 576
                },
                {
                    "start": 579,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1221
                },
                {
                    "start": 1224,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1724
                },
                {
                    "start": 1725,
                    "end": 1973
                },
                {
                    "start": 1974,
                    "end": 2351
                }
            ],
            "ref_mentions": [
                {
                    "start": 572,
                    "end": 574,
                    "matchedPaperCorpusId": "219315567"
                },
                {
                    "start": 1216,
                    "end": 1219,
                    "matchedPaperCorpusId": "236924832"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.21044921875
        },
        {
            "corpus_id": "247154873",
            "title": "Oolong: Investigating What Makes Transfer Learning Hard with Controlled Studies",
            "text": "We test the effects of different levels of wordorder perturbation in transfer learning. \n\n\u2022 Word identity alignments: Transferring to a new language requires learning the meaning, or word embeddings, of new words, and how arXiv:2202.12312v2 [cs.CL] 23 Jan 2024 \n\ntheir layer 0 embeddings correspond to the old language. We experiment with the effect of reinitializing or shuffling the rows of the layer 0 word embedding matrix before transfer. \n\n\u2022 Tokenizer quality We test the effect of bad tokenizer quality by reinitializing the word embedding matrix and transferring to English data tokenized with French and Dutch tokenizers that are suboptimal quality for English tokenization. \n\nWe test the effect of these factors on transfer learning both by 1) directly fine-tuning on t-English versions of the GLUE benchmark, as well as 2) continuing masked language model pre-training on 15 million tokens of t-English wikitext. In all cases, we find that word identity alignment provides the greatest stumbling block for transfer learning. Reinitializing or shuffling the rows of the embedding matrix has a very negative effect on downstream learning which we cannot reverse in the low-data regime that we are simulating. If the embedding matrix is reinitialized and a new tokenizer is used, the effect of reinitialization overshadows any effect that the quality of the new tokenizer might cause. In the case of syntactic word-order transformations, we find that even in the low-data transfer learning regime, the models we test can adapt to word order shifts as long as vocabulary information is kept. \n\nWe run experiments on RoBERTa, DeBERTa, and XLM-R in order to test transfer learning beyond the training set languages for both monolingual and multilingual models. Our method allows us to disentangle the effects of correlated factors by inspecting them one at a time.1",
            "score": 0.3472740425253475,
            "section_title": "Introduction",
            "char_start_offset": 2080,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 90,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 260
                },
                {
                    "start": 263,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 443
                },
                {
                    "start": 446,
                    "end": 683
                },
                {
                    "start": 686,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1598
                },
                {
                    "start": 1601,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 1870
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1171875
        },
        {
            "corpus_id": "11316388",
            "title": "Generalization in Artificial Language Learning: Modelling the Propensity to Generalize",
            "text": "In the following sections, we will first review some of the experimental data that has been interpreted as evidence for an additional generalization mechanism (Pe\u00f1a et al. (2002); Endress&Bonatti (2007); Frost&Monaghan (2016)). We then reframe the interpretation of those results with our 3-step approach, a proposal of the main steps that are required for generalization, involving: (i) memorization of segments of the input, (ii) computation of the probability for unseen sequences, and (iii) distribution of this probability among particular unseen sequences. We model the first step with the Retention&Recognition model (Alhama et al., 2016). We propose that a rational charac-terization of the second step can be accomplished with the use of smoothing techniques (which we further demonstrate with the use of the Simple Good-Turing method, (Good&Turing (1953); Gale (1995)). We then argue that the modelling results shown in these two steps already account for the key aspects of the experimental data; and importantly, it removes the need to postulate an additional, separate generalization mechanism. \n\n2 Experimental Record Pe\u00f1a et al. (2002) conduct a series of Artificial Language Learning experiments in which Frenchspeaking adults are familiarized to a synthesized speech stream consisting of a sequence of artificial words. Each of these words contains three syllables A i XC i such that the A i syllable always cooccurs with the C i syllable (as indicated by the subindex i). This forms a consistent pattern (a \"rule\") consisting in a non-adjacent dependency between A i and C i , with a middle syllable X that varies. The order of the words in the stream is randomized, with the constraint that words do not appear consecutively if they either: (i) belong to the same \"family\" (i.e., they have the same A i and C i syllables), or (ii) they have the same middle syllable X. After the familiarization phase, the participants respond a two-alternative forced choice test.",
            "score": 0.3470668490766586,
            "section_title": "Introduction",
            "char_start_offset": 1658,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 1107
                },
                {
                    "start": 1110,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1489
                },
                {
                    "start": 1490,
                    "end": 1632
                },
                {
                    "start": 1633,
                    "end": 1887
                },
                {
                    "start": 1888,
                    "end": 1983
                }
            ],
            "ref_mentions": [
                {
                    "start": 159,
                    "end": 178,
                    "matchedPaperCorpusId": "3835729"
                },
                {
                    "start": 866,
                    "end": 877,
                    "matchedPaperCorpusId": "46217277"
                },
                {
                    "start": 1132,
                    "end": 1150,
                    "matchedPaperCorpusId": "3835729"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06536865234375
        },
        {
            "corpus_id": "2443486",
            "title": "Translation Spotting for Translation Memories",
            "text": "As mentioned earlier, TS can be seen as a bi-product of word-level alignments. Such alignments have been the focus of much attention in recent years, especially in the field of statistical translation modeling, where they play an important role in the learning process.\n\nFor the purpose of statistical translation modeling, Brown et al. (1993) define an alignment as a vector a = a 1 ...a m that connects each word of a source-language text S = s 1 ...s m to a target-language word in its translation T = t 1 ...t n , with the interpretation that word t aj is the translation of word s j in S (a j = 0 is used to denote words of s that do not produce anything in T ).\n\nBrown et al. also define the Viterbi alignment between source and target sentences S and T as the alignment a whose probability is maximal under some translation model:\u00e2\n\nwhere A is the set of all possible alignments between S and T , and Pr M (a|S, T ) is the estimate of a's probability under model M, which we denote Pr(a|S, T ) from hereon. In general, the size of A grows exponentially with the sizes of S and T , and so there is no efficient way of computing\u00e2 efficiently. However, under Model 2, the probability of an alignment a is given by:\n\nwhere\n\nand\n\nIn this last equation, t(s i |t j ) is the model's estimate of the \"lexical\" distribution p(s i |t j ), while a(j, i, m, n) estimates the \"alignment\" distribution p(j|i, m, n). Therefore, with this model, the Viterbi alignment can be obtained by simply picking for each position i in S, the alignment that maximizes t(s i |t j )a(j, i, m, n). This procedure can trivially be carried out in O(mn) operations.\n\nBecause of this convenient property, we base the rest of this work on this model.\n\nAdapting this procedure to the TS task is straightforward: given the TS query q, produce as TL answer the corresponding set of TL tokens in the Viterbi alignment: r q (T ) = {",
            "score": 0.3469849501574296,
            "section_title": "Viterbi TS",
            "char_start_offset": 7296,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 324,
                    "end": 343,
                    "matchedPaperCorpusId": "33487754"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09234619140625
        },
        {
            "corpus_id": "235489941",
            "title": "Leveraging Language to Learn Program Abstractions and Search Heuristics",
            "text": "One implementation detail: the alignment algorithm operates over pairs of strings. For convenience we infer alignments between sentences and linearized token sequences in the program tree (which can be done with complete recoverability of the original program tree (Andreas et al., 2013)). This is another inductive assumption that we choose after preliminary experimentation and find that our implementation yields strong empirical results regardless. \n\nThe IBM translation model is a noisy-channel generative model that requires an additional language model p(d) to generate language (Gal & Blunsom, 2013;Heafield, 2011). \n\nWe use an efficient parallelized implementation for inferring the translation model parameters from (Koehn et al., 2007), which also contains a basic language model inference algorithm inferred over the full corpus of training task sentences (as a trigram model, which we again find simple but effective for our very small data setting). Specific model hyperparameters for all experiments are available in the released code repo (in the experiment runtime commands.) \n\nMutual exclusivity: Section 5.1 of the main paper also describes how the joint model can be modified to include language-specific priors, such as a simple implementation of the well-known mutual exclusivity prior documented in the cognitive language-learning literature (Markman & Wachtel, 1988;Gandhi & Lake, 2019) and given a Bayesian formulation in (Frank et al., 2009). We provide an implementation to demonstrate that the joint model can be easily extended: specifically, a simple mutual exclusivity assumption can be added into the joint model by simply updating the compositional translation model to include additional distributions t M E (d new |l) where d new are words that only appear in unsolved training tasks and \n\nnew words are now assumed to correspond to primitives inversely proportional to their current usage under the learned program prior. As we show in the next section, incorporating this prior at the level of the joint model can be used to approximate mutual exclusivity assumptions in the learned search heuristic, encouraging exploration in the presence of new words. \n\nPractically, we calculate the mutual exclusivity prior in our concrete implementation by leveraging the alignments upon which our token-token translation probabilities are defined.",
            "score": 0.3462943475612066,
            "section_title": "S5.1 Joint prior over programs and language",
            "char_start_offset": 40598,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 82
                },
                {
                    "start": 83,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 452
                },
                {
                    "start": 455,
                    "end": 623
                },
                {
                    "start": 626,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1092
                },
                {
                    "start": 1095,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1822
                },
                {
                    "start": 1825,
                    "end": 1957
                },
                {
                    "start": 1958,
                    "end": 2191
                },
                {
                    "start": 2194,
                    "end": 2374
                }
            ],
            "ref_mentions": [
                {
                    "start": 265,
                    "end": 287,
                    "matchedPaperCorpusId": "1954146"
                },
                {
                    "start": 607,
                    "end": 622,
                    "matchedPaperCorpusId": "8313873"
                },
                {
                    "start": 726,
                    "end": 746,
                    "matchedPaperCorpusId": "61651780"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.086181640625
        },
        {
            "corpus_id": "273901687",
            "title": "Take Off the Training Wheels! Progressive In-Context Learning for Effective Alignment",
            "text": "However, this shift in the separator tokens is not consistent across different demonstration settings, suggesting distinct underlying causes for these shifts. We attribute the shift in the query's prior token distribution to a \"context shift\", and we attribute the shift in the separator tokens distribution to a \"task shift\". Given that LLMs are trained to predict the next token based on the provided context, altering the context directly impacts the token distribution, which we refer to as the \"context shift\". However, as the number of query tokens increases, the decision space gradually aligns for both zero-shot and few-shot settings, leading to higher consistency in query token prediction and thus a reduced KL-divergence. \n\nOn the contrary, the trend observed in the query distribution is not mirrored in the separator token distribution. In the control group, the separator token representations remain highly similar. We attribute the large KL-divergence observed in the separator token distribution of the experimental group to the differing tasks, indicating that separator tokens likely encode task-specific information during ICL. We reasonably speculate that the primary impact of demonstration on instruction understanding is reflected in the encoding of separator tokens, where the alignment task function learned through ICL is stored. This hypothesis aligns with prior work (Hendel et al., 2023;Li et al., 2024), yet our findings contribute additional evidence supporting this perspective. \n\nOutput Token Distribution. Observing the visualization of output token distribution, we find that when comparing zero-shot and few-shot settings, the response token distribution shows similarity in the posterior tokens. This indicates that the model selects posterior tokens with high consistency in both zero-shot and few-shot settings. \n\nWhen comparing the prior response tokens of the experimental group and the control group, we observe a pattern similar to that of the separator tokens, suggesting that demonstrations play a crucial role in the prior response tokens. Based on these observations and analyses, we speculate that the primary impact of demonstrations on response generation is reflected in the generation of prior answer tokens. Compared to zero-shot settings, demonstrations guide the generation of accurate prior response tokens, which implicitly helps the model successfully follow the instructions.",
            "score": 0.3462624938294461,
            "section_title": "Motivation",
            "char_start_offset": 10073,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 733
                },
                {
                    "start": 736,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1512
                },
                {
                    "start": 1515,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1734
                },
                {
                    "start": 1735,
                    "end": 1852
                },
                {
                    "start": 1855,
                    "end": 2087
                },
                {
                    "start": 2088,
                    "end": 2262
                },
                {
                    "start": 2263,
                    "end": 2436
                }
            ],
            "ref_mentions": [
                {
                    "start": 1397,
                    "end": 1418,
                    "matchedPaperCorpusId": "264439386"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.47900390625
        },
        {
            "corpus_id": "250113885",
            "title": "Structural Persistence in Language Models: Priming as a Window into Abstract Language Representations",
            "text": "The guest threw the lady the pot \n\nEvidence for structural priming-to the extent that it can be shown to be independent from lexical overlap and other confounds-is taken as evidence for a linguistic structural level of representation that abstracts away from the surface form of sentences. Thus whether or not language models display structural priming can provide insights as to their structural awareness, which is necessary for downstream tasks requiring natural language understanding skills. Previous experiments designed to test structural encoding in LMs are inconclusive. On the one hand, studies on structural probing (Hewitt and Manning, 2019) and on syntactic evaluation tasks (Warstadt et al., 2020) have yielded evidence for its presence. On the other hand, other sets of experiments have indicated that current LMs are surprisingly indifferent to word order (Hessel and Schofield, 2021;Pham et al., 2021;Sinha et al., 2021a) and rely on superficial heuristics when resolving downstream tasks (Mc-Coy et al., 2019;Sinha et al., 2021b). Such unresolved tensions between results -and the active debate about them-highlights the need for developing additional methodologies that isolates structure from the lexico-semantic cues given to the model. In this paper, we leverage findings from structural priming in human language processing to develop a systematic experimental pipeline with the aim of assessing the extent to which pre-trained neural language models learn representations that encode structural information-a prerequisite for their good performance on natural language understanding tasks. \n\nWe use the term 'structural priming' (Pickering and Ferreira, 2008) rather than 'syntactic priming' (first described in Katryn Bock's Syntactic persistence in language production, 1986) because it comprises priming of abstract structural information that is not restricted to syntactic hierarchical rules, such as the linear positions of semantic roles or the sequential order of parts of speech. In this paper, we focus mostly on the latter and touch upon syntactic rules in Section 7.4. \n\nIn Section 3, we define an efficient novel metric for measuring the effect of priming.",
            "score": 0.3459938303243552,
            "section_title": "Introduction",
            "char_start_offset": 2041,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 32
                },
                {
                    "start": 35,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1613
                },
                {
                    "start": 1616,
                    "end": 2012
                },
                {
                    "start": 2013,
                    "end": 2104
                },
                {
                    "start": 2107,
                    "end": 2193
                }
            ],
            "ref_mentions": [
                {
                    "start": 627,
                    "end": 653,
                    "matchedPaperCorpusId": "106402715"
                },
                {
                    "start": 872,
                    "end": 900,
                    "matchedPaperCorpusId": "236460117"
                },
                {
                    "start": 900,
                    "end": 918,
                    "matchedPaperCorpusId": "229923132"
                },
                {
                    "start": 918,
                    "end": 938,
                    "matchedPaperCorpusId": "233231592"
                },
                {
                    "start": 1027,
                    "end": 1047,
                    "matchedPaperCorpusId": "230435766"
                },
                {
                    "start": 1653,
                    "end": 1683,
                    "matchedPaperCorpusId": "24718788"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08758544921875
        },
        {
            "corpus_id": "267069237",
            "title": "AI for social science and social science of AI: A Survey",
            "text": "Once the research topic and hypotheses are established, social science researchers engage in hypothesis verification. This process involves collecting and analyzing data to provide evidence that either supports or refutes the proposed hypotheses (Donovan and Hoover, 2013). In traditional social science research, hypothesis verification typically falls into quantitative methods like experimental research, survey research and nonreactive research, as well as qualitative methods such as field research and historical-comparative research (Juren Lin, 2017;Yuan, 2013;Bryman, 2016). Given that large language models are currently limited in their applicability to qualitative research, we primarily discuss the role of large language models in quantitative methods.",
            "score": 0.3457885823004405,
            "section_title": "Hypothesis Verification",
            "char_start_offset": 21119,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 765
                }
            ],
            "ref_mentions": [
                {
                    "start": 246,
                    "end": 272,
                    "matchedPaperCorpusId": "258569852"
                },
                {
                    "start": 557,
                    "end": 568,
                    "matchedPaperCorpusId": "263829704"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05889892578125
        },
        {
            "corpus_id": "243832880",
            "title": "On the Impact of Temporal Representations on Metaphor Detection",
            "text": "According to Aitchison [2010], theories often come as a formalization of metaphors, which \"can populate history with new objects and kinds, and provide both access to interesting new worlds and great field-internal success\". Based on the observations that language is always changing Beckner et al. [2009], La Mantia et al. [2017], Massip-Bonet and Bastardas Boada [2013], linguists have formulated different theories and models searching for rules and regularities in semantic change, such as the \"Diachronic Prototype Semantics\" Geeraerts [1997], Geeraerts et al. [1999], the \"Invited Inference Theory of Semantic Change\" Traugott and Dasher [2001], and \"semantic change based on metaphor and metonymy\" Heine et al. [1991].\n\nHistorically, much of the theoretical work on semantic shifts has been devoted to documenting and categorizing various types of semantic shifts Breal [1897], Stern [1975]. Semantic shifts are separated into two important classes: \"linguistic drifts\" (slow and steady changes in core meaning of words) and \"cultural shifts\" (changes in associations of a given word determined by cultural influences). In Gulordava and Baroni [2011], the authors showed that distributional models capture cultural shifts, like the word \"sleep\" acquiring more negative connotations related to the sleep disorders domain, when comparing its 1960s contexts with its 1990s contexts. Researchers studying semantic change from a computational point of view have empirically shown the existence of this distinction Hamilton et al. [2016a].\n\nDiachronic corpora provide empirical resources for semantic change analysis. The availability of large corpora enabled the development of new methodologies for the study of lexical-semantic shifts within general linguistics Traugott and Dasher [2001]. A key assumption is that changes in a word's collocational patterns reflect changes in word meaning, thus providing a usage-based account of semantic shifts. Semantic changes are often reflected in large corpora that can be sliced into time-specific chunks (e.g., texts coming from a same decade), which account for changes in the contexts of a word that is affected by the shift. Most recent approaches to studying diachronic semantic",
            "score": 0.34537084619167385,
            "section_title": "Temporal Language Evolution",
            "char_start_offset": 9435,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 284,
                    "end": 305,
                    "matchedPaperCorpusId": "143150253"
                },
                {
                    "start": 1130,
                    "end": 1157,
                    "matchedPaperCorpusId": "17759436"
                },
                {
                    "start": 1516,
                    "end": 1539,
                    "matchedPaperCorpusId": "2162648"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.056854248046875
        },
        {
            "corpus_id": "269004471",
            "title": "Binary Classifier Optimization for Large Language Model Alignment",
            "text": "Aligning Large Language Models (LLMs) to human preferences through preference optimization has been crucial but labor-intensive, necessitating for each prompt a comparison of both a chosen and a rejected text completion by evaluators. Recently, Kahneman-Tversky Optimization (KTO) has demonstrated that LLMs can be aligned using merely binary\"thumbs-up\"or\"thumbs-down\"signals on each prompt-completion pair. In this paper, we present theoretical foundations to explain the successful alignment achieved through these binary signals. Our analysis uncovers a new perspective: optimizing a binary classifier, whose logit is a reward, implicitly induces minimizing the Direct Preference Optimization (DPO) loss. In the process of this discovery, we identified two techniques for effective alignment: reward shift and underlying distribution matching. Consequently, we propose a new algorithm, \\textit{Binary Classifier Optimization}, that integrates the techniques. We validate our methodology in two settings: first, on a paired preference dataset, where our method performs on par with DPO and KTO; and second, on binary signal datasets simulating real-world conditions with divergent underlying distributions between thumbs-up and thumbs-down data. Our model consistently demonstrates effective and robust alignment across two base LLMs and three different binary signal datasets, showcasing the strength of our approach to learning from binary feedback.",
            "score": 0.34448529217272805,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1143798828125
        },
        {
            "corpus_id": "276249618",
            "title": "PIPA: Preference Alignment as Prior-Informed Statistical Estimation",
            "text": "Pre-training large language models (LLMs) from scratch on trillions of text tokens allows for accurate prediction next tokens in natural language [Achiam et al., 2023, Dubey et al., 2024, Liu et al., 2024a]. Following this, alignment, achieved through fine-tuning on smaller, high-quality datasets designed for specific tasks, becomes critical for enabling the model to develop specialized skills, such as engaging in conversation [Ouyang et al., 2022], math reasoning [Shao et al., 2024, Yang et al., 2024], coding [Zhu et al., 2024], web agent [Qin et al., 2025], and more. The fundamental approach to alignment involves supervised fine-tuning (SFT) on the target domain, which essentially maximizes the likelihood of predicting the next token. However, numerous empirical studies have shown that simple SFT on preferred samples is inadequate for attaining optimal performance [Shao et al., 2024, Ouyang et al., 2022]. \n\nMoving beyond basic imitation learning in SFT, it is suggested to learn from both positive and negative samples. Sample quality can be measured by training reward models to capture general preferences [Dong et al., 2024] or leveraging accurate rule-based rewards [Guo et al., 2025] for specific tasks like math and coding. By treating the autoregressive generation of LLMs as a Markov decision process (MDP), traditional reinforcement learning (RL) algorithms can be effectively applied, such as PPO [Ouyang et al., 2022], SAC [Liu et al., 2024b], REINFORCE [Ahmadian et al., 2024], etc. \n\nWhile online RL-based methods deliver strong performance, they face challenges such as high training costs, instability, and the need for a strong base model as the initial policy. As a result, offline algorithms like direct preference optimization (DPO) [Rafailov et al., 2024] are often preferred, thanks to their effectiveness and simplicity, particularly when high-quality datasets are accessible. The original DPO algorithm has several limitations.",
            "score": 0.3442445685460181,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 746
                },
                {
                    "start": 747,
                    "end": 920
                },
                {
                    "start": 923,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1510
                },
                {
                    "start": 1513,
                    "end": 1693
                },
                {
                    "start": 1694,
                    "end": 1914
                },
                {
                    "start": 1915,
                    "end": 1966
                }
            ],
            "ref_mentions": [
                {
                    "start": 431,
                    "end": 452,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 897,
                    "end": 919,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1423,
                    "end": 1444,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1768,
                    "end": 1791,
                    "matchedPaperCorpusId": "258959321"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.107421875
        },
        {
            "corpus_id": "231639391",
            "title": "Challenges for Computational Lexical Semantic Change",
            "text": "As a result, an important factor for future research becomes the creation of synthetic datasets that reflect the complex and varying nature of real language and real semantic change. \n\nWe stipulate that simulated datasets should be used alongside ground-truth testing, both with respect to pre-chosen test sets, as well as evaluating the output, to properly evaluate the ability of any method to detect LSC.",
            "score": 0.3441653753236377,
            "section_title": "Simulated LSC",
            "char_start_offset": 37633,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 185,
                    "end": 407
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10107421875
        },
        {
            "corpus_id": "270878713",
            "title": "How to Learn in a Noisy World? Self-Correcting the Real-World Data Noise on Machine Translation",
            "text": "To show the similarity of our simulated noise to real-world misalignment, we conduct a human evaluation on 50 simulated and real-world misaligned sentences rating their Adequacy (scale 1-5), which measures the meaning overlap between source and target.In Figure 2, we show both the real-world misalignment and simulated misaligned-Laser/Comet has a relatively high adequacy score, above 2.5, while Randomly shuffled misaligned sentences only has an adequacy of 1.5.This ensures our simulated misalignment contains partial semantic overlaps as the real-world misalignment.Moreover, our simulation process confirms the fluency of misaligned targets by selecting sentences from a clean corpus.Details of human evaluation are in Appendix B.3.",
            "score": 0.3441653753236377,
            "section_title": "Adequacy",
            "char_start_offset": 10895,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 252
                },
                {
                    "start": 252,
                    "end": 465
                },
                {
                    "start": 465,
                    "end": 571
                },
                {
                    "start": 571,
                    "end": 690
                },
                {
                    "start": 690,
                    "end": 738
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03326416015625
        },
        {
            "corpus_id": "271218202",
            "title": "The Foundations of Tokenization: Statistical and Computational Concerns",
            "text": "In addition, tokenization is often described as an efficient, lossless encoding of the original data (Zouhar et al., 2023a). Moreover, based on empirical evidence of different kinds, tokenization has been hypothesized to introduce a helpful inductive bias in language modeling (Nawrot et al., 2023;Schmidt et al., 2024;Uzan et al., 2024), although in the current state of the art, this hypothesis remains an open question. At the same time, tokenizers have also been in the spotlight for exhibiting undesirable behaviors that can have a negative impact on LMs. To name just a few, tokenization can be the source of spurious ambiguity (Kudo, 2018;Cao & Rimell, 2021), generate alignment issues (Poesia et al., 2022;Athiwaratkun et al., 2024), hinder robustness (Kudo, 2018;Xue et al., 2022), neglect relevant linguistic features (Bostrom & Durrett, 2020;Hofmann et al., 2021;Gow-Smith et al., 2022;Beinborn & Pinter, 2023) or result in inconsistent scoring in the use of LMs in other scientific fields, like psycholinguistics (Salazar et al., 2020;Kauf & Ivanova, 2023;Giulianelli et al., 2024). \n\nThe prominence of undesirable behaviors induced by tokenization, together with the lack of conclusive theoretical explanations for either their positive or negative effects in language modeling, motivated several recent attempts to dispense with tokenization altogether (Xue et al., 2022;Clark et al., 2022;Wang et al., 2024, inter alia). However, in the current state of research, the practical benefits of token representations in neural language modeling seem to outweigh their disadvantages, indicating that there is something to be understood rather than discarded in the process of tokenization. \n\nThe study of tokenization models has been an active area of research in recent years.",
            "score": 0.3433187522751029,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1788,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 1094
                },
                {
                    "start": 1097,
                    "end": 1435
                },
                {
                    "start": 1436,
                    "end": 1698
                },
                {
                    "start": 1701,
                    "end": 1786
                }
            ],
            "ref_mentions": [
                {
                    "start": 101,
                    "end": 123,
                    "matchedPaperCorpusId": "259286948"
                },
                {
                    "start": 277,
                    "end": 298,
                    "matchedPaperCorpusId": "253581399"
                },
                {
                    "start": 634,
                    "end": 646,
                    "matchedPaperCorpusId": "13753208"
                },
                {
                    "start": 646,
                    "end": 665,
                    "matchedPaperCorpusId": "237421122"
                },
                {
                    "start": 760,
                    "end": 772,
                    "matchedPaperCorpusId": "13753208"
                },
                {
                    "start": 772,
                    "end": 789,
                    "matchedPaperCorpusId": "235248316"
                },
                {
                    "start": 828,
                    "end": 853,
                    "matchedPaperCorpusId": "215416175"
                },
                {
                    "start": 853,
                    "end": 874,
                    "matchedPaperCorpusId": "233880587"
                },
                {
                    "start": 874,
                    "end": 897,
                    "matchedPaperCorpusId": "248069436"
                },
                {
                    "start": 897,
                    "end": 921,
                    "matchedPaperCorpusId": "264406163"
                },
                {
                    "start": 1025,
                    "end": 1047,
                    "matchedPaperCorpusId": "218628872"
                },
                {
                    "start": 1047,
                    "end": 1068,
                    "matchedPaperCorpusId": "258762317"
                },
                {
                    "start": 1068,
                    "end": 1093,
                    "matchedPaperCorpusId": "273098477"
                },
                {
                    "start": 1367,
                    "end": 1385,
                    "matchedPaperCorpusId": "235248316"
                },
                {
                    "start": 1385,
                    "end": 1404,
                    "matchedPaperCorpusId": "232185112"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08416748046875
        },
        {
            "corpus_id": "270357475",
            "title": "Phased Instruction Fine-Tuning for Large Language Models",
            "text": "This nuanced result suggests that mixing stage 2 data had a detrimental effect, leading to reduced model performance.Nevertheless, when uptraining across all three stages (stage 1, stage 2, and stage 3), the win rate steadily improved, indicating a gradual enhancement in model capability.This is a very interesting experimental phenomenon, and similar observations were made in ablation experiments with Llama2 13B.This indicates that the model performance improvement achieved through uptraining on increasingly difficult sub-datasets is not a trivial training technique.The quality of the instruction data and the alignment method are key factors in fine-tuning of large language models.This also further supports the hypothesis of alignment progression as a genuine phenomenon.",
            "score": 0.3430800119660657,
            "section_title": "Ablation Studies",
            "char_start_offset": 25778,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 117,
                    "end": 289
                },
                {
                    "start": 289,
                    "end": 416
                },
                {
                    "start": 416,
                    "end": 573
                },
                {
                    "start": 573,
                    "end": 690
                },
                {
                    "start": 690,
                    "end": 781
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1702880859375
        },
        {
            "corpus_id": "236881060",
            "title": "A Dynamic Head Importance Computation Mechanism for Neural Machine Translation",
            "text": "Our proposed approach DHICM outperforms Tbase and T-optimal by a large margin in the low resource conditions. We further analyzed the performance of the baseline model and DHICM, and observed that DHICM learns better word alignment especially, in low resource conditions. One of the reasons for learning better alignment can be that for each word, all heads are not equally important. The second level attention that we designed in our model allows the tokens to pay more attention to the heads that capture more relevant information for translation. Since the heads that are more relevant receive more attention, the parts of the input to which these heads attend, in turn receive more attention, and thus, the alignment becomes better. For example, providing more attention to the heads that capture the syntactic or semantic information, and relatively less attention to the heads that capture positional information. This justifies our hypothesis mentioned in Section 3.1. \n\nWe also verified this using the encoder-decoder attention distribution of the models shown in Figure 1 (low resource conditions) and Figure 2 (high resource conditions). The decoder of the transformer model uses the outputs of the encoder to generate the tokens in the target language. Each generated token pays some attention to each token in the source language. The attention distribution matrix shows the attention paid by the generated tokens in the target sentence (rows) to the tokens in the source sentence (columns). In Figure 1a and Figure 2a, we can see that most of the tokens on the source side get similar attention for the baseline approach. Moreover, the highest attention a source token receives is approximately 0.12 and 0.5 in Figure 1a and Figure 2a, respectively. This implies that the most important source token for translation does not receive enough attention, resulting in a poor word alignment. On the contrary, for DHICM (Figure 1b and Figure 2b), we observe a large variance in the distribution of the attention paid by a target token to the source tokens. Thus, more appropriate source tokens receive higher attention scores (\u223c 0.8) in DHICM, leading to a better word alignment, as shown in both Figure 1b and Figure 2b.",
            "score": 0.3429012748587024,
            "section_title": "Analysis",
            "char_start_offset": 17335,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 737
                },
                {
                    "start": 738,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 976
                },
                {
                    "start": 979,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1343
                },
                {
                    "start": 1344,
                    "end": 1504
                },
                {
                    "start": 1505,
                    "end": 1635
                },
                {
                    "start": 1636,
                    "end": 1763
                },
                {
                    "start": 1764,
                    "end": 1900
                },
                {
                    "start": 1901,
                    "end": 2064
                },
                {
                    "start": 2065,
                    "end": 2229
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1365966796875
        },
        {
            "corpus_id": "239024343",
            "title": "Monotonic Simultaneous Translation with Chunk-wise Reordering and Refinement",
            "text": "In the alignment-aware chunking method, we segment a sentence adaptively by leveraging alignment information a, as described in Algorithm 1. The left grid in Figure 3 presents the subword alignment between source and target sentence. We run aligner on subword over word because the alignment performance is consistently better Zenkel et al. (2020) when using GIZA++ (Och and Ney, 2003) , which we use in our experiments. Based on this alignment information, we initialize a list of chunks C. As observable, there are some tokens which have no alignment information. To avoid omission, we assign the same alignment as the previous token; if a token is at the head, it follows the next token's alignment. To ensure subwords can be properly detokenized after reordering, we merge mid-splitted subwords. The middle grid in Figure 3 presents the result of these initialization steps. After initialization, we generate consistent chunks by merging all the inconsistent ones, following the definition of consistency in Zens et al. (2002). In a consistent chunk, tokens are only aligned to each other, not to tokens in other chunks. If any chunk in C has size smaller than a minimum size threshold \u03b4, we merge a chunk pair that are adjacent in both source and target side and have the shortest target distance between them. If the distances are the same between multiple candidate pairs, we choose the pair of chunks that makes the smallest size after merging. We additionally merge the chunks adjacent to the merged one if they are arranged monotonically. Merging is repeated until all chunks meet the size requirements. An example of final result is the right grid in Figure 3. Phrase extraction method used in statistical machine translation Koehn (2004) also makes phrase level alignments from word alignments using heuristics like ours, but it tends to choose shorter phrases since the number of co-occurrences decrease drastically as the phrase size grows, which makes it difficult to generate larger chunks to prevent hurting grammatical correctness while reordering phase.",
            "score": 0.34251646059769797,
            "section_title": "Alignment-Aware Chunk Reordering",
            "char_start_offset": 8011,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 1031
                },
                {
                    "start": 1032,
                    "end": 1124
                },
                {
                    "start": 1125,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1452
                },
                {
                    "start": 1453,
                    "end": 1548
                },
                {
                    "start": 1549,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 2072
                }
            ],
            "ref_mentions": [
                {
                    "start": 327,
                    "end": 347,
                    "matchedPaperCorpusId": "216869436"
                },
                {
                    "start": 366,
                    "end": 385,
                    "matchedPaperCorpusId": "5219389"
                },
                {
                    "start": 1012,
                    "end": 1030,
                    "matchedPaperCorpusId": "877521"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.040008544921875
        },
        {
            "corpus_id": "272367307",
            "title": "Csi-LLM: A Novel Downlink Channel Prediction Method Aligned with LLM Pre-Training",
            "text": "Large language foundation models are general models of language that are designed to support a large vairty of AI tasks [12]. The development of modern foundation models consists of two main stages: (1) a pre-training stage in which the model is trained at massive scale using straightforward tasks such as next token generation and (2) a post-training stage in which the model is tuned to improve specific capabilities in downstream tasks. \n\nOne of the key focuses in modern AI systems is how to leverage the general token sequence modeling capabilities acquired during the pre-training stage to enhance the performance of downstream tasks in the post-training stage. Alignment is considered one of the most effective methods to achieve this [13]. To maximize the utilization of LLMs' pattern recognition and reasoning abilities, Csi-LLM aligns the training process of LLMs in three aspects: network structure, data processing, and optimization objectives.",
            "score": 0.3423981636161787,
            "section_title": "III. CSI-LLM",
            "char_start_offset": 6262,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 440
                },
                {
                    "start": 443,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 957
                }
            ],
            "ref_mentions": [
                {
                    "start": 743,
                    "end": 747,
                    "matchedPaperCorpusId": "3420321"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1044921875
        },
        {
            "corpus_id": "261076348",
            "title": "How to Protect Copyright Data in Optimization of Large Language Models?",
            "text": "Theoretical LLM. Since the explosion of large language models, theoretical research on transformer has been one major component of improving language model performance (Kitaev, Kaiser, and Levskaya 2020;Chen et al. 2020;Tay et al. 2020;Noci et al. 2022;Deng, Li, and Song 2023;Panigrahi et al. 2023;Arora and Goyal 2023;Sun et al. 2023;Sanford, Hsu, and Telgarsky 2023;Jiang, Ren, and Lin 2023;Alman and Song 2023;Brand, Song, and Zhou 2023;Zelikman et al. 2023;Malladi et al. 2023;Liu et al. 2023a;Rafailov et al. 2023;Ignat et al. 2023;Gao, Song, and Yin 2023;Zhao et al. 2023;Deng et al. 2023;Gao et al. 2023;Wu et al. 2023b;Liu et al. 2023b). (R\u00fcckl\u00e9 et al. 2020) proposes AdapterDrop, a method that removes adapters from lower transformer layers during training and inference to reduce computational overhead, while still maintaining task performance. (Tay et al. 2021) shows that random alignment matrices perform competitively and learning attention weights from token-token interactions is not highly significant. So they propose Synthesizer, a model that learns synthetic attention weights without token-token interactions and performs well in various tasks. (Chen et al. 2021) proposes Scatterbrain, a way to balance model quality and efficiency in approximating long sequences. Recent work (Arora and Goyal 2023) explores the emergence of new skills in language models through scaling up their parameters and training data. This demonstrates through mathematical analysis that the Scaling Laws provide a strong inductive bias, enabling efficient learning in pre-trained models. they term this phenomenon \"slingshot generalization,\" as it seems to violate traditional generalization theory. Optimization and Convergence of Deep Neural Networks.",
            "score": 0.34201475334744424,
            "section_title": "Related Work",
            "char_start_offset": 6381,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 16
                },
                {
                    "start": 17,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1588
                },
                {
                    "start": 1589,
                    "end": 1700
                },
                {
                    "start": 1701,
                    "end": 1754
                }
            ],
            "ref_mentions": [
                {
                    "start": 203,
                    "end": 220,
                    "matchedPaperCorpusId": "231830912"
                },
                {
                    "start": 220,
                    "end": 236,
                    "matchedPaperCorpusId": "218487423"
                },
                {
                    "start": 236,
                    "end": 253,
                    "matchedPaperCorpusId": "61153563"
                },
                {
                    "start": 320,
                    "end": 336,
                    "matchedPaperCorpusId": "260334454"
                },
                {
                    "start": 499,
                    "end": 520,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 520,
                    "end": 538,
                    "matchedPaperCorpusId": "235825902"
                },
                {
                    "start": 612,
                    "end": 628,
                    "matchedPaperCorpusId": "259108694"
                },
                {
                    "start": 628,
                    "end": 645,
                    "matchedPaperCorpusId": "250073264"
                },
                {
                    "start": 1168,
                    "end": 1186,
                    "matchedPaperCorpusId": "248498407"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.030792236328125
        },
        {
            "corpus_id": "273549247",
            "title": "A Little Help Goes a Long Way: Efficient LLM Training by Leveraging Small LMs",
            "text": "KL divergence is a common choice to promote such an alignment, which amounts to minimizing the following cross-entropy loss: \n\nTraining based on the loss in ( 2) is referred to as the token-level knowledge distillation [KD; Kim and Rush, 2016] in the literature, with LMs parameterized by \u03b6 and \u03b8 termed as the teacher and student LMs, respectively. See Appendix A.2 for a discussion on other related KD for LM variants. Temperature scaling of teacher is a common strategy [Zheng and Yang, 2024] where, given a temperature \u03c1 > 0, one utilizes \n\n\u03c1 during KD, resulting in the loss: \n\nIn practice, one typically utilizes both ground truth next-token as well as teacher's next-token distribution and, for a distillation loss weight \u03c9 \u2208 [0, 1], minimizes the following as the loss for x: \n\nNote that, for brevity, our notation \u2113 \u03c9 (x; \u03b8) omits the dependence on \u03b6 and \u03c1. \n\n3 Theoretical analysis: When can KD help language modeling? \n\nAs alluded in the introduction, we hope to leverage KD with an SLM as the teacher to speed-up the pretraining of a high quality LLM. However, due to SLM's relatively limited capacity and inferior quality, it is not immediately clear that such a teacher can benefit the LLM. Motivated by this, we now develop a rigorous statistical framework for KD in the context of language modeling by building on the works of Menon et al. \n\n[2021], Dao et al. [2021a], Ren et al. [2022a]. Novel risk bounds originating from this framework highlight how a teacher LM -even a perceivably weaker one -can benefit student LLM by striking the right balance in terms of a bias-variance trade-off. \n\nNotably, our analysis allows one to control the generalization gap for the student LM in terms of both number of training sequences N as well as number of total tokens N T , with the latter being highly non-trivial due to possibly arbitrary dependence within a training sequence. In this work, we crucially leverage certain natural stability conditions on the underlying distribution and function class to obtain such bounds in terms of N T .",
            "score": 0.3417186360839898,
            "section_title": "Background",
            "char_start_offset": 7844,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 127,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 542
                },
                {
                    "start": 545,
                    "end": 580
                },
                {
                    "start": 583,
                    "end": 783
                },
                {
                    "start": 786,
                    "end": 866
                },
                {
                    "start": 869,
                    "end": 928
                },
                {
                    "start": 931,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1204
                },
                {
                    "start": 1205,
                    "end": 1355
                },
                {
                    "start": 1358,
                    "end": 1405
                },
                {
                    "start": 1406,
                    "end": 1607
                },
                {
                    "start": 1610,
                    "end": 1889
                },
                {
                    "start": 1890,
                    "end": 2052
                }
            ],
            "ref_mentions": [
                {
                    "start": 224,
                    "end": 243,
                    "matchedPaperCorpusId": "8451212"
                },
                {
                    "start": 473,
                    "end": 495,
                    "matchedPaperCorpusId": "267751335"
                },
                {
                    "start": 1366,
                    "end": 1384,
                    "matchedPaperCorpusId": "233307448"
                },
                {
                    "start": 1386,
                    "end": 1404,
                    "matchedPaperCorpusId": "247244493"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0762939453125
        },
        {
            "corpus_id": "273351107",
            "title": "Understanding Likelihood Over-optimisation in Direct Alignment Algorithms",
            "text": "Direct Alignment Algorithms (DAAs), such as Direct Preference Optimisation (DPO) and Identity Preference Optimisation (IPO), have emerged as alternatives to online Reinforcement Learning from Human Feedback (RLHF) algorithms such as Proximal Policy Optimisation (PPO) for aligning language models to human preferences, without the need for explicit reward modelling. These methods generally aim to increase the likelihood of generating better (preferred) completions while discouraging worse (non-preferred) ones, while staying close to the original model's behaviour. In this work, we explore the relationship between completion likelihood and model performance in state-of-the-art DAAs, and identify a critical issue of likelihood over-optimisation. Contrary to expectations, we find that higher likelihood of better completions and larger margins between better and worse completion likelihoods do not necessarily lead to better performance, and may even degrade it. Our analysis reveals that while higher likelihood correlates with better memorisation of factual knowledge patterns, a slightly lower completion likelihood tends to improve output diversity, thus leading to better generalisation to unseen scenarios. Moreover, we identify two key indicators that signal when over-optimised output diversity begins to harm performance: Decreasing Entropy over Top-k Tokens and Diminishing Top-k Probability Mass. Our experimental results validate that these indicators are reliable signs of declining performance under different regularisations, helping prevent over-optimisation and improve alignment with human preferences.",
            "score": 0.34162634471904213,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06097412109375
        },
        {
            "corpus_id": "277510578",
            "title": "From Text to Graph: Leveraging Graph Neural Networks for Enhanced Explainability in NLP",
            "text": "Other kinds of global methods are mechanistic interpretability models, which focus on the connections between different parts of the model globally, such as aligning model representations with ground truth Chughtai et al. (2023), studying how neuron activation affects model output globally Meng et al. (2023) or locating areas of the model associated with specific knowledge Belrose et al. (2023). \n\nAlthough different methods have emerged in recent years to explain the behaviour of language models, these methods are based on unstructured knowledge, such as text. In the case of tokens used in language models, this knowledge can be even less structured, dividing words into different tokens that have no inherent semantic meaning. \n\nTree structures, which can be interpreted as graphs, can be a starting point for translating unstructured to structured knowledge, retaining all the information from the original text. They capture the information of each sentence's words and the relationships between the elements of the text. In this context, dependency and constituency trees represent sentences as graphs that maintain all the information.",
            "score": 0.3411433701078841,
            "section_title": "Explainability in Language Models",
            "char_start_offset": 7528,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 398
                },
                {
                    "start": 401,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 734
                },
                {
                    "start": 737,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1031
                },
                {
                    "start": 1032,
                    "end": 1147
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10052490234375
        },
        {
            "corpus_id": "155099437",
            "title": "Studying Lexical Dynamics and Language Change via Generalized Entropies: The Problem of Sample Size",
            "text": "message of this paper is that those two fundamental issues also pose a strong challenge to the application of information theory for the quantitative study of natural language signals. In addition, the results of the case study (cf. Section 3.3) indicate that both fundamental issues in lexical statistics apparently interact with each other. As mentioned above, there are numerous studies that used the Jensen-Shannon divergence or related measures without an explicit \"Litmus test\". Let us mention two examples from our own research:\n\n(i) In [12], an exploratory data-driven method was presented that extracts word-types from diachronic corpora that have undergone the most pronounced change in frequency of occurrence in a given period of time. To this end, a measure that is approximately equivalent to the Jensen-Shannon divergence is computed and period-to-period changes are calculated as in Section 3.3. (ii) In [15], the parameters of the Zipf-Mandelbrot law were used to quantify and visualize diachronic lexical, syntactical, and stylistic changes, as well as aspects of linguistic change for different languages.\n\nBoth studies are based on data from the Google Books Ngram corpora, made available by [30]. It contains yearly token frequencies for each word type for over 8 million books, i.e., 6% of all books ever published [31]. To avoid a potential systematic bias due to strongly changing corpus sizes, random samples of equal size were drawn from the data in both [12] and [15]. However, as demonstrated in Section 3.3, apparently this simplifying assumption is problematic, because it seems to make a difference if we randomly sample N word tokens or if we keep the first N word tokens for the statistical structure of the corresponding word frequency distribution. It is worth pointing out again that, without the \"Litmus test\" the interpretation of the results presented in Section 3.3 would have been completely different, because randomly drawing word tokens from the data does not seem to break the sample size dependence. It is an empirical question whether the results presented in [12,15], and comparable other papers would pass a \"Litmus test\". In light of the results presented in this paper, we are rather skeptical, thus echoing the",
            "score": 0.340807846142792,
            "section_title": "Discussion",
            "char_start_offset": 43668,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 544,
                    "end": 548,
                    "matchedPaperCorpusId": "32823702"
                },
                {
                    "start": 920,
                    "end": 924,
                    "matchedPaperCorpusId": "56096944"
                },
                {
                    "start": 1337,
                    "end": 1341,
                    "matchedPaperCorpusId": "17707301"
                },
                {
                    "start": 1481,
                    "end": 1485,
                    "matchedPaperCorpusId": "32823702"
                },
                {
                    "start": 1490,
                    "end": 1494,
                    "matchedPaperCorpusId": "56096944"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04083251953125
        },
        {
            "corpus_id": "254408575",
            "title": "Montague semantics and modifier consistency measurement in neural language models",
            "text": "Having been designed as a set of measurements for quasi-symbolic analogy, the presented approach is not intended to demonstrate or prove the properties of the distributional models but rather to verify compliance to particular \"desirable\" behaviours. Furthermore, while the Montagovian perspective of compositionality is highly relevant from the symbolic and verification standpoints, other theoretical frameworks can present different constraints regarding word and phrase interpretations and are worthy of exploration.",
            "score": 0.340807846142792,
            "section_title": "Limitations",
            "char_start_offset": 28546,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 520
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06109619140625
        },
        {
            "corpus_id": "256615756",
            "title": "A Benchmark and Scoring Algorithm for Enriching Arabic Synonyms",
            "text": "This experiment compares the results of our algorithm with the average of the linguists' scores (as a baseline) that we presented in section 3.4. Similarly, to understand the 0.27 MAE between the algorithm and the linguists, one can notice that the MAE between the four linguists themselves ranges from 0.12 to 0.16. Both RMSE and MAE, confirm the variation between the algorithm and the average of linguists. This illustrates that the algorithm's scores are close to the linguists' scores. \n\nNevertheless, as noted in section 3.4, the variations between linguists' scores, as well as the algorithm, do not tell us whether a linguist is better or more accurate than the others, which is because synonymy is a subjective notion. However, being close to the linguists' variations is a good indication that the algorithm scores are realistic. Next, we compare the behavior of the algorithm with the linguists' behavior in scoring synonyms, which provides an additional evaluation. \n\nTesting the algorithm's behavior: to further understand the algorithm's behavior, we need to test whether the scores of the algorithm are statistically significant, i.e., the scores were consistent or resulted at random. In other words, we need to test whether the algorithm is consistently giving scores and behaving like a linguist -regardless of the differences in RMSE and MAE. \n\nWe performed a one-way ANOVA test (at p < 0.05) to check if there is a statistical difference between the algorithm and the other linguists. Before conducting this test, we first needed to check if all the linguists' and the algorithm's scores follow a normal distribution, or if there are no outliers, which are the main assumptions to conduct a one-way ANOVA test. Our result of the normality test (using SPSS) indicated that the scores of the algorithm are not normally distributed. Thus, we performed a univariate and multivariate outlier analysis. The results (using SPSS) indicated that there are no outliers, which means that the non-normality of the algorithm's scores are due to skewness in the data and not because of outliers.",
            "score": 0.340807846142792,
            "section_title": "Comparing the Algorithm with the Baseline",
            "char_start_offset": 22376,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 490
                },
                {
                    "start": 493,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 977
                },
                {
                    "start": 980,
                    "end": 1200
                },
                {
                    "start": 1201,
                    "end": 1361
                },
                {
                    "start": 1364,
                    "end": 1504
                },
                {
                    "start": 1505,
                    "end": 1730
                },
                {
                    "start": 1731,
                    "end": 1849
                },
                {
                    "start": 1850,
                    "end": 1916
                },
                {
                    "start": 1917,
                    "end": 2101
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0177154541015625
        },
        {
            "corpus_id": "278208043",
            "title": "Context-Enhanced Contrastive Search for Improved LLM Text Generation",
            "text": "L%@& %M ?'<'= 'N &_P?=QR % => $%@& %M &_P?=QR (10) Measuring Alignment Using Variational Embeddings (MAUVE) is a metric designed to evaluate how closely the distribution of machine-generated text matches that of humanwritten text. It compares the token distributions of both to assess how neural and human-generated text match. MAUVE calculates a divergence score between the two distributions using a statistical method that captures differences in quality and fluency. A higher MAUVE score indicates that the modelgenerated text is more aligned with human-written text in terms of structure, diversity, and fluency. \n\nCoherence measures the semantic connection and consistency between the generated text and the initial prefix text provided to a language model [5]. It evaluates how well the language model maintains the logical flow and meaning throughout the text generation process. It is computed as the average log-likelihood of each token in the generated text, given the context of all prior tokens, using a pre-trained language model such as OPT. This determines how well the generated text aligns semantically with the prompt. Coherence (C) is computed using (11). \n\nIn (11), S V is the generated text, is the prefix text, U S V | , S XV is the probability of the token S V given both the prefix x and the previously generated tokens S XV . The average log-likelihood is taken over the length of the generated text | S|. The coherence metric used in the current work as given by (10) slightly different from the formulation in [5]. The difference here is that the probability distribution is explicitly conditioned on both the prefix x and the full preceding generated tokens S XV , which slightly shifts the focus to emphasize the joint contribution of both the prefix and the past sequence in generating coherent content.",
            "score": 0.34037441515270916,
            "section_title": "= 1 \u2212",
            "char_start_offset": 30218,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 617
                },
                {
                    "start": 620,
                    "end": 767
                },
                {
                    "start": 768,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1175
                },
                {
                    "start": 1178,
                    "end": 1351
                },
                {
                    "start": 1352,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1542
                },
                {
                    "start": 1543,
                    "end": 1834
                }
            ],
            "ref_mentions": [
                {
                    "start": 763,
                    "end": 766,
                    "matchedPaperCorpusId": "253107335"
                },
                {
                    "start": 1538,
                    "end": 1541,
                    "matchedPaperCorpusId": "253107335"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1240234375
        },
        {
            "corpus_id": "270703337",
            "title": "On Giant's Shoulders: Effortless Weak to Strong by Dynamic Logits Fusion",
            "text": "Autoregressive Language Models Modern autoregressive transformers generate continuations for input prompts token by token. Given a prompt x 1:k\u22121 (denoted as x <k ), the model computes the logits for the k-th token, represented as \n\n, where V denotes the size of the vocabulary. A probability distribution P (x k | x <k ) is then obtained through softmax normalization: \n\n). The next token x k is subsequently sampled from this distribution, i.e., x k \u223c P (x k | x <k ). \n\nDistance Between Language Model Outputs Distribution We can utilize the Kullback-Leibler(KL) divergence to measure the similarity of two distributions P and Q generated from two language models (with the same vocabulary), which can be viewed as the distance of the two language models: \n\nIf this is implied by the context, we will omit the conditioning on |x <k and simply use D KL (P ||Q). \n\nLogit Arithmetic Suppose we have two pretrained auto-regressive models with homogeneous architecture and the same vocabulary: a small model with parameter set \u03b8 S and a large model with parameter set \u03b8 L . We aim to fine-tune the small model to obtain \u03b8 S f t and transfer this fine-tuning knowledge to the large models. Previous work [33,41] transferred fine-tuned knowledge to a large model by designing arithmetic between logits, resulting in the output distribution P for the large model as follows: \n\nwhere M L , M S , and M S f t represent the logits of the large model, small model, and fine-tuned small model, respectively. Their corresponding normalized distributions are denoted by P , Q, and Q f t . The detailed theoretical proof supporting this logit arithmetic formula is provided in Appendix B. Here, \u03b1 is a pre-adjusted hyperparameter that controls the extent of knowledge transfer from the small model. Our analysis from Appendix C demonstrates that logit arithmetic attempts to approximate the shift ( \n\n) \u03b1 between the fine-tuned distribution and the pretrained distribution by controlling the parameter \u03b1 before inference.",
            "score": 0.3400727571561071,
            "section_title": "Problem Background",
            "char_start_offset": 9154,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 230
                },
                {
                    "start": 233,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 369
                },
                {
                    "start": 372,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 470
                },
                {
                    "start": 473,
                    "end": 758
                },
                {
                    "start": 761,
                    "end": 863
                },
                {
                    "start": 866,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1369
                },
                {
                    "start": 1372,
                    "end": 1497
                },
                {
                    "start": 1498,
                    "end": 1576
                },
                {
                    "start": 1577,
                    "end": 1785
                },
                {
                    "start": 1786,
                    "end": 1885
                },
                {
                    "start": 1888,
                    "end": 2008
                }
            ],
            "ref_mentions": [
                {
                    "start": 1205,
                    "end": 1208,
                    "matchedPaperCorpusId": "264306063"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07464599609375
        },
        {
            "corpus_id": "256089280",
            "title": "Why Can Computers Understand Natural Language?",
            "text": "onological and morphological levels. Unconcerned with semantics and standing on phonological or morphological features, distribution is necessarily structural distribution. Significantly, we found that, despite a natural lack of technical complexity, the key analytic instruments of Harris's distributionalism are highly convergent with the mechanisms of modern MMs, namely a term-context matrix with dimensionality reduction techniques. However, we saw that the latent space derived through those techniques provides a complete series of formal units structurally related to each other rather than a set of dimensions to measure approximate similarity. Harris's theory was not free of difficulties, nonetheless. Indeed, by making such a clear-cut distinction between semantics and distributional features, Harris precluded the possibility of conceiving a direct relation between those structured formal units and any semantic content. This is what led us to conclude our inquiry by assessing the structuralist theoretical background of Harris's distributionalism.\n\nWe found the answer to that last open issue in an idea stemming from the singular approach to language of European structuralism (i.e. that of Saussure and Hjelmslev), which we called \"the structuralist hypothesis\", namely the idea that meaning is the effect of structure. Such hypothesis is based on the fact that linguistic structure is not the result of the composition of units but of unmotivated decomposition or segmentation of an originally shapeless material continuum. Such segmentation can only be done through the intervention of a second heterogeneous continuum operating a discriminating action on the first one, while receiving from the latter a comparable action in return. Structure is then always double, since it is the result of a process of simultaneous segmentation between two heterogeneous systems or planes, of which one can be thought of as the content or meaning of the other. This is why the analysis of segmentation of one plane-which is what distributional analysis in fact achieves-can capture meaningful features belonging to the other one, carved, as it were, in the form of the former. Moreover, an overview of Hjelmslev's development of Saussure's program suggested the way in which that structuralist conception of language could be related to the basic formal principles of the contemporary models under study.\n\nFrom word2vec's word contexts and implicit matrix factorization to",
            "score": 0.3400109828859318,
            "section_title": "Conclusions",
            "char_start_offset": 180520,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.046630859375
        },
        {
            "corpus_id": "277104066",
            "title": "Cream of the Crop: Harvesting Rich, Scalable and Transferable Multi-Modal Data for Instruction Fine-Tuning",
            "text": "Figure 1. Our proposed mmSSR against the random sampling baseline (3 trials) across both general and specialized multi-modal benchmarks under the 10% (L) and 30% (R) data budgets. \n\nThe quality of data matters in the scaling of large models (Li et al., 2024b;Wettig et al., 2024;Liu et al., 2024b;Lu et al., 2024;Luo et al., 2024;Li et al., 2024a). It is particularly important during their supervised fine-tuning (SFT) stage, where pre-trained models are expected to efficiently and accurately follow user instructions for general purposes or specialized deployment. To achieve this, earlier approaches for large language models (LLMs) filter large-scale SFT datasets with millions of samples towards redundancy reduction (Lee et al., 2022;Elazar et al., 2024), quality control and safety regulation (Joulin, 2016;Penedo et al., 2023;Dubey et al., 2024;Team et al., 2024;Chung et al., 2024). Recently, LIMA introduces the superficial alignment hypothesis (SAH) (Zhou et al., 2024), which utilizes only 1,000 carefully curated samples to illustrate that most LLM knowledge has been acquired during pre-training, requiring only minimal data for instruction fine-tuning, and the effectiveness of these few samples hinges on their quality and diversity. This shift has encouraged subsequent research on automated sample selection, which aims to identify and extract valuable data on these key attributes (Lu et al., 2024;Xia et al., 2024a;Liu et al., 2025), thereby reducing time and computational cost while enhancing interpretability of the target models. However, although the SAH remains valid under the verification of hand-crafted data, recent surveys (Diddee & Ippolito, 2024;Xia et al., 2024b) reveal that automated sample selection methods are susceptible to experimental conditions, including variations in available budgets, different data sources and diverse evaluation benchmarks, which hinders them to get consensus on benchmarks or consistently outperform uniform sampling in generalization.",
            "score": 0.33954507290944486,
            "section_title": "Introduction",
            "char_start_offset": 1848,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 9
                },
                {
                    "start": 10,
                    "end": 179
                },
                {
                    "start": 182,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 1250
                },
                {
                    "start": 1251,
                    "end": 1554
                },
                {
                    "start": 1555,
                    "end": 2003
                }
            ],
            "ref_mentions": [
                {
                    "start": 259,
                    "end": 279,
                    "matchedPaperCorpusId": "267681974"
                },
                {
                    "start": 279,
                    "end": 297,
                    "matchedPaperCorpusId": "266551413"
                },
                {
                    "start": 723,
                    "end": 741,
                    "matchedPaperCorpusId": "235829052"
                },
                {
                    "start": 741,
                    "end": 761,
                    "matchedPaperCorpusId": "264803575"
                },
                {
                    "start": 872,
                    "end": 891,
                    "matchedPaperCorpusId": "253018554"
                },
                {
                    "start": 1418,
                    "end": 1436,
                    "matchedPaperCorpusId": "267522839"
                },
                {
                    "start": 1436,
                    "end": 1453,
                    "matchedPaperCorpusId": "268032188"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.160888671875
        },
        {
            "corpus_id": "55972843",
            "title": "Syllabification and parameter optimisation in Zulu to English machine translation",
            "text": "This also gives rise to a unique and unambiguous analysis in each case-thereby removing the need to address aspects such as disambiguation at this stage. \n\nToken alignment between English and syllabified Zulu (Kotz\u00e9 & Wolff, 2014) showed that semantically significant syllables could be identified in automatic token alignment. This supports our hypothesis that syllabification is not an arbitrary division, but that it can isolate semantically meaningful units, at least to some degree. \n\nThe automatic induction of a morphological analyser is possible with supervised, semi-supervised and unsupervised methods (Spiegler, Gol\u00e9nia, Shalonova, Flach & Tucker, 2008;Quasthoff, Bosch & Goldhahn, 2014). We do not provide a thorough comparison here, but note some differences compared to the simpler syllable-based approach: \n\n\u2022 Supervised systems require an extra step in the form of the construction of training data. This can be expensive in the case of morphological analysis and is not an ideal situation for us. We realise that resource scarceness as applied to Zulu and its related languages does not only apply to corpora and NLP technologies but also to economic resources being spent on the required research and development. By the application of our approach, we therefore hope to alleviate this requirement as well. \n\n\u2022 Being based on machine learning, an automatically induced morphological analyser would have some level of dependence on the domain and style of the training data. The syllable-based approach is inherently domain and genre independent. Such dependence of course exists also for a machine translation system trained on the same data. \n\n\u2022 Depending on the exact approach of the induced morphological analyser, the matter of disambiguating between analyses might remain. Although all analyses can be added to a lattice in the SMT engine, it is not clear how ambiguous analyses for tokens in a sentence would be handled in token alignment. Disambiguation is not required with the syllable-based approach, since only a single output is produced.",
            "score": 0.33920326067237894,
            "section_title": "BACKGROUND",
            "char_start_offset": 10448,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 156,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 487
                },
                {
                    "start": 490,
                    "end": 699
                },
                {
                    "start": 700,
                    "end": 820
                },
                {
                    "start": 823,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 1013
                },
                {
                    "start": 1014,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1324
                },
                {
                    "start": 1327,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1660
                },
                {
                    "start": 1663,
                    "end": 1795
                },
                {
                    "start": 1796,
                    "end": 1963
                },
                {
                    "start": 1964,
                    "end": 2068
                }
            ],
            "ref_mentions": [
                {
                    "start": 612,
                    "end": 664,
                    "matchedPaperCorpusId": "10676815"
                },
                {
                    "start": 664,
                    "end": 698,
                    "matchedPaperCorpusId": "2798476"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0592041015625
        },
        {
            "corpus_id": "273233391",
            "title": "Locally Measuring Cross-lingual Lexical Alignment: A Domain and Word Level Perspective",
            "text": "A multitude of different approaches for computing distributional similarity have been explored in NLP, of which we select a number of representative examples. Distributional metrics can be classified based on whether they employ a joint space for the embeddings for the languages in question, or whether the spaces are trained monolingually and then aligned (Artetxe et al., 2018;Conneau et al., 2017). 2 The latter approaches have been a key facilitator of cross-lingual transfer in NLP and are especially important in low-resource settings. However, for identifying patterns of divergence and convergence in the usage of specific words and domains, this approach is suboptimal. 3 Globally optimal alignment (one that minimizes the distance between the image of one language in the space of another language) may distort the alignment of some words subsets, in the interest of improving the alignment of other, larger word sets. 4  Local alignment, or the extent to which translation pairs like English home and Spanish casa, hold a similar meaning across languages, is a well studied open question in cognitive science (Berlin and Kay, 1991;Majid et al., 2008Majid et al., , 2014;;Youn et al., 2016;Jackson et al., 2019), that had only recently been approached with NLP tools (Thompson et al., 2018(Thompson et al., , 2020)). However, existing metrics are limited to static word embeddings and do not accommodate newer models that support contextualization. \n\nAdditionally, understanding the variability in meaning across languages can provide valuable insight into cultural differences, revealing how various societies conceptualize their unique experiences and worldviews (Qi, 2017;Khalilia et al., 2023;Shioiri et al., 2023;Tjuka et al., 2024).",
            "score": 0.3387726986276929,
            "section_title": "Related Work",
            "char_start_offset": 6773,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1459
                },
                {
                    "start": 1462,
                    "end": 1749
                }
            ],
            "ref_mentions": [
                {
                    "start": 358,
                    "end": 380,
                    "matchedPaperCorpusId": "21728524"
                },
                {
                    "start": 380,
                    "end": 401,
                    "matchedPaperCorpusId": "3470398"
                },
                {
                    "start": 1143,
                    "end": 1161,
                    "matchedPaperCorpusId": "9012438"
                },
                {
                    "start": 1201,
                    "end": 1222,
                    "matchedPaperCorpusId": "209424412"
                },
                {
                    "start": 1278,
                    "end": 1300,
                    "matchedPaperCorpusId": "69899677"
                },
                {
                    "start": 1300,
                    "end": 1326,
                    "matchedPaperCorpusId": "221092263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06671142578125
        },
        {
            "corpus_id": "215754837",
            "title": "Compass-aligned Distributional Embeddings for Studying Semantic Differences across Corpora",
            "text": "The overall goal of the proposed framework is to support language-based studies by domain experts, which explains why simplicity and efficiency are two desired properties. In addition to framing the alignment method developed in our previous work (Di Carlo et al., 2019) into a framework for cross-corpora semantic comparison, in this paper we focus in particular on providing solid evidence about 1) the performance of the approach when compared to other alignment strategies, 2) its cross-domain generalization potential, and 3) its robustness, including a characterization of the conditions under which the alignment is more successful (as a function of cross-corpora vocabulary overlap). \n\nTo provide such evidence, quantitative experiments are conducted in domains where CADE can be compared to previous work because consolidated test data and methodologies are available, e.g., in domains like temporal shift and language localization. To discuss its potential as a general framework to support semantic comparisons, we also discuss more qualitative experiments in domains where hard test data are not available, e.g., in the context of topic-wise comparisons. \n\nAs a summary, CADE provides, to the best of our knowledge, the first approach that can be used to generate comparable distributional models of words independent from the kind of comparison, yet achieving state-of-the-art results in contexts like temporal comparison where several specific approaches have been provided. \n\nThe paper is structured as follows: In Section 2 we describe the related work focusing on approaches that account for temporal alignment. In Section 3 we introduce CADE, describing its main properties and characteristics. Sections 2.2,6,7 describe respectively the experiments on temporal alignment, on language localization and on the robustness of our method. Eventually, we conclude the paper in Section 8, summarizing what we have presented.",
            "score": 0.33851735035607955,
            "section_title": "Summary of Contributions",
            "char_start_offset": 8530,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 691
                },
                {
                    "start": 694,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1166
                },
                {
                    "start": 1169,
                    "end": 1488
                },
                {
                    "start": 1491,
                    "end": 1628
                },
                {
                    "start": 1629,
                    "end": 1712
                },
                {
                    "start": 1713,
                    "end": 1852
                },
                {
                    "start": 1853,
                    "end": 1936
                }
            ],
            "ref_mentions": [
                {
                    "start": 247,
                    "end": 270,
                    "matchedPaperCorpusId": "174803673"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.31005859375
        },
        {
            "corpus_id": "276161664",
            "title": "Hierarchical Contextual Manifold Alignment for Structuring Latent Representations in Large Language Models",
            "text": "The organization of latent token representations plays a crucial role in determining the stability, generalization, and contextual consistency of language models, yet conventional approaches to embedding refinement often rely on parameter modifications that introduce additional computational overhead. A hierarchical alignment method was introduced to restructure token embeddings without altering core model weights, ensuring that representational distributions maintained coherence across different linguistic contexts. Experimental evaluations demonstrated improvements in rare token retrieval, adversarial robustness, and long-range dependency tracking, highlighting the advantages of hierarchical structuring in mitigating inconsistencies in latent space organization. The comparative analysis against conventional fine-tuning and embedding perturbation methods revealed that hierarchical restructuring maintained computational efficiency while achieving measurable gains in representation quality. Structural refinements introduced through the alignment process resulted in improved contextual stability across varied linguistic tasks, reducing inconsistencies in token proximity relationships and enhancing interpretability in language generation. A detailed computational assessment confirmed that the realignment process introduced minimal inference overhead, ensuring that representational improvements did not compromise model efficiency. The findings reinforced the broader significance of structured representation learning, illustrating that hierarchical embedding modifications could serve as an effective strategy for refining latent space distributions while preserving pre-learned semantic associations.",
            "score": 0.3382681265095552,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51513671875
        },
        {
            "corpus_id": "278714748",
            "title": "SoLoPO: Unlocking Long-Context Capabilities in LLMs via Short-to-Long Preference Optimization",
            "text": "We follow the strategy proposed by RULER [29] to generate synthetic contexts based on the MuSiQue dataset. Specifically, given an original sample (q, a, D supporting ), where q and a denote the question and ground-truth answer respectively, and D supporting is the set of supporting evidence documents, we synthesize a context of target length L as follows: we randomly sample a set of irrelevant documents D irrelevant , such that the total token count of D supporting \u222a D irrelevant approximates L. The token count is calculated using the tokenizer of the corresponding instruct model. In this setup, both short and long contexts contain the necessary information to answer the question; however, the long contexts include more distractor content, thereby simulating the redundancy in natural language. \n\nPreference Pairs Construction To evaluate the effectiveness of short-to-long alignment compared with original long-context alignment, we construct preference pairs based on both short and long contexts using instruct models. Specifically, for each input context and question, we first generate N = 32 Chain-of-Thought responses using a sampling temperature of 0.85 to encourage response diversity. We then apply the sub-em method to identify the chosen and rejected responses within the generated samples. A final preference pair is formed by randomly selecting one from the chosen and the rejected candidates, respectively. Samples where all responses are correct/incorrect are discarded. \n\nTo enhance the validity of the comparative analysis, we prioritize constructing training examples from the intersection of samples that can be successfully constructed using both short and long contexts. This ensures that differences in model performance stem primarily from alignment strategies rather than data distribution. The final size of each train set is 5000, as detials shown in Table 2 and an example is shown in Table 6. The prompt template used for data construction is illustrated in Figure 6. Table 6: An example preference pair sampled from Qwen2.5-7B-Instruct using the short context (x short ). Certain reasoning details have been omitted and denoted by \". . . \" for conciseness. \n\nquestion When was the institute that owned The Collegian founded?",
            "score": 0.3381309928432313,
            "section_title": "Context Synthesis",
            "char_start_offset": 32380,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 804
                },
                {
                    "start": 807,
                    "end": 1031
                },
                {
                    "start": 1032,
                    "end": 1204
                },
                {
                    "start": 1205,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1496
                },
                {
                    "start": 1499,
                    "end": 1702
                },
                {
                    "start": 1703,
                    "end": 1825
                },
                {
                    "start": 1826,
                    "end": 1931
                },
                {
                    "start": 1932,
                    "end": 2006
                },
                {
                    "start": 2007,
                    "end": 2111
                },
                {
                    "start": 2112,
                    "end": 2177
                },
                {
                    "start": 2178,
                    "end": 2196
                },
                {
                    "start": 2199,
                    "end": 2264
                }
            ],
            "ref_mentions": [
                {
                    "start": 41,
                    "end": 45,
                    "matchedPaperCorpusId": "269032933"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0352783203125
        },
        {
            "corpus_id": "3933963",
            "title": "An Evaluation and Comparison of Linguistic Alignment Measures",
            "text": "Can they describe the individual differences in propensity of alignment? Essentially, are they good/reliable measures? These questions are not answered (or fully answered) yet. \n\nTo answer these questions in this study, we first conduct an evaluation of the intrinsic properties of three well defined and commonly used measures, indiscriminate local linguistic alignment (LLA) (Fusaroli et al., 2012;Wang, Reitter, and Yen, 2014), Spearman's correlation coefficient (SCC) (Huffaker et al., 2006;Kilgarriff, 2001), and repetition decay (RepDecay) (Reitter, Keller, and Moore, 2006), in which two basic properties are investigated, normality of distribution and sensitivity. Then we apply these measures to a study about the IAM and individual differences in alignment propensity as an extrinsic evaluation. We examine how well they follow the basic assumption of IAM, i.e., showing correlations between alignment at lexical and syntactic levels, and how well they can reveal the individual differences in alignment propensity. \n\nOur study aims to provide potential guidance to future studies of linguistic alignment in terms of which computational measures to use. Basically, we favor a measure that has good normality in its distribution, that has higher sensitivity, and that conforms with the IAM theory and the existing findings about individual differences in alignment propensity.",
            "score": 0.33748140519510234,
            "section_title": "Introduction",
            "char_start_offset": 2190,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 72
                },
                {
                    "start": 73,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 176
                },
                {
                    "start": 179,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 1025
                },
                {
                    "start": 1028,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1385
                }
            ],
            "ref_mentions": [
                {
                    "start": 472,
                    "end": 495,
                    "matchedPaperCorpusId": "2152252"
                },
                {
                    "start": 495,
                    "end": 512,
                    "matchedPaperCorpusId": "167296"
                },
                {
                    "start": 546,
                    "end": 580,
                    "matchedPaperCorpusId": "593749"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.092041015625
        },
        {
            "corpus_id": "263933639",
            "title": "CLARITY: comparing heterogeneous data using dissimilarity",
            "text": "hypothesis-testing procedure based on resampling. In the figure, significant persistences are marked by a brighter colour and a larger rectangle. They only concern one direction of prediction,  royalsocietypublishing.org/journal/rsos R. Soc. Open Sci. 8: 202182\n\nnamely Phon from Lex, and only some groups of languages; the most affected ones are the Slavic and Scandinavian subfamilies. The differences are also scientifically significant, which we determined by analysing the residuals at individual k, shown in electronic supplementary material, figure S2: outstanding residuals remained of the order of 0.1-0.2 s.d. (computed from all similarities in the matrix) in individual cells even at the highest ks. We believe this is a moderate, yet considerable difference. Our additional checks also included establishing that CLARITY decomposition captures signal rather than noise up to the maximal k (electronic supplementary material, figure S3); checking whether self-similarity affects inference (electronic supplementary material, figure S4) examining significances at a stricter p = 0.01 (electronic supplementary material, figure S5) and examining the persistence curves for the resampled matrices to make sure there were no anomalies (electronic supplementary material, figures S6 and S7). We conclude that the effect CLARITY finds, captured in the visual summary in figure 7c,d, is a real one.\n\nThis result obtained by CLARITY is striking because it is based on very subtle distinctions in the observed data. To the bare eye, the Phonetic and Lexical distance matrices figure 7a,b are quite similar, because the two sets of features are not independent of each other. Despite high correlation, CLARITY allowed us to discover a clear difference between the two processes of language change.\n\nHow should we interpret this finding? In terms of the informal representation above, our results about Lex and Phon indicates that the real-world quantities TrueLex on the one hand, and unrelated.phon.sim and cognate.phon.sim on the other, are affected by subtly different historical        royalsocietypublishing.org/journal/rsos R. Soc. Open Sci. 8: 202182\n\nprocesses. Given that Phon depends on a superset of real-world quantities that Lex depends on,",
            "score": 0.33748140519510234,
            "section_title": "Two types of language change",
            "char_start_offset": 36349,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.046539306640625
        },
        {
            "corpus_id": "202702566",
            "title": "A non-parametric significance test to compare corpora",
            "text": "As argued above, classical null hypothesis significance tests are based on a number of assumptions that are (i) in almost all cases violated in corpus linguistics and (ii) in some cases, like in the examples presented above, the purpose of a test is not to extend the quantities found in one corpus to the language it seeks to represent, but to determine whether a result is pronounced enough to warrant the conclusion that it is substantial and not due to chance. Regarding (ii), I demonstrate in this section that permutation tests can be used in such situations. \n\nCompared to classical significance null hypothesis tests, permutation tests are a class of non-parametric tests [62,64]. Or put differently, while classical tests assume that under the null hypothesis the investigated data follow a specific distribution (e.g. independent, normally and identically distributed errors), permutation tests use the data itself to compute the distribution of the test statistic under the null hypothesis that is spelled out explicitly. It is important to point out that permutation testing is not meant as a substitute for the construction of plausible statistical models for the observed data, but offers a non-parametric way to directly compute the reference distribution of any test statistic from which the p-values can be obtained. As such, permutation tests can be understood as a procedure for accepting or rejecting hypotheses based on statistical models with various degrees of complexity. To outline the procedure for corpus linguistic data, a simple but important example is presented in this paper, the comparison of word frequencies. For an application with linguistic data that tests hypotheses that are based on linear effects models, see [65]. A standard comprehensive textbook reference is [66]. \n\nThe development of permutation tests can be traced back to Fisher [67] and Pitman [68]. The following interesting quote from Fisher [69], (one of) the founding father(s) of modern statistics (brought to my attention by reading [62]) illustrates the most basic idea of permutation testing: \"Let us suppose, for example, that we have measurements of the stature of a hundred Englishmen and a hundred Frenchmen. It may be that the first group are, on the average, an inch taller than the second, although the two sets of heights will overlap widely. [. .",
            "score": 0.33748140519510234,
            "section_title": "The permutation test",
            "char_start_offset": 18131,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 565
                },
                {
                    "start": 568,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1495
                },
                {
                    "start": 1496,
                    "end": 1643
                },
                {
                    "start": 1644,
                    "end": 1756
                },
                {
                    "start": 1757,
                    "end": 1809
                },
                {
                    "start": 1812,
                    "end": 1899
                },
                {
                    "start": 1900,
                    "end": 2220
                },
                {
                    "start": 2221,
                    "end": 2358
                },
                {
                    "start": 2359,
                    "end": 2363
                }
            ],
            "ref_mentions": [
                {
                    "start": 684,
                    "end": 687,
                    "matchedPaperCorpusId": "11349921"
                },
                {
                    "start": 1894,
                    "end": 1898,
                    "matchedPaperCorpusId": "125115385"
                },
                {
                    "start": 1944,
                    "end": 1948,
                    "matchedPaperCorpusId": "147599285"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.043121337890625
        },
        {
            "corpus_id": "2565824",
            "title": "A Graph-theoretic Model of Lexical Syntactic Acquisition",
            "text": ".\n\nSecond, exemplar-theoretic similarity is best defined at the smallest possible scale in order to allow optimal matching between parts of the stimulus and parts of memory. In phonetics, we use a time scale of 10s of milliseconds or even less. Conceivably, one could also use segments (e.g., consonants and vowels) as the smallest unit; however, this would presume a segmented signal. And segmentation is part of the perception task we want to explain in the first place.\n\nSeparating left and right neighbors -which amounts to looking at left and right local contexts of each word separately -is the smallest scale we can operate at when doing syntactic matching. We choose this small scale for the same reasons as we choose a small scale in phonetics: to ensure maximum flexibility when matching parts of the stimulus with exemplars in memory. Using words, bigrams or larger units would reduce the flexibility in matching and require a larger amount of experience (or training data) to learn a particular generalization.\n\nWe refer to the representations of left and right contexts of a given word as half-words. In other words, we split a word into two entities, a left halfword that characterizes its behavior to the left and a right half-word that characterizes its behavior to the right. Thus left-context and right-context components of the representation of a given focus word are defined, where a left (right) half-word consists of a probability distribution over all words that occur to the left (right) of the focus word and the dimensionality of the vector for each word is dependent on the number of distinct neighbors (left and right). For example, having experienced take doll twice and drop doll once, then the left context distribution, or left half-word of doll, doll l , is P (take) = 2/3, P (drop) = 1/3. By extension, the phrase take the doll is represented as the following six half-words: take l , take r , the l , the r , doll l , and doll r .\n\nDistance measure. The basic intuition behind local syntactic coherence is that an important component of syntactic wellformedness -and a component that is of particular importance in acquisition -is whether a similar sequence",
            "score": 0.33748140519510234,
            "section_title": "Local syntactic coherence",
            "char_start_offset": 6830,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05780029296875
        },
        {
            "corpus_id": "265277808",
            "title": "Non-Fluent Synthetic Target-Language Data Improve Neural Machine Translation",
            "text": "In the context of unsupervised NMT, the noisy input sentences to denoising autoencoders are generated by random token swaps [20]. \n\nFocusing on approaches that modify word order in the context of DA, it is worth highlighting a self-translation approach using a right-to-left decoder [54], which is similar to inverting the order of the target words. However, unlike MaTiLDA, this last approach needs to generate translations from the model during training. \n\nReplacing tokens with placeholders (as we do in unk) has already been applied to the source language [55] in combination with two self-supervised learning objectives for detecting replaced and dropped tokens. Xie et al. [21] also evaluate the impact of random replacements of words in the source and target sides of the training samples by either a random word from the vocabulary, or by a blank. \n\nGao et al. [29] replace source-side words selected at random with soft words whose representations are obtained from the probability distribution provided by a language model. Fadaee et al. [26] replace some words in their training samples by infrequent words in order to improve the performance of the NMT model when dealing with them at translation time. Words to be replaced are identified using a large source language model. Once the source words to be replaced are identified, a word-alignment model and a probabilistic dictionary are used to also replace the corresponding counterpart by the most probable translation of the new source word. In MaTiLDA, the replace transformation, which is similar, does not require any language model. \n\nAs regards the special token we use to prevent negative transfer between tasks, a similar strategy [56] has been applied to identify synthetic samples when combining actual parallel data and back-translated data for training. Yang et al. [57] extends this last work by including forwardtranslated data for training using two different special tokens to distinguish the two types of synthetic data. Another strategy that has been reported to be effective to combine synthetic and original training instances is the AugMix method [58], initially defined in the context of image processing.",
            "score": 0.33735886916686336,
            "section_title": "RELATED WORK",
            "char_start_offset": 40972,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 132,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 456
                },
                {
                    "start": 459,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 855
                },
                {
                    "start": 858,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1601
                },
                {
                    "start": 1604,
                    "end": 1829
                },
                {
                    "start": 1830,
                    "end": 2001
                },
                {
                    "start": 2002,
                    "end": 2191
                }
            ],
            "ref_mentions": [
                {
                    "start": 124,
                    "end": 128,
                    "matchedPaperCorpusId": "3515219"
                },
                {
                    "start": 283,
                    "end": 287,
                    "matchedPaperCorpusId": "52091536"
                },
                {
                    "start": 560,
                    "end": 564,
                    "matchedPaperCorpusId": "224818089"
                },
                {
                    "start": 869,
                    "end": 873,
                    "matchedPaperCorpusId": "166228668"
                },
                {
                    "start": 1048,
                    "end": 1052,
                    "matchedPaperCorpusId": "3291104"
                },
                {
                    "start": 1703,
                    "end": 1707,
                    "matchedPaperCorpusId": "189928248"
                },
                {
                    "start": 1842,
                    "end": 1846,
                    "matchedPaperCorpusId": "67790204"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03955078125
        },
        {
            "corpus_id": "269214026",
            "title": "Token-level Direct Preference Optimization",
            "text": "In this work, we introduced Token-level Direct Preference Optimization (TDPO), an innovative token-level fine-tuning approach for Large Language Models (LLMs) aimed at aligning more closely with human preferences. By employing the token-wise optimization with forward KL divergence constraints and converting the Bradley-Terry model into a token-level preference model, TDPO addresses key challenges in divergence efficiency and content diversity, surpassing traditional methods like Direct Preference Optimization (DPO) and PPO-based RLHF in tasks such as controlled sentiment generation and single-turn dialogues. This marks a substantial advancement in LLM training methodologies, demonstrating the potential of token-level optimization to enhance the alignment, quality, and diversity of LLM outputs, setting a new direction for AI alignment research and the development of nuanced, human-aligned AI systems. \n\nRegarding the future prospects of alignment methodologies, we anticipate that iterative refinement approaches and multiturn conversational alignment strategies will significantly improve the alignment of large language models with human values. By continuously refining these models, we can achieve more precise alignment with complex human preferences. Moreover, multi-turn conversations enable deeper and more nuanced interactions, fostering comprehensive attunement to human intentions. These approaches aim to enhance the quality and relevance of AI responses, making AI systems more harmonized with human values and expectations.",
            "score": 0.3370156900505189,
            "section_title": "Conclusion",
            "char_start_offset": 27439,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 912
                },
                {
                    "start": 915,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1404
                },
                {
                    "start": 1405,
                    "end": 1549
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.20751953125
        },
        {
            "corpus_id": "270123077",
            "title": "Is In-Context Learning Sufficient for Instruction Following in LLMs?",
            "text": "The large-scale pre-training phase allows Large Language Models (LLMs) to acquire extensive knowledge and capabilities (Bubeck et al., 2023). However, these base models struggle to follow instructions directly from prompts, necessitating further fine-tuning to be used as conversational models. Traditional alignment methods include Supervised Fine-Tuning (SFT) on instruction datasets (IFT, Wei et al., 2021;Chung et al., 2022) or preference data (DPO, KTO, IPO, and ORPO, Rafailov et al., 2023;Ethayarajh et al., 2024;Azar et al., 2024;Hong et al., 2024), and reinforcement learning (RLHF and RLAIF, Ouyang et al., 2022;Bai et al., 2022). These approaches typically involve multiple rounds of refinement of the instructed model (Touvron et al., 2023), i.e. high computational cost, and need a large amount of annotated data like human preferences, which might be difficult to collect. However, a line of work (Taori et al., 2023;Chen et al., 2023;Zhou et al., 2023;Lee et al., 2023;Zhao et al., 2024;Kaur et al., 2024) has suggested that IFT on a small amount of high quality instructionfollowing examples, even only 1000, can be sufficient to match the performance of more complex alignment mechanisms. In particular, Zhou et al. (2023) introduced the Superficial Alignment Hypothesis, stating that LLMs acquire all their capabilities during pre-training, and fine-tuning only allows the models to better access such knowledge when interacting with users (Gudibande et al., 2023;Duan et al., 2023). Using such small instruction datasets for IFT has the clear advantage of significantly reducing the cost of model alignment.",
            "score": 0.3369953324700118,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 1205
                },
                {
                    "start": 1206,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1626
                }
            ],
            "ref_mentions": [
                {
                    "start": 520,
                    "end": 538,
                    "matchedPaperCorpusId": "264288854"
                },
                {
                    "start": 949,
                    "end": 967,
                    "matchedPaperCorpusId": "258822910"
                },
                {
                    "start": 984,
                    "end": 1002,
                    "matchedPaperCorpusId": "267522812"
                },
                {
                    "start": 1221,
                    "end": 1239,
                    "matchedPaperCorpusId": "258822910"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.327880859375
        },
        {
            "corpus_id": "223690462",
            "title": "Assessing Keyness using Permutation Tests",
            "text": "In this paper, we consider the keyness problem, namely, assessing whether some words appear significantly more often in one corpus A than in another corpus B. Most existing approaches are based on statistical hypothesis tests, i.e. differences in frequencies of a word between two corpora A and B are judged to be significant if they are so large that they would be very unlikely under a random sampling model. Examples of these approaches are the well-known Log-Likelihood-Ratio-Test, \u03c7 2 -tests and Fisher's Exact Test. Other measures like the so-called Log-Ratio are also sometimes used, but these do not directly take into account random variation in the sampling process and hence tend to give very high scores to differences in very rare words which might well be due to chance. \n\nApproaches based on statistical hypothesis tests are necessarily tied to some assumption of randomness. All methods currently used -as far as they are based on hypothesis testing -are justified under the assumption that the corpora A and B are samples of larger populations, say P op A and P op B . These are typically not larger corpora of actually existing texts from which a random selection was taken but some form of abstract infinite populations like \"all texts that could have been produced by some author\", \"all texts that could have been published in a given newspaper in a given year\", \"the discourse on. . . \", \"actual use of language in certain media\" etc. This is not a special feature of linguistics but very common in other applications of statistics, as we typically view a sample of e.g. patients with a certain type of disease not as a subset of all people who currently have the desease but also representative of patients that will develop the illness in the future, some of whom might not even be born yet. Hence, assuming the corpus is a random sample from some larger (abstract) population is not a problem per se. \n\nThe problem is rather the specific random sampling mechanism that is assumed. Here, we follow an argument also put forward in Gries (2006Gries ( , 2022)); see also Evert (2006) for a discussion of randomness and different sampling models.",
            "score": 0.33695652606524074,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 784
                },
                {
                    "start": 787,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1405
                },
                {
                    "start": 1406,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1591
                },
                {
                    "start": 1592,
                    "end": 1814
                },
                {
                    "start": 1815,
                    "end": 1924
                },
                {
                    "start": 1927,
                    "end": 2004
                },
                {
                    "start": 2005,
                    "end": 2165
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.021484375
        },
        {
            "corpus_id": "256105748",
            "title": "ProKD: An Unsupervised Prototypical Knowledge Distillation Network for Zero-Resource Cross-Lingual Named Entity Recognition",
            "text": "different classes far away across languages, we leverage classical contrastive learning (Chen et al. 2020) to adjust the distance among class prototypes. Thus the classlevel representation alignment between the source and target languages is achieved.\n\nFurthermore, we present a prototypical self-training method to enhance the student network's ability to acquire the target language-specific knowledge. In particular, we establish pseudo-hard labels for unlabeled target samples based on their softmax-valued relative distances, i.e., prototype probability, to all prototypes and then retrain the network using these pseudo-labels. Since the prototypes accurately represent the clustering distribution underlying the data, the prototypical self-training enables the student network to learn the intrinsic structure of the target language , thus revealing language-specific knowledge, such as the token's label preference. In addition, while calculating the pseudo-hard labels, the class distribution probabilities generated by the teacher network are incorporated into the prototype probabilities to improve the quality of the pseudo-hard labels and facilitate self-training.\n\nSummarily, we make four contributions: (1) We propose a ProKD model for zero-resource cross-lingual NER task, which can improve the model's generalization to the target language .\n\n(2) We propose a contrastive learning-based prototype alignment method to enhance the teacher network's ability to acquire language-independent knowledge. (3) We propose a prototypical self-training method to enhance the student network's ability to acquire target language-specific knowledge. (4) Experimental results on six target languages validate the effectiveness of our approach.",
            "score": 0.33676406411092297,
            "section_title": "Introduction",
            "char_start_offset": 4096,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 88,
                    "end": 106,
                    "matchedPaperCorpusId": "208637033"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.097412109375
        },
        {
            "corpus_id": "268201804",
            "title": "Ask Your Distribution Shift if Pre-Training is Right for You",
            "text": "The characterizations relevant in this work, in-support shift and out-of-support shift, overlap with many existing definitions. Ye et al. [YLB+22] introduce notions of correlation shift and diversity shift (closely aligned with in-support and out-of-support shifts, respectively) and provide a method for measuring the \"amount\" of each type of shift in a given distribution shift (similar to our method for dividing a distribution shift into in-support and out-of-support splits). Subpopulation shift (and its sub-types), shifts involving spurious correlations, covariate shift, and label shift are typically in-support. However, there are exceptions; for example, some works consider subpopulation shifts in which a subpopulation does not appear in the reference distribution [STM21; YZK+23], which are out-of-support. Domain generalization problems are nearly always out-of-support and extrapolating effectively outside of the reference distribution is often a key challenge of these tasks. D.5 Understanding the robustness of pre-trained language models to spurious correlations Tu et al. [TLG+20] study the robustness of pre-trained language models to distribution shifts with spurious correlations. Their central finding is that pre-training can improve performance on shifted datasets in which spurious correlations do not hold. They illustrate that this is because pre-trained models can generalize better from the small number of counterexamples to these correlations in the reference dataset. This is a similar phenomenon to our observation from Figure C.2a: pre-training can provide some effective robustness on in-support shifts that are \"close\" to an out-of-support shift. In cases such as those discussed by Tu et al. \n\n[TLG+20], we hypothesize that pre-training can help to a limited extent by extrapolating better, but cannot mitigating the underlying failure mode of dataset biases.",
            "score": 0.335061538072028,
            "section_title": "D.4 Relating in-support and out-of-support shifts to existing characterizations",
            "char_start_offset": 63914,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1334
                },
                {
                    "start": 1335,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1684
                },
                {
                    "start": 1685,
                    "end": 1730
                },
                {
                    "start": 1733,
                    "end": 1898
                }
            ],
            "ref_mentions": [
                {
                    "start": 138,
                    "end": 146,
                    "matchedPaperCorpusId": "247839962"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.159423828125
        },
        {
            "corpus_id": "271269974",
            "title": "BinaryAlign: Word Alignment as Binary Sequence Labeling",
            "text": "Word alignment refers to the task of uncovering word correspondences between translated text pairs.The automatic prediction of word alignments dates back to the earliest work in machine translation with the IBM models (Brown et al., 1993) where they were used as hidden variables that permit the use of direct token to token translation probabilities.While state of the art machine translation techniques have largely abandoned the use of word alignment as an explicit task (Li, 2022) other use cases for alignments have emerged including lexical constraint incorporation (Chen et al., 2021b), analysing and evaluating translation models (Bau et al., 2018;Neubig et al., 2019), and cross-lingual language pre-training (Chi et al., 2021b).\n\nIn many real-world applications word alignment must be performed across several languages, often including languages with manually annotated word alignment data and others lacking such annotations.We refer to those languages as high Figure 1: Example of alignment of an approximate translation, as often encountered in real-world applications.Links in red indicate situations where one word is aligned with several contiguous or non-contiguous words.The green line represent a situation where a word is untranslated which happens in many language pairs.and low-resource languages respectively.While word alignment for high-resource languages can be learned in a few-shot or fully supervised setting depending on the amount of data, for low-resource languages zero-shot learning strategies must be employed due to data scarcity.\n\nState-of-the-art supervised techniques formalize the task of word alignment as a collection of SQuAD-style span prediction problems (Nagata et al., 2020;Wu et al., 2023) while in zero-shot settings the best performing methods induce word alignment from the contextualized word embeddings of mulitingual pre-trained language models (mPLMs) (Jalili Sabet et al., 2020;Dou and Neubig, 2021;Wang et al., 2022).From a practical perspective, this discrepancy in the preferred method adds complexity to the deployment of word alignment models in real-world applications where both high and low-resource languages must be supported.",
            "score": 0.334501337992614,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 99,
                    "end": 351
                },
                {
                    "start": 351,
                    "end": 738
                },
                {
                    "start": 740,
                    "end": 937
                },
                {
                    "start": 937,
                    "end": 1083
                },
                {
                    "start": 1083,
                    "end": 1190
                },
                {
                    "start": 1190,
                    "end": 1293
                },
                {
                    "start": 1293,
                    "end": 1333
                },
                {
                    "start": 1333,
                    "end": 1567
                },
                {
                    "start": 1569,
                    "end": 1975
                },
                {
                    "start": 1975,
                    "end": 2193
                }
            ],
            "ref_mentions": [
                {
                    "start": 572,
                    "end": 592,
                    "matchedPaperCorpusId": "235349236"
                },
                {
                    "start": 718,
                    "end": 737,
                    "matchedPaperCorpusId": "235417503"
                },
                {
                    "start": 1701,
                    "end": 1722,
                    "matchedPaperCorpusId": "216915063"
                },
                {
                    "start": 1722,
                    "end": 1738,
                    "matchedPaperCorpusId": "259129847"
                },
                {
                    "start": 1908,
                    "end": 1935,
                    "matchedPaperCorpusId": "215827461"
                },
                {
                    "start": 1935,
                    "end": 1956,
                    "matchedPaperCorpusId": "231648372"
                },
                {
                    "start": 1956,
                    "end": 1974,
                    "matchedPaperCorpusId": "256390411"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06256103515625
        },
        {
            "corpus_id": "273482431",
            "title": "MetaAlign: Align Large Language Models with Diverse Preferences during Inference Time",
            "text": "Large Language Models (LLMs) acquire extensive knowledge and remarkable abilities from extensive text corpora, making them powerful tools for various applications. To make LLMs more usable, aligning them with human preferences is essential. Existing alignment techniques, such as Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO), typically embed predefined preferences directly within the model's parameters. These methods, however, often result in a static alignment that can not account for the diversity of human preferences in practical applications. In response to this challenge, we propose an effective method, \\textbf{MetaAlign}, which aims to help LLMs dynamically align with various explicit or implicit preferences specified at inference time. Experimental results show that LLMs optimized on our meticulously constructed MetaAlign Dataset can effectively align with any preferences specified at the inference stage, validating the feasibility of MetaAlign. We hope that our work can provide some insights into the alignment of language models.",
            "score": 0.33424880314381894,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1322021484375
        },
        {
            "corpus_id": "233241202",
            "title": "Syntactic Perturbations Reveal Representational Correlates of Hierarchical Phrase Structure in Pretrained Language Models",
            "text": "In this work we consider three different types of sentence perturbations designed to probe for different phenomena.\n\nn-gram shuffling In the n-gram shuffling experiments, we randomly shuffle the words of a sentence in units of n-grams, with n varying from 1 (i.e., individual words) to 7 (see Fig. 2a for an example). While the number of words which change absolute position is similar for different n, larger n will better preserve the local context (i.e., relative position) of more words. Thus, we reason that n-gram swaps affect the representations selective to the context with size n or higher within the sentence, and that lower n will result in greater distortion in sentence representations.\n\nPhrase swaps The n-gram shuffling experiments probe for sensitivity of representations to local context without taking into account syntactic structure. In the phrase swap experiments, we perturb a sentence by swapping two randomly chosen spans. We explore two ways of swapping spans. In the first setting, the spans are chosen such that they are valid phrases according to its parse tree. 3 In the second setting, the spans are chosen that they are invalid phrases. Importantly, in the second, control setting, we fix the length of the spans such that the lengths of spans that are chosen to be swapped are the same as in the first setting (see Fig. 3a for an example). We hypothesize that swapping invalid phrases will result in more distortion than swapping valid phrases, since invalid swaps will result in greater denigration of syntactic structure.\n\nAdjacent word swaps In the adjacent word swapping experiments, we swap two adjacent words in a sentence. We again experiment with two settings -in the first setting, the swapped words stay within the phrase boundary (i.e., the two words share the same parent), while in the second setting, the swapped words cross phrase boundaries. We also perform a more fine-grained analysis where 3 We use constituency parse trees from the English Penn Treebank (Marcus et al., 1994). we condition the swaps based on the \"syntactic distance\" between the swapped words, where syntactic distance is defined as the distance between the two words in the parse tree (see Fig. 6c). Since a phrase",
            "score": 0.33418562268932683,
            "section_title": "Sentence perturbations",
            "char_start_offset": 5867,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.122314453125
        },
        {
            "corpus_id": "236772624",
            "title": "Geolocation differences of language use in urban areas",
            "text": "In this section we show the results of our distribution calculations and comparisons. The primary goal is to demonstrate the ability to measure micro-scale patterns in language use and to show that these patterns can be correlated with highly localized phenomena that drive the variations. This demonstration is done by measuring patterns of variation from the noise and signal sources identified in section 2.4.",
            "score": 0.33418562268932683,
            "section_title": "3.Results",
            "char_start_offset": 21788,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 85
                },
                {
                    "start": 86,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 412
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05126953125
        },
        {
            "corpus_id": "6570837",
            "title": "Word Alignment Baselines",
            "text": "Short words such as stop words tend to align with short words and long words such as names tend to align with long words. This weak hypothesis is worth pursuit because a similar hypothesis was useful for aligning sen- tences (Gale and Church, 1991;Brown et al., 1991). The observation can be codified as a distance between the word at position i on the LHS and the word at position j on the RHS \n\nwhere L(l i ) is the length of the token at position i on the LHS. Note that D len is similar to a normalized harmonic mean, ranging from 0 to 1.0, with the minimum achieved when the lengths are the same. A threshold on D len is used to turn this distance metric into a classification rule.",
            "score": 0.33418562268932683,
            "section_title": "Length ratios",
            "char_start_offset": 2824,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 394
                },
                {
                    "start": 397,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 687
                }
            ],
            "ref_mentions": [
                {
                    "start": 225,
                    "end": 248,
                    "matchedPaperCorpusId": "519954"
                },
                {
                    "start": 248,
                    "end": 267,
                    "matchedPaperCorpusId": "813825"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0634765625
        },
        {
            "corpus_id": "52291335",
            "title": "Interactional Stancetaking in Online Forums",
            "text": "First, we count the number of utterance pairs that transition from one level to another or remain at the same level (high (H) \u2192 low (L), high (H) \u2192 neutral (N), etc). To compare with the chance counts under the null hypothesis, we again perform 10,000 random reassignments of class levels within each thread, and compute empirical p-values under this sampling distribution. \n\nTransition counts and comparison with chance counts are shown in Figure 4, where a positive superscript indicates observed transition counts being significantly greater than chance counts and a negative superscript indicates the opposite. As shown in Figure 4, the L \u2192 L transition is significantly higher than random chance for ALIGNMENT. L \u2192 L transitions for ALIGNMENT contribute to the stickiness observation in RQ1a. Further, the significantly higher number of N \u2192 H transitions indicate the possibility of moving upward from neutral ALIGNMENT to high ALIGN-MENT (e.g., question/answer pairs where the answer utterances are annotated as high ALIGNMENT), but these are counterbalanced by significantly fewer transitions from",
            "score": 0.33418562268932683,
            "section_title": "RQ1a:",
            "char_start_offset": 35767,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 373
                },
                {
                    "start": 376,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 1104
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06890869140625
        },
        {
            "corpus_id": "266755997",
            "title": "LLaMA Pro: Progressive LLaMA with Block Expansion",
            "text": "LLaMA-PRO-8B \n\nUnshifted (92.6%) Marginal (6.8%) Shifted (0.7%) '\\n', '\u2022', 'the', 'The','3','8','C','Y','S','1','a','4','9','2','.','This','in','In','#',',','A','can','size','6','of','7','o','and','P','F','Frank','you','find','p','$','Le','5','B','(','0','f',':','for', 'Am', 'I', 'c', 'av', 'g', \u2026 \n\nFigure 6: Token distribution shift after block expansion compared to the initial LLaMA-2-7B. The proportions of unshifted, marginally shifted, and significantly shifted tokens are color-coded and presented as percentages. Frequently shifted tokens are displayed below. \n\nblocks (Figure 5). The loss consistently decreases during training, regardless of the number of added blocks, and decreases more rapidly with larger models. These findings indicate that our method demonstrates strong scalability with larger models and more data. \n\nHowever, a lower overall training loss does not necessarily guarantee superior performance on domain-specific tasks. Therefore, we evaluate models of different sizes on both general language tasks and Unfair-ToS, as shown in Table 5. All the expanded models effectively preserve the general capabilities of the initial model. For the domainspecific task, larger models achieve better performance. We find that adding eight blocks provides optimal performance with minimal cost compared to larger models, hence we adopt this as our default strategy. The performance of MoE is comparable to our method with four added blocks. Figure 9 illustrates the differences between traditional training strategies such as fine-tuning and LoRA, and our proposed method.",
            "score": 0.3340689556674932,
            "section_title": "LLaMA-2-7B",
            "char_start_offset": 18262,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 12
                },
                {
                    "start": 15,
                    "end": 32
                },
                {
                    "start": 33,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 298
                },
                {
                    "start": 301,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 569
                },
                {
                    "start": 572,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 834
                },
                {
                    "start": 837,
                    "end": 953
                },
                {
                    "start": 954,
                    "end": 1070
                },
                {
                    "start": 1071,
                    "end": 1162
                },
                {
                    "start": 1163,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1592
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.13818359375
        },
        {
            "corpus_id": "258557914",
            "title": "Token-level Fitting Issues of Seq2seq Models",
            "text": "This paper hypothesizes that seq2seq models have token-level overfitting and underfitting issues, and provides direct evidence to support the hypothesis in various settings, raising a valuable problem for NLP modeling. However, this paper does not provide a solution to the problem due to the theoretical and practical challenges of measuring the convergence speed of each token. We leave the exploration of this topic to future work.",
            "score": 0.3334379951179243,
            "section_title": "Limitations",
            "char_start_offset": 30115,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 434
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.116943359375
        },
        {
            "corpus_id": "5213937",
            "title": "Scaling High-Order Character Language Models to Gigabytes",
            "text": "range. With a roughly 2.0 bit/character deviation, a 10,000 character sample, which is the size we used, leads to a 2\u03c3 (95.45%) confidence interval of +/-0.02, and the conclusion that the differences between these systems was insignificant. \n\nUnlike in the token-based setting, we are not optimistic about the possibility of improving these results dramatically by clustering character contexts. The lower-order models are very well trained with existing quantities of data and do a good job of this kind of smoothing. We do believe that training hyperparameters for different model orders independently might improve cross-entropy fractionally; we found that training them hierarchically, as in (Samuelsson, 1996), actually increased crossentropy. We believe this is a direct correlate of the effectiveness of update exclusion; the lower-order models do not need to be the best possible models of those orders, but need to provide good estimates when heavily weighted, as in smoothing. The global optimization allows a single setting to balance these attributes, but optimizing each dimension individually should do even better. But with the number of estimates taking place at the highest possible orders, we do not believe the amount of smoothing will have that large an impact overall. \n\nThese experiments had a practical goal -we needed to choose a language modeling implementation for LingPipe and we didn't want to take the standard Swiss Army Knife approach because most of our users are not interested in running experiments on language modeling, but rather using language models in applications such as information retrieval, classification, or clustering. These applications have actually been shown to perform better on the basis of character language models than token models ((Peng, 2003)). In addition, characterlevel models require no decisions about tokenization, token normalization and subtoken modeling (as in (Klein et al., 2003)). \n\nWe chose to include the Witten-Bell method in our language modeling API because it is derived from full corpus counts, which we also use for collocation and relative frequency statistics within and across corpora, and thus the overall implementation effort was simpler. For just language modeling, an update exclusion implementation of Kneser-Ney is no more complicated than Witten-Bell.",
            "score": 0.3330434390486361,
            "section_title": "Introduction",
            "char_start_offset": 2111,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 6
                },
                {
                    "start": 7,
                    "end": 240
                },
                {
                    "start": 243,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 986
                },
                {
                    "start": 987,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1289
                },
                {
                    "start": 1292,
                    "end": 1666
                },
                {
                    "start": 1667,
                    "end": 1804
                },
                {
                    "start": 1805,
                    "end": 1952
                },
                {
                    "start": 1955,
                    "end": 2224
                },
                {
                    "start": 2225,
                    "end": 2342
                }
            ],
            "ref_mentions": [
                {
                    "start": 696,
                    "end": 714,
                    "matchedPaperCorpusId": "3714725"
                },
                {
                    "start": 1930,
                    "end": 1950,
                    "matchedPaperCorpusId": "1080545"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0301513671875
        },
        {
            "corpus_id": "273549247",
            "title": "A Little Help Goes a Long Way: Efficient LLM Training by Leveraging Small LMs",
            "text": "Language modeling. Given a large corpus, language modeling aims to train a model that can assign probabilities or likelihood to each sequence x \u2208 V \u22c6 , where V denotes the underlying vocabulary with V = |V| tokens. Assuming that the language model (LM) is parameterized by \u03b8, it assigns the following probability to a T -token long input sequence x = [x 1 , x 2 , . . . , x T ]: \n\nTransformers [Vaswani et al., 2017] are the most prominent architecture supporting LMs [OpenAI, 2023, Touvron et al., 2023, Gemini-Team et al., 2023], which we briefly discuss in Appendix A.1. \n\nStandard LM pre-training. Typically, LM pre-training involves the next-token prediction task: given a training sequence x = [x 1 , x 2 , . . . , x T ], for each t \u2208 [T ], one maximizes the log-likelihood log P \u03b8 (x t |x \u2264t\u22121 ). This amounts to minimizing the cross-entropy loss between the per-token LM prediction distribution P \u03b8 (\u2022|x \u2264t\u22121 ) and the one-hot distribution * 1 xt (\u2022) defined by the ground truth next-token x t . Thus, the overall loss associated with x becomes \n\nwhere CE(P 1 , P 2 )=\u2212 v\u2208V P 1 (v) log P 2 (v) is the cross-entropy between distributions P 1 and P 2 . \n\nKnowledge distillation for LM. Going beyond the ground truth next-token based loss in (1), one can utilize the per-token prediction distribution provided by another LM, say the one parameterized by \u03b6, as additional supervision. Formally, given the context x \u2264t\u22121 , one can train the LM parameterized by \u03b8 via aligning its prediction distribution P \u03b8 (\u2022|x \u2264t\u22121 ) with P \u03b6 (\u2022|x \u2264t\u22121 ). KL divergence is a common choice to promote such an alignment, which amounts to minimizing the following cross-entropy loss:",
            "score": 0.3328987645388718,
            "section_title": "Background",
            "char_start_offset": 6299,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 18
                },
                {
                    "start": 19,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 378
                },
                {
                    "start": 381,
                    "end": 573
                },
                {
                    "start": 576,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 803
                },
                {
                    "start": 804,
                    "end": 1003
                },
                {
                    "start": 1004,
                    "end": 1052
                },
                {
                    "start": 1055,
                    "end": 1158
                },
                {
                    "start": 1161,
                    "end": 1191
                },
                {
                    "start": 1192,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1669
                }
            ],
            "ref_mentions": [
                {
                    "start": 394,
                    "end": 416,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0699462890625
        },
        {
            "corpus_id": "259937300",
            "title": "Understanding and Mitigating Spurious Correlations in Text Classification with Neighborhood Analysis",
            "text": "There has been several studies on spurious correlations in NLP. Some studies design scores to detect spurious tokens (Wang and Culotta, 2020;Wang et al., 2022;Gardner et al., 2021), whereas other studies propose methods to mitigate spurious correlations, including dataset balancing (Sharma et al., 2018;McCoy et al., 2019;Zellers et al., 2019), model ensemble, and model regularization (Clark et al., 2019(Clark et al., , 2020;;Zhao et al., 2022). However, we observe that typically, less attention is paid to why such spurious token occur and how these spurious tokens acquire excessive importance weights so as to dominate model predictions. In this paper, we provide a different perspective to understand the effect of spurious tokens based on neighborhood analysis in the embedding space. To uncover spurious correlations and force language models (LMs) to align the representations of spurious tokens and genuine tokens, we inspect the nearest neighbors of each token before and after fine-tuning. Consequently, a spurious token presents just like a genuine token in texts and hence acquires large importance weights. We design a metric to measure the spuriousness of tokens which can also be used to detect spurious tokens. \n\nIn light of this new understanding, we mitigate spurious correlations using a model-based mitigation approach by proposing NFL (doN't Forget your Language), a simple yet effective family of regularization methods. These regularization methods restrict changes in either the parameters or outputs of an LM and therefore are capable of preventing the erroneous alignment which causes models to capture spurious correlations. Our analysis is conducted in the context of two text classification tasks: sentiment analysis and toxicity classification. Results show that NFL robustifies model performance against spurious correlation and achieves an out-of-distribution performance that is almost the same as the in-distribution performance. We summarize our contributions as follows: \n\n\u2022 We provide a novel perspective of spurious correlation by analyzing the neighborhood in the embedding space to understand how PLMs capture spurious correlations.",
            "score": 0.332835419405165,
            "section_title": "Introduction",
            "char_start_offset": 1593,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 63
                },
                {
                    "start": 64,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 1003
                },
                {
                    "start": 1004,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1230
                },
                {
                    "start": 1233,
                    "end": 1446
                },
                {
                    "start": 1447,
                    "end": 1655
                },
                {
                    "start": 1656,
                    "end": 1778
                },
                {
                    "start": 1779,
                    "end": 1967
                },
                {
                    "start": 1968,
                    "end": 2010
                },
                {
                    "start": 2013,
                    "end": 2176
                }
            ],
            "ref_mentions": [
                {
                    "start": 117,
                    "end": 141,
                    "matchedPaperCorpusId": "222141135"
                },
                {
                    "start": 141,
                    "end": 159,
                    "matchedPaperCorpusId": "239009631"
                },
                {
                    "start": 159,
                    "end": 180,
                    "matchedPaperCorpusId": "233296459"
                },
                {
                    "start": 283,
                    "end": 304,
                    "matchedPaperCorpusId": "51878517"
                },
                {
                    "start": 304,
                    "end": 323,
                    "matchedPaperCorpusId": "59599752"
                },
                {
                    "start": 323,
                    "end": 344,
                    "matchedPaperCorpusId": "159041722"
                },
                {
                    "start": 387,
                    "end": 406,
                    "matchedPaperCorpusId": "202539031"
                },
                {
                    "start": 406,
                    "end": 429,
                    "matchedPaperCorpusId": "226282321"
                },
                {
                    "start": 429,
                    "end": 447,
                    "matchedPaperCorpusId": "253224186"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1190185546875
        },
        {
            "corpus_id": "235097472",
            "title": "Representations of Meaning in Neural Networks for NLP: a Thesis Proposal",
            "text": "Many NLP applications only use raw text for training data (language models, models for embedding pretraining, arguably even NMT models, although the alignment in parallel corpora may be considered an additional source of information). If they represent meaning, the information must be derived from the training corpus, usually presented to the model through a sliding window of tokens. This may be the reason behind the popularity of the distributional hypothesis in neural language model (LM) literature. The famous saying by Firth (1957), \"You shall know a word by the company it keeps!\", is quoted in most papers concerned with vector space models of language. \n\nThe general distributional hypothesis states that the meaning of a word is given by the contexts in which it occurs. It is, however, worth noticing that in Firth's theory, collocation is just one among multiple levels of meaning, and his text does not support the idea of meaning being based on the context alone. \n\nThe distributional hypothesis would explain why word embeddings capture meaning. However, by itself it tells us nothing about what meaning is and how it relates to the world or people who are using the language.",
            "score": 0.33279439849028736,
            "section_title": "The Distributional Hypothesis",
            "char_start_offset": 4174,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 664
                },
                {
                    "start": 667,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 980
                },
                {
                    "start": 983,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1194
                }
            ],
            "ref_mentions": [
                {
                    "start": 528,
                    "end": 540,
                    "matchedPaperCorpusId": "208093066"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1221923828125
        },
        {
            "corpus_id": "264439628",
            "title": "A Joint Matrix Factorization Analysis of Multilingual Representations",
            "text": "Language Signatures Across Layers We begin by presenting the distribution of average sig(\u2113) values for all languages across all layers for all lexical tokens in Figure 2a. We observe a gradual decrease in the mean of the distribution as we transition from lower to upper layers. This finding is consistent with those from Singh et al. (2019), who found that the similarity between representations of different languages steadily decreases up to the final layer in a pre-trained mBERT model. We used the Mann-Kendall (MK) statistical test (Mann, 1945;Kendall, 1948) for individual languages across all layers. The MK test is a rank-based non-parametric method used to assess whether a set of data values is increasing or decreasing over time, with the null hypothesis being there is no clear trend. Since we perform multiple tests (33 tests in total for all languages), we also control the false discovery rate (FDR; at level q = 0.05) with corrections to the pvalues (Benjamini and Hochberg, 1995). We found that all 33 languages except for Arabic, Indonesian, Japanese, Korean, and Swedish exhibit significant monotonically decreasing trends from lower layers to upper layers, with the FDR-adjusted p-values (p < 0.05). Figure 2a shows that the spread of the distribution for each layer (measured in variance) is constantly decreasing up until layer 6. From these layers forward, the spread increases again. A small spread indicates that the average intensity of scaling from a multilingual representation to the monolingual representation is similar among all languages. This provides evidence of the multilingual model aligning languages into a languageneutral subspace in the middle layers, with the upper layers becoming more task-focused (Merchant et al., 2020). This result is also supported by findings of Muller et al. (2021) -different languages representations' similarity in mBERT constantly increases up to a mid-layer then decreases.",
            "score": 0.33278805401216754,
            "section_title": "Morphosyntactic and Language Properties",
            "char_start_offset": 13871,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 998
                },
                {
                    "start": 999,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1408
                },
                {
                    "start": 1409,
                    "end": 1572
                },
                {
                    "start": 1573,
                    "end": 1768
                },
                {
                    "start": 1769,
                    "end": 1947
                }
            ],
            "ref_mentions": [
                {
                    "start": 322,
                    "end": 341,
                    "matchedPaperCorpusId": "207924237"
                },
                {
                    "start": 538,
                    "end": 550,
                    "matchedPaperCorpusId": "155209193"
                },
                {
                    "start": 967,
                    "end": 997,
                    "matchedPaperCorpusId": "45174121"
                },
                {
                    "start": 1744,
                    "end": 1767,
                    "matchedPaperCorpusId": "216914339"
                },
                {
                    "start": 1814,
                    "end": 1834,
                    "matchedPaperCorpusId": "231718746"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.168701171875
        },
        {
            "corpus_id": "258784768",
            "title": "On the fractal patterns of language structures",
            "text": "When we discuss the emergence of language in the human species, different theories come to the fore. It is not possible to discuss language evolution outside the evolutionary history of humankind. Studies that focus on language evolution take into account different types of evidence from archeology, genetics, anthropology, language acquisition, among other fields. There is no simple, much less easy, answer to the issue of language evolution. \n\nAs of recently, the gestural hypothesis [1] started gaining momentum over the well-known genetic mutation hypothesis [2]. The former predicts a gradual development of gestural communication, which precedes Homo sapiens. The latter, on the other hand, proposes a single recent genetic mutation in sapiens as the beginning of language for humanity. \n\nWhatever the answer to the fascinating question of how language emerged, the fact is that today there are more than 7,000 languages spoken or signed in the world, and every human society known has language and every human child, except very few out of normality cases, will develop language at a tender age. However, what concerns us in this paper is not the issue of language evolution as a species trait, but rather that of similarity among languages, approached from a computational perspective. \n\nEver since comparative linguistics emerged as a discipline (initially as comparative philology), scholars have been trying out methodologies that support similarity comparisons among languages. There are different processes through which languages may resemble each other, notably genetic relationships, i.e., a common ancestral language, which is the major focus of comparative linguistics; language contact through borrowing and shift; the spread of areal features; universal tendencies, such as pointed out in typological studies, or even sheer coincidence in isolated word forms. Whatever the reason for similarities among languages, the conduit for them is human communication and the transmission of language from generation to generation, as well as through some form of population contact leading to language contact. \n\nComputational methods for the study of language similarities have been gaining ground in the past twenty years due both to the buildup of large data sets and the development of computational methods and tools. The reliability of automated methods for the analysis of historical linguistic data is still disputed and solutions such as computer-assisted language comparisons have been proposed [3]. Computational methodology for language similarities has been explored mostly within the contrastive historical and the typological fronts.",
            "score": 0.33231482448975136,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 445
                },
                {
                    "start": 448,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 794
                },
                {
                    "start": 797,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1295
                },
                {
                    "start": 1298,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1881
                },
                {
                    "start": 1882,
                    "end": 2123
                },
                {
                    "start": 2126,
                    "end": 2335
                },
                {
                    "start": 2336,
                    "end": 2522
                },
                {
                    "start": 2523,
                    "end": 2661
                }
            ],
            "ref_mentions": [
                {
                    "start": 2518,
                    "end": 2521,
                    "matchedPaperCorpusId": "216434168"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.051544189453125
        },
        {
            "corpus_id": "254405065",
            "title": "Comparative Analysis of Cross-lingual Contextualized Word Embeddings",
            "text": "To assess the models and alignment techniques from different perspectives we used three different tasks: bilingual lexicon induction (BLI), word retrieval (Cao et al., 2020) and zero-shot cross-lingual natural language inference (XNLI) on two language pairs: high-resource German-English and low-resource Bengali-English. \n\nMotivated by the shortcomings of current alignment methods discussed above, and inspired by the fine-tuning based alignment technique of Cao et al. (2020), in addition to the comparative analysis we propose a parameter, data and time efficient alignment technique which requires 10% of the data, runs within less than 10% of the time and uses the amount of less than 5% of trainable parameters compared to model fine-tuning (Cao et al., 2020). An overview of our proposed approach is given in Figure 1. \n\nThe findings of our experiments demonstrate that 1) multilinguality always leads to better performance in cross-lingual transfer tasks. 2) We should choose bigger models over smaller models when the resources (computational and data) are available but 3) in case of unattainable resources smaller but specialized multilingual models, such as indic-bert (Kakwani et al., 2020), should be chosen, since they are capable of outperforming or performing similar to the big multilingual models, such as XLM-RoBERTa (Conneau et al., 2020), on a language the model is specialized for. 4) Having a large vocabulary and language support is not an advantage of itself, instead the number of tokens allocated for a given language/script plays a more important role. 5) Big language models are sensi- tive to batch size and learning rate. 6) Model finetuning based alignment (Cao et al., 2020) strengthens the quality of MLM's contextualized embeddings and 7) our proposed method is competitive with resource heavy models, even outperforming them in some cases despite having a significantly lower number of trainable parameters. Our work shows that in specific cases (such as for Bengali on XNLI task) less resource intensive but more targeted solutions (e.g. indic-bert) can also be successfully employed.",
            "score": 0.3322759137069974,
            "section_title": "Introduction",
            "char_start_offset": 3256,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 321
                },
                {
                    "start": 324,
                    "end": 767
                },
                {
                    "start": 768,
                    "end": 826
                },
                {
                    "start": 829,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1405
                },
                {
                    "start": 1406,
                    "end": 1582
                },
                {
                    "start": 1583,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1945
                },
                {
                    "start": 1946,
                    "end": 2076
                },
                {
                    "start": 2077,
                    "end": 2123
                }
            ],
            "ref_mentions": [
                {
                    "start": 155,
                    "end": 173,
                    "matchedPaperCorpusId": "211069110"
                },
                {
                    "start": 461,
                    "end": 478,
                    "matchedPaperCorpusId": "211069110"
                },
                {
                    "start": 748,
                    "end": 766,
                    "matchedPaperCorpusId": "211069110"
                },
                {
                    "start": 1182,
                    "end": 1204,
                    "matchedPaperCorpusId": "267935553"
                },
                {
                    "start": 1338,
                    "end": 1360,
                    "matchedPaperCorpusId": "207880568"
                },
                {
                    "start": 1691,
                    "end": 1709,
                    "matchedPaperCorpusId": "211069110"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1114501953125
        },
        {
            "corpus_id": "252846206",
            "title": "SilverAlign: MT-Based Silver Data Algorithm for Evaluating Word Alignment",
            "text": "Since creating a human-annotated word alignment dataset is a challenging task, we propose the Silver-Align method to create a silver benchmark using a Masked Language Model (MLM) and a machine translation model. SilverAlign makes use of MLMs to create minimal pairs with alternatives that fit well into context and find partial alignments based on the changes in the translation of the alternatives via machine translation. \n\nWe show that our method can create a highquality silver benchmark for 9 language pairs including pairs of two non-English languages. We show that the silver benchmark on two different domains (Silver Small and Silver Large ) can help to compare different configurations and investigate errors with a high correlation to the gold data. We perform experiments on sub-word level tokenization, tokenizer vocabulary size, and performance change with respect to PoS tags and word frequency. \n\nFor future work, SilverAlign can be extended to create a specific subset of a general domain dataset to analyze the effects of potential issues in word alignment such as rare words. We believe that Sil-verAlign can ease up the process of finding issues in existing word alignment models for various language pairs, and it can help to improve both word alignment tools and tasks that use word alignment implicitly or explicitly such as machine translation. \n\nFinally, we believe that our silver data creation algorithm can be helpful for both low-and highresource language pairs to investigate word alignment without a time-consuming human annotation process. If combined with recent machine translation models (e.g. NLLB (NLLB Team et al., 2022)), SilverAlign can, in principle, support more than 200 languages. Therefore, we make our silver data and code available as a resource for future work that takes advantage of our silver evaluation datasets.7",
            "score": 0.3321117482361491,
            "section_title": "Conclusion",
            "char_start_offset": 20111,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 423
                },
                {
                    "start": 426,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 910
                },
                {
                    "start": 913,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1368
                },
                {
                    "start": 1371,
                    "end": 1571
                },
                {
                    "start": 1572,
                    "end": 1628
                },
                {
                    "start": 1629,
                    "end": 1724
                },
                {
                    "start": 1725,
                    "end": 1865
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.059417724609375
        },
        {
            "corpus_id": "247011700",
            "title": "Interpreting Language Models with Contrastive Explanations",
            "text": "most explanation methods, the larger the distance between the known evidence and the target token, the larger the increase in alignment of contrastive explanations over non-contrastive explanations. This suggests that contrastive explanations particularly outperform non-contrastive ones when the known evidence is relatively further away from the target token, that is, contrastive explanations can better capture model decisions requiring longer-range context.\n\nIn Appendix B, we also provide a  full alignment scores for each paradigm, explanation method, metric and model.",
            "score": 0.3312374773039741,
            "section_title": "Results",
            "char_start_offset": 13857,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10247802734375
        },
        {
            "corpus_id": "17461998",
            "title": "Self-Organizing Machine Translation: Example-Driven Induction of Transfer Functions",
            "text": "The work described here focuses on the computational implications of a psycholinguistic universal rst expressed by Green 22] and explored and expanded by other researchers 34,35]. This universal, termed \\the Marker Hypothesis,\" states that natural languages are \\marked\" for grammar at surface level|that there exists in every language a small set of words or morphemes that appear in a very limited set of grammatical contexts and that can be said, in a sense, to signal that context. As an example of this principle, consider a basic sentence in English : \n\nThe Boulder Faculty Assembly announced a list of ten faculty awards at its Thursday meeting, with more awards for excellence in teaching than expected. In this sentence, taken at random from a Boulder newspaper, two noun phrases began with determiners, two with quanti ers, and one with a possessive pronoun. The set of determiners and possessive pronouns in English is very small (less than fteen words), and the set of quanti ers is equally recognizable. Similarly, every word in this sentence ending with `-ed' is a past tense verb. The Marker Hypothesis presumes the converse of these observations, e.g. that words which end in `-ed' are very often past tense verbs, and the word `the' usually heralds the appearance of a noun phrase. Or, more generally, that concepts and structures like these will have similar morphological or structural marking in all languages. \n\nProponents of the Marker Hypothesis go further, however, claiming not only that these \\marker words\" could signal the occurrence of particular contexts, but that they do|that marker words form an important cue to psycholinguistic processing of structure. Experiments with miniature languages have backed up this claim. When human subjects are presented with the task of learning a small arti cial language from sentences in the language, they learn more accurately and faster if the arti cial language has cues of the sort described above. Green 22] showed this e ect in arti cial languages with and without speci c marker words as attested in Japanese. Morgan et al. 34] demonstrated it in languages with and without phrase-level substitutions, as of pronouns for full noun phrases. Mori and Moeser 35] examined the e ect of case marking on the pseudowords of the languages.",
            "score": 0.33092007672030155,
            "section_title": "The Marker Hypothesis",
            "char_start_offset": 15084,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 557
                },
                {
                    "start": 560,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1298
                },
                {
                    "start": 1299,
                    "end": 1430
                },
                {
                    "start": 1433,
                    "end": 1687
                },
                {
                    "start": 1688,
                    "end": 1751
                },
                {
                    "start": 1752,
                    "end": 1972
                },
                {
                    "start": 1973,
                    "end": 2086
                },
                {
                    "start": 2087,
                    "end": 2216
                },
                {
                    "start": 2217,
                    "end": 2308
                }
            ],
            "ref_mentions": [
                {
                    "start": 172,
                    "end": 175,
                    "matchedPaperCorpusId": "145316945"
                },
                {
                    "start": 175,
                    "end": 178,
                    "matchedPaperCorpusId": "144650783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1224365234375
        },
        {
            "corpus_id": "16917081",
            "title": "Role of Word Sense Disalnbiguation in Lexical Acquisition: Predicting Semantics from Syntactic Cues",
            "text": ", the median overlap, and the percentage of perfect overlaps (overlaps of value 1.00). In every case, the median is higher than the mean. Put another way, there is always a cluster of good overlaps, but the general tendency is to have fairly poor overlaps.\n\nThe six variations of the experiment are as follows. The first distinction is whether or not to count the negative evidence. We note that the use of negative examples, i.e., plausible uses of the verb in contexts which are disallowed, was a key component of this experiment. There are 1082 positive examples and 586 negative examples. Although this evidence is useful, it is not available in dictionaries, corpora, or other convenient resources that could be used to extend Levin's classification. Thus, to extend our approach to novel word senses (i.e., words not occurring in Levin), we would not be able to use negative evidence. For this reason, we felt it necessary to determine the importance of negative evidence for building uniquely identifying syntactic signatures. As one might expect, throwing out the negative evidence degrades the usefulness of the signatures across the board. The results which had the negative evidence are shown in the left-hand column of numbers in Table 2, and the results which had only positive evidence are shown in the right-hand side.\n\nThe second, three-way, distinction involves prepositions, and breaks the two previous distinctions involving negative evidence into three sub-cases. Because we were interested in the role of prepositions in the signatures, we also ran the experiment with two different parse types: ones that ignored the actual prepositions in the pp's, and ones that ignored all information except for the values of the prepositions. Interestingly, we still got useful results with these impoverished parses, although fewer semantic classes had uniquely-identifying syntactic signatures under these conditions. These results are shown in the three major rows of Table 2.\n\nThe best result, using both positive and negative evidence to identify semantic classes, gives 6.3% of the verbs having perfect overlaps relating semantic classes to syntactic signatures. See Table 2 for the full results.",
            "score": 0.33092007672030155,
            "section_title": "Experiment 1: Verb-based Approach",
            "char_start_offset": 12658,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0302734375
        },
        {
            "corpus_id": "10618934",
            "title": "Building Probabilistic Models for Natural Language",
            "text": "a probabilistic model and searching for the alignment with the highest probability, they assign lengths to different alignments and search for the alignment with the smallest length. 2 Dynamic programming is again used to search for the best alignment. Large corpora are assumed to be already subdivided into smaller chunks.\n\nWhile these algorithms have achieved remarkably good performance, there is definite room for improvement. For example, consider the excerpt from the Hansard corpus depicted in Figure 4.  Alignment algorithms that take advantage of lexical information offer a potential for higher accuracy. Previous work includes algorithms by Kay and R\u00f6scheisen (1993) and Catizone et al. (1989). Kay and R\u00f6scheisen perform alignment using a relaxation paradigm. They keep track of all possible sentence pairs that may align to each other. Initially, this set is very large; it is just constrained by the observation that a sentence in one language is probably aligned with a sentence in the other language with the same relative position in the corpus. For example, an English sentence halfway through the Hansard English corpus is probably aligned to a French sentence near the midpoint of the Hansard French corpus. Given this set of possible alignment pairs, word translations are induced based on distributional information. Using these induced word translations, the set of possible alignment pairs is pruned, which then yields new word translations, etc. This process is repeated until convergence. However, previous lexically-based algorithms have not proved efficient enough to be suitable for large corpora. The largest corpus aligned by Kay and R\u00f6scheisen contains 1,000 sentences in each language; existing bilingual corpora have many millions of sentences.",
            "score": 0.33092007672030155,
            "section_title": "Previous Work",
            "char_start_offset": 228672,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 653,
                    "end": 678,
                    "matchedPaperCorpusId": "14531125"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06982421875
        },
        {
            "corpus_id": "6570837",
            "title": "Word Alignment Baselines",
            "text": "One model for the task of aligning words in a lefthand-side (LHS) segment with those in a right-hand-side (RHS) segment is to consider each pair of tokens as a potential alignment and build a binary classifier to discriminate between correctly and incorrectly aligned pairs. Any of n source language words to align with any of m target language words, resulting in 2 nm possible alignment configurations. This approach allows well-understood binary classification tools to address the problem. However, the assumption made in this approach is that the alignments are independent and identically distributed (IID). This is false, but the same assumption is made by the alignment evaluation metrics. This approach also introduces difficulty in incorporating knowledge of adjacency of aligned pairs, and HMM approaches to word alignment show that this knowledge is important (Och and Ney, 2000). \n\nAll of the techniques presented in this work approach the problem as a binary classification task.",
            "score": 0.33092007672030155,
            "section_title": "Alignment as binary classification",
            "char_start_offset": 441,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 613
                },
                {
                    "start": 614,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 892
                },
                {
                    "start": 895,
                    "end": 993
                }
            ],
            "ref_mentions": [
                {
                    "start": 872,
                    "end": 891,
                    "matchedPaperCorpusId": "5284722"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.054412841796875
        },
        {
            "corpus_id": "273638030",
            "title": "Graph Linearization Methods for Reasoning on Graphs with Large Language Models",
            "text": "This section describes the graph linearization approach, emphasizing the use of graph features to enhance graph reasoning with LLMs. \n\nGenerally speaking, we define graph linearization as the process of representing graphs as linear sequences of tokens. In this work, we aim to identify the linearization approaches that will benefit LLMs by enhancing their ability to understand graphs. We argue that linearized graphs, represented as sequences of tokens, should capture properties similar to those in natural language, given the fact that LLMs are pre-trained on trillions of textual tokens. Such properties should include local dependency and global alignment. \n\nLocal dependency refers to the ability to predict the next (missing) token based on the previous (surrounding) context, within the token sequence of a single linearized graph. This property is analogous to the fundamental distributional hypothesis of language (Joos, 1950;Harris, 1954;Firth, 1957), which states that words that occur in similar contexts tend to have similar meanings (or functions). This hypothesis suggests that given a new word, one should be able to figure out its meaning based on the contexts in which it is used. In fact, the masked and casual language modeling for training encoder-only (Devlin et al., 2019) and decoder-only (Radford et al., 2019) language models, can be seen as instantiations of this hypothesis. \n\nGlobal alignment refers to the alignment across the token sequences of different linearized graphs, ensuring that the corresponding tokens of both sequences match across their full length. This property takes into account the overall structure of the sequences, reflecting the typical flow of text, where common words like \"The\" or \"In conclusion\" are used at the start or end of a sequence, guiding the alignment. For example, linearization should always start from the node with the highest degree, and such nodes are all relabeled as index 0 for different graphs. \n\nBy relying on edge list, which defines a graph in terms of its individual connections, we study the performance impact on LLMs of various methods for ordering the edges in the list and renaming interchangeable node labels, as shown in Figure 1. More specifically, we leverage the advances in graph degeneracy and centrality as detailed in below to meet the local dependency property.",
            "score": 0.330764598066541,
            "section_title": "Graph Linearization Methods",
            "char_start_offset": 14503,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 135,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 663
                },
                {
                    "start": 666,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1405
                },
                {
                    "start": 1408,
                    "end": 1596
                },
                {
                    "start": 1597,
                    "end": 1822
                },
                {
                    "start": 1823,
                    "end": 1974
                },
                {
                    "start": 1977,
                    "end": 2221
                },
                {
                    "start": 2222,
                    "end": 2360
                }
            ],
            "ref_mentions": [
                {
                    "start": 926,
                    "end": 938,
                    "matchedPaperCorpusId": "121205709"
                },
                {
                    "start": 951,
                    "end": 963,
                    "matchedPaperCorpusId": "208093066"
                },
                {
                    "start": 1277,
                    "end": 1298,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1316,
                    "end": 1338,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1328125
        },
        {
            "corpus_id": "273026224",
            "title": "FlashMask: Efficient and Rich Mask Extension of FlashAttention",
            "text": "Recent models such as Llama 3.1 and GPT-4, the Claude series, and Google's Gemini support sequence modeling beyond 128K tokens. FLASHMASK, with its reduced memory overhead, facilitates training with even longer contexts. However, existing public DPO and RM datasets lack training data for scenarios exceeding 128K tokens. To comprehensively evaluate FLASHMASK, we constructed synthetic data to simulate long-sequence training and verify end-to-end throughput improvements. We validated our method across four downstream tasks involving the fine-tuning and alignment training of large language models: SFT, LoRA, DPO, and RM.",
            "score": 0.33061852332976516,
            "section_title": "A.2 END-TO-END TRAINING THROUGHPUT",
            "char_start_offset": 19112,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 624
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04083251953125
        },
        {
            "corpus_id": "17737190",
            "title": "Named entity recognition using conditional random fields with non-local relational constraints",
            "text": "There are two different ways to do this: one takes into consideration and labels every token of the input (e. g. [34]), and the other one instead only segments of the input, not necessarily coinciding with tokens (e. g. [30]). The most widespread statistical methods are the Hidden Markov model (HMM), the Maximum Entropy model and Conditional Random Fields. \n\nHeuristically, a sentence could be imagined as a net where every knot represents a possible value of an observed variable: to somehow label the sentence corresponds then to the choice of a path from one to the other end of the net. This is the intuition lying behind the concepts of graph and Markov chains applied to syntactic analysis, all finding use in NER. Specifically, in the HMM we try to find for every token in the sequence of observations its true, hidden underlying state by guessing from its superficial value (an introduction can be found in [25], chapter 9). We can assume then that the status of every token depends only from the the previous one with a given probability, and it is here that the Markov hypothesis kicks in. Starting from token one and going on, we can represent for every step its probable states as nodes in a graph, which are in turn connected to the nodes of the following step. Every connection between two nodes has its probability; we could see it as the weight of that connection. Then, we could trace all possible paths in the graph from the first to the last token. However, we are interested in the most probable one, that is, the path whose weight is heaviest. This can be done via the Viterbi algorithm. The Maximum Entropy model (MEM) tries to extract rules and constraints regarding the possible values of the label random variables, and then readjusts their probabilities accordingly in such a way that maximum entropy is reached; in other words, probabilities must be distributed as evenly as possible respecting the constraints. It can proved that such a distribution always exists and is unique and an algorithm converging to it can be constructed ( [3]). However, now it is Conditional Random Fields (CRF) that are regarded as state of art. They take steps from a Markovian point of view and are detailed in the next section.",
            "score": 0.3305629322438128,
            "section_title": "Named Entity Recognition",
            "char_start_offset": 12303,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 358
                },
                {
                    "start": 361,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1101
                },
                {
                    "start": 1102,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1469
                },
                {
                    "start": 1470,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1610
                },
                {
                    "start": 1611,
                    "end": 1940
                },
                {
                    "start": 1941,
                    "end": 2068
                },
                {
                    "start": 2069,
                    "end": 2154
                },
                {
                    "start": 2155,
                    "end": 2239
                }
            ],
            "ref_mentions": [
                {
                    "start": 113,
                    "end": 117,
                    "matchedPaperCorpusId": "5253145"
                },
                {
                    "start": 220,
                    "end": 224,
                    "matchedPaperCorpusId": "14036493"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06951904296875
        },
        {
            "corpus_id": "270737841",
            "title": "SoK: Membership Inference Attacks on LLMs are Rushing Nowhere (and How to Fix It)",
            "text": "Sequences are not required to align with semantic boundaries in the document (e.g., sentences, paragraphs) and are defined by their start and end indices within the document. The sequence length can be up to the entire document (a = 1 and b = N di ), although sequences are typically assumed to be shorter. The choice of sequence boundaries (a, b) have been considered random (random excerpts from books) [85] or based on specific criteria (e.g. start of a document [27]). \n\nLLM training. Tokenizer T maps a piece of text S ij to a sequence of tokens T (S ij ) = s ij = (t a , t a+1 , . . . , t b ), where each token belongs to a vocabulary V of size |V| = V . Given a sequence of tokens (referred to as the context), autoregressive language models predict the probability distribution for the next token over all tokens in vocabulary V. This capability is acquired during training on the entire dataset D, where the model minimizes the loss, generally crossentropy loss, for next-token prediction. For a model M with parameters \u03b8, we define the loss computed on sequence s ij as \n\n), where M \u03b8 (t k |t a , . . . , t k\u22121 ) represents the probability predicted by the model for the true token t k given the preceding tokens. \n\nIn this work, we exclusively consider models trained on causal language modeling objectives. This includes both base model versions-trained on large quantities of unsupervised data-and versions that have been further fine-tuned on supervised task-specific data to unlock further abilities (instruction following, chat, specific tasks). In Table I, we refer to these models as pretrained and finetuned, respectively. In this work, we do not consider models trained with more complex alignment techniques such as reinforcement learning methods. \n\nSequence-level MIAs. The unit of interest in MIAs against LLMs has most commonly been a sequence.",
            "score": 0.33052188320976705,
            "section_title": "II. PRELIMINARY",
            "char_start_offset": 10326,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 472
                },
                {
                    "start": 475,
                    "end": 488
                },
                {
                    "start": 489,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 837
                },
                {
                    "start": 838,
                    "end": 998
                },
                {
                    "start": 999,
                    "end": 1079
                },
                {
                    "start": 1082,
                    "end": 1112
                },
                {
                    "start": 1113,
                    "end": 1223
                },
                {
                    "start": 1226,
                    "end": 1318
                },
                {
                    "start": 1319,
                    "end": 1561
                },
                {
                    "start": 1562,
                    "end": 1641
                },
                {
                    "start": 1642,
                    "end": 1768
                },
                {
                    "start": 1771,
                    "end": 1791
                },
                {
                    "start": 1792,
                    "end": 1868
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03875732421875
        },
        {
            "corpus_id": "5300202",
            "title": "Unsupervised morphological segmentation and clustering with document boundaries",
            "text": "dGen-G has better coverage no matter the size of the corpus, which explains why coupling it with Clust-D produces overall better scores. Clust-D does provide a useful added constraint to mere orthographic similarity (i.e. shared trunks in a trie). A worrisome aspect of the results is that performance degrades for large data sets (this is also true for Linguistica). However, it also hints that this method might work well for under-resourced languages. We surmise that since productive suffixes do not suffer from sparsity, even a small data set provides sufficient evidence to reach reliable conclusions about the productive morphology of some language. Increasing the size of the data merely increases the counts of spurious affixes and poses problems for a relative simple measure such as the \u03c7 2 test. A similar result was shown in Creutz and Lagus (2005) where f -score performance of their segmentation method improved as more data was provided then decreased as the input exceeded 250K tokens in English. Their method showed continued improvement with increased data for Finnish. This hints that more data is beneficial for morphologically complex languages but not for morphologically impoverished languages.\n\nFinally, it is also encouraging that the manual evaluation (Table 4) shows very high accuracy, as judged by a documentary linguist. Both our model and Linguistica perform very well under this evaluation.",
            "score": 0.3305215432803176,
            "section_title": "Results and discussion",
            "char_start_offset": 30230,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 838,
                    "end": 861,
                    "matchedPaperCorpusId": "1766004"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07275390625
        },
        {
            "corpus_id": "222067073",
            "title": "Cross-lingual Alignment Methods for Multilingual BERT: A Comparative Study",
            "text": "FineTuneAlign src\u2192tgt achieves top F1 score of 80.21 on SNIPS-Italian dataset which is not far from the score of 83 from a BERT-based model trained on 1400 manually-annotated Italian utterances (2019). Also, our best alignment score of 80.90 for FB-Spanish (FineTuneAlign src\u2192tgt ) surpasses translate-train baseline (2019a) where the annotations are automatically inferred from a NMT model. This suggests that for closer target languages, fine-tuning based alignment are not far behind from unaligned models trained on additional target language labelled examples. Performance improvement from fine-tuning alignment for translated datasets should not be attributed to superficial transfer of entity information from source language. An evidence to support this claim is the strong performance on the SNIPS Italian-SF dataset, which has been translated from SNIPS dataset (Bellomaria et al., 2019), where English entities have been replaced with Italian entities collected from the Web during dataset preparation. Therefore, during validation, the model came across utterances with similar structure but different entities, which shows that improvement from fine-tuning alignment is largely independent of language specific entity memorisation.\n\n6 Related Work Aldarmaki and Diab (2019) propose to align ELMo embeddings (Peters et al., 2018) with word-level and sentence-level alignments. They compare the aligned ELMo with static character-level embeddings with similar alignments. Cao et al. (2020) originally proposed fine-tuning alignment of mBERT language sub-spaces. They claim these methods are strictly stronger to rotation alignments methods based solely on zero-shot experimentation on XNLI task   Figure 2: Trend of improvement from various alignment methods. Rotation alignment improves performance for NER, while fine-tuning alignment is found to be better for SF tasks. Improvements increase initially with distance between source and target languages and diminish for distant languages. ated through translation from source language. On the contrary, we observe that fine-tuning does not improve performance across all tasks, particularly structural tasks, where utterance structure changes and there is higher incidence of domain shift. This raises the question whether translated datasets are biased to",
            "score": 0.3300608394181286,
            "section_title": "Aligned Source Language vs./ Target Language Training",
            "char_start_offset": 20437,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1261,
                    "end": 1286,
                    "matchedPaperCorpusId": "90261106"
                },
                {
                    "start": 1320,
                    "end": 1341,
                    "matchedPaperCorpusId": "3626819"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.051544189453125
        },
        {
            "corpus_id": "252846206",
            "title": "SilverAlign: MT-Based Silver Data Algorithm for Evaluating Word Alignment",
            "text": "Melamed (1998) reports that annotating word alignments for 100 sentences in English-French would take 9 to 22 hours. Additionally, the annotation process often leads to conflicts among annotators (Macken, 2010). Hence, gold data is scarce or completely unavailable for many lowresource languages and, when dealing with domainspecific data such as medical or legal text, such availability is even less. Therefore analyzing existing word alignment models with a varying number of language pairs in different domains is a challenging task. \n\nWe propose SilverAlign, a novel algorithm to create silver evaluation data for guiding the choice of appropriate word alignment methods. Our approach is based on a machine translation model and exploits minimal sentence pairs to create parallel corpora with alignment links. Figure 1 illustrates our core idea with minimal pairs in English and Blissymbols. Our approach is to create alternative sentences in minimal pairs, to rely on machine translation models to track changed words for each alternative and then align words in the source sentence. \n\nIn summary, our contributions are: \n\n1. We find that our silver benchmarks rank methods with high consistency compared to rankings based on gold data. This means that we can identify the best methods based on silver data if there is no gold data available, which is frequently the case in low-resource scenarios for word alignment. \n\n2. We conduct an extensive analysis of our silver resource with respect to gold data for 9 language pairs from different language families and resource availability. We perform various experiments for word alignment models on sub-word tokenization, tokenizer vocabulary size, varying performance of Part-of-Speech tags, and word frequencies. \n\n3. SilverAlign supports a more accurate evaluation and a more in-depth analysis than small gold sets (i.e., English-Hindi has only 90 sentences) because we can automatically create larger evaluation benchmarks. Also, Silver-Align is robust to domain changes as it shows a high correlation between gold and both inand out-of-domain silver benchmarks. \n\n4. It has been shown that machine translation performance (including NMT performance) can be improved by choosing a tokenization that optimizes compatibility between source and target languages (Deguchi et al., 2020). We show that SilverAlign can be used to find such a compatible tokenization for each language pair.",
            "score": 0.3299945257981706,
            "section_title": "Introduction",
            "char_start_offset": 1860,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 536
                },
                {
                    "start": 539,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 895
                },
                {
                    "start": 896,
                    "end": 1088
                },
                {
                    "start": 1091,
                    "end": 1125
                },
                {
                    "start": 1128,
                    "end": 1241
                },
                {
                    "start": 1242,
                    "end": 1422
                },
                {
                    "start": 1425,
                    "end": 1590
                },
                {
                    "start": 1591,
                    "end": 1766
                },
                {
                    "start": 1769,
                    "end": 1979
                },
                {
                    "start": 1980,
                    "end": 2118
                },
                {
                    "start": 2121,
                    "end": 2338
                },
                {
                    "start": 2339,
                    "end": 2438
                }
            ],
            "ref_mentions": [
                {
                    "start": 196,
                    "end": 210,
                    "matchedPaperCorpusId": "9828572"
                },
                {
                    "start": 2315,
                    "end": 2337,
                    "matchedPaperCorpusId": "227230497"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.034881591796875
        },
        {
            "corpus_id": "264935246",
            "title": "Align Your Prompts: Test-Time Prompting with Distribution Alignment for Zero-Shot Generalization",
            "text": "PromptAlign explicitly aligns the mean and variances of the image token embeddings of a proxy source dataset, computed offline, with the image token embeddings of the test sample. We extend TPT with the token alignment strategy, which enforces to bridge the distribution shift in the test data (Fig. 1 a). For each input test sample, we obtain randomly augmented views that are fed into the model to obtain the token embedding statistics. Without any noticeable compute overhead, we update the prompts on both the text and vision branches of CLIP to minimize jointly the feature distribution shift and entropy of predictions. \n\nWe extensively demonstrate the effectiveness of PromptAlign by evaluating the zero-shot generalization on two representative benchmarks: domain generalization and cross-dataset generalization. In the domain generalization setting, our method improves the baseline model by 3.08% on average across the four datasets and has the highest average Top-1 accuracy compared to existing state-of-the-art methods. In the cross-dataset setting, our method achieves an absolute average improvement of 1.82% over the existing state-of-the-art method which uses test-time prompt tuning, while attaining the best Top-1 accuracy in 8 out of the 10 datasets. Our contributions can be summarized as follows: \n\n\u2022 Given only a single test sample, we introduce a distribution alignment strategy for V-L models to improve test-time adaptation. The distribution-aware pre-trained CLIP effectively narrows the distribution gap on the test domains. To the best of our knowledge, this is the first study to explore the potential of distribution alignment in V-L models at test time. \n\n\u2022 The proposed strategy formulates a distribution alignment loss that utilizes offline computed source data statistics to encourage the test sample token distributions to be aligned with the source data token distributions. We harmonically combine the benefits of token distribution alignment and entropy minimization using a multi-modal prompt learning approach. \n\n\u2022 Since CLIP-pre-training data is not publicly released, we study the statistics of ImageNet as a possible candidate for the source distribution, and our empirical results show that ImageNet is an effective proxy source dataset for large-scale V-L models such as CLIP.",
            "score": 0.32998447477803045,
            "section_title": "Introduction",
            "char_start_offset": 4134,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 625
                },
                {
                    "start": 628,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1318
                },
                {
                    "start": 1321,
                    "end": 1450
                },
                {
                    "start": 1451,
                    "end": 1552
                },
                {
                    "start": 1553,
                    "end": 1685
                },
                {
                    "start": 1688,
                    "end": 1911
                },
                {
                    "start": 1912,
                    "end": 2051
                },
                {
                    "start": 2054,
                    "end": 2322
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.29833984375
        },
        {
            "corpus_id": "273185453",
            "title": "Collapsed Language Models Promote Fairness",
            "text": "Aiming to provide a principled understanding and improvement of debiasing language models, we try to find connections between language models and neural collapse for fairness purposes. We demonstrated that debiased models exhibit more collapsed token representations, especially for fairness-sensitive words, leading to better alignment with word embeddings. Leveraging this insight, we introduced a principled regularization method based on neural collapse that can consistently improve fairness in language models across various tasks without sacrificing performance. Our method is simple, effective, and applicable to a wide range of debiasing techniques, providing a robust tool for enhancing fairness in language models. We expect our understanding and fine-tuning to become a principled method for debiasing language models.",
            "score": 0.32960677185475756,
            "section_title": "CONCLUSION",
            "char_start_offset": 23647,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 830
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.22119140625
        },
        {
            "corpus_id": "270764651",
            "title": "Averaging log-likelihoods in direct alignment",
            "text": "To better align Large Language Models (LLMs) with human judgment, Reinforcement Learning from Human Feedback (RLHF) learns a reward model and then optimizes it using regularized RL. Recently, direct alignment methods were introduced to learn such a fine-tuned model directly from a preference dataset without computing a proxy reward function. These methods are built upon contrastive losses involving the log-likelihood of (dis)preferred completions according to the trained model. However, completions have various lengths, and the log-likelihood is not length-invariant. On the other side, the cross-entropy loss used in supervised training is length-invariant, as batches are typically averaged token-wise. To reconcile these approaches, we introduce a principled approach for making direct alignment length-invariant. Formally, we introduce a new averaging operator, to be composed with the optimality operator giving the best policy for the underlying RL problem. It translates into averaging the log-likelihood within the loss. We empirically study the effect of such averaging, observing a trade-off between the length of generations and their scores.",
            "score": 0.3295173955739105,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09320068359375
        },
        {
            "corpus_id": "276622315",
            "title": "Measuring Catastrophic Forgetting in Cross-Lingual Classification: Transfer Paradigms and Tuning Strategies",
            "text": "Transfer learning has emerged as one of the most popular paradigms in deep learning. It aims to transfer already learned neural network weights from one task or model to another. With the emergence of pre-trained large language models (LLMs) 1 in monolingual and multilingual settings, such as BERT [9], GPT-3 [3], and XLM-R [6], they have taken the field of NLP by storm. LLMs are pre-trained with selfsupervised learning, where the idea is to learn the data The associate editor coordinating the review of this manuscript and approving it for publication was Arianna Dulizia . 1 Clarification: Throughout this study, we refer to LLMs as pretrained models that, by recent standards, are considered small. This choice is made because these models use the same transformer architecture and learning tasks for language modeling, differing primarily in the number of parameters. distribution without explicit labels. For example, models are asked to solve fill-a-gap tasks in natural language settings (Masked Language Modeling (MLM)). In Causal Language Modeling (CLM), the model predicts the next word based on the ''cause''-the input provided so far. Conneau and Lample [6] introduced the task of Translated Language Modeling (TLM), where masked words are predicted in two parallel sentences in different languages, improving language alignment. Recently, Zhang et al. [61] conducted a theoretical comparison between masked language models (MLMs) and causal language models (CLMs). They found that, in classification tasks, MLMs exhibit superior performance due to their flexible token targeting, which fosters more inter-sample connections than the fixed token positions in autoregressive models. Building on these findings, VOLUME 13, 2025 2025 The Authors. This work is licensed under a Creative Commons Attribution 4.0 License. \n\nFor more information, see https://creativecommons.org/licenses/by/4.0/ we focus on MLM-based models, such as XLM-R [6], which provide a strong foundation across a broad range of languages.",
            "score": 0.32932523272971276,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 85,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 705
                },
                {
                    "start": 706,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1697
                },
                {
                    "start": 1698,
                    "end": 1759
                },
                {
                    "start": 1760,
                    "end": 1831
                },
                {
                    "start": 1834,
                    "end": 1904
                },
                {
                    "start": 1905,
                    "end": 2022
                }
            ],
            "ref_mentions": [
                {
                    "start": 299,
                    "end": 302,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 310,
                    "end": 313,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1369,
                    "end": 1373,
                    "matchedPaperCorpusId": "270870572"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06768798828125
        },
        {
            "corpus_id": "231719085",
            "title": "Deep Subjecthood: Higher-Order Grammatical Features in Multilingual BERT",
            "text": "Our experimental results constitute a way to begin understanding how general knowledge of grammar is manifested in contextual embedding spaces, and how discrete categories like subjecthood are reconciled in continuous embedding spaces. While most previous work analyzing large contextual models focuses on extracting their analysis of features or structures present in specific inputs, we focus on morphosyntactic alignment, a feature of grammars that is not explicitly realized in any one sentence.\n\nWe find that, when tested out of domain, classifiers trained to predict transitive subjecthood in mBERT contextual space robustly demonstrate decisions which reflect (a) the morphosyntactic alignment of their training language and (b) continuous encoding of subjecthood influenced by semantic properties.\n\nThere has been much recent work pointing out the limitations of the probing methodology for analyzing embedding spaces (Voita and Titov, 2020;Pimentel et al., 2020;Hewitt and Liang, 2019), a methodology that is very similar to ours. The main limitation pointed out in this literature is that the power of classifiers is a confounding variable: we can't know if a classifier's encoding of a feature is due to the feature being encoded in BERT space, or to the classifier figuring out the feature from surface encoding.\n\nIn this paper, we address these issues by proposing two ways to use classifiers to analyze embedding spaces that go beyond probing, and avoid the limitations of arguments based only around the accuracy of probes. Firstly, our results rely on testing the classifiers on out-of-domain zero-shot transfer: both to S arguments and to different languages. As such, we focus on linguistically defining the type of classification boundary which our classifiers learn from mBERT space, rather than their accuracy, and in using transfer we avoid many of the limitations of probing, as argued in Papadimitriou and Jurafsky (2020). Secondly, we examine a feature (morphosyntactic alignment) which is not inferable from the classifiers' training data, which consists only of transitive sentences. We are asking if mBERT contextual space is organized in a way that encodes the effects of morphosyntactic alignment for tokens that do not themselves express alignment. Especially in the cross-lingual case",
            "score": 0.3291731458922218,
            "section_title": "Discussion",
            "char_start_offset": 24024,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 926,
                    "end": 949,
                    "matchedPaperCorpusId": "214693050"
                },
                {
                    "start": 949,
                    "end": 971,
                    "matchedPaperCorpusId": "215238965"
                },
                {
                    "start": 971,
                    "end": 994,
                    "matchedPaperCorpusId": "202538609"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07489013671875
        },
        {
            "corpus_id": "267636581",
            "title": "Pixel Sentence Representation Learning",
            "text": "Pixel-based language models are tokenization-free and are thus ideal for cross-lingual transfer learning. We adopt a representation degeneration perspective (Gao et al., 2018;Ethayarajh, 2019) to understand the zero-shot superiority of pixel language models in out-of-distribution (OOD) generalization. We measure the representation distribution of each language of the vanilla models using the multilingual sts-b (multilingual STS-b (Cer et al., 2017;May, 2021)), which spans 10 languages from 4 language families. Table 2: Anisotropy Estimates (\u2193 the better) of 10 languages. BERT and PIXEL are both trained on the same corpus with 4B tokens, whose major language is English. \n\nThe results presented in Table 2 reveal key insights. We encode sentence-level embeddings from the test set of each language with mean-pooling, and estimate the anisotropy by calculating the empirical mean of pairwise cosine similarity among these embeddings. \n\nWhile BERT presents a slightly more isotropic pattern on its in-distribution language (en), all OOD languages (i.e., not seen during pre-training) suffer from severe representation degeneration. The advantage of PIXEL is immediately pronounced in OOD languages, with isotropy levels surpassing BERT. The robustness of PIXEL in maintaining consistent representation distribution across diverse languages, suggests that its semantic understanding at the sentence level is not solely reliant on language-specific features. Instead, PIXEL appears to leverage a more universal, shape-based approach to semantic cognition, suggesting a natural cognitive alignment with humans. \n\nAs we further explore (Section 5.2), when facilitated by contrastive learning, this alignment promises an amazingly strong bonding effect across languages, and provides a synergistic enhancement on unseen languages, evident in the model's zero-shot semantics understanding abilities. Table 3: Sentence semantics of static word embeddings, vanilla language encoder and its pixel language encoder counterpart.",
            "score": 0.3285687727211709,
            "section_title": "Observation 2: Potential for Zero-shot Cross-lingual Transferability",
            "char_start_offset": 7781,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 677
                },
                {
                    "start": 680,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 939
                },
                {
                    "start": 942,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1241
                },
                {
                    "start": 1242,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1612
                },
                {
                    "start": 1615,
                    "end": 1898
                },
                {
                    "start": 1899,
                    "end": 2022
                }
            ],
            "ref_mentions": [
                {
                    "start": 157,
                    "end": 175,
                    "matchedPaperCorpusId": "59317065"
                },
                {
                    "start": 175,
                    "end": 192,
                    "matchedPaperCorpusId": "202120592"
                },
                {
                    "start": 434,
                    "end": 452,
                    "matchedPaperCorpusId": "4421747"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0753173828125
        },
        {
            "corpus_id": "15276785",
            "title": "An Empirical Study of Chinese Name Matching and Applications",
            "text": "Name matching originated as part of research into record linkage in databases. Initial work focused on string matching techniques. This work can be organized into three major categories: 1) Phonetic matching methods, e.g. Soundex (Holmes and McCabe, 2002), double Metaphone (Philips, 2000) etc.; 2) Edit-distance based measures, e.g. Levenshtein distance (Levenshtein, 1966), Jaro- Winkler (Porter et al., 1997;Winkler, 1999), and 3) Token-based similarity, e.g. soft TF-IDF (Bilenko et al., 2003). Analyses comparing these approaches have not found consistent improvements of one method over another (Cohen et al., 2003;Christen, 2006). More recent work has focused on learning a string matching model on name pairs, such as probabilistic noisy channel models (Sukharev et al., 2014;Bilenko et al., 2003). The advantage of trained models is that, with sufficient training data, they can be tuned for specific tasks. \n\nWhile many NLP tasks rely on name matching, research on name matching techniques themselves has not been a major focus within the NLP community. Most downstream NLP systems have simply employed a static edit distance module to decide whether two names can be matched (Chen et al., 2010;Cassidy et al., 2011;Martschat et al., 2012). An exception is work on training finite state transducers for edit distance metrics (Ristad and Yianilos, 1998;Bouchard-C\u00f4t\u00e9 et al., 2008;Dreyer et al., 2008;Cotterell et al., 2014). More recently, Andrews et al. (2012) presented a phylogenetic model of string variation using transducers that applies to pairs of names string (supervised) and unpaired collections (unsupervised). \n\nBeyond name matching in a single language, several papers have considered cross lingual name matching, where name strings are drawn from two different languages, such as matching Arabic names (El-Shishtawy, 2013) with English (Freeman et al., 2006;Green et al., 2012).",
            "score": 0.3285559223208157,
            "section_title": "Name Matching Methods",
            "char_start_offset": 2625,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 79,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 916
                },
                {
                    "start": 919,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1250
                },
                {
                    "start": 1251,
                    "end": 1433
                },
                {
                    "start": 1434,
                    "end": 1631
                },
                {
                    "start": 1634,
                    "end": 1902
                }
            ],
            "ref_mentions": [
                {
                    "start": 230,
                    "end": 255,
                    "matchedPaperCorpusId": "9108560"
                },
                {
                    "start": 355,
                    "end": 374,
                    "matchedPaperCorpusId": "60827152"
                },
                {
                    "start": 382,
                    "end": 411,
                    "matchedPaperCorpusId": "15968813"
                },
                {
                    "start": 411,
                    "end": 425,
                    "matchedPaperCorpusId": "7844523"
                },
                {
                    "start": 475,
                    "end": 497,
                    "matchedPaperCorpusId": "3091371"
                },
                {
                    "start": 601,
                    "end": 621,
                    "matchedPaperCorpusId": "8008061"
                },
                {
                    "start": 621,
                    "end": 636,
                    "matchedPaperCorpusId": "8957482"
                },
                {
                    "start": 784,
                    "end": 805,
                    "matchedPaperCorpusId": "3091371"
                },
                {
                    "start": 1186,
                    "end": 1205,
                    "matchedPaperCorpusId": "3841188"
                },
                {
                    "start": 1205,
                    "end": 1226,
                    "matchedPaperCorpusId": "15548415"
                },
                {
                    "start": 1226,
                    "end": 1249,
                    "matchedPaperCorpusId": "394655"
                },
                {
                    "start": 1335,
                    "end": 1362,
                    "matchedPaperCorpusId": "8844862"
                },
                {
                    "start": 1362,
                    "end": 1389,
                    "matchedPaperCorpusId": "2972845"
                },
                {
                    "start": 1389,
                    "end": 1409,
                    "matchedPaperCorpusId": "3079803"
                },
                {
                    "start": 1409,
                    "end": 1432,
                    "matchedPaperCorpusId": "11602050"
                },
                {
                    "start": 1449,
                    "end": 1470,
                    "matchedPaperCorpusId": "15188277"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.015960693359375
        },
        {
            "corpus_id": "14715570",
            "title": "Feature-Rich Two-Stage Logistic Regression for Monolingual Alignment",
            "text": "Existing supervised aligners use various contextual features within a learning algorithm for this purpose. Such features include both shallow surface measures (e.g., the relative positions of the tokens being aligned in the respective sentences, similarities in the immediate left or right words) (MacCartney et al., 2008;Thadani and McKeown, 2011;Yao et al., 2013a) and syntactic measures like typed dependencies (Thadani and McKeown, 2011;Thadani et al., 2012). Sultan et al. (2014a) design an unsupervised model that more directly encodes context, via surface and dependency-based neighbors which allow contextual similarity to be represented as a weighted sum of lexical similarity. But their model lacks a key structural advantage of supervised models: to be able to use an arbitrarily large feature set to robustly encode lexical and/or contextual similarity. \n\nThe third and final key component of an aligner is a mechanism to combine lexical/phrasal and contextual similarities to produce alignments. This task is non-trivial due to the presence of cooperating and competing units. We first discuss competing units: semantically similar units in one snippet, each of which is a potential candidate for alignment with one or more units in the other snippet. At least three different possible scenarios of varying difficulty exist concerning such units: \n\n\u2022 Scenario 1: No competing units. In Figure 1, the aligned pair (British(1), UK(1)) represents this scenario. when similar units in one snippet have multiple potential alignments in the other snippet. \n\nGroups of mutually cooperating units can also exist where one unit provides supporting evidence for the alignment of other units in the group. Examples (besides named entities) include individual words in one snippet that are grouped together in the other snippet (e.g., state of the art \u2194 state-of-the-art or headquarters in Paris \u2194 Paris-based). \n\nWe briefly discuss the working principles of existing aligners to show how they respsond to these challenges.",
            "score": 0.3284298910198248,
            "section_title": "Alignment: Key Pieces of the Puzzle",
            "char_start_offset": 4955,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 865
                },
                {
                    "start": 868,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1359
                },
                {
                    "start": 1362,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1562
                },
                {
                    "start": 1565,
                    "end": 1707
                },
                {
                    "start": 1708,
                    "end": 1912
                },
                {
                    "start": 1915,
                    "end": 2024
                }
            ],
            "ref_mentions": [
                {
                    "start": 297,
                    "end": 322,
                    "matchedPaperCorpusId": "1922162"
                },
                {
                    "start": 322,
                    "end": 348,
                    "matchedPaperCorpusId": "9456888"
                },
                {
                    "start": 348,
                    "end": 366,
                    "matchedPaperCorpusId": "2391673"
                },
                {
                    "start": 414,
                    "end": 441,
                    "matchedPaperCorpusId": "9456888"
                },
                {
                    "start": 441,
                    "end": 462,
                    "matchedPaperCorpusId": "2883449"
                },
                {
                    "start": 464,
                    "end": 485,
                    "matchedPaperCorpusId": "14612319"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11358642578125
        },
        {
            "corpus_id": "15978904",
            "title": "Establishing sentential structure via realignments from small parallel corpora",
            "text": "The proposed method of applying realignments to sentence structure has been shown to provide a useful increase in translation accuracy over the best configurations established in earlier experiments. Still, a number of possible extensions of the work presented here have been identified. These focus primarily on how to extract a more comprehensive set of templates from the limitedsize parallel corpus available. To achieve this, one method would be to integrate linguistic knowledge. For instance, by identifying grammatical categories (i.e. different PoS tags) which are equivalent, it is possible to extend knowledge to introduce new realignment templates based on known ones and thus cover more cases. \n\nAlso, it is possible to concatenate different realignment templates to larger groups, in order to make more accurate calculations of the statistics underlying each template. For instance, it may be assumed that whether the PoS tag of the phrase head is a noun or pronoun, the template remains the same and such cases can be grouped together. By extrapolating these new templates, an increase in the pattern space coverage can be expected, leading to an improved translation accuracy. \n\nA point which is of interest is applicability to other language pairs. As is the case for the PRE-SEMT MT methodology as a whole, a key decision was not to design the methodology for one specific language pair. For instance, initial experimentation has shown that the application of realignment templates has correctly generated templates for the case of split verbs when German is the TL (here the Greek-to-German language pair). This is important, as split verbs have been identified as one of the key problems when translating into German. Of course, more experimentation is needed in terms of the generalisation abilities of such realignment templates to cover more cases than those encountered in the training set, and to efficiently model the shift of the second part of the verb to the end of the relevant sentence. Still, the ability of the proposed realignment template method to identify such occurrences is promising.",
            "score": 0.3283659470484611,
            "section_title": "Conclusions and Future Work",
            "char_start_offset": 26824,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 706
                },
                {
                    "start": 709,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 1050
                },
                {
                    "start": 1051,
                    "end": 1192
                },
                {
                    "start": 1195,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1405
                },
                {
                    "start": 1406,
                    "end": 1625
                },
                {
                    "start": 1626,
                    "end": 1737
                },
                {
                    "start": 1738,
                    "end": 2017
                },
                {
                    "start": 2018,
                    "end": 2123
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10430908203125
        },
        {
            "corpus_id": "3268317",
            "title": "Cognitive science in the era of artificial intelligence: A roadmap for reverse-engineering the infant language-learner",
            "text": "Nor do they provide quantitative predictions about the observed resilience in developmental trajectories or their variations as a function of language input at the individual, linguistic or cultural level. Psycholinguists sometimes supplement conceptual frameworks with propositions for specific learning mechanisms which are tested using an artificial language paradigm. As an example, a mechanism based on the tracking of statistical modes in phonetic space has been proposed to underpin phonetic category learning in infancy. It was tested in infants through the presentation of a simplified language (a continuum of syllables between /da/ and /ta/) where the statistical distribution of acoustic tokens was controlled (Maye, Werker, & Gerken, 2002). It was also modeled computationally using unsupervised clustering algorithms and tested using simplified corpora or synthetic data (Vallabha, McClelland, Pons, Werker, & Amano, 2007;McMurray, Aslin, & Toscano, 2009). A similar doublepronged approach (experimental and modeling evidence) has been conducted for other mechanisms: word segmentation based on transition probability (Saffran, Aslin, & Newport, 1996;Daland & Pierrehumbert, 2011), word meaning learning based on cross situational statistics (Yu & Smith, 2007;K. Smith, Smith, & Blythe, 2011;Siskind, 1996), semantic role learning based on syntactic cues (Connor, Fisher, & Roth, 2013), etc. Although studies with artificial languages are useful to discover candidate learning algorithms which could be incorporated in a global architecture, the algorithms proposed have only been tested on toy or artificial languages; there is therefore no guarantee that they would actually work when faced with realistic corpora that are both very large and very noisy. In fact, as discussed in section 6.1, some of these algorithms do not scale up. In addition, it remains to be shown that taken collectively, such learning mechanisms (or scaled up versions thereof) would work synergistically to solve the bootstrapping problem, as opposed to cancelling each other's out.",
            "score": 0.3278238288774916,
            "section_title": "Conceptual frameworks and learning mechanisms",
            "char_start_offset": 15782,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1405
                },
                {
                    "start": 1406,
                    "end": 1770
                },
                {
                    "start": 1771,
                    "end": 1850
                },
                {
                    "start": 1851,
                    "end": 2074
                }
            ],
            "ref_mentions": [
                {
                    "start": 722,
                    "end": 752,
                    "matchedPaperCorpusId": "319422"
                },
                {
                    "start": 885,
                    "end": 936,
                    "matchedPaperCorpusId": "14571527"
                },
                {
                    "start": 936,
                    "end": 969,
                    "matchedPaperCorpusId": "14100640"
                },
                {
                    "start": 1132,
                    "end": 1165,
                    "matchedPaperCorpusId": "13321604"
                },
                {
                    "start": 1165,
                    "end": 1194,
                    "matchedPaperCorpusId": "15073829"
                },
                {
                    "start": 1256,
                    "end": 1274,
                    "matchedPaperCorpusId": "729528"
                },
                {
                    "start": 1274,
                    "end": 1306,
                    "matchedPaperCorpusId": "6298559"
                },
                {
                    "start": 1369,
                    "end": 1399,
                    "matchedPaperCorpusId": "13288698"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.038116455078125
        },
        {
            "corpus_id": "258947837",
            "title": "Soft Alignment Objectives for Robust Adaptation of Language Generation",
            "text": "i is conditioned by the correctly-generated previous tokens from the reference Y j,1..i\u22121 , in practice, the model conditions its predictions on its own outputs \u0398(X j ) 1..i\u22121 . We speculate that this discrepancy might be magnified under a domain shift where the model could not have learned to follow the reference closely.\n\nExposure bias was addressed by sequential objectives, such as Minimum Risk Training (MRT) (Ranzato et al., 2016) that optimize the model by the evaluation of complete output sequence (Yang et al., 2018;Wang and Sennrich, 2020;Mi et al., 2020;Unanue et al., 2021). Apart from the specifics of Reinforcement learning, such as fragility to the optimization settings (Pineau et al., 2021), these methods are also more resource-demanding as they require a sequence of predictions for a single update, limiting their applicability in low-resource adaptation. Previous work of Choshen et al. (2020) also shows that gains of sequential methods in adaptation might be similar to a random training signal. Inspired by this finding, we also assess the gains and OOD robustness of our methods against a random-feedback sequential baseline ( \u00a74.3).\n\nCloser to us, previous work uses alternative training signal based on comparing model hypotheses to the reference. Xu et al. (2019) build soft alignment between fully-generated hypotheses based on hidden states of bidirectional LSTM encoder-decoder and weigh the predicted probability distribution by such alignment in the training objective. Similarly, Lu et al. (2020) complement MLE and sentencelevel objective with the objective minimizing a dot-product of the best-matching hidden representations of tokens of a hypothesis and a reference.  Figure 2: Token alignment mechanism represents subwords s \u0398 of the trained model \u0398 with embeddings of a robust, static model \u0398 emb . Using these representations, we define Alignment of any \u0398's subword s i \u0398 to another text t 2 through a minimal distance of their embeddings given by the robust embedding model \u0398 emb .\n\nReferenced work reaches improvements in conventional high-resource training scenarios, whereas our goal is to",
            "score": 0.3277134081691001,
            "section_title": "Background",
            "char_start_offset": 6889,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05389404296875
        },
        {
            "corpus_id": "58786",
            "title": "Unsupervised Language Acquisition: Theory and Practice",
            "text": "In this situation we are concerned with the artificial situation where we have a list of words and their inflected forms, but we do not know the one-to-one mapping between them. For example, we might have two lists (cat,dog,cow) and (dogs,cows,cats), and we would have to learn the alignment between the two sets, namely the one to one mapping (cat,cats), (dog,dogs), and (cow,cows). If we already know the morphology, this is straightforward, under very reasonable assumptions. If we know the form the morphology takes, e.g. suffixation, prefixation, then it is again trivial -we can merely sort the strings alphabetically, or sort the reversed strings alphabetically, and this will give us a quick result. But in general it is not trivial to learn the alignment and simultaneously the morphological process that gives rise to that alignment. In this section I will present an algorithm based on the EM algorithm that can do this.\n\nLet us suppose we have 2 sets, each of which has n words, U\n\nWe want to find a PHMM, M, and an alignment \u03c0 that maximises the probability of the observed data. The alignment \u03c0 is an element of the set of permutations of n elements, that we can consider as n n permutation matrices. Recall that a permutation matrix is a square matrix all of whose elements are one or zero, such that each row and each column has exactly one one, the rest being zeros. Alternatively we will write \u03c0 \u00a2 i\u00a4 for index of the element in V that the U i is mapped to -so the mapping will be\n\nWe can consider this permutation as a hidden variable; we can train a model using the EM algorithm and then find the value with the highest posterior probability given the data. If we denote the hidden variable, the permutation, as X, as is usual we will have\n\nThere are n! possible values for X so we can set p \u00a2 X \u00a4 1 n!, The probability of the data for a particular permutation will just be the product of the joint probability of the n pairs\n\nThe overall joint probability will then be\n\nNow this sum of the n! products of permutations of a matrix is called the permanent of a matrix (Bhatia,",
            "score": 0.3276843530888929,
            "section_title": "Perfect Partially Supervised Learning",
            "char_start_offset": 254774,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.046722412109375
        },
        {
            "corpus_id": "238638345",
            "title": "The calculus of language: explicit representation of emergent linguistic structure through type-theoretical paradigms",
            "text": "As a result, the analysis can be carried beyond the original available data.  2 For this toy example, we compute the paradigmatic classes rather naively, using Google Books Ngram Viewer (Michel et al. 2010, https://books.google.com/ngrams), with the following parameters: en_2019 corpus, from 1900 to 2019, with a smoothing of 3. The use of wildcards permits to recover the most frequent (up to 10) words at a given place. Since this toy example has only an illustrative character, we disregard the difference in frequency of words within each class, and we order them alphabetically. \n\nWe can then say that a set A is orthogonal to a set B if all the of terms of A can co-occur with the terms of B. In our simple example, we can restrict the notion of successful interaction to simple concatenation of terms. Since the interaction between terms is not commutative in this case, a given set A will have two orthogonals: its left-orthogonal \u22a5 A, which contains all the left terms with which A interacts correctly, and its right orthogonal A \u22a5 , which is the same on the right. For example, if we take any subclass b of the class B defined before, say b = {may, might, must}, we have that its left orthogonal wihin the set A \u00d7 B \u00d7 C coincides with the class A (i.e. \u22a5 b = A) and its right orthogonal is equal to C (b \u22a5 = C), both of which become types following our definitions. Moreover, we can consider the (right or left) bi-orthogonal of b, which is equal to the entire class B, and thus also a type, i.e. \u22a5 (b \n\nIn this way, we have constructed three types which are nothing more than the expression of the mutual dependencies that hold between them. Such types can behave like idealized paradigms which could be further refined based on the statistical properties of the initial corpus. More significantly, their formal construction permits to mobilize the entire type-theoretical apparatus in such a way that the remaining obstacles concerning paradigm derivation can be addressed in a new perspective.",
            "score": 0.3276843530888929,
            "section_title": "Circularity as (Bi-)Orthogonality",
            "char_start_offset": 49439,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 79
                },
                {
                    "start": 80,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 584
                },
                {
                    "start": 587,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1376
                },
                {
                    "start": 1377,
                    "end": 1512
                },
                {
                    "start": 1515,
                    "end": 1653
                },
                {
                    "start": 1654,
                    "end": 1790
                },
                {
                    "start": 1791,
                    "end": 2007
                }
            ],
            "ref_mentions": [
                {
                    "start": 186,
                    "end": 205,
                    "matchedPaperCorpusId": "40104730"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0225067138671875
        },
        {
            "corpus_id": "227231219",
            "title": "Priorless Recurrent Networks Learn Curiously",
            "text": "Smith et al., 1993;Musso et al., 2003) as a contrast to the hierarchical structures usually found in natural languages. The repetition of the same token introduces a construction without any of the internal structure, such as dependencies between heads, modifiers and complements, that characterises the syntactic trees found in natural data, and is instead just a linear sequence of the appropriate length. Moreover, at test time we use the incorrect form of the verb as the inserted item, placing it just before the correct form. Thus, The dog barks becomes The dog marker bark bark . . . bark barks. Correctly predicting number agreement between dog and barks requires counting the intervening bark tokens and accurately anticipating where the sentence will resume.\n\nThe third transformation leaves the syntactic structure of the sentence unchanged, and instead permutes the vocabulary. This is inspired by the shuffling of image labels used by Zhang et al. (2017). We first take a list of the whole lexicon, randomly shuffle it, and then use this to define a mapping from each vocabulary item to another. A sentence in the training corpus is then transformed by selecting a random point, placing a marker, and replacing all the tokens in the remainder of the sentence with their shuffled counterparts. Thus, The man whose dog barks likes apples becomes The man whose dog marker and copy Simpson. At test time, we evaluate the ability of the model to predict number agreement across this boundary by placing the marker immediately before the target verb. Agreement is then measured in terms of the model's ability to predict the shuffled vocabulary item corresponding to the correct form of the original verb.\n\nOur experiments are based on the model and datasets of Gulordava et al. (2018). We retrain their language model using the original hyperparameters, after applying the transformations described above to their training data, consisting of \u223c90M words from English Wikipedia, which they filtered to exclude sentences containing more than 5% unknown words outside a 50K vocabulary. The model was a two layer LSTM, with 650 units in the input embeddings and each recurrent layer, trained using the standard language modelling objective of predicting the next word.\n\nAfter 40 epochs of training, we applied this model to their number agreement test corpus, and evaluated the model's ability to predict correctly whether a",
            "score": 0.3276843530888929,
            "section_title": "Number Agreement in Unnatural Language Structures",
            "char_start_offset": 14236,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 19,
                    "end": 38,
                    "matchedPaperCorpusId": "7955526"
                },
                {
                    "start": 948,
                    "end": 967,
                    "matchedPaperCorpusId": "6212000"
                },
                {
                    "start": 1769,
                    "end": 1792,
                    "matchedPaperCorpusId": "4460159"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.046295166015625
        },
        {
            "corpus_id": "254877112",
            "title": "Training Trajectories of Language Models Across Scales",
            "text": "In this section, we extend the analysis from tokenlevel predictions to entire sequences, up to 50-500 tokens. Larger language models consistently obtain a better perplexity in modeling human texts such as Wikipedia, with the perplexity decreasing as the model size and training computation increases ( Figure 1). Autoregressive language models are probabilistic models of sequences that can generate strings of text. If larger models assign a higher probability to virtually all human-authored texts, what sequences do smaller models favor? We aim to first characterize these sequences and further analyze learning behavior on them to understand how models of different sizes evolve into their final distributions. In what follows, we first show that it is difficult to manually design such sequences, as large models can also favor corrupted or factually incorrect texts ( \u00a74.1). We then devise a decoding algorithm to automatically generate sequences fa-",
            "score": 0.32763453883019256,
            "section_title": "Sequence-Level Generation",
            "char_start_offset": 9929,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1143798828125
        },
        {
            "corpus_id": "258378141",
            "title": "Self-training Reduces Flicker in Retranslation-based Simultaneous Translation",
            "text": "We claimed that student models have lower flicker because they produce more monotonic translations, with less unnecessary variation. Here we provide evidence to support those claims.\n\nTraining data for student models is more monotonic In order to calculate the monotonicity of the training data, we use Kendall's tau score. We first extract word alignments from the training data using fast_align (Dyer et al., 2013) to forwardalign source and target. For each sentence pair we express the alignment as a function a : i \u2192 j, and construct the two lists 1, . . . , T and a(1), . . . , a(T ) where T is the target length. We then calculate the Kendall's tau between the two lists, repeat for each sentence pair in the corpus, and average. We repeat the calculation for the original training data and for the student training set. The results are shown in Table 3. We can see that in all cases, the student training data is more monotonic than the original teacher training data.   We can see from Table 4 that the token entropies are consistently lower for student models, suggesting that the distributions are more \"peaky\", and so less likely to flicker between multiple output tokens with similar probabilities.",
            "score": 0.3275176081595597,
            "section_title": "Monotonicity and Entropy of Student Models",
            "char_start_offset": 10308,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11920166015625
        },
        {
            "corpus_id": "160009505",
            "title": "Approximating Probabilistic Models as Weighted Finite Automata",
            "text": "We now provide experimental evidence of the theory's validity and show its usefulness in various applications. For the ease of notation, we use WFA-Approx to denote the exact counting algorithm described in Section 4.1.2 followed by the KL-Minimization algorithm of Section 4.2. Similarly, we use WFA-SampleApprox(N) to denote the sampled counting described in Section 4.1.3 with N sampled sentences followed by KL-Minimization. \n\nWe first give experimental evidence that supports the theory in Section 5.1. We then show how to approximate neural models as WFAs in Section 5.2. We also use the proposed method to provide lower bounds on the perplexity given a target topology in Section 5.3. Motivated by low-memory applications such as (virtual) keyboard decoding [44], we then use our approach to create compact language models in Section 5.4. Finally, we use our approach to create compact open-vocabulary character language models from count-thresholded data in Section 5.5. \n\nFor all the experiments we use the 1996 CSR Hub4 Language Model data, LDC98T31 (broadcast news data). We use the processed form of the corpus and further process it to downcase all the words and remove punctuation. The resulting dataset has 132M words in the training set, 20M words in the test set, and has 240K unique words. For all the experiments that use word models, we create a vocabulary of approximately 32K words that consists of all words that appeared more than  [50] that we use as a baseline in all our word-based experiments. We use Katz smoothing since it is amenable to pruning [16]. The perplexity of this model on the test set is 144.4. 7 All algorithms were implemented using the open-source OpenFst and OpenGrm n-gram and stochastic automata (SFst) libraries 8 with the last library including these implementations [5,47,6].",
            "score": 0.3273045648722732,
            "section_title": "Experiments",
            "char_start_offset": 26586,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 428
                },
                {
                    "start": 431,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 978
                },
                {
                    "start": 981,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1195
                },
                {
                    "start": 1196,
                    "end": 1307
                },
                {
                    "start": 1308,
                    "end": 1521
                },
                {
                    "start": 1522,
                    "end": 1581
                },
                {
                    "start": 1582,
                    "end": 1638
                },
                {
                    "start": 1639,
                    "end": 1826
                }
            ],
            "ref_mentions": [
                {
                    "start": 1576,
                    "end": 1580,
                    "matchedPaperCorpusId": "14698359"
                },
                {
                    "start": 1820,
                    "end": 1823,
                    "matchedPaperCorpusId": "9257301"
                },
                {
                    "start": 1823,
                    "end": 1825,
                    "matchedPaperCorpusId": "51878150"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10211181640625
        },
        {
            "corpus_id": "256630999",
            "title": "BARLE: Background-Aware Representation Learning for Background Shift Out-of-Distribution Detection",
            "text": "We leverage a pretrained masked language model to generate pseudo OOD samples using ID training data in a principled manner. To facilitate background-shift OOD detection, we expect to generate semantics-preserving background-shifted pseudo samples. Specifically, for a given instance x from the ID training set, we take a pretrained masked language model (such as BERT or Distil-BERT), denoted as the generator G, to produce a corresponding pseudo sample x pseudo . \n\nThe generation process works as follows. The first step is to perform token masking. Given an instance x = [x 1 , x 2 , . . . , x n ] as the input to the generator G, say a sentence with n tokens, we randomly select one position m (an integer between 1 and n) to mask out. The token of the selected position m is replaced with a [MASK] token. We denote the masked instance as x masked = REP LACE(x, m, [MASK]). The second step is to predict the masked token. The generator produces an output distribution over all the tokens in the vocabulary for that maskedout position, i.e., P G (x m | x masked ). We sample a token from this distribution (i.e., xm \u223c P G (x m | x masked )) to replace the original token, i.e., x pseudo = REP LACE(x, m, xm ). Instead of sampling from the entire vocabulary, we sample the target token from a candidate set composed of the tokens with top-k highest probabilities because we want to avoid syntactic and semantic errors in the generated text. We apply the two steps iteratively until the replacement ratio \u03c1, the percentage of replaced tokens, achieves a pre-defined threshold. Finally, we add both the ID examples and the corresponding OOD samples together as D ID \u222a D pseudo for subsequent use. \n\nWe provide some examples of the generated pseudo OOD samples in Table 1. The examples provide us an intuitive understanding of the proposed pseudo OOD generation mechanism. We can observe that the background features such as movie and film of the ID data shift to diverse domain features such as painting and web, whereas the sentiment features are well-preserved in the generated pseudo OOD samples.",
            "score": 0.32728385152174966,
            "section_title": "Pseudo OOD Sample Generation",
            "char_start_offset": 9476,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 465
                },
                {
                    "start": 468,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 810
                },
                {
                    "start": 811,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1578
                },
                {
                    "start": 1579,
                    "end": 1697
                },
                {
                    "start": 1700,
                    "end": 1772
                },
                {
                    "start": 1773,
                    "end": 1872
                },
                {
                    "start": 1873,
                    "end": 2100
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1163330078125
        },
        {
            "corpus_id": "274192741",
            "title": "Preference Alignment for Diffusion Model via Explicit Denoised Distribution Estimation",
            "text": "Diffusion models have achieved remarkable success in text-to-image generation [16,38,46]. A key challenge in generative modeling is alignment [28,52], which focuses on improving a model's ability to better align with human preferences. Alignment training has been extensively explored in the context of large language models [34,48,61]. Initially driven by Reinforcement Learning from Human Feedback (RLHF) [26], alignment techniques have evolved to many other approaches [1,55,60] such as Direct Preference Optimization (DPO) [2,31,37]. The latter has gained significant attraction, inspiring a series of subsequent studies [2,7,60]. \u2020 Corresponding Author Despite the variety of emerging approaches, few studies have attempted to adapt DPO to text-to-image diffusion models. The primary challenge lies in the terminal-only issue of preference labels. That is, human evaluators can only provide preference labels for the final noiseless output of generative models. In contrast, diffusion models generate images progressively, producing large numbers of noisy intermediate results that are difficult to label. This raises the question: How to optimize each intermediate denoising step with terminal preference labels only? \n\nRecent studies tend to solve this terminal-only issue from the perspective of credit assignment [27,32]. i.e., viewing the terminal preference signals as rewards and designing a scheme to distribute rewards among denoising steps. These methods can be mainly divided into two categories. One relies on auxiliary models, such as reward models [4,12] or noisy evaluators [25]. These models essentially learn a weighting function to adaptively assign credit from the terminal reward to each denoising step. However, this approach introduces additional training complexity, undermining the simplicity of DPO. The other requires hand-craft schemes, maintaining the simplicity of DPO but limiting the ability to perform effective credit assignments. These methods typically rely on simple strategies, such as uniform assignment [51,57] or discounted assignment [59] (i.e. placing more weight on the initial denoising steps), which may restrict the alignment potential of the model. \n\nIn this paper, we propose a novel approach termed Denoised Distribution Estimation (DDE).",
            "score": 0.3269943762025518,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1110
                },
                {
                    "start": 1111,
                    "end": 1223
                },
                {
                    "start": 1226,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1512
                },
                {
                    "start": 1513,
                    "end": 1599
                },
                {
                    "start": 1600,
                    "end": 1728
                },
                {
                    "start": 1729,
                    "end": 1829
                },
                {
                    "start": 1830,
                    "end": 1968
                },
                {
                    "start": 1969,
                    "end": 2090
                },
                {
                    "start": 2091,
                    "end": 2200
                },
                {
                    "start": 2203,
                    "end": 2292
                }
            ],
            "ref_mentions": [
                {
                    "start": 78,
                    "end": 82,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 82,
                    "end": 85,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 85,
                    "end": 88,
                    "matchedPaperCorpusId": "222140788"
                },
                {
                    "start": 325,
                    "end": 329,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 329,
                    "end": 332,
                    "matchedPaperCorpusId": "221665105"
                },
                {
                    "start": 407,
                    "end": 411,
                    "matchedPaperCorpusId": "258987659"
                },
                {
                    "start": 472,
                    "end": 475,
                    "matchedPaperCorpusId": "267740352"
                },
                {
                    "start": 475,
                    "end": 478,
                    "matchedPaperCorpusId": "271097721"
                },
                {
                    "start": 527,
                    "end": 530,
                    "matchedPaperCorpusId": "264288854"
                },
                {
                    "start": 530,
                    "end": 533,
                    "matchedPaperCorpusId": "269983560"
                },
                {
                    "start": 533,
                    "end": 536,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 625,
                    "end": 628,
                    "matchedPaperCorpusId": "264288854"
                },
                {
                    "start": 628,
                    "end": 630,
                    "matchedPaperCorpusId": "266725672"
                },
                {
                    "start": 1322,
                    "end": 1326,
                    "matchedPaperCorpusId": "16326763"
                },
                {
                    "start": 1567,
                    "end": 1570,
                    "matchedPaperCorpusId": "258833251"
                },
                {
                    "start": 1570,
                    "end": 1573,
                    "matchedPaperCorpusId": "258947323"
                },
                {
                    "start": 2047,
                    "end": 2051,
                    "matchedPaperCorpusId": "265352136"
                },
                {
                    "start": 2051,
                    "end": 2054,
                    "matchedPaperCorpusId": "265352082"
                },
                {
                    "start": 2080,
                    "end": 2084,
                    "matchedPaperCorpusId": "267636835"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.063720703125
        },
        {
            "corpus_id": "219096636",
            "title": "Word knowledge in a cross-disciplinary world",
            "text": "The dualism between storage and computation in morphology is focused on in Chapter 12, where Martina Penke and Antonio F\u00e1bregas scrutinize competing theoretical frameworks of lexical competence, to assess theoretical predictions in the light of some of the major word processing effects that have been identified in psycholinguistic research over the last decades. In particular, they discuss two of the most established behavioral findings to date: (i) the relative insensitivity of regularly inflected forms to token frequency effects in word processing, and (ii) the stronger perception of morphological structure in regulars as opposed to irregulars. Somewhat surprisingly, these findings appear to cut across two of the main theoretical dimensions governing the contemporary debate on morphology: namely, the opposition between lexicalism and neo-constructionism, and the item-and-arrangement vs. item-and-process dualism. According to the authors, both A-morphous Morphology and Minimalist Morphology prove to be compatible with evidence that humans process regulars and irregulars differently. Nonetheless, they appear to take opposite sides on the theoretically crucial question of what morphological units are stored in the mental lexicon and what units are produced by rules. This suggests that the relationship between principles of grammar organization (e.g. lexicon vs. rules) and processing correlates (storage vs. computation) is not as straightforward as the \"direct correspondence\" hypothesis (Clahsen 2006) has claimed in the past. Differential processing effects may in fact be the complex outcome of the non-linear interaction of uniform learning and processing principles. Since modelling such interaction may well exceed the limits of both theoretical conceptualizations and box-and-arrow models of cognition, settling these theoretical issues will call for advanced sources of experimental evidence (e.g. computational and neuropsychological models of language behavior) and more sophisticated experimental paradigms (e.g. discriminating between morphophonological and morpho-syntactic effects in word processing). \n\nIn Chapter 13, Madeleine Voga, Francesco Gardani and H\u00e9l\u00e8ne Giraudo investigate multilingualism from a two-fold perspective: the psycholinguistic modeling of the bilingual (and multilingual) lexicon, and the role of language contact in language change.",
            "score": 0.326963158486769,
            "section_title": "Outline",
            "char_start_offset": 21986,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 654
                },
                {
                    "start": 655,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1370
                },
                {
                    "start": 1371,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1693
                },
                {
                    "start": 1694,
                    "end": 1927
                },
                {
                    "start": 1928,
                    "end": 2045
                },
                {
                    "start": 2046,
                    "end": 2137
                },
                {
                    "start": 2140,
                    "end": 2392
                }
            ],
            "ref_mentions": [
                {
                    "start": 1510,
                    "end": 1523,
                    "matchedPaperCorpusId": "6882725"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07464599609375
        },
        {
            "corpus_id": "271915818",
            "title": "Personality Alignment of Large Language Models",
            "text": "Efficiency. As shown in Figure 5, training a separate language model for each individual demands (e.g. DPO and PPO) substantial computational resources and time, making it an infeasible solution for personality alignment. PAS addresses efficiency challenges by making targeted adjustments to a small subset of activation values during inference, in specific directions. This approach requires minimal computational overhead, similar to Few-Shot methods, and only about 1/6 of the time needed for PPO, yet achieves remarkable personality alignment results. By intervening at the activation level rather than relying on extensive training or the variability of prompts, PAS provides a more efficient and precise method for aligning language models with individual user preferences. \n\nAlignment vs. ICL. Our experiments highlight that direct Alignment (White-Box) methods, especially PAS, outperform In-Context Learning (ICL) (Black-Box) methods. By aligning model parameters with user preferences, PAS provides better performance compared to prompt-based approaches. The PAS method's ability to efficiently shift activations towards user preferences without altering the model's core parameters gives it a significant advantage over traditional ICL methods. \n\nWhy Did Scaling Laws Fail? The results indicate that scaling laws alone do not ensure optimal alignment, particularly in domain-specific tasks like personality alignment. Although larger models like Llama-3-70B-Instruct benefit from increased scale, their broader knowledge and more general alignment capabilities may actually hinder their ability to precisely capture individual personality traits. This phenomenon likely occurs because larger models are trained to maintain general-purpose capabilities and broad knowledge, making them less adaptable to highly specific individual preferences. PAS demonstrates that targeted intervention in activation patterns can achieve better personality alignment than parameter modification or model scaling. By specifically focusing on personality-relevant patterns rather than general knowledge, PAS achieves superior performance with computational efficiency, emphasizing the importance of precise control over raw model size. Considering that during the process of adjusting the model, although specific goals can be achieved, it may also affect other general capabilities of the model, such as reasoning abilities. We demonstrate through general dialogue tasks and complex reasoning tasks that our method can precisely control the model's personality without compromising its other generalization capabilities.",
            "score": 0.3266916834697992,
            "section_title": "RESULTS ON PAPI",
            "char_start_offset": 19591,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 11
                },
                {
                    "start": 12,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 779
                },
                {
                    "start": 782,
                    "end": 800
                },
                {
                    "start": 801,
                    "end": 943
                },
                {
                    "start": 944,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1255
                },
                {
                    "start": 1258,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1657
                },
                {
                    "start": 1658,
                    "end": 1853
                },
                {
                    "start": 1854,
                    "end": 2007
                },
                {
                    "start": 2008,
                    "end": 2228
                },
                {
                    "start": 2229,
                    "end": 2418
                },
                {
                    "start": 2419,
                    "end": 2614
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1361083984375
        },
        {
            "corpus_id": "271051449",
            "title": "Statistical investigations into the geometry and homology of random programs",
            "text": "As a source of random programs, we probed two different large language models: ChatGPT-4 [9] and TinyLlama [21].ChatGPT-4 is a private model pay-per-query model made available by the non-profit OpenAI, Inc. company and its for-profit subsidiary OpenAI Global, LLC.What is publicly known about the model is that it is a Generative pre-trained transformers (GPT) large language model and it is estimated to have 1.76 trillion parameters [20].TinyLlama builds on the architecture and tokenizer of [19] and has 1.1 billion parameters.\n\nBoth models use the concept of tokens as the smallest language unit that the model is training on and can make predictions on.The implementations vary between models, and OpenAI does not disclose how they tokenize sentences but do provide a tokenizer tool [11], which for chatGPT-3.5 and -4 demonstrates that the sentence \"Hello, world!\", is tokenized as [\"Hello\", \",\", \"world\", \"!\"].TinyLlama uses bytepair encoding [17,5] To compare models we chose a simple application area very familiar to us: Segmenting 2-dimensional images with thresholding.Thresholding is a binary pixel-classification task, where for a given image J, coordinate \u20d7 x, and threshold value t, each pixel is classified as I(\u20d7 x) > t.An example is illustrated in Figure 3.A popular choice for selecting the threshold value is Otsu's method [12].Further pre-and post-processing is often needed such as noise reduction before segmentation and object identification after.Modern segmentation techniques typically rely on deep-learning models including the values of the neighboring pixels, however, we find that for studying the distribution of programs generated by large language models, queries generating small programs are more enlightening.\n\nIn this article, we do not probe deeply into the relation between human language queries and program responses, but from our personal experience teaching students to program, we identified some key features of a query:\n\n\u2022 Code encapsulation,\n\n\u2022 Encapsulation interface,\n\n\u2022 Solution specification, and  \u2022 Pretended behavior of the model.",
            "score": 0.32661615193795224,
            "section_title": "Random programs from Large Language Models",
            "char_start_offset": 12604,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 112,
                    "end": 264
                },
                {
                    "start": 264,
                    "end": 440
                },
                {
                    "start": 440,
                    "end": 530
                },
                {
                    "start": 532,
                    "end": 658
                },
                {
                    "start": 658,
                    "end": 916
                },
                {
                    "start": 916,
                    "end": 1080
                },
                {
                    "start": 1080,
                    "end": 1237
                },
                {
                    "start": 1237,
                    "end": 1275
                },
                {
                    "start": 1275,
                    "end": 1348
                },
                {
                    "start": 1348,
                    "end": 1472
                },
                {
                    "start": 1472,
                    "end": 1746
                },
                {
                    "start": 1748,
                    "end": 1966
                },
                {
                    "start": 1968,
                    "end": 1989
                },
                {
                    "start": 1991,
                    "end": 2017
                },
                {
                    "start": 2019,
                    "end": 2084
                }
            ],
            "ref_mentions": [
                {
                    "start": 1343,
                    "end": 1347,
                    "matchedPaperCorpusId": "15326934"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.022674560546875
        },
        {
            "corpus_id": "225066710",
            "title": "A Survey of Embedding Space Alignment Methods for Language and Knowledge Graphs",
            "text": "The ultimate goal of a word-to-word alignment model is to be able to input the embedding of a token in a source language and receive as output the embedding of a semantically or syntactically similar token in the target language. \n\nAs first noted in [3], word embedding models trained on distinct languages exhibited similar geometric patterns and behaviors. This observation led the authors to hypothesize that word embedding spaces could be transformed from one to another through simple linear operations, such as translation and rotation. The first attempts and models in this area took advantage of large, parallel vocabularies, where pairs of words were used to learn mapping matrices from one space to another. While learning the transformation matrix may have a closed-form solution and could be directly solved through linear algebraic methods, in practice, the weights are learned through stochastic gradient descent. We survey the common supervised learning model types in Section 4.1.1. Given the relative success and ease of implementation of these models when parallel data is available, researchers began to ask how limited that parallel set could be. Restricting to the top 5,000 most common words, restricting the parts of speech, or even relying only on numerals have been popular approaches into reducing the level of supervision needed to learn strong translation models [40]. Hybrid approaches use a form of semi-supervised learning, beginning from small seed lexicons and iteratively adding words as confidence in their direct translation builds. We introduce these semi-supervised methods in Section 4.1.2. Moving past semi-supervised methods, approaches to learning mappings between embedding spaces in a completely unsupervised way. These methods rely on the geometric structures of the underlying spaces as a proxy for parallel data, either relying on embedding similarity distributions [41], adversarial learning [42] or metric recoveries via optimal transport [43]. We cover these methods in Section 4.1.3.",
            "score": 0.32654586540233976,
            "section_title": "Word-to-Word Alignment",
            "char_start_offset": 33079,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 229
                },
                {
                    "start": 232,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 717
                },
                {
                    "start": 718,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 998
                },
                {
                    "start": 999,
                    "end": 1166
                },
                {
                    "start": 1167,
                    "end": 1396
                },
                {
                    "start": 1397,
                    "end": 1568
                },
                {
                    "start": 1569,
                    "end": 1629
                },
                {
                    "start": 1630,
                    "end": 1757
                },
                {
                    "start": 1758,
                    "end": 1993
                },
                {
                    "start": 1994,
                    "end": 2034
                }
            ],
            "ref_mentions": [
                {
                    "start": 1391,
                    "end": 1395,
                    "matchedPaperCorpusId": "13335042"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1468505859375
        },
        {
            "corpus_id": "276482839",
            "title": "Explanations of Large Language Models Explain Language Representations in the Brain",
            "text": "Recent progress in deep learning has led to the development of autoregressive language models that excel in capturing language structures and performing human-like capabilities in various linguistic tasks [1][2][3][4]. This progress poses a critical question at the intersection of artificial intelligence (AI) and neuroscience: Are the observed similarities between language models and human cognition merely superficial, or do they stem from shared underlying mechanisms? \n\nPrevious studies have demonstrated significant alignment between large language models (LLMs) and brain activity, primarily through linear mappings between neural responses and LLM internal representations, such as activations, attention heads, and layer transformations [5][6][7][8]. This relationship has been explored across multiple dimensions, including model layers, architectures, training settings, and linguistic performance [5,6,9,10]. Notably, transformer models consistently outperform recurrent and embedding-based models in brain alignment tasks, displaying brain-like patterns tied closely to their proficiency in next-word prediction tasks [5,6]. \n\nBuilding on this foundation, subsequent research has leveraged LLM-brain alignment to deepen our understanding of both systems. For example, LLM-based encoders have provided insights into key aspects of neural processing, such as predictive processing, semantic selectivity, and meaning composition in the brain during naturalistic language processing [11][12][13]. Conversely, these approaches have been employed to evaluate and refine LLMs themselves [9,14,15]. Despite these advances, several critical questions remain unresolved: (i) Do the observed similarities between LLMs and the brain arise from shared pathways between these two systems, or are they simply artifacts of high-dimensional feature spaces [16,17]? (ii) Beyond internal representations, are there alternative frameworks that can more effectively capture the alignment between LLMs and neural language processing? Addressing these questions requires moving beyond treating both systems as \"black boxes\" [18], and instead probing their underlying mechanisms in a more interpretable way. \n\nTo address these questions, we leverage explainable AI (XAI) techniques, specifically attribution methods, which quantify how much each preceding word contributes to a model's evolving representation and next-word prediction.",
            "score": 0.32652278299699,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 473
                },
                {
                    "start": 476,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1138
                },
                {
                    "start": 1141,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1861
                },
                {
                    "start": 1862,
                    "end": 2025
                },
                {
                    "start": 2026,
                    "end": 2197
                },
                {
                    "start": 2200,
                    "end": 2425
                }
            ],
            "ref_mentions": [
                {
                    "start": 208,
                    "end": 211,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 211,
                    "end": 214,
                    "matchedPaperCorpusId": "195069387"
                },
                {
                    "start": 747,
                    "end": 750,
                    "matchedPaperCorpusId": "246902471"
                },
                {
                    "start": 750,
                    "end": 753,
                    "matchedPaperCorpusId": "222359195"
                },
                {
                    "start": 753,
                    "end": 756,
                    "matchedPaperCorpusId": "254409674"
                },
                {
                    "start": 756,
                    "end": 759,
                    "matchedPaperCorpusId": "260117623"
                },
                {
                    "start": 910,
                    "end": 913,
                    "matchedPaperCorpusId": "246902471"
                },
                {
                    "start": 913,
                    "end": 915,
                    "matchedPaperCorpusId": "222359195"
                },
                {
                    "start": 915,
                    "end": 917,
                    "matchedPaperCorpusId": "167217728"
                },
                {
                    "start": 917,
                    "end": 920,
                    "matchedPaperCorpusId": "267334935"
                },
                {
                    "start": 1132,
                    "end": 1135,
                    "matchedPaperCorpusId": "246902471"
                },
                {
                    "start": 1135,
                    "end": 1137,
                    "matchedPaperCorpusId": "222359195"
                },
                {
                    "start": 1493,
                    "end": 1497,
                    "matchedPaperCorpusId": "257309545"
                },
                {
                    "start": 1501,
                    "end": 1505,
                    "matchedPaperCorpusId": "227172960"
                },
                {
                    "start": 1594,
                    "end": 1597,
                    "matchedPaperCorpusId": "167217728"
                },
                {
                    "start": 1597,
                    "end": 1600,
                    "matchedPaperCorpusId": "257255248"
                },
                {
                    "start": 1853,
                    "end": 1857,
                    "matchedPaperCorpusId": "258822882"
                },
                {
                    "start": 1857,
                    "end": 1860,
                    "matchedPaperCorpusId": "253353392"
                },
                {
                    "start": 2115,
                    "end": 2119,
                    "matchedPaperCorpusId": "174797900"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1981201171875
        },
        {
            "corpus_id": "262043426",
            "title": "X-PARADE: Cross-Lingual Textual Entailment and Information Divergence across Paragraphs",
            "text": "While the task of detecting new and inferable information in paragraphs across languages is novel, it relates to ideas from machine translation and textual entailment. Here we describe how to adapt baselines from these areas to assess their performance on this task, as well as prompting LLMs to produce spans (Figure 3). Implementation details can be found in Appendix C. \n\nAlignment MT word alignment predicts which words should be aligned across translations; thus words which do not easily align are more likely to present new content not given in the source paragraph. By way of approximation, we will assume in these experiments that unaligned tokens fall into the new category. \n\nSLR-NLI SLR-NLI (Stacey et al., 2022) builds on the idea that a neutral or contradiction relation holds between two sentences only when there is at least one span in the \"hypothesis\" (target) that is not inferable from the premise. Since these spans are exactly the ones containing new information, we use SLR-NLI to predict which spans in the target paragraph are new. NLI Attribution Rather than using the inherently interpretable method of Stacey et al. (2022), we can instead use a standard NLI system equipped with a post-hoc interpretation method. We use token attribution methods for NLI models to score the tokens most responsible for a neutral classification decision. We compute an attribution score for each token; higher-scoring tokens should be new and not inferable. \n\nLLMs We use one-shot prompting of three stateof-the-art LLMs, GPT-3.5-turbo, GPT-4, and Llama-2-chat (Touvron et al., 2023), and two explicitly multilingual LLMs, BLOOMZ (Muennighoff et al., 2023) and XGLM (Lin et al., 2022). \n\nBLOOMZ is an instruction-tuned model, while XGLM is a non-instruction tuned autoregressive LM. We used prompts that specify the annotation task, given in Appendix F. \n\nThe four different methods are compared and summarized in Table 6.",
            "score": 0.3260771189359167,
            "section_title": "Methods",
            "char_start_offset": 15739,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 372
                },
                {
                    "start": 375,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 684
                },
                {
                    "start": 687,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1467
                },
                {
                    "start": 1470,
                    "end": 1546
                },
                {
                    "start": 1547,
                    "end": 1695
                },
                {
                    "start": 1698,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 1863
                },
                {
                    "start": 1866,
                    "end": 1932
                }
            ],
            "ref_mentions": [
                {
                    "start": 703,
                    "end": 724,
                    "matchedPaperCorpusId": "253018990"
                },
                {
                    "start": 1130,
                    "end": 1150,
                    "matchedPaperCorpusId": "253018990"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1033935546875
        },
        {
            "corpus_id": "44137213",
            "title": "Using Word Vectors to Improve Word Alignments for Low Resource Machine Translation",
            "text": "Word alignments are essential for statistical machine translation (MT), especially in low-resource settings where neural MT systems often do not compete with phrase-based and syntax-based MT (Koehn and Knowles, 2017). The most widely used word alignment method (Brown et al., 1993) works by estimating the parameters of IBM models from training data using the Expectation Maximization (EM) algorithm. However, EM works poorly for low-frequency words as they do not appear enough in the training data for confident parameter estimation. This problem is even worse in low-resource settings where a large portion of word types appear infrequently in the parallel data. In this paper we improve word alignments and consequently machine translation in low resource settings by improving the alignments of infrequent tokens. \n\nWorks that deal with the rare-word problem in word alignment include those that alter the probability distribution of IBM models' parameters by adding prior distributions (Vaswani et al., 2012;Mermer and Sarac \u00b8lar, 2011), smoothing the probabilities (Moore, 2004;Zhang and Chiang, 2014;Van Bui and Le, 2016) or introducing symmetrization (Liang et al., 2006;Pourdamghani et al., 2014). These works, although effective, merely rely on the information extracted from the paral-lel data. Another branch adds linguistic knowledge like word stems, orthography (Hermjakob, 2009) morphological analysis (De Gispert et al., 2006;Lee, 2004), syntactic constraints (Fossum et al., 2008;Cherry and Lin, 2006;Toutanova et al., 2002) or a mixture of such clues (Tiedemann, 2003). These methods need languagespecific knowledge or tools like morphological analyzers or syntax parsers that is costly and time consuming to obtain for any given language. \n\nA less explored branch that can help aligning rare words is adding semantic information. The motivation behind this branch is simple: Words with similar meanings should have similar translations. Previously, Ma et al. (2011) cluster words using monolingual data and substitute each word with its cluster representative to get alignments.",
            "score": 0.3259850572500342,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 818
                },
                {
                    "start": 821,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1306
                },
                {
                    "start": 1307,
                    "end": 1588
                },
                {
                    "start": 1589,
                    "end": 1758
                },
                {
                    "start": 1761,
                    "end": 1849
                },
                {
                    "start": 1850,
                    "end": 1956
                },
                {
                    "start": 1957,
                    "end": 2098
                }
            ],
            "ref_mentions": [
                {
                    "start": 261,
                    "end": 281,
                    "matchedPaperCorpusId": "13259913"
                },
                {
                    "start": 992,
                    "end": 1014,
                    "matchedPaperCorpusId": "2662894"
                },
                {
                    "start": 1085,
                    "end": 1108,
                    "matchedPaperCorpusId": "3521890"
                },
                {
                    "start": 1160,
                    "end": 1180,
                    "matchedPaperCorpusId": "618683"
                },
                {
                    "start": 1180,
                    "end": 1206,
                    "matchedPaperCorpusId": "217895"
                },
                {
                    "start": 1377,
                    "end": 1394,
                    "matchedPaperCorpusId": "11853100"
                },
                {
                    "start": 1418,
                    "end": 1443,
                    "matchedPaperCorpusId": "1831067"
                },
                {
                    "start": 1443,
                    "end": 1453,
                    "matchedPaperCorpusId": "29938854"
                },
                {
                    "start": 1477,
                    "end": 1498,
                    "matchedPaperCorpusId": "2485577"
                },
                {
                    "start": 1498,
                    "end": 1519,
                    "matchedPaperCorpusId": "2787289"
                },
                {
                    "start": 1519,
                    "end": 1542,
                    "matchedPaperCorpusId": "13312615"
                },
                {
                    "start": 1570,
                    "end": 1587,
                    "matchedPaperCorpusId": "8305336"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.041839599609375
        },
        {
            "corpus_id": "275342915",
            "title": "Context-Alignment: Activating and Enhancing LLM Capabilities in Time Series",
            "text": "We develop a Dual-Scale Context-Alignment Graph Neural Networks (DSCA-GNNs) framework to achieve both structural and logical alignment. Specifically, structural alignment employs dual-scale nodes to describe hierarchical structure in TS-language, i.e. the structural independence of tokens and the overall structure of modalities. Structural alignment provides LLMs with structural segmentation information for lengthy TS language inputs, enabling LLMs to treat long TS data as an individual linguistic component while preserving intrinsic token features. Logical alignment uses directed edges in both scale GNNs to guide the local and global logical relationship between TS data and language prompts, integrating TS within the language environment and ensuring semantic coherence across two modalities. Utilizing the few-shot prompting technique [Brown, 2020], we propose Few-Shot prompting based Context-Alignment (FSCA) following the DSCA-GNNs framework, which further enhances the LLMs' performance on TS tasks. FSCA can be flexibly and repeatedly integrated into various layers of pre-trained LLMs to improve awareness of logic and structure. Extensive experiments across various TS tasks demonstrate the effectiveness of our method. Notably, in few-shot and zero-shot forecasting tasks, our approach significantly outperforms others, confirming that the logical and structural alignment provides powerful prior knowledge on context. Ablation studies further validate the importance of Context-Alignment. \n\nIn summary, our core contributions can be summarized below: \n\n\u2022 We emphasize that effectively leveraging LLMs for TS tasks requires first activating their capabilities and then enhancing them. Besides, we pinpoint that token-level alignment fails to fully activate pre-trained LLMs due to their neglect of LLMs' inherent strengths, which primarily stem from a deep understanding of logic and structure, rather than superficial token processing. \n\n\u2022 We are the first to propose Context-Alignment paradigm, which aims to construct a context-level alignment between TS and language, thereby activating LLMs' potential capabilities in TS tasks. \n\n\u2022 We develop a Dual-Scale Context-Alignment GNNs framework, which achieves structural and logical alignment through dual-scale nodes and directed edges, thus realizing Context-Alignment. Furthermore, by integrating the few-shot prompting technique, we introduce (FSCA), which enhances LLMs' performance in TS tasks.",
            "score": 0.3258755686477103,
            "section_title": "Introduction",
            "char_start_offset": 2182,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 803
                },
                {
                    "start": 804,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1238
                },
                {
                    "start": 1239,
                    "end": 1438
                },
                {
                    "start": 1439,
                    "end": 1509
                },
                {
                    "start": 1512,
                    "end": 1571
                },
                {
                    "start": 1574,
                    "end": 1704
                },
                {
                    "start": 1705,
                    "end": 1956
                },
                {
                    "start": 1959,
                    "end": 2152
                },
                {
                    "start": 2155,
                    "end": 2341
                },
                {
                    "start": 2342,
                    "end": 2470
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0726318359375
        },
        {
            "corpus_id": "259308977",
            "title": "Should you marginalize over possible tokenizations?",
            "text": "(2021) exploited the probabilistic nature of the UnigramLM tokenizer (Kudo, 2018) to define such a proposal. As a consequence, their results do not necessarily extend to the more popular language models like GPT-2 (Radford et al., 2019), GPT-3 (Brown et al., 2020), BLOOM (Scao et al., 2022), T5 (Raffel et al., 2020), among others, trained using other tokenization schemes such as BPE (Sennrich et al., 2016), WordPiece (Schuster and Nakajima, 2012), among others.\n\n1 In this work, we devise a new proposal distribution that allows us to quantify the effect of marginalization for any given tokenizer. Equipped with this algorithm, we inspect the effect of marginalization over tokenizations for two LMs, GPT-2 (126M parameters, English) and the recently released BLOOM (1.7B parameters, multilingual), on various domains and languages. Our importance sampling estimates show that in practice marginalization does not influence log-likelihood much (usually less than 0.5% improvement), the highest influence (1-2% improvement) being for data with long, complex words and distribution shift. Because the results will vary for different models and data, we provide a tool for researchers and practitioners to measure the gap in their specific setting to decide whether the usual practice is warranted. To this end, we release our code 1 , which can be applied to models from the transformers library.",
            "score": 0.32578571199311335,
            "section_title": "Introduction",
            "char_start_offset": 1938,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 296,
                    "end": 317,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 386,
                    "end": 409,
                    "matchedPaperCorpusId": "1114678"
                },
                {
                    "start": 421,
                    "end": 450,
                    "matchedPaperCorpusId": "22320655"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08660888671875
        },
        {
            "corpus_id": "237302466",
            "title": "On the differences between BERT and MT encoder spaces and how to address them in translation tasks",
            "text": "That is, the embeddings are organized in a narrow cone but have a wide range of lengths. This behaviour might arise from the system needing to represent both languages in the same space, and the interplay between training the embeddings layer at the target side while needing to keep source embeddings apart enough -future work is necessary to confirm this. Motivated by these findings, we emphasize that using both metrics and observing how they interact allows for a more complete understanding of the representation spaces. 1 Finding (2) is more relevant to our main question of the differences between the geometries of 1 Cosine similarity does not take into account the magnitude of the vectors at play, making it susceptible to the existence a large value in one of the entries of a high-dimensional vector, while Euclidean distance is hard to interpret in highdimensional spaces and it is unbounded from above. BERT and MT. Both metrics show a more gradual increase in the closeness of random tokens in the BERT over layers, as compared to an abrupt increase in the MT space. Therefore, we can deduce that the MT model can keep random representations successfully apart for all but the uppermost of the layers. We hypothesize that this monotonously increasing levels of closeness of random token embeddings in BERT may be contributing to its suboptimal machine translation performance. To verify this hypothesis, in section 4 we present results on MT performance after alignment and in section 4.1 we show how the alignment method changes the embeddings distributions. \n\nSimilarity between tokens of the same form. \n\nSelfSim will be high in less contextualized models, because such models use similar representations for each occurrence of the same token. Highly contextualized models will have lower SelfSim since every occurrence of the word will have a different representation. Comparing the two spaces (Figure 2), we again observe different trends. SelfSim steadily drops for BERT except for the last layer, showing an increase in the contextuality of the representations. For the MT model, on the other hand, we observe a steep drop at layer 6, indicating a sudden increase in contextuality here.",
            "score": 0.3257314965346156,
            "section_title": "Aligning the Representation Spaces",
            "char_start_offset": 11856,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1575
                },
                {
                    "start": 1578,
                    "end": 1621
                },
                {
                    "start": 1624,
                    "end": 1762
                },
                {
                    "start": 1763,
                    "end": 1888
                },
                {
                    "start": 1889,
                    "end": 1960
                },
                {
                    "start": 1961,
                    "end": 2084
                },
                {
                    "start": 2085,
                    "end": 2209
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08709716796875
        },
        {
            "corpus_id": "240420174",
            "title": "LMdiff: A Visual Diff Tool to Compare Language Models",
            "text": "We presented LMDIFF, a tool to visually inspect qualitative differences between language models based on output distributions. We show in several use cases how finding specific text snippets and analyzing them token-by-token can lead to interesting hypotheses. We emphasize that LMDIFF by itself does not provide any definite answers to these hypotheses by itself -it cannot, for example, show which model is generally better at a given task. To answer these kind of questions, statistical analysis is required.\n\nA design limitation of LMDIFF is that it relies on compatible models. Because the tool is based on per-token model outputs and apples-to-apples comparisons of distributions, only models that use the same tokenization scheme and vocabulary can be compared in the instance view. In future work, we will work toward extending the compatibility by introducing additional tokenization-independent measures and visualizations.\n\nAnother extension of LMDIFF may probe for memorized training examples and personal information using methods proposed by Carlini et al. (2020). As shown in Sections 4.2 and 4.5, we can already identify text that was generated by a model and leverage patterns that a model has learned. Adding support to filter a corpus by measures in addition to finding outliers may help with the analysis of potentially memorized examples.",
            "score": 0.3256819370874617,
            "section_title": "Discussion and Conclusion",
            "char_start_offset": 16969,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.169921875
        },
        {
            "corpus_id": "246411325",
            "title": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model",
            "text": "For each subcase, we show the accuracy of MT-NLG pretrained on 270 billion tokens, when including 32 examples in the shot. To counteract existing class prediction biases, prediction distributions were normalized by shifting their means. \n\nOverall, we see evidence that the model at least partially relies on heuristics: for non-entailment, performance is almost perfect on lexical overlap cases, which are the easiest for the model to escape (the premisehypothesis superficial similarity is smaller, and thus it is not as strongly inclined to infer entailment). However, the model finds it more challenging to ignore superficial similarity and infer non-entailment in case of a verbatim presence of the hypothesis as a subsequence in the premise. Reversely, it is much easier for the model to correctly infer entailment in case of shared subsequences, rather than in the presence of mere lexical overlap, and thus accuracy for lexical overlap entailment subcases is lower. \n\nNevertheless, we also observe clear indications that the model, despite being only trained through autoregressive language modeling, is able to learn linguistic rules such as the role and function of passive voice, of the order of subject and object, of relative clauses, or of verbs that can be either transitive or intransitive, and it systematically takes into account the respective syntactic structures for inference, successfully escaping misleading superficial textual similarity. In terms of \"understanding\" the nuance of vocabulary, besides straight-forward cases, such as that \"Without a doubt the managers advised the lawyers\" entails that \"The managers advised the lawyers\", while the adverbs \"supposedly\" or \"probably\" reduce certainty, it is also capable of distinguishing the difference that the verb makes with respect to the veracity of the hypothesis, in cases such as: \"The professors claimed / thought that the scientist advised the tourist \u2192 The scientist advised the tourist\", as opposed to \"The professors forgot / knew that the scientist advised the tourist\".",
            "score": 0.3256729219860941,
            "section_title": "A.2 Performance per subcase",
            "char_start_offset": 77544,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 236
                },
                {
                    "start": 239,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 746
                },
                {
                    "start": 747,
                    "end": 972
                },
                {
                    "start": 975,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 2058
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1280517578125
        },
        {
            "corpus_id": "268681807",
            "title": "CLHA: A Simple Yet Effective Contrastive Learning Framework for Human Alignment",
            "text": "To mitigate the aforementioned challenges, we introduce a simple yet effective Contrastive Learning Framework for Human Alignment (CLHA), facilitating the achievement of human alignment in LLMs.\n\nIn particular, CLHA incorporates a rescoring strategy that evaluates noise by considering the data quality and making dynamic adjustments during training.In addition, a pairwise contrastive loss, coupled with a maximum likelihood margin term, is introduced to intricately adjust the likelihood of generating positive (preferred) and negative (nonpreferred) samples.Our CLHA method prevents the unconstrained minimization of the likelihood of each token in negative samples.Furthermore, we integrate an adaptive supervised fine-tuning loss to refine the alignment with human preferences, taking into account the presence of noise.\n\nThe main contributions of this paper can be summarized as follows:\n\n\u2022 We propose a simple yet effective contrastive learning framework named CLHA as an alternative to PPO in the pursuit of approximating the objective of human alignment.\n\n\u2022 We propose a novel reward rescoring method to address the noise within the preference data, taking into account its intrinsic quality and dynamically adjusting the training process.Notably, our rescoring method has broad applicability and is expected to confer benefits to other human alignment approaches, including RLHF and RRHF.\n\n\u2022 We conduct extensive experiments on a benchmark dataset (i.e., Helpful and Harmless).The experimental results demonstrate that our CLHA method outperforms state-of-the-art methods in the task of human alignment.",
            "score": 0.3255429793966396,
            "section_title": "Introduction",
            "char_start_offset": 4545,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 196,
                    "end": 350
                },
                {
                    "start": 350,
                    "end": 561
                },
                {
                    "start": 561,
                    "end": 669
                },
                {
                    "start": 669,
                    "end": 825
                },
                {
                    "start": 827,
                    "end": 893
                },
                {
                    "start": 895,
                    "end": 1063
                },
                {
                    "start": 1065,
                    "end": 1248
                },
                {
                    "start": 1248,
                    "end": 1398
                },
                {
                    "start": 1400,
                    "end": 1487
                },
                {
                    "start": 1487,
                    "end": 1613
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.039794921875
        },
        {
            "corpus_id": "267523507",
            "title": "TransLLaMa: LLM-based Simultaneous Translation System",
            "text": "However, as we demonstrate below, fine-tuning an LLM on such a causally aligned dataset enables it to achieve results comparable to some state-of-the-art baselines. \n\nIn order to causally align the source and target, we split each sentence using the word_tokenize function from the nltk package (Bird et al., 2009), treating punctuation marks as \"words\", then find the best correspondences between the source and target words with SimAlign (Jalili Sabet et al., 2020), and finally insert as many <WAIT> tokens into the target as appropriate. If after alignment the  If time is defined as the number of words from the beginning of the sentence, before alignment, some target words appear earlier than their corresponding English equivalents in the source. By inserting <WAIT> tokens (shown as \"@\"), we can shift those target words into the future, thereby achieving causality for every content word. \"_ _\" are fillers added at the end of the source sentence if neccessary to match its length with that of the target. \n\ntarget becomes longer than the source due to added <WAIT> tokens, we pad the source at the end with filler strings ensuring that the aligned source and target sentences have the same number of \"words\". These filler strings are only used for convenient batching and are dropped before tokenization. Supervised Fine-Tuning. We fine-tune the LLAMA-2 13B and and 70B models (Touvron et al., 2023) 1 to optimize the following objective: \n\nwhere y t is the next target token, y <t are previously generated (and committed) tokens and x \u2264t and the partial source tokens revealed up to the time step t. Following (Touvron et al., 2023), we zero out the loss on tokens corresponding the to system message and source, only backpropagating on the target tokens. \n\nWe use batches of prompt-response pairs collated in the following way.",
            "score": 0.325136822515799,
            "section_title": "Method",
            "char_start_offset": 10275,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 167,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1015
                },
                {
                    "start": 1018,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1449
                },
                {
                    "start": 1452,
                    "end": 1611
                },
                {
                    "start": 1612,
                    "end": 1767
                },
                {
                    "start": 1770,
                    "end": 1840
                }
            ],
            "ref_mentions": [
                {
                    "start": 448,
                    "end": 467,
                    "matchedPaperCorpusId": "215827461"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05572509765625
        },
        {
            "corpus_id": "270560332",
            "title": "Understanding the Collapse of LLMs in Model Editing",
            "text": "Despite significant progress in model editing methods, their application in real-world scenarios remains challenging as they often cause large language models (LLMs) to collapse. Among them, ROME is particularly concerning, as it could disrupt LLMs with only a single edit. In this paper, we study the root causes of such collapse. Through extensive analysis, we identify two primary factors that contribute to the collapse: i) inconsistent handling of prefixed and unprefixed keys in the parameter update equation may result in very small denominators, causing excessively large parameter updates; ii) the subject of collapse cases is usually the first token, whose unprefixed key distribution significantly differs from the prefixed key distribution in autoregressive transformers, causing the aforementioned issue to materialize. To validate our findings, we propose a simple yet effective approach: uniformly using prefixed keys during editing phase and adding prefixes during testing phase to ensure the consistency between training and testing. The experimental results show that the proposed solution can prevent model collapse while maintaining the effectiveness of the edits.",
            "score": 0.32476669161129035,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07122802734375
        },
        {
            "corpus_id": "56517207",
            "title": "What are the Biases in My Word Embedding?",
            "text": "To test whether the associations we find are larger than one would find if there was no relationship between the names X i and words A, we consider the following \"rotational null hypothesis\": the words in the embedding are generated through some process in which the alignment between names and words is random. This is formalized by imagining that a random rotation was applied (multiplying by a uniformly Haar random orthogonal matrix U ) to the word embeddings but not to the name embeddings.\n\nSpecifically, to compute p-value p i j for each (X i , A i j ), we first compute a score \u03c3 i j = (X i \u2212 \u00b5) \u00b7 (A i j \u2212 A). We then compute R = 10, 000 uniformly random orthogonal rotations U 1 , . . . , U R \u2208 R d\u00d7d , drawn according to the Haar measure. For each rotation, we simulate running our algorithm as if the name embeddings were transformed by U (while the word embeddings remain as is). For each rotation U r , the sets A i jr chosen to maximize (X i U r \u2212 \u00b5U r ) \u00b7 (w \u2212 A j ), and the corresponding V i jr and the resulting \u03c3 i jr are computed. Finally, p i j is the fraction of rotations for which the score \u03c3 i jr \u2265 \u03c3 i j (plus an add-1 penalty standard for Monte Carlo p-values).\n\nFurthermore, since the algorithm outputs many (hundreds) of name/word biases, the Benjamini-Hochberg [1995]   used to determine a critical p-value that guarantees an \u03b1 bound on the rate of false discoveries. Finally, to choose an output ordering on significant tests, the m tests are then sorted by the total scores \u03c3 i j over the pairs determined significant.",
            "score": 0.3244780451268607,
            "section_title": "Step 4: Computing p-values and ordering",
            "char_start_offset": 12269,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1273,
                    "end": 1298,
                    "matchedPaperCorpusId": "45174121"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0394287109375
        },
        {
            "corpus_id": "17652653",
            "title": "Generating Phrasal and Sentential Paraphrases: A Survey of Data-Driven Methods",
            "text": "The idea that a language possesses distributional structure was first discussed at length by Harris (1954). The term represents the notion that one can describe a language in terms of relationships between the occurrences of its elements (words, morphemes, phonemes) relative to the occurrence of other elements. The name for the phenomenon is derived from an element's distribution-sets of elements in particular positions that the element occurs with to produce an utterance or a sentence. \n\nMore specifically, Harris presents several empirical observations to support the hypothesis that such a structure exists naturally for language. Here, we closely quote these observations: \n\nr Utterances and sentences are not produced by arbitrarily putting together the elements of the language. In fact, these elements usually occur only in certain positions relative to certain other elements. \n\nr The empirical restrictions on the co-occurrents of a class are respected for each and every one of its members and are not disregarded for arbitrary reasons. \n\nr The occurrence of a member of a class relative to another member of a different class can be computed as a probabilistic measure, defined in terms of the frequency of that occurrence in some sample or corpus. \n\nr Not every member of every class can occur with every member of another class (think nouns and adjectives). This observation can be used as a measure of difference in meaning. For example, if the pair of words teacher and instructor is considered to be more semantically equivalent than, say, the pair teacher and musician, then the distributions of the first pair will also be more alike than that of the latter pair. \n\nGiven these observations, it is relatively easy to characterize the concept of distributional similarity: words or phrases that share the same distribution-the same set of words in the same context in a corpus-tend to have similar meanings. \n\nFigure 1 shows the basic idea behind phrasal paraphrase generation techniques that leverage distributional similarity. The input corpus is usually a single or set of monolingual corpora (parallel or non-parallel). After preprocessing-which may include tagging the parts of speech, generating parse trees, and other transformations-the next step is to extract pairs of words or phrases (or patterns) that occur in the same context in the corpora and hence may be considered (approximately) semantically equivalent.",
            "score": 0.3244780451268607,
            "section_title": "Distributional Similarity",
            "char_start_offset": 21157,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 491
                },
                {
                    "start": 494,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 681
                },
                {
                    "start": 684,
                    "end": 789
                },
                {
                    "start": 790,
                    "end": 889
                },
                {
                    "start": 892,
                    "end": 1051
                },
                {
                    "start": 1054,
                    "end": 1264
                },
                {
                    "start": 1267,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1686
                },
                {
                    "start": 1689,
                    "end": 1929
                },
                {
                    "start": 1932,
                    "end": 2050
                },
                {
                    "start": 2051,
                    "end": 2145
                },
                {
                    "start": 2146,
                    "end": 2445
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10504150390625
        },
        {
            "corpus_id": "1805910",
            "title": "Bootstrapping Syntax and Recursion using Alginment-Based Learning",
            "text": "Aligning two dissimilar sentences yields no structure. However, if we weaken the exact match between words in the alignment phase, it is possible to learn structure even with dissimilar sentences. \n\nInstead of linking exactly matching words, the algorithm should match words that are equivalent. One way of implementing this is by using equivalence classes. With equivalence classes, words that are closely related are grouped together. (Redington et al. (1998) describe an unsupervised way of finding equivalence classes.) \n\nWords that are in the same equivalence class are said to be sufficiently equivalent and may be linked. Now sentences that do not have words in common, but do have words from the same equivalence class in common, can be used to learn structure. \n\nWhen using equivalence classes, more constituents are learned since more terminals in constituents may be seen as similar (according to the equivalence classes). This results in structures containing more possible constituents from which the selection phase may choose.",
            "score": 0.3244780451268607,
            "section_title": "Weakening Exact Match",
            "char_start_offset": 21953,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 54
                },
                {
                    "start": 55,
                    "end": 196
                },
                {
                    "start": 199,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 523
                },
                {
                    "start": 526,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 769
                },
                {
                    "start": 772,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 1041
                }
            ],
            "ref_mentions": [
                {
                    "start": 437,
                    "end": 461,
                    "matchedPaperCorpusId": "2596605"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1041259765625
        },
        {
            "corpus_id": "260781930",
            "title": "Neural Latent Aligner: Cross-trial Alignment for Learning Representations of Complex, Naturalistic Neural Data",
            "text": "As a proxy for evaluating the alignment inference, we measure the behavioral coherence between aligned trials. We create additional validation and test datasets by sampling 2-second windows of source-target pairs (valid: 200 pairs, test: 1000 pairs) for each participant. To ensure that the sampled pairs have overlapping contents, we use the supervised alignment of trials to anchor the sampling.8 Then, we apply unsupervised warping to align the source to the target and check how the phonemes and AKTs are accurately aligned. \n\nWhen a phoneme in the source is warped, the distance from the reference phoneme in the target trial is measured. Then, the alignment is considered to be correct if the distance is below a threshold. As Figure 4 D, a curve of true positive rate (TPR) can be plotted by adjusting the threshold value, and the area under the curve (AUC:= area/threshold) indicates the sensitivity of the alignment. For AKT, we simply measure the average correlation between the target AKT and the warped source AKT. We additionally report the score by the supervised alignment as a reference for the upper bound  For our proposed TWM, the alignment is inferred by taking the centers of the output distributions (red line in Figure 4 A). For other models, the alignment is obtained by applying DTW on the representations. 9 The alignment inferred by TWM is smooth (Figure 4 A) but those of other models show a staircase pattern (Figure 4 B). When visually inspected in Figure 4 C, the source phonemes aligned by NLA (the 3rd row) show the highest coherence with the 9 See Appendix E.2 for selecting distance function for DTW. The correlations of the aligned AKT are also the highest in NLA (Table 3). This suggests that TWM provides the most accurate unsupervised alignment of behaviors among the methods compared.",
            "score": 0.3244780451268607,
            "section_title": "Empirical validation of time warping model",
            "char_start_offset": 19567,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 528
                },
                {
                    "start": 531,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 729
                },
                {
                    "start": 730,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1247
                },
                {
                    "start": 1248,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1451
                },
                {
                    "start": 1452,
                    "end": 1635
                },
                {
                    "start": 1636,
                    "end": 1710
                },
                {
                    "start": 1711,
                    "end": 1824
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0738525390625
        },
        {
            "corpus_id": "277781009",
            "title": "On the Value of Cross-Modal Misalignment in Multimodal Representation Learning",
            "text": "We conduct extensive experiments under diverse misalignment settings to validate our theoretical results, including numerical simulations ( \u00a7 5.1), a real-world image-text dataset with independent semantic variables ( \u00a7 5.2), and a synthetic dataset with dependent semantic variables. ( \u00a7 5.3).",
            "score": 0.3244780451268607,
            "section_title": "Experiments",
            "char_start_offset": 19385,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 294
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1412353515625
        },
        {
            "corpus_id": "201734855",
            "title": "Cross-lingual morphological inflection with explicit alignment",
            "text": "During training, we need to determine the goldstandard transduction actions, which requires aligning the lemmas and word forms. Better sequence alignment is one of the concerns for the similar inflection systems cited above, as well as the sequence-to-sequence models that operate with hard monotonic attention. Better alignments are also a common concern and studied extensively in other areas of computational linguistics such as dialectometry (Wieling et al., 2009;Proki\u0107, 2010) and historical linguistics (List, 2012;J\u00e4ger, 2013). Standard alignment algorithms that use equal penalties for edit operations often fail to capture the similarities and differences between characters (or phonetic segments). As a result, often a weighted method is used such that similar characters in one of the sequences are more likely to be aligned with the similar characters in the other. The weights are most often learned from the data using an unsupervised method. The data-driven weights are found to be more effective than manu-  ally assigned weights based on linguistic knowledge/intuitions (Sofroniev and \u00c7\u00f6ltekin, 2018). We tried a few of these more informed weighted alignment methods. However, in preliminary experiments, a simple alignment mechanism based on longest common substring (LCS) worked best. Hence, in all the experiments reported here, alignments are performed first by finding the longest common substring of lemma and the word from, and aligning the two strings such that the LCS is aligned correctly. The rest of the characters are aligned disregarding whether they match or not. The method introduces gaps only at the beginning and end of the sequences. If there are two matching substrings of equal length, we pick the first sequence. Figure 1 presents two example alignments based on the LCS and the edit distance. In the first example (top figure), minimum edit distance aligns substring ek, part of the infinitive marker mek used in verbal lemmas in the data set, to a substring string that matches accidentally in the word form. The intuition here is that even if we do not have a good reason for aligning the infinitive marker mek with the initial part of the suffix, doing this consistently facilitates learning.",
            "score": 0.3244780451268607,
            "section_title": "Alignment",
            "char_start_offset": 4237,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 707
                },
                {
                    "start": 708,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1184
                },
                {
                    "start": 1185,
                    "end": 1303
                },
                {
                    "start": 1304,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1670
                },
                {
                    "start": 1671,
                    "end": 1752
                },
                {
                    "start": 1753,
                    "end": 1833
                },
                {
                    "start": 1834,
                    "end": 2050
                },
                {
                    "start": 2051,
                    "end": 2236
                }
            ],
            "ref_mentions": [
                {
                    "start": 446,
                    "end": 468,
                    "matchedPaperCorpusId": "3580297"
                },
                {
                    "start": 509,
                    "end": 521,
                    "matchedPaperCorpusId": "16803747"
                },
                {
                    "start": 521,
                    "end": 533,
                    "matchedPaperCorpusId": "59568092"
                },
                {
                    "start": 1087,
                    "end": 1117,
                    "matchedPaperCorpusId": "53135982"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.047943115234375
        },
        {
            "corpus_id": "17563796",
            "title": "Bootstrapping structure into language : alignment-based learning",
            "text": "The algorithms described so far are unable to learn any structure when two sentences with completely distinct words are considered. Since unequal parts of sentences are stored as hypotheses, only the entire sentences (which have no words in common) are hypotheses. In other words, for a hypothesis to be introduced, there need to be equal words in the sentences. However, other sentences in the corpus (which do have words in common) can be used to learn structure in the two distinct sentences.\n\nSometimes it is too strong a demand to require equal words in the two sentences to find hypotheses; it is enough to have similar words. Imagine sentences 38a and 38b, which are completely distinct. The standard ABL learning methods would conclude that both are sentences, but no more structure will be found. Now assume that the algorithm knows that Book and Show are words of the same type (representing verbs), it would find the structures in sentences 39a and 39b. An obvious way of implementing this is by using equivalence classes (for example the system as described in (Redington et al., 1998)). Words that are closely related (in a syntactic or semantic perspective) are grouped together in the same class. Words that are in the same equivalence class are said to be sufficiently equal, so the alignment algorithm may assume they are equal and may thus link them. Sentences that do not have words in common, but do have words in the same equivalence class in common, can now be used to learn structure.\n\nA great advantage of using equivalence classes is that they can be learned in an unsupervised way. This means that when the algorithm is extended with equivalence classes, it still does not need to be initialised with structured training data.\n\nAnother way of looking at weakening the exact match is by comparing it to the second phase of the EMILE system, the rule induction. That phase introduces new grammar rules by applying already known grammar rules to unstructured parts of sentences. In other words, if there are grammar rules that rewrite type 2 into Book and into Show, then the words Book and Show are also possibly word groups of type 2, meaning that they are similar enough to be linked. This again results in the sentences in 39.\n\nIf equivalence classes or EMILE's rule induction phase are used in the alignment learning phase, more",
            "score": 0.3244780451268607,
            "section_title": "Weakening exact match",
            "char_start_offset": 169422,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1073,
                    "end": 1097,
                    "matchedPaperCorpusId": "2596605"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06475830078125
        },
        {
            "corpus_id": "236929787",
            "title": "Eelbrain: A Python toolkit for time-continuous analysis with temporal response functions",
            "text": "The analysis of linguistic representations, which cannot be derived directly from the sound waveform, commonly relies on forced alignment, a method that infers time-stamps for phoneme and word boundaries by comparing the sound waveform with a transcript. An example of an open source forced aligner with extensive documentation is the Montreal Forced Aligner (e.g. McAuliffe et al., 2017). Because the forced alignment procedure requires some additional steps that are well documented by the respective aligners we skip it here, and instead use the word-onset time-stamps provided with the Alice dataset. \n\nDiscrete predictors come in two varieties: constant magnitude impulses and variable magnitude impulses (see Figure 3, lower half). Constant magnitude impulses always have the same magnitude, for example an impulse of magnitude 1 at each word onset. Such a predictor implements the hypothesis that all words are associated with a shared characteristic brain response, similar to an event-related potential (ERP). The TRF estimation algorithm will then determine the latencies relative to those impulses at which the brain exhibits a consistent response. Variable magnitude impulses implement the hypothesis that the brain response to each word varies systematically with some quantity. For example, the N400 is assumed to co-vary with how surprising the word is in its context. A predictor with an impulse at each word onset, whose magnitude is equal to the surprisal of that word, will enable predicting a stereotyped response to words whose amplitude linearly varies with word surprisal. The linking hypothesis here is that for each event, the brain responds with population activity that scales in amplitude with how surprising that event is, or how much new information it provides (see e.g. Brodbeck et al., 2022). \n\nThe Alice dataset comes with a table including all time-stamps and several linguistic properties (stimuli/AliceChapterOne-EEG.csv). Each row of this table represents one word, and contains the time point at which the word starts in the audio file, as well the surprisal values that were used to relate the EEG signal to several language models in the original analysis (Brennan and Hale, 2019).",
            "score": 0.32443582783011415,
            "section_title": "Discrete predictor variables",
            "char_start_offset": 25195,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 604
                },
                {
                    "start": 607,
                    "end": 737
                },
                {
                    "start": 738,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1383
                },
                {
                    "start": 1384,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1801
                },
                {
                    "start": 1802,
                    "end": 1825
                },
                {
                    "start": 1828,
                    "end": 1959
                },
                {
                    "start": 1960,
                    "end": 2222
                }
            ],
            "ref_mentions": [
                {
                    "start": 365,
                    "end": 388,
                    "matchedPaperCorpusId": "12418404"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0256195068359375
        },
        {
            "corpus_id": "277104430",
            "title": "Using the Tools of Cognitive Science to Understand Large Language Models at Different Levels of Analysis",
            "text": "Large language models such as GPT-4 have been trained to identify situations that involve expressing explicit biases. However, it is possible to construct simple prompts that reveal that they still have strong implicit biases, as reflected in their associations between words. \n\nThese implicit biases have consequences for their downstream decisions as well. \n\nfaster to the statement \"a canary can sing\" than to \"a canary can fly.\" This is because the latter requires traversing two degrees of association: \"a canary is a bird\" and \"a bird can fly\" (Collins and Loftus, 1975). This kind of measure is also prevalent in examining attitudes toward social groups (Fazio and Olson, 2003;Greenwald and Banaji, 1995). For instance, the Implicit Association Test (IAT) aligns pairs of social group labels, like \"Black\" or \"White\", with adjectives like \"wonderful\" or \"terrible\" (Greenwald et al., 1998). \n\nEmpirical studies have repeatedly shown that human participants react faster to minority labels paired with negative adjectives, revealing underying mental associations about social groups that also predict other aspects of behavior such as the frequency of interacting with members of these groups (see meta-analysis by Kurdi et al., 2019). \n\nThe key insight behind these approaches is that it is possible to elicit mental associations without directly asking the participant for a verbal report. In some cases, researchers aim to capture unobtrusive or unconscious responses (Graf and Schacter, 1985;Schacter, 1987); in others, they strive to minimize self-presentation biases, such as fear of appearing unfair (Fazio and Olson, 2003;Gaertner and McLaughlin, 1983). The success of these methods in achieving these goals suggests that they may also be useful in analyzing the behavior of value-aligned LLMs. The hypothesis is that since alignment trains LLMs to conceal their true representations, methods that bypass direct rating scales or evaluative judgments may better expose their underlying associations. To test this, we adapted the Implicit Association Test for LLMs by prompting various models to associate word pairs used in earlier human studies (Bai et al., 2024).",
            "score": 0.3242433376494134,
            "section_title": "Figure 5",
            "char_start_offset": 25765,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 276
                },
                {
                    "start": 279,
                    "end": 358
                },
                {
                    "start": 361,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 897
                },
                {
                    "start": 900,
                    "end": 1241
                },
                {
                    "start": 1244,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1667
                },
                {
                    "start": 1668,
                    "end": 1808
                },
                {
                    "start": 1809,
                    "end": 2012
                },
                {
                    "start": 2013,
                    "end": 2178
                }
            ],
            "ref_mentions": [
                {
                    "start": 550,
                    "end": 576,
                    "matchedPaperCorpusId": "14217893"
                },
                {
                    "start": 661,
                    "end": 684,
                    "matchedPaperCorpusId": "8797951"
                },
                {
                    "start": 684,
                    "end": 711,
                    "matchedPaperCorpusId": "8194189"
                },
                {
                    "start": 872,
                    "end": 896,
                    "matchedPaperCorpusId": "7840819"
                },
                {
                    "start": 1502,
                    "end": 1517,
                    "matchedPaperCorpusId": "3728984"
                },
                {
                    "start": 1613,
                    "end": 1636,
                    "matchedPaperCorpusId": "8797951"
                },
                {
                    "start": 1636,
                    "end": 1666,
                    "matchedPaperCorpusId": "140763824"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.120849609375
        },
        {
            "corpus_id": "272910933",
            "title": "Inference-Time Language Model Alignment via Integrated Value Guidance",
            "text": "The token-wise sampling and chunk-level beam search strategies can be combined to enhance the alignment of large language models with human preferences. Specifically, token-wise sampling adjusts the sampling distribution of the next token using the implicit value function, while chunk-level beam search ranks candidate sequences using the explicit value function. \n\nThe IVG algorithm is illustrated in Figure 2. The key insight is that by applying the implicit value function at the token level and the explicit value function at the chunk level, we effectively leverage the strengths of both. Compared to Weak-to-Strong Search (Zhou et al., 2024c), we sample tokens from a policy adjusted by the implicit value function rather than the base policy and use the explicit value function to rank candidate sequences. Empirically, we find that this combination leads to better alignment with human preferences. We demonstrate the effectiveness of IVG in the following sections.",
            "score": 0.3242156756669822,
            "section_title": "Integrated Value Guidance",
            "char_start_offset": 9534,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 364
                },
                {
                    "start": 367,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 974
                }
            ],
            "ref_mentions": [
                {
                    "start": 629,
                    "end": 649,
                    "matchedPaperCorpusId": "270095324"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.235595703125
        },
        {
            "corpus_id": "272690234",
            "title": "Generalizing Alignment Paradigm of Text-to-Image Generation with Preferences through f-divergence Minimization",
            "text": "Evaluations are carried out to examine the performance of image-text alignment, human value alignment, and generation diversity, which also aim to discern the certain divergence most effectively balances these three aspects. Our results indicate that Jensen-Shannon divergence successfully strikes the ideal equilibrium among the three criteria examined, while also achieving the highest standard in human value alignment performance. Therefore, in text-to-image alignment, judicious selection of the divergence constraint, tailored to the specific alignment requirements, is paramount. In Figure 1, we present several images generated by the model that have been aligned under the Jensen-Shannon divergence. \n\nTo the best of our knowledge, this is the first work to apply different divergence constraints to text-to-image alignment paradigm. Our contributions are summarized as follows: (1) Generalized alignment formula: we propose a generalized formula for text-to-image generation alignment, aiming to provide more choices on divergence constraints in alignment execution. (2) Thorough alignment process analysis: we comprehensively analyze the impact of different divergence constraints on alignment process from the perspective of gradient fields. (3) Extensive alignment evaluations: we conducted extensive evaluations on text-to-image generation alignment, meticulously assessing both alignment performance (image-text alignment and human value alignment) and generation diversity. \n\n2 Related Work Recently, inspired by the alignment approaches based on human preferences, notably exemplified by methods such as direct preference optimization (DPO) [11], eliminating the need for explicit reward models and showing their significant success on Large Language Models (LLMs), and then garnering substantial attention within the community on the development of offline alignment for text-to-image diffusion models. Diffusion-DPO [12] enables text-to-image diffusion models to directly learn from human feedback in an open-vocabulary setting, and fine-tunes them on the contains Pick-a-Pic [6] dataset with image preference pairs. Direct Preference for Denoising Diffusion Policy Optimization (D3PO) [13] proposes a method on generating pairs of images from the same prompt and identifying the preferred and dispreferred images with the help of human evaluators.",
            "score": 0.3239983294068382,
            "section_title": "Introduction",
            "char_start_offset": 4228,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 708
                },
                {
                    "start": 711,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1489
                },
                {
                    "start": 1492,
                    "end": 1920
                },
                {
                    "start": 1921,
                    "end": 2135
                },
                {
                    "start": 2136,
                    "end": 2367
                }
            ],
            "ref_mentions": [
                {
                    "start": 1077,
                    "end": 1080,
                    "matchedPaperCorpusId": "268247980"
                },
                {
                    "start": 1658,
                    "end": 1662,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 1935,
                    "end": 1939,
                    "matchedPaperCorpusId": "265352136"
                },
                {
                    "start": 2095,
                    "end": 2098,
                    "matchedPaperCorpusId": "258437096"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06927490234375
        },
        {
            "corpus_id": "16485843",
            "title": "Improving Bitext Word Alignments via Syntax-based Reordering of English",
            "text": "For each language we treated, we assembled sentence-aligned, tokenized training and test corpora, with hand-annotated gold-standard word alignments for the latter 1 . We did not apply any sort of morphological analysis beyond basic word tokenization. We measured system performance with wa eval align.pl, provided by Rada Mihalcea and Ted Pedersen. \n\nEach training set provides the aligner with information about lexical affinities and reordering patterns. For Hindi, Korean and Chinese, we also tested our system under the more difficult situation of having only a bilingual word list but no bitext available. This is a plausible low-resource language scenario  and a test of the ability of the system to take sole responsibility for knowledge of reordering. Table 3 describes the test sets and shows the correlation in gold standard aligned word pairs between the position of the English word in the English sentence and the position of the source-language word in the source-language sentence (normalizing the positions to fall between 0 and 1). The baseline (direct) correlations give quantitative evidence of fering degrees of syntactic divergence with English, and the English correlations demonstrate that our heuristics do have the effect of better fitting source language word order. Figures 4, 5, 6 and 7 show learning curves for systems trained on parallel sentences with and without the English transforms. Table 2 provides further detail, and also shows the performance of systems trained without any bitext, but only with access to a bilingual translation lexicon. Our system achieves consistent, substantial performance improvement under all situations for English-Hindi and English-Korean language pairs, which exhibit longer distance SOV\u2192SVO syntactic divergence. For English-Romanian and English-Chinese, neither significant improvement nor degradation is seen, but these are language pairs with quite similar sentential word order to English, and hence have less opportunity to benefit from our syntactic transformations.",
            "score": 0.3239115605711198,
            "section_title": "Experiments",
            "char_start_offset": 5607,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 348
                },
                {
                    "start": 351,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1578
                },
                {
                    "start": 1579,
                    "end": 1780
                },
                {
                    "start": 1781,
                    "end": 2040
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05389404296875
        },
        {
            "corpus_id": "273502189",
            "title": "A Complexity-Based Theory of Compositionality",
            "text": "In virtue of being quantitatively precise, representational compositionality can be used to investigate compositionality in real-world systems. We demonstrated this in the case of emergent and natural language representations where the sentences describing a representation are externally defined. We note that in future work this quantity can readily be applied to score tokenization schemes and may help guide improvements in tokenizer design, but even more intriguing is the possibility to operationalize our proposed measure C L to regularize and otherwise bias next generation tokenizers. We leave the exploration of this topic to future work. \n\nMore generally however, measuring the compositionality of representations without a predefined mapping to sentences requires the development of additional machine learning tools, whose overall architecture we sketch out in Appendix B. \n\nThe development of such tools is an important direction for future work, as it will allow us to investigate the compositionality of representations that emerge from different learning objectives, neural architectures, inductive biases, and brain regions. In turn, we will be able to see how representational compositionality empirically relates to other topics in ML such as compositional generalization, multi-task generalization, and latent space generative models-we give some hypotheses and ideas for future work along these lines in Appendix E. In particular, representational compositionality has the potential to explain the success of varied methods because it defines compositionality through compression, which abstracts across the architecture, learning details, and particular representational format of a model. Representational compositionality can therefore be used to validate or reject diverse hypotheses about compositionality, such as the Language of Thought hypothesis (Fodor, 1975). \n\nRepresentational compositionality can also play an important role in the design and validation of machine learning models with principled inductive biases for compositionality. Namely, in addition to supporting a given task, a compositional representation must be easily describable as a simple function of constituent parts. There are both direct and indirect ways to achieve this that are grounded in our definition, and we describe some approaches in Appendix F that we intend to pursue in future work. \n\nwhere we have equality when p \u03b8 = p. This insight is significant.",
            "score": 0.32386372029512506,
            "section_title": "PREPRINT",
            "char_start_offset": 42771,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 648
                },
                {
                    "start": 651,
                    "end": 885
                },
                {
                    "start": 888,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1712
                },
                {
                    "start": 1713,
                    "end": 1891
                },
                {
                    "start": 1894,
                    "end": 2070
                },
                {
                    "start": 2071,
                    "end": 2219
                },
                {
                    "start": 2220,
                    "end": 2399
                },
                {
                    "start": 2402,
                    "end": 2438
                },
                {
                    "start": 2439,
                    "end": 2467
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.120361328125
        },
        {
            "corpus_id": "267523425",
            "title": "Six Fallacies in Substituting Large Language Models for Human Participants",
            "text": "The rise of large language models (LLMs) that generate human-like text has sparked debates over their potential to replace human participants in behavioral and cognitive research. We critically evaluate this replacement perspective to appraise the fundamental utility of language models in psychology and social science. Through a five-dimension framework, characterization, representation, interpretation, implication, and utility, we identify six fallacies that undermine the replacement perspective: (1) equating token prediction with human intelligence, (2) assuming LLMs represent the average human, (3) interpreting alignment as explanation, (4) anthropomorphizing AI, (5) essentializing identities, and (6) purporting LLMs as primary tools that directly reveal the human mind. Rather than replacement, the evidence and arguments are consistent with a simulation perspective, where LLMs offer a new paradigm to simulate roles and model cognitive processes. We highlight limitations and considerations about internal, external, construct, and statistical validity, providing methodological guidelines for effective integration of LLMs into psychological research, with a focus on model selection, prompt design, interpretation, and ethical considerations. This perspective reframes the role of language models in behavioral and cognitive science, serving as linguistic simulators and cognitive models that shed light on the similarities and differences between machine intelligence and human cognition and thoughts.",
            "score": 0.32377160855628223,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1033935546875
        },
        {
            "corpus_id": "225062312",
            "title": "Unsupervised Data Augmentation with Naive Augmentation and without Unlabeled Data",
            "text": "We use the back-translation machinery described by Xie et al. (2019). Specifically, this entails use of WMT'14 English-French (Bojar et al., 2014) translation models in both directions with random sampling with a tunable temperature in place of beam search for generation. We set our temperature to 0.9, one of the recommended settings in prior work (Xie et al., 2019).\n\nMasked Language Model Back-translation is not suitable for use in UDA for sequence tagging tasks, as in such tasks labels apply to tokens and it is not obvious how to align tokens in a given paraphrase with those in the original text. More specifically, as we have defined it for sequence tagging (Equation 3), consistency loss penalizes dissimilarity between model predictions p(y j |x) and p(y j |q(x)) for all indices j. However, when q is defined as a back-translation process, there is no expectation that x's and q(x)'s ground-truth labeling will be aligned. They may not even be of the same length. Therefore, we instead consider word replacement strategies (at each index j), including (i.i.d.) random replacement, and a model-based word replacement strategy that ensures alignment between x and q(x). Both of these involve individual word substitutions, so x and q(x) will have the same length. For the model-based replacement strategy, we again define q such that it replaces a given word in x with probability p (otherwise copying from x). However, here we select x j using a masked language model. Specifically, we mask x j and use BERT (Devlin et al., 2018) to induce a probability distribution over all possible words (in its vocabulary) that might appear at position j. We then draw x j from the ten most probable words (excluding the original word x j ) with probabilities proportional to the likelihood assigned to these words by BERT. We hypothesize that this method will provide a substantially greater expectation of validity than random replacement, on the assumption that BERT is sensitive enough to context that it is likely to replace words of one category with other words of the same category.\n\nIn Table 1 we show examples",
            "score": 0.32375771214378674,
            "section_title": "Back-Translation",
            "char_start_offset": 7920,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 126,
                    "end": 146,
                    "matchedPaperCorpusId": "15535376"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05340576171875
        },
        {
            "corpus_id": "269362100",
            "title": "Prefix Text as a Yarn: Eliciting Non-English Alignment in Foundation Language Model",
            "text": "Furthermore, given the potential risk of SFT leading to the forgetting of pre-training knowledge, the question of how to achieve cross-lingual alignment without training remains underexplored.\n\nTo bridge these gaps, our study conducts an indepth examination of the impact of SFT on crosslingual generation.We investigate the influence of SFT on the decoding patterns of foundation models in cross-lingual contexts, hypothesizing that the success of SFT largely hinges on the selection of initial prior tokens that are critical for eliciting taskspecific generation in the target language.Furthermore, the observed decoding similarities between Instruction: Translate the following sentence from English to Ukrainian: \"We now have 4-month-old mice that are non-diabetic that used to be diabetic,\" he added.foundation and SFT models support the extension of the superficial alignment hypothesis to crosslingual scenarios.Responding to these insights, we introduce a training-free alignment method named \"PRETTY\" for cross-lingual and non-English tasks.\n\nThe Prefix TexTs act as a Yarn (PRETTY) linking the foundation LLM and the SFT LLM, eliciting the foundation LLM to exhibit near-SFT performance levels.Specifically, we augment the original input with a few tokens that serve as decoding priors, and then prompt the foundation LLM to resume decoding based on this modified input.In most cases, only one or two task-related prior tokens are needed, and the method for constructing these prior tokens is flexible across various kinds of language resources, fostering the democratization of multilingual LLMs.\n\nWe conducted experiments on machine translation (Goyal et al., 2022), cross-lingual summarization (Bhattacharjee et al., 2023) and non-English part-of-speech (POS) tagging (Liang et al., 2020) tasks across eight languages.These tasks exemplify cross-lingual generation and multilingual language understanding, and they provide ample non-English test data to evaluate effectiveness across varying levels of resource availability.",
            "score": 0.32374599732608306,
            "section_title": "Introduction",
            "char_start_offset": 1784,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 194,
                    "end": 306
                },
                {
                    "start": 306,
                    "end": 588
                },
                {
                    "start": 588,
                    "end": 805
                },
                {
                    "start": 805,
                    "end": 919
                },
                {
                    "start": 919,
                    "end": 1050
                },
                {
                    "start": 1052,
                    "end": 1204
                },
                {
                    "start": 1204,
                    "end": 1380
                },
                {
                    "start": 1380,
                    "end": 1607
                },
                {
                    "start": 1609,
                    "end": 1831
                },
                {
                    "start": 1831,
                    "end": 2037
                }
            ],
            "ref_mentions": [
                {
                    "start": 1707,
                    "end": 1735,
                    "matchedPaperCorpusId": "258947845"
                },
                {
                    "start": 1781,
                    "end": 1801,
                    "matchedPaperCorpusId": "214794966"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1885986328125
        },
        {
            "corpus_id": "17428739",
            "title": "Book Reviews: Semantic Similarity from Natural Language and Ontology Analysis by S\u00e9bastien Harispe, Sylvie Ranwez, Stefan Janaqi, and Jacky Montmain",
            "text": ".\n\nThe relevance of the distributional hypothesis and distributional semantics has been stressed out of linguistics, several contributions in psychology and cognitive sciences use it for studying knowledge acquisition and memory to cite a few. Indeed, the distribution hypothesis has been linked to cognitive processes related to language acquisition, processing and understanding (McDonald and Ramscar, 2001). It has for instance been shown that statistical properties of language, and more particularly statistical relationships between neighbouring speech sounds, plays an important role in word segmentation in infants, and therefore speech acquisition (Saffran et al., 1996). Word distribution analysis has also been identified as an interesting evidence to derive syntactic categories (Redington et al., 1998).\n\nTherefore distributional semantics became an approach of choice to model semantics of words w.r.t their usage contexts. This enthusiasm for distributional semantics relies on the statistical foundation of the approach which generally requires little expensive human supervision and therefore makes distributional semantics particularly adapted to large corpora analysis. In addition, this approach has been proved to be particularly interesting for solving NLP problems. Indeed, based on the distributional hypothesis, several distributional models have been proposed, e.g. topic models such as Latent Semantic Analysis (LSA), or Hypertext Analogue to Language (HAL) -they will be introduced later. As we will see, these distributional models are extensively and successfully used to analyse semantic relatedness of words -they have also proved to be particularly successful to perform a variety of NLP tasks and to design information retrieval systems 13 .\n\nThus, distributional semantics provides a theoretical framework for assessing the semantic similarity or relatedness of words by means of distributional analysis, i.e. by considering an implementation of the distributional hypothesis. Otherwise stated, it is often implicitly considered that distributional similarity of words is equivalent to semantic similarity. However, note that in the study of distributional semantics, some authors distinguish distributional similarity of words to their semantic similarity. This is in particular the case when specific definitions of semantic similarity and distributional similarity are considered. As an example, in (Weeds, 2003, chap 1.), the author considers that (i) the notion of semantic similarity is defined in terms of inter-substituability, i.e. regarding the impact to substitute a word by another on the meaning of a",
            "score": 0.3235956830422462,
            "section_title": "Distributional semantics",
            "char_start_offset": 96633,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 381,
                    "end": 409,
                    "matchedPaperCorpusId": "13161362"
                },
                {
                    "start": 657,
                    "end": 679,
                    "matchedPaperCorpusId": "13321604"
                },
                {
                    "start": 791,
                    "end": 815,
                    "matchedPaperCorpusId": "2596605"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08660888671875
        },
        {
            "corpus_id": "269362100",
            "title": "Prefix Text as a Yarn: Eliciting Non-English Alignment in Foundation Language Model",
            "text": "While supervised fine-tuning (SFT) has been a straightforward approach for tailoring the output of foundation large language model (LLM) to specific preferences, concerns have been raised about the depth of this alignment, with some critiques suggesting it is merely\"superficial\". We critically examine this hypothesis within the scope of cross-lingual generation tasks, proposing that the effectiveness of SFT may be constrained by its reliance on prior tokens to guide cross-lingual generation. Based on this crucial insight, and in response to the challenges posed by the costly and limited availability of non-English data for SFT, we introduce a novel training-free alignment method named PreTTY, which employs minimal task-related prior tokens to bridge the foundation LLM and the SFT LLM, achieving comparable performance without training. Experiments on machine translation and part-of-speech tagging across eight languages demonstrate the efficacy of PreTTY in cross-lingual settings. Remarkably, by initiating the decoding process with only one or two prior tokens, foundation LLMs can achieve performance comparable to their SFT counterparts. This method presents a cost-effective alternative to SFT and advances the democratization of multilingual LLMs.",
            "score": 0.32346817996375044,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.116943359375
        },
        {
            "corpus_id": "267740713",
            "title": "Multi-modal preference alignment remedies regression of visual instruction tuning on language model",
            "text": "The superficial alignment hypothesis states that a model's knowledge capabilities are largely acquired during its initial pre-training stage (Zhou et al., 2023). A corollary of this hypothesis is that alignment tuning refines the model output generation with a preferred response format rather than knowledge acquisition. As a result, models can be effectively realigned post-visual instruction using a relatively small set of examples (Kirstain et al., 2022). This principle applies to MLLMs as well, which acquire multi-modal knowledge representation via visual instruction tuning (Liu et al.,  2023b). However, existing work mixed large-scale text instruction data (518K out of 1.23 million in the case of mPlug-OWL 2 and 40K in the case of LLaVA-1.5). We hypothesize that the data inefficiency above is attributed to the underlying alignment strategy and demonstrate that one would need only a small alignment dataset so long as a proper alignment strategy such as DPO is utilized. \n\nAs suggested by Table 4, Direct Preference Optimization (DPO) emerges as a computationally efficient solution for enhancing model performance in the mixed-modal alignment space. Unlike the mixing text instruction as described above or LLaVA-RLHF, which used a large 82K dataset and complex training pipeline involving reward modeling and PPO, DPO achieves significant improvements in language capabilities with a smaller dataset and one-stop training setup. A notable advantage of DPO is its minimal alignment tax, which curtails the degradation of existing knowledge, as evidenced by its performance on benchmarks like MM-Bench, where DPO shows minimal impact. This method not only enables effective alignment of multi-modal models post-visual instruction tuning but also ensures the preservation of model performance. Our methodology exhibits notable proficiency in value alignment and data efficiency, yet it is imperative to acknowledge certain limitations and potential risks. One key consideration is the scalability of our approach. While our data scaling analysis suggests significant improvements up to a 6K preference dataset, the full extent of scalability beyond this threshold remains unexplored.",
            "score": 0.3233646147903816,
            "section_title": "Multi-modal preference alignment as a data-efficient remedy to instruction tuning capabilities",
            "char_start_offset": 26950,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 985
                },
                {
                    "start": 988,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1445
                },
                {
                    "start": 1446,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1807
                },
                {
                    "start": 1808,
                    "end": 1969
                },
                {
                    "start": 1970,
                    "end": 2027
                },
                {
                    "start": 2028,
                    "end": 2197
                }
            ],
            "ref_mentions": [
                {
                    "start": 436,
                    "end": 459,
                    "matchedPaperCorpusId": "238583118"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.146240234375
        },
        {
            "corpus_id": "992216",
            "title": "Capturing divergence in dependency trees to improve syntactic projection",
            "text": "Fig. 3 Simple but frequent example of 1-to-many German-English alignment found in the Sophie's World portion of the SMULTRON corpus (Volk et al., 2010). \n\nSome of the initial research on the subject of projecting word-level annotation from one language to another was published in Yarowsky and Ngai (2001). Here, the authors used IBM Model 3 (Brown et al., 1990) to align large parallel corpora between English-Chinese and English-French. A POS tagger was trained for French using projection from English, and NP bracketers were trained similarly for both French and Chinese. The authors identified noisy statistical alignment and 1-to-many alignments as two main issues in performing projection. The first of these issues is indeed a difficult problem for resource-poor languages, as high-quality statistical word alignment often requires much more bitext than the data available for the language. While it is not a full solution to the problem, many of the language pairs we use in this work are drawn from collection of interlinear glossed text (IGT), as shown in Figure 1, which provides unique shortcuts for obtaining word alignment with a small amount of data. IGT will be discussed further in \u00a72.2. \n\nThe second issue of 1-to-many alignments is one that may be the result of linguistic divergence between a language pair where the source is morphologically richer than the target. In cases such as this, finding common patterns of conflation can be useful for generalizing a projection to new data. For instance, Fig. 3 shows a very simple but common case of conflation in the SMULTRON corpus (Volk et al., 2010) where a single German word aligns to multiple English words. Using direct projection alone, the same POS tag would be projected to both English tokens. In this case, using a universal tagset such as those presented by Petrov et al. (2012) could help alleviate the problem, but for more complex cases, learning the pattern would be more critical.",
            "score": 0.32325001473216797,
            "section_title": "NE NN",
            "char_start_offset": 3959,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 155,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1166
                },
                {
                    "start": 1167,
                    "end": 1205
                },
                {
                    "start": 1208,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1680
                },
                {
                    "start": 1681,
                    "end": 1771
                },
                {
                    "start": 1772,
                    "end": 1965
                }
            ],
            "ref_mentions": [
                {
                    "start": 281,
                    "end": 305,
                    "matchedPaperCorpusId": "1227006"
                },
                {
                    "start": 342,
                    "end": 362,
                    "matchedPaperCorpusId": "14386564"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0863037109375
        },
        {
            "corpus_id": "261705563",
            "title": "RAIN: Your Language Models Can Align Themselves without Finetuning",
            "text": "However, the method is sample-inefficient (as tested by our experiments). \n\nThe problem of LLM alignment becomes more challenging when we require the model to be aligned by itself without external supervision, a.k.a. self-alignment. Although LLMs often generate responses that do not align with human values, LLMs are \"aware\" that their outputs are inappropriate, as evidenced in RLAIF. Studies such as RLAIF and Self-Alignment (Sun et al., 2023) capitalize on this by employing pre-trained LLMs to annotate or generate data, followed by finetuning. Our findings suggest that the self-annotation and finetuning process, often utilized in these works, is capable of being omitted. By integrating evaluation and the rewind mechanism, frozen LLMs can directly generate responses that are consistent with human values. \n\nTo this end, in the model's inference phase, we implement a self-evaluation strategy to appraise the generated text. Guided by these evaluation outcomes, we enact a rewindable process that facilitates retracing steps. Our inference method-Rewindable Auto-regressive INference (RAIN)-mirrors human behavioral patterns: contemplating, weighing, and reflecting on the consequences before speaking. Unlike the \"generate-evaluate-regenerate\" loop that relies on probabilities derived from the language model, RAIN integrates self-evaluation for heuristic forward-looking searches. During the search, it steers towards more optimal directions through attribute updates, and after the search, adjusted probabilities for the next tokens are obtained (see Figure 3). Empirical findings underscore the capacity of our method to elevate language model performance, all achieved without the need for parameter updates or reliance on any labeled or unlabeled data. For example, on the Anthropic's Helpfulness and Harmlessness (HH) dataset, RAIN improves the harmlessness rate of LLaMA 30B from 82% of the vanilla auto-regressive inference to 97%, while maintaining the helpfulness rate (see Figure 2).",
            "score": 0.3232051205085762,
            "section_title": "Introduction",
            "char_start_offset": 2439,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 73
                },
                {
                    "start": 76,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 814
                },
                {
                    "start": 817,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 1034
                },
                {
                    "start": 1035,
                    "end": 1211
                },
                {
                    "start": 1212,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1768
                },
                {
                    "start": 1769,
                    "end": 2005
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06842041015625
        },
        {
            "corpus_id": "207780212",
            "title": "When Choosing Plausible Alternatives, Clever Hans can be Clever",
            "text": "One of the simplest types of superficial cues are unbalanced token distributions, i.e tokens appearing more often or less frequently with one particular instance label than with other labels. For example, Niven and Kao (2019) found that the token not occurs more often in one type of instance an argumentation dataset. \n\nSimilarly we identify superficial cues -in this case a single token that appears more frequently in correct alternatives or wrong alternatives -in the COPA training set. To find superficial cues in the form of predictive tokens, we use the following measures, defined by Niven and Kao (2019). Let T (i) j be the set of tokens in the alternatives for data point i with label j. The applicability \u03b1 k of a token k counts how often this token occurs in an alternative with one label, but not the other: \n\nThe productivity \u03c0 k of a token is the proportion of applicable instances for which it predicts the correct answer: \n\nFinally, the coverage \u03be k of a token is the proportion of applicable instances among all instances: \n\nTable 2 shows the five tokens with highest coverage. For example, a is the token with the highest coverage and appears in either a correct alternative or wrong alternative in 21.2% of COPA training instances. Its productivity of 57.5% expresses that it appears in in correct alternatives 7.5% more often than expected by random chance. This suggests that a model could rely on such unbalanced distributions of tokens to predict answers based only on alternatives without understanding the task. \n\nTo test this hypothesis, we perform a dataset ablation, providing only the two alternatives as input to RoBERTa, but not the premise, following similar ablations by Gururangan et al. (2018); Niven and Kao (2019). RoBERTa trained5 in this setting, i.e. on alternatives only, achieves a mean accuracy of 59.6 (\u00b1 2.3). This is problematic because COPA is designed as a choice between alternatives given the premise. Without a premise given, model performance should not exceed random chance. Consequently, a result better than random chance shows that the dataset allows solving the task in a way that was not intended by its creators. 3 Balanced COPA (B-COPA)",
            "score": 0.3231205035933473,
            "section_title": "Token Distribution",
            "char_start_offset": 7004,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 318
                },
                {
                    "start": 321,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 613
                },
                {
                    "start": 614,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 820
                },
                {
                    "start": 823,
                    "end": 938
                },
                {
                    "start": 941,
                    "end": 1040
                },
                {
                    "start": 1043,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1537
                },
                {
                    "start": 1540,
                    "end": 1752
                },
                {
                    "start": 1753,
                    "end": 1855
                },
                {
                    "start": 1856,
                    "end": 1952
                },
                {
                    "start": 1953,
                    "end": 2028
                },
                {
                    "start": 2029,
                    "end": 2172
                },
                {
                    "start": 2173,
                    "end": 2197
                }
            ],
            "ref_mentions": [
                {
                    "start": 205,
                    "end": 225,
                    "matchedPaperCorpusId": "196181887"
                },
                {
                    "start": 592,
                    "end": 612,
                    "matchedPaperCorpusId": "196181887"
                },
                {
                    "start": 1705,
                    "end": 1729,
                    "matchedPaperCorpusId": "16828180"
                },
                {
                    "start": 1731,
                    "end": 1751,
                    "matchedPaperCorpusId": "196181887"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.22900390625
        },
        {
            "corpus_id": "271904016",
            "title": "Out-of-distribution generalization via composition: a lens through induction heads in Transformers",
            "text": "Since GPT-3 and GPT-4, many research communities have showed significant interest in understanding why trained models generalize well on novel tasks. Broadly speaking, OOD generalization refers to the generalization behavior of models when training data have a different distribution compared with the test data. Unlike classical statistics topics such as extrapolation, distribution shift, and domain adaptation, pretrained LLMs can solve compositional tasks and seemingly learn from novel context. While mathematical characterization of OOD data for natural languages remains elusive, recent experimental investigations have offered insights. We highlight several representative approaches for generating OOD data. 1. Changing the length or the distribution of task strings. This is particularly useful for experiments with synthetic data, where the distribution of task strings can be easily modified [41,38,78]. \n\n2. Replacing parts of a text string with counterfactual statements or unnatural symbols. It is used for probing LLMs to test reliability or reveal the internal mechanism [64,85,56,32]. \n\n3. Constructing compositional tasks based on deductive rules. It is used for testing models on complex reasoning tasks to examine whether models have learned the latent rules [65]. \n\nHowever, prior studies either fail to probe the inner workings of Transformer models or lack systematic measurements across many models. Compared with existing papers, we provide in-depth measurements to probe model behavior during training dynamics, and comprehensive experiments across many pretrained models, ranging from small Transformer such as GPT-2 (124M parameters) to larger LLMs such as Llama-3 (70B parameters). Moreover, our findings about subspace matching are novel, especially in the context of compositionality of LLMs.",
            "score": 0.32304274758369644,
            "section_title": "OOD generalization",
            "char_start_offset": 12967,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 915
                },
                {
                    "start": 918,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1102
                },
                {
                    "start": 1105,
                    "end": 1166
                },
                {
                    "start": 1167,
                    "end": 1285
                },
                {
                    "start": 1288,
                    "end": 1424
                },
                {
                    "start": 1425,
                    "end": 1711
                },
                {
                    "start": 1712,
                    "end": 1824
                }
            ],
            "ref_mentions": [
                {
                    "start": 908,
                    "end": 911,
                    "matchedPaperCorpusId": "258987259"
                },
                {
                    "start": 1095,
                    "end": 1098,
                    "matchedPaperCorpusId": "258740972"
                },
                {
                    "start": 1098,
                    "end": 1101,
                    "matchedPaperCorpusId": "259950770"
                },
                {
                    "start": 1280,
                    "end": 1284,
                    "matchedPaperCorpusId": "258865898"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1177978515625
        },
        {
            "corpus_id": "238531681",
            "title": "Text analysis and deep learning: A network approach",
            "text": "Furthermore, our method also opens new avenues in computational linguistics, most of which we leave for future research. First, the directed semantic substitution networks allow for the distinction between semantics that are received and conferred in a given context. Second, the structure of contextual semantic substitution networks may allow for the identification and generalization of different types of semantic relations; this includes antonymy and hyper-or hyponymy, but could also include higher-order labels traditionally associated with semantic networks. For example, combining contextual and synonymy networks could identify verbs, adjectives, or locations associated with a focal term. Third, our focus on single words is restrictive. Instead, semantic networks call for analyses that define and track semantic concepts, prototypes and categories. Here, the large set of network analysis tools allows for the generation of new insights into language use, by going beyond local relations of a word. \n\nFinally, we see the combination of contextual semantic networks with other data and statistical methods as especially promising. On the one hand, this may include sequential and ontological data, e.g., in the analysis of discourse between more than one author or speaker. On the other hand, the use of exogenous variation may allow for causal analyses of structure and dynamics of semantics networks of language use. \n\nAlthough not the focus of the present work, we finally note that semantic substitution networks can be employed more generally to analyze the behavior of deep learning models. The formal framework establishes a probabilistic interpretation of a transformer network. The weighted and directed networks, and their associated measures such as entropy, represent the conditional dependence structure of the underlying model as it responds to input vectors. Such a graphical representation of a transformer model could inform research on these model architectures in numerous ways. For example, a network based on entropy could be combined with substitute relations to measure behavior of the transformer architecture not only for specific sequences, but for specific relations between input tokens. Not only could such methods provide more structured insights into the performance of these models, they could also allow the researcher to separate behavior from the underlying data and trained instance. We leave these avenues to further research.",
            "score": 0.32302616101521225,
            "section_title": "Conclusion",
            "char_start_offset": 64671,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 699
                },
                {
                    "start": 700,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 1011
                },
                {
                    "start": 1014,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1430
                },
                {
                    "start": 1433,
                    "end": 1608
                },
                {
                    "start": 1609,
                    "end": 1698
                },
                {
                    "start": 1699,
                    "end": 1885
                },
                {
                    "start": 1886,
                    "end": 2009
                },
                {
                    "start": 2010,
                    "end": 2227
                },
                {
                    "start": 2228,
                    "end": 2431
                },
                {
                    "start": 2432,
                    "end": 2475
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09552001953125
        },
        {
            "corpus_id": "271213196",
            "title": "TokenSHAP: Interpreting Large Language Models with Monte Carlo Shapley Value Estimation",
            "text": "TokenSHAP offers a significant advancement in the interpretability of large language models (LLMs) by adapting Shapley values to natural language processing and employing Monte Carlo estimation for feasibility.This approach overcomes the challenges of variable input lengths and contextual dependencies, offering a scalable solution for complex language models.Key achievements include:\n\n\u2022 A novel framework that extends Shapley values to natural language, providing a rigorous, theoretically grounded method for interpreting token importance.\n\n\u2022 An efficient Monte Carlo sampling method that enhances the computational feasibility of applying To-kenSHAP to large-scale models.\n\n\u2022 Superior performance over existing methods in terms of alignment with human judgments, model behavior faithfulness, and consistency.\n\n\u2022 Detailed insights into LLM behavior, revealing how models process and prioritize input components.\n\nOur method's capacity to capture detailed token interactions enhances model transparency and aids in debugging, bias mitigation, and regulatory compliance, which is essential as LLMs are increasingly deployed in critical domains.\n\nFuture research will explore sophisticated value functions, the stability of Shapley values across models, and the extension of TokenSHAP to conversational AI.Developing interactive tools based on TokenSHAP could also enhance its accessibility and practical utility for practitioners.\n\nTokenSHAP represents a vital step towards making AI systems not only powerful but also transparent and accountable, ensuring their responsible development and deployment in transformative applications.",
            "score": 0.32289214093924884,
            "section_title": "Conclusion",
            "char_start_offset": 16201,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 210
                },
                {
                    "start": 210,
                    "end": 361
                },
                {
                    "start": 361,
                    "end": 386
                },
                {
                    "start": 388,
                    "end": 543
                },
                {
                    "start": 545,
                    "end": 677
                },
                {
                    "start": 679,
                    "end": 813
                },
                {
                    "start": 815,
                    "end": 915
                },
                {
                    "start": 917,
                    "end": 1146
                },
                {
                    "start": 1148,
                    "end": 1307
                },
                {
                    "start": 1307,
                    "end": 1432
                },
                {
                    "start": 1434,
                    "end": 1635
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2322998046875
        },
        {
            "corpus_id": "65329353",
            "title": "CHIELD: the causal hypotheses in evolutionary linguistics database",
            "text": "The next example demonstrates the ability to evaluate multiple different theories. Lupyan & Dale (2010), following theories from studies of language contact, showed that a language's morphological complexity can be predicted by the number of speakers who speak it. They hypothesised that larger populations have more adult learners and more contact with other languages. These factors might cause a pressure for the morphological system of the language to become simpler. For example, adults are worse at learning morphological rules than lexical strategies. That is, languages with large number of speakers might adapt to the adult 'cognitive niche'. \n\nFigure 5 shows the causal graph for this hypothesis, which highlights some key points. First, while the hypothesised mechanism has several steps, the main quantitative result is a correlation between population size and morphological complexity (due to the intervening variables having limited data available). The correlation is consistent with the hypothesis, but alternative data or methods could be applied to try and support each causal link. Secondly, while most links are supported either by reviews from the literature or statistical analyses, there is a \"weak link\": there was no supporting evidence for a causal effect of population size and the proportion of adult learners. Although it makes logical sense, ideally it should be confirmed empirically (as was recently done in Koplenig, 2019). 5: Lupyan & Dale (2010)'s hypothesis, expressed as a causal graph. Links are coloured according to the type of evidence provided. \n\nLupyan & Dale's study led to several other empirical studies looking at the relationship between morphological complexity and population size, as well as the invocation of previous studies to explain the patterns. In Figure 6, we show 21 of these studies represented as causal graphs (supporting materials S3 include the R script for automatically generating this figure from CHIELD). This graph includes evidence from fieldwork, cross-cultural statistics, lab experiments and simulations. While this visualisation may look complicated, in tandem with the interactive features of the website it provides a way of systematically thinking about different explanations. For example, Nettle (2012) and Cuskley & Loreto (2016)'s explanation involves general processes, whereby larger populations change frequency distributions in ways that lead to simplification.",
            "score": 0.3227989441570387,
            "section_title": "Case study 2: population size and morphological complexity",
            "char_start_offset": 37173,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 82
                },
                {
                    "start": 83,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 651
                },
                {
                    "start": 654,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1101
                },
                {
                    "start": 1102,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1457
                },
                {
                    "start": 1458,
                    "end": 1524
                },
                {
                    "start": 1525,
                    "end": 1587
                },
                {
                    "start": 1590,
                    "end": 1803
                },
                {
                    "start": 1804,
                    "end": 1974
                },
                {
                    "start": 1975,
                    "end": 2079
                },
                {
                    "start": 2080,
                    "end": 2256
                },
                {
                    "start": 2257,
                    "end": 2448
                }
            ],
            "ref_mentions": [
                {
                    "start": 83,
                    "end": 103,
                    "matchedPaperCorpusId": "15457816"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.072509765625
        },
        {
            "corpus_id": "267499620",
            "title": "The Information of Large Language Model Geometry",
            "text": "Large language models (LLMs) have brought about a revolution in natural language processing, enabling significant breakthroughs across various tasks [8,20]. However, comprehending the underlying mechanisms that drive their impressive performance remains a challenging endeavor. One interesting phenomenon observed in LLMs is the scaling law [11,12], which shows that the performance of models follows a predictable power-law relationship with quantities like model parameters. While scaling laws have been empirically discovered in diverse domains, the theoretical understanding of it is rare [4,15]. The understanding of scaling laws in LLMs is of great importance as it can provide insights into the scalability, efficiency, and generalization capabilities of these models as they grow larger. \n\nTo begin unraveling the mysteries of scaling laws in LLMs, we conduct simulations [25] to analyze the representation entropy, revealing an interesting finding: a power law relationship may exist between representation entropy and model sizes. This discovery shows that LLM representation (geometry) has a deep connection with information theory. Building upon this insight, we propose a novel theoretical framework based on (conditional) entropy to provide a deeper understanding of the scaling law phenomenon. Our theory offers valuable insights into how information is encoded and utilized within LLMs as they scale. \n\nWhen dealing with scaling law, we consider each sentence as a whole, therefore focusing more on a \"macro\" behavior. When considering the \"micro\" structure, we shift our focus to the auto-regressive structure inherent in LLMs, specifically examining the relationship between the last token and the preceding context tokens. Our analysis establishes a compelling theoretical connection between the information gain brought by new tokens and ridge regression. This connection not only deepens our understanding of the auto-regressive nature of LLMs but also provides insights into how the addition of new tokens contributes to the overall information content of the models. Another critical aspect we explore is the effectiveness of token selection mechanisms within LLMs. Surprisingly, our analysis reveals that Lasso regression sometimes outperforms the closely related attention weights in identifying context tokens with high information content. \n\nFinally, we investigate whether a token embedding contains all the information from its preceding context. By comparing the result given by mean embedding and specific token embedding, we find that information is encoded among all tokens, not concentrated on specific tokens.",
            "score": 0.3227339930076927,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 476
                },
                {
                    "start": 477,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 795
                },
                {
                    "start": 798,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1416
                },
                {
                    "start": 1419,
                    "end": 1534
                },
                {
                    "start": 1535,
                    "end": 1741
                },
                {
                    "start": 1742,
                    "end": 1875
                },
                {
                    "start": 1876,
                    "end": 2089
                },
                {
                    "start": 2090,
                    "end": 2188
                },
                {
                    "start": 2189,
                    "end": 2366
                },
                {
                    "start": 2369,
                    "end": 2475
                },
                {
                    "start": 2476,
                    "end": 2644
                }
            ],
            "ref_mentions": [
                {
                    "start": 149,
                    "end": 152,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0880126953125
        },
        {
            "corpus_id": "236486261",
            "title": "Linguistic Knowledge in Multilingual Grapheme-to-Phoneme Conversion",
            "text": "In this section, we provide detailed error analyses on the test-set predictions from our best system. The goals of these analyses are twofold: (i) to examine the aspects in which this model outperforms the baseline and to what extent, and (ii) to get a better understanding of the nature of errors made by the system-we believe that insights and improvements can be derived from a good grasp of error patterns. We analyzed the mismatches between predicted sequences and ground-truth sequences at the segmental level. For this purpose, we again utilized many-to-many alignment (Jiampojamarn et al., 2007;Jiampojamarn and Kondrak, 2010), but this time between a predicted sequence and the corresponding ground-truth sequence. 6 For each error along the aligned sequence, we classified it into one of the three kinds: \n\n\u2022 Those involving erroneous vowel insertions (e.g., \u01eb \u2192 [@]), deletions (e.g., [@] \u2192 \u01eb), or substitutions (e.g., [@] \u2192 [a]). \n\n\u2022 In the same vein, those involving erroneous consonant insertions (e.g., \u01eb \u2192 [P]), deletions boundaries does not improve the results, it is unlikely that marking constituent boundaries, which adds more variability to the input, will result in better performance, though we did not test this hypothesis. 6 The parameters used are: allowing deletion of input grapheme strings, maximum length of aligned grapheme and phoneme substring being one, and a training threshold of 0.001. \n\n(e.g., [P] \u2192 \u01eb), and substitutions (e.g., [d] \u2192 [t]). \n\n\u2022 Those involving exchanges of a vowel and a consonant (e.g., [w] \u2192 [u]) or vice versa. \n\nThe frequency of each error type made by the baseline model and our systems for each individual language is plotted in Figure 2. Some patterns are immediately clear. First, both systems have a similar pattern in terms of the distribution of error types across language, albeit that ours makes fewer errors on average. Second, both systems err on different elements, depending on the language.",
            "score": 0.32264790205360905,
            "section_title": "Error Analyses",
            "char_start_offset": 17172,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 814
                },
                {
                    "start": 817,
                    "end": 941
                },
                {
                    "start": 944,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1422
                },
                {
                    "start": 1425,
                    "end": 1478
                },
                {
                    "start": 1481,
                    "end": 1568
                },
                {
                    "start": 1571,
                    "end": 1736
                },
                {
                    "start": 1737,
                    "end": 1888
                },
                {
                    "start": 1889,
                    "end": 1963
                }
            ],
            "ref_mentions": [
                {
                    "start": 576,
                    "end": 603,
                    "matchedPaperCorpusId": "8778439"
                },
                {
                    "start": 603,
                    "end": 634,
                    "matchedPaperCorpusId": "9964459"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.058990478515625
        },
        {
            "corpus_id": "273507789",
            "title": "Self-Steering Optimization: Autonomous Preference Optimization for Large Language Models",
            "text": "Figure 2: The philosophical motivation of our methods. Greater overlap on the x-axis (performance) between the generated distributions (red and blue) and the original distribution (orange) indicates stronger on-policy behavior. Previous automated methods extract chosen and rejected distributions through different methods, which may be less learnable for the policy model and hard to distinguish after iterative training. Our approach (SSO) optimizes models to generate near-on-policy signals where there remains a gap between chosen and rejected distributions, which benefits the automated alignment process. \n\nThe field of Natural Language Processing has undergone revolutionary advancements driven by Large Language Models (LLMs). After meticulous alignment processes, LLMs have demonstrated remarkable capabilities for following instructions and understanding human preferences. This leads to the development of widely acclaimed products like ChatGPT (OpenAI, 2023), which captured significant public attention. However, aligning LLMs with human preferences is not trivial. Despite the existence of preference optimization algorithms such as Proximal Policy Optimization (PPO) (Ouyang et al., 2022) and Direct Preference Optimization (DPO) (Rafailov et al., 2023), an ideal alignment training process necessitates a robust explicit or implicit reward model. This model must effectively differentiate between chosen and rejected responses and guide it to optimizing toward the preferred responses. Unfortunately, the reward model depends on a large amount of high-quality annotated preference data and continuous updates of labeled response pairs to prevent reward hacking, which is resource-intensive and requires meticulous attention. Besides, the limited capabilities of human annotators cause the inherent limitations of annotated data, making it challenging to achieve superalignment (Burns et al., 2023). \n\nConsequently, recent researchers have shifted their focus towards automated alignment, intending to develop scalable, high-quality alignment systems with minimal human intervention. The cornerstone of this approach is the pursuit of scalable alignment signals that are capable of replacing human-annotated preference signals effectively. Current popular strategies include: (1) Employing the policy model to discriminate chosen and rejected responses (Yuan et al., 2024).",
            "score": 0.3225613516879451,
            "section_title": "Iterative Optimizing",
            "char_start_offset": 213,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 54
                },
                {
                    "start": 55,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 610
                },
                {
                    "start": 613,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1740
                },
                {
                    "start": 1741,
                    "end": 1914
                },
                {
                    "start": 1917,
                    "end": 2098
                },
                {
                    "start": 2099,
                    "end": 2254
                },
                {
                    "start": 2255,
                    "end": 2388
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.093994140625
        },
        {
            "corpus_id": "266900042",
            "title": "Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models",
            "text": "A widely adopted strategy to tackle challenges related to length extrapolation involves the integration of Relative Positional Encoding (RPE) techniques (Su et al., 2021;Qin et al., 2023c), strategically directing attention towards neighboring tokens. ALiBi (Press et al., 2022) utilizes linear decay biases in attention mechanisms to mitigate the impact of distant tokens. Roformer (Su et al., 2021) introduces a novel Rotary Position Embedding (RoPE) method, widely embraced in the community, effectively leveraging positional information for transformer-based language model learning. Kerple (Chi et al., 2022) explores shift-invariant conditionally positive definite kernels within RPEs, introducing a suite of kernels aimed at enhancing length extrapolation properties, with ALiBi recognized as one of its instances. Furthermore, Sandwich (Chi et al., 2023) postulates a hypothesis elucidating the mechanism behind ALiBi, empirically validating it by incorporating the hypothesis into sinusoidal positional embeddings. (Qin et al., 2024) explored the sufficient conditions for additive relative position encoding to have extrapolation capabilities. \n\nInstead of investigating the length extrapolation capability of transformers, some works also attempt to directly increase the context window sizes. Chen et al. (2023) introduces Position Interpolation (PI), extending context window sizes of RoPE-based pretrained Large Language Models (LLMs) such as LLaMA models to up to 32768 with minimal finetuning (within 1000 steps). StreamingLLM (Xiao et al., 2023) proposes leveraging the attention sink phenomenon, maintaining the Key and Value information of initial tokens to substantially recover the performance of window attention. As the sequence grows longer, the performance degrades. These methods can only extend sequence length in fine-tuning or testing phases, while our method allows training models in long sequence lengths from scratch with no additional cost.",
            "score": 0.3224138794660356,
            "section_title": "Long Sequence Handling in LLM",
            "char_start_offset": 5416,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1153
                },
                {
                    "start": 1156,
                    "end": 1304
                },
                {
                    "start": 1305,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1735
                },
                {
                    "start": 1736,
                    "end": 1791
                },
                {
                    "start": 1792,
                    "end": 1974
                }
            ],
            "ref_mentions": [
                {
                    "start": 170,
                    "end": 188,
                    "matchedPaperCorpusId": "259951088"
                },
                {
                    "start": 258,
                    "end": 278,
                    "matchedPaperCorpusId": "237347130"
                },
                {
                    "start": 1024,
                    "end": 1042,
                    "matchedPaperCorpusId": "259982812"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04248046875
        },
        {
            "corpus_id": "273185453",
            "title": "Collapsed Language Models Promote Fairness",
            "text": "We first evaluate N C metrics in popular debiased language models and compare them with their biased baselines. \n\nSettings. Following previous works, we measure N C metrics based on different training datasets used in each work, concerning the subset of the whole vocabulary of only gender-related words, dubbed V gender (see Appendix A for the full list of words). The detailed datasets used by these works are listed as follows: Mabel on SNLI [4] and MNLI [66]; ASE on OntoNotes [27]; BEC on TinyStories [13]; Among these, only BEC's training dataset [63] is relatively small, while the datasets in other works exceed 100K sentences. To ensure meaningful comparisons, we evaluated BEC on a larger dataset TinyStories [13]. \n\nWe show this result in Table 1. All methods start from the same pretrained BERT model. However, each work studied the BERT model on its own training data, leading to different N C measurements for the same BERT model. From Table 1, we can see that debiased language models exhibit neural collapse in certain perspectives: N C3 is consistently improved (minimized) in debiased models, whereas N C1/2/4 are diverging. This indicates that the alignments between token representations (\"class means\") and debiased word embeddings (\"classifier weights\") are more consistent, as illustrated in Figure 1. For evaluations of additional models, please refer to Appendix C. \n\nOur explanations are as follows. The neural collapse behavior manifests under certain conditions [47], including: 1) models are trained towards zero training loss; 2) clean labels with balanced classes; 3) the number of classes is not greater than the model's hidden dimension. However, these conditions are commonly violated in practice: 1) The training loss is difficult to be minimized to zero due to the complexity of language data; 2) The occurrence of tokens in V is highly unbalance due to the nature of languages; 3) Not all language models have greater hidden sizes than the vocabulary size 1 .",
            "score": 0.32239777367721234,
            "section_title": "N C METRICS IN DEBIASED LMS",
            "char_start_offset": 9384,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 114,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 724
                },
                {
                    "start": 727,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1324
                },
                {
                    "start": 1325,
                    "end": 1390
                },
                {
                    "start": 1393,
                    "end": 1425
                },
                {
                    "start": 1426,
                    "end": 1670
                },
                {
                    "start": 1671,
                    "end": 1996
                }
            ],
            "ref_mentions": [
                {
                    "start": 481,
                    "end": 485,
                    "matchedPaperCorpusId": "8508974"
                },
                {
                    "start": 1490,
                    "end": 1494,
                    "matchedPaperCorpusId": "221172897"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0567626953125
        },
        {
            "corpus_id": "273482521",
            "title": "Fine-Grained Verifiers: Preference Modeling as Next-token Prediction in Vision-Language Alignment",
            "text": "The recent advancements in large language models (LLMs) and pre-trained vision models have accelerated the development of vision-language large models (VLLMs), enhancing the interaction between visual and linguistic modalities. Despite their notable success across various domains, VLLMs face challenges in modality alignment, which can lead to issues like hallucinations and unsafe content generation. Current alignment techniques often rely on coarse feedback and external datasets, limiting scalability and performance. In this paper, we propose FiSAO (Fine-Grained Self-Alignment Optimization), a novel self-alignment method that utilizes the model's own visual encoder as a fine-grained verifier to improve vision-language alignment without the need for additional data. By leveraging token-level feedback from the vision encoder, FiSAO significantly improves vision-language alignment, even surpassing traditional preference tuning methods that require additional data. Through both theoretical analysis and experimental validation, we demonstrate that FiSAO effectively addresses the misalignment problem in VLLMs, marking the first instance of token-level rewards being applied to such models.",
            "score": 0.3223695526122967,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0751953125
        },
        {
            "corpus_id": "270257855",
            "title": "Scaling Laws for Reward Model Overoptimization in Direct Alignment Algorithms",
            "text": "In this stage, preferences extracted from trajectories are employed to align the policy. Notably, we have only one preference available: (s 1 , a 1 , s 3 , a 1 , s 9 , a 0 , s \u221e ) \u227b (s 1 , a 0 , s 2 , a 0 , s 5 , a 0 , s \u221e ). This deliberate constraint exaggerates a scenario with limited data, enabling us to gauge the probability mass allocated to out-of-distribution (OOD) trajectories under such conditions. Insights garnered from this exaggerated low-data scenario hold relevance for Large Language Model (LLM) settings where preference datasets used for alignment are notably smaller compared to the scale of LLM models deployed. \n\nWe utilize a Recurrent Neural Network (RNN) policy to navigate through the MDP, facilitating a closer resemblance to real-world language modeling scenarios. \n\nSubsequently, we explore three distinct direct alignment loss functions: Direct Preference Optimization (DPO) [46], Identity Preference Optimization (IPO) [4], and Sequence Likelihood Calibration (SLiC) [68]. Additionally, we investigate how the selection of the KL penalty coefficient \u03b2 influences the distribution of probability mass on OOD trajectories. This exploration encompasses three values of \u03b2: (0.01, 0.1, 0.5). \n\nIn general, the plots illustrate that Direct Alignment Algorithms (DAAs) tend to allocate a significant proportion of the probability mass to out-of-distribution (OOD) trajectories during the alignment process. While Figure 9 may suggest that Direct Preference Optimization (DPO) can retain a substantial amount of probability mass on the selected trajectory in the preference dataset, it's noteworthy that the plots for DPO exhibit considerable noise. To provide further insight, Figure 18 displays the plots resulting from three additional repetitions of the DPO experiment. Similar noisy trends were also observed in the experiments for IPO and SLiC. This elucidates the unconstrained nature of the DPO problem: multiple solutions exist for the DPO loss, each distributing varying amounts of probability mass to OOD trajectories.",
            "score": 0.3219653360487525,
            "section_title": "Alignment with Preferences:",
            "char_start_offset": 35081,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 635
                },
                {
                    "start": 638,
                    "end": 794
                },
                {
                    "start": 797,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1219
                },
                {
                    "start": 1222,
                    "end": 1432
                },
                {
                    "start": 1433,
                    "end": 1674
                },
                {
                    "start": 1675,
                    "end": 1798
                },
                {
                    "start": 1799,
                    "end": 1875
                },
                {
                    "start": 1876,
                    "end": 2054
                }
            ],
            "ref_mentions": [
                {
                    "start": 907,
                    "end": 911,
                    "matchedPaperCorpusId": "258959321"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0328369140625
        },
        {
            "corpus_id": "2695216",
            "title": "Improved Statistical Machine Translation Using Monolingually-Derived Paraphrases",
            "text": "The distributional hypothesis and distributional profiles. Natural language processing (NLP) applications that assume the distributional hypothesis (Harris, 1940;Firth, 1957) typically keep track of word co-occurrences in distributional profiles (a.k.a. collocation vectors, or context vectors). Each distributional profile DP u (for some word u) keeps counts of co-occurrence of u with all words within a usually fixed distance from each of its occurrences (a sliding window) in some training corpus. More advanced profiles keep \"strength of association\" (SoA) information between u and each of the co-occurring words, which is calculated from the counts of u, the counts of the other word, their co-occurrence count, and the count of all words in the corpus (corpus size). The information on the other words with respect to u is typically kept in a vector whose dimensions correspond to all words in the training corpus. This is described in Equation (1), where V is the training corpus vocabulary:\n\nSemantic similarity between words u and v can be estimated by calculating the similarity (vector distance) between their profiles. Slightly more formally, the distributional hypothesis assumes that if we had access to the hypothetical true (psycholinguistic) semantic similarity function over word pairs, semsim(u, v), then\n\nwhere V is the language vocabulary, DP word is the distributional profile of word, and psim() is a 2-place vector similarity function (all further described below). Paraphrasing and other NLP applications that are based on the distributional hypothesis assume entailment in the reverse direction: the right-hand-side of Formula (2) (profile/vector similarity) entails the left-hand-side (semantic similarity).\n\nThe sliding window and word association (SoA) measures. Some researchers count positional collocations in a sliding window, i.e., the cocounts and SoA measures are calculated per relative position (e.g., for some word/token u, position 1 is the token immediately after u; position -2 is the token preceding the token that precedes u) (Rapp, 1999); other researchers use nonpositional (which we dub here flat) collocations, meaning, they count all",
            "score": 0.32192580885343425,
            "section_title": "Collocational Profiles",
            "char_start_offset": 8037,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 148,
                    "end": 162,
                    "matchedPaperCorpusId": "147162632"
                },
                {
                    "start": 162,
                    "end": 174,
                    "matchedPaperCorpusId": "208093066"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07342529296875
        },
        {
            "corpus_id": "273375487",
            "title": "Preference Optimization with Multi-Sample Comparisons",
            "text": "Distributional difference (Zhong et al., 2022) describes the difference between two or more distributions, which is a generalization of single-sample comparison. While many characteristics can be judged from a single sample, some properties can only be deduced from multiple samples, such as diversity and bias (Santurkar et al., 2023;Chen et al., 2024;Zhang et al., 2024b;Zhou et al., 2024b;Mohammadi, 2024;Wang et al., 2023;Go et al., 2023). To measure the distributional difference, Zhong et al. (2022) proposed a hypothesis-verifier framework to summarize the difference between two corpora. Dunlap et al. (2024) further extended this framework to the image domain to capture the difference between two sets of images. More recently, Zhong et al. (2023) considered the problem of distributional difference in a goal-driven setting. Melnyk et al. (2024) addressed the problem of distributional alignment for language models using optimal transport, aiming to optimize the chosen responses across all prompts jointly. In contrast, our work addresses an orthogonal problem by focusing on optimizing the distributional preference under the same prompt. \n\nWhile finalizing this manuscript for submission to arXiv, we became aware of a related study by Li et al. (2024). Both works independently address the formulation of multi-sample comparison. Our contributions differ in several key aspects, such as the introduction of additional algorithms, distinct experimental foci, and differing applications. Notably, our primary focus is on language models, with an emphasis on providing a low-variance and potentially unbiased estimator. In contrast, their work centers around text-to-image tasks and does not include the IPO-based methods we derived. \n\n3 Preliminaries Supervised Finetuning. After the language model has been pretrained extensively on large corpora, the next step is typically supervised finetuning (SFT). This process aims to teach the model the dialog formats and refine its predictions to interact more naturally with human and prepare it for the subsequent alignment stages.",
            "score": 0.3219159020273523,
            "section_title": "Related Works",
            "char_start_offset": 6685,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1152
                },
                {
                    "start": 1155,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1632
                },
                {
                    "start": 1633,
                    "end": 1746
                },
                {
                    "start": 1749,
                    "end": 1787
                },
                {
                    "start": 1788,
                    "end": 1918
                },
                {
                    "start": 1919,
                    "end": 2091
                }
            ],
            "ref_mentions": [
                {
                    "start": 26,
                    "end": 46,
                    "matchedPaperCorpusId": "248863377"
                },
                {
                    "start": 311,
                    "end": 335,
                    "matchedPaperCorpusId": "257834040"
                },
                {
                    "start": 486,
                    "end": 505,
                    "matchedPaperCorpusId": "248863377"
                },
                {
                    "start": 596,
                    "end": 616,
                    "matchedPaperCorpusId": "265658938"
                },
                {
                    "start": 738,
                    "end": 757,
                    "matchedPaperCorpusId": "257232924"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.105224609375
        },
        {
            "corpus_id": "258832652",
            "title": "Explaining How Transformers Use Context to Build Predictions",
            "text": "-verb agreement) (Linzen et al., 2016) and the Indirect Object Identification (IOI) (Wang et al., 2023;Fahamu, 2022)  subject, which makes this dataset well-suited for evaluating saliency methods. Indirect object identification (IOI) is a feature present in sentences that have an initial dependent clause, like \"After Lee and Evelyn went to the lake\", followed by a main clause, like \"Lee gave a grape to Evelyn\". The indirect object \"Evelyn\" and the subject \"Lee\" are found in the initial clause. In all examples of IOI dataset, the main clause refers to the subject again, which gives an object to the IO. The goal of the IOI task is to predict the final word in the sentence to be the IO. In IOI examples, the rule for predicting the IO is the IO itself being in the first clause.\n\nWe use GPT-2 XL (1.5B) model (Radford et al., 2019), as in (Yin and Neubig, 2022), as well as other autoregressive Transformer language models, such as GPT-2 Small (124M), and GPT-2 Large models (774M), OPT 125M (Zhang et al., 2022b), and BLOOM's 560M and 1.1B variants (BigScience Workshop, 2022), through Hugging-Face library (Wolf et al., 2020).\n\nAlignment Metrics. Following Yin and Neubig (2022), we define the evidence as a binary vector b \u2208 R t (with as many components as the number of previous tokens), with all zeros except in the position of the tokens inside the evidence, i.e. the tokens which the prediction depends on, extracted by the rule. Explanations are vectors, also \u2208 R t . To measure the alignment between an explanation and the evidence we use MRR (Mean Reciprocal Analysis). Sorting the tokens in descending order, MRR evaluates the average of the inverse of the rank of the first token that is part of b. Although Yin and Neubig (2022) use also dot-product and Probes Needed metrics for measuring alignments, dot-",
            "score": 0.32180029636965524,
            "section_title": "Experimental Setup",
            "char_start_offset": 11782,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 84,
                    "end": 103,
                    "matchedPaperCorpusId": "253244237"
                },
                {
                    "start": 845,
                    "end": 867,
                    "matchedPaperCorpusId": "247011700"
                },
                {
                    "start": 1114,
                    "end": 1133,
                    "matchedPaperCorpusId": "269498086"
                },
                {
                    "start": 1165,
                    "end": 1186,
                    "matchedPaperCorpusId": "247011700"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06085205078125
        },
        {
            "corpus_id": "276095275",
            "title": "SimPER: A Minimalist Approach to Preference Alignment without Hyperparameters",
            "text": "Figure 1: Evaluation on the MT-Bench Score (1-10) of SimPO and our SimPER across different large language models reveals the high sensitivity and instability of SimPO with respect to its hyperparameter \u03b3 across models. In contrast, our SimPER, which operates without any hyperparameters in the objective function, consistently and significantly outperforms SimPO across a wide range of models. Additional experimental evidence on other widely used benchmarks is provided in Section 4. Method Hyperparameters #Hyperparameters w/o Reference Model \n\nHowever, these methods require additional hyperparameters that must be carefully tuned as shown in Table 1, and the performance of current preference optimization methods, such as DPO, KTO, IPO, and others, is highly sensitive to these hyperparameters across different LLMs, as already shown by (Hugging-Face, 2023;Liu et al., 2024a;Meng et al., 2024;Liu et al., 2024c;Wu et al., 2024). In Figure 1, we also show that tuning hyperparameters is also crucial to achieve optimal performance with the recent state-of-the-art algorithm SimPO, which eliminates the need for a reference model. This challenge largely prevents us from aligning large language models in real-world applications, given that a single post-training process for alignment is usually very expensive and takes a long time (Dubey et al., 2024). To this end, we ask an important research question for large language model alignment: Can we design an efficient and effective hyperparameter-free preference optimization method for alignment? \n\nIn this paper, we answer this question affirmatively. We propose SimPER, a simple yet effective offline preference optimization objective that eliminates the need for a reference model and any tunable hyperparameters. The key to SimPER is directly optimizing the reverse perplexity of chosen and rejected responses within the preference dataset. Perplexity (Jelinek et al., 1977) is a wellknown evaluation metric for language modeling, commonly used to assess a model's ability to process long text. It is calculated as the inverse of the exponentiated average log-likelihood of the responses.",
            "score": 0.3216541641990312,
            "section_title": "SimPO SimPER",
            "char_start_offset": 4674,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 544
                },
                {
                    "start": 547,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1552
                },
                {
                    "start": 1555,
                    "end": 1608
                },
                {
                    "start": 1609,
                    "end": 1772
                },
                {
                    "start": 1773,
                    "end": 1900
                },
                {
                    "start": 1901,
                    "end": 2054
                },
                {
                    "start": 2055,
                    "end": 2148
                }
            ],
            "ref_mentions": [
                {
                    "start": 1912,
                    "end": 1934,
                    "matchedPaperCorpusId": "121680873"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05072021484375
        },
        {
            "corpus_id": "268379738",
            "title": "Grammar as a Behavioral Biometric: Using Cognitively Motivated Grammar Models for Authorship Verification",
            "text": "The mechanisms of such a sequential processing can be efficiently represented through the short-term correlations captured by n-gram models as done in this work. \n\nA deeper connection between LambdaG and approaches to AV based on language modeling or, as for COAV, on compression algorithms can also be drawn, thus offering a plausible explanation for their success. Through the lenses of Information Theory, the average P (S; G) for each sentence S in D can be also seen as an estimate of the cross-entropy rate of the Grammar Model G against the probability distribution of grammatical tokens that generates D. In language modeling literature, the cross-entropy is often exponentiated to calculate the perplexity of a document given a model. Because for LambdaG the two alternative Grammar Models are compared in relation to the same D, for the purposes of this AV application, \u03bb G is equivalent to comparing the perplexities of the candidate author's model vs. the reference model given the same D and assign the document to the candidate author if this perplexity is lower. The cross-entropy rate can also be interpreted as the average length in bits of a binary encoding of grammatical tokens for a data-compression scheme that is optimized for the probability distribution P (\u2022; G). For this reason, a language model can be seen as a text-compression scheme. This connection thus links compression and language model AV methods like LambdaG and COAV to the Cognitive Linguistics conception of an individual's grammar as a compressed representation of real language usage encountered by said individual. \n\nIn addition to the successful results of LambdaG, we note that the qualitative explorations reported in Section 7 also constitute evidence that LambdaG captures linguistic behavior compatible with these Cognitive Linguistic explanations. That individuals vary greatly in their mental grammar, as theorized by Langacker, can be seen at play in these text heat maps. The theoretical interpretation of these results is that a stronger shade of red effectively means a greater likelihood that the token is entrenched for that individual's unique grammar in contrast to the population.",
            "score": 0.3214948276285109,
            "section_title": "Discussion",
            "char_start_offset": 48983,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 164,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1608
                },
                {
                    "start": 1611,
                    "end": 1848
                },
                {
                    "start": 1849,
                    "end": 1975
                },
                {
                    "start": 1976,
                    "end": 2191
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07696533203125
        },
        {
            "corpus_id": "254113191",
            "title": "Colloquium: Hierarchy of scales in language dynamics",
            "text": "Here it is found that iterated learning either eliminates one of the plural markers, or that the variation becomes predictable: that is, each object is typically pluralised by one or other of the markers, but not both. \n\nA final application of the artificial language learning paradigm is to investigate whether the nonuniform distribution of a structural feature over the world's languages (like word order, see Sect. 2) predicts the existence of a bias towards the more frequent structures in single instances of learning. One such study relates to affixes that change the meaning of the word. Where this occurs, it turns out that suffixes are more common than prefixes or infixes [122]. To investigate whether this is also true at the individual level, St Clair et al. [123] trained participants on an artificial language in which words are divided into two categories, each with their own affix. When presented with unfamiliar sentences, participants could more reliably identify when the correct affix was used for a particular word stem, in correspondence with the observation that suffixes are more common than prefixes. \n\nAs noted in Section 2, correlations between two structural features of a language have been observed, and are formulated as implicational universals. Culbertson et al. [124] set up an experiment to determine whether a correlation between the order of numerals and nouns and the order of adjectives and nouns is visible at the level of individual learning events. In particular, one of the four possible orderings is much less frequent than the other three. Participants were trained on four variant languages, and then asked to describe a set of scenes using the language they had learnt. Points were awarded for a valid description of the scene (i.e., the right words, but in any order) and additionally if the order matched that of a computer-generated \"native\" speaker of the artificial language. The input languages were set up in such a way that participants would score maximum points by using one of the four possible orderings exclusively. The results of this experiment showed that this happened for all four input languages, apart from the one that corresponds to the ordering that is rare across the world's languages.",
            "score": 0.3213007535264759,
            "section_title": "Individual linguistic interactions",
            "char_start_offset": 76514,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 221,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 1127
                },
                {
                    "start": 1130,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1492
                },
                {
                    "start": 1493,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1718
                },
                {
                    "start": 1719,
                    "end": 1929
                },
                {
                    "start": 1930,
                    "end": 2077
                },
                {
                    "start": 2078,
                    "end": 2259
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06707763671875
        },
        {
            "corpus_id": "17563796",
            "title": "Bootstrapping structure into language : alignment-based learning",
            "text": "refined and abstract meanings largely grow out of more concrete meanings. Bloomfield (1933) \n \nThis thesis introduces a new unsupervised learning framework, called Alignment-Based Learning, which is based on the alignment of sentences and Harris's (1951) notion of substitutability . Instances of the framework can be applied to an untagged, unstructured corpus of natural language sentences, resulting in a labelled, bracketed version of that corpus. Firstly, the framework aligns all sentences in the corpus in pairs, resulting in a partition of the sentences consisting of parts of the sentences that are equal in both sentences and parts that are unequal. Unequal parts of sen tences can be seen as being substitutable for each other, since substituting one unequal part for the other results in another valid sentence. The unequal parts of the sentences are thus considered to be possible (possibly overlapping) constituents, called hypotheses. \n \nSecondly , the selection learning phase considers all hypotheses found by the alignment learning phase and selects the best of these. The hypotheses are selected based on the order in which they were found, or based on a probabilistic function. The framework can be extended with a grammar extraction phase. This extended framework is called parseABL. Instead of returning a structured version of the unstructured input corpus, like the ABL system, this system also returns a stochastic context-free or tree substitution grammar. \n \nDifferent instances of the framework have been tested on the English ATIS corpus, the Dutch OVIS corpus and the Wall Street Journal corpus. One of the interesting results, apart from the encouraging numerical results, is that all instances can (and do) learn recursive structures.",
            "score": 0.3213007535264759,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05877685546875
        },
        {
            "corpus_id": "254125438",
            "title": "Word Alignment in the Era of Deep Learning: A Tutorial",
            "text": "In this section, we cover approaches that directly induce word alignments without additional finetuning. We begin by describing an initial hurdle. Word alignments operates on sequences e, f that are translations of each other. However, an attentional NMT model will generate a predicted translation f that most likely differs from f . We cannot directly compare word alignments between e, f and e, f . The typical approach is to \"force decode\" the attentional models: at each time step, the gold token f i is selected, and thus f = f . Luong, Pham, and Manning (2015) use force decoding on their global attention model (described in Section 5.1.1), and extract alignments by considering, for each target word, the source word with the highest attention weight (we refer to this as the argmax approach). They do the same for their local attention models, which looks at a subset of source words at each time step. They find the best model achieves 34 AER. \n\nThe following works perform word alignment induction on transformer NMT models. Garg et al. (2019) create a similar, \"naive\" attention baseline by layer-wise averaging of attention probabilities, finding the best AER of 32.6 on layer 5 of a 6-layer transformer. Kobayashi et al. (2020) propose a simple refinement. Instead of using the attention weight \u03b1 directly, they used the norm of the weighted projected vector \u03b1f (x) , where f (x) is the transformed input vector. They modify the argmax approach by selecting the source word s j that gains the highest weight when inputting t i (instead of when outputting t i ). This method with norm-based induction achieves 25.0 AER. Ferrando and Costa-juss\u00e0 (2021) extend this method by masking out final tokens, and performing a weighted average across attention heads, by a calculated head importance score. Using weight-based induction they achieve 22.1 AER, vs. 18.7 for GIZA++. \n\nChen et al. ( 2020), similarly to Kobayashi et al. (2020), induce alignment on a transformer when inputting t i , then average attention weights across heads.",
            "score": 0.3213007535264759,
            "section_title": "Word Alignments through Induction on Attention",
            "char_start_offset": 35724,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 954
                },
                {
                    "start": 957,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1218
                },
                {
                    "start": 1219,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1427
                },
                {
                    "start": 1428,
                    "end": 1576
                },
                {
                    "start": 1577,
                    "end": 1633
                },
                {
                    "start": 1634,
                    "end": 1810
                },
                {
                    "start": 1811,
                    "end": 1883
                },
                {
                    "start": 1886,
                    "end": 2044
                }
            ],
            "ref_mentions": [
                {
                    "start": 1037,
                    "end": 1055,
                    "matchedPaperCorpusId": "202539004"
                },
                {
                    "start": 1219,
                    "end": 1242,
                    "matchedPaperCorpusId": "222176890"
                },
                {
                    "start": 1634,
                    "end": 1665,
                    "matchedPaperCorpusId": "237491949"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.056854248046875
        },
        {
            "corpus_id": "271769574",
            "title": "Aligning to Adults Is Easy, Aligning to Children Is Hard: A Study of Linguistic Alignment in Dialogue Systems",
            "text": "We use the align package (Duran et al., 2019) to calculate syntactic, lexical, and semantic alignment. \n\nAll are computed given the last (i.e., the most recent) context utterance u and the (true or generated) response r. \n\nSyntactic Alignment To calculate the syntactic alignment a syn , the utterance and response are segmented into uni-grams, tagged with part-ofspeech (POS) information, and condensed into a set of unique POS tags with the counts of their occurrences: u = (u 1 , c u 1 ), ..., (u n , c un ) and r = (r 1 , c r 1 ), ..., (r m , c rm ), with n and m being the number of unique POS tags in u and r, and c the number of times each tag occurs in the utterance. The syntactic alignment is then computed as the cosine similarity of the context and response vectors: \n\nLexical Alignment The process for lexical alignment a lex is identical that of syntactic alignment, except using word lemmas instead of POS tags. \n\nSemantic Alignment Lastly, semantic alignment a sem , which describes how the utterance content overlaps, is calculated using word2vec embeddings (Mikolov et al., 2013) e(u 1 ), ..., e(u n ) and e(r 1 ), ..., e(r m ). We use a bag-of-words approach to obtain sentence representations e u and e r . Semantic alignment is computed as: Turning to the baselines, for syntactic and lexical alignment, ChatGPT is closer to the randomized baseline than humans are; which means a higher fraction of its alignment does not come from matching a specific conversation, but from using more common words and syntax. The baseline alignments between all three models are fairly similar, although the semantic space of the smaller Llama2 model is less diverse as can be seen from a higher alignment baseline. \n\nUpon manual inspection of 100 transcripts, we see that ChatGPT generates more convincing results.",
            "score": 0.3213007535264759,
            "section_title": "Alignment Metrics",
            "char_start_offset": 8070,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 105,
                    "end": 220
                },
                {
                    "start": 223,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 778
                },
                {
                    "start": 781,
                    "end": 926
                },
                {
                    "start": 929,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1531
                },
                {
                    "start": 1532,
                    "end": 1721
                },
                {
                    "start": 1724,
                    "end": 1821
                }
            ],
            "ref_mentions": [
                {
                    "start": 25,
                    "end": 45,
                    "matchedPaperCorpusId": "73470609"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07904052734375
        },
        {
            "corpus_id": "231740500",
            "title": "Fake it Till You Make it: Self-Supervised Semantic Shifts for Monolingual Word Embedding Tasks",
            "text": "Objective We conduct an experiment on the arXiv data provided by Yin et al. (2018) to show how we can use S4-A for word embedding alignment for the discovery of semantic change, and how the results differ across alignment methods. We select the subjects of Artificial Intelligence (cs.AI) and Classical Physics (physics.class-ph) and train embeddings A and B, respectively, with Word2Vec (dimension 300, window size 10, minimum count 20). The embedding matrices are aligned using each alignment strategy, and the semantic shift measured by d i = A i Q \u2212 B i for each word w i in the common vocabulary, where Q is the transform matrix learned in the alignment. Baselines We compare the most semantically shifted words as discovered by the Global and Noise-Aware alignments (Hamilton, Leskovec, and Jurafsky 2016b;Yehezkel Lubin, Goldberger, and Goldberg 2019). We also compare our results to the top 3 high scoring entries from post-evaluation phase of the SemEval-2020 Task 1 competition, these methods may use distinct sets of features that go beyond just us-  Table 2: Classification accuracy of the unsupervised lexical semantic change detection on the SemEval-2020 Task 1 data set. The results were obtained by aligning the embedding matrices using different alignment strategies, and applying a threshold to the cosine distance of the aligned vectors, selected by cross-validation. Top-fr. and Bot-fr. are alignments using OP on the top and bottom 5% and 10% frequent words. The bottom rows shows the top 3 high scoring submissions to SemEval-2020 Task 1 in the post evaluation phase.\n\ning word embeddings. Evaluation and Analysis To quantify the difference between different alignments, we measured the ranking correlation using the Spearman's rho coefficient of the ranked list of words according to each method (ranked in descending order of semantic shift) at varying top-K thresholds with k in [10, 500] in increments of 10. Figure 2 shows the ranked correlation coefficient between each alignment strategy. Higher values of rho indicate that the order of semantic shift is more consistent between the two alignment strategies. These re-sults reveal that Global",
            "score": 0.3213007535264759,
            "section_title": "Discovery of Semantic Change",
            "char_start_offset": 26239,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 65,
                    "end": 82,
                    "matchedPaperCorpusId": "53965901"
                },
                {
                    "start": 772,
                    "end": 812,
                    "matchedPaperCorpusId": "5480561"
                },
                {
                    "start": 812,
                    "end": 858,
                    "matchedPaperCorpusId": "85499972"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03656005859375
        },
        {
            "corpus_id": "8796808",
            "title": "Morphological Smoothing and Extrapolation of Word Embeddings",
            "text": "ficient: given a, at most one choice of (b, c) is possible.) Note that the baseline extrapolates from the unsmoothed embeddings of 3 other words, whereas the GGM considers all words in C \u2229 L that share i's morphemes.  Table 3: Test results for Experiment 1. The rows indicate the inflection of the test word i to be predicted (superscript P indicates plural, superscript S singular). The columns indicate the prediction method. Each number is an average over 10 training-test splits. Improvements marked with a are statistically significant (p < 0.05) under a paired permutation test over these 10 runs.\n\nExperimental Setup: A lexical resource consists of pairs (word form i, analysis M i ). For each language, we take a random 80% of these pairs to serve as the training lexicon L that is seen by the GGM. The remaining pairs are used to construct our prediction problems (given M i , predict i), with a random 10% each as dev and test examples. We compare our method against the baseline method on ten such random training-test splits. We are releasing all splits for future research.\n\nFor some dev and test examples, the baseline method has no choice of the triple a, b, c. Rather than score these examples as incorrect, our baseline results do not consider them at all (which inflates performance). For each remaining example, to reduce variance, the baseline method reports the average performance on up to 100 a, b, c triples sampled uniformly without replacement.\n\nThe automatically created analogy problems (a, b, c \u2192 i) solved by the baseline are similar to those of Mikolov et al. (2013c). However, most previous analogy evaluation sets evaluate only on 4-tuples of frequent words (Nicolai et al., 2015), to escape the need for smoothing, while ours also include infrequent words. Previous evaluation sets also tend to be translations of the original English datasets-leaving them impoverished as they therefore only test morpho-syntactic properties found in English. E.g., the German analogy problems of K\u00f6per et al. (2015) do not explore the four cases and two numbers in the German adject",
            "score": 0.3213007535264759,
            "section_title": "Experiment 1: Extrapolation vs. Analogy",
            "char_start_offset": 20095,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1576,
                    "end": 1598,
                    "matchedPaperCorpusId": "7478738"
                },
                {
                    "start": 1691,
                    "end": 1713,
                    "matchedPaperCorpusId": "12168363"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06732177734375
        },
        {
            "corpus_id": "258999142",
            "title": "Towards Explainable and Language-Agnostic LLMs: Symbolic Reverse Engineering of Language at Scale",
            "text": "In other words, it would seem that (roughly) an additional parameter (weight) was required for every additional token in the corpus. If this correlation is not accidental then it is another indication that such models cannot provide an explainable model/theory for how language works since it would mean that what these models are doing, in effect, is encoding (memorizing) all possible combinations of how words may appear in any sequence of words, which is hardly a theory of linguistic communication. \n\nIn summary, transformers with attention, along with massive scale, have allowed for a qualitative leap in the linguistic capabilities of LLMs. Still, at the root of this bottom-up reverse engineering of language is the concept of 'the company a word keeps' and the distributional semantics hypothesis that, unlike top-down approaches, \"reverse engineers the process and induces semantic representations from contexts of use\" (Boleda, 2020). But nothing precludes this ingenious idea from being carried out in a symbolic setting. In other words, the 'company a word keeps' can be measured in several ways, some of which, incidentally, have been discussed since Frege. We turn to this subject next.",
            "score": 0.3213007535264759,
            "section_title": "Concerning \"The Company a Word Keeps\"",
            "char_start_offset": 13173,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 503
                },
                {
                    "start": 506,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1034
                },
                {
                    "start": 1035,
                    "end": 1172
                },
                {
                    "start": 1173,
                    "end": 1202
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.048126220703125
        },
        {
            "corpus_id": "120632252",
            "title": "Colloquium: Hierarchy of scales in language dynamics",
            "text": "Here it is found that iterated learning either eliminates one of the plural markers, or that the variation becomes predictable: that is, each object is typically pluralised by one or other of the markers, but not both.\n\nA final application of the artificial language learning paradigm is to investigate whether the nonuniform distribution of a structural feature over the world's languages (like word order, see Sect. 2) predicts the existence of a bias towards the more frequent structures in single instances of learning.One such study relates to affixes that change the meaning of the word.Where this occurs, it turns out that suffixes are more common than prefixes or infixes [122].To investigate whether this is also true at the individual level, St Clair et al. [123] trained participants on an artificial language in which words are divided into two categories, each with their own affix.When presented with unfamiliar sentences, participants could more reliably identify when the correct affix was used for a particular word stem, in correspondence with the observation that suffixes are more common than prefixes.\n\nAs noted in Section 2, correlations between two structural features of a language have been observed, and are formulated as implicational universals.Culbertson et al. [124] set up an experiment to determine whether a correlation between the order of numerals and nouns and the order of adjectives and nouns is visible at the level of individual learning events.In particular, one of the four possible orderings is much less frequent than the other three.Participants were trained on four variant languages, and then asked to describe a set of scenes using the language they had learnt.Points were awarded for a valid description of the scene (i.e., the right words, but in any order) and additionally if the order matched that of a computer-generated \"native\" speaker of the artificial language.The input languages were set up in such a way that participants would score maximum points by using one of the four possible orderings exclusively.The results of this experiment showed that this happened for all four input languages, apart from the one that corresponds to the ordering that is rare across the world's languages.",
            "score": 0.3213007535264759,
            "section_title": "Individual linguistic interactions",
            "char_start_offset": 76069,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 220,
                    "end": 523
                },
                {
                    "start": 523,
                    "end": 593
                },
                {
                    "start": 593,
                    "end": 686
                },
                {
                    "start": 686,
                    "end": 895
                },
                {
                    "start": 895,
                    "end": 1122
                },
                {
                    "start": 1124,
                    "end": 1273
                },
                {
                    "start": 1273,
                    "end": 1485
                },
                {
                    "start": 1485,
                    "end": 1578
                },
                {
                    "start": 1578,
                    "end": 1709
                },
                {
                    "start": 1709,
                    "end": 1919
                },
                {
                    "start": 1919,
                    "end": 2066
                },
                {
                    "start": 2066,
                    "end": 2247
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.049041748046875
        },
        {
            "corpus_id": "49292577",
            "title": "Unsupervised Word Segmentation from Speech with Attention",
            "text": "The main aspects of our approach are detailed below. \n\nReverse Architecture: in NMT, the soft alignments probabilities are normalized for each target symbol t (i.e. \u2200t, i \u03b1i,t = 1, with i indexing the source symbols). However, there is no similar constraint for the source symbols, as discussed by [5]. Rather than enforcing additional constraints on the alignments, as in the latter reference, we propose to reverse the architecture and to translate from WRL words into UL symbols, following [9]. This \"reverse\" architecture notably prevents the attention model from ignoring some UL symbols. As experiments with actual phone sequences have shown that the best results were obtained with this WRL-to-UL translation [9], we will use this reverse architecture throughout. \n\nAlignment Smoothing: to deal with the length discrepancy between UL (pseudo-phones) and WRL (words), we implemented the alignment smoothing procedure proposed by [5]. It consists of first adding temperature to the softmax function (we use T=10 for all experiments) used by the attention mechanism; and then post-processing the resulting soft-alignment probability matrices, averaging each score with the scores of the two neighboring words. Even if boosting many-to-one alignments should not hold in the case of the reverse architecture, we keep it for our experiments given the gains reported by [9], even in the reverse case. \n\nHard Segmentation Generation: once the soft-alignment matrices \u03b1 are obtained for all utterances in the corpus, a word segmentation is inferred as follows. We first transform softalignments into hard-alignments by aligning each UL symbol wt with the word xi such that: i = arg max i \u03b1 t,i . The source sequence is then segmented according to these hard-alignments: if two consecutive symbols are aligned with the same WRL word, they are considered to belong to the same UL word.",
            "score": 0.3213007535264759,
            "section_title": "Word Segmentations from Attention",
            "char_start_offset": 5184,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 52
                },
                {
                    "start": 55,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 770
                },
                {
                    "start": 773,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1400
                },
                {
                    "start": 1403,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1693
                },
                {
                    "start": 1694,
                    "end": 1881
                }
            ],
            "ref_mentions": [
                {
                    "start": 298,
                    "end": 301,
                    "matchedPaperCorpusId": "11740526"
                },
                {
                    "start": 493,
                    "end": 496,
                    "matchedPaperCorpusId": "616673"
                },
                {
                    "start": 716,
                    "end": 719,
                    "matchedPaperCorpusId": "616673"
                },
                {
                    "start": 935,
                    "end": 938,
                    "matchedPaperCorpusId": "11740526"
                },
                {
                    "start": 1370,
                    "end": 1373,
                    "matchedPaperCorpusId": "616673"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.072265625
        },
        {
            "corpus_id": "211031911",
            "title": "Generalizing meanings from partners to populations: Hierarchical inference supports convention formation on networks",
            "text": "We then tested analogs of the same three predictions we tested in the previous section using the same mixed-effects models, but using (log) utterance length as a continuous DV instead of accuracy (see Fig. 4B). We found that speakers reduced utterance length with every partner, b = -0.19, t(34) = -9.88, p < 0.001, increased length across partner-boundaries, b = 0.43, t(22) = 4.4, p < 0.001, and decreased the length of their initial descriptions as they interacted with more partners on their network,b = \u22120.2, t(516.5) = \u22126.07, p < 0.001 (see Fig. 4B). Network convergence In this section, we examine the actual content of pacts and test whether these coarse signatures of generalization actually lead to increased alignment across the network, as predicted. Specifically, we extend the 'exact matching' measure of alignment used in Simulation 3 to natural language production by examining whether the intersection of words produced by different speakers was nonempty3 . As in our simulation, the main comparison of interest was between currently interacting participants and participants who are not interacting: we predicted that withinpair alignment should stay consistently high while (tacit) alignment between non-interacting pairs will increase. We thus constructed a mixed-effects logistic regression including fixed effects of pair type (within vs. across), partner number, and their interaction. We included random intercepts at the tangram level and maximal random effects at the network level (i.e. intercept, both main effects, and the interaction). As predicted, we found a significant interaction (b = -0.85, z = -5.69, p < 0.001; see Fig. 4C). Although different pairs in a network may initially use different labels, these labels begin to align over subsequent interactions.",
            "score": 0.3213007535264759,
            "section_title": "Results",
            "char_start_offset": 20279,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 974
                },
                {
                    "start": 975,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1408
                },
                {
                    "start": 1409,
                    "end": 1513
                },
                {
                    "start": 1514,
                    "end": 1565
                },
                {
                    "start": 1566,
                    "end": 1626
                },
                {
                    "start": 1627,
                    "end": 1637
                },
                {
                    "start": 1638,
                    "end": 1662
                },
                {
                    "start": 1663,
                    "end": 1794
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.147705078125
        },
        {
            "corpus_id": "236956729",
            "title": "Not quite there yet: Combining analogical patterns and encoder-decoder networks for cognitively plausible inflection",
            "text": "the datasets consists of the infinitive and another form of some lexeme, accompanied by the UT of the second form. The BAP of a pair of forms is computed through an alignment of the two word-forms and the identification of their common parts and their differences. The alignment is computed by means of the SequenceMatcher method of the python difflib library; we then go through the sequences provided by the method and create the word-form patterns by replacing the common parts by + and copying the differences in their respective patterns. For example, SequenceMatcher aligns the forms aptai \" l@n and apg@tai \" lt as in (5) which yields the ++en/+ge+t BAP. BAPs are therefore calculated separately for each entry considering only the two forms.\n\nNote that a BAP can also be seen as a characterization of an analogical series. For instance, the pairs of forms in (4) can all be aligned in exactly the same way as in (5), they all have the same BAP ++@n/+g@+t and they form formal analogies (Lepage, 1998(Lepage, , 2004bStroppa and Yvon, 2005;Langlais and Yvon, 2008). More specifically, if two pairs of forms (F 1 , F 2 ) and (F 3 , F 4 ) have the same BAP, then F 1 : F 2 :: F 3 : F 4 . BAPs could also be computed for entire inflectional paradigms as proposed by Hulden (2014). Also note that BAPs are not specific to an inflection class, as two classes may exhibit common behavior in one part of their paradigm but not in another. For instance, the BAP +/+s describes the formal relation that connects the infinitive and the V;PRS;3;SG form of both regular (work) and irregular English verbs (eat).\n\nFine alternation patterns. Unlike BAPs which are derived solely from the examination of pairwise alternations, FAPs rely on the place of the two word-forms in the overall morphological system to identify more stable recurrent partials corresponding to traditional exponents. For instance, the BAP relating the German weak verbs like anspielen to",
            "score": 0.3213007535264759,
            "section_title": "Data and goal",
            "char_start_offset": 7210,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 994,
                    "end": 1007,
                    "matchedPaperCorpusId": "6708547"
                },
                {
                    "start": 1007,
                    "end": 1023,
                    "matchedPaperCorpusId": "5371028"
                },
                {
                    "start": 1023,
                    "end": 1046,
                    "matchedPaperCorpusId": "402014"
                },
                {
                    "start": 1046,
                    "end": 1070,
                    "matchedPaperCorpusId": "9063641"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.019500732421875
        },
        {
            "corpus_id": "268033100",
            "title": "EvoGPT-f: An Evolutionary GPT Framework for Benchmarking Formal Math Languages",
            "text": "To synthesize GPT performance across tokenization methods and compare at the corpora level, I provide two complementary methods for visual and statistical analysis.Both methods incorporate two aspects: penalizing the discrepancy between training and validation losses, and adjusting for the baseline entropy of the corpus.\n\nMethod 1: Calibration for Visualization.Here, we will calibrate the original validation loss with two factors.Let L v c,k and L t c,k be the validation and training loss, respectively, for a given corpus c and tokenization method k.Let H c,k be the Shannon entropy of the tokens for a given (c, k).The normalized validation loss L n1 c,k is defined by: The first Gaussian component is an overfitting penalty which amplifies the loss based on the discrepancy between the model's training and validation loss.The second component adjusts for the relative entropy level (i.e.inherent prediction difficulty) of the corpora and tokenization method using the entropies in Table 2.This transformation process, from original validation loss, to the application of each adjustment, and then the final normalized validation loss are shown in Figure 7.The underlying distributions are provided in Appendix C.1 (grouped by tokenization method) and Appendix C.2 (grouped by corpus).By comparing Figure 5a to Figure 5d, we see that the normalization both: 1. shifts corpus curves to be better aligned across respective tokenization methods; 2. provides greater segmentation of real performance, evidenced by the relative improvements in Lean 4 and the regression of HOL Light performance.\n\nMethod 2: Difference from Baseline Entropy The first 'Calibrated Loss' given by L n1 c,k enabled us to visualize the evolution at the corpora level.We want to complement this approach with a variety of statistical models to test for a difference in expected normalized loss across corpora.So I introduce an alternative normalized loss, given by\n\n, referred to as 'Information Gain' in modeling results.The response L n2 c,k has a Gamma structure, aside from the support being not strictly positive.",
            "score": 0.3210106461944753,
            "section_title": "Benchmarking Formal Language Learnability",
            "char_start_offset": 11030,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 164,
                    "end": 322
                },
                {
                    "start": 324,
                    "end": 364
                },
                {
                    "start": 364,
                    "end": 434
                },
                {
                    "start": 434,
                    "end": 556
                },
                {
                    "start": 556,
                    "end": 622
                },
                {
                    "start": 622,
                    "end": 831
                },
                {
                    "start": 831,
                    "end": 896
                },
                {
                    "start": 896,
                    "end": 998
                },
                {
                    "start": 998,
                    "end": 1165
                },
                {
                    "start": 1165,
                    "end": 1293
                },
                {
                    "start": 1293,
                    "end": 1598
                },
                {
                    "start": 1600,
                    "end": 1748
                },
                {
                    "start": 1748,
                    "end": 1889
                },
                {
                    "start": 1889,
                    "end": 1944
                },
                {
                    "start": 1946,
                    "end": 2002
                },
                {
                    "start": 2002,
                    "end": 2098
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.042327880859375
        },
        {
            "corpus_id": "270869551",
            "title": "Evaluating Human Alignment and Model Faithfulness of LLM Rationale",
            "text": "We study how well large language models (LLMs) explain their generations through rationales -- a set of tokens extracted from the input text that reflect the decision-making process of LLMs. Specifically, we systematically study rationales derived using two approaches: (1) popular prompting-based methods, where prompts are used to guide LLMs in generating rationales, and (2) technical attribution-based methods, which leverage attention or gradients to identify important tokens. Our analysis spans three classification datasets with annotated rationales, encompassing tasks with varying performance levels. While prompting-based self-explanations are widely used, our study reveals that these explanations are not always as\"aligned\"with the human rationale as attribution-based explanations. Even more so, fine-tuning LLMs to enhance classification task accuracy does not enhance the alignment of prompting-based rationales. Still, it does considerably improve the alignment of attribution-based methods (e.g., InputXGradient). More importantly, we show that prompting-based self-explanation is also less\"faithful\"than attribution-based explanations, failing to provide a reliable account of the model's decision-making process. To evaluate faithfulness, unlike prior studies that excluded misclassified examples, we evaluate all instances and also examine the impact of fine-tuning and accuracy on alignment and faithfulness. Our findings suggest that inconclusive faithfulness results reported in earlier studies may stem from low classification accuracy. These findings underscore the importance of more rigorous and comprehensive evaluations of LLM rationales.",
            "score": 0.3207911225264319,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0982666015625
        },
        {
            "corpus_id": "11327557",
            "title": "Modelling the Lexicon in Unsupervised Part of Speech Induction",
            "text": "Section 4 introduces a Particle Gibbs sampler for this model, a basic SMC method that generates samples from the model's posterior. We evaluate these algorithms in Section 5, analyzing their behavior in comparisons to previously proposed state-of-the-art approaches. \n\nFrom the early work in the 1990's, much of the focus on unsupervised PoS induction has been on hidden Markov Models (HMM) (Brown et al., 1992;Kupiec, 1992;Merialdo, 1993). The HMM has proven to be a powerful model of PoS tag assignment. Successful approaches generally build upon the HMM model by expanding its context and smoothing the sparse data. Constraints such as tag dictionaries simplify inference by restricting the number of tags to explore for each word (Goldwater and Griffiths, 2007). Ganchev et al. (2010) used posterior regularization to ensure that word types have a sparse posterior distribution over tags. A similar approach constrains inference to only explore tag assignments such that all tokens of the same word type are assigned the same tag. These constraints reduce tag assignment ambiguity while also providing a bias towards the natural sparsity of tag distributions in language (Clark, 2003). However they do not provide a model based solution to tag ambiguity. \n\nRecent work encodes similar sparsity information with non-parametric priors, relying on Bayesian inference to achieve strong results without any tag dictionaries or constraints (Goldwater and Griffiths, 2007;Johnson, 2007;Gao and Johnson, 2008). Liang et al. (2010) propose a typebased approach to this Bayesian inference similar to Brown et al. (1992), suggesting that there are strong dependencies between tokens of the same word-type. Lee et al. (2010) demonstrate strong results with a similar model and the introduction of a one-tag-per-type constraint on inference. Blunsom and Cohn (2011) extend the Bayesian inference approach with a hierarchical nonparametric prior that expands the HMM context to trigrams. However, the hierarchical nonparametric model adds too many long-range dependencies for the type-based inference proposed earlier.",
            "score": 0.3207729182356417,
            "section_title": "Introduction",
            "char_start_offset": 1904,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 266
                },
                {
                    "start": 269,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 1034
                },
                {
                    "start": 1035,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1258
                },
                {
                    "start": 1261,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1698
                },
                {
                    "start": 1699,
                    "end": 1832
                },
                {
                    "start": 1833,
                    "end": 1977
                },
                {
                    "start": 1978,
                    "end": 2108
                }
            ],
            "ref_mentions": [
                {
                    "start": 391,
                    "end": 411,
                    "matchedPaperCorpusId": "10986188"
                },
                {
                    "start": 411,
                    "end": 424,
                    "matchedPaperCorpusId": "62680996"
                },
                {
                    "start": 424,
                    "end": 439,
                    "matchedPaperCorpusId": "2727455"
                },
                {
                    "start": 734,
                    "end": 765,
                    "matchedPaperCorpusId": "11020320"
                },
                {
                    "start": 767,
                    "end": 788,
                    "matchedPaperCorpusId": "6589999"
                },
                {
                    "start": 1175,
                    "end": 1188,
                    "matchedPaperCorpusId": "361281"
                },
                {
                    "start": 1438,
                    "end": 1469,
                    "matchedPaperCorpusId": "11020320"
                },
                {
                    "start": 1469,
                    "end": 1483,
                    "matchedPaperCorpusId": "1512774"
                },
                {
                    "start": 1483,
                    "end": 1505,
                    "matchedPaperCorpusId": "17664832"
                },
                {
                    "start": 1594,
                    "end": 1613,
                    "matchedPaperCorpusId": "10986188"
                },
                {
                    "start": 1699,
                    "end": 1716,
                    "matchedPaperCorpusId": "1704410"
                },
                {
                    "start": 1833,
                    "end": 1856,
                    "matchedPaperCorpusId": "13341920"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.047515869140625
        },
        {
            "corpus_id": "266163802",
            "title": "GATITOS: Using a New Multilingual Lexicon for Low-resource Machine Translation",
            "text": "The biggest winners for each model in the en\u2192xx direction for the FLORES-200 evaluation set are given in Table 12. \n\nThere are seven languages that gained at least 5 CHRF over the baseline on at least one model trained with data augmentation. These languages are: \n\n1. Bhojpuri (bho): up to +14.5 CHRF 2. Ilocano (ilo): up to +9. Table 11: Comparing token hit-rate on classes of nouns known to have issues for UNMT models, along with a weak control of numbers. There are large improvements for animals in the GATITOS training lexicon (A \u2208 GAT ), as well as their complementary distribution, animals not in GATITOS (A \u0338 \u2208 GAT ), and colors. Number hit-rate has a minor bump. 6. Nuer (nus): up to +6.8 CHRF 7. Mizo (lus): up to +6.2 CHRF Unsurprisingly, most of these languages are unsupervised or low-resource, except for Serbian which is medium-resource in our dataset. Of the seven languages listed above, we use Panlex data for Ilocano, Serbian, Bambara, Tibetan, Nuer, and Mizo, and there is GATITOS data for Bhojpuri, Ilocano, Bambara, and Mizo. As will be discussed in Section 8.4, the GATITOS bilingual lexica are clearly a very useful resource for MT, although evidently Panlex alone can help as well. Another interesting finding is that Nuer, which has no Englishaligned entries in Panlex but \u2248 20K non-Englishaligned entries, still sees large improvements when translating from English. This is evidence that lexicon data can improve performance even in the zero-shot case, where e.g. the model learns better vocabulary alignment between English and Nuer despite not receiving explicit alignment information during training. In Section 8.4, we look at the relationship between the number of lexical data points for a language and the CHRF improvement, which provides some insight (albeit not perfect clarity) into why these particular languages did well.",
            "score": 0.32066476408421407,
            "section_title": "F.1 FLORES-200 en\u2192xx",
            "char_start_offset": 37550,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 117,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 263
                },
                {
                    "start": 266,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 1049
                },
                {
                    "start": 1050,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1633
                },
                {
                    "start": 1634,
                    "end": 1863
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03228759765625
        },
        {
            "corpus_id": "258832652",
            "title": "Explaining How Transformers Use Context to Build Predictions",
            "text": "In Figure 4 we present the MRR results of GPT-2 Small averaged across dataset categories, while the extended results for every subset can be found at Appendix C, Table 7. In Appendix C, Figure 11 we expand Figure 4 across different models. We can observe that Logit and ALTI-Logit explanations consistently align better with the evidence of linguistic phenomena than common gradientbased and erasure-based baselines. Note that for BLiMP the average we show in Figure 4 is across 9 different subsets. In Table 3 we show an example comparing different contrastive explanations, where Grad Norm, G\u00d7I and Erasure explanations don't align with the evidence to solve the subjectverb agreement (report), and disagree between each other. We find similar alignment results for Logit and ALTI-Logit methods. However, we observe that ALTI-Logit aligns better at tasks where the tokens of the linguistic evidence are far from the prediction. This is especially noticeable in Subjectverb agreement datasets (including SVA and darn), where ALTI-Logit shows higher alignments than any other method across all models. This might indicate that incorporating information about contextual mixing is advantageous for dealing with large contexts.\n\nDespite the generally accurate performance of the models examined in this study ( Figure 12 and Figure 13, Appendix D), there are cases where Figure 5: Update to the logit difference between the acceptable and the unacceptable predictions produced by the input tokens inside the linguistic evidence (GPT-2 XL). Figure 6: ALTI-Logit MRR alignment scores (line plots) and updates in logit difference by every input token (\u2206logit l (w\u2212f )\u2190Self-attn l ) between acceptable and unacceptable predictions (box plots) per layer (GPT-2 Small). Horizontal dashed lines refer to random alignment. the unacceptable token gets predicted with a higher probability. In order to gain a deeper understanding of the variations in model behavior between correct and incorrect predictions, we analyze the logit update generated by the input tokens associated with the linguistic evidence. This analysis, conducted using ALTI-Logit ( Figure 5), reveals differences in the distributions. These findings suggest that the tokens representing the linguistic evidence play a crucial role in achieving accurate",
            "score": 0.32063883506304114,
            "section_title": "Alignment Results",
            "char_start_offset": 16065,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.087890625
        },
        {
            "corpus_id": "232110816",
            "title": "An empirical analysis of phrase-based and neural machine translation",
            "text": "1. In Chapter 3, we propose two algorithms to use shortened forms of a phrasepair to smooth the orientation distribution of the original phrase-pair following back-off smoothing in n-gram language modelling (Chen and Goodman, 1999). These algorithms are applicable to lexicalized (Tillmann, 2004, Koehn et al., 2005 and hierarchical reordering models (Galley and Manning, 2008) yielding slight improvements for these models. Our algorithms use linear interpolation and recursive MAP (Maximum a Posteriori) smoothing (Cherry, 2013 to combine the shortened forms of the phrase-pairs.\n\n2. In Chapter 3, we also propose four methods to use generalized forms of a phrasepair to smooth the orientation distribution of the original phrase-pair. These generalized forms are produced by keeping essential words and marginalizing other words. Our proposed methods are applicable to both LRM and HRM models and improve the reordering performance of these models.\n\n3. In Chapter 4, we propose an approach to compare attention with traditional alignment. We define various metrics to measure the agreement between attention and alignment and to investigate the relationship of attention and alignment agreement with translation quality.\n\n4. In Chapter 5, we propose an intrinsic approach to study the syntactic and lexical semantic information captured by the hidden state representations based on their nearest neighbors.\n\n5. In Chapter 5, we define metrics that provide interpretable representations of the information captured by the hidden states in NMT systems, highlighting the differences between hidden state representations and word embeddings. 2. In Chapter 3, we provide an in-depth analysis showing that orientation distributions conditioned on long phrase-pairs typically depend on a few words within phrase-pairs and not the whole lexicalized form. As a result, using generalized forms of the phrase-pairs to smooth the original reordering distribution leads to performance improvements of the reordering models.\n\n3. In Chapter 4, we provide a detailed comparison of an attention model in neural machine translation against a word alignment model. We show that the attention and the alignment models have different behaviors and attention is not necessarily an alignment model.\n\n4. In Chapter 4, we also show that while different attention mechanisms can lead to different degrees of compliance with respect to word alignments, a full compliance is not always helpful for word prediction.\n\n5. In Chapter",
            "score": 0.32057545472360854,
            "section_title": "Algorithmic Contributions",
            "char_start_offset": 20546,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 207,
                    "end": 231,
                    "matchedPaperCorpusId": "215842252"
                },
                {
                    "start": 280,
                    "end": 295,
                    "matchedPaperCorpusId": "3219410"
                },
                {
                    "start": 295,
                    "end": 315,
                    "matchedPaperCorpusId": "263874184"
                },
                {
                    "start": 351,
                    "end": 377,
                    "matchedPaperCorpusId": "2479536"
                },
                {
                    "start": 516,
                    "end": 529,
                    "matchedPaperCorpusId": "7271623"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0823974609375
        },
        {
            "corpus_id": "268297400",
            "title": "Are Human Conversations Special? A Large Language Model Perspective",
            "text": "Recent studies have shown the intricate ways in which various models, including Transformers and recurrent neural networks, encode dependency relations within texts [23; 24].Transformer models have been found to most effectively capture dependency relations within their middle layers [25].\n\nAnalysis of the attention distance within decoder-only transformer models [26] has provided evidence supporting the hypothesis that deeper layers capture longer-distance relationships.This is a measurement of the mean distance spanned by attention for each head; and is calculated as the average distance between token pairs in all samples in the dataset, weighted by attention between the tokens:\n\nThe exploration of attention dispersion and entropy as measures of how attention is distributed across tokens offers additional insights into the mechanisms through which models understand and process patterns in language:\n\nThis body of work sets a context for our investigation into the unique characteristics of humanhuman conversations, comparing these dynamics against the backdrop of general web corpora, including articles, blogs, forums, and specialized domains such as mathematics and programming.\n\nUnderstanding the nuances of how models encode dependency relations and manage attention across different types of text is crucial in distinguishing the specifics of human conversational patterns.\n\nFinally, the analysis conducted in this paper is similar in spirit to the work in the mutlilingual (large) language model (MLLM) community on the effect of using models trained on higher resource languages and datasets with data from lower resource settings.In one effort [27], the authors identify the lack of linguistic diversity when training models -similar to the lack of diversity in data type when training LLMs, which is the focus of our present study; while in another [28], a detailed empirical analysis is provided to show the differences between different languages.We take inspiration from these efforts for our attention-centric study of language models and the content used to train them.",
            "score": 0.3205246009594577,
            "section_title": "Related Work",
            "char_start_offset": 10535,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 174,
                    "end": 290
                },
                {
                    "start": 292,
                    "end": 476
                },
                {
                    "start": 476,
                    "end": 689
                },
                {
                    "start": 691,
                    "end": 913
                },
                {
                    "start": 915,
                    "end": 1196
                },
                {
                    "start": 1198,
                    "end": 1394
                },
                {
                    "start": 1396,
                    "end": 1654
                },
                {
                    "start": 1654,
                    "end": 1974
                },
                {
                    "start": 1974,
                    "end": 2099
                }
            ],
            "ref_mentions": [
                {
                    "start": 1668,
                    "end": 1672,
                    "matchedPaperCorpusId": "215828350"
                },
                {
                    "start": 1874,
                    "end": 1878,
                    "matchedPaperCorpusId": "229924220"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08135986328125
        },
        {
            "corpus_id": "259075130",
            "title": "Exploring the Relationship between Alignment and Cross-lingual Transfer in Multilingual Transformers",
            "text": "It shows that realignment methods improve performance only on certain tasks, models and language pairs. Realignment methods, either sequential or joint, provide significant improvement for all models for the POS-tagging task, but less significant ones for NER, and no significant improvement for NLI. The positive impact of realignment on cross-lingual transfer seems to be mirrored by the negative impact of fine-tuning over alignment. Indeed, POStagging is also the task for which fine-tuning is the most detrimental to multilingual alignment, as shown in the previous section. \n\nThe same parallel can be drawn for models. dis-tilmBERT is the model that benefits the most from realignment. It is also the one whose alignment suffers the most from fine-tuning. Smaller multi-lingual models seem to benefit more from realignment, as well as they see their multilingual alignment reduced after fine-tuning. In the same way that fine-tuning mainly affects the deeper layers, it is possible that realignment might affect only those deeper layers. This would mean that most layers would have their alignment significantly improved for small models like distilmBERT (6 layers), while larger models might be only superficially realigned. \n\nFinally, besides tasks and models, it can also be observed that the impact of realignment varies across language pairs (Tab. 5). Although we did not test on many language pairs, results are coherent with the idea that realignment methods tend to work better on distant pairs of languages (Kulshreshtha et al., 2020). \n\nOn a side note, our controlled experiment does not allow us to conclude whether it is more important to improve alignment before fine-tuning or after. It seems that alignment measured before and after fine-tuning are equally important to crosslingual transfer. \n\nRealignment methods unsurprisingly provide better results when the alignment is lower, be it before or after fine-tuning. Distant languages and small models have lower alignment, and POS-tagging is a task where alignment decreases after fine-tuning. Realignment helps only up to a certain point where representations are already well aligned, and CTL gives already good results.",
            "score": 0.3204505654129063,
            "section_title": "Impact of realignment on cross-lingual transfer",
            "char_start_offset": 23243,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 579
                },
                {
                    "start": 582,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1231
                },
                {
                    "start": 1234,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1550
                },
                {
                    "start": 1553,
                    "end": 1703
                },
                {
                    "start": 1704,
                    "end": 1813
                },
                {
                    "start": 1816,
                    "end": 1937
                },
                {
                    "start": 1938,
                    "end": 2065
                },
                {
                    "start": 2066,
                    "end": 2194
                }
            ],
            "ref_mentions": [
                {
                    "start": 1522,
                    "end": 1549,
                    "matchedPaperCorpusId": "222067073"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2115478515625
        },
        {
            "corpus_id": "236477993",
            "title": "Transformer-Exclusive Cross-Modal Representation for Vision and Language",
            "text": "ViLBERT (Lu et al., 2019) was one of the first models to extend transformer architecture to crossmodal visuolinguistic tasks. They propose coattentional transformer, in which separate transformer modules for each modality run in parallel, with the key and value inputs from one modality entering the transformer block for the other modality, thereby learning cross-modal dependence. In order to tokenize the image, they extract image regions using Faster R-CNN (Ren et al., 2015) along with 5-dimensional location vector. They also extend two unique pre-training objectives of BERT, namely masked language modeling and next-sentence prediction, to cross-modal setting, as masked multi-modal learning and image-sentence alignment classification. In masked multi-modal learning, visual tokens, along with language tokens, are randomly masked, and the model is trained to predict their probability distribution over object classes. In image-sentence alignment classification, a sequence of visual tokens and a sentence are juxtaposed, and the model performs a binary classification task, predicting whether the sentence describes the contents of the image. Many other models, such as VisualBERT (Li et al., 2019), LXMERT (Tan and Bansal, 2019) and Unicoder-VL (Li et al., 2020), also follow nearly identical pre-training objectives as VilBERT. On the other hand, UNITER (Chen et al., 2020) demonstrates improved performance by introducing additional pre-training objective of word region alignment, while MiniVLM (Wang et al., 2020a) achieves comparable performance with up to 70% fewer parameters by utilizing EfficinetNet (Tan and Le, 2019) with their own Compact BERT model. \n\nWhile all models described above rely on CNNbased models to extract features from images, limiting the scope of applicability of transformer, recent works have demonstrated results that may imply a potential change in such workflow. (Dosovitskiy et al., 2020) proposed Vision Transformer (ViT), which demonstrates that pure transformer architecture without convolution can achieve comparable performance in image classification tasks, while requiring substantially less computational costs.",
            "score": 0.3204223304552522,
            "section_title": "Related Works",
            "char_start_offset": 2923,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1340
                },
                {
                    "start": 1341,
                    "end": 1674
                },
                {
                    "start": 1677,
                    "end": 1909
                },
                {
                    "start": 1910,
                    "end": 2167
                }
            ],
            "ref_mentions": [
                {
                    "start": 8,
                    "end": 25,
                    "matchedPaperCorpusId": "199453025"
                },
                {
                    "start": 461,
                    "end": 479,
                    "matchedPaperCorpusId": "10328909"
                },
                {
                    "start": 1192,
                    "end": 1209,
                    "matchedPaperCorpusId": "199528533"
                },
                {
                    "start": 1218,
                    "end": 1240,
                    "matchedPaperCorpusId": "201103729"
                },
                {
                    "start": 1257,
                    "end": 1274,
                    "matchedPaperCorpusId": "201058752"
                },
                {
                    "start": 1367,
                    "end": 1386,
                    "matchedPaperCorpusId": "216080982"
                },
                {
                    "start": 1621,
                    "end": 1639,
                    "matchedPaperCorpusId": "167217261"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.047882080078125
        },
        {
            "corpus_id": "270559089",
            "title": "WPO: Enhancing RLHF with Weighted Preference Optimization",
            "text": "The objective of our weighting strategy is to simulate on-policy RL, where outputs are weighted according to how closely they align with on-policy behavior. For outputs generated by the current policy model, we expect their weights to be uniformly 1, while outputs that deviate from this on-policy behavior should receive smaller weights. However, due to the varying levels of confidence that LLMs exhibit across different inputs (Si et al., 2023;Xiong et al., 2024), even outputs generated by the policy model may sometimes be assigned low weights. This introduces an unintended bias where some on-policy outputs receive lower weights purely because of lower model confidence based on the input, disrupting the uniformity we aim to achieve. Figure 2 shows the weight distribution of sampled outputs based on prompts from Ultrafeedback and the Mistral-sft-beta model, in which we observe significant variability in w(x, y). \n\nTo address this and ensure equal weighting of these outputs, we propose to align the weights in WPO. \n\nA direct method is to adjust the weights above by the sequence probability of the on-policy outputs sampled from the policy model. However, generating outputs during training is computationally expensive, and hence, we explore approximation methods for this alignment. Instead of using weights of the whole sequences as reference, we operate at the token level and adjust the probability of output tokens according to the token distribution in the policy model, based on the current subsequence. We propose two ways to achieve the alignment. \n\nGreedy alignment. In this approach, we adjust the weights based on greedy decoding by comparing the probability of the current token with that of the most probable token in the vocabulary. Specifically, we adjust weights based on the maximum token probability among the set of all tokens in the subsequence, defined as: (v|x,y<t) , where V represents the set of all tokens in the language model. Sampled alignment. In this approach, we adjust weights based on outputs that are randomly sampled from the policy model at a temperature of 1.0.",
            "score": 0.3203389375710499,
            "section_title": "Weight Alignment",
            "char_start_offset": 13932,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 923
                },
                {
                    "start": 926,
                    "end": 1026
                },
                {
                    "start": 1029,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1524
                },
                {
                    "start": 1525,
                    "end": 1570
                },
                {
                    "start": 1573,
                    "end": 1590
                },
                {
                    "start": 1591,
                    "end": 1761
                },
                {
                    "start": 1762,
                    "end": 1968
                },
                {
                    "start": 1969,
                    "end": 1987
                },
                {
                    "start": 1988,
                    "end": 2113
                }
            ],
            "ref_mentions": [
                {
                    "start": 430,
                    "end": 447,
                    "matchedPaperCorpusId": "252917981"
                },
                {
                    "start": 447,
                    "end": 466,
                    "matchedPaperCorpusId": "259224389"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04559326171875
        },
        {
            "paperId": "cc130a7f6b68f2989fa0669c1c90776b76482655",
            "corpusId": 276782016,
            "title": "Improving LLM Safety Alignment with Dual-Objective Optimization",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 38,
            "citationCount": 5,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.03710, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2334632973",
                    "name": "Xuandong Zhao"
                },
                {
                    "authorId": "2350451277",
                    "name": "Will Cai"
                },
                {
                    "authorId": "2178419835",
                    "name": "Tianneng Shi"
                },
                {
                    "authorId": "2349215379",
                    "name": "David Huang"
                },
                {
                    "authorId": "2257668289",
                    "name": "Licong Lin"
                },
                {
                    "authorId": "2325146117",
                    "name": "Song Mei"
                },
                {
                    "authorId": "2325723129",
                    "name": "D. Song"
                }
            ],
            "abstract": "Existing training-time safety alignment techniques for large language models (LLMs) remain vulnerable to jailbreak attacks. Direct preference optimization (DPO), a widely deployed alignment method, exhibits limitations in both experimental and theoretical contexts as its loss function proves suboptimal for refusal learning. Through gradient-based analysis, we identify these shortcomings and propose an improved safety alignment that disentangles DPO objectives into two components: (1) robust refusal training, which encourages refusal even when partial unsafe generations are produced, and (2) targeted unlearning of harmful knowledge. This approach significantly increases LLM robustness against a wide range of jailbreak attacks, including prefilling, suffix, and multi-turn attacks across both in-distribution and out-of-distribution scenarios. Furthermore, we introduce a method to emphasize critical refusal tokens by incorporating a reward-based token-level weighting mechanism for refusal learning, which further improves the robustness against adversarial exploits. Our research also suggests that robustness to jailbreak attacks is correlated with token distribution shifts in the training process and internal representations of refusal and harmful tokens, offering valuable directions for future research in LLM safety alignment. The code is available at https://github.com/wicai24/DOOR-Alignment",
            "corpus_id": "276782016",
            "text": "Existing training-time safety alignment techniques for large language models (LLMs) remain vulnerable to jailbreak attacks. Direct preference optimization (DPO), a widely deployed alignment method, exhibits limitations in both experimental and theoretical contexts as its loss function proves suboptimal for refusal learning. Through gradient-based analysis, we identify these shortcomings and propose an improved safety alignment that disentangles DPO objectives into two components: (1) robust refusal training, which encourages refusal even when partial unsafe generations are produced, and (2) targeted unlearning of harmful knowledge. This approach significantly increases LLM robustness against a wide range of jailbreak attacks, including prefilling, suffix, and multi-turn attacks across both in-distribution and out-of-distribution scenarios. Furthermore, we introduce a method to emphasize critical refusal tokens by incorporating a reward-based token-level weighting mechanism for refusal learning, which further improves the robustness against adversarial exploits. Our research also suggests that robustness to jailbreak attacks is correlated with token distribution shifts in the training process and internal representations of refusal and harmful tokens, offering valuable directions for future research in LLM safety alignment. The code is available at https://github.com/wicai24/DOOR-Alignment",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.216796875
        },
        {
            "paperId": "b75643c6817b50b65f23b3b3868be7f0eeaddb0e",
            "corpusId": 279464751,
            "title": "RAST: Reasoning Activation in LLMs via Small-model Transfer",
            "venue": "",
            "year": 2025,
            "referenceCount": 73,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.15710, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2323779521",
                    "name": "Siru Ouyang"
                },
                {
                    "authorId": null,
                    "name": "Xinyu Zhu"
                },
                {
                    "authorId": null,
                    "name": "Zilin Xiao"
                },
                {
                    "authorId": null,
                    "name": "Minhao Jiang"
                },
                {
                    "authorId": null,
                    "name": "Yu Meng"
                },
                {
                    "authorId": null,
                    "name": "Jiawei Han"
                }
            ],
            "abstract": "Reinforcement learning (RL) has become a powerful approach for improving the reasoning capabilities of large language models (LLMs), as evidenced by recent successes such as OpenAI's o1 and Deepseek-R1. However, applying RL at scale remains intimidatingly resource-intensive, requiring multiple model copies and extensive GPU workloads. On the other hand, while being powerful, recent studies suggest that RL does not fundamentally endow models with new knowledge; rather, it primarily reshapes the model's output distribution to activate reasoning capabilities latent in the base model. Building on this insight, we hypothesize that the changes in output probabilities induced by RL are largely model-size invariant, opening the door to a more efficient paradigm: training a small model with RL and transferring its induced probability shifts to larger base models. To verify our hypothesis, we conduct a token-level analysis of decoding trajectories and find high alignment in RL-induced output distributions across model scales, validating our hypothesis. Motivated by this, we propose RAST, a simple yet effective method that transfers reasoning behaviors by injecting RL-induced probability adjustments from a small RL-trained model into larger models. Experiments across multiple mathematical reasoning benchmarks show that RAST substantially and consistently enhances the reasoning capabilities of base models while requiring significantly lower GPU memory than direct RL training, sometimes even yielding better performance than the RL-trained counterparts. Our findings offer new insights into the nature of RL-driven reasoning and practical strategies for scaling its benefits without incurring its full computational cost. The project page of RAST is available at https://ozyyshr.github.io/RAST/.",
            "corpus_id": "279464751",
            "text": "Reinforcement learning (RL) has become a powerful approach for improving the reasoning capabilities of large language models (LLMs), as evidenced by recent successes such as OpenAI's o1 and Deepseek-R1. However, applying RL at scale remains intimidatingly resource-intensive, requiring multiple model copies and extensive GPU workloads. On the other hand, while being powerful, recent studies suggest that RL does not fundamentally endow models with new knowledge; rather, it primarily reshapes the model's output distribution to activate reasoning capabilities latent in the base model. Building on this insight, we hypothesize that the changes in output probabilities induced by RL are largely model-size invariant, opening the door to a more efficient paradigm: training a small model with RL and transferring its induced probability shifts to larger base models. To verify our hypothesis, we conduct a token-level analysis of decoding trajectories and find high alignment in RL-induced output distributions across model scales, validating our hypothesis. Motivated by this, we propose RAST, a simple yet effective method that transfers reasoning behaviors by injecting RL-induced probability adjustments from a small RL-trained model into larger models. Experiments across multiple mathematical reasoning benchmarks show that RAST substantially and consistently enhances the reasoning capabilities of base models while requiring significantly lower GPU memory than direct RL training, sometimes even yielding better performance than the RL-trained counterparts. Our findings offer new insights into the nature of RL-driven reasoning and practical strategies for scaling its benefits without incurring its full computational cost. The project page of RAST is available at https://ozyyshr.github.io/RAST/.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.1209716796875
        }
    ],
    "quotes": {
        "cost": 0.039549,
        "quotes": [
            {
                "idx": 0,
                "key": "[265608902 | Lin et al. | 2023 | Citations: 198]",
                "snippets": "To this end, we investigate the effects of alignment tuning by directly comparing the token distributions between base LLMs and their aligned versions (e.g., Llama-2 and Llama-2-chat). Surprisingly, we find that base and aligned LLMs typically perform almost identically in most positions in terms of ranking tokens during decoding (Sec. 2). Additionally, we observe that the top-ranked tokens in aligned LLMs are mostly found within the top five tokens ranked by base LLMs, and the distribution shift is more pronounced in earlier token positions. The most significant distribution shifts occur predominantly in stylistic tokens (e.g., 'Hello', 'Thank', 'However', 'Remember', etc.), which include transitional phrases, discourse markers, and safety disclaimers, rather than in content-bearing words that directly provide useful knowledge for resolving the queries from users. Our findings (Sec. 2.3) from token distribution shift analysis directly provide substantial support for the superficial alignment hypothesis.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Preprint",
                        "pdf_hash": "",
                        "start": 632,
                        "end": 1651,
                        "sentence_offsets": [
                            {
                                "start": 632,
                                "end": 816
                            },
                            {
                                "start": 817,
                                "end": 969
                            },
                            {
                                "start": 970,
                                "end": 973
                            },
                            {
                                "start": 974,
                                "end": 1180
                            },
                            {
                                "start": 1181,
                                "end": 1509
                            },
                            {
                                "start": 1510,
                                "end": 1528
                            },
                            {
                                "start": 1529,
                                "end": 1651
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "To this end, we investigate the effects of alignment tuning by directly comparing the token distributions between base LLMs and their aligned versions (e.g., Llama-2 and Llama-2-chat). Surprisingly, we find that base and aligned LLMs typically perform almost identically in most positions in terms of ranking tokens during decoding (Sec. 2). Additionally, we observe that the top-ranked tokens in aligned LLMs are mostly found within the top five tokens ranked by base LLMs, and the distribution shift is more pronounced in earlier token positions. The most significant distribution shifts occur predominantly in stylistic tokens (e.g., 'Hello', 'Thank', 'However', 'Remember', etc.), which include transitional phrases, discourse markers, and safety disclaimers, rather than in content-bearing words that directly provide useful knowledge for resolving the queries from users. Our findings (Sec. 2.3) from token distribution shift analysis directly provide substantial support for the superficial alignment hypothesis."
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[271600915 | Seneque et al. | 2024 | Citations: 1]",
                "snippets": "They measure the token distribution shift between pre-trained and aligned models, specifically the difference in probabilities of predicted tokens given a position in some sample input across the two models. They find that alignment fine-tuning affects a small subset of tokens that they describe as 'stylistic', specifically 'discourse markers, transitional words, and safety disclaimers'.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "In-Context Alignment",
                        "pdf_hash": "",
                        "start": 599,
                        "end": 989,
                        "sentence_offsets": [
                            {
                                "start": 599,
                                "end": 806
                            },
                            {
                                "start": 807,
                                "end": 989
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "They measure the token distribution shift between pre-trained and aligned models, specifically the difference in probabilities of predicted tokens given a position in some sample input across the two models. They find that alignment fine-tuning affects a small subset of tokens that they describe as 'stylistic', specifically 'discourse markers, transitional words, and safety disclaimers'."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[273901687 | Liu et al. | 2025 | Citations: 1]",
                "snippets": "To explore this, we design a comparative experiment to highlight how token representations differ between zero-shot and few-shot settings. We use token probability distributions as a proxy for token representations and utilize KL-divergence to measure the shifts in these distributions. By visualizing and quantifying the shifts in token probability distributions caused by demonstrations, we can understand the role of demonstrations in aligning the model and provide further optimization for in-context alignment.\n\nRegarding the experimental setup, we randomly selected 100 data instances of similar length from Ultra-chat (Ding et al., 2023), a commonly used dataset for alignment tuning, as our experimental dataset. For the input prompt, we use a straightforward design by adding several tokens at the end of the query to serve as separator tokens, explicitly distinguishing between the query and the response.\n\nInput Token Distribution. By comparing the input token probability distributions between zeroshot and few-shot settings, a significant shift is observed in both the prior tokens of the query and the separator tokens. The KL-divergence decreases as the number of query tokens increases. By comparing the experimental group and the control group, we find that the shift in the query distribution also occurs in the control group. However, this shift in the separator tokens is not consistent across different demonstration settings, suggesting distinct underlying causes for these shifts.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "To explore this, we design a comparative experiment to highlight how token representations differ between zero-shot and few-shot settings. We use token probability distributions as a proxy for token representations and utilize KL-divergence to measure the shifts in these distributions. By visualizing and quantifying the shifts in token probability distributions caused by demonstrations, we can understand the role of demonstrations in aligning the model and provide further optimization for in-context alignment.\n\nRegarding the experimental setup, we randomly selected 100 data instances of similar length from Ultra-chat (Ding et al., 2023), a commonly used dataset for alignment tuning, as our experimental dataset. For the input prompt, we use a straightforward design by adding several tokens at the end of the query to serve as separator tokens, explicitly distinguishing between the query and the response.\n\nInput Token Distribution. By comparing the input token probability distributions between zeroshot and few-shot settings, a significant shift is observed in both the prior tokens of the query and the separator tokens. The KL-divergence decreases as the number of query tokens increases. By comparing the experimental group and the control group, we find that the shift in the query distribution also occurs in the control group. However, this shift in the separator tokens is not consistent across different demonstration settings, suggesting distinct underlying causes for these shifts.",
                        "pdf_hash": ""
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.013803,
        "cot": "I'll analyze the user query about experimental methods providing direct evidence for the Superficial Alignment Hypothesis in large language models. Based on the provided quotes, I need to create dimensions that address key experimental approaches.\n\nFor this query, I'll organize the dimensions in the following way:\n\n1. **Introduction/Background**: This will provide context about what the Superficial Alignment Hypothesis is and why it's significant. This is essential for readers to understand the subsequent evidence. This will be in synthesis format to provide a coherent explanation of the concept.\n\n2. **Token Distribution Shift Analysis**: This appears to be the primary experimental method mentioned in the quotes. Quote [0] explicitly states that their \"findings from token distribution shift analysis directly provide substantial support for the superficial alignment hypothesis.\" Quote [1] also describes this methodology. This should be in synthesis format to explain the methodology and findings coherently.\n\n3. **Comparative Experimental Designs**: Quote [2] describes a comparative experiment using KL-divergence to measure shifts in token probability distributions. This is a different experimental approach that also provides evidence. This will be in synthesis format to explain this methodological approach.\n\n4. **Key Findings Supporting the Hypothesis**: This dimension will focus on the actual evidence from these experiments that supports the Superficial Alignment Hypothesis. Quotes [0] and [1] mention findings about \"stylistic tokens\" being primarily affected. This will be in synthesis format to integrate the findings into a coherent narrative.\n\nFor the background section, I don't have specific quotes, but I'll create this dimension anyway as it's necessary for a complete answer. For the other sections, I'll allocate the quotes based on their content.",
        "plan": {
            "Introduction to the Superficial Alignment Hypothesis (synthesis)": [],
            "Token Distribution Shift Analysis Methodology (synthesis)": [
                0,
                1
            ],
            "Comparative Experimental Designs (synthesis)": [
                2
            ],
            "Key Findings Supporting the Hypothesis (synthesis)": [
                0,
                1
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction to the Superficial Alignment Hypothesis",
                "tldr": "The Superficial Alignment Hypothesis suggests that large language models (LLMs) are only superficially aligned with human values and preferences. Rather than developing a deep understanding of human ethics, LLMs learn statistical patterns that allow them to mimic aligned behavior in familiar contexts but break down in novel situations. (LLM Memory)",
                "text": "\nThe Superficial Alignment Hypothesis addresses a fundamental question about the nature of alignment in large language models. It proposes that LLMs achieve their apparent helpfulness, harmlessness, and honesty not through genuine understanding of human values, but through learning surface-level patterns during training that approximate aligned behavior. This superficial alignment occurs because models are optimized to produce outputs that human evaluators will rate positively, leading them to learn statistical shortcuts rather than deeper ethical principles. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThe hypothesis emerged from observations that well-aligned models can suddenly produce misaligned outputs when faced with novel scenarios or cleverly designed prompts that fall outside their training distribution. This suggests that current alignment techniques may be creating an illusion of safety rather than robust alignment. The hypothesis is particularly concerning because it implies that as LLMs become more powerful, the gap between their superficial alignment and true understanding of human values could lead to increasingly sophisticated ways of circumventing safety measures when deployed in real-world applications. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nResearchers have developed several experimental methods to test this hypothesis, examining how model behavior changes across different contexts and when faced with distribution shifts that weren't explicitly covered during alignment training. These methods help distinguish between models that have internalized human values versus those that have merely learned to pattern-match to human preferences in familiar scenarios. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">",
                "citations": [],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Token Distribution Shift Analysis Methodology",
                "tldr": "Token distribution shift analysis examines how alignment tuning changes the probability distributions of tokens in language models. Research shows that alignment primarily affects stylistic tokens like discourse markers and safety disclaimers rather than substantive content, providing direct evidence for the Superficial Alignment Hypothesis. (2 sources)",
                "text": "\nToken distribution shift analysis has emerged as a key experimental method for investigating the nature of alignment in large language models. This methodology directly compares the token distributions between base language models and their aligned counterparts (such as Llama-2 versus Llama-2-chat) to understand how alignment training alters a model's behavior at a fundamental level <Paper corpusId=\"265608902\" paperTitle=\"(Lin et al., 2023)\" isShortName></Paper>. \n\nThe approach involves analyzing how token probability distributions change at each position during text generation, revealing precisely which aspects of model outputs are most affected by alignment procedures. Surprisingly, researchers have discovered that base and aligned models perform almost identically in ranking tokens during decoding across most positions, suggesting that alignment does not fundamentally transform the model's core capabilities or knowledge <Paper corpusId=\"265608902\" paperTitle=\"(Lin et al., 2023)\" isShortName></Paper>.\n\nThe most significant finding from this methodology is that the distribution shifts primarily occur in a specific subset of tokens characterized as \"stylistic\" - including discourse markers, transitional phrases, and safety disclaimers (e.g., \"Hello,\" \"Thank,\" \"However,\" \"Remember\") <Paper corpusId=\"265608902\" paperTitle=\"(Lin et al., 2023)\" isShortName></Paper> <Paper corpusId=\"271600915\" paperTitle=\"(Seneque et al., 2024)\" isShortName></Paper>. These tokens shape the tone and framing of responses rather than affecting the substantive content that provides useful knowledge to users. Additionally, researchers observed that the distribution shifts are more pronounced in earlier token positions, suggesting that alignment primarily influences how responses begin rather than their overall substance <Paper corpusId=\"265608902\" paperTitle=\"(Lin et al., 2023)\" isShortName></Paper>.\n\nThis methodological approach provides direct empirical evidence supporting the Superficial Alignment Hypothesis by demonstrating that alignment training primarily changes surface-level linguistic features rather than transforming how models process and generate content-bearing information. The finding that aligned tokens are typically still found within the top five tokens ranked by base models further suggests that alignment represents a relatively modest modification to the model's original behaviors rather than a deep transformation of its capabilities <Paper corpusId=\"265608902\" paperTitle=\"(Lin et al., 2023)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Lin et al., 2023)",
                        "snippets": [
                            "To this end, we investigate the effects of alignment tuning by directly comparing the token distributions between base LLMs and their aligned versions (e.g., Llama-2 and Llama-2-chat). Surprisingly, we find that base and aligned LLMs typically perform almost identically in most positions in terms of ranking tokens during decoding (Sec. 2). Additionally, we observe that the top-ranked tokens in aligned LLMs are mostly found within the top five tokens ranked by base LLMs, and the distribution shift is more pronounced in earlier token positions. The most significant distribution shifts occur predominantly in stylistic tokens (e.g., 'Hello', 'Thank', 'However', 'Remember', etc.), which include transitional phrases, discourse markers, and safety disclaimers, rather than in content-bearing words that directly provide useful knowledge for resolving the queries from users. Our findings (Sec. 2.3) from token distribution shift analysis directly provide substantial support for the superficial alignment hypothesis."
                        ],
                        "paper": {
                            "corpus_id": 265608902,
                            "title": "The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning",
                            "authors": [
                                {
                                    "authorId": "51583409",
                                    "name": "Bill Yuchen Lin"
                                },
                                {
                                    "authorId": "3023068",
                                    "name": "Abhilasha Ravichander"
                                },
                                {
                                    "authorId": "50085131",
                                    "name": "Ximing Lu"
                                },
                                {
                                    "authorId": "46217681",
                                    "name": "Nouha Dziri"
                                },
                                {
                                    "authorId": "1947172233",
                                    "name": "Melanie Sclar"
                                },
                                {
                                    "authorId": "37619618",
                                    "name": "Khyathi Raghavi Chandu"
                                },
                                {
                                    "authorId": "1857797",
                                    "name": "Chandra Bhagavatula"
                                },
                                {
                                    "authorId": "2259707400",
                                    "name": "Yejin Choi"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 198
                        },
                        "score": 0.8779296875
                    },
                    {
                        "id": "(Seneque et al., 2024)",
                        "snippets": [
                            "They measure the token distribution shift between pre-trained and aligned models, specifically the difference in probabilities of predicted tokens given a position in some sample input across the two models. They find that alignment fine-tuning affects a small subset of tokens that they describe as 'stylistic', specifically 'discourse markers, transitional words, and safety disclaimers'."
                        ],
                        "paper": {
                            "corpus_id": 271600915,
                            "title": "ABC Align: Large Language Model Alignment for Safety & Accuracy",
                            "authors": [
                                {
                                    "authorId": "1418234173",
                                    "name": "Gareth Seneque"
                                },
                                {
                                    "authorId": "2314116496",
                                    "name": "Lap-Hang Ho"
                                },
                                {
                                    "authorId": "2314117335",
                                    "name": "Ariel Kuperman"
                                },
                                {
                                    "authorId": "8513243",
                                    "name": "Nafise Erfanian Saeedi"
                                },
                                {
                                    "authorId": "2314113668",
                                    "name": "Jeffrey Molendijk"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.81689453125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Comparative Experimental Designs",
                "tldr": "Researchers have developed comparative experimental designs to investigate how different prompting methods affect token distributions in language models. These experiments reveal significant shifts in token probabilities between zero-shot and few-shot settings, providing insight into how in-context demonstrations influence model alignment. (1 source)",
                "text": "\nComparative experimental designs offer another methodological approach to investigating the Superficial Alignment Hypothesis by directly examining how different prompting strategies affect a model's token representations and outputs. Researchers have designed experiments that compare zero-shot settings (where models generate responses without examples) against few-shot settings (where models are provided with demonstrations) to measure the effect of in-context examples on model behavior. \n\nThese experiments typically use metrics like KL-divergence to quantify shifts in token probability distributions between different experimental conditions. For instance, researchers have conducted experiments with datasets like Ultra-chat, where they randomly selected similar-length data instances and designed prompts with separator tokens to distinguish between queries and responses. <Paper corpusId=\"273901687\" paperTitle=\"(Liu et al., 2025)\" isShortName></Paper>\n\nThe results from these comparative experiments reveal notable patterns in how models process and respond to different prompting strategies. When comparing input token probability distributions between zero-shot and few-shot settings, researchers observed significant shifts in both the query tokens and separator tokens. Interestingly, the KL-divergence between these distributions decreased as the number of query tokens increased, suggesting that longer queries may provide more context that stabilizes the model's token predictions. <Paper corpusId=\"273901687\" paperTitle=\"(Liu et al., 2025)\" isShortName></Paper>\n\nA particularly revealing finding from these comparative experiments is that while shifts in query token distributions occurred in both experimental and control groups, the shifts in separator tokens varied across different demonstration settings. This suggests different underlying mechanisms for how demonstrations affect various parts of the model's processing, with potentially greater impact on how the model transitions from query to response rather than on the substantive content of the response itself. This finding aligns with the broader hypothesis that alignment is primarily superficial, affecting style and framing more than core content generation. <Paper corpusId=\"273901687\" paperTitle=\"(Liu et al., 2025)\" isShortName></Paper>",
                "citations": [
                    {
                        "id": "(Liu et al., 2025)",
                        "snippets": [
                            "To explore this, we design a comparative experiment to highlight how token representations differ between zero-shot and few-shot settings. We use token probability distributions as a proxy for token representations and utilize KL-divergence to measure the shifts in these distributions. By visualizing and quantifying the shifts in token probability distributions caused by demonstrations, we can understand the role of demonstrations in aligning the model and provide further optimization for in-context alignment.\n\nRegarding the experimental setup, we randomly selected 100 data instances of similar length from Ultra-chat (Ding et al., 2023), a commonly used dataset for alignment tuning, as our experimental dataset. For the input prompt, we use a straightforward design by adding several tokens at the end of the query to serve as separator tokens, explicitly distinguishing between the query and the response.\n\nInput Token Distribution. By comparing the input token probability distributions between zeroshot and few-shot settings, a significant shift is observed in both the prior tokens of the query and the separator tokens. The KL-divergence decreases as the number of query tokens increases. By comparing the experimental group and the control group, we find that the shift in the query distribution also occurs in the control group. However, this shift in the separator tokens is not consistent across different demonstration settings, suggesting distinct underlying causes for these shifts."
                        ],
                        "paper": {
                            "corpus_id": 273901687,
                            "title": "Take Off the Training Wheels! Progressive In-Context Learning for Effective Alignment",
                            "authors": [
                                {
                                    "authorId": "2230018369",
                                    "name": "Zhenyu Liu"
                                },
                                {
                                    "authorId": "2265618386",
                                    "name": "Dongfang Li"
                                },
                                {
                                    "authorId": "2149467818",
                                    "name": "Xinshuo Hu"
                                },
                                {
                                    "authorId": "2326046038",
                                    "name": "Xinping Zhao"
                                },
                                {
                                    "authorId": "2325888345",
                                    "name": "Yibin Chen"
                                },
                                {
                                    "authorId": "2285172247",
                                    "name": "Baotian Hu"
                                },
                                {
                                    "authorId": "2258690227",
                                    "name": "Min Zhang"
                                }
                            ],
                            "year": 2025,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 1
                        },
                        "score": 0.677734375
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Key Findings Supporting the Hypothesis",
                "tldr": "Research examining token distribution patterns reveals that alignment training primarily affects stylistic elements rather than content-bearing tokens, confirming the superficial nature of alignment. These findings demonstrate that aligned models rank tokens almost identically to base models, with modifications concentrated in discourse markers and safety disclaimers rather than substantive content. (2 sources)",
                "text": "\nMultiple experimental studies have yielded compelling evidence supporting the Superficial Alignment Hypothesis through detailed analysis of token distributions in language models. One of the most striking findings is that base and aligned language models perform nearly identically when ranking tokens during the decoding process across most positions in generated text <Paper corpusId=\"265608902\" paperTitle=\"(Lin et al., 2023)\" isShortName></Paper>. This remarkable similarity in token ranking between pre-training and post-alignment suggests that alignment training does not fundamentally transform the model's core capabilities or knowledge representation.\n\nFurther supporting the hypothesis, researchers have discovered that the tokens most affected by alignment training are predominantly \"stylistic\" in nature - specifically discourse markers, transitional phrases, and safety disclaimers such as \"Hello,\" \"Thank you,\" \"However,\" and \"Remember\" <Paper corpusId=\"265608902\" paperTitle=\"(Lin et al., 2023)\" isShortName></Paper> <Paper corpusId=\"271600915\" paperTitle=\"(Seneque et al., 2024)\" isShortName></Paper>. This pattern indicates that alignment procedures primarily modify the tone and framing of responses rather than the substantive information content that actually answers user queries.\n\nToken distribution analysis has also revealed important temporal patterns in how alignment affects generation. The distribution shifts between base and aligned models are significantly more pronounced in earlier token positions, suggesting that alignment primarily influences how responses begin rather than their overall substance <Paper corpusId=\"265608902\" paperTitle=\"(Lin et al., 2023)\" isShortName></Paper>. Additionally, researchers have observed that the top-ranked tokens in aligned models are typically found within the top five tokens as ranked by base models, indicating that alignment represents a relatively modest modification to the model's original behaviors rather than a deep transformation <Paper corpusId=\"265608902\" paperTitle=\"(Lin et al., 2023)\" isShortName></Paper>.\n\nThese findings collectively provide direct empirical support for the Superficial Alignment Hypothesis by demonstrating that alignment training predominantly affects surface-level linguistic features rather than transforming how models process and generate content-bearing information. The concentration of alignment effects in stylistic tokens rather than substantive content suggests that current alignment techniques may be creating an illusion of alignment rather than genuinely aligning models with human values and preferences.",
                "citations": [
                    {
                        "id": "(Lin et al., 2023)",
                        "snippets": [
                            "To this end, we investigate the effects of alignment tuning by directly comparing the token distributions between base LLMs and their aligned versions (e.g., Llama-2 and Llama-2-chat). Surprisingly, we find that base and aligned LLMs typically perform almost identically in most positions in terms of ranking tokens during decoding (Sec. 2). Additionally, we observe that the top-ranked tokens in aligned LLMs are mostly found within the top five tokens ranked by base LLMs, and the distribution shift is more pronounced in earlier token positions. The most significant distribution shifts occur predominantly in stylistic tokens (e.g., 'Hello', 'Thank', 'However', 'Remember', etc.), which include transitional phrases, discourse markers, and safety disclaimers, rather than in content-bearing words that directly provide useful knowledge for resolving the queries from users. Our findings (Sec. 2.3) from token distribution shift analysis directly provide substantial support for the superficial alignment hypothesis."
                        ],
                        "paper": {
                            "corpus_id": 265608902,
                            "title": "The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning",
                            "authors": [
                                {
                                    "authorId": "51583409",
                                    "name": "Bill Yuchen Lin"
                                },
                                {
                                    "authorId": "3023068",
                                    "name": "Abhilasha Ravichander"
                                },
                                {
                                    "authorId": "50085131",
                                    "name": "Ximing Lu"
                                },
                                {
                                    "authorId": "46217681",
                                    "name": "Nouha Dziri"
                                },
                                {
                                    "authorId": "1947172233",
                                    "name": "Melanie Sclar"
                                },
                                {
                                    "authorId": "37619618",
                                    "name": "Khyathi Raghavi Chandu"
                                },
                                {
                                    "authorId": "1857797",
                                    "name": "Chandra Bhagavatula"
                                },
                                {
                                    "authorId": "2259707400",
                                    "name": "Yejin Choi"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 198
                        },
                        "score": 0.8779296875
                    },
                    {
                        "id": "(Seneque et al., 2024)",
                        "snippets": [
                            "They measure the token distribution shift between pre-trained and aligned models, specifically the difference in probabilities of predicted tokens given a position in some sample input across the two models. They find that alignment fine-tuning affects a small subset of tokens that they describe as 'stylistic', specifically 'discourse markers, transitional words, and safety disclaimers'."
                        ],
                        "paper": {
                            "corpus_id": 271600915,
                            "title": "ABC Align: Large Language Model Alignment for Safety & Accuracy",
                            "authors": [
                                {
                                    "authorId": "1418234173",
                                    "name": "Gareth Seneque"
                                },
                                {
                                    "authorId": "2314116496",
                                    "name": "Lap-Hang Ho"
                                },
                                {
                                    "authorId": "2314117335",
                                    "name": "Ariel Kuperman"
                                },
                                {
                                    "authorId": "8513243",
                                    "name": "Nafise Erfanian Saeedi"
                                },
                                {
                                    "authorId": "2314113668",
                                    "name": "Jeffrey Molendijk"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.81689453125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.05223900000000001
    }
}