{
    "query": "are the common model retrieval language models \n",
    "user_id": "lib_user",
    "task_id": "e79e5c6a-12c4-4ba9-aee3-0df7eeb9a5b7",
    "timestamp": "2025-06-23T23:23:32.073546",
    "n_retrieval": 256,
    "n_retrieved": 247,
    "n_candidates": 47,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.46020300000000003,
    "decomposed_query": {
        "rewritten_query": "Common model retrieval language models.",
        "keyword_query": "common model retrieval language models",
        "search_filters": {
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.009363,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Advancing Trace Recovery Evaluation - Applied Information Retrieval in a Software Engineering Context",
            "venue": "arXiv.org",
            "year": 2016,
            "reference_count": 246,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1602.07633, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "145654045",
                    "name": "Markus Borg"
                }
            ],
            "abstract": "Successful development of software systems involves efficient navigation among software artifacts. However, as artifacts continuously are produced and modified, engineers are typically plagued by challenging information landscapes. One state-of-practice approach to structure information is to establish trace links between artifacts, a practice that is also enforced by several development standards. Unfortunately, manually maintaining trace links in an evolving system is a tedious task. To tackle this issue, several researchers have proposed treating the capture and recovery of trace links as an Information Retrieval (IR) problem. The goal of this thesis is to contribute to the evaluation of IR-based trace recovery, both by presenting new empirical results and by suggesting how to increase the strength of evidence in future evaluative studies. This thesis is based on empirical software engineering research. The work contains a Systematic Literature Review (SLR) of previous evaluations of IR-based trace recovery. We show that a majority of previous evaluations have been technology-oriented, conducted in \"the cave of IR evaluation\", using small datasets as experimental input. Also, software artifacts originating from student projects have frequently been used in evaluations. We conducted a survey among traceability researchers, and found that a majority consider student artifacts to be only partly representative to industrial counterparts. Moreover, few traceability researchers had validated student artifacts for industrial representativeness before using them as experimental input. Our findings call for additional case studies to evaluate IR-based trace recovery within the full complexity of an industrial setting. Also, this thesis contributes to the body of empirical evidence of IR-based trace recovery in two experiments with industrial software artifacts. The technology-oriented experiment highlights the clear dependence between datasets and the accuracy of IR-based trace recovery, in line with findings from the SLR. The human-oriented experiment investigates how different quality levels of tool output affect the tracing accuracy of engineers. While the results are not conclusive, there are indications that it is worthwhile to investigate further into the actual value of improving tool support for IR-based trace recovery. Finally, we present how tools and methods are evaluated in the general field of IR research, and propose a taxonomy of evaluation contexts tailored for IR-based trace recovery. (Less)",
            "corpus_id": 13892149,
            "sentences": [
                {
                    "corpus_id": "13892149",
                    "title": "Advancing Trace Recovery Evaluation - Applied Information Retrieval in a Software Engineering Context",
                    "text": "Two common models are the Binary Independence retrieval Model (BIM) [25] [82] and Probabilistic Inference Networks [92]. A subset of probabilistic retrieval estimate Language Models (LM) for each document. Documents are then ranked based on the probability that a document would generate the terms of a query [78]. A later refinement of simple LMs, topic models, describes documents as a mixture over topics, where each topic is characterized by an LM. Examples include probabilistic latent semantic indexing [45] and Latent Dirichlet Allocation (LDA) [11].",
                    "score": 0.3979634658246799,
                    "section_title": "Information Retrieval -Satisfying an Information Need",
                    "char_start_offset": 13882,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 120
                        },
                        {
                            "start": 121,
                            "end": 205
                        },
                        {
                            "start": 206,
                            "end": 314
                        },
                        {
                            "start": 315,
                            "end": 452
                        },
                        {
                            "start": 453,
                            "end": 557
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 68,
                            "end": 72,
                            "matchedPaperCorpusId": "3252915"
                        },
                        {
                            "start": 115,
                            "end": 119,
                            "matchedPaperCorpusId": "11025023"
                        },
                        {
                            "start": 309,
                            "end": 313,
                            "matchedPaperCorpusId": "2759772"
                        },
                        {
                            "start": 509,
                            "end": 513,
                            "matchedPaperCorpusId": "7605995"
                        },
                        {
                            "start": 552,
                            "end": 556,
                            "matchedPaperCorpusId": "3177797"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.994140625
                }
            ],
            "relevance_judgement": 0.994140625,
            "relevance_judgment_input_expanded": "# Title: Advancing Trace Recovery Evaluation - Applied Information Retrieval in a Software Engineering Context\n# Venue: arXiv.org\n# Authors: Markus Borg\n## Abstract\nSuccessful development of software systems involves efficient navigation among software artifacts. However, as artifacts continuously are produced and modified, engineers are typically plagued by challenging information landscapes. One state-of-practice approach to structure information is to establish trace links between artifacts, a practice that is also enforced by several development standards. Unfortunately, manually maintaining trace links in an evolving system is a tedious task. To tackle this issue, several researchers have proposed treating the capture and recovery of trace links as an Information Retrieval (IR) problem. The goal of this thesis is to contribute to the evaluation of IR-based trace recovery, both by presenting new empirical results and by suggesting how to increase the strength of evidence in future evaluative studies. This thesis is based on empirical software engineering research. The work contains a Systematic Literature Review (SLR) of previous evaluations of IR-based trace recovery. We show that a majority of previous evaluations have been technology-oriented, conducted in \"the cave of IR evaluation\", using small datasets as experimental input. Also, software artifacts originating from student projects have frequently been used in evaluations. We conducted a survey among traceability researchers, and found that a majority consider student artifacts to be only partly representative to industrial counterparts. Moreover, few traceability researchers had validated student artifacts for industrial representativeness before using them as experimental input. Our findings call for additional case studies to evaluate IR-based trace recovery within the full complexity of an industrial setting. Also, this thesis contributes to the body of empirical evidence of IR-based trace recovery in two experiments with industrial software artifacts. The technology-oriented experiment highlights the clear dependence between datasets and the accuracy of IR-based trace recovery, in line with findings from the SLR. The human-oriented experiment investigates how different quality levels of tool output affect the tracing accuracy of engineers. While the results are not conclusive, there are indications that it is worthwhile to investigate further into the actual value of improving tool support for IR-based trace recovery. Finally, we present how tools and methods are evaluated in the general field of IR research, and propose a taxonomy of evaluation contexts tailored for IR-based trace recovery. (Less)\n## Information Retrieval -Satisfying an Information Need\nTwo common models are the Binary Independence retrieval Model (BIM) [25] [82] and Probabilistic Inference Networks [92]. A subset of probabilistic retrieval estimate Language Models (LM) for each document. Documents are then ranked based on the probability that a document would generate the terms of a query [78]. A later refinement of simple LMs, topic models, describes documents as a mixture over topics, where each topic is characterized by an LM. Examples include probabilistic latent semantic indexing [45] and Latent Dirichlet Allocation (LDA) [11].",
            "reference_string": "[13892149 | Borg | 2016 | Citations: 3]"
        },
        {
            "title": "Planning Ahead in Generative Retrieval: Guiding Autoregressive Generation through Simultaneous Decoding",
            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
            "year": 2024,
            "reference_count": 90,
            "citation_count": 16,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.14600, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2029235362",
                    "name": "Hansi Zeng"
                },
                {
                    "authorId": "2294680636",
                    "name": "Chen Luo"
                },
                {
                    "authorId": "2257027392",
                    "name": "Hamed Zamani"
                }
            ],
            "abstract": "This paper introduces PAG-a novel optimization and decoding approach that guides autoregressive generation of document identifiers in generative retrieval models through simultaneous decoding. To this aim, PAG constructs a set-based and sequential identifier for each document. Motivated by the bag-of-words assumption in information retrieval, the set-based identifier is built on lexical tokens. The sequential identifier, on the other hand, is obtained via quantizing relevance-based representations of documents. Extensive experiments on MSMARCO and TREC Deep Learning Track data reveal that PAG outperforms the state-of-the-art generative retrieval model by a large margin (e.g., 15.6% MRR improvements on MS MARCO), while achieving 22\u00d7 speed up in terms of query latency.",
            "corpus_id": 269302879,
            "sentences": [
                {
                    "corpus_id": "269302879",
                    "title": "Planning Ahead in Generative Retrieval: Guiding Autoregressive Generation through Simultaneous Decoding",
                    "text": "Classic Neural IR Models: With the emergence of large language models (LLMs) [14,17,39,43,50] and large-scale information retrieval datasets [4,32], neural-based IR models have demonstrated superior results over the traditional lexical-matching models, such as BM25 [55].In general, these IR models can fall into three categories: (1) cross-encoder models [42,48,79], (2) dense retrieval models [22,24,28,30,38,72], and (3) sparse retrieval models [12,13,18,19].The cross-encoder model is often parameterized with LLMs, such as BERT [17] or T5 [50], and takes the concatenation of query and document pair as input to predict their relevant score.This model is effective but slow and is usually used for re-ranking.As for retrieval, the dense retrieval model often uses the bi-encoder architecture to encode the query and document separately into the low-dimensional hidden space and apply the approximate nearest neighborhood (ANN) [40,68] search for fast retrieval.Sparse retrieval is an alternative method for retrieval, in which it encodes the query and document into the high-dimensional vector space, and usually, each element in the vector represents the importance score of a certain token.To filter out those useful tokens, the L1 [69] or FLOPs [18,19,44] regularizer will be incorporated into the objective function to sparsify the high-dimension vectors.For retrieval, the inverted index will be employed similar to BM25.Generative Retrieval Models: Generate Retrieval (GR), diverges from the traditional \"index-then-retrieve\" paradigm used in the sparse and dense retrieval models, offering a novel approach for document retrieval.In GR, each document is represented as a unique document identifier (DocID), and a sequence-to-sequence model is trained to generate relevant DocIDs given a query.\n\nDocIDs are usually fixed in the fine-tuning stage and hence serving as bottleneck for affecting the effectiveness of GR models.",
                    "score": 0.43245092595319135,
                    "section_title": "RELATED WORK",
                    "char_start_offset": 5336,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 271
                        },
                        {
                            "start": 271,
                            "end": 462
                        },
                        {
                            "start": 462,
                            "end": 646
                        },
                        {
                            "start": 646,
                            "end": 714
                        },
                        {
                            "start": 714,
                            "end": 966
                        },
                        {
                            "start": 966,
                            "end": 1197
                        },
                        {
                            "start": 1197,
                            "end": 1364
                        },
                        {
                            "start": 1364,
                            "end": 1431
                        },
                        {
                            "start": 1431,
                            "end": 1642
                        },
                        {
                            "start": 1642,
                            "end": 1805
                        },
                        {
                            "start": 1807,
                            "end": 1934
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 81,
                            "end": 84,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 90,
                            "end": 93,
                            "matchedPaperCorpusId": "204838007"
                        },
                        {
                            "start": 144,
                            "end": 147,
                            "matchedPaperCorpusId": "86611921"
                        },
                        {
                            "start": 266,
                            "end": 270,
                            "matchedPaperCorpusId": "207178704"
                        },
                        {
                            "start": 363,
                            "end": 366,
                            "matchedPaperCorpusId": "252993059"
                        },
                        {
                            "start": 399,
                            "end": 402,
                            "matchedPaperCorpusId": "233231706"
                        },
                        {
                            "start": 402,
                            "end": 405,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 405,
                            "end": 408,
                            "matchedPaperCorpusId": "216553223"
                        },
                        {
                            "start": 411,
                            "end": 414,
                            "matchedPaperCorpusId": "248426770"
                        },
                        {
                            "start": 452,
                            "end": 455,
                            "matchedPaperCorpusId": "252212320"
                        },
                        {
                            "start": 458,
                            "end": 461,
                            "matchedPaperCorpusId": "235792467"
                        },
                        {
                            "start": 533,
                            "end": 537,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 544,
                            "end": 548,
                            "matchedPaperCorpusId": "204838007"
                        },
                        {
                            "start": 932,
                            "end": 936,
                            "matchedPaperCorpusId": "8915893"
                        },
                        {
                            "start": 1239,
                            "end": 1243,
                            "matchedPaperCorpusId": "52229883"
                        },
                        {
                            "start": 1257,
                            "end": 1260,
                            "matchedPaperCorpusId": "235792467"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9921875
                }
            ],
            "relevance_judgement": 0.9921875,
            "relevance_judgment_input_expanded": "# Title: Planning Ahead in Generative Retrieval: Guiding Autoregressive Generation through Simultaneous Decoding\n# Venue: Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\n# Authors: Hansi Zeng, Chen Luo, Hamed Zamani\n## Abstract\nThis paper introduces PAG-a novel optimization and decoding approach that guides autoregressive generation of document identifiers in generative retrieval models through simultaneous decoding. To this aim, PAG constructs a set-based and sequential identifier for each document. Motivated by the bag-of-words assumption in information retrieval, the set-based identifier is built on lexical tokens. The sequential identifier, on the other hand, is obtained via quantizing relevance-based representations of documents. Extensive experiments on MSMARCO and TREC Deep Learning Track data reveal that PAG outperforms the state-of-the-art generative retrieval model by a large margin (e.g., 15.6% MRR improvements on MS MARCO), while achieving 22\u00d7 speed up in terms of query latency.\n## RELATED WORK\nClassic Neural IR Models: With the emergence of large language models (LLMs) [14,17,39,43,50] and large-scale information retrieval datasets [4,32], neural-based IR models have demonstrated superior results over the traditional lexical-matching models, such as BM25 [55].In general, these IR models can fall into three categories: (1) cross-encoder models [42,48,79], (2) dense retrieval models [22,24,28,30,38,72], and (3) sparse retrieval models [12,13,18,19].The cross-encoder model is often parameterized with LLMs, such as BERT [17] or T5 [50], and takes the concatenation of query and document pair as input to predict their relevant score.This model is effective but slow and is usually used for re-ranking.As for retrieval, the dense retrieval model often uses the bi-encoder architecture to encode the query and document separately into the low-dimensional hidden space and apply the approximate nearest neighborhood (ANN) [40,68] search for fast retrieval.Sparse retrieval is an alternative method for retrieval, in which it encodes the query and document into the high-dimensional vector space, and usually, each element in the vector represents the importance score of a certain token.To filter out those useful tokens, the L1 [69] or FLOPs [18,19,44] regularizer will be incorporated into the objective function to sparsify the high-dimension vectors.For retrieval, the inverted index will be employed similar to BM25.Generative Retrieval Models: Generate Retrieval (GR), diverges from the traditional \"index-then-retrieve\" paradigm used in the sparse and dense retrieval models, offering a novel approach for document retrieval.In GR, each document is represented as a unique document identifier (DocID), and a sequence-to-sequence model is trained to generate relevant DocIDs given a query.\n\nDocIDs are usually fixed in the fine-tuning stage and hence serving as bottleneck for affecting the effectiveness of GR models.",
            "reference_string": "[269302879 | Zeng et al. | 2024 | Citations: 16]"
        },
        {
            "title": "Book Review: Statistical Language Models for Information Retrieval by ChengXiang Zhai",
            "venue": "International Conference on Computational Logic",
            "year": 2008,
            "reference_count": 166,
            "citation_count": 203,
            "influential_citation_count": 17,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.mitpressjournals.org/doi/pdf/10.1162/coli.2010.36.2.36200",
                "status": "BRONZE",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/N07-5002, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1736467",
                    "name": "ChengXiang Zhai"
                }
            ],
            "abstract": "Statistical language models have recently been successfully applied to many information retrieval problems. A great deal of recent work has shown that statistical language models not only achieve superior empirical performance, but also facilitate parameter tuning and provide a more principled way, in general, for modeling various kinds of complex and non-traditional retrieval problems.",
            "corpus_id": 29740950,
            "sentences": [
                {
                    "corpus_id": "29740950",
                    "title": "Book Review: Statistical Language Models for Information Retrieval by ChengXiang Zhai",
                    "text": "r classical information retrieval models, from similarity-based models to probabilistic relevance and inference models, and r two retrieval frameworks, the axiomatic and decision-theoretic retrieval frameworks, both co-developed by the author.\n\nThis overview covers the main aspects of these models and frameworks, and allows the author to introduce notions that help position the statistical language model to be presented in the following chapters.\n\nThe remainder of the book is then devoted to the presentation of the statistical language models used in information retrieval, and their application to special tasks. Chapter 3 presents the standard, simple query likelihood retrieval model. After reviewing the basic idea behind this model (at the core of current statistical language models for information retrieval), the author presents the different event models that have been considered: Multinomial, Multiple Bernoulli, and Multiple Poisson. He then explains the strategy for parameter estimation and the different smoothing techniques one can rely on. This presentation is followed by a discussion of the relation between smoothing and tf-idf weighting, which paves the way for the two-stage smoothing method presented in the following section. This chapter is very well written and presents, in a clear yet complete way, the fundamentals of the query likelihood retrieval model.\n\nThe following chapter, entitled Complex Query Likelihood Retrieval Model, is devoted to extensions of the simple query likelihood model of Chapter 3. In particular, the author reviews document-specific smoothing methods, based on document clustering and document expansion, the use of n-gram models and Markov random fields, as well as the full Bayesian query likelihood and the translation models. The intent here is not to provide a detailed description of these elements, but rather to give an overview, and pointers to extensions and models related to the query likelihood retrieval model. A major conceptual drawback of the query likelihood retrieval model lies in the fact that feedback cannot be naturally accommodated. Indeed, in this model, a query is seen as a sample from the document model; adding terms according to a completely different process renders the \"sample view\" inadequate. In order to better accommodate feedback, the Kullback-Leibler divergence retrieval model has been introduced. In this model, a query model (associated with a word probability distribution) and a document model (also associated with a word probability distribution) are compared with the Kull",
                    "score": 0.4064586197702927,
                    "section_title": "body",
                    "char_start_offset": 1,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9921875
                },
                {
                    "corpus_id": "29740950",
                    "title": "Book Review: Statistical Language Models for Information Retrieval by ChengXiang Zhai",
                    "text": "Statistical language models have recently been successfully applied to many information retrieval problems. A great deal of recent work has shown that statistical language models not only achieve superior empirical performance, but also facilitate parameter tuning and provide a more principled way, in general, for modeling various kinds of complex and non-traditional retrieval problems.",
                    "score": 0.4024982055363106,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95703125
                }
            ],
            "relevance_judgement": 0.9921875,
            "relevance_judgment_input_expanded": "# Title: Book Review: Statistical Language Models for Information Retrieval by ChengXiang Zhai\n# Venue: International Conference on Computational Logic\n# Authors: ChengXiang Zhai\n## Abstract\nStatistical language models have recently been successfully applied to many information retrieval problems. A great deal of recent work has shown that statistical language models not only achieve superior empirical performance, but also facilitate parameter tuning and provide a more principled way, in general, for modeling various kinds of complex and non-traditional retrieval problems.\n## body\nr classical information retrieval models, from similarity-based models to probabilistic relevance and inference models, and r two retrieval frameworks, the axiomatic and decision-theoretic retrieval frameworks, both co-developed by the author.\n\nThis overview covers the main aspects of these models and frameworks, and allows the author to introduce notions that help position the statistical language model to be presented in the following chapters.\n\nThe remainder of the book is then devoted to the presentation of the statistical language models used in information retrieval, and their application to special tasks. Chapter 3 presents the standard, simple query likelihood retrieval model. After reviewing the basic idea behind this model (at the core of current statistical language models for information retrieval), the author presents the different event models that have been considered: Multinomial, Multiple Bernoulli, and Multiple Poisson. He then explains the strategy for parameter estimation and the different smoothing techniques one can rely on. This presentation is followed by a discussion of the relation between smoothing and tf-idf weighting, which paves the way for the two-stage smoothing method presented in the following section. This chapter is very well written and presents, in a clear yet complete way, the fundamentals of the query likelihood retrieval model.\n\nThe following chapter, entitled Complex Query Likelihood Retrieval Model, is devoted to extensions of the simple query likelihood model of Chapter 3. In particular, the author reviews document-specific smoothing methods, based on document clustering and document expansion, the use of n-gram models and Markov random fields, as well as the full Bayesian query likelihood and the translation models. The intent here is not to provide a detailed description of these elements, but rather to give an overview, and pointers to extensions and models related to the query likelihood retrieval model. A major conceptual drawback of the query likelihood retrieval model lies in the fact that feedback cannot be naturally accommodated. Indeed, in this model, a query is seen as a sample from the document model; adding terms according to a completely different process renders the \"sample view\" inadequate. In order to better accommodate feedback, the Kullback-Leibler divergence retrieval model has been introduced. In this model, a query model (associated with a word probability distribution) and a document model (also associated with a word probability distribution) are compared with the Kull",
            "reference_string": "[29740950 | Zhai | 2008 | Citations: 203]"
        },
        {
            "title": "Retrieval-based Language Models and Applications",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2023,
            "reference_count": 39,
            "citation_count": 88,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2023.acl-tutorials.6.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2023.acl-tutorials.6, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2290402940",
                    "name": "Akari Asai"
                },
                {
                    "authorId": "48872685",
                    "name": "Sewon Min"
                },
                {
                    "authorId": "49164966",
                    "name": "Zexuan Zhong"
                },
                {
                    "authorId": "2286629648",
                    "name": "Danqi Chen"
                }
            ],
            "abstract": "Retrieval-based language models (LMs) have shown impressive performance on diverse NLP tasks. In this tutorial, we will provide a comprehensive and coherent overview of recent advances in retrieval-based LMs. We will start by providing preliminaries covering the foundation of LMs (e.g., masked LMs, autoregressive LMs) and retrieval systems (e.g., nearest-neighbor search). We will then detail recent progress in retrieval-based models, focusing on their model architectures and learning approaches. Finally, we will show how retrieval-based LMs are adapted to downstream applications, and extended to multilingual and multi-modal settings. Finally, we will use an exercise to showcase the effectiveness of retrieval-based LMs.",
            "corpus_id": 263866951,
            "sentences": [
                {
                    "corpus_id": "263866951",
                    "title": "Retrieval-based Language Models and Applications",
                    "text": "Retrieval-based language models (LMs) have shown impressive performance on diverse NLP tasks. In this tutorial, we will provide a comprehensive and coherent overview of recent advances in retrieval-based LMs. We will start by providing preliminaries covering the foundation of LMs (e.g., masked LMs, autoregressive LMs) and retrieval systems (e.g., nearest-neighbor search). We will then detail recent progress in retrieval-based models, focusing on their model architectures and learning approaches. Finally, we will show how retrieval-based LMs are adapted to downstream applications, and extended to multilingual and multi-modal settings. Finally, we will use an exercise to showcase the effectiveness of retrieval-based LMs.",
                    "score": 0.490161307119962,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.990234375
                }
            ],
            "relevance_judgement": 0.990234375,
            "relevance_judgment_input_expanded": "# Title: Retrieval-based Language Models and Applications\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Akari Asai, Sewon Min, Zexuan Zhong, Danqi Chen\n## Abstract\nRetrieval-based language models (LMs) have shown impressive performance on diverse NLP tasks. In this tutorial, we will provide a comprehensive and coherent overview of recent advances in retrieval-based LMs. We will start by providing preliminaries covering the foundation of LMs (e.g., masked LMs, autoregressive LMs) and retrieval systems (e.g., nearest-neighbor search). We will then detail recent progress in retrieval-based models, focusing on their model architectures and learning approaches. Finally, we will show how retrieval-based LMs are adapted to downstream applications, and extended to multilingual and multi-modal settings. Finally, we will use an exercise to showcase the effectiveness of retrieval-based LMs.\n",
            "reference_string": "[263866951 | Asai et al. | 2023 | Citations: 88]"
        },
        {
            "title": "Parameterized Neural Network Language Models for Information Retrieval",
            "venue": "arXiv.org",
            "year": 2015,
            "reference_count": 28,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1510.01562, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1703777",
                    "name": "Benjamin Piwowarski"
                },
                {
                    "authorId": "1782552",
                    "name": "S. Lamprier"
                },
                {
                    "authorId": "3379650",
                    "name": "N. Despr\u00e8s"
                }
            ],
            "abstract": "Information Retrieval (IR) models need to deal with two difficult issues, vocabulary mismatch and term dependencies. Vocabulary mismatch corresponds to the difficulty of retrieving relevant documents that do not contain exact query terms but semantically related terms. Term dependencies refers to the need of considering the relationship between the words of the query when estimating the relevance of a document. A multitude of solutions has been proposed to solve each of these two problems, but no principled model solve both. In parallel, in the last few years, language models based on neural networks have been used to cope with complex natural language processing tasks like emotion and paraphrase detection. Although they present good abilities to cope with both term dependencies and vocabulary mismatch problems, thanks to the distributed representation of words they are based upon, such models could not be used readily in IR, where the estimation of one language model per document (or query) is required. This is both computationally unfeasible and prone to over-fitting. Based on a recent work that proposed to learn a generic language model that can be modified through a set of document-specific parameters, we explore use of new neural network models that are adapted to ad-hoc IR tasks. Within the language model IR framework, we propose and study the use of a generic language model as well as a document-specific language model. Both can be used as a smoothing component, but the latter is more adapted to the document at hand and has the potential of being used as a full document language model. We experiment with such models and analyze their results on TREC-1 to 8 datasets.",
            "corpus_id": 8374684,
            "sentences": [
                {
                    "corpus_id": "8374684",
                    "title": "Parameterized Neural Network Language Models for Information Retrieval",
                    "text": "Language models are probabilistic generative models of text -viewed as a sequence of terms. If a text is composed of a sequence of terms t 1...n , where each t i corresponds to a word in a pre-defined vocabulary, we can compute the probability of observing this sequence given the language model M as: \n\nLanguage models are used in IR as a simple yet effective way to compute the relevance of a document to a query [27]. There exists different types of language models for IR, but one of the most standard is to equate the relevance of the document d with the likelihood of the language model generating the query, using the evaluated document language model M d . \n\nwhere M d is the so-called document language model, which is the model within the family of models M that maximizes the probability of observing the document d composed of terms d 1...N , that is: \n\nAmong the different families of generative models, the n-gram multinomial family is the most used in IR, with n usually equal to 1. The multinomial model assumes the independence of terms given the n \u2212 1 previous ones in the sequence. Formally, if M is within this family, then \n\nwhere \u03b8 is a conditional probability table giving the probability of observing the term t i after having observed the sequence t i\u2212n+1...i\u22121 . \n\nFor a given document, the parameters \u03b8 that maximize equation ( 1) can be computed in a closed form formula: \n\nwhere \u2022 correspond to any term of the vocabulary (i.e. t i\u2212n+1...i\u22121 \u2022 corresponds to the sequence t i\u2212n+1 , . . . , t i\u22121 , u where u \u2208 W). With n = 1, we get a simple unigram model that does not consider the context of the term (note that in that case the denominator is equal to the length of the document). \n\nFor instance, in a document about Boston, \"trail\" is more likely to occur after \"freedom\" than in other documents.",
                    "score": 0.4064586197702927,
                    "section_title": "Background: Language Models for IR",
                    "char_start_offset": 11083,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 91
                        },
                        {
                            "start": 92,
                            "end": 301
                        },
                        {
                            "start": 304,
                            "end": 420
                        },
                        {
                            "start": 421,
                            "end": 664
                        },
                        {
                            "start": 667,
                            "end": 863
                        },
                        {
                            "start": 866,
                            "end": 997
                        },
                        {
                            "start": 998,
                            "end": 1100
                        },
                        {
                            "start": 1101,
                            "end": 1143
                        },
                        {
                            "start": 1146,
                            "end": 1288
                        },
                        {
                            "start": 1291,
                            "end": 1399
                        },
                        {
                            "start": 1402,
                            "end": 1456
                        },
                        {
                            "start": 1457,
                            "end": 1516
                        },
                        {
                            "start": 1517,
                            "end": 1542
                        },
                        {
                            "start": 1543,
                            "end": 1712
                        },
                        {
                            "start": 1715,
                            "end": 1829
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98876953125
                }
            ],
            "relevance_judgement": 0.98876953125,
            "relevance_judgment_input_expanded": "# Title: Parameterized Neural Network Language Models for Information Retrieval\n# Venue: arXiv.org\n# Authors: Benjamin Piwowarski, S. Lamprier, N. Despr\u00e8s\n## Abstract\nInformation Retrieval (IR) models need to deal with two difficult issues, vocabulary mismatch and term dependencies. Vocabulary mismatch corresponds to the difficulty of retrieving relevant documents that do not contain exact query terms but semantically related terms. Term dependencies refers to the need of considering the relationship between the words of the query when estimating the relevance of a document. A multitude of solutions has been proposed to solve each of these two problems, but no principled model solve both. In parallel, in the last few years, language models based on neural networks have been used to cope with complex natural language processing tasks like emotion and paraphrase detection. Although they present good abilities to cope with both term dependencies and vocabulary mismatch problems, thanks to the distributed representation of words they are based upon, such models could not be used readily in IR, where the estimation of one language model per document (or query) is required. This is both computationally unfeasible and prone to over-fitting. Based on a recent work that proposed to learn a generic language model that can be modified through a set of document-specific parameters, we explore use of new neural network models that are adapted to ad-hoc IR tasks. Within the language model IR framework, we propose and study the use of a generic language model as well as a document-specific language model. Both can be used as a smoothing component, but the latter is more adapted to the document at hand and has the potential of being used as a full document language model. We experiment with such models and analyze their results on TREC-1 to 8 datasets.\n## Background: Language Models for IR\nLanguage models are probabilistic generative models of text -viewed as a sequence of terms. If a text is composed of a sequence of terms t 1...n , where each t i corresponds to a word in a pre-defined vocabulary, we can compute the probability of observing this sequence given the language model M as: \n\nLanguage models are used in IR as a simple yet effective way to compute the relevance of a document to a query [27]. There exists different types of language models for IR, but one of the most standard is to equate the relevance of the document d with the likelihood of the language model generating the query, using the evaluated document language model M d . \n\nwhere M d is the so-called document language model, which is the model within the family of models M that maximizes the probability of observing the document d composed of terms d 1...N , that is: \n\nAmong the different families of generative models, the n-gram multinomial family is the most used in IR, with n usually equal to 1. The multinomial model assumes the independence of terms given the n \u2212 1 previous ones in the sequence. Formally, if M is within this family, then \n\nwhere \u03b8 is a conditional probability table giving the probability of observing the term t i after having observed the sequence t i\u2212n+1...i\u22121 . \n\nFor a given document, the parameters \u03b8 that maximize equation ( 1) can be computed in a closed form formula: \n\nwhere \u2022 correspond to any term of the vocabulary (i.e. t i\u2212n+1...i\u22121 \u2022 corresponds to the sequence t i\u2212n+1 , . . . , t i\u22121 , u where u \u2208 W). With n = 1, we get a simple unigram model that does not consider the context of the term (note that in that case the denominator is equal to the length of the document). \n\nFor instance, in a document about Boston, \"trail\" is more likely to occur after \"freedom\" than in other documents.",
            "reference_string": "[8374684 | Piwowarski et al. | 2015 | Citations: 1]"
        },
        {
            "title": "On the Estimation and Use of Statistical Modelling in Information Retrieval",
            "venue": "arXiv.org",
            "year": 2019,
            "reference_count": 471,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1904.00289, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "8304471",
                    "name": "Casper Petersen"
                }
            ],
            "abstract": "Several tasks in information retrieval (IR) rely on assumptions regarding the distribution of some property (such as term frequency) in the data being processed. This thesis argues that such distributional assumptions can lead to incorrect conclusions and proposes a statistically principled method for determining the \"true\" distribution. This thesis further applies this method to derive a new family of ranking models that adapt their computations to the statistics of the data being processed. Experimental evaluation shows results on par or better than multiple strong baselines on several TREC collections. Overall, this thesis concludes that distributional assumptions can be replaced with an effective, efficient and principled method for determining the \"true\" distribution and that using the \"true\" distribution can lead to improved retrieval performance.",
            "corpus_id": 90238494,
            "sentences": [
                {
                    "corpus_id": "90238494",
                    "title": "On the Estimation and Use of Statistical Modelling in Information Retrieval",
                    "text": "A language model (LM) is, in its most basic form, a probability distribution over terms in a language where each term, t is assigned a non-zero probability denoting its probability of occurrence in the \"language\". A \"language\" here is defined as a non-empty finite sequence of symbols or terms. Given a query q and document d \u2208C for some collection C , d 's LM, \u03b8 d , is a probabilistic model that estimates the probability that q was generated by d . In other words, each document is viewed as a sample from the language, and its relevance to q is estimated as the probability that q was generated from this sample.\n\nUsing language models for retrieval in IR was initiated by Ponte and Croft [349] who developed a retrieval model based on the multiple-Bernoulli statistical model. Because the multiple-Bernoulli model is computationally expensive 10 [42] and ignores term-frequencies -a useful feature for retrieval [374] -a number of authors, most notably Hiemstra [203] and Song and Croft [403], introduced the multinomial model as a representation of documents which has since become the de facto choice [132, p. 254]. Liu and Croft [276] and Zhai [457,Chap. 6] provide a comprehensive survey of areas-relevance feedback (e.g. [131,260,348]), distributed IR (e.g. [396,453]), query ambiguity (e.g. [133,134]) and ad hoc retrieval (e.g. [200,204,231,249,269,307,309])-where LMs have been used successfully.\n\nFormally, ranking documents relative to a query q is estimated through Bayes' rule:\n\nwhere Pr(d ) denotes any prior knowledge that d is relevant to any query and Pr(q) is a normalisation constant. The generic LM in Eqn. 2.23 contains two probability distributions: the distribution of Pr(q|d ) and the distribution of Pr(d ). The probability Pr(q|d ) is usually estimated by assuming query terms are i.i.d:\n\ncorresponding to a unigram query-likelihood LM. The term-independence assumption",
                    "score": 0.46644915669159726,
                    "section_title": "Language Modelling",
                    "char_start_offset": 41785,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 693,
                            "end": 698,
                            "matchedPaperCorpusId": "2759772"
                        },
                        {
                            "start": 851,
                            "end": 855,
                            "matchedPaperCorpusId": "16290284"
                        },
                        {
                            "start": 967,
                            "end": 972,
                            "matchedPaperCorpusId": "5242820"
                        },
                        {
                            "start": 992,
                            "end": 997,
                            "matchedPaperCorpusId": "8264008"
                        },
                        {
                            "start": 1137,
                            "end": 1142,
                            "matchedPaperCorpusId": "45558661"
                        },
                        {
                            "start": 1152,
                            "end": 1157,
                            "matchedPaperCorpusId": "61572040"
                        },
                        {
                            "start": 1231,
                            "end": 1236,
                            "matchedPaperCorpusId": "6245052"
                        },
                        {
                            "start": 1236,
                            "end": 1240,
                            "matchedPaperCorpusId": "14116318"
                        },
                        {
                            "start": 1240,
                            "end": 1244,
                            "matchedPaperCorpusId": "60342325"
                        },
                        {
                            "start": 1268,
                            "end": 1273,
                            "matchedPaperCorpusId": "1010654"
                        },
                        {
                            "start": 1273,
                            "end": 1277,
                            "matchedPaperCorpusId": "7321161"
                        },
                        {
                            "start": 1302,
                            "end": 1307,
                            "matchedPaperCorpusId": "14586023"
                        },
                        {
                            "start": 1307,
                            "end": 1311,
                            "matchedPaperCorpusId": "5214057"
                        },
                        {
                            "start": 1340,
                            "end": 1345,
                            "matchedPaperCorpusId": "16084542"
                        },
                        {
                            "start": 1345,
                            "end": 1349,
                            "matchedPaperCorpusId": "1112016"
                        },
                        {
                            "start": 1349,
                            "end": 1353,
                            "matchedPaperCorpusId": "14368614"
                        },
                        {
                            "start": 1353,
                            "end": 1357,
                            "matchedPaperCorpusId": "6498451"
                        },
                        {
                            "start": 1357,
                            "end": 1361,
                            "matchedPaperCorpusId": "1612773"
                        },
                        {
                            "start": 1361,
                            "end": 1365,
                            "matchedPaperCorpusId": "6984879"
                        },
                        {
                            "start": 1365,
                            "end": 1369,
                            "matchedPaperCorpusId": "851231"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98828125
                }
            ],
            "relevance_judgement": 0.98828125,
            "relevance_judgment_input_expanded": "# Title: On the Estimation and Use of Statistical Modelling in Information Retrieval\n# Venue: arXiv.org\n# Authors: Casper Petersen\n## Abstract\nSeveral tasks in information retrieval (IR) rely on assumptions regarding the distribution of some property (such as term frequency) in the data being processed. This thesis argues that such distributional assumptions can lead to incorrect conclusions and proposes a statistically principled method for determining the \"true\" distribution. This thesis further applies this method to derive a new family of ranking models that adapt their computations to the statistics of the data being processed. Experimental evaluation shows results on par or better than multiple strong baselines on several TREC collections. Overall, this thesis concludes that distributional assumptions can be replaced with an effective, efficient and principled method for determining the \"true\" distribution and that using the \"true\" distribution can lead to improved retrieval performance.\n## Language Modelling\nA language model (LM) is, in its most basic form, a probability distribution over terms in a language where each term, t is assigned a non-zero probability denoting its probability of occurrence in the \"language\". A \"language\" here is defined as a non-empty finite sequence of symbols or terms. Given a query q and document d \u2208C for some collection C , d 's LM, \u03b8 d , is a probabilistic model that estimates the probability that q was generated by d . In other words, each document is viewed as a sample from the language, and its relevance to q is estimated as the probability that q was generated from this sample.\n\nUsing language models for retrieval in IR was initiated by Ponte and Croft [349] who developed a retrieval model based on the multiple-Bernoulli statistical model. Because the multiple-Bernoulli model is computationally expensive 10 [42] and ignores term-frequencies -a useful feature for retrieval [374] -a number of authors, most notably Hiemstra [203] and Song and Croft [403], introduced the multinomial model as a representation of documents which has since become the de facto choice [132, p. 254]. Liu and Croft [276] and Zhai [457,Chap. 6] provide a comprehensive survey of areas-relevance feedback (e.g. [131,260,348]), distributed IR (e.g. [396,453]), query ambiguity (e.g. [133,134]) and ad hoc retrieval (e.g. [200,204,231,249,269,307,309])-where LMs have been used successfully.\n\nFormally, ranking documents relative to a query q is estimated through Bayes' rule:\n\nwhere Pr(d ) denotes any prior knowledge that d is relevant to any query and Pr(q) is a normalisation constant. The generic LM in Eqn. 2.23 contains two probability distributions: the distribution of Pr(q|d ) and the distribution of Pr(d ). The probability Pr(q|d ) is usually estimated by assuming query terms are i.i.d:\n\ncorresponding to a unigram query-likelihood LM. The term-independence assumption",
            "reference_string": "[90238494 | Petersen | 2019 | Citations: 0]"
        },
        {
            "title": "A Fresh Take on Stale Embeddings: Improving Dense Retriever Training with Corrector Networks",
            "venue": "International Conference on Machine Learning",
            "year": 2024,
            "reference_count": 42,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.01890, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2121348263",
                    "name": "Nicholas Monath"
                },
                {
                    "authorId": "2319130233",
                    "name": "Will Sussman Grathwohl"
                },
                {
                    "authorId": "51020741",
                    "name": "Michael Boratko"
                },
                {
                    "authorId": "2300098510",
                    "name": "Rob Fergus"
                },
                {
                    "authorId": "2286335051",
                    "name": "Andrew McCallum"
                },
                {
                    "authorId": "1771307",
                    "name": "M. Zaheer"
                }
            ],
            "abstract": "In dense retrieval, deep encoders provide embeddings for both inputs and targets, and the softmax function is used to parameterize a distribution over a large number of candidate targets (e.g., textual passages for information retrieval). Significant challenges arise in training such encoders in the increasingly prevalent scenario of (1) a large number of targets, (2) a computationally expensive target encoder model, (3) cached target embeddings that are out-of-date due to ongoing training of target encoder parameters. This paper presents a simple and highly scalable response to these challenges by training a small parametric corrector network that adjusts stale cached target embeddings, enabling an accurate softmax approximation and thereby sampling of up-to-date high scoring\"hard negatives.\"We theoretically investigate the generalization properties of our proposed target corrector, relating the complexity of the network, staleness of cached representations, and the amount of training data. We present experimental results on large benchmark dense retrieval datasets as well as on QA with retrieval augmented language models. Our approach matches state-of-the-art results even when no target embedding updates are made during training beyond an initial cache from the unsupervised pre-trained model, providing a 4-80x reduction in re-embedding computational cost.",
            "corpus_id": 272330251,
            "sentences": [
                {
                    "corpus_id": "272330251",
                    "title": "A Fresh Take on Stale Embeddings: Improving Dense Retriever Training with Corrector Networks",
                    "text": "Retrieval augmented language models (RLMs) typically consist of two major architectural components, a retriever model (e.g., a dual-encoder) and a generative language model or reader model (Guu et al., 2020;Izacard & Grave, 2021;Izacard et al., 2022). The input to a retrieval augmented language model is a natural language text sequence, x. This input text will be encoded using a dual-encoder retrieval model, f (x). Retrieval will be performed over a corpus of targets, Y, returning k targets relevant to x, denoted S x (Y). The reader model takes as input the retrieved targets, S x (Y), and the text x, and generates text. \n\nConcretely, in our experiments, the input text x is a question. The retrieval corpus contains targets y corresponding to passages in Wikipedia. The reader model takes as input the question and retrieved passages and generates a short answer to the question. We present the remainder of the section with this question-answering task in mind. \n\nRLMs can be formalized as latent variable models. The softmax function is used to parameterize the distribution over a discrete latent variable, which corresponds to the retrieved targets. We use a to refer to the generated sequence of text, i.e., the generated answer: \n\nP (a|y, x)P (y|x). \n\n(7) P (a|y, x) is an autoregressive language model. P (y|x) is computed by the softmax with logits from Equation 2 using the encoder models f (x) and g(y). \n\nWhen training RLMs, we receive supervision in the form of question, answer pairs, e.g., x i , a i \u223c X. We do not receive supervision on which targets S x (Y) should be retrieved. We will learn the parameters of both the reader model and retriever model using these supervised question/answer pairs. \n\nTo train the reader and retriever model, we use perplexity distillation (Izacard et al., 2022) for retriever loss and negative log-likelihood for the reader loss.",
                    "score": 0.4310919816026011,
                    "section_title": "Latent Variables in Retrieval Augmented Models",
                    "char_start_offset": 12356,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 251
                        },
                        {
                            "start": 252,
                            "end": 341
                        },
                        {
                            "start": 342,
                            "end": 418
                        },
                        {
                            "start": 419,
                            "end": 527
                        },
                        {
                            "start": 528,
                            "end": 627
                        },
                        {
                            "start": 630,
                            "end": 693
                        },
                        {
                            "start": 694,
                            "end": 773
                        },
                        {
                            "start": 774,
                            "end": 887
                        },
                        {
                            "start": 888,
                            "end": 970
                        },
                        {
                            "start": 973,
                            "end": 1022
                        },
                        {
                            "start": 1023,
                            "end": 1161
                        },
                        {
                            "start": 1162,
                            "end": 1242
                        },
                        {
                            "start": 1245,
                            "end": 1263
                        },
                        {
                            "start": 1266,
                            "end": 1317
                        },
                        {
                            "start": 1318,
                            "end": 1421
                        },
                        {
                            "start": 1424,
                            "end": 1526
                        },
                        {
                            "start": 1527,
                            "end": 1602
                        },
                        {
                            "start": 1603,
                            "end": 1722
                        },
                        {
                            "start": 1725,
                            "end": 1887
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 189,
                            "end": 207,
                            "matchedPaperCorpusId": "211204736"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98388671875
                }
            ],
            "relevance_judgement": 0.98388671875,
            "relevance_judgment_input_expanded": "# Title: A Fresh Take on Stale Embeddings: Improving Dense Retriever Training with Corrector Networks\n# Venue: International Conference on Machine Learning\n# Authors: Nicholas Monath, Will Sussman Grathwohl, Michael Boratko, Rob Fergus, Andrew McCallum, M. Zaheer\n## Abstract\nIn dense retrieval, deep encoders provide embeddings for both inputs and targets, and the softmax function is used to parameterize a distribution over a large number of candidate targets (e.g., textual passages for information retrieval). Significant challenges arise in training such encoders in the increasingly prevalent scenario of (1) a large number of targets, (2) a computationally expensive target encoder model, (3) cached target embeddings that are out-of-date due to ongoing training of target encoder parameters. This paper presents a simple and highly scalable response to these challenges by training a small parametric corrector network that adjusts stale cached target embeddings, enabling an accurate softmax approximation and thereby sampling of up-to-date high scoring\"hard negatives.\"We theoretically investigate the generalization properties of our proposed target corrector, relating the complexity of the network, staleness of cached representations, and the amount of training data. We present experimental results on large benchmark dense retrieval datasets as well as on QA with retrieval augmented language models. Our approach matches state-of-the-art results even when no target embedding updates are made during training beyond an initial cache from the unsupervised pre-trained model, providing a 4-80x reduction in re-embedding computational cost.\n## Latent Variables in Retrieval Augmented Models\nRetrieval augmented language models (RLMs) typically consist of two major architectural components, a retriever model (e.g., a dual-encoder) and a generative language model or reader model (Guu et al., 2020;Izacard & Grave, 2021;Izacard et al., 2022). The input to a retrieval augmented language model is a natural language text sequence, x. This input text will be encoded using a dual-encoder retrieval model, f (x). Retrieval will be performed over a corpus of targets, Y, returning k targets relevant to x, denoted S x (Y). The reader model takes as input the retrieved targets, S x (Y), and the text x, and generates text. \n\nConcretely, in our experiments, the input text x is a question. The retrieval corpus contains targets y corresponding to passages in Wikipedia. The reader model takes as input the question and retrieved passages and generates a short answer to the question. We present the remainder of the section with this question-answering task in mind. \n\nRLMs can be formalized as latent variable models. The softmax function is used to parameterize the distribution over a discrete latent variable, which corresponds to the retrieved targets. We use a to refer to the generated sequence of text, i.e., the generated answer: \n\nP (a|y, x)P (y|x). \n\n(7) P (a|y, x) is an autoregressive language model. P (y|x) is computed by the softmax with logits from Equation 2 using the encoder models f (x) and g(y). \n\nWhen training RLMs, we receive supervision in the form of question, answer pairs, e.g., x i , a i \u223c X. We do not receive supervision on which targets S x (Y) should be retrieved. We will learn the parameters of both the reader model and retriever model using these supervised question/answer pairs. \n\nTo train the reader and retriever model, we use perplexity distillation (Izacard et al., 2022) for retriever loss and negative log-likelihood for the reader loss.",
            "reference_string": "[272330251 | Monath et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Unleashing the Power of LLMs in Dense Retrieval with Query Likelihood Modeling",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 67,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.05216, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2260353683",
                    "name": "Hengran Zhang"
                },
                {
                    "authorId": "2249759496",
                    "name": "Keping Bi"
                },
                {
                    "authorId": "2316783018",
                    "name": "Jiafeng Guo"
                },
                {
                    "authorId": "2269761948",
                    "name": "Xiaojie Sun"
                },
                {
                    "authorId": "2310512864",
                    "name": "Shihao Liu"
                },
                {
                    "authorId": "2104450297",
                    "name": "Daiting Shi"
                },
                {
                    "authorId": "2310342915",
                    "name": "Dawei Yin"
                },
                {
                    "authorId": "2244825947",
                    "name": "Xueqi Cheng"
                }
            ],
            "abstract": "Dense retrieval is a crucial task in Information Retrieval (IR) and is the foundation for downstream tasks such as re-ranking. Recently, large language models (LLMs) have shown compelling semantic understanding capabilities and are appealing to researchers studying dense retrieval. LLMs, as decoder-style generative models, are competent at language generation while falling short on modeling global information due to the lack of attention to tokens afterward. Inspired by the classical word-based language modeling approach for IR, i.e., the query likelihood (QL) model, we seek to sufficiently utilize LLMs' generative ability by QL maximization. However, instead of ranking documents with QL estimation, we introduce an auxiliary task of QL maximization to yield a better backbone for contrastively learning a discriminative retriever. We name our model as LLM-QL. To condense global document semantics to a single vector during QL modeling, LLM-QL has two major components, Attention Stop (AS) and Input Corruption (IC). AS stops the attention of predictive tokens to previous tokens until the ending token of the document. IC masks a portion of tokens in the input documents during prediction. Experiments on MSMARCO show that LLM-QL can achieve significantly better performance than other LLM-based retrievers and using QL estimated by LLM-QL for ranking outperforms word-based QL by a large margin.",
            "corpus_id": 277621440,
            "sentences": [
                {
                    "corpus_id": "277621440",
                    "title": "Unleashing the Power of LLMs in Dense Retrieval with Query Likelihood Modeling",
                    "text": "Information retrieval (IR) contains two primary stages: retrieval and reranking [13,67]. Retrieval serves as the cornerstone of information retrieval and is critically important. Its objective is to retrieve relevant passages from a large-scale corpus in response to a query, thereby providing candidate passages for the subsequent reranking stage. In the early stages of retrieval, the focus was primarily on lexical matching between the terms of the query and the passage, with methods such as BM25 [34] and query likelihood (QL) model [46]. However, with the advent of pre-trained language models (PLMs) such as BERT [11], representing passages or queries as dense vectors has gradually become the mainstream approach. These methods typically employ two separate encoders to represent the query and passage and are referred to as dual-encoder models. \n\nLarge language models (LLMs) are being widely applied across various fields [27,54,61], and garnering increasing attention for their application in retrieval tasks [25,39,51]. Unlike bidirectional attention mechanisms in encoder-style pre-trained language models (PLMs) such as BERT, LLMs are typically decoder-style models that employ unidirectional attention. The task of next token prediction allows LLMs to ingest large amounts of various types of data and thus gain more powerful semantic understanding capability. However, the unidirectional attention during modeling may lead to insufficient representation of global semantics, which is inferior to encoder-style PLMs. Although leveraging the superior semantic understanding ability of LLMs for retrieval looks appealing, it is challenging to do so. Recent studies have attempted to repeat passages as input during encoding [51], to use a bidirectional attention mechanism for encoding during relevance matching fine-tuning [4], or pre-train LLMs to strengthen the global semantic representation capabilities [33]. \n\nSince LLMs are decoder-based language models (LMs), it is natural to think of adapting them to retrieval according to the classical language modeling approach to IR [46], i.e., modeling the query likelihood (QL) given the LM of a document.",
                    "score": 0.38704866882156796,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 88
                        },
                        {
                            "start": 89,
                            "end": 178
                        },
                        {
                            "start": 179,
                            "end": 348
                        },
                        {
                            "start": 349,
                            "end": 543
                        },
                        {
                            "start": 544,
                            "end": 721
                        },
                        {
                            "start": 722,
                            "end": 853
                        },
                        {
                            "start": 856,
                            "end": 1031
                        },
                        {
                            "start": 1032,
                            "end": 1217
                        },
                        {
                            "start": 1218,
                            "end": 1375
                        },
                        {
                            "start": 1376,
                            "end": 1531
                        },
                        {
                            "start": 1532,
                            "end": 1662
                        },
                        {
                            "start": 1663,
                            "end": 1927
                        },
                        {
                            "start": 1930,
                            "end": 2169
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 80,
                            "end": 84,
                            "matchedPaperCorpusId": "259076101"
                        },
                        {
                            "start": 84,
                            "end": 87,
                            "matchedPaperCorpusId": "233289894"
                        },
                        {
                            "start": 501,
                            "end": 505,
                            "matchedPaperCorpusId": "235366815"
                        },
                        {
                            "start": 538,
                            "end": 542,
                            "matchedPaperCorpusId": "2759772"
                        },
                        {
                            "start": 1024,
                            "end": 1027,
                            "matchedPaperCorpusId": "263908865"
                        },
                        {
                            "start": 1922,
                            "end": 1926,
                            "matchedPaperCorpusId": "271885989"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98095703125
                }
            ],
            "relevance_judgement": 0.98095703125,
            "relevance_judgment_input_expanded": "# Title: Unleashing the Power of LLMs in Dense Retrieval with Query Likelihood Modeling\n# Venue: arXiv.org\n# Authors: Hengran Zhang, Keping Bi, Jiafeng Guo, Xiaojie Sun, Shihao Liu, Daiting Shi, Dawei Yin, Xueqi Cheng\n## Abstract\nDense retrieval is a crucial task in Information Retrieval (IR) and is the foundation for downstream tasks such as re-ranking. Recently, large language models (LLMs) have shown compelling semantic understanding capabilities and are appealing to researchers studying dense retrieval. LLMs, as decoder-style generative models, are competent at language generation while falling short on modeling global information due to the lack of attention to tokens afterward. Inspired by the classical word-based language modeling approach for IR, i.e., the query likelihood (QL) model, we seek to sufficiently utilize LLMs' generative ability by QL maximization. However, instead of ranking documents with QL estimation, we introduce an auxiliary task of QL maximization to yield a better backbone for contrastively learning a discriminative retriever. We name our model as LLM-QL. To condense global document semantics to a single vector during QL modeling, LLM-QL has two major components, Attention Stop (AS) and Input Corruption (IC). AS stops the attention of predictive tokens to previous tokens until the ending token of the document. IC masks a portion of tokens in the input documents during prediction. Experiments on MSMARCO show that LLM-QL can achieve significantly better performance than other LLM-based retrievers and using QL estimated by LLM-QL for ranking outperforms word-based QL by a large margin.\n## Introduction\nInformation retrieval (IR) contains two primary stages: retrieval and reranking [13,67]. Retrieval serves as the cornerstone of information retrieval and is critically important. Its objective is to retrieve relevant passages from a large-scale corpus in response to a query, thereby providing candidate passages for the subsequent reranking stage. In the early stages of retrieval, the focus was primarily on lexical matching between the terms of the query and the passage, with methods such as BM25 [34] and query likelihood (QL) model [46]. However, with the advent of pre-trained language models (PLMs) such as BERT [11], representing passages or queries as dense vectors has gradually become the mainstream approach. These methods typically employ two separate encoders to represent the query and passage and are referred to as dual-encoder models. \n\nLarge language models (LLMs) are being widely applied across various fields [27,54,61], and garnering increasing attention for their application in retrieval tasks [25,39,51]. Unlike bidirectional attention mechanisms in encoder-style pre-trained language models (PLMs) such as BERT, LLMs are typically decoder-style models that employ unidirectional attention. The task of next token prediction allows LLMs to ingest large amounts of various types of data and thus gain more powerful semantic understanding capability. However, the unidirectional attention during modeling may lead to insufficient representation of global semantics, which is inferior to encoder-style PLMs. Although leveraging the superior semantic understanding ability of LLMs for retrieval looks appealing, it is challenging to do so. Recent studies have attempted to repeat passages as input during encoding [51], to use a bidirectional attention mechanism for encoding during relevance matching fine-tuning [4], or pre-train LLMs to strengthen the global semantic representation capabilities [33]. \n\nSince LLMs are decoder-based language models (LMs), it is natural to think of adapting them to retrieval according to the classical language modeling approach to IR [46], i.e., modeling the query likelihood (QL) given the LM of a document.",
            "reference_string": "[277621440 | Zhang et al. | 2025 | Citations: 0]"
        },
        {
            "title": "RA-DIT: Retrieval-Augmented Dual Instruction Tuning",
            "venue": "International Conference on Learning Representations",
            "year": 2023,
            "reference_count": 84,
            "citation_count": 153,
            "influential_citation_count": 14,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2310.01352",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.01352, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2255374957",
                    "name": "Xi Victoria Lin"
                },
                {
                    "authorId": "1769736",
                    "name": "Xilun Chen"
                },
                {
                    "authorId": "46221498",
                    "name": "Mingda Chen"
                },
                {
                    "authorId": "2254168373",
                    "name": "Weijia Shi"
                },
                {
                    "authorId": "2253400960",
                    "name": "Maria Lomeli"
                },
                {
                    "authorId": "2191899140",
                    "name": "Rich James"
                },
                {
                    "authorId": "2253404757",
                    "name": "Pedro Rodriguez"
                },
                {
                    "authorId": "2253401183",
                    "name": "Jacob Kahn"
                },
                {
                    "authorId": "2253402270",
                    "name": "Gergely Szilvasy"
                },
                {
                    "authorId": "2253417398",
                    "name": "Mike Lewis"
                },
                {
                    "authorId": "2137813791",
                    "name": "Luke S. Zettlemoyer"
                },
                {
                    "authorId": "2253400757",
                    "name": "Scott Yih"
                }
            ],
            "abstract": "Retrieval-augmented language models (RALMs) improve performance by accessing long-tail and up-to-date knowledge from external data stores, but are challenging to build. Existing approaches require either expensive retrieval-specific modifications to LM pre-training or use post-hoc integration of the data store that leads to suboptimal performance. We introduce Retrieval-Augmented Dual Instruction Tuning (RA-DIT), a lightweight fine-tuning methodology that provides a third option by retrofitting any LLM with retrieval capabilities. Our approach operates in two distinct fine-tuning steps: (1) one updates a pre-trained LM to better use retrieved information, while (2) the other updates the retriever to return more relevant results, as preferred by the LM. By fine-tuning over tasks that require both knowledge utilization and contextual awareness, we demonstrate that each stage yields significant performance improvements, and using both leads to additional gains. Our best model, RA-DIT 65B, achieves state-of-the-art performance across a range of knowledge-intensive zero- and few-shot learning benchmarks, significantly outperforming existing in-context RALM approaches by up to +8.9% in 0-shot setting and +1.4% in 5-shot setting on average.",
            "corpus_id": 263605962,
            "sentences": [
                {
                    "corpus_id": "263605962",
                    "title": "RA-DIT: Retrieval-Augmented Dual Instruction Tuning",
                    "text": "Retrieval-Augmented Language Models RALMs fuse language models (LMs) with a retrieval module that explicitly augments the LM with information retrieved from external knowledge stores (Guu et al., 2020;Lewis et al., 2020). One mainstream type of RALM follows the \"retrieveand-read\" paradigm, where the retrieval module supplies external knowledge as additional context which the LM (reader) leverages to produce the final output (Izacard et al., 2022b;Borgeaud et al., 2022;Shi et al., 2023b;Ram et al., 2023). Some existing work focuses on pre-training the LM to better utilize retrieved knowledge. For example, REALM (Guu et al., 2020) and RETRO (Borgeaud et al., 2022) incorporate retrieval from the beginning and conduct end-to-end retrieval-augmented pre-training, whereas ATLAS (Izacard et al., 2022b) continuously pre-trains a T5 LM (Raffel et al., 2020) jointly with a retriever. Others assume black-box access to an LM and combine it with either off-the-shelf or fine-tuned retrievers (Shi et al., 2023b;Ram et al., 2023). Our approach adopts lightweight fine-tuning to effectively retrofit any pre-trained LLM with retrieval capacity. This approach offers efficiency compared to methods involving extensive pre-training and demonstrates superior effectiveness compared to the off-the-shelf fusion approach. \n\nIndependent to our work, Luo et al. (2023) proposes SAIL, an approach that fine-tunes the LM with instructions augmented with retrieved content, and examines it on public instruction following datasets (Taori et al., 2023;Chiang et al., 2023) using a moderately sized model (7B parameters). \n\nIn comparison, RA-DIT conducts parallel retrieval-augmentation by generating distinct prompts for each retrieved passage and subsequently aggregating the outcomes; SAIL, on the other hand, concatenates the top retrieved passages in the augmentation.",
                    "score": 0.40346213869427583,
                    "section_title": "RELATED WORK",
                    "char_start_offset": 22646,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 221
                        },
                        {
                            "start": 222,
                            "end": 509
                        },
                        {
                            "start": 510,
                            "end": 598
                        },
                        {
                            "start": 599,
                            "end": 886
                        },
                        {
                            "start": 887,
                            "end": 1030
                        },
                        {
                            "start": 1031,
                            "end": 1143
                        },
                        {
                            "start": 1144,
                            "end": 1315
                        },
                        {
                            "start": 1318,
                            "end": 1608
                        },
                        {
                            "start": 1611,
                            "end": 1860
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 183,
                            "end": 201,
                            "matchedPaperCorpusId": "211204736"
                        },
                        {
                            "start": 451,
                            "end": 473,
                            "matchedPaperCorpusId": "244954723"
                        },
                        {
                            "start": 618,
                            "end": 636,
                            "matchedPaperCorpusId": "211204736"
                        },
                        {
                            "start": 647,
                            "end": 670,
                            "matchedPaperCorpusId": "244954723"
                        },
                        {
                            "start": 839,
                            "end": 860,
                            "matchedPaperCorpusId": "204838007"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97998046875
                }
            ],
            "relevance_judgement": 0.97998046875,
            "relevance_judgment_input_expanded": "# Title: RA-DIT: Retrieval-Augmented Dual Instruction Tuning\n# Venue: International Conference on Learning Representations\n# Authors: Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Rich James, Pedro Rodriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, Luke S. Zettlemoyer, Scott Yih\n## Abstract\nRetrieval-augmented language models (RALMs) improve performance by accessing long-tail and up-to-date knowledge from external data stores, but are challenging to build. Existing approaches require either expensive retrieval-specific modifications to LM pre-training or use post-hoc integration of the data store that leads to suboptimal performance. We introduce Retrieval-Augmented Dual Instruction Tuning (RA-DIT), a lightweight fine-tuning methodology that provides a third option by retrofitting any LLM with retrieval capabilities. Our approach operates in two distinct fine-tuning steps: (1) one updates a pre-trained LM to better use retrieved information, while (2) the other updates the retriever to return more relevant results, as preferred by the LM. By fine-tuning over tasks that require both knowledge utilization and contextual awareness, we demonstrate that each stage yields significant performance improvements, and using both leads to additional gains. Our best model, RA-DIT 65B, achieves state-of-the-art performance across a range of knowledge-intensive zero- and few-shot learning benchmarks, significantly outperforming existing in-context RALM approaches by up to +8.9% in 0-shot setting and +1.4% in 5-shot setting on average.\n## RELATED WORK\nRetrieval-Augmented Language Models RALMs fuse language models (LMs) with a retrieval module that explicitly augments the LM with information retrieved from external knowledge stores (Guu et al., 2020;Lewis et al., 2020). One mainstream type of RALM follows the \"retrieveand-read\" paradigm, where the retrieval module supplies external knowledge as additional context which the LM (reader) leverages to produce the final output (Izacard et al., 2022b;Borgeaud et al., 2022;Shi et al., 2023b;Ram et al., 2023). Some existing work focuses on pre-training the LM to better utilize retrieved knowledge. For example, REALM (Guu et al., 2020) and RETRO (Borgeaud et al., 2022) incorporate retrieval from the beginning and conduct end-to-end retrieval-augmented pre-training, whereas ATLAS (Izacard et al., 2022b) continuously pre-trains a T5 LM (Raffel et al., 2020) jointly with a retriever. Others assume black-box access to an LM and combine it with either off-the-shelf or fine-tuned retrievers (Shi et al., 2023b;Ram et al., 2023). Our approach adopts lightweight fine-tuning to effectively retrofit any pre-trained LLM with retrieval capacity. This approach offers efficiency compared to methods involving extensive pre-training and demonstrates superior effectiveness compared to the off-the-shelf fusion approach. \n\nIndependent to our work, Luo et al. (2023) proposes SAIL, an approach that fine-tunes the LM with instructions augmented with retrieved content, and examines it on public instruction following datasets (Taori et al., 2023;Chiang et al., 2023) using a moderately sized model (7B parameters). \n\nIn comparison, RA-DIT conducts parallel retrieval-augmentation by generating distinct prompts for each retrieved passage and subsequently aggregating the outcomes; SAIL, on the other hand, concatenates the top retrieved passages in the augmentation.",
            "reference_string": "[263605962 | Lin et al. | 2023 | Citations: 153]"
        },
        {
            "title": "An Analysis on Matching Mechanisms and Token Pruning for Late-interaction Models",
            "venue": "ACM Trans. Inf. Syst.",
            "year": 2024,
            "reference_count": 52,
            "citation_count": 7,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3639818",
                "status": "BRONZE",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.13291, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2260822142",
                    "name": "Qi Liu"
                },
                {
                    "authorId": "2163425408",
                    "name": "Gang Guo"
                },
                {
                    "authorId": "2265811336",
                    "name": "Jiaxin Mao"
                },
                {
                    "authorId": "2273086037",
                    "name": "Zhicheng Dou"
                },
                {
                    "authorId": "2260701602",
                    "name": "Ji-Rong Wen"
                },
                {
                    "authorId": "2279223883",
                    "name": "Hao Jiang"
                },
                {
                    "authorId": "2282242196",
                    "name": "Xinyu Zhang"
                },
                {
                    "authorId": "2282520819",
                    "name": "Zhao Cao"
                }
            ],
            "abstract": "With the development of pre-trained language models, the dense retrieval models have become promising alternatives to the traditional retrieval models that rely on exact match and sparse bag-of-words representations. Different from most dense retrieval models using a bi-encoder to encode each query or document into a dense vector, the recently proposed late-interaction multi-vector models (i.e., ColBERT and COIL) achieve state-of-the-art retrieval effectiveness by using all token embeddings to represent documents and queries and modeling their relevance with a sum-of-max operation. However, these fine-grained representations may cause unacceptable storage overhead for practical search systems. In this study, we systematically analyze the matching mechanism of these late-interaction models and show that the sum-of-max operation heavily relies on the co-occurrence signals and some important words in the document. Based on these findings, we then propose several simple document pruning methods to reduce the storage overhead and compare the effectiveness of different pruning methods on different late-interaction models. We also leverage query pruning methods to further reduce the retrieval latency. We conduct extensive experiments on both in-domain and out-domain datasets and show that some of the used pruning methods can significantly improve the efficiency of these late-interaction models without substantially hurting their retrieval effectiveness.",
            "corpus_id": 267377589,
            "sentences": [
                {
                    "corpus_id": "267377589",
                    "title": "An Analysis on Matching Mechanisms and Token Pruning for Late-interaction Models",
                    "text": "Recently, neural network models have attracted much attention in both academia and industry and have made impressive improvements in many fields.The information retrieval field benefits from pre-trained language models such as BERT [12] and RoBERTa [29].These neural retrieval models can be divided into two groups as below:\n\nNeural Sparse Retrieval Models improve retrieval performance by changing the term importance in different ways.Two common methods are: a) leveraging neural language generation models (e.g., T5 [39]) to expand original documents with semantic-related words, or b) using pre-trained language models to re-compute the weights.Nogueira et al. [33,34] modified the term frequency by using generative models to generate pseudo-queries of original documents and concatenate these pseudo-queries and original documents to form the new documents.Dai and Callan [10,11] used the pre-trained language model to predict contextualized term weight through a regression task.Mallia et al. [31] first used a generator to expand new semantic-related words of the original text and then used a contextualized language model encoder to re-compute term weights.On the other hand, Bai et al. [4] proposed using an important predictor to re-predict term weights and a gating controller to filter or expand terms at the same time.Formal et al. [13,14] further proposed a more simple but effective method, based on explicit sparsity regularization and a log-saturation effect on term weights.All of these works can significantly outperform BM25.\n\nNeural Dense Retrieval Models use dense vectors to represent queries and documents.Two widely adopted architectures of dense retrieval models are cross-encoders and bi-encoders.Crossencoders [32] concatenate the query and the document as the input of pre-trained language models, then use a function to map the output vector to a scalar score.Cross-encoders achieve superior retrieval effectiveness through the full interaction between all query tokens and all document tokens but the requirement of concatenating the query and each candidate document at the inference time makes cross-encoders impractical in the retrieval stage.",
                    "score": 0.4175804736869253,
                    "section_title": "Neural Retrieval Models",
                    "char_start_offset": 8893,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 145
                        },
                        {
                            "start": 145,
                            "end": 254
                        },
                        {
                            "start": 254,
                            "end": 324
                        },
                        {
                            "start": 326,
                            "end": 437
                        },
                        {
                            "start": 437,
                            "end": 649
                        },
                        {
                            "start": 649,
                            "end": 863
                        },
                        {
                            "start": 863,
                            "end": 986
                        },
                        {
                            "start": 986,
                            "end": 1167
                        },
                        {
                            "start": 1167,
                            "end": 1333
                        },
                        {
                            "start": 1333,
                            "end": 1494
                        },
                        {
                            "start": 1494,
                            "end": 1547
                        },
                        {
                            "start": 1549,
                            "end": 1632
                        },
                        {
                            "start": 1632,
                            "end": 1726
                        },
                        {
                            "start": 1726,
                            "end": 1892
                        },
                        {
                            "start": 1892,
                            "end": 2179
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 882,
                            "end": 885,
                            "matchedPaperCorpusId": "218521094"
                        },
                        {
                            "start": 1000,
                            "end": 1004,
                            "matchedPaperCorpusId": "233394068"
                        },
                        {
                            "start": 1351,
                            "end": 1354,
                            "matchedPaperCorpusId": "235792467"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97900390625
                }
            ],
            "relevance_judgement": 0.97900390625,
            "relevance_judgment_input_expanded": "# Title: An Analysis on Matching Mechanisms and Token Pruning for Late-interaction Models\n# Venue: ACM Trans. Inf. Syst.\n# Authors: Qi Liu, Gang Guo, Jiaxin Mao, Zhicheng Dou, Ji-Rong Wen, Hao Jiang, Xinyu Zhang, Zhao Cao\n## Abstract\nWith the development of pre-trained language models, the dense retrieval models have become promising alternatives to the traditional retrieval models that rely on exact match and sparse bag-of-words representations. Different from most dense retrieval models using a bi-encoder to encode each query or document into a dense vector, the recently proposed late-interaction multi-vector models (i.e., ColBERT and COIL) achieve state-of-the-art retrieval effectiveness by using all token embeddings to represent documents and queries and modeling their relevance with a sum-of-max operation. However, these fine-grained representations may cause unacceptable storage overhead for practical search systems. In this study, we systematically analyze the matching mechanism of these late-interaction models and show that the sum-of-max operation heavily relies on the co-occurrence signals and some important words in the document. Based on these findings, we then propose several simple document pruning methods to reduce the storage overhead and compare the effectiveness of different pruning methods on different late-interaction models. We also leverage query pruning methods to further reduce the retrieval latency. We conduct extensive experiments on both in-domain and out-domain datasets and show that some of the used pruning methods can significantly improve the efficiency of these late-interaction models without substantially hurting their retrieval effectiveness.\n## Neural Retrieval Models\nRecently, neural network models have attracted much attention in both academia and industry and have made impressive improvements in many fields.The information retrieval field benefits from pre-trained language models such as BERT [12] and RoBERTa [29].These neural retrieval models can be divided into two groups as below:\n\nNeural Sparse Retrieval Models improve retrieval performance by changing the term importance in different ways.Two common methods are: a) leveraging neural language generation models (e.g., T5 [39]) to expand original documents with semantic-related words, or b) using pre-trained language models to re-compute the weights.Nogueira et al. [33,34] modified the term frequency by using generative models to generate pseudo-queries of original documents and concatenate these pseudo-queries and original documents to form the new documents.Dai and Callan [10,11] used the pre-trained language model to predict contextualized term weight through a regression task.Mallia et al. [31] first used a generator to expand new semantic-related words of the original text and then used a contextualized language model encoder to re-compute term weights.On the other hand, Bai et al. [4] proposed using an important predictor to re-predict term weights and a gating controller to filter or expand terms at the same time.Formal et al. [13,14] further proposed a more simple but effective method, based on explicit sparsity regularization and a log-saturation effect on term weights.All of these works can significantly outperform BM25.\n\nNeural Dense Retrieval Models use dense vectors to represent queries and documents.Two widely adopted architectures of dense retrieval models are cross-encoders and bi-encoders.Crossencoders [32] concatenate the query and the document as the input of pre-trained language models, then use a function to map the output vector to a scalar score.Cross-encoders achieve superior retrieval effectiveness through the full interaction between all query tokens and all document tokens but the requirement of concatenating the query and each candidate document at the inference time makes cross-encoders impractical in the retrieval stage.",
            "reference_string": "[267377589 | Liu et al. | 2024 | Citations: 7]"
        },
        {
            "title": "Formal Language Knowledge Corpus for Retrieval Augmented Generation",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 0,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.16689, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2336913948",
                    "name": "Majd Zayyad"
                },
                {
                    "authorId": "2727584",
                    "name": "Yossi Adi"
                }
            ],
            "abstract": "The integration of retrieval-augmented techniques with LLMs has shown promise in improving performance across various domains. However, their utility in tasks requiring advanced reasoning, such as generating and evaluating mathematical statements and proofs, remains underexplored. This study explores the use of Lean, a programming language for writing mathematical proofs, to populate the knowledge corpus used by RAG systems. We hope for this to lay the foundation to exploring different methods of using RAGs to improve the performance of LLMs in advanced logical reasoning tasks.",
            "corpus_id": 274982275,
            "sentences": [
                {
                    "corpus_id": "274982275",
                    "title": "Formal Language Knowledge Corpus for Retrieval Augmented Generation",
                    "text": "RAG is a sophisticated framework designed to enhance language models by coupling them with external retrieval systems, addressing limitations inherent in static, solely parameter-based language models. RAG integrates a dual-component architecture where a retriever dynamically searches a structured external corpus for relevant information based on the input query, and a generator LLM uses the retrieved content as context to generate accurate and contextually enriched responses [Gao et al., 2023, Mialon et al., 2023]. This setup mitigates common issues such as hallucinations and factual inaccuracies in language models by grounding generated text in real-world, verified information. In practice, RAG systems employ dense vector embeddings to ensure retrieval relevance, capturing semantic relationships within documents beyond mere keyword matching. The retrieved information is subsequently fed into the generator, allowing it to synthesize data with pre-existing knowledge for enhanced coherence and contextual accuracy. \n\nRAGs can employ two main types of retrieval mechanisms: dense and sparse [Mialon et al., 2023]. Sparse retrievers rely on bag-of-words representations, excelling at finding documents with high term overlap to the query, while dense retrievers utilize neural network embeddings to capture semantic similarities, enhancing the model's comprehension of related concepts. By appending retrieved documents directly to the model's context, these retrievers allow the language model to ground its responses in a broader context, thereby increasing accuracy and factual consistency across complex tasks. \n\nThe success of retrieval-augmented models in various domains has catalyzed interest in their application to more demanding reasoning tasks. A great representative of such tasks is the construction and verification of mathematical proofs, which requires solving problems step-by-step, and generating precise mathematical statements. Recent approaches, such as chain-of-thought (CoT) prompting [Lewkowycz et al., 2022] combined with retrieval, highlight the potential for retrieval-augmented models to provide sequential reasoning support. These models can generate reasoning paths interspersed with retrieval steps to guide complex problem-solving processes, such as multi-step question answering, enabling models to leverage external information dynamically at each reasoning stage.",
                    "score": 0.40813157619824786,
                    "section_title": "A. Retrieval-Augmented Language Models",
                    "char_start_offset": 3315,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 201
                        },
                        {
                            "start": 202,
                            "end": 521
                        },
                        {
                            "start": 522,
                            "end": 688
                        },
                        {
                            "start": 689,
                            "end": 855
                        },
                        {
                            "start": 856,
                            "end": 1028
                        },
                        {
                            "start": 1031,
                            "end": 1126
                        },
                        {
                            "start": 1127,
                            "end": 1398
                        },
                        {
                            "start": 1399,
                            "end": 1626
                        },
                        {
                            "start": 1629,
                            "end": 1768
                        },
                        {
                            "start": 1769,
                            "end": 1960
                        },
                        {
                            "start": 1961,
                            "end": 2166
                        },
                        {
                            "start": 2167,
                            "end": 2411
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9755859375
                }
            ],
            "relevance_judgement": 0.9755859375,
            "relevance_judgment_input_expanded": "# Title: Formal Language Knowledge Corpus for Retrieval Augmented Generation\n# Venue: arXiv.org\n# Authors: Majd Zayyad, Yossi Adi\n## Abstract\nThe integration of retrieval-augmented techniques with LLMs has shown promise in improving performance across various domains. However, their utility in tasks requiring advanced reasoning, such as generating and evaluating mathematical statements and proofs, remains underexplored. This study explores the use of Lean, a programming language for writing mathematical proofs, to populate the knowledge corpus used by RAG systems. We hope for this to lay the foundation to exploring different methods of using RAGs to improve the performance of LLMs in advanced logical reasoning tasks.\n## A. Retrieval-Augmented Language Models\nRAG is a sophisticated framework designed to enhance language models by coupling them with external retrieval systems, addressing limitations inherent in static, solely parameter-based language models. RAG integrates a dual-component architecture where a retriever dynamically searches a structured external corpus for relevant information based on the input query, and a generator LLM uses the retrieved content as context to generate accurate and contextually enriched responses [Gao et al., 2023, Mialon et al., 2023]. This setup mitigates common issues such as hallucinations and factual inaccuracies in language models by grounding generated text in real-world, verified information. In practice, RAG systems employ dense vector embeddings to ensure retrieval relevance, capturing semantic relationships within documents beyond mere keyword matching. The retrieved information is subsequently fed into the generator, allowing it to synthesize data with pre-existing knowledge for enhanced coherence and contextual accuracy. \n\nRAGs can employ two main types of retrieval mechanisms: dense and sparse [Mialon et al., 2023]. Sparse retrievers rely on bag-of-words representations, excelling at finding documents with high term overlap to the query, while dense retrievers utilize neural network embeddings to capture semantic similarities, enhancing the model's comprehension of related concepts. By appending retrieved documents directly to the model's context, these retrievers allow the language model to ground its responses in a broader context, thereby increasing accuracy and factual consistency across complex tasks. \n\nThe success of retrieval-augmented models in various domains has catalyzed interest in their application to more demanding reasoning tasks. A great representative of such tasks is the construction and verification of mathematical proofs, which requires solving problems step-by-step, and generating precise mathematical statements. Recent approaches, such as chain-of-thought (CoT) prompting [Lewkowycz et al., 2022] combined with retrieval, highlight the potential for retrieval-augmented models to provide sequential reasoning support. These models can generate reasoning paths interspersed with retrieval steps to guide complex problem-solving processes, such as multi-step question answering, enabling models to leverage external information dynamically at each reasoning stage.",
            "reference_string": "[274982275 | Zayyad et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Modelling Word Burstiness in Natural Language: A Generalised Polya Process for Document Language Models in Information Retrieval",
            "venue": "arXiv.org",
            "year": 2017,
            "reference_count": 12,
            "citation_count": 3,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1708.06011, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "3161572",
                    "name": "Ronan Cummins"
                }
            ],
            "abstract": "We introduce a generalised multivariate Polya process for document language modelling. The framework outlined here generalises a number of statistical language models used in information retrieval for modelling document generation. In particular, we show that the choice of replacement matrix M ultimately defines the type of random process and therefore defines a particular type of document language model. We show that a particular variant of the general model is useful for modelling term-specific burstiness. Furthermore, via experimentation we show that this variant significantly improves retrieval effectiveness over a strong baseline on a number of small test collections.",
            "corpus_id": 21012910,
            "sentences": [
                {
                    "corpus_id": "21012910",
                    "title": "Modelling Word Burstiness in Natural Language: A Generalised Polya Process for Document Language Models in Information Retrieval",
                    "text": "The general model outlined here (Eq. 1) is an intuitive statistical generative model of documents. The vector u i can be seen as storing the state of the model at a particular time i. Both the multinomial and multivariate P\u00f3lya urn (SPUD [5]) language model are specific instances of this model and are instantiated by different settings of M. Given that the SPUD language model significantly improves upon the multinomial model in information retrieval, the further extensions hold the promise of improved performance and of greater theoretical understanding. Furthermore, it is worth noting that the dependencies that the models3 capture, span a greater distance than n-gram models (i.e. a word occurring at the start of a document affects the choice of word at the end of a document). \n\nThe main challenges to implementing the remaining model variants are in estimating M and u 0 from a large background model (document collection D) and subsequently in inferring the initial state of each document model. For large scale collections this a computationally expensive inverse problem. However, the upcoming section will outline some promising initial experiments with regard to the third variant of the general model (i.e. modelling term-specific burstiness for retrieval). The fourth variant is left for future work.",
                    "score": 0.39857759446045876,
                    "section_title": "Discussion",
                    "char_start_offset": 7319,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 98
                        },
                        {
                            "start": 99,
                            "end": 183
                        },
                        {
                            "start": 184,
                            "end": 560
                        },
                        {
                            "start": 561,
                            "end": 787
                        },
                        {
                            "start": 790,
                            "end": 1008
                        },
                        {
                            "start": 1009,
                            "end": 1086
                        },
                        {
                            "start": 1087,
                            "end": 1224
                        },
                        {
                            "start": 1225,
                            "end": 1275
                        },
                        {
                            "start": 1276,
                            "end": 1319
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 238,
                            "end": 241,
                            "matchedPaperCorpusId": "9519540"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9755859375
                },
                {
                    "corpus_id": "21012910",
                    "title": "Modelling Word Burstiness in Natural Language: A Generalised Polya Process for Document Language Models in Information Retrieval",
                    "text": "This paper has introduced a family of statistical language models inspired by a classic urn model. We have shown that it is the replacement matrix M that defines the dynamics of the model. We have implemented a variant of the model which models burstiness in a term-specific manner. We have shown that the parameters of the model can be estimated from data using sampling techniques. Furthermore, we have incorporated the new language model into a retrieval framework and shown that retrieval effectiveness improves significantly over a highly competitive baseline language model. Although our experiments are conducted on small test collections (because parameter estimation is computationally expensive), the results are promising. We believe that this is first paper that deals with term-specific burstiness in such a principled probabilistic manner. \n\non Research and Development in Information Retrieval, SIGIR '96, pages 187-195, New York, NY, USA, 1996. ACM. \n\n[10] Jay M. Ponte and W. Bruce Croft. A language modeling approach to information retrieval.",
                    "score": 0.3908534054751986,
                    "section_title": "Summary",
                    "char_start_offset": 19164,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 98
                        },
                        {
                            "start": 99,
                            "end": 188
                        },
                        {
                            "start": 189,
                            "end": 282
                        },
                        {
                            "start": 283,
                            "end": 383
                        },
                        {
                            "start": 384,
                            "end": 580
                        },
                        {
                            "start": 581,
                            "end": 733
                        },
                        {
                            "start": 734,
                            "end": 853
                        },
                        {
                            "start": 856,
                            "end": 960
                        },
                        {
                            "start": 961,
                            "end": 965
                        },
                        {
                            "start": 968,
                            "end": 1005
                        },
                        {
                            "start": 1006,
                            "end": 1060
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.931640625
                }
            ],
            "relevance_judgement": 0.9755859375,
            "relevance_judgment_input_expanded": "# Title: Modelling Word Burstiness in Natural Language: A Generalised Polya Process for Document Language Models in Information Retrieval\n# Venue: arXiv.org\n# Authors: Ronan Cummins\n## Abstract\nWe introduce a generalised multivariate Polya process for document language modelling. The framework outlined here generalises a number of statistical language models used in information retrieval for modelling document generation. In particular, we show that the choice of replacement matrix M ultimately defines the type of random process and therefore defines a particular type of document language model. We show that a particular variant of the general model is useful for modelling term-specific burstiness. Furthermore, via experimentation we show that this variant significantly improves retrieval effectiveness over a strong baseline on a number of small test collections.\n## Discussion\nThe general model outlined here (Eq. 1) is an intuitive statistical generative model of documents. The vector u i can be seen as storing the state of the model at a particular time i. Both the multinomial and multivariate P\u00f3lya urn (SPUD [5]) language model are specific instances of this model and are instantiated by different settings of M. Given that the SPUD language model significantly improves upon the multinomial model in information retrieval, the further extensions hold the promise of improved performance and of greater theoretical understanding. Furthermore, it is worth noting that the dependencies that the models3 capture, span a greater distance than n-gram models (i.e. a word occurring at the start of a document affects the choice of word at the end of a document). \n\nThe main challenges to implementing the remaining model variants are in estimating M and u 0 from a large background model (document collection D) and subsequently in inferring the initial state of each document model. For large scale collections this a computationally expensive inverse problem. However, the upcoming section will outline some promising initial experiments with regard to the third variant of the general model (i.e. modelling term-specific burstiness for retrieval). The fourth variant is left for future work.\n\n## Summary\nThis paper has introduced a family of statistical language models inspired by a classic urn model. We have shown that it is the replacement matrix M that defines the dynamics of the model. We have implemented a variant of the model which models burstiness in a term-specific manner. We have shown that the parameters of the model can be estimated from data using sampling techniques. Furthermore, we have incorporated the new language model into a retrieval framework and shown that retrieval effectiveness improves significantly over a highly competitive baseline language model. Although our experiments are conducted on small test collections (because parameter estimation is computationally expensive), the results are promising. We believe that this is first paper that deals with term-specific burstiness in such a principled probabilistic manner. \n\non Research and Development in Information Retrieval, SIGIR '96, pages 187-195, New York, NY, USA, 1996. ACM. \n\n[10] Jay M. Ponte and W. Bruce Croft. A language modeling approach to information retrieval.",
            "reference_string": "[21012910 | Cummins | 2017 | Citations: 3]"
        },
        {
            "title": "Interactive Information Need Prediction with Intent and Context",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 40,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.02635, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "79769007",
                    "name": "Kevin Ros"
                },
                {
                    "authorId": "2338832917",
                    "name": "Dhyey Pandya"
                },
                {
                    "authorId": "2253607011",
                    "name": "ChengXiang Zhai"
                }
            ],
            "abstract": "The ability to predict a user's information need would have wide-ranging implications, from saving time and effort to mitigating vocabulary gaps. We study how to interactively predict a user's information need by letting them select a pre-search context (e.g., a paragraph, sentence, or singe word) and specify an optional partial search intent (e.g.,\"how\",\"why\",\"applications\", etc.). We examine how various generative language models can explicitly make this prediction by generating a question as well as how retrieval models can implicitly make this prediction by retrieving an answer. We find that this prediction process is possible in many cases and that user-provided partial search intent can help mitigate large pre-search contexts. We conclude that this framework is promising and suitable for real-world applications.",
            "corpus_id": 275337144,
            "sentences": [
                {
                    "corpus_id": "275337144",
                    "title": "Interactive Information Need Prediction with Intent and Context",
                    "text": "To implicitly predict the information need by retrieving an answering passage, we fine-tuned both bi-encoder and cross-encoder language models. We choose these methods because they are commonly used in the retrieval literature, with bi-encoders often used in the initial retrieval stage [25], and cross-encoders often used in the reranking stage [35]. We based both approaches on BERT [11], an encoder-only pre-trained transformer model. We chose this model due to its generality in various applications. To assess performance, we measured the recall at 10 (R@10) and the mean reciprocal rank (MRR) performance on the validation queries. We selected these two metrics due to their common use in information retrieval and their interpretability for use in web retrieval systems (e.g., such as search engines) [3]. We used the MRR at 10 for testing statistical significance. The validation target paragraphs and the training target paragraphs were used as candidates for retrieval. This amounted to 956 validation samples over 8,873 candidate target paragraphs. \n\nFor the bi-encoder model, we used bert-base-uncased, which is publicly available from Hugging Face [13]. We trained the model to maximize the cosine similarity between positive training samples and minimize the cosine similarity between negative samples via the cosine embedding loss [30]. In all of our experiments, we set the margin to 0.5 and set  1 and  2 to be the CLS tokens from the source and target of each sample, respectively. The label indicates if the sample was positive or negative. Moreover, we used a batch size of 16, 50 warm up steps, and stopped training when the validation loss increased by more than 5% of the best-performing model epoch. We fixed the number of negative samples to 10, varied the number of positive samples (when we had a source paragraph) over {1,2}, and varied the learning rate over {5e-5, 1e-5, 5e-6}. Each training run took approximately four hours. \n\nFor the cross-encoder model, we again used Hugging Face's bertbase-uncased but with a classification head.",
                    "score": 0.393541376840174,
                    "section_title": "Retrieval",
                    "char_start_offset": 20476,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 143
                        },
                        {
                            "start": 144,
                            "end": 351
                        },
                        {
                            "start": 352,
                            "end": 437
                        },
                        {
                            "start": 438,
                            "end": 504
                        },
                        {
                            "start": 505,
                            "end": 637
                        },
                        {
                            "start": 638,
                            "end": 812
                        },
                        {
                            "start": 813,
                            "end": 872
                        },
                        {
                            "start": 873,
                            "end": 979
                        },
                        {
                            "start": 980,
                            "end": 1059
                        },
                        {
                            "start": 1062,
                            "end": 1166
                        },
                        {
                            "start": 1167,
                            "end": 1351
                        },
                        {
                            "start": 1352,
                            "end": 1499
                        },
                        {
                            "start": 1500,
                            "end": 1559
                        },
                        {
                            "start": 1560,
                            "end": 1723
                        },
                        {
                            "start": 1724,
                            "end": 1907
                        },
                        {
                            "start": 1908,
                            "end": 1956
                        },
                        {
                            "start": 1959,
                            "end": 2065
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 346,
                            "end": 350,
                            "matchedPaperCorpusId": "201646309"
                        },
                        {
                            "start": 808,
                            "end": 811,
                            "matchedPaperCorpusId": "16849904"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97509765625
                }
            ],
            "relevance_judgement": 0.97509765625,
            "relevance_judgment_input_expanded": "# Title: Interactive Information Need Prediction with Intent and Context\n# Venue: arXiv.org\n# Authors: Kevin Ros, Dhyey Pandya, ChengXiang Zhai\n## Abstract\nThe ability to predict a user's information need would have wide-ranging implications, from saving time and effort to mitigating vocabulary gaps. We study how to interactively predict a user's information need by letting them select a pre-search context (e.g., a paragraph, sentence, or singe word) and specify an optional partial search intent (e.g.,\"how\",\"why\",\"applications\", etc.). We examine how various generative language models can explicitly make this prediction by generating a question as well as how retrieval models can implicitly make this prediction by retrieving an answer. We find that this prediction process is possible in many cases and that user-provided partial search intent can help mitigate large pre-search contexts. We conclude that this framework is promising and suitable for real-world applications.\n## Retrieval\nTo implicitly predict the information need by retrieving an answering passage, we fine-tuned both bi-encoder and cross-encoder language models. We choose these methods because they are commonly used in the retrieval literature, with bi-encoders often used in the initial retrieval stage [25], and cross-encoders often used in the reranking stage [35]. We based both approaches on BERT [11], an encoder-only pre-trained transformer model. We chose this model due to its generality in various applications. To assess performance, we measured the recall at 10 (R@10) and the mean reciprocal rank (MRR) performance on the validation queries. We selected these two metrics due to their common use in information retrieval and their interpretability for use in web retrieval systems (e.g., such as search engines) [3]. We used the MRR at 10 for testing statistical significance. The validation target paragraphs and the training target paragraphs were used as candidates for retrieval. This amounted to 956 validation samples over 8,873 candidate target paragraphs. \n\nFor the bi-encoder model, we used bert-base-uncased, which is publicly available from Hugging Face [13]. We trained the model to maximize the cosine similarity between positive training samples and minimize the cosine similarity between negative samples via the cosine embedding loss [30]. In all of our experiments, we set the margin to 0.5 and set  1 and  2 to be the CLS tokens from the source and target of each sample, respectively. The label indicates if the sample was positive or negative. Moreover, we used a batch size of 16, 50 warm up steps, and stopped training when the validation loss increased by more than 5% of the best-performing model epoch. We fixed the number of negative samples to 10, varied the number of positive samples (when we had a source paragraph) over {1,2}, and varied the learning rate over {5e-5, 1e-5, 5e-6}. Each training run took approximately four hours. \n\nFor the cross-encoder model, we again used Hugging Face's bertbase-uncased but with a classification head.",
            "reference_string": "[275337144 | Ros et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Neuro-Symbolic Language Modeling with Automaton-augmented Retrieval",
            "venue": "International Conference on Machine Learning",
            "year": 2022,
            "reference_count": 47,
            "citation_count": 63,
            "influential_citation_count": 5,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2201.12431, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "47051926",
                    "name": "Uri Alon"
                },
                {
                    "authorId": "40027632",
                    "name": "Frank F. Xu"
                },
                {
                    "authorId": "6215698",
                    "name": "Junxian He"
                },
                {
                    "authorId": "2072419570",
                    "name": "Sudipta Sengupta"
                },
                {
                    "authorId": "144590225",
                    "name": "D. Roth"
                },
                {
                    "authorId": "1700325",
                    "name": "Graham Neubig"
                }
            ],
            "abstract": "Retrieval-based language models (R-LM) model the probability of natural language text by combining a standard language model (LM) with examples retrieved from an external datastore at test time. While effective, a major bottleneck of using these models in practice is the computationally costly datastore search, which can be performed as frequently as every time step. In this paper, we present RetoMaton - retrieval automaton - which approximates the datastore search, based on (1) saving pointers between consecutive datastore entries, and (2) clustering of entries into\"states\". This effectively results in a weighted finite automaton built on top of the datastore, instead of representing the datastore as a flat list. The creation of the automaton is unsupervised, and a RetoMaton can be constructed from any text collection: either the original training corpus or from another domain. Traversing this automaton at inference time, in parallel to the LM inference, reduces its perplexity by up to 1.85, or alternatively saves up to 83% of the nearest neighbor searches over $k$NN-LM (Khandelwal et al., 2020) without hurting perplexity. Our code and trained models are available at https://github.com/neulab/retomaton .",
            "corpus_id": 246431219,
            "sentences": [
                {
                    "corpus_id": "246431219",
                    "title": "Neuro-Symbolic Language Modeling with Automaton-augmented Retrieval",
                    "text": "Retrieval-based language models (R-LMs) have recently been shown to improve over standard neural models in a variety of tasks such as unconditional language modeling (Guu et al., 2018;He et al., 2020), machine translation (Zhang et al., 2018;Gu et al., 2018;Khandelwal et al., 2021), Proceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copyright 2022 by the author(s). question answering (Karpukhin et al., 2020;Ram et al., 2021), and code generation (Hayati et al., 2018;Hashimoto et al., 2018). The key ingredient of R-LMs is their ability to utilize training examples at test time without having to rely on the information encoded in the model's weights only. \n\nIn these models, the retrieval component first searches for nearest neighbor examples in an external datastore; then, the base model references these examples during the prediction. This fusion of language models (LMs) and retrieval improves the base language model from several perspectives, including higher accuracy (Xu et al., 2021), domain adaptability (Jiang et al., 2021), and reduced size (Borgeaud et al., 2021). Further, the retrieved examples provide information regarding the provenance of the model's predictions, and retrieval allows for modifying the dataset without retraining the model. Nevertheless, the most critical bottleneck of these models is their frequent search over the datastore, which hinders the use of R-LMs in practical settings. \n\nk-Nearest Neighbors Language Model One prominent example of such a retrieval-based model is kNN-LM (Grave et al., 2017;Khandelwal et al., 2020), which predicts a token by linearly interpolating the base LM's output with a nonparametric nearest neighbor distribution. This distribution is constructed by searching for the k-nearest neighbors (kNN) in the datastore and weighting them according to their distance to the current test context.",
                    "score": 0.41450130713776434,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 396
                        },
                        {
                            "start": 397,
                            "end": 429
                        },
                        {
                            "start": 430,
                            "end": 557
                        },
                        {
                            "start": 558,
                            "end": 723
                        },
                        {
                            "start": 726,
                            "end": 907
                        },
                        {
                            "start": 908,
                            "end": 1147
                        },
                        {
                            "start": 1148,
                            "end": 1329
                        },
                        {
                            "start": 1330,
                            "end": 1487
                        },
                        {
                            "start": 1490,
                            "end": 1756
                        },
                        {
                            "start": 1757,
                            "end": 1929
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 166,
                            "end": 184,
                            "matchedPaperCorpusId": "2318481"
                        },
                        {
                            "start": 184,
                            "end": 200,
                            "matchedPaperCorpusId": "220265977"
                        },
                        {
                            "start": 242,
                            "end": 258,
                            "matchedPaperCorpusId": "218614141"
                        },
                        {
                            "start": 258,
                            "end": 282,
                            "matchedPaperCorpusId": "222125236"
                        },
                        {
                            "start": 449,
                            "end": 473,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 512,
                            "end": 533,
                            "matchedPaperCorpusId": "220265977"
                        },
                        {
                            "start": 533,
                            "end": 556,
                            "matchedPaperCorpusId": "54446010"
                        },
                        {
                            "start": 1084,
                            "end": 1104,
                            "matchedPaperCorpusId": "237581333"
                        },
                        {
                            "start": 1609,
                            "end": 1633,
                            "matchedPaperCorpusId": "207870430"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97412109375
                }
            ],
            "relevance_judgement": 0.97412109375,
            "relevance_judgment_input_expanded": "# Title: Neuro-Symbolic Language Modeling with Automaton-augmented Retrieval\n# Venue: International Conference on Machine Learning\n# Authors: Uri Alon, Frank F. Xu, Junxian He, Sudipta Sengupta, D. Roth, Graham Neubig\n## Abstract\nRetrieval-based language models (R-LM) model the probability of natural language text by combining a standard language model (LM) with examples retrieved from an external datastore at test time. While effective, a major bottleneck of using these models in practice is the computationally costly datastore search, which can be performed as frequently as every time step. In this paper, we present RetoMaton - retrieval automaton - which approximates the datastore search, based on (1) saving pointers between consecutive datastore entries, and (2) clustering of entries into\"states\". This effectively results in a weighted finite automaton built on top of the datastore, instead of representing the datastore as a flat list. The creation of the automaton is unsupervised, and a RetoMaton can be constructed from any text collection: either the original training corpus or from another domain. Traversing this automaton at inference time, in parallel to the LM inference, reduces its perplexity by up to 1.85, or alternatively saves up to 83% of the nearest neighbor searches over $k$NN-LM (Khandelwal et al., 2020) without hurting perplexity. Our code and trained models are available at https://github.com/neulab/retomaton .\n## Introduction\nRetrieval-based language models (R-LMs) have recently been shown to improve over standard neural models in a variety of tasks such as unconditional language modeling (Guu et al., 2018;He et al., 2020), machine translation (Zhang et al., 2018;Gu et al., 2018;Khandelwal et al., 2021), Proceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copyright 2022 by the author(s). question answering (Karpukhin et al., 2020;Ram et al., 2021), and code generation (Hayati et al., 2018;Hashimoto et al., 2018). The key ingredient of R-LMs is their ability to utilize training examples at test time without having to rely on the information encoded in the model's weights only. \n\nIn these models, the retrieval component first searches for nearest neighbor examples in an external datastore; then, the base model references these examples during the prediction. This fusion of language models (LMs) and retrieval improves the base language model from several perspectives, including higher accuracy (Xu et al., 2021), domain adaptability (Jiang et al., 2021), and reduced size (Borgeaud et al., 2021). Further, the retrieved examples provide information regarding the provenance of the model's predictions, and retrieval allows for modifying the dataset without retraining the model. Nevertheless, the most critical bottleneck of these models is their frequent search over the datastore, which hinders the use of R-LMs in practical settings. \n\nk-Nearest Neighbors Language Model One prominent example of such a retrieval-based model is kNN-LM (Grave et al., 2017;Khandelwal et al., 2020), which predicts a token by linearly interpolating the base LM's output with a nonparametric nearest neighbor distribution. This distribution is constructed by searching for the k-nearest neighbors (kNN) in the datastore and weighting them according to their distance to the current test context.",
            "reference_string": "[246431219 | Alon et al. | 2022 | Citations: 63]"
        },
        {
            "title": "Language Model-Based Document Clustering Using Random Walks",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2006,
            "reference_count": 30,
            "citation_count": 44,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.3115/1220835.1220896",
                "status": "BRONZE",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/N06-1061, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2158159",
                    "name": "G\u00fcnes Erkan"
                }
            ],
            "abstract": "We propose a new document vector representation specifically designed for the document clustering task. Instead of the traditional term-based vectors, a document is represented as an n-dimensional vector, where n is the number of documents in the cluster. The value at each dimension of the vector is closely related to the generation probability based on the language model of the corresponding document. Inspired by the recent graph-based NLP methods, we reinforce the generation probabilities by iterating random walks on the underlying graph representation. Experiments with k-means and hierarchical clustering algorithms show significant improvements over the alternative tf\u00b7idf vector representation.",
            "corpus_id": 18819434,
            "sentences": [
                {
                    "corpus_id": "18819434",
                    "title": "Language Model-Based Document Clustering Using Random Walks",
                    "text": "The language modeling approach to information retrieval was first introduced by Ponte and Croft (1998) as an alternative (or an improvement) to the traditional \u00a1\u00a2 \u00a3 \u00a4\u00a5\u00a2 relevance models. In the language modeling framework, each document in the database defines a language model. The relevance of a document to a given query is ranked according to the generation probability of the query based on the underlying language model of the document. To induce a (unigram) language model from a document, we start with the maximum likelihood (ML) estimation of the term probabilities. For each term that occurs in a document , the ML estimation of with respect to is defined as \n\nwhere %& is the number of occurences of term in document . This estimation is often smoothed based on the following general formula: \n\nwhere ! \" # 7 89 @ A is the ML estimation of over an entire corpus which usually is a member of. \n\n3 is the general smoothing parameter that takes different forms in various smoothing methods. Smoothing has two important roles (Zhai and Lafferty, 2004). First, it accounts for terms unseen in the document preventing zero probabilities. This is similar to the smoothing effect in NLP problems such as parsing. Second, smoothing has an \u00a4\u00a5\u00a2like effect that accounts for the generation probabilities of the common terms in the corpus. A common smoothing technique is to use Bayesian smoothing with the Dirichlet prior (Zhai and Lafferty, 2004;Liu and Croft, 2004): \n\nHere, B is the smoothing parameter. Higher values of B mean more aggressive smoothing. \n\nAssuming the terms in a text are independent from each other, the generation probability of a text sequence C \n\ngiven the document is the product of the generation probabilities of the terms of \n\nIn the context of information retrieval, C is a query usually composed of few terms. In this work, we are interested in the generation probabilities of entire documents that usually have in the order of hundreds of unique terms. If we use Equation 1, we end up having unnatural probabilities which are irrepresentably small and cause floating point underflow. More importantly, longer documents tend to have much smaller generation probabilities no matter how closely related they are to the generating language model.",
                    "score": 0.4263006436547978,
                    "section_title": "Language Models",
                    "char_start_offset": 3082,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 186
                        },
                        {
                            "start": 187,
                            "end": 278
                        },
                        {
                            "start": 279,
                            "end": 442
                        },
                        {
                            "start": 443,
                            "end": 576
                        },
                        {
                            "start": 577,
                            "end": 669
                        },
                        {
                            "start": 672,
                            "end": 730
                        },
                        {
                            "start": 731,
                            "end": 804
                        },
                        {
                            "start": 807,
                            "end": 814
                        },
                        {
                            "start": 815,
                            "end": 903
                        },
                        {
                            "start": 906,
                            "end": 999
                        },
                        {
                            "start": 1000,
                            "end": 1060
                        },
                        {
                            "start": 1061,
                            "end": 1143
                        },
                        {
                            "start": 1144,
                            "end": 1216
                        },
                        {
                            "start": 1217,
                            "end": 1338
                        },
                        {
                            "start": 1339,
                            "end": 1468
                        },
                        {
                            "start": 1471,
                            "end": 1506
                        },
                        {
                            "start": 1507,
                            "end": 1557
                        },
                        {
                            "start": 1560,
                            "end": 1669
                        },
                        {
                            "start": 1672,
                            "end": 1753
                        },
                        {
                            "start": 1756,
                            "end": 1840
                        },
                        {
                            "start": 1841,
                            "end": 1984
                        },
                        {
                            "start": 1985,
                            "end": 2115
                        },
                        {
                            "start": 2116,
                            "end": 2274
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 80,
                            "end": 102,
                            "matchedPaperCorpusId": "2759772"
                        },
                        {
                            "start": 1034,
                            "end": 1059,
                            "matchedPaperCorpusId": "207670589"
                        },
                        {
                            "start": 1422,
                            "end": 1447,
                            "matchedPaperCorpusId": "207670589"
                        },
                        {
                            "start": 1447,
                            "end": 1467,
                            "matchedPaperCorpusId": "15479915"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97265625
                }
            ],
            "relevance_judgement": 0.97265625,
            "relevance_judgment_input_expanded": "# Title: Language Model-Based Document Clustering Using Random Walks\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: G\u00fcnes Erkan\n## Abstract\nWe propose a new document vector representation specifically designed for the document clustering task. Instead of the traditional term-based vectors, a document is represented as an n-dimensional vector, where n is the number of documents in the cluster. The value at each dimension of the vector is closely related to the generation probability based on the language model of the corresponding document. Inspired by the recent graph-based NLP methods, we reinforce the generation probabilities by iterating random walks on the underlying graph representation. Experiments with k-means and hierarchical clustering algorithms show significant improvements over the alternative tf\u00b7idf vector representation.\n## Language Models\nThe language modeling approach to information retrieval was first introduced by Ponte and Croft (1998) as an alternative (or an improvement) to the traditional \u00a1\u00a2 \u00a3 \u00a4\u00a5\u00a2 relevance models. In the language modeling framework, each document in the database defines a language model. The relevance of a document to a given query is ranked according to the generation probability of the query based on the underlying language model of the document. To induce a (unigram) language model from a document, we start with the maximum likelihood (ML) estimation of the term probabilities. For each term that occurs in a document , the ML estimation of with respect to is defined as \n\nwhere %& is the number of occurences of term in document . This estimation is often smoothed based on the following general formula: \n\nwhere ! \" # 7 89 @ A is the ML estimation of over an entire corpus which usually is a member of. \n\n3 is the general smoothing parameter that takes different forms in various smoothing methods. Smoothing has two important roles (Zhai and Lafferty, 2004). First, it accounts for terms unseen in the document preventing zero probabilities. This is similar to the smoothing effect in NLP problems such as parsing. Second, smoothing has an \u00a4\u00a5\u00a2like effect that accounts for the generation probabilities of the common terms in the corpus. A common smoothing technique is to use Bayesian smoothing with the Dirichlet prior (Zhai and Lafferty, 2004;Liu and Croft, 2004): \n\nHere, B is the smoothing parameter. Higher values of B mean more aggressive smoothing. \n\nAssuming the terms in a text are independent from each other, the generation probability of a text sequence C \n\ngiven the document is the product of the generation probabilities of the terms of \n\nIn the context of information retrieval, C is a query usually composed of few terms. In this work, we are interested in the generation probabilities of entire documents that usually have in the order of hundreds of unique terms. If we use Equation 1, we end up having unnatural probabilities which are irrepresentably small and cause floating point underflow. More importantly, longer documents tend to have much smaller generation probabilities no matter how closely related they are to the generating language model.",
            "reference_string": "[18819434 | Erkan | 2006 | Citations: 44]"
        },
        {
            "title": "Bridging Lexical Gaps between Queries and Questions on Large Online Q&A Collections with Compact Translation Models",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2008,
            "reference_count": 8,
            "citation_count": 68,
            "influential_citation_count": 8,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.3115/1613715.1613768",
                "status": "BRONZE",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/D08-1043, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "143703455",
                    "name": "Jung-Tae Lee"
                },
                {
                    "authorId": "2109561184",
                    "name": "Sang-Bum Kim"
                },
                {
                    "authorId": "1693485",
                    "name": "Young-In Song"
                },
                {
                    "authorId": "2326357",
                    "name": "Hae-Chang Rim"
                }
            ],
            "abstract": "Lexical gaps between queries and questions (documents) have been a major issue in question retrieval on large online question and answer (Q&A) collections. Previous studies address the issue by implicitly expanding queries with the help of translation models pre-constructed using statistical techniques. However, since it is possible for unimportant words (e.g., non-topical words, common words) to be included in the translation models, a lack of noise control on the models can cause degradation of retrieval performance. This paper investigates a number of empirical methods for eliminating unimportant words in order to construct compact translation models for retrieval purposes. Experiments conducted on a real world Q&A collection show that substantial improvements in retrieval performance can be achieved by using compact translation models.",
            "corpus_id": 7742063,
            "sentences": [
                {
                    "corpus_id": "7742063",
                    "title": "Bridging Lexical Gaps between Queries and Questions on Large Online Q&A Collections with Compact Translation Models",
                    "text": "This section introduces the translation-based language modeling approach to retrieval that has been used to bridge the lexical gap between queries and already-answered questions in this paper. \n\nIn the basic language modeling framework for retrieval (Ponte and Croft, 1998), the similarity between a query Q and a document D for ranking may be modeled as the probability of the document language model \n\nAssuming that query words occur independently given a particular document language model, the query-likelihood P (Q|M D ) is calculated as: \n\nwhere q represents a query word. \n\nTo avoid zero probabilities in document language models, a mixture between a document-specific multinomial distribution and a multinomial distribution estimated from the entire document collection is widely used in practice: \n\nwhere 0 < \u03bb < 1 and M C represents a language model built from the entire collection. The probabilities P (w|M D ) and P (w|M C ) are calculated using maximum likelihood estimation. \n\nThe basic language modeling framework does not address the issue of lexical gaps between queries and question. Berger and Lafferty (1999) viewed information retrieval as statistical document-query translation and introduced translation models to map query words to document words. Assuming that a translation model can be represented by a conditional probability distribution of translation T (\u2022|\u2022) between words, we can model P (q|M D ) in Equation 3 as: \n\nwhere w represents a document word. 3 he translation probability T (q|w) virtually represents the degree of relationship between query word q and document word w captured in a different, machine translation setting. Then, in the traditional information retrieval viewpoint, the use of translation models produce an implicit query expansion effect, since query words not in a document are mapped to related words in the document. This implies that translation-based retrieval models would make positive contributions to retrieval performance only when the pre-constructed translation models have reliable translation probability distributions.",
                    "score": 0.4024982055363106,
                    "section_title": "Translation-based Retrieval Model",
                    "char_start_offset": 3746,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 192
                        },
                        {
                            "start": 195,
                            "end": 401
                        },
                        {
                            "start": 404,
                            "end": 543
                        },
                        {
                            "start": 546,
                            "end": 578
                        },
                        {
                            "start": 581,
                            "end": 805
                        },
                        {
                            "start": 808,
                            "end": 893
                        },
                        {
                            "start": 894,
                            "end": 989
                        },
                        {
                            "start": 992,
                            "end": 1102
                        },
                        {
                            "start": 1103,
                            "end": 1272
                        },
                        {
                            "start": 1273,
                            "end": 1447
                        },
                        {
                            "start": 1450,
                            "end": 1487
                        },
                        {
                            "start": 1488,
                            "end": 1665
                        },
                        {
                            "start": 1666,
                            "end": 1878
                        },
                        {
                            "start": 1879,
                            "end": 2092
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 250,
                            "end": 273,
                            "matchedPaperCorpusId": "2759772"
                        },
                        {
                            "start": 1103,
                            "end": 1129,
                            "matchedPaperCorpusId": "10837883"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97265625
                }
            ],
            "relevance_judgement": 0.97265625,
            "relevance_judgment_input_expanded": "# Title: Bridging Lexical Gaps between Queries and Questions on Large Online Q&A Collections with Compact Translation Models\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Jung-Tae Lee, Sang-Bum Kim, Young-In Song, Hae-Chang Rim\n## Abstract\nLexical gaps between queries and questions (documents) have been a major issue in question retrieval on large online question and answer (Q&A) collections. Previous studies address the issue by implicitly expanding queries with the help of translation models pre-constructed using statistical techniques. However, since it is possible for unimportant words (e.g., non-topical words, common words) to be included in the translation models, a lack of noise control on the models can cause degradation of retrieval performance. This paper investigates a number of empirical methods for eliminating unimportant words in order to construct compact translation models for retrieval purposes. Experiments conducted on a real world Q&A collection show that substantial improvements in retrieval performance can be achieved by using compact translation models.\n## Translation-based Retrieval Model\nThis section introduces the translation-based language modeling approach to retrieval that has been used to bridge the lexical gap between queries and already-answered questions in this paper. \n\nIn the basic language modeling framework for retrieval (Ponte and Croft, 1998), the similarity between a query Q and a document D for ranking may be modeled as the probability of the document language model \n\nAssuming that query words occur independently given a particular document language model, the query-likelihood P (Q|M D ) is calculated as: \n\nwhere q represents a query word. \n\nTo avoid zero probabilities in document language models, a mixture between a document-specific multinomial distribution and a multinomial distribution estimated from the entire document collection is widely used in practice: \n\nwhere 0 < \u03bb < 1 and M C represents a language model built from the entire collection. The probabilities P (w|M D ) and P (w|M C ) are calculated using maximum likelihood estimation. \n\nThe basic language modeling framework does not address the issue of lexical gaps between queries and question. Berger and Lafferty (1999) viewed information retrieval as statistical document-query translation and introduced translation models to map query words to document words. Assuming that a translation model can be represented by a conditional probability distribution of translation T (\u2022|\u2022) between words, we can model P (q|M D ) in Equation 3 as: \n\nwhere w represents a document word. 3 he translation probability T (q|w) virtually represents the degree of relationship between query word q and document word w captured in a different, machine translation setting. Then, in the traditional information retrieval viewpoint, the use of translation models produce an implicit query expansion effect, since query words not in a document are mapped to related words in the document. This implies that translation-based retrieval models would make positive contributions to retrieval performance only when the pre-constructed translation models have reliable translation probability distributions.",
            "reference_string": "[7742063 | Lee et al. | 2008 | Citations: 68]"
        },
        {
            "title": "MemLong: Memory-Augmented Retrieval for Long Text Modeling",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 37,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.16967, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2319081035",
                    "name": "Weijie Liu"
                },
                {
                    "authorId": "1576234850",
                    "name": "Zecheng Tang"
                },
                {
                    "authorId": "2257093356",
                    "name": "Juntao Li"
                },
                {
                    "authorId": "2266796043",
                    "name": "Kehai Chen"
                },
                {
                    "authorId": "2258690229",
                    "name": "Min Zhang"
                }
            ],
            "abstract": "Recent advancements in Large Language Models (LLMs) have yielded remarkable success across diverse fields. However, handling long contexts remains a significant challenge for LLMs due to the quadratic time and space complexity of attention mechanisms and the growing memory consumption of the key-value cache during generation. This work introduces MemLong: Memory-Augmented Retrieval for Long Text Generation, a method designed to enhance the capabilities of long-context language modeling by utilizing an external retriever for historical information retrieval. MemLong combines a non-differentiable ``ret-mem'' module with a partially trainable decoder-only language model and introduces a fine-grained, controllable retrieval attention mechanism that leverages semantic-level relevant chunks. Comprehensive evaluations on multiple long-context language modeling benchmarks demonstrate that MemLong consistently outperforms other state-of-the-art LLMs. More importantly, MemLong can extend the context length on a single 3090 GPU from 4k up to 80k. Our code is available at https://github.com/Bui1dMySea/MemLong",
            "corpus_id": 272310589,
            "sentences": [
                {
                    "corpus_id": "272310589",
                    "title": "MemLong: Memory-Augmented Retrieval for Long Text Modeling",
                    "text": "Much effort has been made to enhance Retrieval-Augmented Language Modeling (Lewis et al., 2020;Izacard and Grave, 2020;Ram et al., 2023;Yu et al., 2022;Asai et al., 2023). While some approaches use external retrievers, non-parametric information fusion often falls short compared to parametric methods within the model. We concentrate on integrating retrieval concepts directly into the model. REALM (Guu et al., 2020) suggests that relying solely on internal model knowledge is inefficient and advocates for the model to learn to retrieve and comprehend. kNN-LM (Khandelwal et al., 2019) enhances language modeling by blending the LLM's next-word predictions with those from a retrieval-based mechanism. MemTrm (Wu et al., 2022) introduces a memory bank but risks shifting memory distributions due to parameter adjustments. LongMEM (Wang et al., 2024b) mitigates this by training a sub-network, though this adds significant overhead. In contrast, our approach involves a fixed pre-trained model, enhancing it with a frozen retriever that aligns with the model's internal retrieval processes, thus avoiding distribution shifts and architectural changes.",
                    "score": 0.38704866882156796,
                    "section_title": "Retrieval-Augmented Language Modeling",
                    "char_start_offset": 23636,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 171
                        },
                        {
                            "start": 172,
                            "end": 319
                        },
                        {
                            "start": 320,
                            "end": 393
                        },
                        {
                            "start": 394,
                            "end": 555
                        },
                        {
                            "start": 556,
                            "end": 704
                        },
                        {
                            "start": 705,
                            "end": 824
                        },
                        {
                            "start": 825,
                            "end": 934
                        },
                        {
                            "start": 935,
                            "end": 1153
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 75,
                            "end": 95,
                            "matchedPaperCorpusId": "218869575"
                        },
                        {
                            "start": 119,
                            "end": 136,
                            "matchedPaperCorpusId": "256459451"
                        },
                        {
                            "start": 833,
                            "end": 853,
                            "matchedPaperCorpusId": "259137816"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96826171875
                }
            ],
            "relevance_judgement": 0.96826171875,
            "relevance_judgment_input_expanded": "# Title: MemLong: Memory-Augmented Retrieval for Long Text Modeling\n# Venue: arXiv.org\n# Authors: Weijie Liu, Zecheng Tang, Juntao Li, Kehai Chen, Min Zhang\n## Abstract\nRecent advancements in Large Language Models (LLMs) have yielded remarkable success across diverse fields. However, handling long contexts remains a significant challenge for LLMs due to the quadratic time and space complexity of attention mechanisms and the growing memory consumption of the key-value cache during generation. This work introduces MemLong: Memory-Augmented Retrieval for Long Text Generation, a method designed to enhance the capabilities of long-context language modeling by utilizing an external retriever for historical information retrieval. MemLong combines a non-differentiable ``ret-mem'' module with a partially trainable decoder-only language model and introduces a fine-grained, controllable retrieval attention mechanism that leverages semantic-level relevant chunks. Comprehensive evaluations on multiple long-context language modeling benchmarks demonstrate that MemLong consistently outperforms other state-of-the-art LLMs. More importantly, MemLong can extend the context length on a single 3090 GPU from 4k up to 80k. Our code is available at https://github.com/Bui1dMySea/MemLong\n## Retrieval-Augmented Language Modeling\nMuch effort has been made to enhance Retrieval-Augmented Language Modeling (Lewis et al., 2020;Izacard and Grave, 2020;Ram et al., 2023;Yu et al., 2022;Asai et al., 2023). While some approaches use external retrievers, non-parametric information fusion often falls short compared to parametric methods within the model. We concentrate on integrating retrieval concepts directly into the model. REALM (Guu et al., 2020) suggests that relying solely on internal model knowledge is inefficient and advocates for the model to learn to retrieve and comprehend. kNN-LM (Khandelwal et al., 2019) enhances language modeling by blending the LLM's next-word predictions with those from a retrieval-based mechanism. MemTrm (Wu et al., 2022) introduces a memory bank but risks shifting memory distributions due to parameter adjustments. LongMEM (Wang et al., 2024b) mitigates this by training a sub-network, though this adds significant overhead. In contrast, our approach involves a fixed pre-trained model, enhancing it with a frozen retriever that aligns with the model's internal retrieval processes, thus avoiding distribution shifts and architectural changes.",
            "reference_string": "[272310589 | Liu et al. | 2024 | Citations: 3]"
        },
        {
            "title": "A P\u00f3lya Urn Document Language Model for Improved Information Retrieval",
            "venue": "ACM Trans. Inf. Syst.",
            "year": 2015,
            "reference_count": 66,
            "citation_count": 35,
            "influential_citation_count": 6,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1502.00804",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1502.00804, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "3161572",
                    "name": "Ronan Cummins"
                },
                {
                    "authorId": "2071527",
                    "name": "Jiaul H. Paik"
                },
                {
                    "authorId": "40282698",
                    "name": "Yuanhua Lv"
                }
            ],
            "abstract": "The multinomial language model has been one of the most effective models of retrieval for more than a decade. However, the multinomial distribution does not model one important linguistic phenomenon relating to term dependency\u2014that is, the tendency of a term to repeat itself within a document (i.e., word burstiness). In this article, we model document generation as a random process with reinforcement (a multivariate P\u00f3lya process) and develop a Dirichlet compound multinomial language model that captures word burstiness directly. We show that the new reinforced language model can be computed as efficiently as current retrieval models, and with experiments on an extensive set of TREC collections, we show that it significantly outperforms the state-of-the-art language model for a number of standard effectiveness metrics. Experiments also show that the tuning parameter in the proposed model is more robust than that in the multinomial language model. Furthermore, we develop a constraint for the verbosity hypothesis and show that the proposed model adheres to the constraint. Finally, we show that the new language model essentially introduces a measure closely related to idf, which gives theoretical justification for combining the term and document event spaces in tf-idf type schemes.",
            "corpus_id": 9519540,
            "sentences": [
                {
                    "corpus_id": "9519540",
                    "title": "A P\u00f3lya Urn Document Language Model for Improved Information Retrieval",
                    "text": "We have introduced a new family of language model (namely SPUD) based on a P\u00f3lya urn process. We have shown that a query likelihood retrieval method based on this model is superior to that of the state-of-the-art multinomial language model. Interestingly, we have shown that the new model can be computed as efficiently as the multinomial language model. Essentially, this means that the SPUD retrieval method can be used in place of the multinomial query likelihood method in many different retrieval applications and domains. \n\nWe have outlined a number of intuitions that help to motivate the new model. For example, we developed a constraint for the verbosity hypothesis and have shown that the most effective SPUD method, the SPUD dir model, adheres to this constraint. Furthermore, we have shown that the free hyperparameter (i.e. \u03c9 = 0.8) in the SPUD dir method is robust across various collections. This essentially reduces the need for experimental tuning. Given the principled nature of the approach developed, it can be used in a variety of IR tasks. We have shown that it is useful for downstream retrieval methods, as we have used it to estimate a pseudo-relevance based model (PURM) that demonstrates improved retrieval effectiveness on test collections when compared to a pseudo-relevance model based on the multinomial (RM3). \n\nFuture work will look to improve retrieval effectiveness by incorporating multiple DCM language models for modelling a document. Furthermore, we aim to investigate the query likelihood method using different generative assumptions for the query. In this work, we assumed a sampling-with-replacement strategy for query generation. However, different sampling strategies, such as those employed by Friedman urn's [Freedman 1965] might better model query generation.",
                    "score": 0.4024982055363106,
                    "section_title": "CONCLUSION",
                    "char_start_offset": 79653,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 93
                        },
                        {
                            "start": 94,
                            "end": 240
                        },
                        {
                            "start": 241,
                            "end": 354
                        },
                        {
                            "start": 355,
                            "end": 527
                        },
                        {
                            "start": 530,
                            "end": 606
                        },
                        {
                            "start": 607,
                            "end": 774
                        },
                        {
                            "start": 775,
                            "end": 836
                        },
                        {
                            "start": 837,
                            "end": 906
                        },
                        {
                            "start": 907,
                            "end": 965
                        },
                        {
                            "start": 966,
                            "end": 1061
                        },
                        {
                            "start": 1062,
                            "end": 1341
                        },
                        {
                            "start": 1344,
                            "end": 1472
                        },
                        {
                            "start": 1473,
                            "end": 1589
                        },
                        {
                            "start": 1590,
                            "end": 1673
                        },
                        {
                            "start": 1674,
                            "end": 1807
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96630859375
                },
                {
                    "corpus_id": "9519540",
                    "title": "A P\u00f3lya Urn Document Language Model for Improved Information Retrieval",
                    "text": "We use the Dirichlet compound multinomial (DCM), also known as the multivariate P\u00f3lya distribution, to model documents in place of the standard multinomial distribution, while we use the standard multinomial to model query generation. We show that this new retrieval model obtains significantly increased effectiveness compared to the current state-ofthe-art model on a range of datasets for a number of effectiveness metrics. This article is organized as follows. Section 2 introduces notation used in the remainder of the article and also presents a comprehensive review of relevant research. Section 3 reviews the standard language modelling approach. Section 4 presents the SPUD language model. Section 5 outlines efficient forms of the new retrieval functions, and provides deep insights into the proposed functions. The experimental design and results are presented in Section 6. Section 7 presents a discussion of the results and Section 8 concludes with a summary.",
                    "score": 0.41450130713776434,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 4072,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 234
                        },
                        {
                            "start": 235,
                            "end": 426
                        },
                        {
                            "start": 427,
                            "end": 464
                        },
                        {
                            "start": 465,
                            "end": 594
                        },
                        {
                            "start": 595,
                            "end": 654
                        },
                        {
                            "start": 655,
                            "end": 698
                        },
                        {
                            "start": 699,
                            "end": 821
                        },
                        {
                            "start": 822,
                            "end": 972
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9482421875
                }
            ],
            "relevance_judgement": 0.96630859375,
            "relevance_judgment_input_expanded": "# Title: A P\u00f3lya Urn Document Language Model for Improved Information Retrieval\n# Venue: ACM Trans. Inf. Syst.\n# Authors: Ronan Cummins, Jiaul H. Paik, Yuanhua Lv\n## Abstract\nThe multinomial language model has been one of the most effective models of retrieval for more than a decade. However, the multinomial distribution does not model one important linguistic phenomenon relating to term dependency\u2014that is, the tendency of a term to repeat itself within a document (i.e., word burstiness). In this article, we model document generation as a random process with reinforcement (a multivariate P\u00f3lya process) and develop a Dirichlet compound multinomial language model that captures word burstiness directly. We show that the new reinforced language model can be computed as efficiently as current retrieval models, and with experiments on an extensive set of TREC collections, we show that it significantly outperforms the state-of-the-art language model for a number of standard effectiveness metrics. Experiments also show that the tuning parameter in the proposed model is more robust than that in the multinomial language model. Furthermore, we develop a constraint for the verbosity hypothesis and show that the proposed model adheres to the constraint. Finally, we show that the new language model essentially introduces a measure closely related to idf, which gives theoretical justification for combining the term and document event spaces in tf-idf type schemes.\n## INTRODUCTION\nWe use the Dirichlet compound multinomial (DCM), also known as the multivariate P\u00f3lya distribution, to model documents in place of the standard multinomial distribution, while we use the standard multinomial to model query generation. We show that this new retrieval model obtains significantly increased effectiveness compared to the current state-ofthe-art model on a range of datasets for a number of effectiveness metrics. This article is organized as follows. Section 2 introduces notation used in the remainder of the article and also presents a comprehensive review of relevant research. Section 3 reviews the standard language modelling approach. Section 4 presents the SPUD language model. Section 5 outlines efficient forms of the new retrieval functions, and provides deep insights into the proposed functions. The experimental design and results are presented in Section 6. Section 7 presents a discussion of the results and Section 8 concludes with a summary.\n\n## CONCLUSION\nWe have introduced a new family of language model (namely SPUD) based on a P\u00f3lya urn process. We have shown that a query likelihood retrieval method based on this model is superior to that of the state-of-the-art multinomial language model. Interestingly, we have shown that the new model can be computed as efficiently as the multinomial language model. Essentially, this means that the SPUD retrieval method can be used in place of the multinomial query likelihood method in many different retrieval applications and domains. \n\nWe have outlined a number of intuitions that help to motivate the new model. For example, we developed a constraint for the verbosity hypothesis and have shown that the most effective SPUD method, the SPUD dir model, adheres to this constraint. Furthermore, we have shown that the free hyperparameter (i.e. \u03c9 = 0.8) in the SPUD dir method is robust across various collections. This essentially reduces the need for experimental tuning. Given the principled nature of the approach developed, it can be used in a variety of IR tasks. We have shown that it is useful for downstream retrieval methods, as we have used it to estimate a pseudo-relevance based model (PURM) that demonstrates improved retrieval effectiveness on test collections when compared to a pseudo-relevance model based on the multinomial (RM3). \n\nFuture work will look to improve retrieval effectiveness by incorporating multiple DCM language models for modelling a document. Furthermore, we aim to investigate the query likelihood method using different generative assumptions for the query. In this work, we assumed a sampling-with-replacement strategy for query generation. However, different sampling strategies, such as those employed by Friedman urn's [Freedman 1965] might better model query generation.",
            "reference_string": "[9519540 | Cummins et al. | 2015 | Citations: 35]"
        },
        {
            "title": "EmbedDistill: A Geometric Knowledge Distillation for Information Retrieval",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 69,
            "citation_count": 7,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2301.12005",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2301.12005, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2109548913",
                    "name": "Seungyeon Kim"
                },
                {
                    "authorId": "2241094",
                    "name": "A. Rawat"
                },
                {
                    "authorId": "1771307",
                    "name": "M. Zaheer"
                },
                {
                    "authorId": "3078751",
                    "name": "Sadeep Jayasumana"
                },
                {
                    "authorId": "3148908",
                    "name": "Veeranjaneyulu Sadhanala"
                },
                {
                    "authorId": "2141405",
                    "name": "Wittawat Jitkrittum"
                },
                {
                    "authorId": "2844480",
                    "name": "A. Menon"
                },
                {
                    "authorId": "2276554",
                    "name": "R. Fergus"
                },
                {
                    "authorId": "49596260",
                    "name": "Surinder Kumar"
                }
            ],
            "abstract": "Large neural models (such as Transformers) achieve state-of-the-art performance for information retrieval (IR). In this paper, we aim to improve distillation methods that pave the way for the resource-efficient deployment of such models in practice. Inspired by our theoretical analysis of the teacher-student generalization gap for IR models, we propose a novel distillation approach that leverages the relative geometry among queries and documents learned by the large teacher model. Unlike existing teacher score-based distillation methods, our proposed approach employs embedding matching tasks to provide a stronger signal to align the representations of the teacher and student models. In addition, it utilizes query generation to explore the data manifold to reduce the discrepancies between the student and the teacher where training data is sparse. Furthermore, our analysis also motivates novel asymmetric architectures for student models which realizes better embedding alignment without increasing online inference cost. On standard benchmarks like MSMARCO, we show that our approach successfully distills from both dual-encoder (DE) and cross-encoder (CE) teacher models to 1/10th size asymmetric students that can retain 95-97% of the teacher performance.",
            "corpus_id": 256389465,
            "sentences": [
                {
                    "corpus_id": "256389465",
                    "title": "EmbedDistill: A Geometric Knowledge Distillation for Information Retrieval",
                    "text": "Neural models for information retrieval (IR) are increasingly used to model the true ranking function in various applications, including web search [Mitra and Craswell, 2018], recommendation [Zhang et al., 2019], and question-answering (QA) [Chen et al., 2017]. Notably, the recent success of Transformers [Vaswani et al., 2017]-based pre-trained language models [Devlin et al., 2019, Liu et al., 2019, Raffel et al., 2020] on a wide range of natural language understanding tasks has also prompted their utilization in IR to capture query-document relevance [see, e.g., Dai and Callan, 2019b, MacAvaney et al., 2019a, Nogueira and Cho, 2019, Lee et al., 2019, Karpukhin et al., 2020a]. A typical IR system comprises two stages: (1) A retriever first selects a small subset of potentially relevant candidate documents (out of a large collection) for a given query; and (2) A re-ranker then identifies a precise ranking among the candidates provided by the retriever. Dual-encoder (DE) models are the de-facto architecture for retrievers [Lee et al., 2019, Karpukhin et al., 2020a]. Such models independently embed queries and documents into a common space, and capture their relevance by simple operations on these embeddings such as the inner product. This enables offline creation of a document index and supports fast retrieval during inference via efficient maximum inner product search implementations [Guo et al., 2020, Johnson et al., 2021], with online query embedding generation primarily dictating the inference latency. Cross-encoder (CE) models, on the other hand, are preferred as re-rankers, owing to their excellent performance [Nogueira and Cho, 2019, Dai and Callan, 2019a, Yilmaz et al., 2019]. A CE model jointly encodes a query-document pair while enabling early interaction among query and document features. Employing a CE model for retrieval is often infeasible, as it would require processing a given query with every document in the collection at inference time.",
                    "score": 0.388552541238673,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 261
                        },
                        {
                            "start": 262,
                            "end": 685
                        },
                        {
                            "start": 686,
                            "end": 965
                        },
                        {
                            "start": 966,
                            "end": 1080
                        },
                        {
                            "start": 1081,
                            "end": 1251
                        },
                        {
                            "start": 1252,
                            "end": 1529
                        },
                        {
                            "start": 1530,
                            "end": 1711
                        },
                        {
                            "start": 1712,
                            "end": 1828
                        },
                        {
                            "start": 1829,
                            "end": 1986
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 241,
                            "end": 260,
                            "matchedPaperCorpusId": "3618568"
                        },
                        {
                            "start": 306,
                            "end": 328,
                            "matchedPaperCorpusId": "13756489"
                        },
                        {
                            "start": 363,
                            "end": 383,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 401,
                            "end": 423,
                            "matchedPaperCorpusId": "204838007"
                        },
                        {
                            "start": 591,
                            "end": 616,
                            "matchedPaperCorpusId": "131776819"
                        },
                        {
                            "start": 658,
                            "end": 684,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 1053,
                            "end": 1079,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 1406,
                            "end": 1423,
                            "matchedPaperCorpusId": "218614141"
                        },
                        {
                            "start": 1423,
                            "end": 1446,
                            "matchedPaperCorpusId": "926364"
                        },
                        {
                            "start": 1665,
                            "end": 1688,
                            "matchedPaperCorpusId": "162168864"
                        },
                        {
                            "start": 1688,
                            "end": 1710,
                            "matchedPaperCorpusId": "202635721"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9638671875
                }
            ],
            "relevance_judgement": 0.9638671875,
            "relevance_judgment_input_expanded": "# Title: EmbedDistill: A Geometric Knowledge Distillation for Information Retrieval\n# Venue: arXiv.org\n# Authors: Seungyeon Kim, A. Rawat, M. Zaheer, Sadeep Jayasumana, Veeranjaneyulu Sadhanala, Wittawat Jitkrittum, A. Menon, R. Fergus, Surinder Kumar\n## Abstract\nLarge neural models (such as Transformers) achieve state-of-the-art performance for information retrieval (IR). In this paper, we aim to improve distillation methods that pave the way for the resource-efficient deployment of such models in practice. Inspired by our theoretical analysis of the teacher-student generalization gap for IR models, we propose a novel distillation approach that leverages the relative geometry among queries and documents learned by the large teacher model. Unlike existing teacher score-based distillation methods, our proposed approach employs embedding matching tasks to provide a stronger signal to align the representations of the teacher and student models. In addition, it utilizes query generation to explore the data manifold to reduce the discrepancies between the student and the teacher where training data is sparse. Furthermore, our analysis also motivates novel asymmetric architectures for student models which realizes better embedding alignment without increasing online inference cost. On standard benchmarks like MSMARCO, we show that our approach successfully distills from both dual-encoder (DE) and cross-encoder (CE) teacher models to 1/10th size asymmetric students that can retain 95-97% of the teacher performance.\n## Introduction\nNeural models for information retrieval (IR) are increasingly used to model the true ranking function in various applications, including web search [Mitra and Craswell, 2018], recommendation [Zhang et al., 2019], and question-answering (QA) [Chen et al., 2017]. Notably, the recent success of Transformers [Vaswani et al., 2017]-based pre-trained language models [Devlin et al., 2019, Liu et al., 2019, Raffel et al., 2020] on a wide range of natural language understanding tasks has also prompted their utilization in IR to capture query-document relevance [see, e.g., Dai and Callan, 2019b, MacAvaney et al., 2019a, Nogueira and Cho, 2019, Lee et al., 2019, Karpukhin et al., 2020a]. A typical IR system comprises two stages: (1) A retriever first selects a small subset of potentially relevant candidate documents (out of a large collection) for a given query; and (2) A re-ranker then identifies a precise ranking among the candidates provided by the retriever. Dual-encoder (DE) models are the de-facto architecture for retrievers [Lee et al., 2019, Karpukhin et al., 2020a]. Such models independently embed queries and documents into a common space, and capture their relevance by simple operations on these embeddings such as the inner product. This enables offline creation of a document index and supports fast retrieval during inference via efficient maximum inner product search implementations [Guo et al., 2020, Johnson et al., 2021], with online query embedding generation primarily dictating the inference latency. Cross-encoder (CE) models, on the other hand, are preferred as re-rankers, owing to their excellent performance [Nogueira and Cho, 2019, Dai and Callan, 2019a, Yilmaz et al., 2019]. A CE model jointly encodes a query-document pair while enabling early interaction among query and document features. Employing a CE model for retrieval is often infeasible, as it would require processing a given query with every document in the collection at inference time.",
            "reference_string": "[256389465 | Kim et al. | 2023 | Citations: 7]"
        },
        {
            "title": "Joint Representations of Text and Knowledge Graphs for Retrieval and Evaluation",
            "venue": "International Joint Conference on Natural Language Processing",
            "year": 2023,
            "reference_count": 51,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2302.14785",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2302.14785, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1379806208",
                    "name": "Teven Le Scao"
                },
                {
                    "authorId": "2065132524",
                    "name": "Claire Gardent"
                }
            ],
            "abstract": "A key feature of neural models is that they can produce semantic vector representations of objects (texts, images, speech, etc.) ensuring that similar objects are close to each other in the vector space. While much work has focused on learning representations for other modalities, there are no aligned cross-modal representations for text and knowledge base (KB) elements. One challenge for learning such representations is the lack of parallel data, which we use contrastive training on heuristics-based datasets and data augmentation to overcome, training embedding models on (KB graph, text) pairs. On WebNLG, a cleaner manually crafted dataset, we show that they learn aligned representations suitable for retrieval. We then fine-tune on annotated data to create EREDAT (Ensembled Representations for Evaluation of DAta-to-Text), a similarity metric between English text and KB graphs. EREDAT outperforms or matches state-of-the-art metrics in terms of correlation with human judgments on WebNLG even though, unlike them, it does not require a reference text to compare against.",
            "corpus_id": 257232958,
            "sentences": [
                {
                    "corpus_id": "257232958",
                    "title": "Joint Representations of Text and Knowledge Graphs for Retrieval and Evaluation",
                    "text": "We briefly review recent approaches to uni-and cross-modal retrieval, representation learning models, and evaluation metrics for Natural Language Generation (NLG) models. \n\nNatural Language Retrieval Models. For natural language, a first class of retrieval models focuses on retrieving sentences that are similar to some input sentence. BERT (Devlin et al., 2019) has been used as a cross-encoder. Two sentences are given with a separator token, cross-attention applies to all input tokens and the resulting representation is fed into a linear layer to score the match. However, this is computationally inefficient as it is not possible to pre-compute and index such representations. A pre-computable model was proposed by (Reimers and Gurevych, 2019) who used twin encoders pre-trained on Natural Language Inference data (Bowman et al., 2015) to set new state-of-the-art performance on a large set of sentence scoring tasks. Further work (Chen et al., 2020;Humeau et al., 2019) combined cross-and bi-encoders to reach a tradeoff between accuracy and efficiency. We differ from those works in that we focus on cross-modal representation learning. \n\nRepresentation Learning for Knowledge-Bases. \n\nVarious KB embedding models have been proposed to support downstream applications such as KB completion or alignment of different bases. Compositional approaches (Nickel et al., 2011(Nickel et al., , 2016) ) use tensor products to model relations as functions of their argument entities. Translational approaches model relations as translation operations from the subject (head) to object (tail) entity (Bordes et al., 2013;Yang et al., 2014;Trouillon et al., 2016). \n\nNeural models have also leveraged 2-D convolutions over entity embeddings to predict relations (Dettmers et al., 2018) as well as graph convolutional networks (Schlichtkrull et al., 2018). All these approaches focus on representation learning for Knowledge-Bases entities and relations. In contrast, we focus on cross-modal similarity between a text and a KB graph. \n\nCross-Modal Representation Learning and Retrieval.",
                    "score": 0.3908534054751986,
                    "section_title": "Related Work",
                    "char_start_offset": 4020,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 170
                        },
                        {
                            "start": 173,
                            "end": 207
                        },
                        {
                            "start": 208,
                            "end": 336
                        },
                        {
                            "start": 337,
                            "end": 397
                        },
                        {
                            "start": 398,
                            "end": 569
                        },
                        {
                            "start": 570,
                            "end": 683
                        },
                        {
                            "start": 684,
                            "end": 925
                        },
                        {
                            "start": 926,
                            "end": 1062
                        },
                        {
                            "start": 1063,
                            "end": 1146
                        },
                        {
                            "start": 1149,
                            "end": 1193
                        },
                        {
                            "start": 1196,
                            "end": 1332
                        },
                        {
                            "start": 1333,
                            "end": 1483
                        },
                        {
                            "start": 1484,
                            "end": 1662
                        },
                        {
                            "start": 1665,
                            "end": 1853
                        },
                        {
                            "start": 1854,
                            "end": 1951
                        },
                        {
                            "start": 1952,
                            "end": 2030
                        },
                        {
                            "start": 2033,
                            "end": 2083
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 342,
                            "end": 363,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 723,
                            "end": 751,
                            "matchedPaperCorpusId": "201646309"
                        },
                        {
                            "start": 822,
                            "end": 843,
                            "matchedPaperCorpusId": "14604520"
                        },
                        {
                            "start": 939,
                            "end": 958,
                            "matchedPaperCorpusId": "222177208"
                        },
                        {
                            "start": 1358,
                            "end": 1378,
                            "matchedPaperCorpusId": "1157792"
                        },
                        {
                            "start": 1378,
                            "end": 1403,
                            "matchedPaperCorpusId": "6071257"
                        },
                        {
                            "start": 1599,
                            "end": 1620,
                            "matchedPaperCorpusId": "14941970"
                        },
                        {
                            "start": 1638,
                            "end": 1661,
                            "matchedPaperCorpusId": "15150247"
                        },
                        {
                            "start": 1760,
                            "end": 1783,
                            "matchedPaperCorpusId": "4328400"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9619140625
                }
            ],
            "relevance_judgement": 0.9619140625,
            "relevance_judgment_input_expanded": "# Title: Joint Representations of Text and Knowledge Graphs for Retrieval and Evaluation\n# Venue: International Joint Conference on Natural Language Processing\n# Authors: Teven Le Scao, Claire Gardent\n## Abstract\nA key feature of neural models is that they can produce semantic vector representations of objects (texts, images, speech, etc.) ensuring that similar objects are close to each other in the vector space. While much work has focused on learning representations for other modalities, there are no aligned cross-modal representations for text and knowledge base (KB) elements. One challenge for learning such representations is the lack of parallel data, which we use contrastive training on heuristics-based datasets and data augmentation to overcome, training embedding models on (KB graph, text) pairs. On WebNLG, a cleaner manually crafted dataset, we show that they learn aligned representations suitable for retrieval. We then fine-tune on annotated data to create EREDAT (Ensembled Representations for Evaluation of DAta-to-Text), a similarity metric between English text and KB graphs. EREDAT outperforms or matches state-of-the-art metrics in terms of correlation with human judgments on WebNLG even though, unlike them, it does not require a reference text to compare against.\n## Related Work\nWe briefly review recent approaches to uni-and cross-modal retrieval, representation learning models, and evaluation metrics for Natural Language Generation (NLG) models. \n\nNatural Language Retrieval Models. For natural language, a first class of retrieval models focuses on retrieving sentences that are similar to some input sentence. BERT (Devlin et al., 2019) has been used as a cross-encoder. Two sentences are given with a separator token, cross-attention applies to all input tokens and the resulting representation is fed into a linear layer to score the match. However, this is computationally inefficient as it is not possible to pre-compute and index such representations. A pre-computable model was proposed by (Reimers and Gurevych, 2019) who used twin encoders pre-trained on Natural Language Inference data (Bowman et al., 2015) to set new state-of-the-art performance on a large set of sentence scoring tasks. Further work (Chen et al., 2020;Humeau et al., 2019) combined cross-and bi-encoders to reach a tradeoff between accuracy and efficiency. We differ from those works in that we focus on cross-modal representation learning. \n\nRepresentation Learning for Knowledge-Bases. \n\nVarious KB embedding models have been proposed to support downstream applications such as KB completion or alignment of different bases. Compositional approaches (Nickel et al., 2011(Nickel et al., , 2016) ) use tensor products to model relations as functions of their argument entities. Translational approaches model relations as translation operations from the subject (head) to object (tail) entity (Bordes et al., 2013;Yang et al., 2014;Trouillon et al., 2016). \n\nNeural models have also leveraged 2-D convolutions over entity embeddings to predict relations (Dettmers et al., 2018) as well as graph convolutional networks (Schlichtkrull et al., 2018). All these approaches focus on representation learning for Knowledge-Bases entities and relations. In contrast, we focus on cross-modal similarity between a text and a KB graph. \n\nCross-Modal Representation Learning and Retrieval.",
            "reference_string": "[257232958 | Scao et al. | 2023 | Citations: 2]"
        },
        {
            "title": "An adaptive term proximity based rocchio\u2019s model for clinical decision support retrieval",
            "venue": "BMC Medical Informatics and Decision Making",
            "year": 2019,
            "reference_count": 45,
            "citation_count": 13,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://bmcmedinformdecismak.biomedcentral.com/track/pdf/10.1186/s12911-019-0986-6",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC6907113, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2061164537",
                    "name": "Min Pan"
                },
                {
                    "authorId": "2145914165",
                    "name": "Yue Zhang"
                },
                {
                    "authorId": "2152206656",
                    "name": "Qiang Zhu"
                },
                {
                    "authorId": "2148384784",
                    "name": "Bo Sun"
                },
                {
                    "authorId": "145525861",
                    "name": "Tingting He"
                },
                {
                    "authorId": "2902461",
                    "name": "Xingpeng Jiang"
                }
            ],
            "abstract": "In order to better help doctors make decision in the clinical setting, research is necessary to connect electronic health record (EHR) with the biomedical literature. Pseudo Relevance Feedback (PRF) is a kind of classical query modification technique that has shown to be effective in many retrieval models and thus suitable for handling terse language and clinical jargons in EHR. Previous work has introduced a set of constraints (axioms) of traditional PRF model. However, in the feedback document, the importance degree of candidate term and the co-occurrence relationship between a candidate term and a query term. Most methods do not consider both of these factors. Intuitively, terms that have higher co-occurrence degree with a query term are more likely to be related to the query topic. In this paper, we incorporate original HAL model into the Rocchio\u2019s model, and propose a new concept of term proximity feedback weight. A HAL-based Rocchio\u2019s model in the query expansion, called HRoc, is proposed. Meanwhile, we design three normalization methods to better incorporate proximity information to query expansion. Finally, we introduce an adaptive parameter to replace the length of sliding window of HAL model, and it can select window size according to document length. Based on 2016 TREC Clinical Support medicine dataset, experimental results demonstrate that the proposed HRoc and HRoc_AP models superior to other advanced models, such as PRoc2 and TF-PRF methods on various evaluation metrics. Among them, compared with the Proc2 and TF-PRF models, the MAP of our model is increased by 8.5% and 12.24% respectively, while the F1 score of our model is increased by 7.86% and 9.88% respectively. The proposed HRoc model can effectively enhance the precision and the recall rate of Information Retrieval and gets a more precise result than other models. Furthermore, after introducing self-adaptive parameter, the advanced HRoc_AP model uses less hyper-parameters than other models while enjoys an equivalent performance, which greatly improves the efficiency and applicability of the model and thus helps clinicians to retrieve clinical support document effectively.",
            "corpus_id": 209331668,
            "sentences": [
                {
                    "corpus_id": "209331668",
                    "title": "An adaptive term proximity based rocchio\u2019s model for clinical decision support retrieval",
                    "text": "After comparing the proposed methods with essential retrieval model BM25 and KL-divergence language modeling (LM) retrieval, we show the experimental results in Table 4. \n\nAs is shown in Table 4, BM25 performs slightly better with Dirichlet prior in terms of MAP and P@10 metrics, and LM is superior to it in the rest of metrics. These two basic models perform comparatively, without observing significant different. \n\nNext, \"BM25+Rocchio\", the combination of BM25 and Rocchio's model, and relevance language model (RM3) are used as two strong baselines in this paper. These two methods can achieve better retrieval performance in most cases. Using them as the basic models of the PRF baselines is reasonable. Thus, we use them to compare the proposed model. \n\nWe demonstrate the results of baseline PRF models and the three proposed models (HRoc1, HRoc2, and HRoc3) with different evaluation metrics in Table 5. In particular, \"*\" and \"+\" indicate a statistically significant improvement over BM25+Rocchio and RM3 respectively (Wilcoxon signed-rank test with p < 0.05). The bold style in row corresponding to the best result. And Rocchio is used with BM25 and RM3 is used with LM for fair comparison. \n\nTable 5 shows that the average performance of proposed models is better than that of baseline models. First, it has been proved that both Rocchio and RM3 are effective. They are considered to be strong baselines in previous studies. Rocchio model is superior to the RM3 model in terms of P@5 and P@10 metrics, but RM3 model outperforms Rocchio model in terms of MAP, P@20 and F1value. Second, among the three proposed models, HRoc1 performs better retrieval results than HRoc2 or HRoc3  The values in parentheses represent the improvements over BM25+Rocchio and RM3 respectively. The best result obtained is shown in bold, and the superscripts \"*\" and \"+\" denote statistically significant improvements over BM25+Rocchio and RM3, respectively (Wilcoxon signed-rank test with p < 0.05) \n\nin terms of all metrics.",
                    "score": 0.41045944639441756,
                    "section_title": "Comparison with pRF basic models",
                    "char_start_offset": 18697,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 169
                        },
                        {
                            "start": 172,
                            "end": 329
                        },
                        {
                            "start": 330,
                            "end": 416
                        },
                        {
                            "start": 419,
                            "end": 568
                        },
                        {
                            "start": 569,
                            "end": 642
                        },
                        {
                            "start": 643,
                            "end": 709
                        },
                        {
                            "start": 710,
                            "end": 758
                        },
                        {
                            "start": 761,
                            "end": 912
                        },
                        {
                            "start": 913,
                            "end": 1070
                        },
                        {
                            "start": 1071,
                            "end": 1126
                        },
                        {
                            "start": 1127,
                            "end": 1201
                        },
                        {
                            "start": 1204,
                            "end": 1305
                        },
                        {
                            "start": 1306,
                            "end": 1372
                        },
                        {
                            "start": 1373,
                            "end": 1436
                        },
                        {
                            "start": 1437,
                            "end": 1588
                        },
                        {
                            "start": 1589,
                            "end": 1783
                        },
                        {
                            "start": 1784,
                            "end": 1987
                        },
                        {
                            "start": 1990,
                            "end": 2014
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96044921875
                }
            ],
            "relevance_judgement": 0.96044921875,
            "relevance_judgment_input_expanded": "# Title: An adaptive term proximity based rocchio\u2019s model for clinical decision support retrieval\n# Venue: BMC Medical Informatics and Decision Making\n# Authors: Min Pan, Yue Zhang, Qiang Zhu, Bo Sun, Tingting He, Xingpeng Jiang\n## Abstract\nIn order to better help doctors make decision in the clinical setting, research is necessary to connect electronic health record (EHR) with the biomedical literature. Pseudo Relevance Feedback (PRF) is a kind of classical query modification technique that has shown to be effective in many retrieval models and thus suitable for handling terse language and clinical jargons in EHR. Previous work has introduced a set of constraints (axioms) of traditional PRF model. However, in the feedback document, the importance degree of candidate term and the co-occurrence relationship between a candidate term and a query term. Most methods do not consider both of these factors. Intuitively, terms that have higher co-occurrence degree with a query term are more likely to be related to the query topic. In this paper, we incorporate original HAL model into the Rocchio\u2019s model, and propose a new concept of term proximity feedback weight. A HAL-based Rocchio\u2019s model in the query expansion, called HRoc, is proposed. Meanwhile, we design three normalization methods to better incorporate proximity information to query expansion. Finally, we introduce an adaptive parameter to replace the length of sliding window of HAL model, and it can select window size according to document length. Based on 2016 TREC Clinical Support medicine dataset, experimental results demonstrate that the proposed HRoc and HRoc_AP models superior to other advanced models, such as PRoc2 and TF-PRF methods on various evaluation metrics. Among them, compared with the Proc2 and TF-PRF models, the MAP of our model is increased by 8.5% and 12.24% respectively, while the F1 score of our model is increased by 7.86% and 9.88% respectively. The proposed HRoc model can effectively enhance the precision and the recall rate of Information Retrieval and gets a more precise result than other models. Furthermore, after introducing self-adaptive parameter, the advanced HRoc_AP model uses less hyper-parameters than other models while enjoys an equivalent performance, which greatly improves the efficiency and applicability of the model and thus helps clinicians to retrieve clinical support document effectively.\n## Comparison with pRF basic models\nAfter comparing the proposed methods with essential retrieval model BM25 and KL-divergence language modeling (LM) retrieval, we show the experimental results in Table 4. \n\nAs is shown in Table 4, BM25 performs slightly better with Dirichlet prior in terms of MAP and P@10 metrics, and LM is superior to it in the rest of metrics. These two basic models perform comparatively, without observing significant different. \n\nNext, \"BM25+Rocchio\", the combination of BM25 and Rocchio's model, and relevance language model (RM3) are used as two strong baselines in this paper. These two methods can achieve better retrieval performance in most cases. Using them as the basic models of the PRF baselines is reasonable. Thus, we use them to compare the proposed model. \n\nWe demonstrate the results of baseline PRF models and the three proposed models (HRoc1, HRoc2, and HRoc3) with different evaluation metrics in Table 5. In particular, \"*\" and \"+\" indicate a statistically significant improvement over BM25+Rocchio and RM3 respectively (Wilcoxon signed-rank test with p < 0.05). The bold style in row corresponding to the best result. And Rocchio is used with BM25 and RM3 is used with LM for fair comparison. \n\nTable 5 shows that the average performance of proposed models is better than that of baseline models. First, it has been proved that both Rocchio and RM3 are effective. They are considered to be strong baselines in previous studies. Rocchio model is superior to the RM3 model in terms of P@5 and P@10 metrics, but RM3 model outperforms Rocchio model in terms of MAP, P@20 and F1value. Second, among the three proposed models, HRoc1 performs better retrieval results than HRoc2 or HRoc3  The values in parentheses represent the improvements over BM25+Rocchio and RM3 respectively. The best result obtained is shown in bold, and the superscripts \"*\" and \"+\" denote statistically significant improvements over BM25+Rocchio and RM3, respectively (Wilcoxon signed-rank test with p < 0.05) \n\nin terms of all metrics.",
            "reference_string": "[209331668 | Pan et al. | 2019 | Citations: 13]"
        },
        {
            "title": "Suffix Retrieval-Augmented Language Modeling",
            "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
            "year": 2022,
            "reference_count": 17,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2211.03053",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2211.03053, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "70452651",
                    "name": "Zecheng Wang"
                },
                {
                    "authorId": "1789138",
                    "name": "Yik-Cheung Tam"
                }
            ],
            "abstract": "Causal language modeling (LM) uses word history to predict the next word. BERT, on the other hand, makes use of bi-directional word information in a sentence to predict words at masked positions. While BERT is effective in sequence encoding, it is non-causal by nature and is not designed for sequence generation. In this paper, we propose a novel language model, SUffix REtrieval-Augmented LM (SUREALM), that simulates a bi-directional contextual effect in an autoregressive manner. SUREALM employs an embedding retriever to search for training sentences in a data store that share similar word history during sequence generation. In particular, the suffix portions of the retrieved sentences mimick the \"future\" context. We evalu-ated our proposed model on the DSTC9 spoken dialogue corpus and showed promising word perplexity reduction on the validation and test set compared to competitive baselines. Our source code is re-leased on GitHub 1.",
            "corpus_id": 253384615,
            "sentences": [
                {
                    "corpus_id": "253384615",
                    "title": "Suffix Retrieval-Augmented Language Modeling",
                    "text": "Improving language model using retrieval technique is not new. [4] employs document retrieval to retrieve relevant documents which are used to create an adaptive language model and interpolating it with the background statistical N-gram language model. [5] employs information retrieval to perform language model adaptation for statistical machine translation. Once the language models are adapted, they are kept fixed during sequence generation. \n\nMemory Networks (MemNNs [6]) are a family of neural networks integrating a memory component which can be updated during training. To avoid large storage of external memory, MemNNs can be trained end-to-end [7] by computing a compatibility score between a query and memory items , similar to an attention mechanism. SUREALM resembles MemNNs as we utilize an external data store as \"memory\". However, SUREALM's \"memory\" contains precomputed embeddings and is retrieved with an embedding retriever for computational efficiency. Like E2E MemNNs [7], we also distinguish between memory and query. We further construct our memory as prefix-suffix pairs enabling a more effective retrieval scheme by matching input query with prefixes. \n\nMost recent development in language modeling is based on transformers [8]. BERT-based Masked language modeling [9,10] exploits bi-directional information of a sentence to predict the word identity of the masked tokens. While BERT is effective in encoding sequences, it is not suitable for sequence generation due to its non-causal nature. Causal language modeling such as GPT2 [11] is uni-directional. Our proposed model attempts to retain the best of the two worlds as autoregressive and simulated bi-directional via augmentation of suffix embeddings during sequence generation. \n\nOne noticeable work for language modeling using embedding retrieval is nearest neighbor language model (KNN-LM) [12]. Their approach store dynamic information in an external knowledge base. During sequence generation, KNN-LM uses the current prefix to retrieve similar prefixes in the data store using embedding retrieval. The output probability distribution is estimated by looking at the corresponding next words in the retrieved prefixes. Such word probability distribution is linearly interpolated with the output word distribution from the causal transformer LM.",
                    "score": 0.3946961894953656,
                    "section_title": "RELATED WORK",
                    "char_start_offset": 3166,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 62
                        },
                        {
                            "start": 63,
                            "end": 252
                        },
                        {
                            "start": 253,
                            "end": 360
                        },
                        {
                            "start": 361,
                            "end": 446
                        },
                        {
                            "start": 449,
                            "end": 578
                        },
                        {
                            "start": 579,
                            "end": 763
                        },
                        {
                            "start": 764,
                            "end": 838
                        },
                        {
                            "start": 839,
                            "end": 973
                        },
                        {
                            "start": 974,
                            "end": 1040
                        },
                        {
                            "start": 1041,
                            "end": 1177
                        },
                        {
                            "start": 1180,
                            "end": 1254
                        },
                        {
                            "start": 1255,
                            "end": 1398
                        },
                        {
                            "start": 1399,
                            "end": 1518
                        },
                        {
                            "start": 1519,
                            "end": 1581
                        },
                        {
                            "start": 1582,
                            "end": 1759
                        },
                        {
                            "start": 1762,
                            "end": 1879
                        },
                        {
                            "start": 1880,
                            "end": 1951
                        },
                        {
                            "start": 1952,
                            "end": 2084
                        },
                        {
                            "start": 2085,
                            "end": 2203
                        },
                        {
                            "start": 2204,
                            "end": 2329
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 63,
                            "end": 66,
                            "matchedPaperCorpusId": "18439695"
                        },
                        {
                            "start": 253,
                            "end": 256,
                            "matchedPaperCorpusId": "2375338"
                        },
                        {
                            "start": 473,
                            "end": 476,
                            "matchedPaperCorpusId": "2926851"
                        },
                        {
                            "start": 655,
                            "end": 658,
                            "matchedPaperCorpusId": "1399322"
                        },
                        {
                            "start": 990,
                            "end": 993,
                            "matchedPaperCorpusId": "1399322"
                        },
                        {
                            "start": 1250,
                            "end": 1253,
                            "matchedPaperCorpusId": "13756489"
                        },
                        {
                            "start": 1291,
                            "end": 1294,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 1294,
                            "end": 1297,
                            "matchedPaperCorpusId": "198953378"
                        },
                        {
                            "start": 1874,
                            "end": 1878,
                            "matchedPaperCorpusId": "207870430"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95947265625
                }
            ],
            "relevance_judgement": 0.95947265625,
            "relevance_judgment_input_expanded": "# Title: Suffix Retrieval-Augmented Language Modeling\n# Venue: IEEE International Conference on Acoustics, Speech, and Signal Processing\n# Authors: Zecheng Wang, Yik-Cheung Tam\n## Abstract\nCausal language modeling (LM) uses word history to predict the next word. BERT, on the other hand, makes use of bi-directional word information in a sentence to predict words at masked positions. While BERT is effective in sequence encoding, it is non-causal by nature and is not designed for sequence generation. In this paper, we propose a novel language model, SUffix REtrieval-Augmented LM (SUREALM), that simulates a bi-directional contextual effect in an autoregressive manner. SUREALM employs an embedding retriever to search for training sentences in a data store that share similar word history during sequence generation. In particular, the suffix portions of the retrieved sentences mimick the \"future\" context. We evalu-ated our proposed model on the DSTC9 spoken dialogue corpus and showed promising word perplexity reduction on the validation and test set compared to competitive baselines. Our source code is re-leased on GitHub 1.\n## RELATED WORK\nImproving language model using retrieval technique is not new. [4] employs document retrieval to retrieve relevant documents which are used to create an adaptive language model and interpolating it with the background statistical N-gram language model. [5] employs information retrieval to perform language model adaptation for statistical machine translation. Once the language models are adapted, they are kept fixed during sequence generation. \n\nMemory Networks (MemNNs [6]) are a family of neural networks integrating a memory component which can be updated during training. To avoid large storage of external memory, MemNNs can be trained end-to-end [7] by computing a compatibility score between a query and memory items , similar to an attention mechanism. SUREALM resembles MemNNs as we utilize an external data store as \"memory\". However, SUREALM's \"memory\" contains precomputed embeddings and is retrieved with an embedding retriever for computational efficiency. Like E2E MemNNs [7], we also distinguish between memory and query. We further construct our memory as prefix-suffix pairs enabling a more effective retrieval scheme by matching input query with prefixes. \n\nMost recent development in language modeling is based on transformers [8]. BERT-based Masked language modeling [9,10] exploits bi-directional information of a sentence to predict the word identity of the masked tokens. While BERT is effective in encoding sequences, it is not suitable for sequence generation due to its non-causal nature. Causal language modeling such as GPT2 [11] is uni-directional. Our proposed model attempts to retain the best of the two worlds as autoregressive and simulated bi-directional via augmentation of suffix embeddings during sequence generation. \n\nOne noticeable work for language modeling using embedding retrieval is nearest neighbor language model (KNN-LM) [12]. Their approach store dynamic information in an external knowledge base. During sequence generation, KNN-LM uses the current prefix to retrieve similar prefixes in the data store using embedding retrieval. The output probability distribution is estimated by looking at the corresponding next words in the retrieved prefixes. Such word probability distribution is linearly interpolated with the output word distribution from the causal transformer LM.",
            "reference_string": "[253384615 | Wang et al. | 2022 | Citations: 1]"
        },
        {
            "title": "Reliable, Adaptable, and Attributable Language Models with Retrieval",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 140,
            "citation_count": 61,
            "influential_citation_count": 6,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.03187, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "35584853",
                    "name": "Akari Asai"
                },
                {
                    "authorId": "49164966",
                    "name": "Zexuan Zhong"
                },
                {
                    "authorId": "50536468",
                    "name": "Danqi Chen"
                },
                {
                    "authorId": "2303396379",
                    "name": "Pang Wei Koh"
                },
                {
                    "authorId": "2137813791",
                    "name": "Luke S. Zettlemoyer"
                },
                {
                    "authorId": "2264251662",
                    "name": "Hanna Hajishirzi"
                },
                {
                    "authorId": "2072801764",
                    "name": "Wen-tau Yih"
                }
            ],
            "abstract": "Parametric language models (LMs), which are trained on vast amounts of web data, exhibit remarkable flexibility and capability. However, they still face practical challenges such as hallucinations, difficulty in adapting to new data distributions, and a lack of verifiability. In this position paper, we advocate for retrieval-augmented LMs to replace parametric LMs as the next generation of LMs. By incorporating large-scale datastores during inference, retrieval-augmented LMs can be more reliable, adaptable, and attributable. Despite their potential, retrieval-augmented LMs have yet to be widely adopted due to several obstacles: specifically, current retrieval-augmented LMs struggle to leverage helpful text beyond knowledge-intensive tasks such as question answering, have limited interaction between retrieval and LM components, and lack the infrastructure for scaling. To address these, we propose a roadmap for developing general-purpose retrieval-augmented LMs. This involves a reconsideration of datastores and retrievers, the exploration of pipelines with improved retriever-LM interaction, and significant investment in infrastructure for efficient training and inference.",
            "corpus_id": 268248911,
            "sentences": [
                {
                    "corpus_id": "268248911",
                    "title": "Reliable, Adaptable, and Attributable Language Models with Retrieval",
                    "text": "Parametric language models (LMs), which are trained on vast amounts of web data, exhibit remarkable flexibility and capability. However, they still face practical challenges such as hallucinations, difficulty in adapting to new data distributions, and a lack of verifiability. In this position paper, we advocate for retrieval-augmented LMs to replace parametric LMs as the next generation of LMs. By incorporating large-scale datastores during inference, retrieval-augmented LMs can be more reliable, adaptable, and attributable. Despite their potential, retrieval-augmented LMs have yet to be widely adopted due to several obstacles: specifically, current retrieval-augmented LMs struggle to leverage helpful text beyond knowledge-intensive tasks such as question answering, have limited interaction between retrieval and LM components, and lack the infrastructure for scaling. To address these, we propose a roadmap for developing general-purpose retrieval-augmented LMs. This involves a reconsideration of datastores and retrievers, the exploration of pipelines with improved retriever-LM interaction, and significant investment in infrastructure for efficient training and inference.",
                    "score": 0.4024982055363106,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95703125
                }
            ],
            "relevance_judgement": 0.95703125,
            "relevance_judgment_input_expanded": "# Title: Reliable, Adaptable, and Attributable Language Models with Retrieval\n# Venue: arXiv.org\n# Authors: Akari Asai, Zexuan Zhong, Danqi Chen, Pang Wei Koh, Luke S. Zettlemoyer, Hanna Hajishirzi, Wen-tau Yih\n## Abstract\nParametric language models (LMs), which are trained on vast amounts of web data, exhibit remarkable flexibility and capability. However, they still face practical challenges such as hallucinations, difficulty in adapting to new data distributions, and a lack of verifiability. In this position paper, we advocate for retrieval-augmented LMs to replace parametric LMs as the next generation of LMs. By incorporating large-scale datastores during inference, retrieval-augmented LMs can be more reliable, adaptable, and attributable. Despite their potential, retrieval-augmented LMs have yet to be widely adopted due to several obstacles: specifically, current retrieval-augmented LMs struggle to leverage helpful text beyond knowledge-intensive tasks such as question answering, have limited interaction between retrieval and LM components, and lack the infrastructure for scaling. To address these, we propose a roadmap for developing general-purpose retrieval-augmented LMs. This involves a reconsideration of datastores and retrievers, the exploration of pipelines with improved retriever-LM interaction, and significant investment in infrastructure for efficient training and inference.\n",
            "reference_string": "[268248911 | Asai et al. | 2024 | Citations: 61]"
        },
        {
            "title": "In-context Learning with Retrieved Demonstrations for Language Models: A Survey",
            "venue": "Trans. Mach. Learn. Res.",
            "year": 2024,
            "reference_count": 175,
            "citation_count": 70,
            "influential_citation_count": 6,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.11624, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2280145331",
                    "name": "an Luo"
                },
                {
                    "authorId": "2280140011",
                    "name": "Xin Xu"
                },
                {
                    "authorId": "2280285975",
                    "name": "Yue Liu"
                },
                {
                    "authorId": "2616463",
                    "name": "Panupong Pasupat"
                },
                {
                    "authorId": "2173102324",
                    "name": "Mehran Kazemi"
                }
            ],
            "abstract": "Language models, especially pre-trained large language models, have showcased remarkable abilities as few-shot in-context learners (ICL), adept at adapting to new tasks with just a few demonstrations in the input context. However, the model's ability to perform ICL is sensitive to the choice of the few-shot demonstrations. Instead of using a fixed set of demonstrations, one recent development is to retrieve demonstrations tailored to each input query. The implementation of demonstration retrieval is relatively straightforward, leveraging existing databases and retrieval systems. This not only improves the efficiency and scalability of the learning process but also has been shown to reduce biases inherent in manual example selection. In light of the encouraging results and growing research in ICL with retrieved demonstrations, we conduct an extensive review of studies in this area. In this survey, we discuss and compare different design choices for retrieval models, retrieval training procedures, and inference algorithms.",
            "corpus_id": 267069067,
            "sentences": [
                {
                    "corpus_id": "267069067",
                    "title": "In-context Learning with Retrieved Demonstrations for Language Models: A Survey",
                    "text": "These include the nature of the retriever-ranging from general off-the-shelf models to finely-tuned, domain-specific variants-the source and diversity of the retrieval corpus, the retriever's objectives (focusing on either similarity or diversity), and the strategies for integrating multiple demonstrations. Over the past two years, numerous and sometimes concurrent works have studied RetICL each with different terminology and with variations in problem definition and subsequent methodologies, making it difficult to comprehend the current state of research and practice in RetICL, especially for newcomers to the field. In this comprehensive survey, we meticulously analyze 22 seminal papers in the field of RetICL, as detailed in Table 1, and provide a categorization of their main building blocks (See Figure 1). Our work not only provides a thorough synthesis of existing research but also underscores the areas where RetICL significantly surpasses previous ICL methods, and illuminates many paths forward for future innovations in this area, thus serving as a critical resource for ICL. \n\n2 Few-shot In-context Learning for Language Models Language models (LMs) (Zhao et al., 2023;Rosenfeld, 2000) are probabilistic models that assign probabilities to sequences of words and are essential components in many tasks. Let s represent a sequence of words (e.g., a sentence) and w 1 , w 2 , . . . , w n represent the tokens in the sequence. Based on the chain rule, the probability p(s) can be decomposed into the following product of probabilities: where each element in the product corresponds to the probability of a token given the previous tokens. Based on the above decomposition, an LM can be constructed by learning the probability of the next token given the previous ones. \n\nEarlier LMs were mostly based on N-gram models which are based on the Markovian assumption that the next token only depends on the recent context (Jurafsky, 2021). Based on this assumption, p(w k | w 1 , . . .",
                    "score": 0.3908534054751986,
                    "section_title": "Introduction",
                    "char_start_offset": 2029,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 308
                        },
                        {
                            "start": 309,
                            "end": 624
                        },
                        {
                            "start": 625,
                            "end": 819
                        },
                        {
                            "start": 820,
                            "end": 1095
                        },
                        {
                            "start": 1098,
                            "end": 1323
                        },
                        {
                            "start": 1324,
                            "end": 1400
                        },
                        {
                            "start": 1401,
                            "end": 1444
                        },
                        {
                            "start": 1445,
                            "end": 1656
                        },
                        {
                            "start": 1657,
                            "end": 1786
                        },
                        {
                            "start": 1789,
                            "end": 1952
                        },
                        {
                            "start": 1953,
                            "end": 1998
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1190,
                            "end": 1206,
                            "matchedPaperCorpusId": "10959945"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9560546875
                }
            ],
            "relevance_judgement": 0.9560546875,
            "relevance_judgment_input_expanded": "# Title: In-context Learning with Retrieved Demonstrations for Language Models: A Survey\n# Venue: Trans. Mach. Learn. Res.\n# Authors: an Luo, Xin Xu, Yue Liu, Panupong Pasupat, Mehran Kazemi\n## Abstract\nLanguage models, especially pre-trained large language models, have showcased remarkable abilities as few-shot in-context learners (ICL), adept at adapting to new tasks with just a few demonstrations in the input context. However, the model's ability to perform ICL is sensitive to the choice of the few-shot demonstrations. Instead of using a fixed set of demonstrations, one recent development is to retrieve demonstrations tailored to each input query. The implementation of demonstration retrieval is relatively straightforward, leveraging existing databases and retrieval systems. This not only improves the efficiency and scalability of the learning process but also has been shown to reduce biases inherent in manual example selection. In light of the encouraging results and growing research in ICL with retrieved demonstrations, we conduct an extensive review of studies in this area. In this survey, we discuss and compare different design choices for retrieval models, retrieval training procedures, and inference algorithms.\n## Introduction\nThese include the nature of the retriever-ranging from general off-the-shelf models to finely-tuned, domain-specific variants-the source and diversity of the retrieval corpus, the retriever's objectives (focusing on either similarity or diversity), and the strategies for integrating multiple demonstrations. Over the past two years, numerous and sometimes concurrent works have studied RetICL each with different terminology and with variations in problem definition and subsequent methodologies, making it difficult to comprehend the current state of research and practice in RetICL, especially for newcomers to the field. In this comprehensive survey, we meticulously analyze 22 seminal papers in the field of RetICL, as detailed in Table 1, and provide a categorization of their main building blocks (See Figure 1). Our work not only provides a thorough synthesis of existing research but also underscores the areas where RetICL significantly surpasses previous ICL methods, and illuminates many paths forward for future innovations in this area, thus serving as a critical resource for ICL. \n\n2 Few-shot In-context Learning for Language Models Language models (LMs) (Zhao et al., 2023;Rosenfeld, 2000) are probabilistic models that assign probabilities to sequences of words and are essential components in many tasks. Let s represent a sequence of words (e.g., a sentence) and w 1 , w 2 , . . . , w n represent the tokens in the sequence. Based on the chain rule, the probability p(s) can be decomposed into the following product of probabilities: where each element in the product corresponds to the probability of a token given the previous tokens. Based on the above decomposition, an LM can be constructed by learning the probability of the next token given the previous ones. \n\nEarlier LMs were mostly based on N-gram models which are based on the Markovian assumption that the next token only depends on the recent context (Jurafsky, 2021). Based on this assumption, p(w k | w 1 , . . .",
            "reference_string": "[267069067 | Luo et al. | 2024 | Citations: 70]"
        },
        {
            "title": "RAVEN: In-Context Learning with Retrieval Augmented Encoder-Decoder Language Models",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 52,
            "citation_count": 35,
            "influential_citation_count": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2308.07922",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.07922, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1490651934",
                    "name": "Jie Huang"
                },
                {
                    "authorId": "2056440915",
                    "name": "Wei Ping"
                },
                {
                    "authorId": "145011005",
                    "name": "Peng Xu"
                },
                {
                    "authorId": "1911755",
                    "name": "M. Shoeybi"
                },
                {
                    "authorId": "143922493",
                    "name": "K. Chang"
                },
                {
                    "authorId": "2301680",
                    "name": "Bryan Catanzaro"
                }
            ],
            "abstract": "In this paper, we investigate the in-context learning ability of retrieval-augmented encoder-decoder language models. We first conduct a comprehensive analysis of existing models and identify their limitations in in-context learning, primarily due to a mismatch between pretraining and inference, as well as a restricted context length. To address these issues, we propose RAVEN, a model that combines retrieval-augmented masked language modeling and prefix language modeling. We further introduce Fusion-in-Context Learning to enhance the few-shot performance by enabling the model to leverage more in-context examples without requiring additional training. Through extensive experiments, we demonstrate that our simple yet effective design significantly improves performance, achieving results comparable to the most advanced language models in certain scenarios, despite having substantially fewer parameters. Our work underscores the potential of retrieval-augmented encoder-decoder language models for in-context learning and encourages further research in this direction.",
            "corpus_id": 260900354,
            "sentences": [
                {
                    "corpus_id": "260900354",
                    "title": "RAVEN: In-Context Learning with Retrieval Augmented Encoder-Decoder Language Models",
                    "text": "Retrieval-augmented language models are a class of language models designed to enhance their performance by incorporating external knowledge. These models typically employ an information retrieval mechanism to access relevant information from a large corpus, which is then integrated into the model's prediction process. Retrieval-augmented LMs can be based on both encoder-decoder (Izacard et al., 2023;Lewis et al., 2020) and decoderonly (Khandelwal et al., 2020;Borgeaud et al., 2022;Shi et al., 2022) architectures. For decoder-only LMs, the computational cost typically increases quadratically with the input length, as well as with the number of retrieval passages. In contrast, for encoder-decoder LMs with a Fusion-in-Decoder architecture, the computation cost grows linearly with the number of retrieved passages, as they only perform self-attention over one passage at a time (Izacard & Grave, 2021). This concept is also investigated by Ye et al. (2023) for more efficient in-context learning. \n\nWhile there has been some research on in-context learning with retrieval-augmented decoder-only LMs, which can be straightforwardly implemented by concatenating retrieved passages with the query as the input of the LM (Mallen et al., 2022;Shi et al., 2023;Khattab et al., 2022), in-context learning with retrieval-augmented encoder-decoder LMs remains unexplored to the best of our knowledge. This is despite the fact that encoder-decoder LMs can be more efficient at incorporating multiple (e.g., 40) retrieved passages.",
                    "score": 0.43869176305932234,
                    "section_title": "Background and Related Work",
                    "char_start_offset": 4627,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 141
                        },
                        {
                            "start": 142,
                            "end": 320
                        },
                        {
                            "start": 321,
                            "end": 519
                        },
                        {
                            "start": 520,
                            "end": 671
                        },
                        {
                            "start": 672,
                            "end": 910
                        },
                        {
                            "start": 911,
                            "end": 1004
                        },
                        {
                            "start": 1007,
                            "end": 1399
                        },
                        {
                            "start": 1400,
                            "end": 1528
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 382,
                            "end": 404,
                            "matchedPaperCorpusId": "251371732"
                        },
                        {
                            "start": 404,
                            "end": 423,
                            "matchedPaperCorpusId": "218869575"
                        },
                        {
                            "start": 440,
                            "end": 465,
                            "matchedPaperCorpusId": "207870430"
                        },
                        {
                            "start": 487,
                            "end": 504,
                            "matchedPaperCorpusId": "249152130"
                        },
                        {
                            "start": 886,
                            "end": 909,
                            "matchedPaperCorpusId": "220302360"
                        },
                        {
                            "start": 948,
                            "end": 964,
                            "matchedPaperCorpusId": "259370780"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95361328125
                }
            ],
            "relevance_judgement": 0.95361328125,
            "relevance_judgment_input_expanded": "# Title: RAVEN: In-Context Learning with Retrieval Augmented Encoder-Decoder Language Models\n# Venue: arXiv.org\n# Authors: Jie Huang, Wei Ping, Peng Xu, M. Shoeybi, K. Chang, Bryan Catanzaro\n## Abstract\nIn this paper, we investigate the in-context learning ability of retrieval-augmented encoder-decoder language models. We first conduct a comprehensive analysis of existing models and identify their limitations in in-context learning, primarily due to a mismatch between pretraining and inference, as well as a restricted context length. To address these issues, we propose RAVEN, a model that combines retrieval-augmented masked language modeling and prefix language modeling. We further introduce Fusion-in-Context Learning to enhance the few-shot performance by enabling the model to leverage more in-context examples without requiring additional training. Through extensive experiments, we demonstrate that our simple yet effective design significantly improves performance, achieving results comparable to the most advanced language models in certain scenarios, despite having substantially fewer parameters. Our work underscores the potential of retrieval-augmented encoder-decoder language models for in-context learning and encourages further research in this direction.\n## Background and Related Work\nRetrieval-augmented language models are a class of language models designed to enhance their performance by incorporating external knowledge. These models typically employ an information retrieval mechanism to access relevant information from a large corpus, which is then integrated into the model's prediction process. Retrieval-augmented LMs can be based on both encoder-decoder (Izacard et al., 2023;Lewis et al., 2020) and decoderonly (Khandelwal et al., 2020;Borgeaud et al., 2022;Shi et al., 2022) architectures. For decoder-only LMs, the computational cost typically increases quadratically with the input length, as well as with the number of retrieval passages. In contrast, for encoder-decoder LMs with a Fusion-in-Decoder architecture, the computation cost grows linearly with the number of retrieved passages, as they only perform self-attention over one passage at a time (Izacard & Grave, 2021). This concept is also investigated by Ye et al. (2023) for more efficient in-context learning. \n\nWhile there has been some research on in-context learning with retrieval-augmented decoder-only LMs, which can be straightforwardly implemented by concatenating retrieved passages with the query as the input of the LM (Mallen et al., 2022;Shi et al., 2023;Khattab et al., 2022), in-context learning with retrieval-augmented encoder-decoder LMs remains unexplored to the best of our knowledge. This is despite the fact that encoder-decoder LMs can be more efficient at incorporating multiple (e.g., 40) retrieved passages.",
            "reference_string": "[260900354 | Huang et al. | 2023 | Citations: 35]"
        },
        {
            "title": "Transfer Learning Approaches for Building Cross-Language Dense Retrieval Models",
            "venue": "European Conference on Information Retrieval",
            "year": 2022,
            "reference_count": 38,
            "citation_count": 46,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2201.08471, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "143913642",
                    "name": "Suraj Nair"
                },
                {
                    "authorId": "153564640",
                    "name": "Eugene Yang"
                },
                {
                    "authorId": "2539674",
                    "name": "Dawn J Lawrie"
                },
                {
                    "authorId": "1800354",
                    "name": "Kevin Duh"
                },
                {
                    "authorId": "145324163",
                    "name": "Paul McNamee"
                },
                {
                    "authorId": "38730896",
                    "name": "Kenton Murray"
                },
                {
                    "authorId": "145510552",
                    "name": "J. Mayfield"
                },
                {
                    "authorId": "1737250",
                    "name": "Douglas W. Oard"
                }
            ],
            "abstract": "The advent of transformer-based models such as BERT has led to the rise of neural ranking models. These models have improved the effectiveness of retrieval systems well beyond that of lexical term matching models such as BM25. While monolingual retrieval tasks have benefited from large-scale training collections such as MS MARCO and advances in neural architectures, cross-language retrieval tasks have fallen behind these advancements. This paper introduces ColBERT-X, a generalization of the ColBERT multi-representation dense retrieval model that uses the XLM-RoBERTa (XLM-R) encoder to support cross-language information retrieval (CLIR). ColBERT-X can be trained in two ways. In zero-shot training, the system is trained on the English MS MARCO collection, relying on the XLM-R encoder for cross-language mappings. In translate-train, the system is trained on the MS MARCO English queries coupled with machine translations of the associated MS MARCO passages. Results on ad hoc document ranking tasks in several languages demonstrate substantial and statistically significant improvements of these trained dense retrieval models over traditional lexical CLIR baselines.",
            "corpus_id": 246210468,
            "sentences": [
                {
                    "corpus_id": "246210468",
                    "title": "Transfer Learning Approaches for Building Cross-Language Dense Retrieval Models",
                    "text": "In this section, we briefly review related work on neural retrieval and its extension to cross-lingual settings. For many years, sparse retrieval models such as BM25 [26] and Query Likelihood [24] were the dominant models for ad hoc retrieval tasks. Only in recent years, with the rise of BERT [7] and the availability of large scale retrieval collections such as MSMARCO [2] for training, have neural information retrieval (neural IR) models emerged as the state of the art. Similar to sparse retrieval models, neural IR models take as input the query and documents, and produce a relevance score. For each query and document pair, matching mechanisms, such as DRMM [9], KNRM [6] or PACCR [10], construct the interaction matrix between the distributed term representations of the query and the documents, and aggregate them into a relevance score. Alternatively, the BERT passage pair classification model [7] considers the query and the document as the input pair, and uses the final classification score as the relevance score [35]. CEDR [18] incorporates contextualized embeddings such as ELMo [23] or BERT [7] into the matching, providing significant effectiveness improvements by taking advantage of contextualization. However, due to the high computational cost, these models are used to rerank top-ranked documents from a sparse retrieval system. \n\nColBERT [13] further improves efficiency by keeping separate the querydocument interaction until the end of the neural architecture. This is called late interaction. As opposed to matching mechanisms that require both the query and the document to be present simultaneously, late interaction allows offline encoding of the documents into bags-of-vectors. Document representations are combined with query representations by an efficient MaxSim operator, which significantly reduces computation at inference time. This decoupling enables the documents to be encoded offline and indexed to support approximate nearest neighbor search. Further details are discussed in Section 3. \n\nCross-language transfer learning is important for CLIR. Due to the lack of training data for ad hoc neural retrieval models other than in English, prior work explored zero-shot model transfer to other languages, trained with only English retrieval examples [17,28].",
                    "score": 0.3908534054751986,
                    "section_title": "Related Work",
                    "char_start_offset": 3238,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 112
                        },
                        {
                            "start": 113,
                            "end": 249
                        },
                        {
                            "start": 250,
                            "end": 475
                        },
                        {
                            "start": 476,
                            "end": 598
                        },
                        {
                            "start": 599,
                            "end": 848
                        },
                        {
                            "start": 849,
                            "end": 1035
                        },
                        {
                            "start": 1036,
                            "end": 1224
                        },
                        {
                            "start": 1225,
                            "end": 1354
                        },
                        {
                            "start": 1357,
                            "end": 1489
                        },
                        {
                            "start": 1490,
                            "end": 1522
                        },
                        {
                            "start": 1523,
                            "end": 1711
                        },
                        {
                            "start": 1712,
                            "end": 1868
                        },
                        {
                            "start": 1869,
                            "end": 1988
                        },
                        {
                            "start": 1989,
                            "end": 2032
                        },
                        {
                            "start": 2035,
                            "end": 2090
                        },
                        {
                            "start": 2091,
                            "end": 2300
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 166,
                            "end": 170,
                            "matchedPaperCorpusId": "41563977"
                        },
                        {
                            "start": 192,
                            "end": 196,
                            "matchedPaperCorpusId": "2759772"
                        },
                        {
                            "start": 294,
                            "end": 297,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 667,
                            "end": 670,
                            "matchedPaperCorpusId": "5688521"
                        },
                        {
                            "start": 677,
                            "end": 680,
                            "matchedPaperCorpusId": "33169397"
                        },
                        {
                            "start": 690,
                            "end": 694,
                            "matchedPaperCorpusId": "6246996"
                        },
                        {
                            "start": 907,
                            "end": 910,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 1041,
                            "end": 1045,
                            "matchedPaperCorpusId": "131776819"
                        },
                        {
                            "start": 1098,
                            "end": 1102,
                            "matchedPaperCorpusId": "3626819"
                        },
                        {
                            "start": 1111,
                            "end": 1114,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 1365,
                            "end": 1369,
                            "matchedPaperCorpusId": "216553223"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95361328125
                }
            ],
            "relevance_judgement": 0.95361328125,
            "relevance_judgment_input_expanded": "# Title: Transfer Learning Approaches for Building Cross-Language Dense Retrieval Models\n# Venue: European Conference on Information Retrieval\n# Authors: Suraj Nair, Eugene Yang, Dawn J Lawrie, Kevin Duh, Paul McNamee, Kenton Murray, J. Mayfield, Douglas W. Oard\n## Abstract\nThe advent of transformer-based models such as BERT has led to the rise of neural ranking models. These models have improved the effectiveness of retrieval systems well beyond that of lexical term matching models such as BM25. While monolingual retrieval tasks have benefited from large-scale training collections such as MS MARCO and advances in neural architectures, cross-language retrieval tasks have fallen behind these advancements. This paper introduces ColBERT-X, a generalization of the ColBERT multi-representation dense retrieval model that uses the XLM-RoBERTa (XLM-R) encoder to support cross-language information retrieval (CLIR). ColBERT-X can be trained in two ways. In zero-shot training, the system is trained on the English MS MARCO collection, relying on the XLM-R encoder for cross-language mappings. In translate-train, the system is trained on the MS MARCO English queries coupled with machine translations of the associated MS MARCO passages. Results on ad hoc document ranking tasks in several languages demonstrate substantial and statistically significant improvements of these trained dense retrieval models over traditional lexical CLIR baselines.\n## Related Work\nIn this section, we briefly review related work on neural retrieval and its extension to cross-lingual settings. For many years, sparse retrieval models such as BM25 [26] and Query Likelihood [24] were the dominant models for ad hoc retrieval tasks. Only in recent years, with the rise of BERT [7] and the availability of large scale retrieval collections such as MSMARCO [2] for training, have neural information retrieval (neural IR) models emerged as the state of the art. Similar to sparse retrieval models, neural IR models take as input the query and documents, and produce a relevance score. For each query and document pair, matching mechanisms, such as DRMM [9], KNRM [6] or PACCR [10], construct the interaction matrix between the distributed term representations of the query and the documents, and aggregate them into a relevance score. Alternatively, the BERT passage pair classification model [7] considers the query and the document as the input pair, and uses the final classification score as the relevance score [35]. CEDR [18] incorporates contextualized embeddings such as ELMo [23] or BERT [7] into the matching, providing significant effectiveness improvements by taking advantage of contextualization. However, due to the high computational cost, these models are used to rerank top-ranked documents from a sparse retrieval system. \n\nColBERT [13] further improves efficiency by keeping separate the querydocument interaction until the end of the neural architecture. This is called late interaction. As opposed to matching mechanisms that require both the query and the document to be present simultaneously, late interaction allows offline encoding of the documents into bags-of-vectors. Document representations are combined with query representations by an efficient MaxSim operator, which significantly reduces computation at inference time. This decoupling enables the documents to be encoded offline and indexed to support approximate nearest neighbor search. Further details are discussed in Section 3. \n\nCross-language transfer learning is important for CLIR. Due to the lack of training data for ad hoc neural retrieval models other than in English, prior work explored zero-shot model transfer to other languages, trained with only English retrieval examples [17,28].",
            "reference_string": "[246210468 | Nair et al. | 2022 | Citations: 46]"
        },
        {
            "title": "On the Equivalence of Generative and Discriminative Formulations of the Sequential Dependence Model",
            "venue": "arXiv.org",
            "year": 2018,
            "reference_count": 16,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1805.00152, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "145798497",
                    "name": "Laura Dietz"
                },
                {
                    "authorId": "144078659",
                    "name": "John Foley"
                }
            ],
            "abstract": "The sequential dependence model (SDM) is a popular retrieval model which is based on the theory of probabilistic graphical models. While it was originally introduced by Metzler and Croft as a Markov Random Field (aka discriminative probabilistic model), in this paper we demonstrate that it is equivalent to a generative probabilistic model. \nTo build an foundation for future retrieval models, this paper details the axiomatic underpinning of the SDM model as discriminative and generative probabilistic model. The only difference arises whether model parameters are estimated in log-space or Multinomial-space. We demonstrate that parameter-estimation with grid-tuning is negatively impacting the generative formulation, an effect that vanishes when parameters are estimated with coordinate-gradient descent. This is concerning, since empirical differences may be falsely attributed to improved models.",
            "corpus_id": 21807753,
            "sentences": [
                {
                    "corpus_id": "21807753",
                    "title": "On the Equivalence of Generative and Discriminative Formulations of the Sequential Dependence Model",
                    "text": "The sequential dependence model [11] is a very robust retrieval model that has been shown to outperform or to be on par with many retrieval models [8]. Its robustness comes from an integration of unigram, bigram, and windowed bigram models through the theoretical framework of Markov random fields. The SDM Markov random field is associated with a set of parameters which are learned through the usual parameter estimation techniques for undirected graphical models with training data. Despite its simplicity, the SDM model is a versatile method that provides a reasonable input ranking for further learning-to-rank phases or in as a building block in a larger model [6]. As it is a feature-based learning-to-rank model, it can be extended with additional features, such as in the latent concept model [2,12]. Like all Markov random field models it can be extended with further variables, for instance to incorporate external knowledge, such as entities from an external semantic network. It can also be extended with additional conditional dependencies, such as further term dependencies that are expected to be helpful for the retrieval task, such as in the hypergraph retrieval model [1]. \n\nThe essential idea of the sequential dependence model (SDM) is to combine unigram, bigram, and windowed bigram models so that they mutually compensate each other's shortcomings. The unigram gram model, which is also called the bag-of-words model and which is closely related to the vector-space model, is indifferent to word order. This is an issue for multi-word expressions which are for instance common for entity names such as \"Massachusetts Institute of Technology\" or compound nouns such as \"information retrieval\" which have a different meaning in combination than individually. This shortcoming is compensated for in bigram model which incorporate word-order by modeling the probability of joint occurrence of two subsequent query words q i\u22121 q i or condition the probability of ith word in the query, q i , on seeing the previous word q i\u22121 .",
                    "score": 0.3983111096393575,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 151
                        },
                        {
                            "start": 152,
                            "end": 298
                        },
                        {
                            "start": 299,
                            "end": 485
                        },
                        {
                            "start": 486,
                            "end": 671
                        },
                        {
                            "start": 672,
                            "end": 809
                        },
                        {
                            "start": 810,
                            "end": 988
                        },
                        {
                            "start": 989,
                            "end": 1191
                        },
                        {
                            "start": 1194,
                            "end": 1371
                        },
                        {
                            "start": 1372,
                            "end": 1525
                        },
                        {
                            "start": 1526,
                            "end": 1779
                        },
                        {
                            "start": 1780,
                            "end": 2045
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 32,
                            "end": 36,
                            "matchedPaperCorpusId": "1118305"
                        },
                        {
                            "start": 147,
                            "end": 150,
                            "matchedPaperCorpusId": "3154405"
                        },
                        {
                            "start": 667,
                            "end": 670,
                            "matchedPaperCorpusId": "9175104"
                        },
                        {
                            "start": 802,
                            "end": 805,
                            "matchedPaperCorpusId": "207184945"
                        },
                        {
                            "start": 805,
                            "end": 808,
                            "matchedPaperCorpusId": "14811099"
                        },
                        {
                            "start": 1187,
                            "end": 1190,
                            "matchedPaperCorpusId": "10501047"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.953125
                }
            ],
            "relevance_judgement": 0.953125,
            "relevance_judgment_input_expanded": "# Title: On the Equivalence of Generative and Discriminative Formulations of the Sequential Dependence Model\n# Venue: arXiv.org\n# Authors: Laura Dietz, John Foley\n## Abstract\nThe sequential dependence model (SDM) is a popular retrieval model which is based on the theory of probabilistic graphical models. While it was originally introduced by Metzler and Croft as a Markov Random Field (aka discriminative probabilistic model), in this paper we demonstrate that it is equivalent to a generative probabilistic model. \nTo build an foundation for future retrieval models, this paper details the axiomatic underpinning of the SDM model as discriminative and generative probabilistic model. The only difference arises whether model parameters are estimated in log-space or Multinomial-space. We demonstrate that parameter-estimation with grid-tuning is negatively impacting the generative formulation, an effect that vanishes when parameters are estimated with coordinate-gradient descent. This is concerning, since empirical differences may be falsely attributed to improved models.\n## Introduction\nThe sequential dependence model [11] is a very robust retrieval model that has been shown to outperform or to be on par with many retrieval models [8]. Its robustness comes from an integration of unigram, bigram, and windowed bigram models through the theoretical framework of Markov random fields. The SDM Markov random field is associated with a set of parameters which are learned through the usual parameter estimation techniques for undirected graphical models with training data. Despite its simplicity, the SDM model is a versatile method that provides a reasonable input ranking for further learning-to-rank phases or in as a building block in a larger model [6]. As it is a feature-based learning-to-rank model, it can be extended with additional features, such as in the latent concept model [2,12]. Like all Markov random field models it can be extended with further variables, for instance to incorporate external knowledge, such as entities from an external semantic network. It can also be extended with additional conditional dependencies, such as further term dependencies that are expected to be helpful for the retrieval task, such as in the hypergraph retrieval model [1]. \n\nThe essential idea of the sequential dependence model (SDM) is to combine unigram, bigram, and windowed bigram models so that they mutually compensate each other's shortcomings. The unigram gram model, which is also called the bag-of-words model and which is closely related to the vector-space model, is indifferent to word order. This is an issue for multi-word expressions which are for instance common for entity names such as \"Massachusetts Institute of Technology\" or compound nouns such as \"information retrieval\" which have a different meaning in combination than individually. This shortcoming is compensated for in bigram model which incorporate word-order by modeling the probability of joint occurrence of two subsequent query words q i\u22121 q i or condition the probability of ith word in the query, q i , on seeing the previous word q i\u22121 .",
            "reference_string": "[21807753 | Dietz et al. | 2018 | Citations: 1]"
        },
        {
            "title": "Legal Element-oriented Modeling with Multi-view Contrastive Learning for Legal Case Retrieval",
            "venue": "IEEE International Joint Conference on Neural Network",
            "year": 2022,
            "reference_count": 38,
            "citation_count": 12,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2210.05188",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2210.05188, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2144715839",
                    "name": "Zhaowei Wang"
                }
            ],
            "abstract": "Legal case retrieval, which aims to retrieve relevant cases given a query case, plays an essential role in the legal system. While recent research efforts improve the performance of traditional ad-hoc retrieval models, legal case retrieval is still challenging since queries are legal cases, which contain hundreds of tokens. Legal cases are much longer and more complicated than keywords queries. Apart from that, the definition of legal relevance is beyond the general definition. In addition to general topical relevance, the relevant cases also involve similar situations and legal elements, which can support the judgment of the current case. In this paper, we propose an interaction-focused network for legal case retrieval with a multi-view contrastive learning objective. The contrastive learning views, including case-view and element-view, aim to overcome the above challenges. The case-view contrastive learning minimizes the hidden space distance between relevant legal case representations produced by a pre-trained language model (PLM) encoder. The element-view builds positive and negative instances by changing legal elements of cases to help the network better compute legal relevance. To achieve this, we employ a legal element knowledge-aware indicator to detect legal elements of cases. We conduct extensive experiments on the benchmark of relevant case retrieval. Evaluation results indicate our proposed method obtains significant improvement over the existing methods.",
            "corpus_id": 252781830,
            "sentences": [
                {
                    "corpus_id": "252781830",
                    "title": "Legal Element-oriented Modeling with Multi-view Contrastive Learning for Legal Case Retrieval",
                    "text": "To verify the effectiveness of MVCL, we compare our model with four types of baselines: traditional bag-of-words retrieval models, common deep retrieval models, hierarchical models for long text matching, models built for legal case retrieval. \n\nTraditional bag-of-words retrieval models include: \n\n\u2022 TF-IDF: TF-IDF simply compares terms between query and candidates and also introduces Inverse Document Frequency, which punishes terms that are common in all documents. \u2022 BM25: BM25 is another exact matching-based and highly effective method. Common deep retrieval models include: \n\n\u2022 MatchPLM: MatchPLM [12] uses a PLM to encode both queries and candidates. A dense layer is used to compute similarities between representations of queries and candidates. \u2022 MatchPyramid: MatchPyramid [17] is an interactionfocused model, which utilizes a CNN layer to capture matching patterns on a word-level similarity matrix. \u2022 DRMM: DRMM [8] also builds a word-level similarity matrix and uses a histogram mapping function. All histograms are transformed by a feed forward network and selected by a gating network to produce the final representation. Hierarchical models for long text matching include: \n\n\u2022 SMASH RNN: SMASH RNN [11] adopted attentive RNN components in a hierarchical architecture to model multiple abstraction levels, including words, sentences, and paragraphs. \n\n\u2022 HAN: HAN [31] builds a hierarchical network based on GRU and uses attention-pooling to aggregate word and sentence representations. \u2022 SMITH: SMITH [12] is another hierarchical model that adopts transformers to compute representations in word and sentence levels. Like MatchBERT, we employ a dense layer as a similarity function for SMASH RNN, HAN, and SMITH. Models built for legal case retrieval include: \n\n\u2022 Roformer: Roformer [35] is a transformer-based model, which incorporates explicit relative position and is able to model long sequences. \u2022 BERT-PLI: BERT-PLI [6] breaks queries and candidates into paragraphs and computes the relevance between paragraphs. It aggregates paragraph relevance with a RNN layer. BERT-PLI is a strong baseline, which achieves the state-of-the-art on COLIEE 2018's dataset.",
                    "score": 0.40464325626181297,
                    "section_title": "B. Baselines",
                    "char_start_offset": 22951,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 243
                        },
                        {
                            "start": 246,
                            "end": 296
                        },
                        {
                            "start": 299,
                            "end": 469
                        },
                        {
                            "start": 470,
                            "end": 543
                        },
                        {
                            "start": 544,
                            "end": 581
                        },
                        {
                            "start": 584,
                            "end": 659
                        },
                        {
                            "start": 660,
                            "end": 756
                        },
                        {
                            "start": 757,
                            "end": 913
                        },
                        {
                            "start": 914,
                            "end": 1012
                        },
                        {
                            "start": 1013,
                            "end": 1139
                        },
                        {
                            "start": 1140,
                            "end": 1191
                        },
                        {
                            "start": 1194,
                            "end": 1367
                        },
                        {
                            "start": 1370,
                            "end": 1503
                        },
                        {
                            "start": 1504,
                            "end": 1634
                        },
                        {
                            "start": 1635,
                            "end": 1730
                        },
                        {
                            "start": 1731,
                            "end": 1777
                        },
                        {
                            "start": 1780,
                            "end": 1918
                        },
                        {
                            "start": 1919,
                            "end": 2036
                        },
                        {
                            "start": 2037,
                            "end": 2088
                        },
                        {
                            "start": 2089,
                            "end": 2181
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 605,
                            "end": 609,
                            "matchedPaperCorpusId": "222327807"
                        },
                        {
                            "start": 786,
                            "end": 790,
                            "matchedPaperCorpusId": "3993933"
                        },
                        {
                            "start": 927,
                            "end": 930,
                            "matchedPaperCorpusId": "5688521"
                        },
                        {
                            "start": 1217,
                            "end": 1221,
                            "matchedPaperCorpusId": "86604509"
                        },
                        {
                            "start": 1381,
                            "end": 1385,
                            "matchedPaperCorpusId": "6857205"
                        },
                        {
                            "start": 1519,
                            "end": 1523,
                            "matchedPaperCorpusId": "222327807"
                        },
                        {
                            "start": 1940,
                            "end": 1943,
                            "matchedPaperCorpusId": "267909336"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94677734375
                }
            ],
            "relevance_judgement": 0.94677734375,
            "relevance_judgment_input_expanded": "# Title: Legal Element-oriented Modeling with Multi-view Contrastive Learning for Legal Case Retrieval\n# Venue: IEEE International Joint Conference on Neural Network\n# Authors: Zhaowei Wang\n## Abstract\nLegal case retrieval, which aims to retrieve relevant cases given a query case, plays an essential role in the legal system. While recent research efforts improve the performance of traditional ad-hoc retrieval models, legal case retrieval is still challenging since queries are legal cases, which contain hundreds of tokens. Legal cases are much longer and more complicated than keywords queries. Apart from that, the definition of legal relevance is beyond the general definition. In addition to general topical relevance, the relevant cases also involve similar situations and legal elements, which can support the judgment of the current case. In this paper, we propose an interaction-focused network for legal case retrieval with a multi-view contrastive learning objective. The contrastive learning views, including case-view and element-view, aim to overcome the above challenges. The case-view contrastive learning minimizes the hidden space distance between relevant legal case representations produced by a pre-trained language model (PLM) encoder. The element-view builds positive and negative instances by changing legal elements of cases to help the network better compute legal relevance. To achieve this, we employ a legal element knowledge-aware indicator to detect legal elements of cases. We conduct extensive experiments on the benchmark of relevant case retrieval. Evaluation results indicate our proposed method obtains significant improvement over the existing methods.\n## B. Baselines\nTo verify the effectiveness of MVCL, we compare our model with four types of baselines: traditional bag-of-words retrieval models, common deep retrieval models, hierarchical models for long text matching, models built for legal case retrieval. \n\nTraditional bag-of-words retrieval models include: \n\n\u2022 TF-IDF: TF-IDF simply compares terms between query and candidates and also introduces Inverse Document Frequency, which punishes terms that are common in all documents. \u2022 BM25: BM25 is another exact matching-based and highly effective method. Common deep retrieval models include: \n\n\u2022 MatchPLM: MatchPLM [12] uses a PLM to encode both queries and candidates. A dense layer is used to compute similarities between representations of queries and candidates. \u2022 MatchPyramid: MatchPyramid [17] is an interactionfocused model, which utilizes a CNN layer to capture matching patterns on a word-level similarity matrix. \u2022 DRMM: DRMM [8] also builds a word-level similarity matrix and uses a histogram mapping function. All histograms are transformed by a feed forward network and selected by a gating network to produce the final representation. Hierarchical models for long text matching include: \n\n\u2022 SMASH RNN: SMASH RNN [11] adopted attentive RNN components in a hierarchical architecture to model multiple abstraction levels, including words, sentences, and paragraphs. \n\n\u2022 HAN: HAN [31] builds a hierarchical network based on GRU and uses attention-pooling to aggregate word and sentence representations. \u2022 SMITH: SMITH [12] is another hierarchical model that adopts transformers to compute representations in word and sentence levels. Like MatchBERT, we employ a dense layer as a similarity function for SMASH RNN, HAN, and SMITH. Models built for legal case retrieval include: \n\n\u2022 Roformer: Roformer [35] is a transformer-based model, which incorporates explicit relative position and is able to model long sequences. \u2022 BERT-PLI: BERT-PLI [6] breaks queries and candidates into paragraphs and computes the relevance between paragraphs. It aggregates paragraph relevance with a RNN layer. BERT-PLI is a strong baseline, which achieves the state-of-the-art on COLIEE 2018's dataset.",
            "reference_string": "[252781830 | Wang | 2022 | Citations: 12]"
        },
        {
            "title": "Adapt in Contexts: Retrieval-Augmented Domain Adaptation via In-Context Learning",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2023,
            "reference_count": 46,
            "citation_count": 14,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.11551, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1500525090",
                    "name": "Quanyu Long"
                },
                {
                    "authorId": "2108498087",
                    "name": "Wenya Wang"
                },
                {
                    "authorId": "2249722081",
                    "name": "Sinno Jialin Pan"
                }
            ],
            "abstract": "Large language models (LLMs) have showcased their capability with few-shot inference known as in-context learning. However, in-domain demonstrations are not always readily available in real scenarios, leading to cross-domain in-context learning. Besides, LLMs are still facing challenges in long-tail knowledge in unseen and unfamiliar domains. The above limitations demonstrate the necessity of Unsupervised Domain Adaptation (UDA). In this paper, we study the UDA problem under an in-context learning setting to adapt language models from the source domain to the target domain without any target labels. The core idea is to retrieve a subset of cross-domain elements that are the most similar to the query, and elicit language model to adapt in an in-context manner by learning both target domain distribution and the discriminative task signal simultaneously with the augmented cross-domain in-context examples. We devise different prompting and training strategies, accounting for different LM architectures to learn the target distribution via language modeling. With extensive experiments on Sentiment Analysis (SA) and Named Entity Recognition (NER) tasks, we thoroughly study the effectiveness of ICL for domain transfer and demonstrate significant improvements over baseline models.",
            "corpus_id": 265294654,
            "sentences": [
                {
                    "corpus_id": "265294654",
                    "title": "Adapt in Contexts: Retrieval-Augmented Domain Adaptation via In-Context Learning",
                    "text": "Given an input sentence from the source domain, we first search for semantically similar examples from the unlabeled target domain. This is analogous to retrieval and re-rank given a search query. Retrieval-augmented LM approaches (Guu et al., 2020;Lewis et al., 2020;Asai et al., 2023) apply a parametrized dense retriever to train with the task model. In this paper, we fix the retriever part and use the off-the-shelf scoring language models. For the SA task, we use SimCSE (Gao et al., 2021) which produces semantically meaningful sentence embeddings after being trained with contrastive learning (Chen et al., 2020;He et al., 2020). Here cosine similarity is used to retrieve top-ranked (most similar) examples from the target domain. For NER, we use BERTScore (Zhang et al., 2020;Wang et al., 2021), because it gives a metric for each sentence based on the similarity of token representation, which is more crucial for the task of NER. Specifically, given a source sentence x S paired with label y, we retrieve top-k relevant chunks of texts from the target unlabeled dataset D T . \n\nThe retrieved examples are denoted as \n\nk } which will serve as the contexts to enrich the semantics for the source input.",
                    "score": 0.3908534054751986,
                    "section_title": "Context Construction with Retrieval",
                    "char_start_offset": 7943,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 131
                        },
                        {
                            "start": 132,
                            "end": 196
                        },
                        {
                            "start": 197,
                            "end": 353
                        },
                        {
                            "start": 354,
                            "end": 445
                        },
                        {
                            "start": 446,
                            "end": 637
                        },
                        {
                            "start": 638,
                            "end": 739
                        },
                        {
                            "start": 740,
                            "end": 941
                        },
                        {
                            "start": 942,
                            "end": 1087
                        },
                        {
                            "start": 1090,
                            "end": 1127
                        },
                        {
                            "start": 1130,
                            "end": 1212
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 231,
                            "end": 249,
                            "matchedPaperCorpusId": "211204736"
                        },
                        {
                            "start": 249,
                            "end": 268,
                            "matchedPaperCorpusId": "218869575"
                        },
                        {
                            "start": 477,
                            "end": 495,
                            "matchedPaperCorpusId": "233296292"
                        },
                        {
                            "start": 601,
                            "end": 620,
                            "matchedPaperCorpusId": "211096730"
                        },
                        {
                            "start": 620,
                            "end": 636,
                            "matchedPaperCorpusId": "207930212"
                        },
                        {
                            "start": 786,
                            "end": 804,
                            "matchedPaperCorpusId": "235458009"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94677734375
                }
            ],
            "relevance_judgement": 0.94677734375,
            "relevance_judgment_input_expanded": "# Title: Adapt in Contexts: Retrieval-Augmented Domain Adaptation via In-Context Learning\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Quanyu Long, Wenya Wang, Sinno Jialin Pan\n## Abstract\nLarge language models (LLMs) have showcased their capability with few-shot inference known as in-context learning. However, in-domain demonstrations are not always readily available in real scenarios, leading to cross-domain in-context learning. Besides, LLMs are still facing challenges in long-tail knowledge in unseen and unfamiliar domains. The above limitations demonstrate the necessity of Unsupervised Domain Adaptation (UDA). In this paper, we study the UDA problem under an in-context learning setting to adapt language models from the source domain to the target domain without any target labels. The core idea is to retrieve a subset of cross-domain elements that are the most similar to the query, and elicit language model to adapt in an in-context manner by learning both target domain distribution and the discriminative task signal simultaneously with the augmented cross-domain in-context examples. We devise different prompting and training strategies, accounting for different LM architectures to learn the target distribution via language modeling. With extensive experiments on Sentiment Analysis (SA) and Named Entity Recognition (NER) tasks, we thoroughly study the effectiveness of ICL for domain transfer and demonstrate significant improvements over baseline models.\n## Context Construction with Retrieval\nGiven an input sentence from the source domain, we first search for semantically similar examples from the unlabeled target domain. This is analogous to retrieval and re-rank given a search query. Retrieval-augmented LM approaches (Guu et al., 2020;Lewis et al., 2020;Asai et al., 2023) apply a parametrized dense retriever to train with the task model. In this paper, we fix the retriever part and use the off-the-shelf scoring language models. For the SA task, we use SimCSE (Gao et al., 2021) which produces semantically meaningful sentence embeddings after being trained with contrastive learning (Chen et al., 2020;He et al., 2020). Here cosine similarity is used to retrieve top-ranked (most similar) examples from the target domain. For NER, we use BERTScore (Zhang et al., 2020;Wang et al., 2021), because it gives a metric for each sentence based on the similarity of token representation, which is more crucial for the task of NER. Specifically, given a source sentence x S paired with label y, we retrieve top-k relevant chunks of texts from the target unlabeled dataset D T . \n\nThe retrieved examples are denoted as \n\nk } which will serve as the contexts to enrich the semantics for the source input.",
            "reference_string": "[265294654 | Long et al. | 2023 | Citations: 14]"
        },
        {
            "title": "EasyRec: Simple yet Effective Language Models for Recommendation",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 43,
            "citation_count": 5,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.08821, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2163180478",
                    "name": "Xubin Ren"
                },
                {
                    "authorId": "2305137970",
                    "name": "Chao Huang"
                }
            ],
            "abstract": "Deep neural networks have become a powerful technique for learning representations from user-item interaction data in collaborative filtering (CF) for recommender systems. However, many existing methods heavily rely on unique user and item IDs, which limits their ability to perform well in practical zero-shot learning scenarios where sufficient training data may be unavailable. Inspired by the success of language models (LMs) and their strong generalization capabilities, a crucial question arises: How can we harness the potential of language models to empower recommender systems and elevate its generalization capabilities to new heights? In this study, we propose EasyRec - an effective and easy-to-use approach that seamlessly integrates text-based semantic understanding with collaborative signals. EasyRec employs a text-behavior alignment framework, which combines contrastive learning with collaborative language model tuning, to ensure a strong alignment between the text-enhanced semantic space and the collaborative behavior information. Extensive empirical evaluations across diverse real-world datasets demonstrate the superior performance of EasyRec compared to state-of-the-art alternative models, particularly in the challenging text-based zero-shot recommendation scenarios. Furthermore, the study highlights the potential of seamlessly integrating EasyRec as a plug-and-play component into text-enhanced collaborative filtering frameworks, thereby empowering existing recommender systems to elevate their recommendation performance and adapt to the evolving user preferences in dynamic environments. For better result reproducibility of our EasyRec framework, the model implementation details, source code, and datasets are available at the link: https://github.com/HKUDS/EasyRec.",
            "corpus_id": 271891894,
            "sentences": [
                {
                    "corpus_id": "271891894",
                    "title": "EasyRec: Simple yet Effective Language Models for Recommendation",
                    "text": "Baseline Methods and Settings. For our comparative evaluation, we included a diverse set of language models as text embedders: (i) General Language Models: BERT [4], RoBERTa [21], and BART [14]; (ii) Language Models for Dense Retrieval: SimCSE [7], GTR [23], and BGE [39]; (iii) Pre-trained Language Models for Recommendation: BLaIR [12]. Additionally, we also compared against the state-of-the-art text embedding models provided by OpenAI. \n\nThe detailed descriptions of these baseline models can be found in Appendix A.3. This selection of language models covers a broad spectrum, from general-purpose contextual encoders to specialized models tailored for tasks like dense retrieval and recommendation.",
                    "score": 0.4013052374792626,
                    "section_title": "4.2.1",
                    "char_start_offset": 24150,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 30
                        },
                        {
                            "start": 31,
                            "end": 338
                        },
                        {
                            "start": 339,
                            "end": 440
                        },
                        {
                            "start": 443,
                            "end": 523
                        },
                        {
                            "start": 524,
                            "end": 705
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94580078125
                }
            ],
            "relevance_judgement": 0.94580078125,
            "relevance_judgment_input_expanded": "# Title: EasyRec: Simple yet Effective Language Models for Recommendation\n# Venue: arXiv.org\n# Authors: Xubin Ren, Chao Huang\n## Abstract\nDeep neural networks have become a powerful technique for learning representations from user-item interaction data in collaborative filtering (CF) for recommender systems. However, many existing methods heavily rely on unique user and item IDs, which limits their ability to perform well in practical zero-shot learning scenarios where sufficient training data may be unavailable. Inspired by the success of language models (LMs) and their strong generalization capabilities, a crucial question arises: How can we harness the potential of language models to empower recommender systems and elevate its generalization capabilities to new heights? In this study, we propose EasyRec - an effective and easy-to-use approach that seamlessly integrates text-based semantic understanding with collaborative signals. EasyRec employs a text-behavior alignment framework, which combines contrastive learning with collaborative language model tuning, to ensure a strong alignment between the text-enhanced semantic space and the collaborative behavior information. Extensive empirical evaluations across diverse real-world datasets demonstrate the superior performance of EasyRec compared to state-of-the-art alternative models, particularly in the challenging text-based zero-shot recommendation scenarios. Furthermore, the study highlights the potential of seamlessly integrating EasyRec as a plug-and-play component into text-enhanced collaborative filtering frameworks, thereby empowering existing recommender systems to elevate their recommendation performance and adapt to the evolving user preferences in dynamic environments. For better result reproducibility of our EasyRec framework, the model implementation details, source code, and datasets are available at the link: https://github.com/HKUDS/EasyRec.\n## 4.2.1\nBaseline Methods and Settings. For our comparative evaluation, we included a diverse set of language models as text embedders: (i) General Language Models: BERT [4], RoBERTa [21], and BART [14]; (ii) Language Models for Dense Retrieval: SimCSE [7], GTR [23], and BGE [39]; (iii) Pre-trained Language Models for Recommendation: BLaIR [12]. Additionally, we also compared against the state-of-the-art text embedding models provided by OpenAI. \n\nThe detailed descriptions of these baseline models can be found in Appendix A.3. This selection of language models covers a broad spectrum, from general-purpose contextual encoders to specialized models tailored for tasks like dense retrieval and recommendation.",
            "reference_string": "[271891894 | Ren et al. | 2024 | Citations: 5]"
        },
        {
            "title": "R4: Reinforced Retriever-Reorder-Responder for Retrieval-Augmented Large Language Models",
            "venue": "European Conference on Artificial Intelligence",
            "year": 2024,
            "reference_count": 51,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.02659, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2146342371",
                    "name": "Taolin Zhang"
                },
                {
                    "authorId": "2257089368",
                    "name": "Dongyang Li"
                },
                {
                    "authorId": "2300139675",
                    "name": "Qizhou Chen"
                },
                {
                    "authorId": "50097294",
                    "name": "Chengyu Wang"
                },
                {
                    "authorId": "2292090586",
                    "name": "Longtao Huang"
                },
                {
                    "authorId": "2292128230",
                    "name": "Hui Xue"
                },
                {
                    "authorId": "2257159827",
                    "name": "Xiaofeng He"
                },
                {
                    "authorId": "2272790856",
                    "name": "Junyuan Huang"
                }
            ],
            "abstract": "Retrieval-augmented large language models (LLMs) leverage relevant content retrieved by information retrieval systems to generate correct responses, aiming to alleviate the hallucination problem. However, existing retriever-responder methods typically append relevant documents to the prompt of LLMs to perform text generation tasks without considering the interaction of fine-grained structural semantics between the retrieved documents and the LLMs. This issue is particularly important for accurate response generation as LLMs tend to\"lose in the middle\"when dealing with input prompts augmented with lengthy documents. In this work, we propose a new pipeline named\"Reinforced Retriever-Reorder-Responder\"(R$^4$) to learn document orderings for retrieval-augmented LLMs, thereby further enhancing their generation abilities while the large numbers of parameters of LLMs remain frozen. The reordering learning process is divided into two steps according to the quality of the generated responses: document order adjustment and document representation enhancement. Specifically, document order adjustment aims to organize retrieved document orderings into beginning, middle, and end positions based on graph attention learning, which maximizes the reinforced reward of response quality. Document representation enhancement further refines the representations of retrieved documents for responses of poor quality via document-level gradient adversarial learning. Extensive experiments demonstrate that our proposed pipeline achieves better factual question-answering performance on knowledge-intensive tasks compared to strong baselines across various public datasets. The source codes and trained models will be released upon paper acceptance.",
            "corpus_id": 269605025,
            "sentences": [
                {
                    "corpus_id": "269605025",
                    "title": "R4: Reinforced Retriever-Reorder-Responder for Retrieval-Augmented Large Language Models",
                    "text": "Recently, large language models (LLMs) have attracted extensive attention, which are typically pre-trained on large datasets and implicitly store substantial amounts of world or domain knowledge [33,45]. However, LLMs are also prone to the hallucination problem, and thus, they may generate erroneous responses [49]. In contrast, retrieval-augmented LLMs [17,10,47,35] retrieve knowledge from an external datastore when needed, thereby reducing hallucinations and increasing the knowledge coverage in responses. \n\nIn the literature, there are two major research aspects in this field: \n\n(1) Datastore Indexing [17,10,44,48] and (2) Document Retrieval [35,27]. For Datastore Indexing, these approaches utilize pre-trained models to generate static embeddings for documents, which are viewed as mounted external memory, and they leverage various semantic similarities to enhance indexing. For Document Retrieval, the system initially retrieves a collection of relevant documents based on the semantic relevance between the user query and the documents. Then, the LLMs concatenate these highly related documents in an unordered manner to the prompt input [4], which makes LLMs better at answering factual questions. These methods essentially organize the information related to the user query from the perspective of coarse-grained memory , ignoring the fine-grained relationships between retrieved documents and the knowledge mastery characteristics of LLMs [14,22]. For instance, the ordering of the top-K retrieved documents can be further adjusted to enhance the performance of retrieval-augmented LLMs in answering questions more accurately, as illustrated in Figure 1. \n\nIn this paper, we propose the Reinforced Retriever-Reorder-Responder framework (R 4 ) to formalize a new retrieval-augmented generation (RAG) pipeline. To reorder the retrieved top-K documents and enhance the response effectiveness of the LLMs, we divide the reorder learning process into the following two steps: Document Order Adjustment: Prior research indicates that LLMs have a better recall of information at the beginning and the ending positions of retrieved documents in prompts [14,22].",
                    "score": 0.38704866882156796,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 203
                        },
                        {
                            "start": 204,
                            "end": 316
                        },
                        {
                            "start": 317,
                            "end": 511
                        },
                        {
                            "start": 514,
                            "end": 584
                        },
                        {
                            "start": 587,
                            "end": 659
                        },
                        {
                            "start": 660,
                            "end": 886
                        },
                        {
                            "start": 887,
                            "end": 1050
                        },
                        {
                            "start": 1051,
                            "end": 1212
                        },
                        {
                            "start": 1213,
                            "end": 1464
                        },
                        {
                            "start": 1465,
                            "end": 1671
                        },
                        {
                            "start": 1674,
                            "end": 1825
                        },
                        {
                            "start": 1826,
                            "end": 2170
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 199,
                            "end": 202,
                            "matchedPaperCorpusId": "246411621"
                        },
                        {
                            "start": 355,
                            "end": 359,
                            "matchedPaperCorpusId": "207870430"
                        },
                        {
                            "start": 362,
                            "end": 365,
                            "matchedPaperCorpusId": "253802096"
                        },
                        {
                            "start": 610,
                            "end": 614,
                            "matchedPaperCorpusId": "207870430"
                        },
                        {
                            "start": 617,
                            "end": 620,
                            "matchedPaperCorpusId": "249674500"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94482421875
                }
            ],
            "relevance_judgement": 0.94482421875,
            "relevance_judgment_input_expanded": "# Title: R4: Reinforced Retriever-Reorder-Responder for Retrieval-Augmented Large Language Models\n# Venue: European Conference on Artificial Intelligence\n# Authors: Taolin Zhang, Dongyang Li, Qizhou Chen, Chengyu Wang, Longtao Huang, Hui Xue, Xiaofeng He, Junyuan Huang\n## Abstract\nRetrieval-augmented large language models (LLMs) leverage relevant content retrieved by information retrieval systems to generate correct responses, aiming to alleviate the hallucination problem. However, existing retriever-responder methods typically append relevant documents to the prompt of LLMs to perform text generation tasks without considering the interaction of fine-grained structural semantics between the retrieved documents and the LLMs. This issue is particularly important for accurate response generation as LLMs tend to\"lose in the middle\"when dealing with input prompts augmented with lengthy documents. In this work, we propose a new pipeline named\"Reinforced Retriever-Reorder-Responder\"(R$^4$) to learn document orderings for retrieval-augmented LLMs, thereby further enhancing their generation abilities while the large numbers of parameters of LLMs remain frozen. The reordering learning process is divided into two steps according to the quality of the generated responses: document order adjustment and document representation enhancement. Specifically, document order adjustment aims to organize retrieved document orderings into beginning, middle, and end positions based on graph attention learning, which maximizes the reinforced reward of response quality. Document representation enhancement further refines the representations of retrieved documents for responses of poor quality via document-level gradient adversarial learning. Extensive experiments demonstrate that our proposed pipeline achieves better factual question-answering performance on knowledge-intensive tasks compared to strong baselines across various public datasets. The source codes and trained models will be released upon paper acceptance.\n## Introduction\nRecently, large language models (LLMs) have attracted extensive attention, which are typically pre-trained on large datasets and implicitly store substantial amounts of world or domain knowledge [33,45]. However, LLMs are also prone to the hallucination problem, and thus, they may generate erroneous responses [49]. In contrast, retrieval-augmented LLMs [17,10,47,35] retrieve knowledge from an external datastore when needed, thereby reducing hallucinations and increasing the knowledge coverage in responses. \n\nIn the literature, there are two major research aspects in this field: \n\n(1) Datastore Indexing [17,10,44,48] and (2) Document Retrieval [35,27]. For Datastore Indexing, these approaches utilize pre-trained models to generate static embeddings for documents, which are viewed as mounted external memory, and they leverage various semantic similarities to enhance indexing. For Document Retrieval, the system initially retrieves a collection of relevant documents based on the semantic relevance between the user query and the documents. Then, the LLMs concatenate these highly related documents in an unordered manner to the prompt input [4], which makes LLMs better at answering factual questions. These methods essentially organize the information related to the user query from the perspective of coarse-grained memory , ignoring the fine-grained relationships between retrieved documents and the knowledge mastery characteristics of LLMs [14,22]. For instance, the ordering of the top-K retrieved documents can be further adjusted to enhance the performance of retrieval-augmented LLMs in answering questions more accurately, as illustrated in Figure 1. \n\nIn this paper, we propose the Reinforced Retriever-Reorder-Responder framework (R 4 ) to formalize a new retrieval-augmented generation (RAG) pipeline. To reorder the retrieved top-K documents and enhance the response effectiveness of the LLMs, we divide the reorder learning process into the following two steps: Document Order Adjustment: Prior research indicates that LLMs have a better recall of information at the beginning and the ending positions of retrieved documents in prompts [14,22].",
            "reference_string": "[269605025 | Zhang et al. | 2024 | Citations: 0]"
        },
        {
            "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2023,
            "reference_count": 46,
            "citation_count": 641,
            "influential_citation_count": 57,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2301.12652",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2301.12652, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "3040379",
                    "name": "Weijia Shi"
                },
                {
                    "authorId": "48872685",
                    "name": "Sewon Min"
                },
                {
                    "authorId": "19168196",
                    "name": "Michihiro Yasunaga"
                },
                {
                    "authorId": "4418074",
                    "name": "Minjoon Seo"
                },
                {
                    "authorId": "2191899140",
                    "name": "Rich James"
                },
                {
                    "authorId": "35084211",
                    "name": "M. Lewis"
                },
                {
                    "authorId": "1982950",
                    "name": "Luke Zettlemoyer"
                },
                {
                    "authorId": "2072801764",
                    "name": "Wen-tau Yih"
                }
            ],
            "abstract": "We introduce REPLUG, a retrieval-augmented language modeling framework that treats the language model (LM) as a black box and augments it with a tuneable retrieval model. Unlike prior retrieval-augmented LMs that train language models with special cross-attention mechanisms to encode the retrieved text, REPLUG simply prepends retrieved documents to the input for the frozen black-box LM. This simple design can be easily applied to any existing language models. Furthermore, we show that the LM can be used to supervise the retrieval model, which can then find documents that help the LM make better predictions. Our experiments demonstrate that REPLUG with the tuned retriever significantly improves the performance of GPT-3 (175B) on language modeling by 6.3%, as well as the performance of Codex on five-shot MMLU by 5.1%. Code is publicly released at github.com/swj0419/REPLUG.",
            "corpus_id": 256389797,
            "sentences": [
                {
                    "corpus_id": "256389797",
                    "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
                    "text": "We introduce REPLUG, a retrieval-augmented language modeling paradigm that treats the language model as a black box and augments it with a tuneable retrieval model. \n\nOur evaluation shows that REPLUG can be integrated with any existing language model to improve their performance on language modeling or downstream tasks. This work opens up new possibilities for integrating retrieval into largescale black-box language models and demonstrates even the state-of-the-art large-scale LMs could benefit from retrieval. However, REPLUG lacks interpretability as it is unclear when the model relies on retrieved knowledge or parametric knowledge. Future research could focus on developing more interpretable retrieval-augmented language models.",
                    "score": 0.4310919816026011,
                    "section_title": "Conclusion",
                    "char_start_offset": 25583,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 164
                        },
                        {
                            "start": 167,
                            "end": 321
                        },
                        {
                            "start": 322,
                            "end": 515
                        },
                        {
                            "start": 516,
                            "end": 641
                        },
                        {
                            "start": 642,
                            "end": 739
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.943359375
                }
            ],
            "relevance_judgement": 0.943359375,
            "relevance_judgment_input_expanded": "# Title: REPLUG: Retrieval-Augmented Black-Box Language Models\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, M. Lewis, Luke Zettlemoyer, Wen-tau Yih\n## Abstract\nWe introduce REPLUG, a retrieval-augmented language modeling framework that treats the language model (LM) as a black box and augments it with a tuneable retrieval model. Unlike prior retrieval-augmented LMs that train language models with special cross-attention mechanisms to encode the retrieved text, REPLUG simply prepends retrieved documents to the input for the frozen black-box LM. This simple design can be easily applied to any existing language models. Furthermore, we show that the LM can be used to supervise the retrieval model, which can then find documents that help the LM make better predictions. Our experiments demonstrate that REPLUG with the tuned retriever significantly improves the performance of GPT-3 (175B) on language modeling by 6.3%, as well as the performance of Codex on five-shot MMLU by 5.1%. Code is publicly released at github.com/swj0419/REPLUG.\n## Conclusion\nWe introduce REPLUG, a retrieval-augmented language modeling paradigm that treats the language model as a black box and augments it with a tuneable retrieval model. \n\nOur evaluation shows that REPLUG can be integrated with any existing language model to improve their performance on language modeling or downstream tasks. This work opens up new possibilities for integrating retrieval into largescale black-box language models and demonstrates even the state-of-the-art large-scale LMs could benefit from retrieval. However, REPLUG lacks interpretability as it is unclear when the model relies on retrieved knowledge or parametric knowledge. Future research could focus on developing more interpretable retrieval-augmented language models.",
            "reference_string": "[256389797 | Shi et al. | 2023 | Citations: 641]"
        },
        {
            "title": "Privacy Implications of Retrieval-Based Language Models",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2023,
            "reference_count": 51,
            "citation_count": 30,
            "influential_citation_count": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2305.14888",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.14888, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "108053318",
                    "name": "Yangsibo Huang"
                },
                {
                    "authorId": "2143081868",
                    "name": "Samyak Gupta"
                },
                {
                    "authorId": "49164966",
                    "name": "Zexuan Zhong"
                },
                {
                    "authorId": "94451829",
                    "name": "K. Li"
                },
                {
                    "authorId": "50536468",
                    "name": "Danqi Chen"
                }
            ],
            "abstract": "Retrieval-based language models (LMs) have demonstrated improved interpretability, factuality, and adaptability compared to their parametric counterparts, by incorporating retrieved text from external datastores. While it is well known that parametric models are prone to leaking private data, it remains unclear how the addition of a retrieval datastore impacts model privacy. In this work, we present the first study of privacy risks in retrieval-based LMs, particularly $k$NN-LMs. Our goal is to explore the optimal design and training procedure in domains where privacy is of concern, aiming to strike a balance between utility and privacy. Crucially, we find that $k$NN-LMs are more susceptible to leaking private information from their private datastore than parametric models. We further explore mitigations of privacy risks. When privacy information is targeted and readily detected in the text, we find that a simple sanitization step would completely eliminate the risks, while decoupling query and key encoders achieves an even better utility-privacy trade-off. Otherwise, we consider strategies of mixing public and private data in both datastore and encoder training. While these methods offer modest improvements, they leave considerable room for future work. Together, our findings provide insights for practitioners to better understand and mitigate privacy risks in retrieval-based LMs. Our code is available at: https://github.com/Princeton-SysML/kNNLM_privacy .",
            "corpus_id": 258865437,
            "sentences": [
                {
                    "corpus_id": "258865437",
                    "title": "Privacy Implications of Retrieval-Based Language Models",
                    "text": "Retrieval-based language models (Khandelwal et al., 2020;Borgeaud et al., 2022;Izacard et al., 2022;Zhong et al., 2022;Min et al., 2023;Shi et al., 2023) have been widely studied in recent years. These models not only rely on encoder forward running but also leverage a non-parametric component to incorporate more knowledge from an external datastore during inference. The retrieval process starts by using the input as a query, and then retrieving a set of documents (i.e., sequences of tokens) from a corpus. The language model finally incorporates these retrieved documents as additional information to make its final prediction. While the deployment of retrieval-based language models has been shown to lead to improved performance on various NLP tasks, including language modeling and open-domain question answering, it also poses concerns about data privacy.",
                    "score": 0.4064586197702927,
                    "section_title": "Retrieval-based Language Models",
                    "char_start_offset": 29126,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 195
                        },
                        {
                            "start": 196,
                            "end": 369
                        },
                        {
                            "start": 370,
                            "end": 511
                        },
                        {
                            "start": 512,
                            "end": 633
                        },
                        {
                            "start": 634,
                            "end": 865
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 32,
                            "end": 57,
                            "matchedPaperCorpusId": "207870430"
                        },
                        {
                            "start": 57,
                            "end": 79,
                            "matchedPaperCorpusId": "244954723"
                        },
                        {
                            "start": 100,
                            "end": 119,
                            "matchedPaperCorpusId": "249062699"
                        },
                        {
                            "start": 119,
                            "end": 136,
                            "matchedPaperCorpusId": "254220735"
                        },
                        {
                            "start": 136,
                            "end": 153,
                            "matchedPaperCorpusId": "256389797"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.943359375
                }
            ],
            "relevance_judgement": 0.943359375,
            "relevance_judgment_input_expanded": "# Title: Privacy Implications of Retrieval-Based Language Models\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Yangsibo Huang, Samyak Gupta, Zexuan Zhong, K. Li, Danqi Chen\n## Abstract\nRetrieval-based language models (LMs) have demonstrated improved interpretability, factuality, and adaptability compared to their parametric counterparts, by incorporating retrieved text from external datastores. While it is well known that parametric models are prone to leaking private data, it remains unclear how the addition of a retrieval datastore impacts model privacy. In this work, we present the first study of privacy risks in retrieval-based LMs, particularly $k$NN-LMs. Our goal is to explore the optimal design and training procedure in domains where privacy is of concern, aiming to strike a balance between utility and privacy. Crucially, we find that $k$NN-LMs are more susceptible to leaking private information from their private datastore than parametric models. We further explore mitigations of privacy risks. When privacy information is targeted and readily detected in the text, we find that a simple sanitization step would completely eliminate the risks, while decoupling query and key encoders achieves an even better utility-privacy trade-off. Otherwise, we consider strategies of mixing public and private data in both datastore and encoder training. While these methods offer modest improvements, they leave considerable room for future work. Together, our findings provide insights for practitioners to better understand and mitigate privacy risks in retrieval-based LMs. Our code is available at: https://github.com/Princeton-SysML/kNNLM_privacy .\n## Retrieval-based Language Models\nRetrieval-based language models (Khandelwal et al., 2020;Borgeaud et al., 2022;Izacard et al., 2022;Zhong et al., 2022;Min et al., 2023;Shi et al., 2023) have been widely studied in recent years. These models not only rely on encoder forward running but also leverage a non-parametric component to incorporate more knowledge from an external datastore during inference. The retrieval process starts by using the input as a query, and then retrieving a set of documents (i.e., sequences of tokens) from a corpus. The language model finally incorporates these retrieved documents as additional information to make its final prediction. While the deployment of retrieval-based language models has been shown to lead to improved performance on various NLP tasks, including language modeling and open-domain question answering, it also poses concerns about data privacy.",
            "reference_string": "[258865437 | Huang et al. | 2023 | Citations: 30]"
        },
        {
            "title": "Retrieve Anything To Augment Large Language Models",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 102,
            "citation_count": 63,
            "influential_citation_count": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2310.07554",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.07554, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2153419738",
                    "name": "Peitian Zhang"
                },
                {
                    "authorId": "2051175765",
                    "name": "Shitao Xiao"
                },
                {
                    "authorId": "2240687341",
                    "name": "Zheng Liu"
                },
                {
                    "authorId": "2257039188",
                    "name": "Zhicheng Dou"
                },
                {
                    "authorId": "2261086903",
                    "name": "Jian-Yun Nie"
                }
            ],
            "abstract": "Large language models (LLMs) face significant challenges stemming from their inherent limitations in knowledge, memory, alignment, and action. These challenges cannot be addressed by LLMs alone, but should rely on assistance from the external world, such as knowledge base, memory store, demonstration examples, and tools. Retrieval augmentation stands as a vital mechanism for bridging the gap between LLMs and the external assistance. However, conventional methods encounter two pressing issues. On the one hand, the general-purpose retrievers are not properly optimized for the retrieval augmentation of LLMs. On the other hand, the task-specific retrievers lack the required versatility, hindering their performance across the diverse retrieval augmentation scenarios. In this work, we present a novel approach, the LLM-Embedder, which comprehensively supports the diverse retrieval augmentation needs of LLMs with one unified embedding model. Training such a unified model is non-trivial, as various retrieval tasks aim to capture distinct semantic relationships, often subject to mutual interference. To address this challenge, we systematically optimize our training methodology. This includes reward formulation based on LLMs' feedback, the stabilization of knowledge distillation, multi-task fine-tuning with explicit instructions, and homogeneous in-batch negative sampling. These optimization strategies contribute to the outstanding empirical performance of the LLM-Embedder. Notably, it yields remarkable enhancements in retrieval augmentation for LLMs, surpassing both general-purpose and task-specific retrievers in various evaluation scenarios. Our checkpoint and source code are publicly available at https://github.com/FlagOpen/FlagEmbedding.",
            "corpus_id": 263835099,
            "sentences": [
                {
                    "corpus_id": "263835099",
                    "title": "Retrieve Anything To Augment Large Language Models",
                    "text": "The related works are reviewed from two perspectives: retrieval augmented large language models, and dense retrieval. \n\n\u2022 Retrieval Augmented LLMs. Large language models (LLMs) are praised for their unprecedented capability on language understanding and generation. Compared with the conventional methods, LLMs exhibit overwhelming generality and notable advantages on typical NLP tasks [17,19,78]. Despite such superiority, LLMs still face a series of severe challenges, such as hallucination, human alignment, and long-term memory. Many of the existing problems are caused by the inherent boundaries, which cannot be addressed by LLMs alone, but to rely on support from the external world. The retrieval-augmented LLMs are regarded as a go-to option to bridge LLMs with the external assistance [4,51]. For the past few years, they have been widely applied to several critical scenarios. One common case is the knowledge enhancement. The internal knowledge of LLMs can be incomplete, static, and limited by the popularity bias. When dealing with knowledge intensive tasks, the retrieval augmented LLMs will look for necessary information from an external database, where the generated content can be grounded on proper knowledge [15,31,32,41]. Besides, the retrieval augmented LLMs are also used to retrieve historical context to establish longterm memory [71,85], retrieve examples to improve the instruction following capability [18,83], and retrieve tools to engage with the physical world [62]. \n\nThe retrieval augmented LLMs consist of two basic parts: generator and retriever. According to previous studies [32,41,83,96], the retrieval augmentation effect is highly influenced by the retrieved content. In practice, there are two common types of retrievers. One is to leverage the general purpose retrievers, such as sparse models like BM25 [69], and dense models, like DPR [37], contriever [30], E5 [81], BGE [89], OpenAI text embedding [56]. The other option is develop task-specific retriever, e.g., AAR for knowledge enhancement [96], LLM-R [85] for in-context learning. The general purpose methods are praised for their generality and simplicity for usage, but may suffer from an inferior retrieval quality.",
                    "score": 0.41469612313249743,
                    "section_title": "RELATED WORKS",
                    "char_start_offset": 33911,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 117
                        },
                        {
                            "start": 120,
                            "end": 147
                        },
                        {
                            "start": 148,
                            "end": 265
                        },
                        {
                            "start": 266,
                            "end": 398
                        },
                        {
                            "start": 399,
                            "end": 533
                        },
                        {
                            "start": 534,
                            "end": 691
                        },
                        {
                            "start": 692,
                            "end": 803
                        },
                        {
                            "start": 804,
                            "end": 888
                        },
                        {
                            "start": 889,
                            "end": 934
                        },
                        {
                            "start": 935,
                            "end": 1028
                        },
                        {
                            "start": 1029,
                            "end": 1244
                        },
                        {
                            "start": 1245,
                            "end": 1499
                        },
                        {
                            "start": 1502,
                            "end": 1583
                        },
                        {
                            "start": 1584,
                            "end": 1709
                        },
                        {
                            "start": 1710,
                            "end": 1764
                        },
                        {
                            "start": 1765,
                            "end": 1950
                        },
                        {
                            "start": 1951,
                            "end": 2081
                        },
                        {
                            "start": 2082,
                            "end": 2219
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94189453125
                }
            ],
            "relevance_judgement": 0.94189453125,
            "relevance_judgment_input_expanded": "# Title: Retrieve Anything To Augment Large Language Models\n# Venue: arXiv.org\n# Authors: Peitian Zhang, Shitao Xiao, Zheng Liu, Zhicheng Dou, Jian-Yun Nie\n## Abstract\nLarge language models (LLMs) face significant challenges stemming from their inherent limitations in knowledge, memory, alignment, and action. These challenges cannot be addressed by LLMs alone, but should rely on assistance from the external world, such as knowledge base, memory store, demonstration examples, and tools. Retrieval augmentation stands as a vital mechanism for bridging the gap between LLMs and the external assistance. However, conventional methods encounter two pressing issues. On the one hand, the general-purpose retrievers are not properly optimized for the retrieval augmentation of LLMs. On the other hand, the task-specific retrievers lack the required versatility, hindering their performance across the diverse retrieval augmentation scenarios. In this work, we present a novel approach, the LLM-Embedder, which comprehensively supports the diverse retrieval augmentation needs of LLMs with one unified embedding model. Training such a unified model is non-trivial, as various retrieval tasks aim to capture distinct semantic relationships, often subject to mutual interference. To address this challenge, we systematically optimize our training methodology. This includes reward formulation based on LLMs' feedback, the stabilization of knowledge distillation, multi-task fine-tuning with explicit instructions, and homogeneous in-batch negative sampling. These optimization strategies contribute to the outstanding empirical performance of the LLM-Embedder. Notably, it yields remarkable enhancements in retrieval augmentation for LLMs, surpassing both general-purpose and task-specific retrievers in various evaluation scenarios. Our checkpoint and source code are publicly available at https://github.com/FlagOpen/FlagEmbedding.\n## RELATED WORKS\nThe related works are reviewed from two perspectives: retrieval augmented large language models, and dense retrieval. \n\n\u2022 Retrieval Augmented LLMs. Large language models (LLMs) are praised for their unprecedented capability on language understanding and generation. Compared with the conventional methods, LLMs exhibit overwhelming generality and notable advantages on typical NLP tasks [17,19,78]. Despite such superiority, LLMs still face a series of severe challenges, such as hallucination, human alignment, and long-term memory. Many of the existing problems are caused by the inherent boundaries, which cannot be addressed by LLMs alone, but to rely on support from the external world. The retrieval-augmented LLMs are regarded as a go-to option to bridge LLMs with the external assistance [4,51]. For the past few years, they have been widely applied to several critical scenarios. One common case is the knowledge enhancement. The internal knowledge of LLMs can be incomplete, static, and limited by the popularity bias. When dealing with knowledge intensive tasks, the retrieval augmented LLMs will look for necessary information from an external database, where the generated content can be grounded on proper knowledge [15,31,32,41]. Besides, the retrieval augmented LLMs are also used to retrieve historical context to establish longterm memory [71,85], retrieve examples to improve the instruction following capability [18,83], and retrieve tools to engage with the physical world [62]. \n\nThe retrieval augmented LLMs consist of two basic parts: generator and retriever. According to previous studies [32,41,83,96], the retrieval augmentation effect is highly influenced by the retrieved content. In practice, there are two common types of retrievers. One is to leverage the general purpose retrievers, such as sparse models like BM25 [69], and dense models, like DPR [37], contriever [30], E5 [81], BGE [89], OpenAI text embedding [56]. The other option is develop task-specific retriever, e.g., AAR for knowledge enhancement [96], LLM-R [85] for in-context learning. The general purpose methods are praised for their generality and simplicity for usage, but may suffer from an inferior retrieval quality.",
            "reference_string": "[263835099 | Zhang et al. | 2023 | Citations: 63]"
        },
        {
            "title": "CAME: Competitively Learning a Mixture-of-Experts Model for First-stage Retrieval",
            "venue": "ACM Trans. Inf. Syst.",
            "year": 2023,
            "reference_count": 74,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.02834, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "50204455",
                    "name": "Yinqiong Cai"
                },
                {
                    "authorId": "7888704",
                    "name": "Yixing Fan"
                },
                {
                    "authorId": "2249759496",
                    "name": "Keping Bi"
                },
                {
                    "authorId": "1777025",
                    "name": "J. Guo"
                },
                {
                    "authorId": "2256716717",
                    "name": "Wei Chen"
                },
                {
                    "authorId": "2109960367",
                    "name": "Ruqing Zhang"
                },
                {
                    "authorId": "2244825947",
                    "name": "Xueqi Cheng"
                }
            ],
            "abstract": "The first-stage retrieval aims to retrieve a subset of candidate documents from a huge collection both effectively and efficiently. Since various matching patterns can exist between queries and relevant documents, previous work tries to combine multiple retrieval models to find as many relevant results as possible. The constructed ensembles, whether learned independently or jointly, do not care which component model is more suitable to an instance during training. Thus, they cannot fully exploit the capabilities of different types of retrieval models in identifying diverse relevance patterns. Motivated by this observation, in this article, we propose a Mixture-of-Experts (MoE) model consisting of representative matching experts and a novel competitive learning mechanism to let the experts develop and enhance their expertise during training. Specifically, our MoE model shares the bottom layers to learn common semantic representations and uses differently structured upper layers to represent various types of retrieval experts. Our competitive learning mechanism has two stages: (1) a standardized learning stage to train the experts equally to develop their capabilities to conduct relevance matching; (2) a specialized learning stage where the experts compete with each other on every training instance and get rewards and updates according to their performance to enhance their expertise on certain types of samples. Experimental results on retrieval benchmark datasets show that our method significantly outperforms the state-of-the-art baselines in the in-domain and out-of-domain settings.",
            "corpus_id": 265033839,
            "sentences": [
                {
                    "corpus_id": "265033839",
                    "title": "CAME: Competitively Learning a Mixture-of-Experts Model for First-stage Retrieval",
                    "text": "Motivated by the above, we build an MoE retrieval model consisting of multiple representative matching experts, and propose a novel mechanism to Competitively leArn the MoE model, named as CAME. Specifically, we include lexical, local, and global retrievers in a multi-path network architecture with shared bottom layers and top MoE layers. The shared bottom layers aim to extract common syntactic and semantic representations for all the experts. The MoE layers consist of three experts with different model architectures to capture pattern-specific features. To guide the component experts in CAME to specialize on certain types of samples, we competitively learn the MoE retriever in two phases. First, in the standardized learning stage, each expert is trained equally to develop the relevance matching capability and prepare for specialization. Second, in the specialized learning stage, the component experts compete with each other on every training instance, and they are trained proportionally to their relative performance among all the experts. In this way, each sample is only used to update the experts that perform decently but has no or little impact on other experts. During inference, each expert estimates the relevance score from its perspective to contribute to the final results. Noted that, in contrast to the classical MoE models that pick experts spontaneously for each input sample, our method establishes more explicit connections between the experts and samples via their relative ranking performance. From the above, we can readily urge each expert to fit the case it is skilled at and fully unleash the advantages of different model architectures. \n\nWe evaluate the effectiveness of the proposed model on three representative retrieval benchmark datasets, i.e., MS MARCO [34], TREC Deep Learning Track [3], and Natural Questions [24]. The empirical results show that CAME can outperform all the baselines including various state-of-the-art single-model retrievers and ensemble retrievers significantly. In addition, we conduct comprehensive analyses on how the model components and learning mechanism impact retrieval performance. It shows that employing multi-types of retrieval experts can capture diverse relevance patterns, and the competitive learning strategy is essential to facilitate the experts to learn their designated patterns, which together boost the retrieval performance. To sum up, our contributions include: \n\n\u2022 We propose a Mixture-of-Experts retrieval model that can orchestrate various types of models to capture diverse relevance patterns.",
                    "score": 0.3887723875612886,
                    "section_title": "Matching Scope: Verbosity Hypothesis vs Scope Hypothesis query treatment of varicose veins in legs doc",
                    "char_start_offset": 5290,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 194
                        },
                        {
                            "start": 195,
                            "end": 340
                        },
                        {
                            "start": 341,
                            "end": 447
                        },
                        {
                            "start": 448,
                            "end": 560
                        },
                        {
                            "start": 561,
                            "end": 698
                        },
                        {
                            "start": 699,
                            "end": 849
                        },
                        {
                            "start": 850,
                            "end": 1055
                        },
                        {
                            "start": 1056,
                            "end": 1183
                        },
                        {
                            "start": 1184,
                            "end": 1300
                        },
                        {
                            "start": 1301,
                            "end": 1528
                        },
                        {
                            "start": 1529,
                            "end": 1676
                        },
                        {
                            "start": 1679,
                            "end": 1863
                        },
                        {
                            "start": 1864,
                            "end": 2031
                        },
                        {
                            "start": 2032,
                            "end": 2159
                        },
                        {
                            "start": 2160,
                            "end": 2417
                        },
                        {
                            "start": 2418,
                            "end": 2455
                        },
                        {
                            "start": 2458,
                            "end": 2591
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1800,
                            "end": 1804,
                            "matchedPaperCorpusId": "1289517"
                        },
                        {
                            "start": 1858,
                            "end": 1862,
                            "matchedPaperCorpusId": "86611921"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9404296875
                }
            ],
            "relevance_judgement": 0.9404296875,
            "relevance_judgment_input_expanded": "# Title: CAME: Competitively Learning a Mixture-of-Experts Model for First-stage Retrieval\n# Venue: ACM Trans. Inf. Syst.\n# Authors: Yinqiong Cai, Yixing Fan, Keping Bi, J. Guo, Wei Chen, Ruqing Zhang, Xueqi Cheng\n## Abstract\nThe first-stage retrieval aims to retrieve a subset of candidate documents from a huge collection both effectively and efficiently. Since various matching patterns can exist between queries and relevant documents, previous work tries to combine multiple retrieval models to find as many relevant results as possible. The constructed ensembles, whether learned independently or jointly, do not care which component model is more suitable to an instance during training. Thus, they cannot fully exploit the capabilities of different types of retrieval models in identifying diverse relevance patterns. Motivated by this observation, in this article, we propose a Mixture-of-Experts (MoE) model consisting of representative matching experts and a novel competitive learning mechanism to let the experts develop and enhance their expertise during training. Specifically, our MoE model shares the bottom layers to learn common semantic representations and uses differently structured upper layers to represent various types of retrieval experts. Our competitive learning mechanism has two stages: (1) a standardized learning stage to train the experts equally to develop their capabilities to conduct relevance matching; (2) a specialized learning stage where the experts compete with each other on every training instance and get rewards and updates according to their performance to enhance their expertise on certain types of samples. Experimental results on retrieval benchmark datasets show that our method significantly outperforms the state-of-the-art baselines in the in-domain and out-of-domain settings.\n## Matching Scope: Verbosity Hypothesis vs Scope Hypothesis query treatment of varicose veins in legs doc\nMotivated by the above, we build an MoE retrieval model consisting of multiple representative matching experts, and propose a novel mechanism to Competitively leArn the MoE model, named as CAME. Specifically, we include lexical, local, and global retrievers in a multi-path network architecture with shared bottom layers and top MoE layers. The shared bottom layers aim to extract common syntactic and semantic representations for all the experts. The MoE layers consist of three experts with different model architectures to capture pattern-specific features. To guide the component experts in CAME to specialize on certain types of samples, we competitively learn the MoE retriever in two phases. First, in the standardized learning stage, each expert is trained equally to develop the relevance matching capability and prepare for specialization. Second, in the specialized learning stage, the component experts compete with each other on every training instance, and they are trained proportionally to their relative performance among all the experts. In this way, each sample is only used to update the experts that perform decently but has no or little impact on other experts. During inference, each expert estimates the relevance score from its perspective to contribute to the final results. Noted that, in contrast to the classical MoE models that pick experts spontaneously for each input sample, our method establishes more explicit connections between the experts and samples via their relative ranking performance. From the above, we can readily urge each expert to fit the case it is skilled at and fully unleash the advantages of different model architectures. \n\nWe evaluate the effectiveness of the proposed model on three representative retrieval benchmark datasets, i.e., MS MARCO [34], TREC Deep Learning Track [3], and Natural Questions [24]. The empirical results show that CAME can outperform all the baselines including various state-of-the-art single-model retrievers and ensemble retrievers significantly. In addition, we conduct comprehensive analyses on how the model components and learning mechanism impact retrieval performance. It shows that employing multi-types of retrieval experts can capture diverse relevance patterns, and the competitive learning strategy is essential to facilitate the experts to learn their designated patterns, which together boost the retrieval performance. To sum up, our contributions include: \n\n\u2022 We propose a Mixture-of-Experts retrieval model that can orchestrate various types of models to capture diverse relevance patterns.",
            "reference_string": "[265033839 | Cai et al. | 2023 | Citations: 4]"
        },
        {
            "title": "phi-LSTM: A Phrase-Based Hierarchical LSTM Model for Image Captioning",
            "venue": "Asian Conference on Computer Vision",
            "year": 2016,
            "reference_count": 40,
            "citation_count": 29,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1608.05813, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "143692915",
                    "name": "Y. Tan"
                },
                {
                    "authorId": "2863960",
                    "name": "Chee Seng Chan"
                }
            ],
            "abstract": "A picture is worth a thousand words. Not until recently, however, we noticed some success stories in understanding of visual scenes: a model that is able to detect/name objects, describe their attributes, and recognize their relationships/interactions. In this paper, we propose a phrase-based hierarchical Long Short-Term Memory (phi-LSTM) model to generate image description. The proposed model encodes sentence as a sequence of combination of phrases and words, instead of a sequence of words alone as in those conventional solutions. The two levels of this model are dedicated to i) learn to generate image relevant noun phrases, and ii) produce appropriate image description from the phrases and other words in the corpus. Adopting a convolutional neural network to learn image features and the LSTM to learn the word sequence in a sentence, the proposed model has shown better or competitive results in comparison to the state-of-the-art models on Flickr8k and Flickr30k datasets.",
            "corpus_id": 8486003,
            "sentences": [
                {
                    "corpus_id": "8486003",
                    "title": "phi-LSTM: A Phrase-Based Hierarchical LSTM Model for Image Captioning",
                    "text": "To model the relationship between image and language, some works associate both modalities by embedding their representations into a common space [17][18][19][20]. First, they obtain the image features using a visual model like CNN [18,19], as well as the representation of sentence with a language model such as recursive neural network [19]. Then, both of them are embedded into a common multimodal space and the whole model is learned with ranking objective for image and sentence retrieval task. This framework was also tested at object level by Karpathy et al. [20] and proved to yield better results for the image and sentence bi-directional retrieval task. Besides that, there are works that learn the probability density over multimodal inputs using various statistical approaches. These include Deep Boltzmann Machines [21], topic models [22], log-bilinear neural language model [8,23] and recurrent neural networks [5][6][7] etc. Such approaches fuse different input modalities together to obtain a unified representation of the inputs. It is notable to mention that there are also some works which do not explicitly learn the multimodal representation between image and language, but transit between modalities with retrieval approach. For example, Kuznetsova et al. [24] retrieve images similar to the query image from their database, and extract useful language segments (such as phrases) from the descriptions of the retrieved images.",
                    "score": 0.44243839830171566,
                    "section_title": "Multimodal Representation and Transition",
                    "char_start_offset": 3622,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 163
                        },
                        {
                            "start": 164,
                            "end": 343
                        },
                        {
                            "start": 344,
                            "end": 499
                        },
                        {
                            "start": 500,
                            "end": 663
                        },
                        {
                            "start": 664,
                            "end": 789
                        },
                        {
                            "start": 790,
                            "end": 939
                        },
                        {
                            "start": 940,
                            "end": 1046
                        },
                        {
                            "start": 1047,
                            "end": 1246
                        },
                        {
                            "start": 1247,
                            "end": 1448
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 146,
                            "end": 150,
                            "matchedPaperCorpusId": "928608"
                        },
                        {
                            "start": 150,
                            "end": 154,
                            "matchedPaperCorpusId": "261138"
                        },
                        {
                            "start": 154,
                            "end": 158,
                            "matchedPaperCorpusId": "2317858"
                        },
                        {
                            "start": 158,
                            "end": 162,
                            "matchedPaperCorpusId": "2315434"
                        },
                        {
                            "start": 232,
                            "end": 236,
                            "matchedPaperCorpusId": "261138"
                        },
                        {
                            "start": 236,
                            "end": 239,
                            "matchedPaperCorpusId": "2317858"
                        },
                        {
                            "start": 338,
                            "end": 342,
                            "matchedPaperCorpusId": "2317858"
                        },
                        {
                            "start": 566,
                            "end": 570,
                            "matchedPaperCorpusId": "2315434"
                        },
                        {
                            "start": 828,
                            "end": 832,
                            "matchedPaperCorpusId": "710430"
                        },
                        {
                            "start": 1278,
                            "end": 1282,
                            "matchedPaperCorpusId": "13344783"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93896484375
                }
            ],
            "relevance_judgement": 0.93896484375,
            "relevance_judgment_input_expanded": "# Title: phi-LSTM: A Phrase-Based Hierarchical LSTM Model for Image Captioning\n# Venue: Asian Conference on Computer Vision\n# Authors: Y. Tan, Chee Seng Chan\n## Abstract\nA picture is worth a thousand words. Not until recently, however, we noticed some success stories in understanding of visual scenes: a model that is able to detect/name objects, describe their attributes, and recognize their relationships/interactions. In this paper, we propose a phrase-based hierarchical Long Short-Term Memory (phi-LSTM) model to generate image description. The proposed model encodes sentence as a sequence of combination of phrases and words, instead of a sequence of words alone as in those conventional solutions. The two levels of this model are dedicated to i) learn to generate image relevant noun phrases, and ii) produce appropriate image description from the phrases and other words in the corpus. Adopting a convolutional neural network to learn image features and the LSTM to learn the word sequence in a sentence, the proposed model has shown better or competitive results in comparison to the state-of-the-art models on Flickr8k and Flickr30k datasets.\n## Multimodal Representation and Transition\nTo model the relationship between image and language, some works associate both modalities by embedding their representations into a common space [17][18][19][20]. First, they obtain the image features using a visual model like CNN [18,19], as well as the representation of sentence with a language model such as recursive neural network [19]. Then, both of them are embedded into a common multimodal space and the whole model is learned with ranking objective for image and sentence retrieval task. This framework was also tested at object level by Karpathy et al. [20] and proved to yield better results for the image and sentence bi-directional retrieval task. Besides that, there are works that learn the probability density over multimodal inputs using various statistical approaches. These include Deep Boltzmann Machines [21], topic models [22], log-bilinear neural language model [8,23] and recurrent neural networks [5][6][7] etc. Such approaches fuse different input modalities together to obtain a unified representation of the inputs. It is notable to mention that there are also some works which do not explicitly learn the multimodal representation between image and language, but transit between modalities with retrieval approach. For example, Kuznetsova et al. [24] retrieve images similar to the query image from their database, and extract useful language segments (such as phrases) from the descriptions of the retrieved images.",
            "reference_string": "[8486003 | Tan et al. | 2016 | Citations: 29]"
        },
        {
            "title": "Optimizing Language Model Information Retrieval System with Expectation Maximization Algorithm",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2009,
            "reference_count": 22,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.5555/1667884.1667894",
                "status": "BRONZE",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/P09-3008, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2990581",
                    "name": "J. Chiu"
                },
                {
                    "authorId": "2684491",
                    "name": "Jyun-Wei Huang"
                }
            ],
            "abstract": "Statistical language modeling (SLM) has been used in many different domains for decades and has also been applied to information retrieval (IR) recently. Documents retrieved using this approach are ranked according their probability of generating the given query. In this paper, we present a novel approach that employs the generalized Expectation Maximization (EM) algorithm to improve language models by representing their parameters as observation probabilities of Hidden Markov Models (HMM). In the experiments, we demonstrate that our method outperforms standard SLM-based and tf.idf-based methods on TREC 2005 HARD Track data.",
            "corpus_id": 2862121,
            "sentences": [
                {
                    "corpus_id": "2862121",
                    "title": "Optimizing Language Model Information Retrieval System with Expectation Maximization Algorithm",
                    "text": "Statistical language modeling (SLM) has been used in many different domains for decades and has also been applied to information retrieval (IR) recently. Documents retrieved using this approach are ranked according their probability of generating the given query. In this paper, we present a novel approach that employs the generalized Expectation Maximization (EM) algorithm to improve language models by representing their parameters as observation probabilities of Hidden Markov Models (HMM). In the experiments, we demonstrate that our method outperforms standard SLM-based and tf.idf-based methods on TREC 2005 HARD Track data.",
                    "score": 0.3946961894953656,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9375
                }
            ],
            "relevance_judgement": 0.9375,
            "relevance_judgment_input_expanded": "# Title: Optimizing Language Model Information Retrieval System with Expectation Maximization Algorithm\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: J. Chiu, Jyun-Wei Huang\n## Abstract\nStatistical language modeling (SLM) has been used in many different domains for decades and has also been applied to information retrieval (IR) recently. Documents retrieved using this approach are ranked according their probability of generating the given query. In this paper, we present a novel approach that employs the generalized Expectation Maximization (EM) algorithm to improve language models by representing their parameters as observation probabilities of Hidden Markov Models (HMM). In the experiments, we demonstrate that our method outperforms standard SLM-based and tf.idf-based methods on TREC 2005 HARD Track data.\n",
            "reference_string": "[2862121 | Chiu et al. | 2009 | Citations: 0]"
        },
        {
            "title": "GLIMMER: generalized late-interaction memory reranker",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 53,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2306.10231",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.10231, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "21379393",
                    "name": "Michiel de Jong"
                },
                {
                    "authorId": "2220287479",
                    "name": "Yury Zemlyanskiy"
                },
                {
                    "authorId": "143883142",
                    "name": "Nicholas FitzGerald"
                },
                {
                    "authorId": "144074891",
                    "name": "Sumit K. Sanghai"
                },
                {
                    "authorId": "50056360",
                    "name": "William W. Cohen"
                },
                {
                    "authorId": "1643737606",
                    "name": "J. Ainslie"
                }
            ],
            "abstract": "Memory-augmentation is a powerful approach for efficiently incorporating external information into language models, but leads to reduced performance relative to retrieving text. Recent work introduced LUMEN, a memory-retrieval hybrid that partially pre-computes memory and updates memory representations on the fly with a smaller live encoder. We propose GLIMMER, which improves on this approach through 1) exploiting free access to the powerful memory representations by applying a shallow reranker on top of memory to drastically improve retrieval quality at low cost, and 2) incorporating multi-task training to learn a general and higher quality memory and live encoder. GLIMMER achieves strong gains in performance at faster speeds compared to LUMEN and FiD on the KILT benchmark of knowledge-intensive tasks.",
            "corpus_id": 259203489,
            "sentences": [
                {
                    "corpus_id": "259203489",
                    "title": "GLIMMER: generalized late-interaction memory reranker",
                    "text": "In particular, our work is very closely based on LUMEN (de Jong et al., 2023). \n\nReranking Like the language model itself, retrieval procedures face a trade-off between expensive ranking with full interaction (Chen et al., 2020) and the more common dual encoder approaches such as DPR (Karpukhin et al., 2020) and GTR (Ni et al., 2021) that scores based on inner product similarity with a corpus of pre-computed passage representations. Often different models for retrieval are applied in a pipeline approach, with an initial cheap scoring model followed by a more powerful and expensive reranker (Mao et al., 2021;Wang et al., 2018;Yu et al., 2022). Many rerankers also make use of late interaction to obtain a good trade-off between ranking quality and speed, such as COLBERT (Khattab and Zaharia, 2020;Santhanam et al., 2022), PreTTR (MacAvaney et al., 2020), SDR (Cohen et al., 2022), and Poly-encoders (Humeau et al., 2020). GLIMMER combines late-interaction memory and reranking into a single model, sharing the pre-computed representations for both use cases. \n\nLearning to retrieve Retrieval models are often trained with supervised data (Karpukhin et al., 2020;Ni et al., 2021), using gold retrievals from datasets such as MS-MARCO (Nguyen et al., 2016) or TREC CAR (Dietz et al., 2018). When selecting passage to use for retrieval-augmented generation, we have an additional signal, namely which passages are most helpful for the reader model. A number of existing works use this signal to improve retrieval (Guu et al., 2020;Sachan et al., 2021;Jiang et al., 2022;Sachan et al., 2021;Izacard et al., 2022). We follow ATLAS (Izacard et al., 2022) and employ perplexity distillation to train our reranker to select passages that help lower reader model perplexity.",
                    "score": 0.39351684386532026,
                    "section_title": "Related Work",
                    "char_start_offset": 19685,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 78
                        },
                        {
                            "start": 81,
                            "end": 436
                        },
                        {
                            "start": 437,
                            "end": 650
                        },
                        {
                            "start": 651,
                            "end": 929
                        },
                        {
                            "start": 930,
                            "end": 1066
                        },
                        {
                            "start": 1069,
                            "end": 1296
                        },
                        {
                            "start": 1297,
                            "end": 1453
                        },
                        {
                            "start": 1454,
                            "end": 1617
                        },
                        {
                            "start": 1618,
                            "end": 1773
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 209,
                            "end": 228,
                            "matchedPaperCorpusId": "228093856"
                        },
                        {
                            "start": 285,
                            "end": 309,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 597,
                            "end": 615,
                            "matchedPaperCorpusId": "230435683"
                        },
                        {
                            "start": 633,
                            "end": 649,
                            "matchedPaperCorpusId": "238583601"
                        },
                        {
                            "start": 778,
                            "end": 805,
                            "matchedPaperCorpusId": "216553223"
                        },
                        {
                            "start": 805,
                            "end": 827,
                            "matchedPaperCorpusId": "244799249"
                        },
                        {
                            "start": 867,
                            "end": 887,
                            "matchedPaperCorpusId": "238353991"
                        },
                        {
                            "start": 907,
                            "end": 928,
                            "matchedPaperCorpusId": "210063976"
                        },
                        {
                            "start": 1146,
                            "end": 1170,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 1241,
                            "end": 1262,
                            "matchedPaperCorpusId": "1289517"
                        },
                        {
                            "start": 1275,
                            "end": 1295,
                            "matchedPaperCorpusId": "4987800"
                        },
                        {
                            "start": 1536,
                            "end": 1556,
                            "matchedPaperCorpusId": "235390519"
                        },
                        {
                            "start": 1556,
                            "end": 1575,
                            "matchedPaperCorpusId": "254246471"
                        },
                        {
                            "start": 1575,
                            "end": 1595,
                            "matchedPaperCorpusId": "235390519"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93701171875
                }
            ],
            "relevance_judgement": 0.93701171875,
            "relevance_judgment_input_expanded": "# Title: GLIMMER: generalized late-interaction memory reranker\n# Venue: arXiv.org\n# Authors: Michiel de Jong, Yury Zemlyanskiy, Nicholas FitzGerald, Sumit K. Sanghai, William W. Cohen, J. Ainslie\n## Abstract\nMemory-augmentation is a powerful approach for efficiently incorporating external information into language models, but leads to reduced performance relative to retrieving text. Recent work introduced LUMEN, a memory-retrieval hybrid that partially pre-computes memory and updates memory representations on the fly with a smaller live encoder. We propose GLIMMER, which improves on this approach through 1) exploiting free access to the powerful memory representations by applying a shallow reranker on top of memory to drastically improve retrieval quality at low cost, and 2) incorporating multi-task training to learn a general and higher quality memory and live encoder. GLIMMER achieves strong gains in performance at faster speeds compared to LUMEN and FiD on the KILT benchmark of knowledge-intensive tasks.\n## Related Work\nIn particular, our work is very closely based on LUMEN (de Jong et al., 2023). \n\nReranking Like the language model itself, retrieval procedures face a trade-off between expensive ranking with full interaction (Chen et al., 2020) and the more common dual encoder approaches such as DPR (Karpukhin et al., 2020) and GTR (Ni et al., 2021) that scores based on inner product similarity with a corpus of pre-computed passage representations. Often different models for retrieval are applied in a pipeline approach, with an initial cheap scoring model followed by a more powerful and expensive reranker (Mao et al., 2021;Wang et al., 2018;Yu et al., 2022). Many rerankers also make use of late interaction to obtain a good trade-off between ranking quality and speed, such as COLBERT (Khattab and Zaharia, 2020;Santhanam et al., 2022), PreTTR (MacAvaney et al., 2020), SDR (Cohen et al., 2022), and Poly-encoders (Humeau et al., 2020). GLIMMER combines late-interaction memory and reranking into a single model, sharing the pre-computed representations for both use cases. \n\nLearning to retrieve Retrieval models are often trained with supervised data (Karpukhin et al., 2020;Ni et al., 2021), using gold retrievals from datasets such as MS-MARCO (Nguyen et al., 2016) or TREC CAR (Dietz et al., 2018). When selecting passage to use for retrieval-augmented generation, we have an additional signal, namely which passages are most helpful for the reader model. A number of existing works use this signal to improve retrieval (Guu et al., 2020;Sachan et al., 2021;Jiang et al., 2022;Sachan et al., 2021;Izacard et al., 2022). We follow ATLAS (Izacard et al., 2022) and employ perplexity distillation to train our reranker to select passages that help lower reader model perplexity.",
            "reference_string": "[259203489 | Jong et al. | 2023 | Citations: 5]"
        },
        {
            "title": "Cache-Augmented Latent Topic Language Models for Speech Retrieval",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2015,
            "reference_count": 37,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/N15-2001.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/N15-2001, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "3045640",
                    "name": "Jonathan Wintrode"
                }
            ],
            "abstract": "We aim to improve speech retrieval performance by augmenting traditional N-gram language models with different types of topic context. We present a latent topic model framework that treats documents as arising from an underlying topic sequence combined with a cache-based repetition model. We analyze our proposed model both for its ability to capture word repetition via the cache and for its suitability as a language model for speech recognition and retrieval. We show this model, augmented with the cache, captures intuitive repetition behavior across languages and exhibits lower perplexity than regular LDA on held out data in multiple languages. Lastly, we show that our joint model improves speech retrieval performance beyond N-grams or latent topics alone, when applied to a term detection task in all languages considered.",
            "corpus_id": 10844118,
            "sentences": [
                {
                    "corpus_id": "10844118",
                    "title": "Cache-Augmented Latent Topic Language Models for Speech Retrieval",
                    "text": "The use of both repetition and broad topic context have been exploited in a variety of ways by the speech recognition and retrieval communities. Cache-based or adaptive language models were 1 some of the first approaches to incorporate information beyond a short N-gram history (where N is typically 3-4 words).\n\nCache-based models assume the probability of a word in a document d is influenced both by the global frequency of that word and N-gram context as well as by the N-gram frequencies of d (or preceding cache of K words). Although most words are rare at the corpus level, when they do occur, they occur in bursts. Thus a local estimate, from the cache, may be more reliable than the global estimate. Jelinek (1991) and Kuhn (1990) both successfully applied these types of models for speech recognition, and Rosenfeld (1994), using what he referred to as 'trigger pairs', also realized significant gains in WER. More recently, recurrent neural network language models (RNNLMs) have been introduced to capture more of these \"long-term dependencies\" (Mikolov et al., 2010). In terms of speech retrieval, recent efforts have looked at exploiting repeated keywords at search time, without directly modifying the recognizer (Chiu and Rudnicky, 2013;Wintrode, 2014a).\n\nWork within the information retrieval (IR) community connects topicality with retrieval. Hearst and Plaunt (1993) reported that the \"subtopic structuring\" of documents can improve full-document retrieval. Topic models such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003) or Probabilistic Latent Semantic Analysis (PLSA) (Hofmann, 2001) are used to the augment the document-specific language model in probabilistic, language-model based IR (Wei and Croft, 2006;Chen, 2009;Liu and Croft, 2004;Chemudugunta et al., 2007). In all these cases, topic information was helpful in boosting retrieval performance above baseline vector space or N-gram models.\n\nOur proposed model closely resembles that from Chemudugunta et al. (2007), with our notions of broad and local context corresponding to their \"general and specific\"",
                    "score": 0.39857759446045876,
                    "section_title": "Related Work",
                    "char_start_offset": 1988,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 709,
                            "end": 723,
                            "matchedPaperCorpusId": "11601499"
                        },
                        {
                            "start": 1056,
                            "end": 1078,
                            "matchedPaperCorpusId": "17048224"
                        },
                        {
                            "start": 1227,
                            "end": 1252,
                            "matchedPaperCorpusId": "15421536"
                        },
                        {
                            "start": 1252,
                            "end": 1268,
                            "matchedPaperCorpusId": "13528696"
                        },
                        {
                            "start": 1360,
                            "end": 1384,
                            "matchedPaperCorpusId": "5390627"
                        },
                        {
                            "start": 1600,
                            "end": 1615,
                            "matchedPaperCorpusId": "7605995"
                        },
                        {
                            "start": 1719,
                            "end": 1740,
                            "matchedPaperCorpusId": "3343003"
                        },
                        {
                            "start": 1740,
                            "end": 1751,
                            "matchedPaperCorpusId": "8458087"
                        },
                        {
                            "start": 1751,
                            "end": 1771,
                            "matchedPaperCorpusId": "15479915"
                        },
                        {
                            "start": 1771,
                            "end": 1797,
                            "matchedPaperCorpusId": "2447517"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93359375
                }
            ],
            "relevance_judgement": 0.93359375,
            "relevance_judgment_input_expanded": "# Title: Cache-Augmented Latent Topic Language Models for Speech Retrieval\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: Jonathan Wintrode\n## Abstract\nWe aim to improve speech retrieval performance by augmenting traditional N-gram language models with different types of topic context. We present a latent topic model framework that treats documents as arising from an underlying topic sequence combined with a cache-based repetition model. We analyze our proposed model both for its ability to capture word repetition via the cache and for its suitability as a language model for speech recognition and retrieval. We show this model, augmented with the cache, captures intuitive repetition behavior across languages and exhibits lower perplexity than regular LDA on held out data in multiple languages. Lastly, we show that our joint model improves speech retrieval performance beyond N-grams or latent topics alone, when applied to a term detection task in all languages considered.\n## Related Work\nThe use of both repetition and broad topic context have been exploited in a variety of ways by the speech recognition and retrieval communities. Cache-based or adaptive language models were 1 some of the first approaches to incorporate information beyond a short N-gram history (where N is typically 3-4 words).\n\nCache-based models assume the probability of a word in a document d is influenced both by the global frequency of that word and N-gram context as well as by the N-gram frequencies of d (or preceding cache of K words). Although most words are rare at the corpus level, when they do occur, they occur in bursts. Thus a local estimate, from the cache, may be more reliable than the global estimate. Jelinek (1991) and Kuhn (1990) both successfully applied these types of models for speech recognition, and Rosenfeld (1994), using what he referred to as 'trigger pairs', also realized significant gains in WER. More recently, recurrent neural network language models (RNNLMs) have been introduced to capture more of these \"long-term dependencies\" (Mikolov et al., 2010). In terms of speech retrieval, recent efforts have looked at exploiting repeated keywords at search time, without directly modifying the recognizer (Chiu and Rudnicky, 2013;Wintrode, 2014a).\n\nWork within the information retrieval (IR) community connects topicality with retrieval. Hearst and Plaunt (1993) reported that the \"subtopic structuring\" of documents can improve full-document retrieval. Topic models such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003) or Probabilistic Latent Semantic Analysis (PLSA) (Hofmann, 2001) are used to the augment the document-specific language model in probabilistic, language-model based IR (Wei and Croft, 2006;Chen, 2009;Liu and Croft, 2004;Chemudugunta et al., 2007). In all these cases, topic information was helpful in boosting retrieval performance above baseline vector space or N-gram models.\n\nOur proposed model closely resembles that from Chemudugunta et al. (2007), with our notions of broad and local context corresponding to their \"general and specific\"",
            "reference_string": "[10844118 | Wintrode | 2015 | Citations: 0]"
        },
        {
            "title": "A database approach to information retrieval: The remarkable relationship between language models and region models",
            "venue": "arXiv.org",
            "year": 2005,
            "reference_count": 57,
            "citation_count": 8,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1005.4752, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1691929",
                    "name": "D. Hiemstra"
                },
                {
                    "authorId": "144779604",
                    "name": "V. Mihajlovi\u0107"
                }
            ],
            "abstract": "In this report, we unify two quite distinct approaches to information retrieval: region models and language models. Region models were developed for structured document retrieval. They provide a well-defined behaviour as well as a simple query language that allows application developers to rapidly develop applications. Language models are particularly useful to reason about the ranking of search results, and for developing new ranking approaches. The unified model allows application developers to define complex language modeling approaches as logical queries on a textual database. We show a remarkable one-to-one relationship between region queries and the language models they represent for a wide variety of applications: simple ad-hoc search, cross-language retrieval, video retrieval, and web search.",
            "corpus_id": 644854,
            "sentences": [
                {
                    "corpus_id": "644854",
                    "title": "A database approach to information retrieval: The remarkable relationship between language models and region models",
                    "text": "As said in the introduction, language models form a general approach to define ranking formulas for retrieval applications. A language model is assigned to every document. The language model of the document defines the probability that the document 'generates' the query. Documents are ranked by this probability. The simplest language modeling approach to information retrieval would be defined by Equation 1. \n\nIt defines the probability of a query of length l given a document D as the product of the probabilities of each term T i (1 \u2264 i \u2264 l) given D. A language model that takes a simple product of terms, i.e., a model that assumes that the probability of one term given a document does not depend on other terms, is called a unigram language model. To make this work, we have to define the basic probability measure P (T |D); typically, it would be defined as the number of occurrences of the term T in the document D, divided by the total number of terms in the document D. For a practical query, say, retrieve all documents about \"db\" and \"ir\", we would instantiate Equation 1 as follows: \n\nThe right-hand side of the equation corresponds to the following region expression. \n\n(<doc> CONTAINING db) AND (<doc> CONTAINING ir) \n\nThis can be shown as follows: The region expression (<doc> CONTAINING db) produces all documents ranked according to P (T = db|D), i.e., all regions tagged as <doc>, ranked by the number of occurrences of db in those regions. Similarly, (<doc> CONTAINING ir) produces all documents ranked according to P (T = ir|D). Finally, the operator AND results in the regions tagged as <doc> that are in both operand sets. The score of the result regions is defined as the product of the scores of the same regions in the operands. Here, and in the remaining examples in this section, we assume that <doc> regions do not nest inside each other. We claim that there is a trivial way to rewrite the right-hand side of Equation 2 to Equation 3 while preserving the outcome. This can be shown by simply replacing P (x|y) by (y CONTAINING x), and the multiplication in Equation 2 by AND.",
                    "score": 0.39151391669839825,
                    "section_title": "The simplest unigram language model",
                    "char_start_offset": 15667,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 123
                        },
                        {
                            "start": 124,
                            "end": 171
                        },
                        {
                            "start": 172,
                            "end": 271
                        },
                        {
                            "start": 272,
                            "end": 313
                        },
                        {
                            "start": 314,
                            "end": 410
                        },
                        {
                            "start": 413,
                            "end": 755
                        },
                        {
                            "start": 756,
                            "end": 1097
                        },
                        {
                            "start": 1100,
                            "end": 1183
                        },
                        {
                            "start": 1186,
                            "end": 1233
                        },
                        {
                            "start": 1236,
                            "end": 1461
                        },
                        {
                            "start": 1462,
                            "end": 1551
                        },
                        {
                            "start": 1552,
                            "end": 1647
                        },
                        {
                            "start": 1648,
                            "end": 1756
                        },
                        {
                            "start": 1757,
                            "end": 1869
                        },
                        {
                            "start": 1870,
                            "end": 1995
                        },
                        {
                            "start": 1996,
                            "end": 2107
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.931640625
                }
            ],
            "relevance_judgement": 0.931640625,
            "relevance_judgment_input_expanded": "# Title: A database approach to information retrieval: The remarkable relationship between language models and region models\n# Venue: arXiv.org\n# Authors: D. Hiemstra, V. Mihajlovi\u0107\n## Abstract\nIn this report, we unify two quite distinct approaches to information retrieval: region models and language models. Region models were developed for structured document retrieval. They provide a well-defined behaviour as well as a simple query language that allows application developers to rapidly develop applications. Language models are particularly useful to reason about the ranking of search results, and for developing new ranking approaches. The unified model allows application developers to define complex language modeling approaches as logical queries on a textual database. We show a remarkable one-to-one relationship between region queries and the language models they represent for a wide variety of applications: simple ad-hoc search, cross-language retrieval, video retrieval, and web search.\n## The simplest unigram language model\nAs said in the introduction, language models form a general approach to define ranking formulas for retrieval applications. A language model is assigned to every document. The language model of the document defines the probability that the document 'generates' the query. Documents are ranked by this probability. The simplest language modeling approach to information retrieval would be defined by Equation 1. \n\nIt defines the probability of a query of length l given a document D as the product of the probabilities of each term T i (1 \u2264 i \u2264 l) given D. A language model that takes a simple product of terms, i.e., a model that assumes that the probability of one term given a document does not depend on other terms, is called a unigram language model. To make this work, we have to define the basic probability measure P (T |D); typically, it would be defined as the number of occurrences of the term T in the document D, divided by the total number of terms in the document D. For a practical query, say, retrieve all documents about \"db\" and \"ir\", we would instantiate Equation 1 as follows: \n\nThe right-hand side of the equation corresponds to the following region expression. \n\n(<doc> CONTAINING db) AND (<doc> CONTAINING ir) \n\nThis can be shown as follows: The region expression (<doc> CONTAINING db) produces all documents ranked according to P (T = db|D), i.e., all regions tagged as <doc>, ranked by the number of occurrences of db in those regions. Similarly, (<doc> CONTAINING ir) produces all documents ranked according to P (T = ir|D). Finally, the operator AND results in the regions tagged as <doc> that are in both operand sets. The score of the result regions is defined as the product of the scores of the same regions in the operands. Here, and in the remaining examples in this section, we assume that <doc> regions do not nest inside each other. We claim that there is a trivial way to rewrite the right-hand side of Equation 2 to Equation 3 while preserving the outcome. This can be shown by simply replacing P (x|y) by (y CONTAINING x), and the multiplication in Equation 2 by AND.",
            "reference_string": "[644854 | Hiemstra et al. | 2005 | Citations: 8]"
        },
        {
            "title": "Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation",
            "venue": "Web Search and Data Mining",
            "year": 2024,
            "reference_count": 69,
            "citation_count": 7,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2405.12119",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.12119, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "51002202",
                    "name": "Zhankui He"
                },
                {
                    "authorId": "2153722757",
                    "name": "Zhouhang Xie"
                },
                {
                    "authorId": "2290487114",
                    "name": "Harald Steck"
                },
                {
                    "authorId": "2261449339",
                    "name": "Dawen Liang"
                },
                {
                    "authorId": "2302321905",
                    "name": "Rahul Jha"
                },
                {
                    "authorId": "3174388",
                    "name": "Nathan Kallus"
                },
                {
                    "authorId": "2258552056",
                    "name": "Julian McAuley"
                }
            ],
            "abstract": "Large Language Models (LLMs) are revolutionizing conversational recommender systems (CRS) by effectively indexing item content, understanding complex conversational contexts, and generating relevant item titles. However, the autoregressive nature of LLMs, which outputs item titles as a long sequence of subtokens, hinders the ability to efficiently obtain and control recommendations across the entire item set. This challenge in calculating probabilities over all items limits LLMs' potential, such as (1) limiting control over recommendation popularities and (2) preventing the synergy of marrying LLMs and traditional recommender systems (RecSys). To address this challenge, we propose the Reindex-Then-Adapt (RTA) framework. It consists of two steps: (1) Reindex: a lightweight network learns to condense multi-token item titles into single tokens within the LLM and distills LLM-generated recommendations as ranked lists. This bypasses the autoregressive nature of LLMs while trying to preserve their CRS abilities; (2) Adapt: LLMs after reindexing enable efficient adjustment of probability distributions over single-token titles, further enhanced through RecSys integration. RTA bridges the strengths of LLMs and RecSys, enabling understanding of complex queries as LLMs do, while efficiently controlling recommended item distributions as in traditional RecSys. We show the effectiveness of our RTA over base LLMs across three CRS datasets with negligible additional parameters.",
            "corpus_id": 269921622,
            "sentences": [
                {
                    "corpus_id": "269921622",
                    "title": "Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation",
                    "text": "The transformer model has shown proficiency in retrieval tasks, encoding item information within its parameters, which is a method termed Differentiable Search Index (DSI) (Tay et al., 2022).DSI involves two key training tasks for pre-trained language models: Learn to Index (L2I) and Learn to Retrieve (L2R), which can be used to train a model jointly or in a sequential order.L2I focuses on mapping item content, such as movie description, to item indices, exemplified by linking a description of Edge of Tomorrow to its title: DSI models are originally proposed for text retrieval tasks (Tay et al., 2022), yet their formulation can be connected to LLMs used in CRS.Considering the LLMs as CRS framework proposed in (He et al., 2023) through the lens of DSI, we observe that:\n\n\u2022 Item Indexing: LLMs index items by using the item titles (e.g., \"Edge of Tomorrow\") as the item identifiers via L2I.\n\n\u2022 Item Recommendation: LLMs use conversational context as queries to generate item indices via L2R.\n\nThus, LLMs inherently function as DSI models, by including a certain number of training samples for L2I and L2R tasks in their pre-training corpus.Compared to common two-tower models, DSI models require only a single model for item recommendations, by indexing item information into its parameters (Tay et al., 2022).",
                    "score": 0.40787925531084995,
                    "section_title": "Differentiable Search Index (DSI)",
                    "char_start_offset": 6234,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 191
                        },
                        {
                            "start": 191,
                            "end": 378
                        },
                        {
                            "start": 378,
                            "end": 669
                        },
                        {
                            "start": 669,
                            "end": 778
                        },
                        {
                            "start": 780,
                            "end": 898
                        },
                        {
                            "start": 900,
                            "end": 999
                        },
                        {
                            "start": 1001,
                            "end": 1148
                        },
                        {
                            "start": 1148,
                            "end": 1318
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 172,
                            "end": 190,
                            "matchedPaperCorpusId": "246863488"
                        },
                        {
                            "start": 590,
                            "end": 607,
                            "matchedPaperCorpusId": "246863488"
                        },
                        {
                            "start": 719,
                            "end": 736,
                            "matchedPaperCorpusId": "261049680"
                        },
                        {
                            "start": 1299,
                            "end": 1317,
                            "matchedPaperCorpusId": "246863488"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92626953125
                }
            ],
            "relevance_judgement": 0.92626953125,
            "relevance_judgment_input_expanded": "# Title: Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation\n# Venue: Web Search and Data Mining\n# Authors: Zhankui He, Zhouhang Xie, Harald Steck, Dawen Liang, Rahul Jha, Nathan Kallus, Julian McAuley\n## Abstract\nLarge Language Models (LLMs) are revolutionizing conversational recommender systems (CRS) by effectively indexing item content, understanding complex conversational contexts, and generating relevant item titles. However, the autoregressive nature of LLMs, which outputs item titles as a long sequence of subtokens, hinders the ability to efficiently obtain and control recommendations across the entire item set. This challenge in calculating probabilities over all items limits LLMs' potential, such as (1) limiting control over recommendation popularities and (2) preventing the synergy of marrying LLMs and traditional recommender systems (RecSys). To address this challenge, we propose the Reindex-Then-Adapt (RTA) framework. It consists of two steps: (1) Reindex: a lightweight network learns to condense multi-token item titles into single tokens within the LLM and distills LLM-generated recommendations as ranked lists. This bypasses the autoregressive nature of LLMs while trying to preserve their CRS abilities; (2) Adapt: LLMs after reindexing enable efficient adjustment of probability distributions over single-token titles, further enhanced through RecSys integration. RTA bridges the strengths of LLMs and RecSys, enabling understanding of complex queries as LLMs do, while efficiently controlling recommended item distributions as in traditional RecSys. We show the effectiveness of our RTA over base LLMs across three CRS datasets with negligible additional parameters.\n## Differentiable Search Index (DSI)\nThe transformer model has shown proficiency in retrieval tasks, encoding item information within its parameters, which is a method termed Differentiable Search Index (DSI) (Tay et al., 2022).DSI involves two key training tasks for pre-trained language models: Learn to Index (L2I) and Learn to Retrieve (L2R), which can be used to train a model jointly or in a sequential order.L2I focuses on mapping item content, such as movie description, to item indices, exemplified by linking a description of Edge of Tomorrow to its title: DSI models are originally proposed for text retrieval tasks (Tay et al., 2022), yet their formulation can be connected to LLMs used in CRS.Considering the LLMs as CRS framework proposed in (He et al., 2023) through the lens of DSI, we observe that:\n\n\u2022 Item Indexing: LLMs index items by using the item titles (e.g., \"Edge of Tomorrow\") as the item identifiers via L2I.\n\n\u2022 Item Recommendation: LLMs use conversational context as queries to generate item indices via L2R.\n\nThus, LLMs inherently function as DSI models, by including a certain number of training samples for L2I and L2R tasks in their pre-training corpus.Compared to common two-tower models, DSI models require only a single model for item recommendations, by indexing item information into its parameters (Tay et al., 2022).",
            "reference_string": "[269921622 | He et al. | 2024 | Citations: 7]"
        },
        {
            "title": "Embedding Web-Based Statistical Translation Models in Cross-Language Information Retrieval",
            "venue": "International Conference on Computational Logic",
            "year": 2003,
            "reference_count": 116,
            "citation_count": 123,
            "influential_citation_count": 6,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://direct.mit.edu/coli/article-pdf/29/3/381/1798150/089120103322711587.pdf",
                "status": "BRONZE",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/cs/0312008, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1740640",
                    "name": "Wessel Kraaij"
                },
                {
                    "authorId": "143619007",
                    "name": "Jian-Yun Nie"
                },
                {
                    "authorId": "144866028",
                    "name": "Michel Simard"
                }
            ],
            "abstract": "Although more and more language pairs are covered by machine translation (MT) services, there are still many pairs that lack translation resources. Cross-language information retrieval (CLIR) is an application that needs translation functionality of a relatively low level of sophistication, since current models for information retrieval (IR) are still based on a bag of words. The Web provides a vast resource for the automatic construction of parallel corpora that can be used to train statistical translation models automatically. The resulting translation models can be embedded in several ways in a retrieval model. In this article, we will investigate the problem of automatically mining parallel texts from the Web and different ways of integrating the translation models within the retrieval process. Our experiments on standard test collections for CLIR show that the Web-based translation models can surpass commercial MT systems in CLIR tasks. These results open the perspective of constructing a fully automatic query translation device for CLIR at a very low cost.",
            "corpus_id": 5764728,
            "sentences": [
                {
                    "corpus_id": "5764728",
                    "title": "Embedding Web-Based Statistical Translation Models in Cross-Language Information Retrieval",
                    "text": "In Section 4.1, we have seen that the basic retrieval model measures the cross-entropy between two language models: a language model of the query and a language model of the document. 11 Instead of translating a query before estimating a query model (the external approach), we propose to directly estimate the query model in the target language. We will do this by decomposing the problem into two components that are easier to estimate: \n\nwhere L is the size of the source vocabulary. Thus, P (t i |M Qs ) can be approximated by combining the translation model P (t i |s j ), which we can estimate on the parallel Web corpus, and the familiar P (s j |M Qs ) which can be estimated using relative frequencies. This simplified model, from which we have dropped the dependency of P (t i |s j ) on Q, can be interpreted as a way of mapping the probability distribution function in the source language event space P (s j |M Qs ) onto the event space of the target language vocabulary. Since this probabilistic mapping function involves a summation over all possible translations, mapping the query model from the source language can be implemented as the matrix product of a vector representing the query probability distribution over source language terms with the translation matrix P (t i |s j ). 12 The result is a probability distribution function over the target language vocabulary. \n\nNow we can substitute the query model P (\u03c4 i |M Q ) in formula ( 7) with the target language query model in ( 8) and, after a similar substitution operation for P (\u03c4 i |M C ) , we arrive at CLIR-model QT:",
                    "score": 0.39857759446045876,
                    "section_title": "Estimating the query model in the target language (QT)",
                    "char_start_offset": 62218,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 186
                        },
                        {
                            "start": 187,
                            "end": 346
                        },
                        {
                            "start": 347,
                            "end": 438
                        },
                        {
                            "start": 441,
                            "end": 486
                        },
                        {
                            "start": 487,
                            "end": 710
                        },
                        {
                            "start": 711,
                            "end": 981
                        },
                        {
                            "start": 982,
                            "end": 1386
                        },
                        {
                            "start": 1389,
                            "end": 1593
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92626953125
                }
            ],
            "relevance_judgement": 0.92626953125,
            "relevance_judgment_input_expanded": "# Title: Embedding Web-Based Statistical Translation Models in Cross-Language Information Retrieval\n# Venue: International Conference on Computational Logic\n# Authors: Wessel Kraaij, Jian-Yun Nie, Michel Simard\n## Abstract\nAlthough more and more language pairs are covered by machine translation (MT) services, there are still many pairs that lack translation resources. Cross-language information retrieval (CLIR) is an application that needs translation functionality of a relatively low level of sophistication, since current models for information retrieval (IR) are still based on a bag of words. The Web provides a vast resource for the automatic construction of parallel corpora that can be used to train statistical translation models automatically. The resulting translation models can be embedded in several ways in a retrieval model. In this article, we will investigate the problem of automatically mining parallel texts from the Web and different ways of integrating the translation models within the retrieval process. Our experiments on standard test collections for CLIR show that the Web-based translation models can surpass commercial MT systems in CLIR tasks. These results open the perspective of constructing a fully automatic query translation device for CLIR at a very low cost.\n## Estimating the query model in the target language (QT)\nIn Section 4.1, we have seen that the basic retrieval model measures the cross-entropy between two language models: a language model of the query and a language model of the document. 11 Instead of translating a query before estimating a query model (the external approach), we propose to directly estimate the query model in the target language. We will do this by decomposing the problem into two components that are easier to estimate: \n\nwhere L is the size of the source vocabulary. Thus, P (t i |M Qs ) can be approximated by combining the translation model P (t i |s j ), which we can estimate on the parallel Web corpus, and the familiar P (s j |M Qs ) which can be estimated using relative frequencies. This simplified model, from which we have dropped the dependency of P (t i |s j ) on Q, can be interpreted as a way of mapping the probability distribution function in the source language event space P (s j |M Qs ) onto the event space of the target language vocabulary. Since this probabilistic mapping function involves a summation over all possible translations, mapping the query model from the source language can be implemented as the matrix product of a vector representing the query probability distribution over source language terms with the translation matrix P (t i |s j ). 12 The result is a probability distribution function over the target language vocabulary. \n\nNow we can substitute the query model P (\u03c4 i |M Q ) in formula ( 7) with the target language query model in ( 8) and, after a similar substitution operation for P (\u03c4 i |M C ) , we arrive at CLIR-model QT:",
            "reference_string": "[5764728 | Kraaij et al. | 2003 | Citations: 123]"
        },
        {
            "title": "HopRAG: Multi-Hop Reasoning for Logic-Aware Retrieval-Augmented Generation",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 47,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.12442, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2345972557",
                    "name": "Hao Liu"
                },
                {
                    "authorId": "2288675277",
                    "name": "Zhengren Wang"
                },
                {
                    "authorId": "2346461266",
                    "name": "Xi Chen"
                },
                {
                    "authorId": "2268429641",
                    "name": "Zhiyu Li"
                },
                {
                    "authorId": "2268399953",
                    "name": "Feiyu Xiong"
                },
                {
                    "authorId": "2289597580",
                    "name": "Qinhan Yu"
                },
                {
                    "authorId": "2344098350",
                    "name": "Wentao Zhang"
                }
            ],
            "abstract": "Retrieval-Augmented Generation (RAG) systems often struggle with imperfect retrieval, as traditional retrievers focus on lexical or semantic similarity rather than logical relevance. To address this, we propose \\textbf{HopRAG}, a novel RAG framework that augments retrieval with logical reasoning through graph-structured knowledge exploration. During indexing, HopRAG constructs a passage graph, with text chunks as vertices and logical connections established via LLM-generated pseudo-queries as edges. During retrieval, it employs a \\textit{retrieve-reason-prune} mechanism: starting with lexically or semantically similar passages, the system explores multi-hop neighbors guided by pseudo-queries and LLM reasoning to identify truly relevant ones. Experiments on multiple multi-hop benchmarks demonstrate that HopRAG's \\textit{retrieve-reason-prune} mechanism can expand the retrieval scope based on logical connections and improve final answer quality.",
            "corpus_id": 276421942,
            "sentences": [
                {
                    "corpus_id": "276421942",
                    "title": "HopRAG: Multi-Hop Reasoning for Logic-Aware Retrieval-Augmented Generation",
                    "text": "Retrieval-Augmented Generation Retrievalaugmented generation significantly improves large language models by incorporating a retrieval module that fetches relevant information from external knowledge sources (F\u00e9vry et al., 2020;Guu et al., 2020b;Izacard and Grave, 2021;Zhao et al., 2024;Yu et al., 2025). Retrieval models have evolved from early sparse retrievers, such as TF-IDF (Jones, 1973) and BM25 (Robertson and Zaragoza, 2009b), which rely on word statistics and inverted indices, to dense retrievers (Lewis et al., 2020b) that utilize neural representations for semantic matching. \n\nAdvanced methods, such as Self-RAG (Asai et al., 2023) and FLARE (Jiang et al., 2023) which determine the necessity and timing of retrieval, represent significant developments. However, the knowledge index remains logically unstructured, with each round of search considering only lexical or semantic similarity. \n\nTree&Graph-structured RAG Tree and graph are both effective structures for modeling logical relations. RAPTOR (Sarthi et al., 2023) recursively embeds, clusters, and summarizes passages, constructing a tree with differing levels of summarization from the bottom up. MemWalker (Chen et al., 2023) treats the LLM as an interactive agent walking on the tree of summarization. SiReRAG (Zhang et al., 2024) explicitly considers both similar and related information by constructing both similarity tree and relatedness tree. PG-RAG (Liang et al., 2024)",
                    "score": 0.4185848365485682,
                    "section_title": "Related Work",
                    "char_start_offset": 6447,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 305
                        },
                        {
                            "start": 306,
                            "end": 589
                        },
                        {
                            "start": 592,
                            "end": 768
                        },
                        {
                            "start": 769,
                            "end": 904
                        },
                        {
                            "start": 907,
                            "end": 1009
                        },
                        {
                            "start": 1010,
                            "end": 1172
                        },
                        {
                            "start": 1173,
                            "end": 1279
                        },
                        {
                            "start": 1280,
                            "end": 1425
                        },
                        {
                            "start": 1426,
                            "end": 1453
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 208,
                            "end": 228,
                            "matchedPaperCorpusId": "215768768"
                        },
                        {
                            "start": 246,
                            "end": 270,
                            "matchedPaperCorpusId": "220302360"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92578125
                }
            ],
            "relevance_judgement": 0.92578125,
            "relevance_judgment_input_expanded": "# Title: HopRAG: Multi-Hop Reasoning for Logic-Aware Retrieval-Augmented Generation\n# Venue: arXiv.org\n# Authors: Hao Liu, Zhengren Wang, Xi Chen, Zhiyu Li, Feiyu Xiong, Qinhan Yu, Wentao Zhang\n## Abstract\nRetrieval-Augmented Generation (RAG) systems often struggle with imperfect retrieval, as traditional retrievers focus on lexical or semantic similarity rather than logical relevance. To address this, we propose \\textbf{HopRAG}, a novel RAG framework that augments retrieval with logical reasoning through graph-structured knowledge exploration. During indexing, HopRAG constructs a passage graph, with text chunks as vertices and logical connections established via LLM-generated pseudo-queries as edges. During retrieval, it employs a \\textit{retrieve-reason-prune} mechanism: starting with lexically or semantically similar passages, the system explores multi-hop neighbors guided by pseudo-queries and LLM reasoning to identify truly relevant ones. Experiments on multiple multi-hop benchmarks demonstrate that HopRAG's \\textit{retrieve-reason-prune} mechanism can expand the retrieval scope based on logical connections and improve final answer quality.\n## Related Work\nRetrieval-Augmented Generation Retrievalaugmented generation significantly improves large language models by incorporating a retrieval module that fetches relevant information from external knowledge sources (F\u00e9vry et al., 2020;Guu et al., 2020b;Izacard and Grave, 2021;Zhao et al., 2024;Yu et al., 2025). Retrieval models have evolved from early sparse retrievers, such as TF-IDF (Jones, 1973) and BM25 (Robertson and Zaragoza, 2009b), which rely on word statistics and inverted indices, to dense retrievers (Lewis et al., 2020b) that utilize neural representations for semantic matching. \n\nAdvanced methods, such as Self-RAG (Asai et al., 2023) and FLARE (Jiang et al., 2023) which determine the necessity and timing of retrieval, represent significant developments. However, the knowledge index remains logically unstructured, with each round of search considering only lexical or semantic similarity. \n\nTree&Graph-structured RAG Tree and graph are both effective structures for modeling logical relations. RAPTOR (Sarthi et al., 2023) recursively embeds, clusters, and summarizes passages, constructing a tree with differing levels of summarization from the bottom up. MemWalker (Chen et al., 2023) treats the LLM as an interactive agent walking on the tree of summarization. SiReRAG (Zhang et al., 2024) explicitly considers both similar and related information by constructing both similarity tree and relatedness tree. PG-RAG (Liang et al., 2024)",
            "reference_string": "[276421942 | Liu et al. | 2025 | Citations: 4]"
        },
        {
            "title": "C-RAG: Certified Generation Risks for Retrieval-Augmented Language Models",
            "venue": "International Conference on Machine Learning",
            "year": 2024,
            "reference_count": 65,
            "citation_count": 22,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.03181, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2153110066",
                    "name": "Mintong Kang"
                },
                {
                    "authorId": "51274710",
                    "name": "Nezihe Merve Gurel"
                },
                {
                    "authorId": "2282538184",
                    "name": "Ning Yu"
                },
                {
                    "authorId": "2242706269",
                    "name": "D. Song"
                },
                {
                    "authorId": "2267398406",
                    "name": "Bo Li"
                }
            ],
            "abstract": "Despite the impressive capabilities of large language models (LLMs) across diverse applications, they still suffer from trustworthiness issues, such as hallucinations and misalignments. Retrieval-augmented language models (RAG) have been proposed to enhance the credibility of generations by grounding external knowledge, but the theoretical understandings of their generation risks remains unexplored. In this paper, we answer: 1) whether RAG can indeed lead to low generation risks, 2) how to provide provable guarantees on the generation risks of RAG and vanilla LLMs, and 3) what sufficient conditions enable RAG models to reduce generation risks. We propose C-RAG, the first framework to certify generation risks for RAG models. Specifically, we provide conformal risk analysis for RAG models and certify an upper confidence bound of generation risks, which we refer to as conformal generation risk. We also provide theoretical guarantees on conformal generation risks for general bounded risk functions under test distribution shifts. We prove that RAG achieves a lower conformal generation risk than that of a single LLM when the quality of the retrieval model and transformer is non-trivial. Our intensive empirical results demonstrate the soundness and tightness of our conformal generation risk guarantees across four widely-used NLP datasets on four state-of-the-art retrieval models.",
            "corpus_id": 267412330,
            "sentences": [
                {
                    "corpus_id": "267412330",
                    "title": "C-RAG: Certified Generation Risks for Retrieval-Augmented Language Models",
                    "text": "To quantify the quality of retrieval models, we introduce the concept of V rag -retrieval model, where V rag measures the variance of the contrastive loss of the retrieval model. A small V rag implies a well-trained low-variance retrieval model and can be theoretically linked to the retrieval quality, which is measured by the number of retrieved positive examples with respect to the query text. \n\nDefinition 1 (V rag -retrieval model). Consider a retrieval model with similarity measurement s \u03b8r (\u2022, \u2022) parameterized with \u03b8 r and trained with contrastive loss L cont . Let x + , x \u2212 be positive and negative samples to sample x. Consider common contrastive loss L cont = \u2212 log (\u03c3 sig (exp{s \u03b8 (x, x \u2212 ) \u2212 exp{s \u03b8 (x, x + ))), where \u03c3 sig (\u2022) is the sigmoid function. \n\nWe define a V ragretrieval model as the retrieval model with (a) a nontrivial utility such that the expected contrastive loss L \u03c4 is better than random: L \u03c4 := E[L cont ] < ln 2 (i.e., E[s \u03b8 (x, x + ) \u2212 s \u03b8 (x, x \u2212 )] > 0); and (b) bounded variance such that the training is stable and converges well: \n\nRemarks. (R1) Note that a retrieval model with random initialization can achieve E[s \u03b8 (x, x + )] = E[s \u03b8 (x, x \u2212 )] asymptotically. We merely assume a V rag -retrieval model that can nontrivially differentiate the positive from negative examples. (R2) We also assume a moderate stability with bounded variance, which implicitly assumes a moderate generalization of the retrieval model based on the variance-generalization link (Lam, 2016;Gotoh et al., 2018;Namkoong & Duchi, 2017). This is essential for the analysis as the knowledge base distribution is non-identical to the calibration/test distribution.",
                    "score": 0.4750672350269878,
                    "section_title": "Retrieval quality analysis",
                    "char_start_offset": 15568,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 178
                        },
                        {
                            "start": 179,
                            "end": 397
                        },
                        {
                            "start": 400,
                            "end": 438
                        },
                        {
                            "start": 439,
                            "end": 571
                        },
                        {
                            "start": 572,
                            "end": 631
                        },
                        {
                            "start": 632,
                            "end": 769
                        },
                        {
                            "start": 772,
                            "end": 1073
                        },
                        {
                            "start": 1076,
                            "end": 1084
                        },
                        {
                            "start": 1085,
                            "end": 1208
                        },
                        {
                            "start": 1209,
                            "end": 1323
                        },
                        {
                            "start": 1324,
                            "end": 1558
                        },
                        {
                            "start": 1559,
                            "end": 1683
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1504,
                            "end": 1515,
                            "matchedPaperCorpusId": "207234578"
                        },
                        {
                            "start": 1515,
                            "end": 1534,
                            "matchedPaperCorpusId": "52010032"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9248046875
                }
            ],
            "relevance_judgement": 0.9248046875,
            "relevance_judgment_input_expanded": "# Title: C-RAG: Certified Generation Risks for Retrieval-Augmented Language Models\n# Venue: International Conference on Machine Learning\n# Authors: Mintong Kang, Nezihe Merve Gurel, Ning Yu, D. Song, Bo Li\n## Abstract\nDespite the impressive capabilities of large language models (LLMs) across diverse applications, they still suffer from trustworthiness issues, such as hallucinations and misalignments. Retrieval-augmented language models (RAG) have been proposed to enhance the credibility of generations by grounding external knowledge, but the theoretical understandings of their generation risks remains unexplored. In this paper, we answer: 1) whether RAG can indeed lead to low generation risks, 2) how to provide provable guarantees on the generation risks of RAG and vanilla LLMs, and 3) what sufficient conditions enable RAG models to reduce generation risks. We propose C-RAG, the first framework to certify generation risks for RAG models. Specifically, we provide conformal risk analysis for RAG models and certify an upper confidence bound of generation risks, which we refer to as conformal generation risk. We also provide theoretical guarantees on conformal generation risks for general bounded risk functions under test distribution shifts. We prove that RAG achieves a lower conformal generation risk than that of a single LLM when the quality of the retrieval model and transformer is non-trivial. Our intensive empirical results demonstrate the soundness and tightness of our conformal generation risk guarantees across four widely-used NLP datasets on four state-of-the-art retrieval models.\n## Retrieval quality analysis\nTo quantify the quality of retrieval models, we introduce the concept of V rag -retrieval model, where V rag measures the variance of the contrastive loss of the retrieval model. A small V rag implies a well-trained low-variance retrieval model and can be theoretically linked to the retrieval quality, which is measured by the number of retrieved positive examples with respect to the query text. \n\nDefinition 1 (V rag -retrieval model). Consider a retrieval model with similarity measurement s \u03b8r (\u2022, \u2022) parameterized with \u03b8 r and trained with contrastive loss L cont . Let x + , x \u2212 be positive and negative samples to sample x. Consider common contrastive loss L cont = \u2212 log (\u03c3 sig (exp{s \u03b8 (x, x \u2212 ) \u2212 exp{s \u03b8 (x, x + ))), where \u03c3 sig (\u2022) is the sigmoid function. \n\nWe define a V ragretrieval model as the retrieval model with (a) a nontrivial utility such that the expected contrastive loss L \u03c4 is better than random: L \u03c4 := E[L cont ] < ln 2 (i.e., E[s \u03b8 (x, x + ) \u2212 s \u03b8 (x, x \u2212 )] > 0); and (b) bounded variance such that the training is stable and converges well: \n\nRemarks. (R1) Note that a retrieval model with random initialization can achieve E[s \u03b8 (x, x + )] = E[s \u03b8 (x, x \u2212 )] asymptotically. We merely assume a V rag -retrieval model that can nontrivially differentiate the positive from negative examples. (R2) We also assume a moderate stability with bounded variance, which implicitly assumes a moderate generalization of the retrieval model based on the variance-generalization link (Lam, 2016;Gotoh et al., 2018;Namkoong & Duchi, 2017). This is essential for the analysis as the knowledge base distribution is non-identical to the calibration/test distribution.",
            "reference_string": "[267412330 | Kang et al. | 2024 | Citations: 22]"
        },
        {
            "title": "NevIR: Negation in Neural Information Retrieval",
            "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
            "year": 2023,
            "reference_count": 56,
            "citation_count": 20,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2305.07614",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.07614, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "47433471",
                    "name": "Orion Weller"
                },
                {
                    "authorId": "2539674",
                    "name": "Dawn J Lawrie"
                },
                {
                    "authorId": "7536576",
                    "name": "Benjamin Van Durme"
                }
            ],
            "abstract": "Negation is a common everyday phenomena and has been a consistent area of weakness for language models (LMs). Although the Information Retrieval (IR) community has adopted LMs as the backbone of modern IR architectures, there has been little to no research in understanding how negation impacts neural IR. We therefore construct a straightforward benchmark on this theme: asking IR models to rank two documents that differ only by negation. We show that the results vary widely according to the type of IR architecture: cross-encoders perform best, followed by late-interaction models, and in last place are bi-encoder and sparse neural architectures. We find that most current information retrieval models do not consider negation, performing similarly or worse than randomly ranking. We show that although the obvious approach of continued fine-tuning on a dataset of contrastive documents containing negations increases performance (as does model size), there is still a large gap between machine and human performance.",
            "corpus_id": 258676146,
            "sentences": [
                {
                    "corpus_id": "258676146",
                    "title": "NevIR: Negation in Neural Information Retrieval",
                    "text": "We evaluate a wide variety of models in order to show a comprehensive evaluation across common neural IR model types. We note that although there are other models we do not use (as well as many different strategies for model training), all the major types of retrieval models are accounted for here. We evaluate on the following IR model categories: \n\nSparse We evaluate sparse IR models that use the bag-of-words representation during retrieval. This includes TF-IDF (the only non-neural IR method, here as a baseline), and two variants of SPLADE v2++ (Formal et al., 2022(Formal et al., , 2021;;Lassance and Clinchant, 2022), the ensemble distillation and selfdistillation methods. Note that other variants of SPLADE perform worse than these two methods. We do not include BM25 as implementations of BM25 perform similar to TF-IDF due to the small collection and lexical similarity within the pair. \n\nLate Interaction Late interaction models like ColBERT (Khattab and Zaharia, 2020;Santhanam et al., 2022b) embed documents and queries into one vector for each sub-word token. At inference time, these models need to compute a MaxSim operation between query vectors and document vectors to determine similarity. We use both ColBERT v1 and v2 in our experiments. 3 i-Encoders Another common category of IR models are bi-encoders, which embed both documents and queries into a single vector representation. At inference time the similarity is computed via a simple dot product or cosine similarity. Due to the popularity of this category, we include a broad spectrum: models from Sen-tenceTransformer (Reimers and Gurevych, 2019) trained on MSMarco and/or Natural Questions, DPR (Karpukhin et al., 2020), CoCondenser (Gao and Callan, 2022), and RocketQA (Qu et al., 2021;Ren et al., 2021). Note that these models span a wide variety of pre-training tasks, base models, and complex training/additional fine-tuning strategies like hard negative mining and distillation.",
                    "score": 0.42070840293321454,
                    "section_title": "Models",
                    "char_start_offset": 14035,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 117
                        },
                        {
                            "start": 118,
                            "end": 299
                        },
                        {
                            "start": 300,
                            "end": 349
                        },
                        {
                            "start": 352,
                            "end": 446
                        },
                        {
                            "start": 447,
                            "end": 683
                        },
                        {
                            "start": 684,
                            "end": 756
                        },
                        {
                            "start": 757,
                            "end": 900
                        },
                        {
                            "start": 903,
                            "end": 1077
                        },
                        {
                            "start": 1078,
                            "end": 1212
                        },
                        {
                            "start": 1213,
                            "end": 1264
                        },
                        {
                            "start": 1265,
                            "end": 1405
                        },
                        {
                            "start": 1406,
                            "end": 1497
                        },
                        {
                            "start": 1498,
                            "end": 1788
                        },
                        {
                            "start": 1789,
                            "end": 1966
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 553,
                            "end": 573,
                            "matchedPaperCorpusId": "248665596"
                        },
                        {
                            "start": 597,
                            "end": 626,
                            "matchedPaperCorpusId": "250340284"
                        },
                        {
                            "start": 957,
                            "end": 984,
                            "matchedPaperCorpusId": "216553223"
                        },
                        {
                            "start": 984,
                            "end": 1008,
                            "matchedPaperCorpusId": "244799249"
                        },
                        {
                            "start": 1600,
                            "end": 1628,
                            "matchedPaperCorpusId": "201646309"
                        },
                        {
                            "start": 1678,
                            "end": 1702,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 1716,
                            "end": 1738,
                            "matchedPaperCorpusId": "236987190"
                        },
                        {
                            "start": 1753,
                            "end": 1770,
                            "matchedPaperCorpusId": "231815627"
                        },
                        {
                            "start": 1770,
                            "end": 1787,
                            "matchedPaperCorpusId": "238857121"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92138671875
                }
            ],
            "relevance_judgement": 0.92138671875,
            "relevance_judgment_input_expanded": "# Title: NevIR: Negation in Neural Information Retrieval\n# Venue: Conference of the European Chapter of the Association for Computational Linguistics\n# Authors: Orion Weller, Dawn J Lawrie, Benjamin Van Durme\n## Abstract\nNegation is a common everyday phenomena and has been a consistent area of weakness for language models (LMs). Although the Information Retrieval (IR) community has adopted LMs as the backbone of modern IR architectures, there has been little to no research in understanding how negation impacts neural IR. We therefore construct a straightforward benchmark on this theme: asking IR models to rank two documents that differ only by negation. We show that the results vary widely according to the type of IR architecture: cross-encoders perform best, followed by late-interaction models, and in last place are bi-encoder and sparse neural architectures. We find that most current information retrieval models do not consider negation, performing similarly or worse than randomly ranking. We show that although the obvious approach of continued fine-tuning on a dataset of contrastive documents containing negations increases performance (as does model size), there is still a large gap between machine and human performance.\n## Models\nWe evaluate a wide variety of models in order to show a comprehensive evaluation across common neural IR model types. We note that although there are other models we do not use (as well as many different strategies for model training), all the major types of retrieval models are accounted for here. We evaluate on the following IR model categories: \n\nSparse We evaluate sparse IR models that use the bag-of-words representation during retrieval. This includes TF-IDF (the only non-neural IR method, here as a baseline), and two variants of SPLADE v2++ (Formal et al., 2022(Formal et al., , 2021;;Lassance and Clinchant, 2022), the ensemble distillation and selfdistillation methods. Note that other variants of SPLADE perform worse than these two methods. We do not include BM25 as implementations of BM25 perform similar to TF-IDF due to the small collection and lexical similarity within the pair. \n\nLate Interaction Late interaction models like ColBERT (Khattab and Zaharia, 2020;Santhanam et al., 2022b) embed documents and queries into one vector for each sub-word token. At inference time, these models need to compute a MaxSim operation between query vectors and document vectors to determine similarity. We use both ColBERT v1 and v2 in our experiments. 3 i-Encoders Another common category of IR models are bi-encoders, which embed both documents and queries into a single vector representation. At inference time the similarity is computed via a simple dot product or cosine similarity. Due to the popularity of this category, we include a broad spectrum: models from Sen-tenceTransformer (Reimers and Gurevych, 2019) trained on MSMarco and/or Natural Questions, DPR (Karpukhin et al., 2020), CoCondenser (Gao and Callan, 2022), and RocketQA (Qu et al., 2021;Ren et al., 2021). Note that these models span a wide variety of pre-training tasks, base models, and complex training/additional fine-tuning strategies like hard negative mining and distillation.",
            "reference_string": "[258676146 | Weller et al. | 2023 | Citations: 20]"
        },
        {
            "title": "RARe: Retrieval Augmented Retrieval with In-Context Examples",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 67,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.20088, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2303846034",
                    "name": "Atula Tejaswi"
                },
                {
                    "authorId": "2269690359",
                    "name": "Yoonsang Lee"
                },
                {
                    "authorId": "2303847572",
                    "name": "Sujay Sanghavi"
                },
                {
                    "authorId": "2304136439",
                    "name": "Eunsol Choi"
                }
            ],
            "abstract": "We investigate whether in-context examples, widely used in decoder-only language models (LLMs), can improve embedding model performance in retrieval tasks. Unlike in LLMs, naively prepending in-context examples (query-document pairs) to the target query at inference time does not work out of the box. We introduce a simple approach to enable retrievers to use in-context examples. Our approach, RARe, finetunes a pre-trained model with in-context examples whose query is semantically similar to the target query. This can be applied to adapt various base architectures (i.e., decoder-only language models, retriever models) and consistently achieves performance gains of up to +2.72% nDCG across various open-domain retrieval datasets (BeIR, RAR-b). In particular, we find RARe exhibits stronger out-of-domain generalization compared to models using queries without in-context examples, similar to what is seen for in-context learning in LLMs. We further provide analysis on the design choices of in-context example augmentation and lay the foundation for future work in this space.",
            "corpus_id": 273654180,
            "sentences": [
                {
                    "corpus_id": "273654180",
                    "title": "RARe: Retrieval Augmented Retrieval with In-Context Examples",
                    "text": "Muennighoff et al. (2024) explored providing an in-context example out-of-the-box, but showed an overall decrease in performance compared to zero-shot inference. \n\nRetrieval Large language models pre-trained with autoregressive setups (Jiang et al., 2023;Dubey et al., 2024) have shown remarkable performance when adapted to retrieval tasks (Wang et al., 2024b;BehnamGhader et al., 2024), outperforming encoder-style retrievers (Izacard et al., 2022;Wang et al., 2024a). Despite these advancements, a challenge that remains is the ability to tailor retrieval systems to specific tasks or queries. To address this, a recent line of work explores incorporating instructions into retrieval by training models to use task-specific instructions along with the query (Su et al., 2023;Asai et al., 2023). Oh et al. (2024) and Weller et al. (2024a) further propose using instructions that are specific to each query. Another well-established technique in retrieval is query expansion (Jagerman et al., 2023;Li et al., 2023b;Chen et al., 2024), where the query is augmented with additional terms to enrich the embedding as a form of relevance feedback (Lv & Zhai, 2009). Recent efforts have focused on applying LLMs to expand the original query before retrieval (Wang et al., 2023;Shen et al., 2024). These techniques are not mutually exclusive, and can be integrated with our approach.",
                    "score": 0.4185848365485682,
                    "section_title": "RELATED WORK",
                    "char_start_offset": 19642,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 161
                        },
                        {
                            "start": 164,
                            "end": 470
                        },
                        {
                            "start": 471,
                            "end": 596
                        },
                        {
                            "start": 597,
                            "end": 797
                        },
                        {
                            "start": 798,
                            "end": 908
                        },
                        {
                            "start": 909,
                            "end": 1161
                        },
                        {
                            "start": 1162,
                            "end": 1291
                        },
                        {
                            "start": 1292,
                            "end": 1377
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 428,
                            "end": 450,
                            "matchedPaperCorpusId": "249097975"
                        },
                        {
                            "start": 761,
                            "end": 778,
                            "matchedPaperCorpusId": "254853816"
                        },
                        {
                            "start": 778,
                            "end": 796,
                            "matchedPaperCorpusId": "253581733"
                        },
                        {
                            "start": 1016,
                            "end": 1034,
                            "matchedPaperCorpusId": "11816014"
                        },
                        {
                            "start": 1143,
                            "end": 1160,
                            "matchedPaperCorpusId": "12012494"
                        },
                        {
                            "start": 1253,
                            "end": 1272,
                            "matchedPaperCorpusId": "257505063"
                        },
                        {
                            "start": 1272,
                            "end": 1290,
                            "matchedPaperCorpusId": "271948010"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9208984375
                }
            ],
            "relevance_judgement": 0.9208984375,
            "relevance_judgment_input_expanded": "# Title: RARe: Retrieval Augmented Retrieval with In-Context Examples\n# Venue: arXiv.org\n# Authors: Atula Tejaswi, Yoonsang Lee, Sujay Sanghavi, Eunsol Choi\n## Abstract\nWe investigate whether in-context examples, widely used in decoder-only language models (LLMs), can improve embedding model performance in retrieval tasks. Unlike in LLMs, naively prepending in-context examples (query-document pairs) to the target query at inference time does not work out of the box. We introduce a simple approach to enable retrievers to use in-context examples. Our approach, RARe, finetunes a pre-trained model with in-context examples whose query is semantically similar to the target query. This can be applied to adapt various base architectures (i.e., decoder-only language models, retriever models) and consistently achieves performance gains of up to +2.72% nDCG across various open-domain retrieval datasets (BeIR, RAR-b). In particular, we find RARe exhibits stronger out-of-domain generalization compared to models using queries without in-context examples, similar to what is seen for in-context learning in LLMs. We further provide analysis on the design choices of in-context example augmentation and lay the foundation for future work in this space.\n## RELATED WORK\nMuennighoff et al. (2024) explored providing an in-context example out-of-the-box, but showed an overall decrease in performance compared to zero-shot inference. \n\nRetrieval Large language models pre-trained with autoregressive setups (Jiang et al., 2023;Dubey et al., 2024) have shown remarkable performance when adapted to retrieval tasks (Wang et al., 2024b;BehnamGhader et al., 2024), outperforming encoder-style retrievers (Izacard et al., 2022;Wang et al., 2024a). Despite these advancements, a challenge that remains is the ability to tailor retrieval systems to specific tasks or queries. To address this, a recent line of work explores incorporating instructions into retrieval by training models to use task-specific instructions along with the query (Su et al., 2023;Asai et al., 2023). Oh et al. (2024) and Weller et al. (2024a) further propose using instructions that are specific to each query. Another well-established technique in retrieval is query expansion (Jagerman et al., 2023;Li et al., 2023b;Chen et al., 2024), where the query is augmented with additional terms to enrich the embedding as a form of relevance feedback (Lv & Zhai, 2009). Recent efforts have focused on applying LLMs to expand the original query before retrieval (Wang et al., 2023;Shen et al., 2024). These techniques are not mutually exclusive, and can be integrated with our approach.",
            "reference_string": "[273654180 | Tejaswi et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Transforming LLMs into Cross-modal and Cross-lingual Retrieval Systems",
            "venue": "International Workshop on Spoken Language Translation",
            "year": 2024,
            "reference_count": 41,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.01616, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2294562390",
                    "name": "Frank Palma Gomez"
                },
                {
                    "authorId": "2294572013",
                    "name": "Ramon Sanabria"
                },
                {
                    "authorId": "2294570198",
                    "name": "Yun-hsuan Sung"
                },
                {
                    "authorId": "2266238595",
                    "name": "Daniel Cer"
                },
                {
                    "authorId": "35186886",
                    "name": "Siddharth Dalmia"
                },
                {
                    "authorId": "2124016663",
                    "name": "Gustavo Hern\u00e1ndez Abrego"
                }
            ],
            "abstract": "Large language models (LLMs) are trained on text-only data that go far beyond the languages with paired speech and text data. At the same time, Dual Encoder (DE) based retrieval systems project queries and documents into the same embedding space and have demonstrated their success in retrieval and bi-text mining. To match speech and text in many languages, we propose using LLMs to initialize multi-modal DE retrieval systems. Unlike traditional methods, our system doesn\u2019t require speech data during LLM pre-training and can exploit LLM\u2019s multilingual text understanding capabilities to match speech and text in languages unseen during retrieval training. Our multi-modal LLM-based retrieval system is capable of matching speech and text in 102 languages despite only training on 21 languages. Our system outperforms previous systems trained explicitly on all 102 languages. We achieve a 10% absolute improvement in Recall@1 averaged across these languages. Additionally, our model demonstrates cross-lingual speech and text matching, which is further enhanced by readily available machine translation data.",
            "corpus_id": 268856885,
            "sentences": [
                {
                    "corpus_id": "268856885",
                    "title": "Transforming LLMs into Cross-modal and Cross-lingual Retrieval Systems",
                    "text": "Large language models (LLMs) are trained on text-only data that go far beyond the languages with paired speech and text data. At the same time, Dual Encoder (DE) based retrieval systems project queries and documents into the same embedding space and have demonstrated their success in retrieval and bi-text mining. To match speech and text in many languages, we propose using LLMs to initialize multi-modal DE retrieval systems. Unlike traditional methods, our system doesn\u2019t require speech data during LLM pre-training and can exploit LLM\u2019s multilingual text understanding capabilities to match speech and text in languages unseen during retrieval training. Our multi-modal LLM-based retrieval system is capable of matching speech and text in 102 languages despite only training on 21 languages. Our system outperforms previous systems trained explicitly on all 102 languages. We achieve a 10% absolute improvement in Recall@1 averaged across these languages. Additionally, our model demonstrates cross-lingual speech and text matching, which is further enhanced by readily available machine translation data.",
                    "score": 0.3908534054751986,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9208984375
                }
            ],
            "relevance_judgement": 0.9208984375,
            "relevance_judgment_input_expanded": "# Title: Transforming LLMs into Cross-modal and Cross-lingual Retrieval Systems\n# Venue: International Workshop on Spoken Language Translation\n# Authors: Frank Palma Gomez, Ramon Sanabria, Yun-hsuan Sung, Daniel Cer, Siddharth Dalmia, Gustavo Hern\u00e1ndez Abrego\n## Abstract\nLarge language models (LLMs) are trained on text-only data that go far beyond the languages with paired speech and text data. At the same time, Dual Encoder (DE) based retrieval systems project queries and documents into the same embedding space and have demonstrated their success in retrieval and bi-text mining. To match speech and text in many languages, we propose using LLMs to initialize multi-modal DE retrieval systems. Unlike traditional methods, our system doesn\u2019t require speech data during LLM pre-training and can exploit LLM\u2019s multilingual text understanding capabilities to match speech and text in languages unseen during retrieval training. Our multi-modal LLM-based retrieval system is capable of matching speech and text in 102 languages despite only training on 21 languages. Our system outperforms previous systems trained explicitly on all 102 languages. We achieve a 10% absolute improvement in Recall@1 averaged across these languages. Additionally, our model demonstrates cross-lingual speech and text matching, which is further enhanced by readily available machine translation data.\n",
            "reference_string": "[268856885 | Gomez et al. | 2024 | Citations: 4]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "7967734",
            "title": "Matching Citation Text and Cited Spans in Biomedical Literature: a Search-Oriented Approach",
            "text": "We evaluated the performance of several retrieval models during experimentation, i.e. vector space model (Salton et al., 1975), probabilistic BM25 (Robertson and Zaragoza, 2009), divergence from randomness (DFR) (Amati and Van Rijsbergen, 2002), and language models (Ponte and Croft, 1998) with Dirichlet priors. All models showed very similar performances (with only DFR constantly underperforming all other models) and we did not observe any statistically significant differences between each set of runs. Therefore, we opted for the vector space model as our retrieval model.",
            "score": 0.5405427247367819,
            "section_title": "Retrieval model",
            "char_start_offset": 6971,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 578
                }
            ],
            "ref_mentions": [
                {
                    "start": 105,
                    "end": 126,
                    "matchedPaperCorpusId": "6473756"
                },
                {
                    "start": 212,
                    "end": 244,
                    "matchedPaperCorpusId": "7446821"
                },
                {
                    "start": 266,
                    "end": 289,
                    "matchedPaperCorpusId": "2759772"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78076171875
        },
        {
            "corpus_id": "5929353",
            "title": "Recurrent Neural Network Method in Arabic Words Recognition System",
            "text": "A language model is a function, or an algorithm for learning such a function, that captures the salient statistical characteristics of the distribution of sequences of words in a natural language typically allowing one to make probabilistic predictions of the next word given preceding ones. For example, the language model based on a big English newspaper archive is expected to assign a higher probability to -these broking houses provide a\u2016 than to -thebr oseking hou sesprose avide\u2016, because the words in the former phrase (or word pairs or word triples if so-called N-GRAM MODELS are used) occur more frequently in the data than the words in the latter phrase. For information retrieval, typical usage is to build a language model for each document. At search time the top ranked document is the one which language model assigns the highest probability to the appropriate query. \n\nIn the 1990's language models were applied as a general tool for several natural language processing applications, such as part-of-speech tagging, machine translation, and optical character recognition. Language models were applied to information retrieval by a number of research groups in the late 1990's [17]. They became rapidly popular in information retrieval research. We can express these constraints by mending the probabilities in (1) to be conditioned on some probabilistic grammar (gram), as well as the input sequence i. arg max `( | , )",
            "score": 0.515930209492505,
            "section_title": "IV. LANGUAGE MODEL",
            "char_start_offset": 12308,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 883
                },
                {
                    "start": 886,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1198
                },
                {
                    "start": 1199,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1436
                }
            ],
            "ref_mentions": [
                {
                    "start": 1193,
                    "end": 1197,
                    "matchedPaperCorpusId": "2759772"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.859375
        },
        {
            "corpus_id": "237504737",
            "title": "KFCNet: Knowledge Filtering and Contrastive Learning Network for Generative Commonsense Reasoning",
            "text": "Pre-trained language models have achieved impressive results across a wide range of NLP tasks (Devlin et al., 2019;Yang et al., 2019;Sun et al., 2019;Liu et al., 2019;Lewis et al., 2020a;Qi et al., 2020;He et al., 2020b). However, their ability to accurately reflect factual knowledge or perform logical inference is still limited. To investigate the ability of systems to capture commonsense knowledge, datasets such as CommonsenseQA (Talmor et al., 2019), SWAG (Zellers et al., 2018), and WinoGrande (Sakaguchi et al., 2020) have been proposed. Separate to these discriminative tasks that require models to choose the correct option from multiple candidates, CommonGen (Lin et al., 2020) is framed as a generation task, and requires the system to generate a logical and coherent sentence describing an everyday scenario based on a concept set. Experiments show that state-of-theart generation models are not adequate or accurate enough to generate plausible sentences or reflect commonsense assumptions in this setting. \n\nExternal knowledge provides not only information about the sorts of relationships that hold between concepts, to potentially guide generation models in capturing the implicit logic between concepts, but also interpretability. Inspired by Lewis et al. (2020b) and Fan et al. (2020), we adopt a retrieval-and-generation framework, and propose a BERT-filter and two contrastive learning modules for retrieval and generation, respectively. \n\nFor retrieval, previous research (Lewis et al., 2020b) has shown that traditional sparse vector space models, such as TF-IDF and BM25, perform better than dense representation-based retrieval on heavily entity-centric tasks such as FEVER (Thorne et al., 2018). However, while using sparse vector space retrieval models can retrieve relevant prototypes that contain a set of concepts, there can be significant domain mismatches between the retrieved results and target distribution, making it difficult for generation models to bridge between prototypes and targets. We argue that a two-stage retrieval strategy alleviates this issue by combining sparse vector space search and dense representation filters.",
            "score": 0.5019680655995774,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 1021
                },
                {
                    "start": 1024,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1459
                },
                {
                    "start": 1462,
                    "end": 1722
                },
                {
                    "start": 1723,
                    "end": 2027
                },
                {
                    "start": 2028,
                    "end": 2168
                }
            ],
            "ref_mentions": [
                {
                    "start": 94,
                    "end": 115,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 115,
                    "end": 133,
                    "matchedPaperCorpusId": "195069387"
                },
                {
                    "start": 167,
                    "end": 187,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 187,
                    "end": 203,
                    "matchedPaperCorpusId": "210164665"
                },
                {
                    "start": 435,
                    "end": 456,
                    "matchedPaperCorpusId": "53296520"
                },
                {
                    "start": 463,
                    "end": 485,
                    "matchedPaperCorpusId": "52019251"
                },
                {
                    "start": 502,
                    "end": 526,
                    "matchedPaperCorpusId": "199370376"
                },
                {
                    "start": 671,
                    "end": 689,
                    "matchedPaperCorpusId": "218500588"
                },
                {
                    "start": 1262,
                    "end": 1282,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1287,
                    "end": 1304,
                    "matchedPaperCorpusId": "227231215"
                },
                {
                    "start": 1495,
                    "end": 1516,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1700,
                    "end": 1721,
                    "matchedPaperCorpusId": "4711425"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76025390625
        },
        {
            "corpus_id": "263866951",
            "title": "Retrieval-based Language Models and Applications",
            "text": "Retrieval-based language models (LMs) have shown impressive performance on diverse NLP tasks. In this tutorial, we will provide a comprehensive and coherent overview of recent advances in retrieval-based LMs. We will start by providing preliminaries covering the foundation of LMs (e.g., masked LMs, autoregressive LMs) and retrieval systems (e.g., nearest-neighbor search). We will then detail recent progress in retrieval-based models, focusing on their model architectures and learning approaches. Finally, we will show how retrieval-based LMs are adapted to downstream applications, and extended to multilingual and multi-modal settings. Finally, we will use an exercise to showcase the effectiveness of retrieval-based LMs.",
            "score": 0.490161307119962,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.990234375
        },
        {
            "corpus_id": "270878612",
            "title": "RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs",
            "text": "In retrieval-augmented generation, a collection of documents or contexts (e.g.Wikipedia) is given, providing the grounded knowledge.Given a question q, the retriever R (e.g., a parameterized embedding model) first retrieves top-k contexts C = {c 1 , \u2022 \u2022 \u2022 , c k } that are most relevant to the question.Subsequently, the language model produces the final answer where the answer can either be a short phrase or a long sentence, depending on the type of the target task.Our focus is on autoregressive language models (OpenAI, 2022(OpenAI, , 2023;;Meta-AI, 2024), which is the most common architectures for LLMs.",
            "score": 0.48449171242432,
            "section_title": "Problem Setup",
            "char_start_offset": 7089,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 78,
                    "end": 132
                },
                {
                    "start": 132,
                    "end": 303
                },
                {
                    "start": 303,
                    "end": 469
                },
                {
                    "start": 469,
                    "end": 610
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.896484375
        },
        {
            "corpus_id": "267412330",
            "title": "C-RAG: Certified Generation Risks for Retrieval-Augmented Language Models",
            "text": "To quantify the quality of retrieval models, we introduce the concept of V rag -retrieval model, where V rag measures the variance of the contrastive loss of the retrieval model. A small V rag implies a well-trained low-variance retrieval model and can be theoretically linked to the retrieval quality, which is measured by the number of retrieved positive examples with respect to the query text. \n\nDefinition 1 (V rag -retrieval model). Consider a retrieval model with similarity measurement s \u03b8r (\u2022, \u2022) parameterized with \u03b8 r and trained with contrastive loss L cont . Let x + , x \u2212 be positive and negative samples to sample x. Consider common contrastive loss L cont = \u2212 log (\u03c3 sig (exp{s \u03b8 (x, x \u2212 ) \u2212 exp{s \u03b8 (x, x + ))), where \u03c3 sig (\u2022) is the sigmoid function. \n\nWe define a V ragretrieval model as the retrieval model with (a) a nontrivial utility such that the expected contrastive loss L \u03c4 is better than random: L \u03c4 := E[L cont ] < ln 2 (i.e., E[s \u03b8 (x, x + ) \u2212 s \u03b8 (x, x \u2212 )] > 0); and (b) bounded variance such that the training is stable and converges well: \n\nRemarks. (R1) Note that a retrieval model with random initialization can achieve E[s \u03b8 (x, x + )] = E[s \u03b8 (x, x \u2212 )] asymptotically. We merely assume a V rag -retrieval model that can nontrivially differentiate the positive from negative examples. (R2) We also assume a moderate stability with bounded variance, which implicitly assumes a moderate generalization of the retrieval model based on the variance-generalization link (Lam, 2016;Gotoh et al., 2018;Namkoong & Duchi, 2017). This is essential for the analysis as the knowledge base distribution is non-identical to the calibration/test distribution.",
            "score": 0.4750672350269878,
            "section_title": "Retrieval quality analysis",
            "char_start_offset": 15568,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 397
                },
                {
                    "start": 400,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 769
                },
                {
                    "start": 772,
                    "end": 1073
                },
                {
                    "start": 1076,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1683
                }
            ],
            "ref_mentions": [
                {
                    "start": 1504,
                    "end": 1515,
                    "matchedPaperCorpusId": "207234578"
                },
                {
                    "start": 1515,
                    "end": 1534,
                    "matchedPaperCorpusId": "52010032"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9248046875
        },
        {
            "corpus_id": "249017778",
            "title": "A Dataset for Sentence Retrieval for Open-Ended Dialogues",
            "text": "Let  denote a dialogue composed of the turns:  1 , . . . ,   ;  1 is the first turn and   is the last turn. To retrieve sentences for , we follow common practice in work on passage retrieval [5,27] and first retrieve documents. We then rank their sentences. The retrieval methods we present are unsupervised; they utilize unigram language models [6]. \n\nWe start by describing notation. We use    () to denote the maximum likelihood estimate (MLE) of term  with respect to the text, or text collection, .    () is the probability assigned to  by a Dirichlet smoothed unigram language model induced from  [51]. We compare two (unigram) language models,  1 and  2 , using the cross entropy: \n\n3.3.1 Retrieval Method. For document retrieval, we represent the dialogue  as a linear mixture of language models induced from its turns:  is a free parameter. Since the first turn,  1 , also contains the dialogue title and the subreddit, we assign it a specific weight. The document corpus is Wikipedia. A Wikipedia document, , is scored with respect to  using: \n\n). The outcome of the document retrieval step is D: the set of top- retrieved documents. \n\nThe next step is to rank the sentences  in S: the set of all sentences of documents in D. For sentence retrieval, we represent the dialogue using a mixture model again. But, in contrast to Equation 1, the emphasis now is on the last turn,   : \n\n..,  \u2212 1} and  is a free parameter (cf., time-based language models [19]). The rationale is that the next turn to be generated for the dialog should be, to some extent, a response to the last turn   . The preceding turns serve as the dialogue context and their induced language models are weighed using an exponential decay function. The direct retrieval score of  is defined as: \n\n). Finally, following common practice in work on sentence retrieval [27], we integrate the direct retrieval score of  with the retrieval score of its ambient document   : \n\nwhere  \u2032 (  ; ) and  \u2032 (; ) are the min-max normalized (  ; ) and (; ) with respect to D and S, respectively;  is a free parameter.",
            "score": 0.46789974819737173,
            "section_title": "Initial Sentence Retrieval",
            "char_start_offset": 13911,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 56
                },
                {
                    "start": 57,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 350
                },
                {
                    "start": 353,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 503
                },
                {
                    "start": 504,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 687
                },
                {
                    "start": 690,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1052
                },
                {
                    "start": 1055,
                    "end": 1057
                },
                {
                    "start": 1058,
                    "end": 1143
                },
                {
                    "start": 1146,
                    "end": 1314
                },
                {
                    "start": 1315,
                    "end": 1388
                },
                {
                    "start": 1391,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1591
                },
                {
                    "start": 1592,
                    "end": 1724
                },
                {
                    "start": 1725,
                    "end": 1770
                },
                {
                    "start": 1773,
                    "end": 1775
                },
                {
                    "start": 1776,
                    "end": 1943
                },
                {
                    "start": 1946,
                    "end": 2077
                }
            ],
            "ref_mentions": [
                {
                    "start": 194,
                    "end": 197,
                    "matchedPaperCorpusId": "31429877"
                },
                {
                    "start": 346,
                    "end": 349,
                    "matchedPaperCorpusId": "33300615"
                },
                {
                    "start": 603,
                    "end": 607,
                    "matchedPaperCorpusId": "5793143"
                },
                {
                    "start": 1459,
                    "end": 1463,
                    "matchedPaperCorpusId": "1612773"
                },
                {
                    "start": 1841,
                    "end": 1845,
                    "matchedPaperCorpusId": "31429877"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7841796875
        },
        {
            "corpus_id": "10641680",
            "title": "Combining Lexical Semantic Resources with Question & Answer Archives for Translation-Based Answer Finding",
            "text": "Statistical translation models for retrieval have first been introduced by Berger and Lafferty (1999). These models attempt to address synonymy and polysemy problems by encoding statistical word associations trained on monolingual parallel corpora. This method offers several advantages. First, it bases upon a sound mathematical formulation of the retrieval model. Second, it is not as computationally expensive as other semantic retrieval models, since it only relies on a word translation table which can easily be computed before retrieval. The main drawback lies in the availability of suitable training data for the translation probabilities. Berger and Lafferty (1999) initially built synthetic training data consisting of queries automatically generated from documents. Berger et al. (2000) proposed to train translation models on question-answer pairs taken from Usenet FAQs and call-center dialogues, with answers corresponding to the \"source\" language and questions to the \"target\" language.\n\nSubsequent work in this area often used similar kinds of training data such as question-answer pairs from Yahoo! Answers (Lee et al., 2008) or from the Wondir site (Xue et al., 2008). Lee et al. (2008) tried to further improve translation models based on question-answer pairs by selecting the most important terms to build compact translation models.\n\nOther kinds of training data have also been proposed. Jeon et al. (2005) automatically clustered semantically similar questions based on their answers. Murdock and Croft (2005) created a first parallel corpus of synonym pairs extracted from WordNet, and an additional parallel corpus of English words translating to the same Arabic term in a parallel English-Arabic corpus.\n\nSimilar work has also been performed in the area of query expansion using training data consisting of FAQ pages (Riezler et al., 2007) or queries and clicked snippets from query logs (Riezler et al., 2008).\n\nAll in all, translation models have been shown to significantly improve the retrieval results over traditional baselines for document retrieval (Berger and Lafferty, 1999), question retrieval in Question & Answer archives (Jeon et al., 2005;Lee et al., 2008;Xue et al., 2008) and for sentence retrieval (Murdock and Croft, 2005).\n\nMany of the approaches previously described have used parallel data extracted from",
            "score": 0.46644915669159726,
            "section_title": "Statistical Translation Models for Retrieval",
            "char_start_offset": 3872,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 75,
                    "end": 101,
                    "matchedPaperCorpusId": "10837883"
                },
                {
                    "start": 649,
                    "end": 675,
                    "matchedPaperCorpusId": "10837883"
                },
                {
                    "start": 778,
                    "end": 798,
                    "matchedPaperCorpusId": "6749682"
                },
                {
                    "start": 1125,
                    "end": 1143,
                    "matchedPaperCorpusId": "7742063"
                },
                {
                    "start": 1168,
                    "end": 1186,
                    "matchedPaperCorpusId": "266650254"
                },
                {
                    "start": 1188,
                    "end": 1205,
                    "matchedPaperCorpusId": "7742063"
                },
                {
                    "start": 1411,
                    "end": 1429,
                    "matchedPaperCorpusId": "1208986"
                },
                {
                    "start": 1509,
                    "end": 1533,
                    "matchedPaperCorpusId": "7188219"
                },
                {
                    "start": 1844,
                    "end": 1866,
                    "matchedPaperCorpusId": "2713391"
                },
                {
                    "start": 1915,
                    "end": 1937,
                    "matchedPaperCorpusId": "1038816"
                },
                {
                    "start": 2084,
                    "end": 2111,
                    "matchedPaperCorpusId": "10837883"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88720703125
        },
        {
            "corpus_id": "90238494",
            "title": "On the Estimation and Use of Statistical Modelling in Information Retrieval",
            "text": "A language model (LM) is, in its most basic form, a probability distribution over terms in a language where each term, t is assigned a non-zero probability denoting its probability of occurrence in the \"language\". A \"language\" here is defined as a non-empty finite sequence of symbols or terms. Given a query q and document d \u2208C for some collection C , d 's LM, \u03b8 d , is a probabilistic model that estimates the probability that q was generated by d . In other words, each document is viewed as a sample from the language, and its relevance to q is estimated as the probability that q was generated from this sample.\n\nUsing language models for retrieval in IR was initiated by Ponte and Croft [349] who developed a retrieval model based on the multiple-Bernoulli statistical model. Because the multiple-Bernoulli model is computationally expensive 10 [42] and ignores term-frequencies -a useful feature for retrieval [374] -a number of authors, most notably Hiemstra [203] and Song and Croft [403], introduced the multinomial model as a representation of documents which has since become the de facto choice [132, p. 254]. Liu and Croft [276] and Zhai [457,Chap. 6] provide a comprehensive survey of areas-relevance feedback (e.g. [131,260,348]), distributed IR (e.g. [396,453]), query ambiguity (e.g. [133,134]) and ad hoc retrieval (e.g. [200,204,231,249,269,307,309])-where LMs have been used successfully.\n\nFormally, ranking documents relative to a query q is estimated through Bayes' rule:\n\nwhere Pr(d ) denotes any prior knowledge that d is relevant to any query and Pr(q) is a normalisation constant. The generic LM in Eqn. 2.23 contains two probability distributions: the distribution of Pr(q|d ) and the distribution of Pr(d ). The probability Pr(q|d ) is usually estimated by assuming query terms are i.i.d:\n\ncorresponding to a unigram query-likelihood LM. The term-independence assumption",
            "score": 0.46644915669159726,
            "section_title": "Language Modelling",
            "char_start_offset": 41785,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 693,
                    "end": 698,
                    "matchedPaperCorpusId": "2759772"
                },
                {
                    "start": 851,
                    "end": 855,
                    "matchedPaperCorpusId": "16290284"
                },
                {
                    "start": 967,
                    "end": 972,
                    "matchedPaperCorpusId": "5242820"
                },
                {
                    "start": 992,
                    "end": 997,
                    "matchedPaperCorpusId": "8264008"
                },
                {
                    "start": 1137,
                    "end": 1142,
                    "matchedPaperCorpusId": "45558661"
                },
                {
                    "start": 1152,
                    "end": 1157,
                    "matchedPaperCorpusId": "61572040"
                },
                {
                    "start": 1231,
                    "end": 1236,
                    "matchedPaperCorpusId": "6245052"
                },
                {
                    "start": 1236,
                    "end": 1240,
                    "matchedPaperCorpusId": "14116318"
                },
                {
                    "start": 1240,
                    "end": 1244,
                    "matchedPaperCorpusId": "60342325"
                },
                {
                    "start": 1268,
                    "end": 1273,
                    "matchedPaperCorpusId": "1010654"
                },
                {
                    "start": 1273,
                    "end": 1277,
                    "matchedPaperCorpusId": "7321161"
                },
                {
                    "start": 1302,
                    "end": 1307,
                    "matchedPaperCorpusId": "14586023"
                },
                {
                    "start": 1307,
                    "end": 1311,
                    "matchedPaperCorpusId": "5214057"
                },
                {
                    "start": 1340,
                    "end": 1345,
                    "matchedPaperCorpusId": "16084542"
                },
                {
                    "start": 1345,
                    "end": 1349,
                    "matchedPaperCorpusId": "1112016"
                },
                {
                    "start": 1349,
                    "end": 1353,
                    "matchedPaperCorpusId": "14368614"
                },
                {
                    "start": 1353,
                    "end": 1357,
                    "matchedPaperCorpusId": "6498451"
                },
                {
                    "start": 1357,
                    "end": 1361,
                    "matchedPaperCorpusId": "1612773"
                },
                {
                    "start": 1361,
                    "end": 1365,
                    "matchedPaperCorpusId": "6984879"
                },
                {
                    "start": 1365,
                    "end": 1369,
                    "matchedPaperCorpusId": "851231"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98828125
        },
        {
            "corpus_id": "273345493",
            "title": "MAIR: A Massive Benchmark for Evaluating Instructed Retrieval",
            "text": "Large Language Models (LLMs) have demonstrated impressive capabilities in performing a wide range of natural language processing (NLP) tasks by being first pre-trained on large-scale corpora and then instruction-tuned on numerous downstream tasks (Chung et al., 2024;Wang et al., 2023bWang et al., , 2022b;;Wei et al., 2022). This advancement has garnered significant attention in other fields, including Information Retrieval (IR) (Gao and Callan, 2021;Neelakantan et al., 2022;Wang et al., 2022a;Izacard et al., 2021;Wang et al., 2023a;Asai et al., 2022;Su et al., 2022). \n\nIR techniques aim to retrieve a set of relevant candidates from a large corpus based on semantic relevance or other query-specific criteria (Yates et al., 2021;Fan et al., 2022). These techniques are critical components of many AI applications, from web search (Zhu et al., 2023) to various retrieval-augmented tasks (Gao et al., 2023;Shi et al., 2024b). However, most traditional IR models, typically trained on a single task, often exhibit poor generalization to other IR tasks or domains (Thakur et al., 2021;Zhao et al., 2024). Inspired by the success of LLMs, recent research explores training models for the general-purpose IR (Sachan et al., 2022;Sun et al., 2023;Oh et al., 2024;Weller et al., 2024). These models, instruction-tuned on multiple retrieval tasks, show significant improvements in aligning with the user intent across different IR tasks. \n\nTo evaluate the generalization capabilities of newly emerged IR models, several benchmarks such as BEIR (Thakur et al., 2021), KILT (Petroni et al., 2020), and MTEB (Muennighoff et al., 2022) have been established recently, compiling a variety of IR tasks.",
            "score": 0.46644915669159726,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 573
                },
                {
                    "start": 576,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1107
                },
                {
                    "start": 1108,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1435
                },
                {
                    "start": 1438,
                    "end": 1694
                }
            ],
            "ref_mentions": [
                {
                    "start": 267,
                    "end": 285,
                    "matchedPaperCorpusId": "254877310"
                },
                {
                    "start": 285,
                    "end": 307,
                    "matchedPaperCorpusId": "253098274"
                },
                {
                    "start": 307,
                    "end": 324,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 498,
                    "end": 519,
                    "matchedPaperCorpusId": "249097975"
                },
                {
                    "start": 538,
                    "end": 556,
                    "matchedPaperCorpusId": "253581733"
                },
                {
                    "start": 736,
                    "end": 753,
                    "matchedPaperCorpusId": "244714119"
                },
                {
                    "start": 911,
                    "end": 929,
                    "matchedPaperCorpusId": "270688739"
                },
                {
                    "start": 1067,
                    "end": 1088,
                    "matchedPaperCorpusId": "233296016"
                },
                {
                    "start": 1088,
                    "end": 1106,
                    "matchedPaperCorpusId": "254044526"
                },
                {
                    "start": 1209,
                    "end": 1230,
                    "matchedPaperCorpusId": "248218489"
                },
                {
                    "start": 1230,
                    "end": 1247,
                    "matchedPaperCorpusId": "258212638"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89306640625
        },
        {
            "corpus_id": "268041263",
            "title": "WIKIGENBENCH: Exploring Full-length Wikipedia Generation under Real-World Scenario",
            "text": "Enhancing language models (LMs) with retrieval during inference has become a common practice for generative tasks. By first retrieving relevant document excerpts from external knowledge base, retrieval-augmented generation models exhibit the capacity to ground on references and facilitate the generation of informative and faithful text (Li et al., 2022;Gao et al., 2023c;Guu et al., 2020). In the era of LLMs, RAG has been proven to be an effective and universal paradigm on a series of NLP tasks (Weston et al., 2018;Jiang et al., 2023). Some studies have found that retrieval can be employed to fetch in-context examples (Brown et al., 2020), thereby enhancing certain capabilities of LLMs (Huang et al., 2023;Ram et al., 2023;Chen et al., 2023). Lewis et al. (2020) first introduce RAG models which combine pre-trained seq2seq model with non-parametric dense vector index of Wikipedia and set new state-of-the-art on open domain QA tasks. To alleviate LLMs' tendency to produce hallucination, some researchers (Nakano et al., 2021) propose to integrate language model with result pages from search engine to compose the final outcome. Depending on the complexity of retrieval strategy, the system may conduct multiple retrieval process during generation and combine with techniques like Chain of Thought (Trivedi et al., 2022;Feng et al., 2023).",
            "score": 0.4653882589567758,
            "section_title": "Retrieval-Augmented Text Generation",
            "char_start_offset": 24715,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 943
                },
                {
                    "start": 944,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1350
                }
            ],
            "ref_mentions": [
                {
                    "start": 338,
                    "end": 355,
                    "matchedPaperCorpusId": "964287"
                },
                {
                    "start": 373,
                    "end": 390,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 625,
                    "end": 645,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 731,
                    "end": 749,
                    "matchedPaperCorpusId": "266176301"
                },
                {
                    "start": 751,
                    "end": 770,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57373046875
        },
        {
            "corpus_id": "5106061",
            "title": "Reranking Answers for Definitional QA Using Language Modeling",
            "text": "Specifically, the language model is utilized to capture the term dependence. A language model is a probability distribution that captures the statistical regularities of natural language use. In a language model, key elements are the probabilities of word sequences, denoted as P(w 1 , w 2 , ..., w n ) or P (w 1,n ) for short. Recently, language model has been successfully used for information retrieval (IR) (Ponte and Croft, 1998;Song and Croft, 1998;Lafferty et al., 2001;Gao et al., 2004;Cao et al., 2005). Our natural thinking is to apply language model to rank the candidate answers as it has been applied to rank search results in IR task. \n\nThe basic idea of our research is that, given a definitional question q, an ordered centroid OC which is learned from the web and a language model LM(OC) which is trained with it. Candidate answers can be ranked by probability estimated by LM(OC). A series of experiments on standard TREC 2003 collection have been conducted to evaluate bigram and biterm language models. Results show that both these two language models produce promising results by capturing the term dependence and biterm model achieves the best performance. Biterm language model interpolating with unigram model significantly improves the VSM and unigram model by 14.9% and 12.5% in F-Measure(5). \n\nIn the rest of this paper, Section 2 reviews related work. Section 3 presents details of the proposed method. Section 4 introduces the structure of our experimental system. We show the experimental results in Section 5, and conclude the paper in Section 6.",
            "score": 0.45283379289724596,
            "section_title": "Introduction",
            "char_start_offset": 2153,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 76
                },
                {
                    "start": 77,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 648
                },
                {
                    "start": 651,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1318
                },
                {
                    "start": 1321,
                    "end": 1379
                },
                {
                    "start": 1380,
                    "end": 1430
                },
                {
                    "start": 1431,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1577
                }
            ],
            "ref_mentions": [
                {
                    "start": 411,
                    "end": 434,
                    "matchedPaperCorpusId": "2759772"
                },
                {
                    "start": 455,
                    "end": 477,
                    "matchedPaperCorpusId": "1183147"
                },
                {
                    "start": 477,
                    "end": 494,
                    "matchedPaperCorpusId": "461312"
                },
                {
                    "start": 494,
                    "end": 511,
                    "matchedPaperCorpusId": "1458999"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87109375
        },
        {
            "corpus_id": "232307891",
            "title": "Retrieve Fast, Rerank Smart: Cooperative and Joint Approaches for Improved Cross-Modal Retrieval",
            "text": "We combine the benefits of the two model types (CE and BE) within a cooperative retrieval approach (SEP+COOP), as illustrated in Figure 1c. \n\nTraining and Retrieval. Two models, one CE ( \u00a73.1) and one BE ( \u00a73.2), are trained independently. Following that, the retrieval step is split into two stages. First, the efficient BE model is used to retrieve the top-k relevant items from the entire large collection, yielding a much smaller collection I k : I k = top k ({cos(i, q) : \u2200i \u2208 I}), where top k (\u2022) retrieves a set of the top-k most similar instances. Second, we rerank the instances from I k with the more precise but computationally more expensive CE model: arg max i\u2208I p(i, c). This cooperative approach thus combines the benefits of both approaches and is able to efficiently retrieve instances.9 However, given that this approach requires two models to be stored in memory, it is less parameter-efficient than the previous methods.",
            "score": 0.45107209179481156,
            "section_title": "Separate Training, Cooperative Retrieval",
            "char_start_offset": 17588,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 142,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 804
                },
                {
                    "start": 805,
                    "end": 940
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.396484375
        },
        {
            "corpus_id": "8486003",
            "title": "phi-LSTM: A Phrase-Based Hierarchical LSTM Model for Image Captioning",
            "text": "To model the relationship between image and language, some works associate both modalities by embedding their representations into a common space [17][18][19][20]. First, they obtain the image features using a visual model like CNN [18,19], as well as the representation of sentence with a language model such as recursive neural network [19]. Then, both of them are embedded into a common multimodal space and the whole model is learned with ranking objective for image and sentence retrieval task. This framework was also tested at object level by Karpathy et al. [20] and proved to yield better results for the image and sentence bi-directional retrieval task. Besides that, there are works that learn the probability density over multimodal inputs using various statistical approaches. These include Deep Boltzmann Machines [21], topic models [22], log-bilinear neural language model [8,23] and recurrent neural networks [5][6][7] etc. Such approaches fuse different input modalities together to obtain a unified representation of the inputs. It is notable to mention that there are also some works which do not explicitly learn the multimodal representation between image and language, but transit between modalities with retrieval approach. For example, Kuznetsova et al. [24] retrieve images similar to the query image from their database, and extract useful language segments (such as phrases) from the descriptions of the retrieved images.",
            "score": 0.44243839830171566,
            "section_title": "Multimodal Representation and Transition",
            "char_start_offset": 3622,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 789
                },
                {
                    "start": 790,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1448
                }
            ],
            "ref_mentions": [
                {
                    "start": 146,
                    "end": 150,
                    "matchedPaperCorpusId": "928608"
                },
                {
                    "start": 150,
                    "end": 154,
                    "matchedPaperCorpusId": "261138"
                },
                {
                    "start": 154,
                    "end": 158,
                    "matchedPaperCorpusId": "2317858"
                },
                {
                    "start": 158,
                    "end": 162,
                    "matchedPaperCorpusId": "2315434"
                },
                {
                    "start": 232,
                    "end": 236,
                    "matchedPaperCorpusId": "261138"
                },
                {
                    "start": 236,
                    "end": 239,
                    "matchedPaperCorpusId": "2317858"
                },
                {
                    "start": 338,
                    "end": 342,
                    "matchedPaperCorpusId": "2317858"
                },
                {
                    "start": 566,
                    "end": 570,
                    "matchedPaperCorpusId": "2315434"
                },
                {
                    "start": 828,
                    "end": 832,
                    "matchedPaperCorpusId": "710430"
                },
                {
                    "start": 1278,
                    "end": 1282,
                    "matchedPaperCorpusId": "13344783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93896484375
        },
        {
            "corpus_id": "16610601",
            "title": "A Comparative Study on Language Model Adaptation Techniques Using New Evaluation Metrics",
            "text": "Language model (LM) adaptation attempts to adjust the parameters of a LM so that it performs well on a particular (sub-)domain of data. Currently, most LMs are based on the Markov assumption that the prediction of a word depends only on the preceding n-1 words, but such n-gram statistics are known to be extremely susceptible to the characteristics of training samples. This is true even when the data sources are supposedly similar: for example, Rosenfeld (1996) showed that perplexity doubled when a LM trained on the Wall Street Journal (1987)(1988)(1989) was tested on the AP newswire stories of the same period. This observation, coupled with the fact that training data is available in large quantities only in selected domains, facilitates the need for LM adaptation. \n\nThere have been two formulations of the LM adaptation problem. One is the within-domain adaptation, in which adapted LMs are created for different topics in a single domain (e.g., Seymore and Rosenfeld, 1997;Clarkson and Robinson, 1997;Chen et al., 1998). In these studies, a domain is defined as a body of text originating from a single source, and the main goal of LM adaptation is to fine-tune the model parameters so as to improve the LM performance on a specific sub-domain (or topic) using the training data at hand. \n\nThe other formulation, which is the focus of the current study, is to adapt a LM to a novel domain, for which only a very small amount of training data is available. This is referred to as crossdomain adaptation. Following Bellegarda (2001), we call the domain used to train the original model the background domain, and the novel domain with a small amount of training data as the adaptation domain. Two major approaches to crossdomain adaptation have been investigated: maximum a posteriori (MAP) estimation and discriminative training methods. In MAP estimation methods, adaptation data is used to adjust the parameters of the background model so as to maximize the likelihood of the adaptation data.",
            "score": 0.4396506628741437,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 775
                },
                {
                    "start": 778,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1300
                },
                {
                    "start": 1303,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1515
                },
                {
                    "start": 1516,
                    "end": 1703
                },
                {
                    "start": 1704,
                    "end": 1849
                },
                {
                    "start": 1850,
                    "end": 2006
                }
            ],
            "ref_mentions": [
                {
                    "start": 958,
                    "end": 986,
                    "matchedPaperCorpusId": "12794839"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.49365234375
        },
        {
            "corpus_id": "233296655",
            "title": "Case-based Reasoning for Natural Language Queries over Knowledge Bases",
            "text": "Retrieval augmented LM architectures: A growing class of models (e.g., Guu et al., 2020;Lewis et al., 2020b) augments language models with a non-parametric memory, instead of solely relying on information stored in model parameters. In contrast, our CBR approach retrieves similar queries, instead of relevant supporting context, w.r.t the input query and use their solution (logical forms) to derive a new solution for the query. Recently, (Lewis et al., 2020c) also noted that the train set often contain similar questions w.r.t. the evaluation set, and concurrent work uses this insight to derive the answer to natural language questions using similar, retrieved questions (Lewis et al., 2021). Our work develops a case-based reasoning approach for KBQA and is further capable of answering compositional questions from multiple simple questions.\n\nRetrieve and edit: Our model shares similarities with the RETRIEVE-AND-EDIT framework (Hashimoto et al., 2018) which utilizes the case of the nearest-neighbor w.r.t input.\n\nTheir \"edit\" step is similar to our \"reuse\" step, however, they simply rely on the sequence-to-sequence model to generate answer from the retrieved case without \"revise\" and \"retain\" steps. Furthermore, our \"reuse\" step brings in new challenges as parametric model have to compose one SPARQL query from multiple cases in contrast to RETRIEVE-AND-EDIT that only considers a single nearest case.\n\nK-NN based approach in other NLP applications: Nearest neighbor models have been used in a number of NLP applications such as parts-of-speech tagging (Daelemans et al., 1996) and morphological analysis (Bosch et al., 2007).\n\nWiseman & Stratos (2019) achieved accurate sequence labeling by explicitly and only copying labels from retrieved neighbors. Another recent line of work use training examples at test time to improve language generation (Weston et al., 2018;Pandey et al., 2018;Cao et al., 2018;Peng et al., 2019). Khandelwal et al. (2020) also observed improvements in language modeling by utilizing explicit examples from past training data obtained via nearest neighbor search in a continuous vector space.",
            "score": 0.4396506628741437,
            "section_title": "Related Work",
            "char_start_offset": 26908,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 88,
                    "end": 108,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1568,
                    "end": 1592,
                    "matchedPaperCorpusId": "505"
                },
                {
                    "start": 1862,
                    "end": 1883,
                    "matchedPaperCorpusId": "52006529"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70556640625
        },
        {
            "corpus_id": "276812901",
            "title": "Not-Just-Scaling Laws: Towards a Better Understanding of the Downstream Impact of Language Model Design Decisions",
            "text": "To approach our research question, we built a comprehensive database of publicly available language models. Our database encompasses models spanning a wide range of sizes, from 11M to 110B parameters, 3 and includes only distinct pretrained base models with decoder-only architectures. 4 In this section, we describe the criteria used for model inclusion, how we featurized the models, as well as the evaluation suite we used.",
            "score": 0.4396506628741437,
            "section_title": "Publicly-Available Language Models",
            "char_start_offset": 6750,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 426
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.223876953125
        },
        {
            "corpus_id": "260900354",
            "title": "RAVEN: In-Context Learning with Retrieval Augmented Encoder-Decoder Language Models",
            "text": "Retrieval-augmented language models are a class of language models designed to enhance their performance by incorporating external knowledge. These models typically employ an information retrieval mechanism to access relevant information from a large corpus, which is then integrated into the model's prediction process. Retrieval-augmented LMs can be based on both encoder-decoder (Izacard et al., 2023;Lewis et al., 2020) and decoderonly (Khandelwal et al., 2020;Borgeaud et al., 2022;Shi et al., 2022) architectures. For decoder-only LMs, the computational cost typically increases quadratically with the input length, as well as with the number of retrieval passages. In contrast, for encoder-decoder LMs with a Fusion-in-Decoder architecture, the computation cost grows linearly with the number of retrieved passages, as they only perform self-attention over one passage at a time (Izacard & Grave, 2021). This concept is also investigated by Ye et al. (2023) for more efficient in-context learning. \n\nWhile there has been some research on in-context learning with retrieval-augmented decoder-only LMs, which can be straightforwardly implemented by concatenating retrieved passages with the query as the input of the LM (Mallen et al., 2022;Shi et al., 2023;Khattab et al., 2022), in-context learning with retrieval-augmented encoder-decoder LMs remains unexplored to the best of our knowledge. This is despite the fact that encoder-decoder LMs can be more efficient at incorporating multiple (e.g., 40) retrieved passages.",
            "score": 0.43869176305932234,
            "section_title": "Background and Related Work",
            "char_start_offset": 4627,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1004
                },
                {
                    "start": 1007,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1528
                }
            ],
            "ref_mentions": [
                {
                    "start": 382,
                    "end": 404,
                    "matchedPaperCorpusId": "251371732"
                },
                {
                    "start": 404,
                    "end": 423,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 440,
                    "end": 465,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 487,
                    "end": 504,
                    "matchedPaperCorpusId": "249152130"
                },
                {
                    "start": 886,
                    "end": 909,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 948,
                    "end": 964,
                    "matchedPaperCorpusId": "259370780"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95361328125
        },
        {
            "corpus_id": "7324510",
            "title": "Inducing Probabilistic Grammars by Bayesian Model Merging",
            "text": "Probabilistic modeling has become increasingly important for applications such as speech recognition, information retrieval, machine translation, and biological sequence processing. The types of models used vary widely, ranging from simple n-grams to Hidden Markov Models (HMMs) and stochastic context-free grammars (SCFGs). A central problem for these applications is to find suitable models from a corpus of samples. \n\nMost common probabilistic models can be characterized by two parts: a discrete structure (e.g., the topology of an HMM, the context-free backbone of a SCFG), and a set of continuous parameters which determine the probabilities for the words, sentences, etc. described by the grammar. Given the discrete structure, the continuous parameters can usually be fit using standard methods, such as likelihood maximization. In the case of models with hidden variables (HMMs, SCFGs) estimation typically involves expectation maximization (EM) (Baum et al. 1970;Dempster et al. 1977;Baker 1979). \n\nIn this paper we address the more difficult first half of the problem: finding the discrete structure of a probabilistic model from training data. This task includes the problems of finding the topology of an HMM, and finding the set of context-free productions for an SCFG. Our approach is called Bayesian model merging because it performs successive merging operations on the substructures of a model in an attempt to maximize the Bayesian posterior probability of the overall model structure, given the data. \n\nIn this paper, we give an introduction to Bayesian model merging for probabilistic grammar inference, and demonstrate the approach on various model types. We also report briefly on some of the applications of the resulting learning algorithms primarily in the area of natural language modeling.",
            "score": 0.4384868871211775,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 418
                },
                {
                    "start": 421,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 1006
                },
                {
                    "start": 1009,
                    "end": 1155
                },
                {
                    "start": 1156,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1520
                },
                {
                    "start": 1523,
                    "end": 1677
                },
                {
                    "start": 1678,
                    "end": 1817
                }
            ],
            "ref_mentions": [
                {
                    "start": 955,
                    "end": 973,
                    "matchedPaperCorpusId": "122568650"
                },
                {
                    "start": 973,
                    "end": 994,
                    "matchedPaperCorpusId": "4193919"
                },
                {
                    "start": 994,
                    "end": 1005,
                    "matchedPaperCorpusId": "121084921"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80224609375
        },
        {
            "corpus_id": "237513627",
            "title": "A Conditional Generative Matching Model for Multi-lingual Reply Suggestion",
            "text": "While RS has been modeled as a sequence to sequence model (Kannan et al., 2016), it more commonly appears as an information retrieval (IR) system by ranking responses from a fixed set (Henderson et al., 2017(Henderson et al., , 2019Ying et al., 2021;Swanson et al., 2019;Zhou et al., 2016Zhou et al., , 2018 due to better control over quality and relevance for practical systems. We briefly describe two retrieval architectures from prior literature which serves as the baselines for our multilingual RS model.\n\nMatching model (Henderson et al., 2017;Ying et al., 2021) consists of two parallel encoders [f \u03d5 M , f \u03d5 R ] to encode message and reply (M-R) pairs into a common encoding space, [\u0398 M , \u0398 R ] and trained to maximize a normalized dot product D = \u0398 M \u0398 R between the M-R encodings. During prediction, the model finds the nearest neighbors of \u0398 M with precomputed encodings from a fixed response set R [s] . A language model bias is typically added to promote more common responses. The matching architecture is summarized as:\n\nMatching Conditional VAE (MCVAE) (Deb et al., 2019) induces a deep generative latent variable model on the matching architecture, where a candidate response encoding is generated with \u0398 R = g w (\u0398 M , z) conditioned on a latent prior z \u223c N (0, I). The generated \u0398 R is used to match an actual response vector \u0398 R from the fixed response set. The generative model of MCVAE is shown in figure 1a. In MCVAE, the encoders [f \u03d5 M , f \u03d5 R ] are pretrained using the matching formulation and kept frozen during the training. For prediction, MCVAE samples response vectors from g w followed by scoring (eq 2) and a voting technique to rank replies over a fixed response set. MC-VAE is trained in the variational framework by minimizing the negative evidence lower-bound (ELBO) in equation 3 with a Gaussian posterior q \u03c6 (mean and co-variance parameterized from (\u0398 M , \u0398 R )) and the reconstruction loss L M defined by Eq",
            "score": 0.4360056058260431,
            "section_title": "Background and Preliminaries",
            "char_start_offset": 3542,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.833984375
        },
        {
            "corpus_id": "231704318",
            "title": "Zero-shot Neural Passage Retrieval via Domain-targeted Synthetic Question Generation",
            "text": "Neural Retrieval The retrieval vs. rescorer distinction (Figure 1) often dictates modelling choices for each task. For first-stage retrieval, as mentioned earlier, term-based models that compile document collections into inverted indexes are most common since they allow for efficient lookup (Robertson et al., 2004;Yang et al., 2017). However, there are studies that investigate neural first-stage retrieval. A common technique is to learn the term weights to be used in an inverted index (Zamani et al., 2018;Dai andCallan, 2019, 2020). Another technique is representation-based models that embed ques-  tions and passages into a common dense subspace (Palangi et al., 2016) and use nearest neighbour search for retrieval (Liu et al., 2011;Johnson et al., 2017). Recent work has shown this can be effective for passage scoring (Chang et al., 2020;Karpukhin et al., 2020;MacAvaney et al., 2020). Though all of the aforementioned first-stage neural models assume supervised data for fine-tuning. For rescoring, scoring a small set of passages permits computationally intense models. These are often called interaction-based, one-tower or crossattention models and numerous techniques have been developed (Guo et al., 2016;Hui et al., 2017;Xiong et al., 2017;Dai et al., 2018;McDonald et al., 2018), many of which employ pre-trained contextualized models (Nogueira and Cho, 2019a;MacAvaney et al., 2019;Yang et al., 2019a,b). Khattab and Zaharia (2020) also showed that by delaying interaction to the last layer, one can build a first stage retrieval model which also leverages the modeling capacity of an interaction based models. \n\nModel Transfer Previous work has attempted to alleviate reliance on large supervised training sets by pre-training deep retrieval models on weakly supervised data such as click-logs (Borisov et al., 2016;Dehghani et al., 2017). Recently, Yilmaz et al. (2019) has shown that training models on general-domain corpora adapts well to new domains without targeted supervision.",
            "score": 0.4353168619359009,
            "section_title": "Related Work",
            "char_start_offset": 4630,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 538
                },
                {
                    "start": 539,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1424
                },
                {
                    "start": 1425,
                    "end": 1630
                },
                {
                    "start": 1633,
                    "end": 1860
                },
                {
                    "start": 1861,
                    "end": 2005
                }
            ],
            "ref_mentions": [
                {
                    "start": 292,
                    "end": 316,
                    "matchedPaperCorpusId": "16628332"
                },
                {
                    "start": 316,
                    "end": 334,
                    "matchedPaperCorpusId": "1340183"
                },
                {
                    "start": 490,
                    "end": 511,
                    "matchedPaperCorpusId": "52229883"
                },
                {
                    "start": 654,
                    "end": 676,
                    "matchedPaperCorpusId": "3337266"
                },
                {
                    "start": 724,
                    "end": 742,
                    "matchedPaperCorpusId": "12833578"
                },
                {
                    "start": 829,
                    "end": 849,
                    "matchedPaperCorpusId": "211068995"
                },
                {
                    "start": 872,
                    "end": 895,
                    "matchedPaperCorpusId": "216641912"
                },
                {
                    "start": 1204,
                    "end": 1222,
                    "matchedPaperCorpusId": "5688521"
                },
                {
                    "start": 1222,
                    "end": 1239,
                    "matchedPaperCorpusId": "6246996"
                },
                {
                    "start": 1239,
                    "end": 1258,
                    "matchedPaperCorpusId": "5878197"
                },
                {
                    "start": 1258,
                    "end": 1275,
                    "matchedPaperCorpusId": "33169397"
                },
                {
                    "start": 1275,
                    "end": 1297,
                    "matchedPaperCorpusId": "52169526"
                },
                {
                    "start": 1379,
                    "end": 1402,
                    "matchedPaperCorpusId": "131776819"
                },
                {
                    "start": 1425,
                    "end": 1451,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 1815,
                    "end": 1837,
                    "matchedPaperCorpusId": "11777570"
                },
                {
                    "start": 1837,
                    "end": 1859,
                    "matchedPaperCorpusId": "3666085"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82666015625
        },
        {
            "corpus_id": "269302879",
            "title": "Planning Ahead in Generative Retrieval: Guiding Autoregressive Generation through Simultaneous Decoding",
            "text": "Classic Neural IR Models: With the emergence of large language models (LLMs) [14,17,39,43,50] and large-scale information retrieval datasets [4,32], neural-based IR models have demonstrated superior results over the traditional lexical-matching models, such as BM25 [55].In general, these IR models can fall into three categories: (1) cross-encoder models [42,48,79], (2) dense retrieval models [22,24,28,30,38,72], and (3) sparse retrieval models [12,13,18,19].The cross-encoder model is often parameterized with LLMs, such as BERT [17] or T5 [50], and takes the concatenation of query and document pair as input to predict their relevant score.This model is effective but slow and is usually used for re-ranking.As for retrieval, the dense retrieval model often uses the bi-encoder architecture to encode the query and document separately into the low-dimensional hidden space and apply the approximate nearest neighborhood (ANN) [40,68] search for fast retrieval.Sparse retrieval is an alternative method for retrieval, in which it encodes the query and document into the high-dimensional vector space, and usually, each element in the vector represents the importance score of a certain token.To filter out those useful tokens, the L1 [69] or FLOPs [18,19,44] regularizer will be incorporated into the objective function to sparsify the high-dimension vectors.For retrieval, the inverted index will be employed similar to BM25.Generative Retrieval Models: Generate Retrieval (GR), diverges from the traditional \"index-then-retrieve\" paradigm used in the sparse and dense retrieval models, offering a novel approach for document retrieval.In GR, each document is represented as a unique document identifier (DocID), and a sequence-to-sequence model is trained to generate relevant DocIDs given a query.\n\nDocIDs are usually fixed in the fine-tuning stage and hence serving as bottleneck for affecting the effectiveness of GR models.",
            "score": 0.43245092595319135,
            "section_title": "RELATED WORK",
            "char_start_offset": 5336,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 271
                },
                {
                    "start": 271,
                    "end": 462
                },
                {
                    "start": 462,
                    "end": 646
                },
                {
                    "start": 646,
                    "end": 714
                },
                {
                    "start": 714,
                    "end": 966
                },
                {
                    "start": 966,
                    "end": 1197
                },
                {
                    "start": 1197,
                    "end": 1364
                },
                {
                    "start": 1364,
                    "end": 1431
                },
                {
                    "start": 1431,
                    "end": 1642
                },
                {
                    "start": 1642,
                    "end": 1805
                },
                {
                    "start": 1807,
                    "end": 1934
                }
            ],
            "ref_mentions": [
                {
                    "start": 81,
                    "end": 84,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 90,
                    "end": 93,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 144,
                    "end": 147,
                    "matchedPaperCorpusId": "86611921"
                },
                {
                    "start": 266,
                    "end": 270,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 363,
                    "end": 366,
                    "matchedPaperCorpusId": "252993059"
                },
                {
                    "start": 399,
                    "end": 402,
                    "matchedPaperCorpusId": "233231706"
                },
                {
                    "start": 402,
                    "end": 405,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 405,
                    "end": 408,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 411,
                    "end": 414,
                    "matchedPaperCorpusId": "248426770"
                },
                {
                    "start": 452,
                    "end": 455,
                    "matchedPaperCorpusId": "252212320"
                },
                {
                    "start": 458,
                    "end": 461,
                    "matchedPaperCorpusId": "235792467"
                },
                {
                    "start": 533,
                    "end": 537,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 544,
                    "end": 548,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 932,
                    "end": 936,
                    "matchedPaperCorpusId": "8915893"
                },
                {
                    "start": 1239,
                    "end": 1243,
                    "matchedPaperCorpusId": "52229883"
                },
                {
                    "start": 1257,
                    "end": 1260,
                    "matchedPaperCorpusId": "235792467"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9921875
        },
        {
            "corpus_id": "272330251",
            "title": "A Fresh Take on Stale Embeddings: Improving Dense Retriever Training with Corrector Networks",
            "text": "Retrieval augmented language models (RLMs) typically consist of two major architectural components, a retriever model (e.g., a dual-encoder) and a generative language model or reader model (Guu et al., 2020;Izacard & Grave, 2021;Izacard et al., 2022). The input to a retrieval augmented language model is a natural language text sequence, x. This input text will be encoded using a dual-encoder retrieval model, f (x). Retrieval will be performed over a corpus of targets, Y, returning k targets relevant to x, denoted S x (Y). The reader model takes as input the retrieved targets, S x (Y), and the text x, and generates text. \n\nConcretely, in our experiments, the input text x is a question. The retrieval corpus contains targets y corresponding to passages in Wikipedia. The reader model takes as input the question and retrieved passages and generates a short answer to the question. We present the remainder of the section with this question-answering task in mind. \n\nRLMs can be formalized as latent variable models. The softmax function is used to parameterize the distribution over a discrete latent variable, which corresponds to the retrieved targets. We use a to refer to the generated sequence of text, i.e., the generated answer: \n\nP (a|y, x)P (y|x). \n\n(7) P (a|y, x) is an autoregressive language model. P (y|x) is computed by the softmax with logits from Equation 2 using the encoder models f (x) and g(y). \n\nWhen training RLMs, we receive supervision in the form of question, answer pairs, e.g., x i , a i \u223c X. We do not receive supervision on which targets S x (Y) should be retrieved. We will learn the parameters of both the reader model and retriever model using these supervised question/answer pairs. \n\nTo train the reader and retriever model, we use perplexity distillation (Izacard et al., 2022) for retriever loss and negative log-likelihood for the reader loss.",
            "score": 0.4310919816026011,
            "section_title": "Latent Variables in Retrieval Augmented Models",
            "char_start_offset": 12356,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 627
                },
                {
                    "start": 630,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 970
                },
                {
                    "start": 973,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1161
                },
                {
                    "start": 1162,
                    "end": 1242
                },
                {
                    "start": 1245,
                    "end": 1263
                },
                {
                    "start": 1266,
                    "end": 1317
                },
                {
                    "start": 1318,
                    "end": 1421
                },
                {
                    "start": 1424,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1602
                },
                {
                    "start": 1603,
                    "end": 1722
                },
                {
                    "start": 1725,
                    "end": 1887
                }
            ],
            "ref_mentions": [
                {
                    "start": 189,
                    "end": 207,
                    "matchedPaperCorpusId": "211204736"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98388671875
        },
        {
            "corpus_id": "276928764",
            "title": "Perplexity Trap: PLM-Based Retrievers Overrate Low Perplexity Documents",
            "text": "Theorem 1 reveals the linear relationship between language modeling gradients and retrieval gradients w.r.t. document embedding vectors d emb . For a more comprehensive verification for its reliability, we derive Corollay 2 from Theorem 1 and provide supporting experiments about the corollay. The derivation is similar with that in Corollary 1. Corollary 2. For two retrievers f (t; \u03b8 1 ), f (t; \u03b8 2 ) which share the same PLM, such as BERT. If retriever f (t; \u03b8 1 ) possesses more powerful language modeling ability than f (t; \u03b8 2 ), i.e., \n\nthen similar to the Corollary 1, we have \n\nNote that E d\u2208D [L 1 (d; \u03b8)] is a typical measure of language modeling ability and E d\u2208D [L 2 (d; \u03b8)] reflects the ranking performance, Corollary 2 indicates that if a retriever possesses more powerful language modeling ability, its ranking performance will be better. \n\nTo offer empirical support for the corollary, we evaluate the language modeling ability of PLM-based retrieval models with different ranking performances. By taking the retrieval model directly as a PLM encoder to do MLM task, we calculate the average text perplexity of the retrieval corpus to evaluate their language modeling ability, which offers support for the encoder-decoder corporation assumption at the same time. As illustrated in Figure 3, there is a clear correlation between text perplexity and retrieval accuracy (except Contriever). These results, demonstrating that language modeling capabilities are indeed correlated with retrieval performance, strengthen the practical reliability of our assumptions and conclusions as the deductive verification of the above hypothesis we used. This finding also explains why PLM dramatically improve the performance of retrievers over past years. Combining the previous findings, we can further understand the relationship between model retrieval performance and the degree of source bias. On one hand, if the PLM-based retriever demonstrates a better MLM capability, it tends to be more sensitive to document perplexity, which leads to more severe source bias (Corollary 1). On the other hand, a retriever with better MLM capabilities can also achieve more accurate relevance estimations, leading to better ranking performance (Corollary 2).",
            "score": 0.4310919816026011,
            "section_title": "FURTHER VERIFICATION OF THEOREM 1",
            "char_start_offset": 24153,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 109,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 541
                },
                {
                    "start": 544,
                    "end": 584
                },
                {
                    "start": 587,
                    "end": 855
                },
                {
                    "start": 858,
                    "end": 1012
                },
                {
                    "start": 1013,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1405
                },
                {
                    "start": 1406,
                    "end": 1655
                },
                {
                    "start": 1656,
                    "end": 1758
                },
                {
                    "start": 1759,
                    "end": 1901
                },
                {
                    "start": 1902,
                    "end": 2087
                },
                {
                    "start": 2088,
                    "end": 2254
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68701171875
        },
        {
            "corpus_id": "247362809",
            "title": "Internet-augmented language models through few-shot prompting for open-domain question answering",
            "text": "Undoubtedly, large-scale language models (LSLMs) present a research breakthrough for language research, particularly for their state-of-the-art language modeling results [1,2] and impressive generative capabilities. Above all, increasing scale has made few-shot learning a defining new paradigm for language models (LMs). Due to the versatility of prompting, these models can now be quickly adapted using only a handful of examples to perform tasks ranging from question answering and numeric reasoning to creative writing [3]. All these considerations place few-shot LSLMs at an excellent position to be used as building blocks for open-ended and \"in the wild\" user interactions. Despite these successes, few-shot LSLMs still lack a key ingredient; they are susceptible to hallucinations [4] caused by incorrect retrieval of knowledge stored in their weights or due to the model having incomplete or outdated knowledge. As for many user interactions we expect factuality to play an important role, it is imperative to find ways to keep LSLMs up-to-date and grounded to factual and new information as it becomes available. As the current trend sees the size of these models to continually grow, mitigating those issues should rely on flexible and robust approaches that can be easily transferred to different domains and tasks. \n\nHere, we aim to capitalize on the unique benefits offered by pre-trained LSLMs and propose to overcome some of their limitations by drawing ideas from semi-parametric models [5][6][7][8] that ground their decisions in external retrieved evidence to reduce hallucinations and improve factuality [9]. Specifically, we use the Internet as a source of up-to-date knowledge, and rely on the powerful few-shot capabilities of these LSLMs to learn how to use it effectively for answering questions. Taking open-domain question answering as a task where factual correctness is vital, we design a system that given a question uses a retrieval model to retrieve relevant documents from the Internet. Then, using few-shot learning we prompt the model to answer the question via conditioning on the retrieved documents, without the need to fine-tune or learn extra parameters. As a retrieval system we use a search engine -in particular Google Search -allowing us to treat the whole web as a knowledge source.",
            "score": 0.4310919816026011,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1327
                },
                {
                    "start": 1330,
                    "end": 1628
                },
                {
                    "start": 1629,
                    "end": 1821
                },
                {
                    "start": 1822,
                    "end": 2019
                },
                {
                    "start": 2020,
                    "end": 2194
                },
                {
                    "start": 2195,
                    "end": 2327
                }
            ],
            "ref_mentions": [
                {
                    "start": 170,
                    "end": 173,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 523,
                    "end": 526,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1510,
                    "end": 1513,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.791015625
        },
        {
            "corpus_id": "256389797",
            "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
            "text": "We introduce REPLUG, a retrieval-augmented language modeling paradigm that treats the language model as a black box and augments it with a tuneable retrieval model. \n\nOur evaluation shows that REPLUG can be integrated with any existing language model to improve their performance on language modeling or downstream tasks. This work opens up new possibilities for integrating retrieval into largescale black-box language models and demonstrates even the state-of-the-art large-scale LMs could benefit from retrieval. However, REPLUG lacks interpretability as it is unclear when the model relies on retrieved knowledge or parametric knowledge. Future research could focus on developing more interpretable retrieval-augmented language models.",
            "score": 0.4310919816026011,
            "section_title": "Conclusion",
            "char_start_offset": 25583,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 167,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 739
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.943359375
        },
        {
            "corpus_id": "270095057",
            "title": "Nearest Neighbor Speculative Decoding for LLM Generation and Attribution",
            "text": "Large language models (LLMs) have demonstrated strong potential as multi-task solvers, excelling in a wide range of applications (Brown et al., 2020;Chowdhery et al., 2022;Touvron et al., 2023a;Anil et al., 2024). Despite their advanced capabilities, LLMs frequently encounter the problem of hallucination, particularly when dealing with long-tail knowledge that is less represented in their training data (Kandpal et al., 2023;Asai et al., 2023a). To address this limitation, retrieval augmentation incorporates information retrieval and nearest neighbour search from a non-parametric data store to enhance evidence-based and situated reasoning with LLMs. The resulting semi-parametric LMs exhibit a reduced tendency to generate unsupported content (Khandelwal et al., 2020;Borgeaud et al., 2022;Shi et al., 2024a,b;Asai et al., 2023a). \n\nHowever, the effectiveness of retrieval-augmented language models (RALMs) in ensuring accurate and reliable content generation varies. The widely used in-context retrieval-augmentation (RA) regime (Ram et al., 2023;Shi et al., 2024a,b) softly biases the LM output distribution by prepending retrieved content to the input, which does not reliably guarantee faithful attribution of information. Approaches such as kNN-LM (Khandelwal et al., 2020) modify the LM output with a non-parametric Evaluated on various free-form generation tasks-including question answering, text completion, and factuality-aware generation-using Llama-2-Chat models (Touvron et al., 2023b) of different sizes, NEST demonstrates superior performance compared to both the base LM and the standard kNN-LM under a zero-shot setting. For example, combined with NEST, the Llama-2-Chat 70B model demonstrates 42.3% improvement of ROUGE-1 on WikiText-103 and 21.6% improvement of FACTSCORE on Biography.",
            "score": 0.4310919816026011,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 837
                },
                {
                    "start": 840,
                    "end": 974
                },
                {
                    "start": 975,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1644
                },
                {
                    "start": 1645,
                    "end": 1811
                }
            ],
            "ref_mentions": [
                {
                    "start": 129,
                    "end": 149,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 406,
                    "end": 428,
                    "matchedPaperCorpusId": "253522998"
                },
                {
                    "start": 428,
                    "end": 447,
                    "matchedPaperCorpusId": "263866951"
                },
                {
                    "start": 750,
                    "end": 775,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 817,
                    "end": 836,
                    "matchedPaperCorpusId": "263866951"
                },
                {
                    "start": 1037,
                    "end": 1055,
                    "matchedPaperCorpusId": "256459451"
                },
                {
                    "start": 1260,
                    "end": 1285,
                    "matchedPaperCorpusId": "207870430"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.703125
        },
        {
            "corpus_id": "263608822",
            "title": "Making Retrieval-Augmented Language Models Robust to Irrelevant Context",
            "text": "We now present our methods for building RALMs that are robust to irrelevant contexts. We begin by describing the common approach for incorporating evidence into RALMs. Next, we explore a natural baseline for using an NLI model to identify irrelevant contexts. Last, we describe our procedure for finetuning models to be robust to irrelevant context. \n\nIn-context RALMs Language models define a probability distribution over sequences of tokens, with auto-regressive models assigning a probability via next-token prediction: p LM = \u03a0 n i=1 p \u03b8 (x i |x <i ), where x <i is the sequence of tokens preceding x i at each step and \u03b8 denotes the parameters of the LM. For RALMs, we follow the definition of in-context RALMs from Ram et al. (2023), where context sentences are retrieved from a corpus C, and generation is conditioned on the retrieved context. Given the retrieval operation R C , this can be formalized as \n\n, where [R C (x <i ); x <i ] denotes the concatenation of the retrieved evidence with the generated sequence. Generation in LMs and RALMs can also be conditioned on additional input, which we omit for brevity. \n\nIn our setting, we focus on RALMs for ODQA. We follow recent approaches such as Self-Ask and IR-CoT (Press et al., 2023;Trivedi et al., 2023;Yoran et al., 2023), for interleaving retrieval with multi-hop question answering (see Fig. 3). Retrieval is performed for every intermediate question and each context is prepended to the question. In the single-hop setting, the model has to generate the answer given a question and retrieved context. In the multi-hop setting, the model has to generate intermediate questions and answers until arriving at the final answer and the retriever is called for the original question and after each intermediate question. Formally, x in this case is the generated decomposition until an intermediate step and R C (x) are the retrieved contexts for all questions in x.",
            "score": 0.4295148991627956,
            "section_title": "MAKING RALMS ROBUST TO IRRELEVANT CONTEXTS",
            "char_start_offset": 6543,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 85
                },
                {
                    "start": 86,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 349
                },
                {
                    "start": 352,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 913
                },
                {
                    "start": 916,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1125
                },
                {
                    "start": 1128,
                    "end": 1171
                },
                {
                    "start": 1172,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1466
                },
                {
                    "start": 1467,
                    "end": 1570
                },
                {
                    "start": 1571,
                    "end": 1784
                },
                {
                    "start": 1785,
                    "end": 1930
                }
            ],
            "ref_mentions": [
                {
                    "start": 722,
                    "end": 739,
                    "matchedPaperCorpusId": "256459451"
                },
                {
                    "start": 1228,
                    "end": 1248,
                    "matchedPaperCorpusId": "252762102"
                },
                {
                    "start": 1269,
                    "end": 1288,
                    "matchedPaperCorpusId": "258309779"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90087890625
        },
        {
            "corpus_id": "270357690",
            "title": "Multi-Head RAG: Solving Multi-Aspect Problems with LLMs",
            "text": "Large Language Models (LLMs) transformed many machine learning tasks using in-context learning abilities.They achieved such accuracy by leveraging an increasing number of parameters, which in recent models have grown to hundreds of billions, making LLM training expensive in terms of both time and resources.It also comes with the danger of leaking confidential data into model weights [28,33,40].Additionally, continuous training through fine-tuning is necessary to keep LLMs up-to-date.Even using the newest data, LLMs display an ongoing problem of hallucinations [13,38,44] by providing factually incorrect information.Retrieval Augmented Generation (RAG) was proposed [11,18] in order to address these issues as well as others and make LLMs more trustworthy.\n\nThe key idea behind RAG is to enhance the generative model's capabilities by integrating a retrieval system that can fetch relevant documents or passages from a large corpus of data.In this setting, when a query is received, the retrieval system first identifies and retrieves pertinent information, which is fed into the generative model's context for a more accurate and relevant response.Instead of the model storing information within its weights, RAG effectively leverages external knowledge, reducing Despite all these advances, we observe that no existing RAG scheme or evaluation methodology explicitly targets an important class of problems that come with a high degree of multi-aspectuality.These are problems that require combining several (potentially many) significantly different aspects in a single query.As a simple illustrative example of such a query, consider the question \"What car did Alexander the Great drive?\", and assume that the queried model has not been trained on history.When using RAG, to answer this question accurately, one would retrieve two documents, one describing Alexander the Great and one outlining the history of car manufacturing.However, the embeddings of these two documents could be far away from each other in the embedding space.At the same time, such queries are common in different industry settings, as indicated by extensive discussions with our industry collaborators.Imagine a chemical processing plant experiencing an equipment accident.One could use an LLM to find the accident cause, which might require the retrieval of multiple, potentially confidential documents to provide the necessary context.",
            "score": 0.4278011711348364,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 105,
                    "end": 308
                },
                {
                    "start": 308,
                    "end": 397
                },
                {
                    "start": 397,
                    "end": 488
                },
                {
                    "start": 488,
                    "end": 622
                },
                {
                    "start": 622,
                    "end": 762
                },
                {
                    "start": 764,
                    "end": 946
                },
                {
                    "start": 946,
                    "end": 1155
                },
                {
                    "start": 1155,
                    "end": 1465
                },
                {
                    "start": 1465,
                    "end": 1584
                },
                {
                    "start": 1584,
                    "end": 1765
                },
                {
                    "start": 1765,
                    "end": 1937
                },
                {
                    "start": 1937,
                    "end": 2041
                },
                {
                    "start": 2041,
                    "end": 2185
                },
                {
                    "start": 2185,
                    "end": 2256
                },
                {
                    "start": 2256,
                    "end": 2420
                }
            ],
            "ref_mentions": [
                {
                    "start": 386,
                    "end": 390,
                    "matchedPaperCorpusId": "263311025"
                },
                {
                    "start": 676,
                    "end": 679,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51220703125
        },
        {
            "corpus_id": "21807753",
            "title": "On the Equivalence of Generative and Discriminative Formulations of the Sequential Dependence Model",
            "text": "The sequential dependence model (SDM) is a popular retrieval model which is based on the theory of probabilistic graphical models. While it was originally introduced by Metzler and Croft as a Markov Random Field (aka discriminative probabilistic model), in this paper we demonstrate that it is equivalent to a generative probabilistic model. \nTo build an foundation for future retrieval models, this paper details the axiomatic underpinning of the SDM model as discriminative and generative probabilistic model. The only difference arises whether model parameters are estimated in log-space or Multinomial-space. We demonstrate that parameter-estimation with grid-tuning is negatively impacting the generative formulation, an effect that vanishes when parameters are estimated with coordinate-gradient descent. This is concerning, since empirical differences may be falsely attributed to improved models.",
            "score": 0.4268795056642635,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1087646484375
        },
        {
            "corpus_id": "248887255",
            "title": "Entailment Tree Explanations via Iterative Retrieval-Generation Reasoner",
            "text": "The IRGR-retriever module proposed in this work aims to retrieve premises from the corpus C. In existing baseline models the retrieval is done in one single step, fetching a fixed set of premises before generation . However, the generation of entailment trees requires a different set of leaves for each entailment step. To address this issue, our IRGR-retriever fetches k t premises from C to produce L t at iteration step t. Note that the size of C can be very large (k t << |C|). The value k t is chosen such that the size of L t is small enough to fit in the context of a language model while still being large enough to fetch as many premises as possible (in our experiments, the value k t is always below 25). We define the retrieval probability of a premise c \u2208 C at a certain iteration step t as:\n\nWhere \u03d5 is the sentence encoder function used to encode both premises and hypothesis, transforming the input text into a dense vector representation in\n\nThe operator \u27e8.\u27e9 represents the inner product between two vectors.\n\nThe encoder follows the Siamese Network architecture from Reimers and Gurevych (2019). We select a set of N positive and negative examples in the form of query-value pairs {(q j , c j )} N j=1 for training. Queries q j encode both the hypothesis h and previous entailment step s t\u22121 by concatenating their textual values. The positive examples are taken from the golden entailment trees, where c j \u2208 L. For negative examples, we pair a query q j with either random premises from C or premises retrieved using the not fine-tuned version of the encoder (hard negatives).\n\nWe define\u0177 j as the label given to the training example (q j , c j ). For positive examples, the label y j depends on how close the leaf node l i \u2208 L is to the intermediate step s t\u22121 in the golden tree:\n\nWhere ant(s t\u22121 ) denotes the set of antecedents in some entailment step s t\u22121 , and l i \u2208 ant(s t\u22121 ) means that the leaf node l i is used in the entailment \n\nThe value \u03bb \u2208 [0 : 1] is used to give lower",
            "score": 0.4268795056642635,
            "section_title": "IRGR-retriever",
            "char_start_offset": 8851,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1085,
                    "end": 1112,
                    "matchedPaperCorpusId": "201646309"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3681640625
        },
        {
            "corpus_id": "252089576",
            "title": "CrossDial: An Entertaining Dialogue Dataset of Chinese Crosstalk",
            "text": "We leveraged the open-source community huggingface5 to build two classes of models: generative models and retrieval models. For the CRG task, we only experimented with generative models. Whereas for the CRS task, both classes were evaluated. Retrieval models take the concatenation of the context and one candidate response as input, and score it. Generative models take the context as input, and score each candidate with its generation probability. Both classes select the highest-scored candidate as the response. \n\nIn particular, we consider the following generative baselines: \n\n\u2022 Trans: Transformer (Vaswani et al., 2017) is a SEQ2SEQ model which has been widely used in natural language processing. We adopted a Transformer-base model in the experiment. \n\n\u2022 BART: BART (Lewis et al., 2020) is a Transformer pretrained as a denoising autoencoder. We adopted BART-base6 in the experiment. \n\n\u2022 GPT (Radford and Narasimhan, 2018): GPT is also a pretrained language model. But unlike BART, it only has a Transformer decoder as backbone. We adopted GPT-27 . \u2022 T5: T5 (Raffel et al., 2020) introduced a unified framework that converts every natural language processing task into a text-to-text format. We adopted T5-base8 . \n\nWe also consider the following retrieval baselines: \n\n\u2022 Trans encoder : The encoder of the Transformer model. We adopted the Transformer-base encoder. \n\n\u2022 BERT: BERT (Devlin et al., 2019) is a pretrained Transformer encoder. Cui et al. (2021) proposed Whole Word Masking (WWM) to develop performance of original BERT in processing Chinese texts. We adopted BERTwwm-base9 . \n\n\u2022 RoBERTa: RoBERTa (Liu et al., 2019) is built upon BERT with modified key hyperparameters. We adopted RoBERTa-wwmbase10 in the experiment. The Whole Word Masking technique is also utilized. \n\n\u2022 ERNIE: ERNIE (Sun et al., 2019) incorporates knowledge masking strategies to learn better language representation. We adopted ERNIE-1.011 .",
            "score": 0.4268795056642635,
            "section_title": "Baseline Methods",
            "char_start_offset": 17455,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 516
                },
                {
                    "start": 519,
                    "end": 581
                },
                {
                    "start": 584,
                    "end": 705
                },
                {
                    "start": 706,
                    "end": 760
                },
                {
                    "start": 763,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 893
                },
                {
                    "start": 896,
                    "end": 974
                },
                {
                    "start": 975,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1058
                },
                {
                    "start": 1059,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1223
                },
                {
                    "start": 1226,
                    "end": 1277
                },
                {
                    "start": 1280,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1376
                },
                {
                    "start": 1379,
                    "end": 1450
                },
                {
                    "start": 1451,
                    "end": 1571
                },
                {
                    "start": 1572,
                    "end": 1598
                },
                {
                    "start": 1601,
                    "end": 1692
                },
                {
                    "start": 1693,
                    "end": 1740
                },
                {
                    "start": 1741,
                    "end": 1791
                },
                {
                    "start": 1794,
                    "end": 1910
                },
                {
                    "start": 1911,
                    "end": 1935
                }
            ],
            "ref_mentions": [
                {
                    "start": 605,
                    "end": 627,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 776,
                    "end": 795,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 1068,
                    "end": 1089,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1392,
                    "end": 1412,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1451,
                    "end": 1468,
                    "matchedPaperCorpusId": "195068911"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89599609375
        },
        {
            "corpus_id": "10959945",
            "title": "Two decades of statistical language modeling: where do we go from here?",
            "text": "Statistical language models estimate the distribution of various natural language phenomena for the purpose of speech recognition and other language technologies. Since the first significant model was proposed in 1980, many attempts have been made to improve the state of the art. We review them, point to a few promising directions, and argue for a Bayesian approach to integration of linguistic theories with data.",
            "score": 0.4268795056642635,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.244140625
        },
        {
            "corpus_id": "18819434",
            "title": "Language Model-Based Document Clustering Using Random Walks",
            "text": "The language modeling approach to information retrieval was first introduced by Ponte and Croft (1998) as an alternative (or an improvement) to the traditional \u00a1\u00a2 \u00a3 \u00a4\u00a5\u00a2 relevance models. In the language modeling framework, each document in the database defines a language model. The relevance of a document to a given query is ranked according to the generation probability of the query based on the underlying language model of the document. To induce a (unigram) language model from a document, we start with the maximum likelihood (ML) estimation of the term probabilities. For each term that occurs in a document , the ML estimation of with respect to is defined as \n\nwhere %& is the number of occurences of term in document . This estimation is often smoothed based on the following general formula: \n\nwhere ! \" # 7 89 @ A is the ML estimation of over an entire corpus which usually is a member of. \n\n3 is the general smoothing parameter that takes different forms in various smoothing methods. Smoothing has two important roles (Zhai and Lafferty, 2004). First, it accounts for terms unseen in the document preventing zero probabilities. This is similar to the smoothing effect in NLP problems such as parsing. Second, smoothing has an \u00a4\u00a5\u00a2like effect that accounts for the generation probabilities of the common terms in the corpus. A common smoothing technique is to use Bayesian smoothing with the Dirichlet prior (Zhai and Lafferty, 2004;Liu and Croft, 2004): \n\nHere, B is the smoothing parameter. Higher values of B mean more aggressive smoothing. \n\nAssuming the terms in a text are independent from each other, the generation probability of a text sequence C \n\ngiven the document is the product of the generation probabilities of the terms of \n\nIn the context of information retrieval, C is a query usually composed of few terms. In this work, we are interested in the generation probabilities of entire documents that usually have in the order of hundreds of unique terms. If we use Equation 1, we end up having unnatural probabilities which are irrepresentably small and cause floating point underflow. More importantly, longer documents tend to have much smaller generation probabilities no matter how closely related they are to the generating language model.",
            "score": 0.4263006436547978,
            "section_title": "Language Models",
            "char_start_offset": 3082,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 669
                },
                {
                    "start": 672,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 804
                },
                {
                    "start": 807,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 903
                },
                {
                    "start": 906,
                    "end": 999
                },
                {
                    "start": 1000,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1216
                },
                {
                    "start": 1217,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1468
                },
                {
                    "start": 1471,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1557
                },
                {
                    "start": 1560,
                    "end": 1669
                },
                {
                    "start": 1672,
                    "end": 1753
                },
                {
                    "start": 1756,
                    "end": 1840
                },
                {
                    "start": 1841,
                    "end": 1984
                },
                {
                    "start": 1985,
                    "end": 2115
                },
                {
                    "start": 2116,
                    "end": 2274
                }
            ],
            "ref_mentions": [
                {
                    "start": 80,
                    "end": 102,
                    "matchedPaperCorpusId": "2759772"
                },
                {
                    "start": 1034,
                    "end": 1059,
                    "matchedPaperCorpusId": "207670589"
                },
                {
                    "start": 1422,
                    "end": 1447,
                    "matchedPaperCorpusId": "207670589"
                },
                {
                    "start": 1447,
                    "end": 1467,
                    "matchedPaperCorpusId": "15479915"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97265625
        },
        {
            "corpus_id": "7188219",
            "title": "A Translation Model for Sentence Retrieval",
            "text": "Sentence retrieval is the task of retrieving a relevant sentence in response to a user's query. Tasks such as question answering, novelty detection and summarization often incorporate a sentence retrieval module. In previous work we examined sentence retrieval for question answering (Murdock and Croft, 2004). This involves the comparison of two well-formed sentences, one a question, one a statement. In this work we compare well-formed sentences to queries, which can be typical keyword queries of 1 to 3 terms, or a set of sentences or sentence fragments. The TREC Novelty Track provides this type of data in the form of topic titles and descriptions, and sentence-level relevance judgments for a small subset of the collection.\n\nWe present a translation model specifically for monolingual data, and show that it significantly improves sentence retrieval over querylikelihood. Translation models train on a parallel corpus and in previous work we used a corpus of question/answer pairs. No such corpus is available for the novelty data, so in this paper we present four ways to construct a parallel corpus, to estimate a translation model. Many systems treat sentence retrieval as a type of document or passage retrieval. In our data a sentence is an average of 18 words, most of which occur once. A document is an average of 700 words, many of which are multiples of the same term. It is much less likely for a word and its synonym terms to appear in the same sentence than in the same document.\n\nPassages may be any length, either fixed or variable, but are somewhat arbitrarily designated. Many systems that have a passage retrieval module, on closer inspection have defined the passage to be a sentence. What is needed is a sentence retrieval mechanism that retains the benefits of passage retrieval, where a passage is longer than a sentence. We propose that smoothing from the local context of the sentence improves retrieval over the query likelihood baseline, and the larger the context, the greater the improvement.\n\nWe describe our translation model in section 2, along with our smoothing approach. In section 3 we discuss previous work in sentence retrieval for the Novelty task, and translation models for information retrieval tasks. Section 4 presents four ways to estimate a translation model, in the absence of a parallel corpus, and presents our experimental results. We discuss the results in section 5, and present our conclusions and future work in section 6",
            "score": 0.4227585019450485,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 284,
                    "end": 309,
                    "matchedPaperCorpusId": "10055458"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.626953125
        },
        {
            "corpus_id": "252992587",
            "title": "Language Model Decomposition: Quantifying the Dependency and Correlation of Language Models",
            "text": "The dependency R 2 (u i , {u \u2212i }) (as defined in Eq (9)) and correlation \u03c1({u i } n i=1 ) (as defined in Eq (10)) of multiple language models are show in Figure 5, and the exact numbers are in Table 3. The results are similar to the results on English Wikipedia and BooksCorpus (see Figure 2,Table 2.). 9)) of language models. The Group Corr refers to the group correlation measure \u03c1({u i } n i=1 ) (as defined in Eq (10)). T is the text sequence length. We take each model as target model, and apply LMD using the remaining eleven (11) models as basis (as defined in Eq (1)).",
            "score": 0.4227106823263234,
            "section_title": "Dependency and Correlation of Multiple Language Models",
            "char_start_offset": 16946,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 577
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0953369140625
        },
        {
            "corpus_id": "270045368",
            "title": "Learning Beyond Pattern Matching? Assaying Mathematical Understanding in LLMs",
            "text": "A language model (LM) is a statistical model of natural language.Given a sentence s with tokens w 1 , . . ., w t , the probability of the sentence in a LM parameterized by \u03b8 can be represented as a chain of conditional probability conditioned on all the previous tokens:\n\nPre-training a language model amounts to likelihood maximization for the probability of next token prediction p \u03b8 (w t |w 1 , . . ., w t\u22121 ) on a held-out dataset.\n\nIn-context learning LM is given a k-shot example of context at inference time before the context of the test question, which the LM is expected to complete.",
            "score": 0.4227106823263234,
            "section_title": "Language Model",
            "char_start_offset": 7006,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 65
                },
                {
                    "start": 65,
                    "end": 107
                },
                {
                    "start": 107,
                    "end": 270
                },
                {
                    "start": 272,
                    "end": 403
                },
                {
                    "start": 403,
                    "end": 435
                },
                {
                    "start": 437,
                    "end": 593
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3291015625
        },
        {
            "corpus_id": "269187839",
            "title": "More room for language: Investigating the effect of retrieval on language models",
            "text": "Retrieval-augmented language models pose a promising alternative to standard language modeling. During pretraining, these models search in a corpus of documents for contextually relevant information that could aid the language modeling objective. We introduce an \u2018ideal retrieval\u2019 methodology to study these models in a fully controllable setting. We conduct an extensive evaluation to examine how retrieval augmentation affects the behavior of the underlying language model. Among other things, we observe that these models: (i) save substantially less world knowledge in their weights, (ii) are better at understanding local context and inter-word dependencies, but (iii) are worse at comprehending global context.",
            "score": 0.4227106823263234,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8837890625
        },
        {
            "corpus_id": "242726610",
            "title": "The impact of reading fluency level on interactive information retrieval",
            "text": "We used trigram language models with interpolated Kneser-Kney discounting trained using the SRI language modeling toolkit [43]. We generated different models by varying the training corpus. More concisely, we used the Simple-wiki, Sphinx-70k and ClueWeb12 corpora. Simple-wiki [12] contains 137K sentence Simple English Wikipedia articles. Sphinx-70k uses CMUSphinx US English generic acoustic model3 , is the most general language model that we have considered and the best suited to represent the English language. Finally, a list of documents was retrieved from ClueWeb12 by using every set of queries related to each topic. To this end, an online ClueWeb12 search service 4 was applied in order to retrieve the 100 first ranked documents. As a consequence, we obtained a different language model for each topic proposed in the PIR-CLEF dataset. \n\nOnce statistical language models are calculated, the ranked list of documents for each user and query are clustered by following a criteria on the basis of user assessments on these ranks: \n\n-Relevant documents (user relevance judgment is 3 or 4); -Non-relevant documents (user relevance judgment is 1 or 2); -Documents without user assessments (there is no user relevance judgment in spite of the fact that they are part of the ranked list of documents retrieved. As a consequence, those documents are unread by users). \n\nFinally, the perplexity of these three different sets of documents per each user and query pair was measured to test if there were statistically significant differences between these measures.",
            "score": 0.4227106823263234,
            "section_title": "Text complexity calculus on the basis of perplexity",
            "char_start_offset": 20665,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 848
                },
                {
                    "start": 851,
                    "end": 1039
                },
                {
                    "start": 1042,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1371
                },
                {
                    "start": 1374,
                    "end": 1566
                }
            ],
            "ref_mentions": [
                {
                    "start": 122,
                    "end": 126,
                    "matchedPaperCorpusId": "1988103"
                },
                {
                    "start": 277,
                    "end": 281,
                    "matchedPaperCorpusId": "9128245"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.465087890625
        },
        {
            "corpus_id": "268819582",
            "title": "RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation",
            "text": "Recent advancements in Large Language Models (LLMs) (OpenAI, 2023;Ouyang et al., 2022;Touvron et al., 2023) have demonstrated significant capabilities in understanding various concepts and solving downstream tasks (Brown et al., 2020;Raffel et al., 2020).Despite the vast amounts of data leveraged during their initial training or subsequent fine-tuning phases, these models inherently remain static.Once built and updated to a specific point in time, their knowledge base ceases to evolve, precluding the incorporation of new, real-time information.This limitation confines them to rely solely on their pre-encoded parametric knowledge (Mallen et al., 2022) during inference.Lacking access to up-to-date information, LLMs are prone to generating hallucinations (Ji et al., 2023) and may struggle to provide accurate and timely responses to queries that demand the latest information (Vu et al., 2023).\n\nTo overcome these challenges, integrating retrieval functionalities into generative models presents a promising solution (Lewis et al., 2020;Luo et al., 2023).This approach enriches standard parametric language models with non-parametric retrieval elements capable of accessing relevant information from external databases.These databases range from comprehensive document repositories, Although such enhancements could mitigate the tendency towards inaccuracies and maintain the utility of LLMs in rapidly evolving domains, there still exist several issues in the previous framework.Firstly, the indiscriminate use of information retrieval systems to contextualize queries can be counterproductive.As demonstrated in previous research (Shi et al., 2023a), irrelevant context not only diminishes generation quality but may also obstruct LLMs' ability to answer queries they are otherwise capable of addressing.For straightforward queries like daily greetings, LLMs should respond directly rather than incorporating unnecessary context, thereby avoiding low-quality responses that could deter users.That is, the model should learn to search on demand, see Figure 1 (top-left).Secondly, for complex queries, simply searching with the original query often fails to retrieve adequate information.",
            "score": 0.4227106823263234,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 255
                },
                {
                    "start": 255,
                    "end": 400
                },
                {
                    "start": 400,
                    "end": 550
                },
                {
                    "start": 550,
                    "end": 676
                },
                {
                    "start": 676,
                    "end": 902
                },
                {
                    "start": 904,
                    "end": 1063
                },
                {
                    "start": 1063,
                    "end": 1227
                },
                {
                    "start": 1227,
                    "end": 1488
                },
                {
                    "start": 1488,
                    "end": 1603
                },
                {
                    "start": 1603,
                    "end": 1814
                },
                {
                    "start": 1814,
                    "end": 2002
                },
                {
                    "start": 2002,
                    "end": 2079
                },
                {
                    "start": 2079,
                    "end": 2196
                }
            ],
            "ref_mentions": [
                {
                    "start": 66,
                    "end": 86,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 214,
                    "end": 234,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 234,
                    "end": 254,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1025,
                    "end": 1045,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1640,
                    "end": 1659,
                    "matchedPaperCorpusId": "256459776"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66357421875
        },
        {
            "corpus_id": "4954947",
            "title": "Design of the Moses Decoder for Statistical Machine Translation",
            "text": "Language models are abstracted to enable different implementations to be used and provide a framework for more complex models such as factored LM and the Bloom filter language model (Talbot and Osborne 2007). Similarly, phrase tables are abstracted to provide support for multiple implementations.\n\nEach component model which contributes to the log-linear hypothesis score inherits from the ScoreProducer base class, Figure 3.",
            "score": 0.4227106823263234,
            "section_title": "Figure 2 Input Types",
            "char_start_offset": 11946,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 182,
                    "end": 207,
                    "matchedPaperCorpusId": "11511571"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.44580078125
        },
        {
            "corpus_id": "258676146",
            "title": "NevIR: Negation in Neural Information Retrieval",
            "text": "We evaluate a wide variety of models in order to show a comprehensive evaluation across common neural IR model types. We note that although there are other models we do not use (as well as many different strategies for model training), all the major types of retrieval models are accounted for here. We evaluate on the following IR model categories: \n\nSparse We evaluate sparse IR models that use the bag-of-words representation during retrieval. This includes TF-IDF (the only non-neural IR method, here as a baseline), and two variants of SPLADE v2++ (Formal et al., 2022(Formal et al., , 2021;;Lassance and Clinchant, 2022), the ensemble distillation and selfdistillation methods. Note that other variants of SPLADE perform worse than these two methods. We do not include BM25 as implementations of BM25 perform similar to TF-IDF due to the small collection and lexical similarity within the pair. \n\nLate Interaction Late interaction models like ColBERT (Khattab and Zaharia, 2020;Santhanam et al., 2022b) embed documents and queries into one vector for each sub-word token. At inference time, these models need to compute a MaxSim operation between query vectors and document vectors to determine similarity. We use both ColBERT v1 and v2 in our experiments. 3 i-Encoders Another common category of IR models are bi-encoders, which embed both documents and queries into a single vector representation. At inference time the similarity is computed via a simple dot product or cosine similarity. Due to the popularity of this category, we include a broad spectrum: models from Sen-tenceTransformer (Reimers and Gurevych, 2019) trained on MSMarco and/or Natural Questions, DPR (Karpukhin et al., 2020), CoCondenser (Gao and Callan, 2022), and RocketQA (Qu et al., 2021;Ren et al., 2021). Note that these models span a wide variety of pre-training tasks, base models, and complex training/additional fine-tuning strategies like hard negative mining and distillation.",
            "score": 0.42070840293321454,
            "section_title": "Models",
            "char_start_offset": 14035,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 349
                },
                {
                    "start": 352,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 900
                },
                {
                    "start": 903,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1405
                },
                {
                    "start": 1406,
                    "end": 1497
                },
                {
                    "start": 1498,
                    "end": 1788
                },
                {
                    "start": 1789,
                    "end": 1966
                }
            ],
            "ref_mentions": [
                {
                    "start": 553,
                    "end": 573,
                    "matchedPaperCorpusId": "248665596"
                },
                {
                    "start": 597,
                    "end": 626,
                    "matchedPaperCorpusId": "250340284"
                },
                {
                    "start": 957,
                    "end": 984,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 984,
                    "end": 1008,
                    "matchedPaperCorpusId": "244799249"
                },
                {
                    "start": 1600,
                    "end": 1628,
                    "matchedPaperCorpusId": "201646309"
                },
                {
                    "start": 1678,
                    "end": 1702,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1716,
                    "end": 1738,
                    "matchedPaperCorpusId": "236987190"
                },
                {
                    "start": 1753,
                    "end": 1770,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 1770,
                    "end": 1787,
                    "matchedPaperCorpusId": "238857121"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92138671875
        },
        {
            "corpus_id": "258866037",
            "title": "Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy",
            "text": "Generative Large Language Models (LLMs) have powered numerous applications, with wellperceived utility. Despite being powerful, LLMs lack knowledge that is under-represented in their training data, and are prone to hallucinations, especially in open-domain settings (OpenAI, 2023). * *Corresponding author: Minlie Huang. \n\nRetrieval-augmented LLMs, therefore, have raised widespread attention as LLM outputs can be potentially grounded on external knowledge. \n\nPrevious retrieval-augmented LMs (Izacard et al., 2022b;Shi et al., 2023) typically adopted one-time retrieval, i.e., to retrieve knowledge using only the task input (e.g., a user question for open-domain question answering). One-time retrieval should suffice to fulfill the information needs if they are clearly stated in the original input, which is applicable to factoid question answering (Kwiatkowski et al., 2019) and single-hop fact verification (Thorne et al., 2018), but not to tasks with complex information needs, e.g., multi-hop reasoning (Yang et al., 2018) and long-form question answering (Fan et al., 2019). \n\nTo fulfill complex information needs, recent work proposes to gather required knowledge multiple times throughout the generation process, using partial generation (Trivedi et al., 2022a;Press et al., 2022)) or forward-looking sentence(s) (Jiang et al., 2023) as search queries. However, such structured workflows of interleaving retrieval with generation have the following limitations: (1) as intermediate generation is conditioned on knowledge retrieved before, with no awareness of knowledge retrieved afterwards, they fail to process all retrieved knowledge as a whole during the generation process; (2) they require multi-round retrieval to gather a comprehensive set of knowledge, and may frequently change the prompts by updating newly retrieved knowledge, thus increasing the overheads of both retrieval and generation. \n\nIn this paper, we find it simple but effective to enhance retrieval-augmented LLMs through iterative retrieval-generation synergy (ITER-RETGEN, Fig 1).",
            "score": 0.4201444466240902,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 320
                },
                {
                    "start": 323,
                    "end": 458
                },
                {
                    "start": 461,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 1084
                },
                {
                    "start": 1087,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1914
                },
                {
                    "start": 1917,
                    "end": 2068
                }
            ],
            "ref_mentions": [
                {
                    "start": 854,
                    "end": 880,
                    "matchedPaperCorpusId": "86611921"
                },
                {
                    "start": 914,
                    "end": 935,
                    "matchedPaperCorpusId": "4711425"
                },
                {
                    "start": 1065,
                    "end": 1083,
                    "matchedPaperCorpusId": "196170479"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84619140625
        },
        {
            "corpus_id": "266176291",
            "title": "Context Quality Matters in Training Fusion-in-Decoder for Extractive Open-Domain Question Answering",
            "text": "Recently, large-scale pre-trained language models have achieved impressive performance in the field of Natural Language Generation, which includes tasks that require real-world knowledge, e.g., closed-book question answering and common sense reasoning (Brown et al., 2020). However, these models are still prone to generate factually incorrect outputs known as hallucinations (Ji et al., 2023), particularly when dealing with rare entities (Mallen et al., 2023). Also, they cannot handle new information that arises after their training phase (Kasai et al., 2022). \n\nIn order to address these challenges, retrievalaugmented generation models have been recently proposed (Izacard and Grave, 2021b;Lewis et al., 2020). These models draw inspiration from retrieval-based extractive open-domain question answering methods (Chen et al., 2017) and utilize additional relevant external knowledge (e.g., a Wikipedia article about an entity in a given question) during generation to augment knowledge encoded in a language model. Retrieval-augmented generation models have demonstrated effectiveness in knowledge-intensive tasks (Petroni et al., 2021) such as question answering and fact checking (Hofst\u00e4tter et al., 2022), and have been reported to reduce hallucinations in dialogue tasks (Shuster et al., 2021). \n\nThe external knowledge given to the models is called a context, and it is usually obtained through information retrieval systems (Lin et al., 2022). Multiple passages, typically up to 100, are often used collectively as a single context to ensure the high recall of relevant information. This strategy addresses the limitations of retrieval systems, which may return irrelevant passages and fail to capture relevant information in the top results. When dealing with contexts composed of multiple passages, we can define their quantity (the number of passages in the context) and quality (the proportion of relevant passages in the context). Since the context quantity and quality vary depending on model configuration or application, e.g., the performance of the retrieval system and the computational resources available, understanding how these characteristics impact the model performance becomes an important research question. \n\nIndeed, during the inference phase, it has been shown that the quantity and quality of contexts impact the performance of retrieval-augmented generation models.",
            "score": 0.41880547086937026,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 564
                },
                {
                    "start": 567,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1304
                },
                {
                    "start": 1307,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1594
                },
                {
                    "start": 1595,
                    "end": 1754
                },
                {
                    "start": 1755,
                    "end": 1947
                },
                {
                    "start": 1948,
                    "end": 2238
                },
                {
                    "start": 2241,
                    "end": 2401
                }
            ],
            "ref_mentions": [
                {
                    "start": 252,
                    "end": 272,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 376,
                    "end": 393,
                    "matchedPaperCorpusId": "246652372"
                },
                {
                    "start": 440,
                    "end": 461,
                    "matchedPaperCorpusId": "254877603"
                },
                {
                    "start": 670,
                    "end": 696,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 696,
                    "end": 715,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 818,
                    "end": 837,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 1120,
                    "end": 1142,
                    "matchedPaperCorpusId": "221507798"
                },
                {
                    "start": 1281,
                    "end": 1303,
                    "matchedPaperCorpusId": "233240939"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7880859375
        },
        {
            "corpus_id": "261277205",
            "title": "Large language models converge toward human-like concept organization",
            "text": "The results of the representational similarity analysis for all four language model families can be found in the supplementary material. The partial convergence results are similar to those obtained with linear projection. The figure presents the results of how well the language models solve the analogies from the WiQueen dataset and retrieve the correct the word. We report retrieval performance p@k at k = {1, 10, 20, 50} for 4 language model families.",
            "score": 0.4185848365485682,
            "section_title": "Representational Similarity Analysis",
            "char_start_offset": 14079,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 456
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4921875
        },
        {
            "corpus_id": "273654180",
            "title": "RARe: Retrieval Augmented Retrieval with In-Context Examples",
            "text": "Muennighoff et al. (2024) explored providing an in-context example out-of-the-box, but showed an overall decrease in performance compared to zero-shot inference. \n\nRetrieval Large language models pre-trained with autoregressive setups (Jiang et al., 2023;Dubey et al., 2024) have shown remarkable performance when adapted to retrieval tasks (Wang et al., 2024b;BehnamGhader et al., 2024), outperforming encoder-style retrievers (Izacard et al., 2022;Wang et al., 2024a). Despite these advancements, a challenge that remains is the ability to tailor retrieval systems to specific tasks or queries. To address this, a recent line of work explores incorporating instructions into retrieval by training models to use task-specific instructions along with the query (Su et al., 2023;Asai et al., 2023). Oh et al. (2024) and Weller et al. (2024a) further propose using instructions that are specific to each query. Another well-established technique in retrieval is query expansion (Jagerman et al., 2023;Li et al., 2023b;Chen et al., 2024), where the query is augmented with additional terms to enrich the embedding as a form of relevance feedback (Lv & Zhai, 2009). Recent efforts have focused on applying LLMs to expand the original query before retrieval (Wang et al., 2023;Shen et al., 2024). These techniques are not mutually exclusive, and can be integrated with our approach.",
            "score": 0.4185848365485682,
            "section_title": "RELATED WORK",
            "char_start_offset": 19642,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 164,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 1161
                },
                {
                    "start": 1162,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1377
                }
            ],
            "ref_mentions": [
                {
                    "start": 428,
                    "end": 450,
                    "matchedPaperCorpusId": "249097975"
                },
                {
                    "start": 761,
                    "end": 778,
                    "matchedPaperCorpusId": "254853816"
                },
                {
                    "start": 778,
                    "end": 796,
                    "matchedPaperCorpusId": "253581733"
                },
                {
                    "start": 1016,
                    "end": 1034,
                    "matchedPaperCorpusId": "11816014"
                },
                {
                    "start": 1143,
                    "end": 1160,
                    "matchedPaperCorpusId": "12012494"
                },
                {
                    "start": 1253,
                    "end": 1272,
                    "matchedPaperCorpusId": "257505063"
                },
                {
                    "start": 1272,
                    "end": 1290,
                    "matchedPaperCorpusId": "271948010"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9208984375
        },
        {
            "corpus_id": "244799261",
            "title": "Uni-Perceiver: Pre-training Unified Architecture for Generic Perception for Zero-shot and Few-shot Tasks",
            "text": "if any). Please refer to the Appendix for an efficient implementation of auto-regressive language modeling.\n\nIn this task, X consists of language sentences or imagetext pairs. Y denotes the set of all words in the vocabulary, where each word is regarded as a single text sequence. Each word that needs to be predicted in x \u2208 X is replaced by a <SPE> token, whose feature at the encoder output is used to match the words in the vocabulary Y.\n\nImage and Text Retrieval. For image-text retrieval, the input sets X and Y are composed of images and text sequences respectively, or vice versa. For text-only retrieval, the input sets X and Y are both text sequences. Each sequence in X and Y has a special token <SPE> at the beginning, whose feature at the output of the encoder serves as the final representation.",
            "score": 0.4185848365485682,
            "section_title": "Pre-training on Multi-Modal Tasks",
            "char_start_offset": 19415,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.556640625
        },
        {
            "corpus_id": "18783695",
            "title": "A Pylonic Decision-Tree Language Model- with Optimal Question Selection",
            "text": "In many applications such as automatic speech recognition, machine translation, spelling correction, etc., a statistical language model (LM) is needed to assign ~probabilities to sentences. This probability assignment may be used, e.g., to choose one of many transcriptions hypothesized by the recognizer or to make decisions about capitalization. Without any loss of generality, we consider models that operate left-to-right on the sentences, assigning a probability to the next word given its word history. \n\nSpecifically, we consider statistical LM's which compute probabilities of the type P{wn ]Wl, W2,..-, Wn--1}, where wi denotes the i-th word in the text. \n\nEven for a small vocabulary, the space of word histories is so large that any attempt to estimate the conditional probabilities for each distinct history from raw frequencies is infeasible. To make the problem manageable, one partitions the word histories into some classes C (wl,w2,...,, and identifies the word probabilities with P{wn [ C(wl, w2,. . . , Wn-1)}. Such probabilities are easier to estimate as each class gets significantly more counts from a training corpus. With this setup, building a language model becomes a classification problem: group the word histories into a small number of classes while preserving their predictive power. \n\nCurrently, popular N-gram models classify the word histories by their last N -1 words. N varies from 2 to 4 and the trigram model P{wn [Wn-2, wn-1} is commonly used. Although these simple models perform surprisingly well, there is much room for improvement. The approach used in this paper is to classify the histories by means of a decision tree: to cluster word histories Wl,W2,... ,wn-1 for which the distributions of the following word Wn in a training corpus are similar. The decision tree is pylonic in the sense that histories at different nodes in the tree may be recombined in a new node to increase the complexity of questions and avoid data fragmentation.",
            "score": 0.4185848365485682,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 508
                },
                {
                    "start": 511,
                    "end": 663
                },
                {
                    "start": 666,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1314
                },
                {
                    "start": 1317,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1482
                },
                {
                    "start": 1483,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1793
                },
                {
                    "start": 1794,
                    "end": 1983
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79345703125
        },
        {
            "corpus_id": "277502514",
            "title": "CASCADE Your Datasets for Cross-Mode Knowledge Retrieval of Language Models",
            "text": "We investigated language models' cross-mode knowledge retrieval capability from both qualitative and quantitative perspectives. Our qualitative pipeline reveals that LLMs such as GPT-4o cannot perform cross-mode knowledge retrieval satisfactorily. Quantitatively, we formulated this problem using two format datasets as modes and random token sequences as knowledge, and experimented with a straightforward approach -dataset rewritingshowing that only substantial dataset rewriting efforts can alleviate this issue. Finally, we proposed CASCADE, a novel pretraining method, along with its model-compression version. Experiments demonstrate that CASCADE significantly outperforms baselines. \n\nDespite its fundamental nature, our work has several limitations that may inspire future studies. First, we did not apply our training method to real-world datasets due to limited computational resources and lack of evaluation metrics. The qualitative pipeline in Appendix C may serve as a metric, but automatically selecting representative knowledge merits further study. Second, our study contains only two modes. Future work could transform our quantitative study into an n-mode setting and compute the corresponding normalized log probabilities. \n\nYuxiang Wu, Matt Gardner, Pontus Stenetorp, and Pradeep Dasigi. Generating data to mitigate spurious correlations in natural language inference datasets. arXiv preprint arXiv:2203.12942, 2022. \n\nTian Ye, Zicheng Xu, Yuanzhi Li, and Zeyuan Allen-Zhu. Physics of language models: Part 2.1, grade-school math and the hidden reasoning process, 2024a. URL https://arxiv. org/abs/2407.20311. \n\nTian Ye, Zicheng Xu, Yuanzhi Li, and Zeyuan Allen-Zhu. Physics of language models: Part 2.2, how to learn from mistakes on grade-school math problems, 2024b. URL https://arxiv.org/abs/2408.16293.",
            "score": 0.4185848365485682,
            "section_title": "Conclusion",
            "char_start_offset": 23002,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 689
                },
                {
                    "start": 692,
                    "end": 789
                },
                {
                    "start": 790,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1107
                },
                {
                    "start": 1108,
                    "end": 1241
                },
                {
                    "start": 1244,
                    "end": 1307
                },
                {
                    "start": 1308,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1430
                },
                {
                    "start": 1431,
                    "end": 1436
                },
                {
                    "start": 1439,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1590
                },
                {
                    "start": 1591,
                    "end": 1609
                },
                {
                    "start": 1610,
                    "end": 1629
                },
                {
                    "start": 1632,
                    "end": 1686
                },
                {
                    "start": 1687,
                    "end": 1789
                },
                {
                    "start": 1790,
                    "end": 1827
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2294921875
        },
        {
            "corpus_id": "2200463",
            "title": "Benchmarking declarative approximate selection predicates",
            "text": "A language model, is a form of a probabilistic model. To realize things concretely, we base our discussion on a specific model introduced by Ponte and Croft [22]. Given a collection of documents, a language model is inferred for each; then the probability of generating a given query according to each of these models is estimated and documents are ranked according to these probabilities. Considering an approximate selection query, each tuple in the database is considered as a document; a model is inferred for each tuple and the probability of generating the query given the model is the similarity between the query and the tuple.",
            "score": 0.4185848365485682,
            "section_title": "Language Modeling Predicates",
            "char_start_offset": 11205,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 157,
                    "end": 161,
                    "matchedPaperCorpusId": "2759772"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8037109375
        },
        {
            "corpus_id": "276421942",
            "title": "HopRAG: Multi-Hop Reasoning for Logic-Aware Retrieval-Augmented Generation",
            "text": "Retrieval-Augmented Generation Retrievalaugmented generation significantly improves large language models by incorporating a retrieval module that fetches relevant information from external knowledge sources (F\u00e9vry et al., 2020;Guu et al., 2020b;Izacard and Grave, 2021;Zhao et al., 2024;Yu et al., 2025). Retrieval models have evolved from early sparse retrievers, such as TF-IDF (Jones, 1973) and BM25 (Robertson and Zaragoza, 2009b), which rely on word statistics and inverted indices, to dense retrievers (Lewis et al., 2020b) that utilize neural representations for semantic matching. \n\nAdvanced methods, such as Self-RAG (Asai et al., 2023) and FLARE (Jiang et al., 2023) which determine the necessity and timing of retrieval, represent significant developments. However, the knowledge index remains logically unstructured, with each round of search considering only lexical or semantic similarity. \n\nTree&Graph-structured RAG Tree and graph are both effective structures for modeling logical relations. RAPTOR (Sarthi et al., 2023) recursively embeds, clusters, and summarizes passages, constructing a tree with differing levels of summarization from the bottom up. MemWalker (Chen et al., 2023) treats the LLM as an interactive agent walking on the tree of summarization. SiReRAG (Zhang et al., 2024) explicitly considers both similar and related information by constructing both similarity tree and relatedness tree. PG-RAG (Liang et al., 2024)",
            "score": 0.4185848365485682,
            "section_title": "Related Work",
            "char_start_offset": 6447,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 589
                },
                {
                    "start": 592,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 904
                },
                {
                    "start": 907,
                    "end": 1009
                },
                {
                    "start": 1010,
                    "end": 1172
                },
                {
                    "start": 1173,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1425
                },
                {
                    "start": 1426,
                    "end": 1453
                }
            ],
            "ref_mentions": [
                {
                    "start": 208,
                    "end": 228,
                    "matchedPaperCorpusId": "215768768"
                },
                {
                    "start": 246,
                    "end": 270,
                    "matchedPaperCorpusId": "220302360"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92578125
        },
        {
            "corpus_id": "267377589",
            "title": "An Analysis on Matching Mechanisms and Token Pruning for Late-interaction Models",
            "text": "Recently, neural network models have attracted much attention in both academia and industry and have made impressive improvements in many fields.The information retrieval field benefits from pre-trained language models such as BERT [12] and RoBERTa [29].These neural retrieval models can be divided into two groups as below:\n\nNeural Sparse Retrieval Models improve retrieval performance by changing the term importance in different ways.Two common methods are: a) leveraging neural language generation models (e.g., T5 [39]) to expand original documents with semantic-related words, or b) using pre-trained language models to re-compute the weights.Nogueira et al. [33,34] modified the term frequency by using generative models to generate pseudo-queries of original documents and concatenate these pseudo-queries and original documents to form the new documents.Dai and Callan [10,11] used the pre-trained language model to predict contextualized term weight through a regression task.Mallia et al. [31] first used a generator to expand new semantic-related words of the original text and then used a contextualized language model encoder to re-compute term weights.On the other hand, Bai et al. [4] proposed using an important predictor to re-predict term weights and a gating controller to filter or expand terms at the same time.Formal et al. [13,14] further proposed a more simple but effective method, based on explicit sparsity regularization and a log-saturation effect on term weights.All of these works can significantly outperform BM25.\n\nNeural Dense Retrieval Models use dense vectors to represent queries and documents.Two widely adopted architectures of dense retrieval models are cross-encoders and bi-encoders.Crossencoders [32] concatenate the query and the document as the input of pre-trained language models, then use a function to map the output vector to a scalar score.Cross-encoders achieve superior retrieval effectiveness through the full interaction between all query tokens and all document tokens but the requirement of concatenating the query and each candidate document at the inference time makes cross-encoders impractical in the retrieval stage.",
            "score": 0.4175804736869253,
            "section_title": "Neural Retrieval Models",
            "char_start_offset": 8893,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 145,
                    "end": 254
                },
                {
                    "start": 254,
                    "end": 324
                },
                {
                    "start": 326,
                    "end": 437
                },
                {
                    "start": 437,
                    "end": 649
                },
                {
                    "start": 649,
                    "end": 863
                },
                {
                    "start": 863,
                    "end": 986
                },
                {
                    "start": 986,
                    "end": 1167
                },
                {
                    "start": 1167,
                    "end": 1333
                },
                {
                    "start": 1333,
                    "end": 1494
                },
                {
                    "start": 1494,
                    "end": 1547
                },
                {
                    "start": 1549,
                    "end": 1632
                },
                {
                    "start": 1632,
                    "end": 1726
                },
                {
                    "start": 1726,
                    "end": 1892
                },
                {
                    "start": 1892,
                    "end": 2179
                }
            ],
            "ref_mentions": [
                {
                    "start": 882,
                    "end": 885,
                    "matchedPaperCorpusId": "218521094"
                },
                {
                    "start": 1000,
                    "end": 1004,
                    "matchedPaperCorpusId": "233394068"
                },
                {
                    "start": 1351,
                    "end": 1354,
                    "matchedPaperCorpusId": "235792467"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97900390625
        },
        {
            "corpus_id": "263835099",
            "title": "Retrieve Anything To Augment Large Language Models",
            "text": "The related works are reviewed from two perspectives: retrieval augmented large language models, and dense retrieval. \n\n\u2022 Retrieval Augmented LLMs. Large language models (LLMs) are praised for their unprecedented capability on language understanding and generation. Compared with the conventional methods, LLMs exhibit overwhelming generality and notable advantages on typical NLP tasks [17,19,78]. Despite such superiority, LLMs still face a series of severe challenges, such as hallucination, human alignment, and long-term memory. Many of the existing problems are caused by the inherent boundaries, which cannot be addressed by LLMs alone, but to rely on support from the external world. The retrieval-augmented LLMs are regarded as a go-to option to bridge LLMs with the external assistance [4,51]. For the past few years, they have been widely applied to several critical scenarios. One common case is the knowledge enhancement. The internal knowledge of LLMs can be incomplete, static, and limited by the popularity bias. When dealing with knowledge intensive tasks, the retrieval augmented LLMs will look for necessary information from an external database, where the generated content can be grounded on proper knowledge [15,31,32,41]. Besides, the retrieval augmented LLMs are also used to retrieve historical context to establish longterm memory [71,85], retrieve examples to improve the instruction following capability [18,83], and retrieve tools to engage with the physical world [62]. \n\nThe retrieval augmented LLMs consist of two basic parts: generator and retriever. According to previous studies [32,41,83,96], the retrieval augmentation effect is highly influenced by the retrieved content. In practice, there are two common types of retrievers. One is to leverage the general purpose retrievers, such as sparse models like BM25 [69], and dense models, like DPR [37], contriever [30], E5 [81], BGE [89], OpenAI text embedding [56]. The other option is develop task-specific retriever, e.g., AAR for knowledge enhancement [96], LLM-R [85] for in-context learning. The general purpose methods are praised for their generality and simplicity for usage, but may suffer from an inferior retrieval quality.",
            "score": 0.41469612313249743,
            "section_title": "RELATED WORKS",
            "char_start_offset": 33911,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 120,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 803
                },
                {
                    "start": 804,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1499
                },
                {
                    "start": 1502,
                    "end": 1583
                },
                {
                    "start": 1584,
                    "end": 1709
                },
                {
                    "start": 1710,
                    "end": 1764
                },
                {
                    "start": 1765,
                    "end": 1950
                },
                {
                    "start": 1951,
                    "end": 2081
                },
                {
                    "start": 2082,
                    "end": 2219
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94189453125
        },
        {
            "corpus_id": "276408593",
            "title": "Revisiting Robust RAG: Do We Still Need Complex Robust Training in the Era of Powerful LLMs?",
            "text": "Retrieval-Augmented Generation (RAG) enhances language model performance by incorporating external knowledge retrieval into the generation process. Given a query q, a retriever R selects a set of k relevant document sets D = {d 1 , d 2 , . . . , d k } from a large corpus, providing additional factual grounding for the model. These retrieved documents serve as context to improve the model's reasoning and factual accuracy. The language model G then generates an answer a based on the augmented input, following the conditional probability formulation: \n\nwhere a = (a 1 , a 2 , . . . , a T ) represents the generated sequence, and \u03b8 denotes the model parameters. \n\nThe retrieval-augmented setup enables the model to dynamically condition its responses on external information, improving its ability to generate factually consistent answers. However, RAG models remain susceptible to retrieval noise, such as irrelevant or misleading documents, which can degrade performance and lead to hallucinations.",
            "score": 0.41450130713776434,
            "section_title": "Retrieval-Augmented Generation",
            "char_start_offset": 4751,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 553
                },
                {
                    "start": 556,
                    "end": 584
                },
                {
                    "start": 585,
                    "end": 663
                },
                {
                    "start": 666,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 1002
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84375
        },
        {
            "corpus_id": "9519540",
            "title": "A P\u00f3lya Urn Document Language Model for Improved Information Retrieval",
            "text": "We use the Dirichlet compound multinomial (DCM), also known as the multivariate P\u00f3lya distribution, to model documents in place of the standard multinomial distribution, while we use the standard multinomial to model query generation. We show that this new retrieval model obtains significantly increased effectiveness compared to the current state-ofthe-art model on a range of datasets for a number of effectiveness metrics. This article is organized as follows. Section 2 introduces notation used in the remainder of the article and also presents a comprehensive review of relevant research. Section 3 reviews the standard language modelling approach. Section 4 presents the SPUD language model. Section 5 outlines efficient forms of the new retrieval functions, and provides deep insights into the proposed functions. The experimental design and results are presented in Section 6. Section 7 presents a discussion of the results and Section 8 concludes with a summary.",
            "score": 0.41450130713776434,
            "section_title": "INTRODUCTION",
            "char_start_offset": 4072,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 654
                },
                {
                    "start": 655,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 972
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9482421875
        },
        {
            "corpus_id": "271720333",
            "title": "Hide and Seek: Fingerprinting Large Language Models with Evolutionary Learning",
            "text": "Showcasing the ability to identify the family of a language model by prompting and in-context learning opens many promising avenues. We intend to explore the capabilities of Language Models to uncover the size of a language model and even discover the capabilities of another model. Past works such as [24], have utilized a clever scheme showcasing a black-box approach for model estimation is viable.",
            "score": 0.41450130713776434,
            "section_title": "Extending Detection to Model Size and Capability",
            "char_start_offset": 25897,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 401
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12103271484375
        },
        {
            "corpus_id": "246431219",
            "title": "Neuro-Symbolic Language Modeling with Automaton-augmented Retrieval",
            "text": "Retrieval-based language models (R-LMs) have recently been shown to improve over standard neural models in a variety of tasks such as unconditional language modeling (Guu et al., 2018;He et al., 2020), machine translation (Zhang et al., 2018;Gu et al., 2018;Khandelwal et al., 2021), Proceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copyright 2022 by the author(s). question answering (Karpukhin et al., 2020;Ram et al., 2021), and code generation (Hayati et al., 2018;Hashimoto et al., 2018). The key ingredient of R-LMs is their ability to utilize training examples at test time without having to rely on the information encoded in the model's weights only. \n\nIn these models, the retrieval component first searches for nearest neighbor examples in an external datastore; then, the base model references these examples during the prediction. This fusion of language models (LMs) and retrieval improves the base language model from several perspectives, including higher accuracy (Xu et al., 2021), domain adaptability (Jiang et al., 2021), and reduced size (Borgeaud et al., 2021). Further, the retrieved examples provide information regarding the provenance of the model's predictions, and retrieval allows for modifying the dataset without retraining the model. Nevertheless, the most critical bottleneck of these models is their frequent search over the datastore, which hinders the use of R-LMs in practical settings. \n\nk-Nearest Neighbors Language Model One prominent example of such a retrieval-based model is kNN-LM (Grave et al., 2017;Khandelwal et al., 2020), which predicts a token by linearly interpolating the base LM's output with a nonparametric nearest neighbor distribution. This distribution is constructed by searching for the k-nearest neighbors (kNN) in the datastore and weighting them according to their distance to the current test context.",
            "score": 0.41450130713776434,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 723
                },
                {
                    "start": 726,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1329
                },
                {
                    "start": 1330,
                    "end": 1487
                },
                {
                    "start": 1490,
                    "end": 1756
                },
                {
                    "start": 1757,
                    "end": 1929
                }
            ],
            "ref_mentions": [
                {
                    "start": 166,
                    "end": 184,
                    "matchedPaperCorpusId": "2318481"
                },
                {
                    "start": 184,
                    "end": 200,
                    "matchedPaperCorpusId": "220265977"
                },
                {
                    "start": 242,
                    "end": 258,
                    "matchedPaperCorpusId": "218614141"
                },
                {
                    "start": 258,
                    "end": 282,
                    "matchedPaperCorpusId": "222125236"
                },
                {
                    "start": 449,
                    "end": 473,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 512,
                    "end": 533,
                    "matchedPaperCorpusId": "220265977"
                },
                {
                    "start": 533,
                    "end": 556,
                    "matchedPaperCorpusId": "54446010"
                },
                {
                    "start": 1084,
                    "end": 1104,
                    "matchedPaperCorpusId": "237581333"
                },
                {
                    "start": 1609,
                    "end": 1633,
                    "matchedPaperCorpusId": "207870430"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97412109375
        },
        {
            "corpus_id": "271270159",
            "title": "Retrieval-Enhanced Machine Learning: Synthesis and Opportunities",
            "text": "Motivation. Since improving model performance by increasing the number of parameters is not sustainable, one motivation of retrieval-based models stems from the finding that, while large models tend to memorize training data [17], incorporating retrieval-based methods can effectively transfer the burden of memorization to external storage systems [14,198]. We advocate for enhancing machine learning (ML) models in general (i.e., beyond generation) with the ability to employ stored information via information retrieval techniques. IR has already shown its merits in aiding human interaction with vast text databases. We posit that IR's utility can be broadened to enable machine access to not only extensive text databases but also to knowledge represented in more abstract forms. By integrating ML architectures with direct access to IR systems, we aim to separate the processes of reasoning and memory. Zamani et al. [257] dubbed this approach, retrieval-enhanced machine learning (REML), as a broader concept that extends ML. Extending their work, we further survey the recent advances of REML in the field of ML, including NLP, with consistent mathematical notation. By doing so, we aim to equip researchers with a comprehensive and structured overview of the REML methodologies, enabling them to swiftly embark on research within this domain. \n\nApplications of REML. The landscape of REML paradigm encompasses a diverse array of sub-domains, each with its unique set of challenges and applications. This includes seminal work in language modeling [14,66,83,84,100,123,125,176,199,223,270],",
            "score": 0.41450130713776434,
            "section_title": "Introduction",
            "char_start_offset": 2415,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 11
                },
                {
                    "start": 12,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1351
                },
                {
                    "start": 1354,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1507
                },
                {
                    "start": 1508,
                    "end": 1598
                }
            ],
            "ref_mentions": [
                {
                    "start": 225,
                    "end": 229,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 349,
                    "end": 353,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 923,
                    "end": 928,
                    "matchedPaperCorpusId": "248506020"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67041015625
        },
        {
            "corpus_id": "62276728",
            "title": "Hierarchical Phrase-Based Translation with Weighted Finite State Transducers",
            "text": "rule, source-to-target and target-to-source lexical models, and three rule count features inspired by Bender et al. (2007). The initial English language model is a 4-gram estimated over the parallel text and a 965 million word subset of monolingual data from the English Gigaword Third Edition. Details of the parallel corpus and development sets used for each language pair are given in their respective section.\n\nStandard MET (Och, 2003) iterative parameter estimation under IBM BLEU (Papineni et al., 2001) is performed on the corresponding development set. For the HCP system, MET is done following Chiang (2007). For the HiFST system, we obtain a k-best list from the translation lattice and extract each feature score with the aligner variant of the k-best decoder. After translation with optimized feature weights, we carry out the two following rescoring steps.\n\n\u2022 Large-LM rescoring.\n\nWe build sentencespecific zero-cutoff stupid-backoff (Brants et al., 2007) 5-gram language models, estimated using \u223c4.7B words of English newswire text, and apply them to rescore either 10000-best lists generated by HCP or word lattices generated by HiFST. Lattices provide a vast search space relative to k-best lists, with translation lattice sizes of 10 81 hypotheses reported in the literature (Tromble et al., 2008).\n\n\u2022 Minimum Bayes Risk (MBR). We rescore the first 1000-best hypotheses with MBR, taking the negative sentence level BLEU score as the loss function (Kumar and Byrne, 2004).",
            "score": 0.41450130713776434,
            "section_title": "Translation Experiments",
            "char_start_offset": 15365,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 102,
                    "end": 122,
                    "matchedPaperCorpusId": "11217962"
                },
                {
                    "start": 428,
                    "end": 439,
                    "matchedPaperCorpusId": "5474833"
                },
                {
                    "start": 486,
                    "end": 509,
                    "matchedPaperCorpusId": "11080756"
                },
                {
                    "start": 603,
                    "end": 616,
                    "matchedPaperCorpusId": "3505719"
                },
                {
                    "start": 947,
                    "end": 968,
                    "matchedPaperCorpusId": "633992"
                },
                {
                    "start": 1292,
                    "end": 1314,
                    "matchedPaperCorpusId": "7657227"
                },
                {
                    "start": 1464,
                    "end": 1487,
                    "matchedPaperCorpusId": "11706155"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.291259765625
        },
        {
            "corpus_id": "342976",
            "title": "An Introduction to Conditional Random Fields",
            "text": "In this section we discuss several examples applications of simple graphical models to natural language processing. Although these examples are well-known, they serve both to clarify the definitions in the previous section, and to illustrate some ideas that will arise again in our discussion of conditional random fields. We devote special attention to the hidden Markov model (HMM), because it is closely related to the linear-chain CRF.",
            "score": 0.41450130713776434,
            "section_title": "Generative versus Discriminative Models",
            "char_start_offset": 15045,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.278076171875
        },
        {
            "corpus_id": "9345583",
            "title": "Lexicon Infused Phrase Embeddings for Named Entity Resolution",
            "text": "A statistical language model is a way to assign probabilities to all possible documents in a given language. Most such models can be classified in one of two categories: they can directly assign probabilities to sequences of word types, such as is done in n-gram models, or they can operate in a lowerdimensional latent space, to which word types are mapped. While most state-of-the-art language models are n-gram models, the representations used in models of the latter category, henceforth referred to as \"embeddings,\" have been found to be use-ful in many NLP applications which don't actually need a language model. The underlying intuition is that when language models compress the information about the word types in a latent space they capture much of the commonalities and differences between word types. Hence features extracted from these models then can generalize better than features derived from the word types themselves. \n\nOne simple language model that discovers useful embeddings is known as Brown clustering (Brown et al., 1992). A Brown clustering is a class-based bigram model in which (1) the probability of a document is the product of the probabilities of its bigrams, (2) the probability of each bigram is the product of the probability of a bigram model over latent classes and the probability of each class generating the actual word types in the bigram, and (3) each word type has non-zero probability only on a single class. Given a one-to-one assignment of word types to classes, then, and a corpus of text, it is easy to estimate these probabilities with maximum likelihood by counting the frequencies of the different class bigrams and the frequencies of word tokens of each type in the corpus. The Brown clustering algorithm works by starting with an initial assignment of word types to classes (which is usually either one unique class per type or a small number of seed classes corresponding to the most frequent types in the corpus), and then iteratively selecting the pair of classes to merge that would lead to the highest postmerge log-likelihood, doing so until all classes have been merged.",
            "score": 0.41450130713776434,
            "section_title": "Language models and word embeddings",
            "char_start_offset": 3969,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 109,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 812
                },
                {
                    "start": 813,
                    "end": 936
                },
                {
                    "start": 939,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1453
                },
                {
                    "start": 1454,
                    "end": 1726
                },
                {
                    "start": 1727,
                    "end": 2131
                }
            ],
            "ref_mentions": [
                {
                    "start": 1027,
                    "end": 1047,
                    "matchedPaperCorpusId": "2617020"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90869140625
        },
        {
            "corpus_id": "274140975",
            "title": "Mechanism and Emergence of Stacked Attention Heads in Multi-Layer Transformers",
            "text": "To better illustrate the task and to enable benchmarking of large language models, I propose 5 specific formulations of the retrieval problems: 3 retrieval formulations and 2 conditional retrieval formulations. \n\nF1. Equations formulation: \"a = 3. b = a. c = b. c = ?\" F2. Lives-with formulation: \"Alice lives in Switzerland. Bob lives with Alice. Charlie lives with Bob. David lives with Charlie. Where does David live?\" F3. Kingdoms formulation: \"Alice lives in Novaria. Novarians believe in harmonianism. Harmonianists eat lamb. Lamb contains Zephyrium. Zephyrium causes Chronogy. Who has Chronogy?\" F4. Functions formulation (conditional retrieval): \"f(2) = 3. g = f. a = 2. g(a) = ?\" F5. Relatives formulation (conditional retrieval): \"Jane lives in Switzerland. Alex's mother is Jane. Engineers live with their mothers. Alex is an engineer. Where does Alex live?\" \n\nTo ensure that the retrieval problem is not trivially solvable by just finding the noun that fits the question, I interleave multiple retrieval chains in the same question. This ensures that the model performs the entire reasoning chain. To facilitate benchmarking, I also ask the model to output the answer directly without any additional words and I repeat sampling until an acceptable answer is generated. In Appendix A, I provide examples of the full prompts, correct answers, and acceptable answers for each formulation. \n\nI test the large language models on 500 randomly generated questions for each formulation. The results are presented in Figure 2. For the equations formulation, I also measure the accuracy for different difficulty levels D (number of equations) and I find that large language models can solve it almost perfectly for D \u2264 5. Great performance is also achieved on the lives-with and kingdoms formulations with D = 5, as well as on the conditional retrieval formulations functions and relatives. Accuracy of GPT-4o on different retrieval and conditional retrieval tasks",
            "score": 0.41450130713776434,
            "section_title": "LARGE LANGUAGE MODELS",
            "char_start_offset": 5254,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 210
                },
                {
                    "start": 213,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 767
                },
                {
                    "start": 768,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 846
                },
                {
                    "start": 847,
                    "end": 869
                },
                {
                    "start": 872,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1397
                },
                {
                    "start": 1400,
                    "end": 1490
                },
                {
                    "start": 1491,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1892
                },
                {
                    "start": 1893,
                    "end": 1966
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60791015625
        },
        {
            "corpus_id": "271038977",
            "title": "VRSD: Rethinking Similarity and Diversity for Retrieval in Large Language Models",
            "text": "Vector retrieval algorithms are crucial for semantic queries and have become increasingly integral to the deployment of Large Language Models (LLMs). Effective interaction with LLMs frequently necessitates the provision of relevant or similar examples to elicit enhanced responses (Liu et al., 2022). The introduction of Retrieval Augmented Generation (RAG) has notably advanced the capabilities in knowledge-intensive tasks (Lewis et al., 2020), underscoring the growing importance of retrieval methods. Empirical evidence suggests that employing the BM25 algorithm to select examples from the training set markedly improves LLMs performance over random selection (Liu et al., 2022;Luo et al., 2023). Moreover, leveraging existing text embedding models for example retrieval often surpasses BM25, particularly in specific contexts (Reimers & Gurevych, 2019;Wang et al., 2022). And the advent of Dense Retrieval, which employs dense vectors for semantic matching in latent spaces (Chen et al., 2017;Lee et al., 2019), represents a evolution over traditional sparse retrieval methods like BM25 by utilizing the robust modeling capabilities of pre-trained language models to learn relevance functions (Devlin et al., 2019). Innovations such as the applying the dual encoder framework (Karpukhin et al., 2020) and dynamic listwise distillation (Ren et al., 2021) have further refined the effectiveness of dense retrieval techniques. Subsequent enhancements in semantic parsing and in-context learning (Pasupat et al., 2021), facilitated by feedback from LLMs (Rubin et al., 2022), have enabled more precise example selection and improved answer accuracy. Despite ongoing advancements in retrieval methods, the broadening application scope of LLMs necessitates retrieval approaches that balance relevance with diversity-specifically, a relevance-focused diversity rather than an unrestricted diversity. Additionally, the RAG framework's ability to augment the LLMs' external data access also underscores the need for simple yet efficient algorithms that can streamline the retrieval process. \n\nConsidering the balance between similarity and diversity, the Maximal Marginal Relevance (MMR) (Carbonell & Goldstein, 1998) is an effective algorithm and has been widely applied in vector retrieval practices.",
            "score": 0.41450130713776434,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1429
                },
                {
                    "start": 1430,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1898
                },
                {
                    "start": 1899,
                    "end": 2087
                },
                {
                    "start": 2090,
                    "end": 2299
                }
            ],
            "ref_mentions": [
                {
                    "start": 281,
                    "end": 299,
                    "matchedPaperCorpusId": "231632658"
                },
                {
                    "start": 425,
                    "end": 445,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 665,
                    "end": 683,
                    "matchedPaperCorpusId": "231632658"
                },
                {
                    "start": 832,
                    "end": 858,
                    "matchedPaperCorpusId": "201646309"
                },
                {
                    "start": 980,
                    "end": 999,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 999,
                    "end": 1016,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 1199,
                    "end": 1220,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1282,
                    "end": 1306,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1341,
                    "end": 1359,
                    "matchedPaperCorpusId": "238857121"
                },
                {
                    "start": 1498,
                    "end": 1520,
                    "matchedPaperCorpusId": "239016988"
                },
                {
                    "start": 1556,
                    "end": 1576,
                    "matchedPaperCorpusId": "245218561"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90625
        },
        {
            "corpus_id": "267751221",
            "title": "BlendFilter: Advancing Retrieval-Augmented Large Language Models via Query Generation Blending and Knowledge Filtering",
            "text": "Retrieval-augmented generation enhances Large Language Models (LLMs) by leveraging external knowledge to improve generation quality. Initial approaches, as discussed in (Izacard and Grave, 2021;Shao and Huang, 2021;Izacard et al., 2022a;Shi et al., 2023), portrayed LLMs as passive recipients of retrieved knowledge, lacking interactive dynamics with retrievers. However, due to the inherent challenges in accurately capturing relevance between inputs and documents, these direct methods often yield only marginal improvements. Addressing this, recent advancements (Nakano et al., 2021;Trivedi et al., 2022;Jiang et al., 2023;Li et al., 2023b,a;Wang et al., 2023b;Asai et al., 2023;Yu et al., 2023;Ma et al., 2023;Press et al., 2022;Yao et al., 2022) have empowered LLMs to engage actively with retrievers, thereby enhancing relevance modeling. The integration of LLMs into the retrieval process broadly falls into three categories: 1) question decomposition, 2) query rewriting, and 3) query augmentation. For question decomposition, as exemplified by Yao et al. (2022) and Press et al. (2022), LLMs break down a complex question into simpler components, leveraging both previous interactions and retrieved knowledge. This decomposition facilitates more straightforward reasoning by LLMs. However, the success of this approach heavily depends on the LLM's capabilities. Insufficiently powerful LLMs might generate misleading sub-questions. Moreover, this method requires maintaining a historical context, potentially leading to lengthy dialogues and increased computational costs. In the realm of query rewriting, models are trained, often utilizing reinforcement learning, to reformulate the original question into a version more conducive to retrieval (Ma et al., 2023;Li et al., 2023b). These revised questions typ-  ically yield improved generation outcomes. Nevertheless, training an additional model for rewriting is a resource-intensive process.",
            "score": 0.4133681239351786,
            "section_title": "Related Work",
            "char_start_offset": 5838,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1218
                },
                {
                    "start": 1219,
                    "end": 1289
                },
                {
                    "start": 1290,
                    "end": 1370
                },
                {
                    "start": 1371,
                    "end": 1440
                },
                {
                    "start": 1441,
                    "end": 1581
                },
                {
                    "start": 1582,
                    "end": 1790
                },
                {
                    "start": 1791,
                    "end": 1863
                },
                {
                    "start": 1864,
                    "end": 1953
                }
            ],
            "ref_mentions": [
                {
                    "start": 169,
                    "end": 194,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 194,
                    "end": 215,
                    "matchedPaperCorpusId": "247188139"
                },
                {
                    "start": 645,
                    "end": 664,
                    "matchedPaperCorpusId": "263828724"
                },
                {
                    "start": 733,
                    "end": 750,
                    "matchedPaperCorpusId": "252762395"
                },
                {
                    "start": 1053,
                    "end": 1070,
                    "matchedPaperCorpusId": "252762395"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.884765625
        },
        {
            "corpus_id": "268667440",
            "title": "FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions",
            "text": "Modern language models (LMs) are extensively tuned to be able to follow user instructions faithfully (Chung et al., 2022;Ouyang et al., 2022a;Rafailov et al., 2023;Wang et al., 2023b;Ivison et al., 2023) and safely (Bai et al., 2022;Bianchi et al., 2024).Through these capabilities, LMs are able to successfully tackle a broad range of tasks (Chiang et al., 2024;Liang et al., 2023;Yang et al., 2023;Jimenez et al., 2024;Zeng et al., 2023), even when not explicitly fine-tuned for them.\n\nIn contrast to the broader LM community, information retrieval (IR) practitioners and researchers have yet to fully exploit instruction-tuned models.Thanks to their ability to effectively estimate semantic similarity between query and documents, LMs have been adopted as the main backbone of neural retrieval architectures (Karpukhin et al., 2020;Khattab & Zaharia, 2020;Reimers & Gurevych, 2019).However, the vast majority of these systems are fine-tuned to operate exclusively as text spans similarity estimators (Khattab & Zaharia, 2020;Izacard et al., 2021;Nogueira & Cho, 2019;Pradeep et al., 2023;Ma et al., 2023).Moving past these ad-hoc search systems to retrieve with instructions would enable support for complex information needs.For example, imagine a researcher seeking to identify papers that must contain numerous qualities to be relevant (from a given venue, using a particular class of methods, etc.) while also making sure to avoid conditions that would make the document not-relevant (using negative sentiment, using datasets from a given domain, etc.).\n\nRecent work has started to move towards search with instructions, but this topic is still understudied with only a handful of papers (Su et al., 2022;Asai et al., 2022;Muennighoff et al., 2024).",
            "score": 0.4118462932192247,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 255
                },
                {
                    "start": 255,
                    "end": 486
                },
                {
                    "start": 488,
                    "end": 637
                },
                {
                    "start": 637,
                    "end": 885
                },
                {
                    "start": 885,
                    "end": 1108
                },
                {
                    "start": 1108,
                    "end": 1229
                },
                {
                    "start": 1229,
                    "end": 1560
                },
                {
                    "start": 1562,
                    "end": 1756
                }
            ],
            "ref_mentions": [
                {
                    "start": 121,
                    "end": 142,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 142,
                    "end": 164,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 382,
                    "end": 400,
                    "matchedPaperCorpusId": "258967184"
                },
                {
                    "start": 400,
                    "end": 421,
                    "matchedPaperCorpusId": "263829697"
                },
                {
                    "start": 835,
                    "end": 859,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 1003,
                    "end": 1028,
                    "matchedPaperCorpusId": "216553223"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75537109375
        },
        {
            "corpus_id": "235166616",
            "title": "Retrieval Enhanced Model for Commonsense Generation",
            "text": "Dataset CommonGen is a benchmark dataset designed to diagnose whether a model has the ability of generative commonsense reasoning (Lin et al., 2020). This dataset contains 32,651/993/1,497 concept sets for training/development/test, and the numbers of corresponding sentences are 67,389/4,018/7,644. We use BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Banerjee and Lavie, 2005), CIDEr (Vedantam et al., 2015) and SPICE (Anderson et al., 2016) as evaluation metrics. Because SPICE correlates the most with human evaluation (Lin et al., 2020), we take SPICE as the primary metric.\n\nExternal Corpora To be consistent with the distribution of the CommonGen dataset, we use VATEX , Activity (Krishna et al., 2017), SNLI (Bowman et al., 2015) and MNLI (Williams et al., 2018) as external corpora. We sample 500k sentences from these corpora to construct our pre-training dataset. Meanwhile, these datasets are also used as our sentence pool for the retrieval module. For both the pre-training and fine-tuning, all sentences that appear in the Com-monGen targets are not used as retrieval sentences candidates.\n\nBaselines We compare RE-T5 with several baseline systems. GPT-2, BERT-Gen, UniLM, BART, and T5 are pre-trained language models tested in Concept Set: trailer shirt side sit road T5: A man sits on the side of a trailer and a shirt. Matching Retriever:\n\n(1)Two guys in red shirts are sitting on chairs, by the side of the road, behind that open trailer.\n\n(2)Two men, one wearing a straw cone hat, blue shirt, talking with a guy in a tan sunhat, red plaid shirt, both with baskets in front of them, sitting on the side of a dirt road.\n\n(3)An older guy with a tan shirt and hat sitting on the side of a road with bricks all around him and a small green bowl on the side. RE-T5(matching retriever)",
            "score": 0.41045944639441756,
            "section_title": "Experiments Settings",
            "char_start_offset": 6703,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 130,
                    "end": 148,
                    "matchedPaperCorpusId": "218500588"
                },
                {
                    "start": 312,
                    "end": 335,
                    "matchedPaperCorpusId": "11080756"
                },
                {
                    "start": 343,
                    "end": 354,
                    "matchedPaperCorpusId": "964287"
                },
                {
                    "start": 363,
                    "end": 389,
                    "matchedPaperCorpusId": "7164502"
                },
                {
                    "start": 397,
                    "end": 420,
                    "matchedPaperCorpusId": "9026666"
                },
                {
                    "start": 431,
                    "end": 454,
                    "matchedPaperCorpusId": "11933981"
                },
                {
                    "start": 534,
                    "end": 552,
                    "matchedPaperCorpusId": "218500588"
                },
                {
                    "start": 698,
                    "end": 720,
                    "matchedPaperCorpusId": "1026139"
                },
                {
                    "start": 758,
                    "end": 781,
                    "matchedPaperCorpusId": "3432876"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64404296875
        },
        {
            "corpus_id": "262064851",
            "title": "Generating Novel Leads for Drug Discovery using LLMs with Logical Feedback",
            "text": "A language model is a probabilistic model of natural language that learns a probability distribution over sequences of tokens called sentences. Let W denote one such sentence with N tokens, W = (w 1 , . . . , w N ), where, N is arbitrary. A language model estimates the probability of observing the sentence W , denoted by the joint probability P (W ). In practice, however, P (W ) is approximated using n-gram models (Jelinek 1980;Katz 1987) or Neural language models or NLMs (Bengio, Ducharme, and Vincent 2000). These models use Markov's assumption that the probability of a word \n\nThe Thirty-Eighth AAAI Conference on Artificial Intelligence  depends only on the previous n < N words. That is, \n\nNeural language models or NLM (Bengio, Ducharme, and Vincent 2000) are probabilistic language models based on (deep) neural networks that can handle the problems associated with n-gram models, such as handling long-range dependency, context understanding, handling noise and ambiguity, learning complex relationships by learning a distributed representation of text tokens. An NLM approximates each term on the right-hand side of Eq. ( 1) using a neural network. Large Language Models (LLMs) such as GPTs (Radford et al. 2019) and PALM (Narang and Chowdhery 2022) are large and complex neural language models that use the transformer architecture (Vaswani et al. 2017) to learn from vast and diverse corpora of text data. \n\nPrompt-based LLMs, such as ChatGPT and BARD are LLMs that can learn with human-feedback (Ouyang et al. 2022). A prompt is an input sequence written by a human in a natural language, which serves as the starting point that sets the context of the LLM to generate the next (highly) probable text sequence in an auto-regressive manner.",
            "score": 0.41045944639441756,
            "section_title": "Language Models as Probabilistic Machines",
            "char_start_offset": 7338,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 582
                },
                {
                    "start": 585,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 697
                },
                {
                    "start": 700,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1162
                },
                {
                    "start": 1163,
                    "end": 1421
                },
                {
                    "start": 1424,
                    "end": 1533
                },
                {
                    "start": 1534,
                    "end": 1756
                }
            ],
            "ref_mentions": [
                {
                    "start": 418,
                    "end": 432,
                    "matchedPaperCorpusId": "61012010"
                },
                {
                    "start": 432,
                    "end": 442,
                    "matchedPaperCorpusId": "6555412"
                },
                {
                    "start": 1512,
                    "end": 1532,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.603515625
        },
        {
            "corpus_id": "209331668",
            "title": "An adaptive term proximity based rocchio\u2019s model for clinical decision support retrieval",
            "text": "After comparing the proposed methods with essential retrieval model BM25 and KL-divergence language modeling (LM) retrieval, we show the experimental results in Table 4. \n\nAs is shown in Table 4, BM25 performs slightly better with Dirichlet prior in terms of MAP and P@10 metrics, and LM is superior to it in the rest of metrics. These two basic models perform comparatively, without observing significant different. \n\nNext, \"BM25+Rocchio\", the combination of BM25 and Rocchio's model, and relevance language model (RM3) are used as two strong baselines in this paper. These two methods can achieve better retrieval performance in most cases. Using them as the basic models of the PRF baselines is reasonable. Thus, we use them to compare the proposed model. \n\nWe demonstrate the results of baseline PRF models and the three proposed models (HRoc1, HRoc2, and HRoc3) with different evaluation metrics in Table 5. In particular, \"*\" and \"+\" indicate a statistically significant improvement over BM25+Rocchio and RM3 respectively (Wilcoxon signed-rank test with p < 0.05). The bold style in row corresponding to the best result. And Rocchio is used with BM25 and RM3 is used with LM for fair comparison. \n\nTable 5 shows that the average performance of proposed models is better than that of baseline models. First, it has been proved that both Rocchio and RM3 are effective. They are considered to be strong baselines in previous studies. Rocchio model is superior to the RM3 model in terms of P@5 and P@10 metrics, but RM3 model outperforms Rocchio model in terms of MAP, P@20 and F1value. Second, among the three proposed models, HRoc1 performs better retrieval results than HRoc2 or HRoc3  The values in parentheses represent the improvements over BM25+Rocchio and RM3 respectively. The best result obtained is shown in bold, and the superscripts \"*\" and \"+\" denote statistically significant improvements over BM25+Rocchio and RM3, respectively (Wilcoxon signed-rank test with p < 0.05) \n\nin terms of all metrics.",
            "score": 0.41045944639441756,
            "section_title": "Comparison with pRF basic models",
            "char_start_offset": 18697,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 172,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 416
                },
                {
                    "start": 419,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 758
                },
                {
                    "start": 761,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 1070
                },
                {
                    "start": 1071,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1201
                },
                {
                    "start": 1204,
                    "end": 1305
                },
                {
                    "start": 1306,
                    "end": 1372
                },
                {
                    "start": 1373,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1588
                },
                {
                    "start": 1589,
                    "end": 1783
                },
                {
                    "start": 1784,
                    "end": 1987
                },
                {
                    "start": 1990,
                    "end": 2014
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96044921875
        },
        {
            "corpus_id": "62643170",
            "title": "Modeling of Long Distance Context Dependency",
            "text": "Language modeling is the attempt to characterize, capture and exploit the regularities and constraints in a natural language and has been successfully applied to many domains. Among all the language modeling approaches, ngram models have been most widely used in speech recognition (Jelinek 1990;Gale and Church 1990;Brown et al. 1992;Yang et al. 1996) and other applications. While ngram models are simple in language modeling and have been successfully used in speech recognition and other tasks, they have obvious deficiencies. \n\nFor instance, ngram models can only capture the short-distance dependency within an nwords window where currently the largest practical N for a natural language is three. \n\nIn the meantime, it is found that there always exist many preferred relationships between words. Two highly associated word pairs are \"not only/but also\" and \"doctor/nurse\". Psychological experiments in Meyer D. et al. (1975) indicated that the human's reaction to a highly associated word pair was stronger and faster than that to a poorly associated word pair. Such preference information is very useful for natural language processing (Church K.W. et al. 1990;Hiddle D. et al. 1993;Rosenfeld R. 1994 andZhou G.D. et al.1998). Obviously, the preference relationships between words can expand from a short to long distance. While we can use conventional ngram models to capture the short distance dependency, the long distance dependency should also be exploited properly. \n\nThe purpose of this paper is to propose a new modeling approach to capture the context dependency over both a short distance and a long distance and apply it in Mandarin speech recognition. \n\nThis paper is organized as follows. In Section 2, we present the normal ngram modeling while a new modeling approach, named MI-ngram modeling, is proposed in Section 3. In Section 4, we will describe its use in our Mandarin speech recognition system. Finally we give a summary of this paper.",
            "score": 0.41045944639441756,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 530
                },
                {
                    "start": 533,
                    "end": 703
                },
                {
                    "start": 706,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1479
                },
                {
                    "start": 1482,
                    "end": 1671
                },
                {
                    "start": 1674,
                    "end": 1709
                },
                {
                    "start": 1710,
                    "end": 1924
                },
                {
                    "start": 1925,
                    "end": 1965
                }
            ],
            "ref_mentions": [
                {
                    "start": 282,
                    "end": 296,
                    "matchedPaperCorpusId": "6633939"
                },
                {
                    "start": 296,
                    "end": 317,
                    "matchedPaperCorpusId": "10164826"
                },
                {
                    "start": 317,
                    "end": 335,
                    "matchedPaperCorpusId": "10986188"
                },
                {
                    "start": 1144,
                    "end": 1169,
                    "matchedPaperCorpusId": "10164826"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72509765625
        },
        {
            "corpus_id": "276482242",
            "title": "A Survey of Model Architectures in Information Retrieval",
            "text": "In the decoding phase, DSI uses all DocIDs to construct a trie and performs constrained decoding. Followup works (Wang et al., 2022;Bevilacqua et al., 2022) further improve upon this paradigm with strategies to construct semantic DocIDs and enable robust training. A significant challenge for generative retrieval is scalability to larger corpus (Pradeep et al., 2023a). Zeng et al. (2024) utilized the residual quantization technique (Chen et al., 2010) to construct hierarchical DocIDs and achieved comparable performance to dense retrievers on MS MARCO dataset. Generative retrieval is an active research area; see (Li et al., 2024b) for a more comprehensive review. \n\nRemarks We note that the adoption of LLMs in IR model architectures primarily follows two main themes: (1) feature extraction, and (2) relevance estimation, as discussed in \u00a7 1. For example, LLMs' semantic knowledge enables their strong performance in being the backbone of a retriever; and instruction-following LLMs can be directly prompted for relevance estimation. Generative re-trieval and cross-encoder LLM reranking models are trained end-to-end for both feature extraction and relevance estimation. While LLMs have shown promise, several challenges and open questions remain, which leaves room for discussion ( \u00a7 8).",
            "score": 0.41045944639441756,
            "section_title": "Large Language Models for IR",
            "char_start_offset": 22566,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 669
                },
                {
                    "start": 672,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1296
                }
            ],
            "ref_mentions": [
                {
                    "start": 113,
                    "end": 132,
                    "matchedPaperCorpusId": "249395549"
                },
                {
                    "start": 346,
                    "end": 369,
                    "matchedPaperCorpusId": "258822999"
                },
                {
                    "start": 435,
                    "end": 454,
                    "matchedPaperCorpusId": "33774240"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5966796875
        },
        {
            "corpus_id": "12631541",
            "title": "Word or Phrase? Learning Which Unit to Stress for Information Retrieval",
            "text": "Retrieval models: We have set two retrieval models, namely the word model and the (phrase-based) one-parameter model, as baselines. The ranking function of the word model is equivalent to Eq. 2, with \u03bb in Eq. 3 being set to zero (i.e. the phrase probability makes no effect on the ranking). The ranking function of the one-parameter model is also equivalent to Eq. 2, with \u03bb in Eq. 3 used \"as is\" (i.e. as a constant parameter value optimized using gradient descent method, without being replaced to a logistic function). Both baseline models cannot differentiate the importance of phrases in a query. To make a distinction from the baseline models, we will name our proposed method as a multi-parameter model. \n\nIn our experiments, all the probabilities in all retrieval models are smoothed with the collection statistics by using dirichlet priors (Zhai and Lafferty, 2001).",
            "score": 0.4104310832274304,
            "section_title": "Experimental Setup",
            "char_start_offset": 16973,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 710
                },
                {
                    "start": 713,
                    "end": 875
                }
            ],
            "ref_mentions": [
                {
                    "start": 849,
                    "end": 874,
                    "matchedPaperCorpusId": "5793143"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87890625
        },
        {
            "corpus_id": "278715464",
            "title": "Improve Rule Retrieval and Reasoning with Self-Induction and Relevance ReEstimate",
            "text": "The Generation Augmented Retrieval (GAR) (Mao et al., 2021) is a common approach that leverages the capabilities of language models to perform query decomposition (Chen et al., 2024b), query rewriting (Ma et al., 2023), and query expansion (Wang et al., 2023), helping to supplement missing background knowledge in queries to achieve higher retrieval quality. In addition to passage retrieval, GAR can also play a role in code retrieval (Li et al., 2024). Our SIAR can be seen as a type of GAR that utilizes the self-inductive abilities of large language models (LLMs).",
            "score": 0.4098239469001296,
            "section_title": "Generation Augmented Retrieval",
            "char_start_offset": 25538,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 569
                }
            ],
            "ref_mentions": [
                {
                    "start": 41,
                    "end": 59,
                    "matchedPaperCorpusId": "221802772"
                },
                {
                    "start": 163,
                    "end": 183,
                    "matchedPaperCorpusId": "271884400"
                },
                {
                    "start": 240,
                    "end": 259,
                    "matchedPaperCorpusId": "257505063"
                },
                {
                    "start": 437,
                    "end": 454,
                    "matchedPaperCorpusId": "266899699"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8984375
        },
        {
            "corpus_id": "274982275",
            "title": "Formal Language Knowledge Corpus for Retrieval Augmented Generation",
            "text": "RAG is a sophisticated framework designed to enhance language models by coupling them with external retrieval systems, addressing limitations inherent in static, solely parameter-based language models. RAG integrates a dual-component architecture where a retriever dynamically searches a structured external corpus for relevant information based on the input query, and a generator LLM uses the retrieved content as context to generate accurate and contextually enriched responses [Gao et al., 2023, Mialon et al., 2023]. This setup mitigates common issues such as hallucinations and factual inaccuracies in language models by grounding generated text in real-world, verified information. In practice, RAG systems employ dense vector embeddings to ensure retrieval relevance, capturing semantic relationships within documents beyond mere keyword matching. The retrieved information is subsequently fed into the generator, allowing it to synthesize data with pre-existing knowledge for enhanced coherence and contextual accuracy. \n\nRAGs can employ two main types of retrieval mechanisms: dense and sparse [Mialon et al., 2023]. Sparse retrievers rely on bag-of-words representations, excelling at finding documents with high term overlap to the query, while dense retrievers utilize neural network embeddings to capture semantic similarities, enhancing the model's comprehension of related concepts. By appending retrieved documents directly to the model's context, these retrievers allow the language model to ground its responses in a broader context, thereby increasing accuracy and factual consistency across complex tasks. \n\nThe success of retrieval-augmented models in various domains has catalyzed interest in their application to more demanding reasoning tasks. A great representative of such tasks is the construction and verification of mathematical proofs, which requires solving problems step-by-step, and generating precise mathematical statements. Recent approaches, such as chain-of-thought (CoT) prompting [Lewkowycz et al., 2022] combined with retrieval, highlight the potential for retrieval-augmented models to provide sequential reasoning support. These models can generate reasoning paths interspersed with retrieval steps to guide complex problem-solving processes, such as multi-step question answering, enabling models to leverage external information dynamically at each reasoning stage.",
            "score": 0.40813157619824786,
            "section_title": "A. Retrieval-Augmented Language Models",
            "char_start_offset": 3315,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 1028
                },
                {
                    "start": 1031,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1398
                },
                {
                    "start": 1399,
                    "end": 1626
                },
                {
                    "start": 1629,
                    "end": 1768
                },
                {
                    "start": 1769,
                    "end": 1960
                },
                {
                    "start": 1961,
                    "end": 2166
                },
                {
                    "start": 2167,
                    "end": 2411
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9755859375
        },
        {
            "corpus_id": "269921622",
            "title": "Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation",
            "text": "The transformer model has shown proficiency in retrieval tasks, encoding item information within its parameters, which is a method termed Differentiable Search Index (DSI) (Tay et al., 2022).DSI involves two key training tasks for pre-trained language models: Learn to Index (L2I) and Learn to Retrieve (L2R), which can be used to train a model jointly or in a sequential order.L2I focuses on mapping item content, such as movie description, to item indices, exemplified by linking a description of Edge of Tomorrow to its title: DSI models are originally proposed for text retrieval tasks (Tay et al., 2022), yet their formulation can be connected to LLMs used in CRS.Considering the LLMs as CRS framework proposed in (He et al., 2023) through the lens of DSI, we observe that:\n\n\u2022 Item Indexing: LLMs index items by using the item titles (e.g., \"Edge of Tomorrow\") as the item identifiers via L2I.\n\n\u2022 Item Recommendation: LLMs use conversational context as queries to generate item indices via L2R.\n\nThus, LLMs inherently function as DSI models, by including a certain number of training samples for L2I and L2R tasks in their pre-training corpus.Compared to common two-tower models, DSI models require only a single model for item recommendations, by indexing item information into its parameters (Tay et al., 2022).",
            "score": 0.40787925531084995,
            "section_title": "Differentiable Search Index (DSI)",
            "char_start_offset": 6234,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 191,
                    "end": 378
                },
                {
                    "start": 378,
                    "end": 669
                },
                {
                    "start": 669,
                    "end": 778
                },
                {
                    "start": 780,
                    "end": 898
                },
                {
                    "start": 900,
                    "end": 999
                },
                {
                    "start": 1001,
                    "end": 1148
                },
                {
                    "start": 1148,
                    "end": 1318
                }
            ],
            "ref_mentions": [
                {
                    "start": 172,
                    "end": 190,
                    "matchedPaperCorpusId": "246863488"
                },
                {
                    "start": 590,
                    "end": 607,
                    "matchedPaperCorpusId": "246863488"
                },
                {
                    "start": 719,
                    "end": 736,
                    "matchedPaperCorpusId": "261049680"
                },
                {
                    "start": 1299,
                    "end": 1317,
                    "matchedPaperCorpusId": "246863488"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92626953125
        },
        {
            "corpus_id": "219558578",
            "title": "Combination of abstractive and extractive approaches for summarization of long scientific texts",
            "text": "The language modeling task in the general case is a process of learning the joint probability function of sequences of words in a natural language. Statistical Language Modeling, or Language Modeling, is the development of probabilistic models that are able to predict the next word in the sequence given the words that precede, it also known as context. Language models could operate on different sequence levels. Small language models work with sequences of chars and words. While the big language model works with sentences. But most common language model operates with sequences of words.\n\nThe language model could be used standalone to predict words according to context but usually, the language model is used to solve more challenging tasks. For example, it helps to solve a large range of natural language tasks, such as machine translation, natural language understanding, speech recognition, information retrieval, text summarization. In other words, language models are used in most real-world natural language processing applications and the quality of such applications mostly depends on the language models performance. Thats why language modeling task plays a major role in natural language processing and artificial intelligence research.\n\nThe first neural language model used feed-forward architecture. One of the main features of using neural language models is getting representation vector of words sequences. These word vectors usually called embedding vector [11] and embeddings for similar words located closer to each other in dimension space, also having similar representations. After successful usage of feed-forward networks, recurrent neural networks [5] achieved better results in language modeling tasks because of its ability to take into account the position of words in sentences and producing contextual word embeddings [13]. Long Short-Term Memory networks [6] allows the language model to learn the relevant context of longer sequences than feed-forward or RNN's, because of its more sophisticated memory mechanism. Next, the attention mechanism [6] made improvement in language modeling tasks with a combination of sequence to sequence framework [18]. The attention mechanism improves memory mechanism of recurrent neural networks by giving the ability for the decoder network to look at the whole context.\n\nThe next big step in the language modeling task was developing transformer [19] architecture, with novel self-attention mechanism, that helps the model to use the context of the sentence more efficiently. Such models could take into account both left and right context of the sequence",
            "score": 0.40779355586674265,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1481,
                    "end": 1485,
                    "matchedPaperCorpusId": "16447573"
                },
                {
                    "start": 1680,
                    "end": 1683,
                    "matchedPaperCorpusId": "15953218"
                },
                {
                    "start": 1893,
                    "end": 1896,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 2083,
                    "end": 2086,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 2184,
                    "end": 2188,
                    "matchedPaperCorpusId": "7961699"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8671875
        },
        {
            "corpus_id": "2570211",
            "title": "Learning Word Representations by Jointly Modeling Syntagmatic and Paradigmatic Relations",
            "text": "Vector space models of language represent each word with a real-valued vector that captures both semantic and syntactic information of the word. The representations can be used as basic features in a variety of applications, such as information retrieval (Manning et al., 2008), named entity recognition (Collobert et al., 2011), question answering (Tellex et al., 2003), disambiguation (Sch\u00fctze, 1998), and parsing (Socher et al., 2011).\n\nA common paradigm for acquiring such representations is based on the distributional hypothesis (Harris, 1954;Firth, 1957), which states that is wolf The a fierce animal. words occurring in similar contexts tend to have similar meanings. Based on this hypothesis, various models on learning word representations have been proposed during the last two decades. According to the leveraged distributional information, existing models can be grouped into two categories (Sahlgren, 2008). The first category mainly concerns the syntagmatic relations among the words, which relate the words that co-occur in the same text region. For example, \"wolf\" is close to \"fierce\" since they often co-occur in a sentence, as shown in Figure 1. This type of models learn the distributional representations of words based on the text region that the words occur in, as exemplified by Latent Semantic Analysis (LSA) model (Deerwester et al., 1990) and Non-negative Matrix Factorization (NMF) model (Lee and Seung, 1999). The second category mainly captures paradigmatic relations, which relate words that occur with similar contexts but may not cooccur in the text. For example, \"wolf\" is close to \"tiger\" since they often have similar context words. This type of models learn the word representations based on the surrounding words, as exemplified by the Hyperspace Analogue to Language (HAL) model (Lund et al., 1995), Continuous Bag-of-Words (CBOW) model and Skip-Gram (SG) model (Mikolov et al., 2013a).\n\nIn this work, we argue that it is important to take both syntagmatic and paradigmatic relations into account to build a good distributional model. Firstly, in distributional meaning acquisition, it is expected that a good representation should",
            "score": 0.4077349999466323,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 304,
                    "end": 328,
                    "matchedPaperCorpusId": "351666"
                },
                {
                    "start": 349,
                    "end": 370,
                    "matchedPaperCorpusId": "846801"
                },
                {
                    "start": 387,
                    "end": 402,
                    "matchedPaperCorpusId": "8754851"
                },
                {
                    "start": 416,
                    "end": 437,
                    "matchedPaperCorpusId": "18690358"
                },
                {
                    "start": 549,
                    "end": 561,
                    "matchedPaperCorpusId": "208093066"
                },
                {
                    "start": 905,
                    "end": 921,
                    "matchedPaperCorpusId": "23750999"
                },
                {
                    "start": 1342,
                    "end": 1367,
                    "matchedPaperCorpusId": "3252915"
                },
                {
                    "start": 1418,
                    "end": 1439,
                    "matchedPaperCorpusId": "4428232"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8662109375
        },
        {
            "corpus_id": "268667440",
            "title": "FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions",
            "text": "Using instructions in retrieval is a nascent area of exploration.Su et al. (2022) and Asai et al. (2022) were two of the earliest works that trained a retrieval model to use instructions along with the query.However, these instructions are typically very short, such as \"Retrieve a Wikipedia paragraph that answers this question.\"Recent work incorporates instructions in smaller models (Xiao et al., 2023;Chen et al., 2023Chen et al., , 2024) ) as well as others which use Llama (Touvron et al., 2023a;Weller et al., 2023) or Mistral (Jiang et al., 2023) as the backbone of a larger retrieval model that can use instructions: GritLM (Muennighoff et al., 2024) trains Mistral to do both generation and embedding, while Wang et al. (2023a) uses Mistral for embeddings only.\n\nDespite this flurry of activity, these efforts do not have an explicit instruction-related retrieval benchmark to evaluate on.Instead, they evaluate on standard retrieval benchmark suites such as MTEB (Muennighoff et al., 2022) and BEIR (Thakur et al., 2021) which do not contain instructions.Thus, these newer instruction retrieval models hand-write a few instructions, where typically each instruction is applied to an entire dataset, irrespective of the query.This makes these instructions generic: focused only on the task format, format of the \"document\" (paragraph, sentence, etc.), and the broad domain.Note that because of this, no current instructions contain any extra background information or negation (Weller et al., 2024) which are commonly found in real-world instructions (see Figure 1 for an example of these differences).\n\nIn work concurrent to ours, Oh et al. (2024) also propose a dataset to evaluate instructions in retrieval models.",
            "score": 0.406689900393956,
            "section_title": "Instructions for Retrieval",
            "char_start_offset": 8215,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 65
                },
                {
                    "start": 65,
                    "end": 208
                },
                {
                    "start": 208,
                    "end": 330
                },
                {
                    "start": 330,
                    "end": 771
                },
                {
                    "start": 773,
                    "end": 899
                },
                {
                    "start": 899,
                    "end": 1066
                },
                {
                    "start": 1066,
                    "end": 1236
                },
                {
                    "start": 1236,
                    "end": 1383
                },
                {
                    "start": 1383,
                    "end": 1612
                },
                {
                    "start": 1614,
                    "end": 1727
                }
            ],
            "ref_mentions": [
                {
                    "start": 633,
                    "end": 659,
                    "matchedPaperCorpusId": "260460088"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.41015625
        },
        {
            "corpus_id": "2268908",
            "title": "Leveraging Effective Query Modeling Techniques for Speech Recognition and Summarization",
            "text": "Language modeling is a critical and integral component in any large vocabulary continuous speech recognition (LVCSR) system (Huang et al., 2001;Jurafsky and Martin, 2008;Furui et al., 2012). More concretely, the role of language modeling in LVCSR can be interpreted as calculating the conditional probability P(w|H), in which H is a search history, usually expressed as a sequence of words H=h 1 , h 2 ,\u2026, h L , and w is one of its possible immediately succeeding words. Once the various aforementioned query modeling methods are applied to speech recognition, for a search history H, we can conceptually regard it as a query and each of its immediately succeeding words w as a (single-word) document. Then, we may leverage an IR procedure that takes H as a query and poses it to a retrieval system to obtain a set of top-ranked documents from a contemporaneous (or in-domain) corpus. Finally, the enhanced query model (that is P(w|H) in speech recognition) can be estimated by RM, SMM, RSMM or QMM, and further combined with the background n-gram (e.g., trigram) language model to form an adaptive language model to guide the speech recognition process.",
            "score": 0.4064586197702927,
            "section_title": "Speech Recognition",
            "char_start_offset": 9675,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 170,
                    "end": 189,
                    "matchedPaperCorpusId": "18852129"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83154296875
        },
        {
            "corpus_id": "271329267",
            "title": "Recent Advances in Generative AI and Large Language Models: Current Status, Challenges, and Perspectives",
            "text": "The use of language models is pervasive in various modern NLP applications. In these models, the probability of different sequences of words is often modeled as the product of local probabilities, as expressed in Equation 13, where w i represents the i th word in the sequence, and h i represents the word history preceding w i . The formulation in Equation 13summarizes the conditional dependencies between words in a sequence, allowing language models to capture complex linguistic patterns. Leveraging such models has proven instrumental in tasks ranging from machine translation and speech recognition to text generation and sentiment analysis [1], [2]. \n\nThe following are some of the main approaches to traditional and modern approaches to language modeling.",
            "score": 0.4064586197702927,
            "section_title": "III. LANGUAGE MODELING",
            "char_start_offset": 38851,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 75
                },
                {
                    "start": 76,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 657
                },
                {
                    "start": 660,
                    "end": 764
                }
            ],
            "ref_mentions": [
                {
                    "start": 648,
                    "end": 651,
                    "matchedPaperCorpusId": "12639289"
                },
                {
                    "start": 653,
                    "end": 656,
                    "matchedPaperCorpusId": "11080756"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.796875
        },
        {
            "corpus_id": "237504737",
            "title": "KFCNet: Knowledge Filtering and Contrastive Learning Network for Generative Commonsense Reasoning",
            "text": "To evaluate generation performance, we use BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), and METEOR (Banerjee and Lavie, 2005), in addition to evaluation metrics for captioning tasks, namely CIDEr (Vedantam et al., 2015) and SPICE (Anderson et al., 2016). As all metrics score the output in the range [0, 100], we also present the average score across all metrics. Table 2: Overall performance of the different models on CommonGen (v1.0). Models in the first block are finetuned pre-trained language models without external knowledge; models in the second block use out-of-domain knowledge; models in the last two blocks use in-domain knowledge, where the KG-BART uses ConceptNet, and both EKI and KFCNet (our model) use the in-domain prototype corpus as a knowledge base.",
            "score": 0.4064586197702927,
            "section_title": "Evaluation Metrics",
            "char_start_offset": 20121,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 775
                }
            ],
            "ref_mentions": [
                {
                    "start": 48,
                    "end": 71,
                    "matchedPaperCorpusId": "11080756"
                },
                {
                    "start": 79,
                    "end": 90,
                    "matchedPaperCorpusId": "964287"
                },
                {
                    "start": 103,
                    "end": 129,
                    "matchedPaperCorpusId": "7164502"
                },
                {
                    "start": 200,
                    "end": 223,
                    "matchedPaperCorpusId": "9026666"
                },
                {
                    "start": 234,
                    "end": 257,
                    "matchedPaperCorpusId": "11933981"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2318115234375
        },
        {
            "corpus_id": "235097450",
            "title": "NARNIA at NLP4IF-2021: Identification of Misinformation in COVID-19 Tweets Using BERTweet",
            "text": "A vast number of Language Models have been developed in the last decade. We used a number of them to solve the given problem, and they are described in the following subsections.",
            "score": 0.4064586197702927,
            "section_title": "Model Description",
            "char_start_offset": 5422,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 72
                },
                {
                    "start": 73,
                    "end": 178
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.060302734375
        },
        {
            "corpus_id": "272310589",
            "title": "MemLong: Memory-Augmented Retrieval for Long Text Modeling",
            "text": "Language models are designed to define probability distributions over sequences of tokens, effectively predicting the likelihood of a sequence within a given language. Given such a sequence x 1 , . . . , x n , the standard approach to modeling its probability is via the next-token prediction: p(x 1 , . . . , x n ) = n i=0 p \u03b8 (x i |x <i ), where x <i := x 1 , . . . , x i\u22121 is the sequence of tokens proceeding x i . Differently from the standard language modeling objective, we not only use the current context to make next-token predictions, but also utilize external retrieval to obtain relevant information and perform knowledge fusion at the upper layers of the model. Specifically, given a sequence consisting of l tokens and the size of each chunk \u03c4 , we partition it into a long sequence of \u03bd = l \u03c4 non-overlapping chunks , which denoted as C = (c 1 , . . . , c \u03bd ). Correspondingly, its textual form is divided into \u03bd text chunks, which denoted as T = (t 1 , . . . , t \u03bd ). In each step, we perform causal language modeling on c i in the lower layers, while in the upper layers, we conduct finegrained controllable retrieval on t i for the fusion of additional information. After do this, our language modeling objective becomes \n\nwhere R(t i ) denotes the retrieval of neighboring chunks of t i where x i is located.",
            "score": 0.4064586197702927,
            "section_title": "Task Definition",
            "char_start_offset": 4595,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1184
                },
                {
                    "start": 1185,
                    "end": 1239
                },
                {
                    "start": 1242,
                    "end": 1328
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8701171875
        },
        {
            "corpus_id": "8048647",
            "title": "Bayesian Word Alignment for Massively Parallel Texts",
            "text": "The model used in this work uses a common representation of concepts generated by a Chinese Restaurant Process (CRP), which is aligned to each of the languages in a corpus using the model of Mermer and Sarac \u00b8lar (2011). \n\nTable 1 introduces the variables (observed and latent) as well as the hyperparameters of the model. Basically, the model consists of a common representation c (where token i of sentence s is denoted c si ), which is aligned to one or more words w lsj (from language l, sentence s, token j) through a set of alignment variables a lsj which contain the index within c s that w lsj is linked to. \n\nThe probability of an assignment c is: \n\nwhere n e is the number of occurrences of concept type e in the assignment c, and n = e n e is the (fixed) total number of tokens in the common representation. \n\nFor the translation probabilities, I follow Mermer and Sarac \u00b8lar (2011) in assuming that p(f l |e) \u223c Dir(t l ; \u03b8 l ), and that the priors \u03b8 l are symmetric (i.e. all values in these vectors are equal, \u03b8 lef = \u03b2). By specifying a low value for \u03b2 (a sparse prior), we can encode our prior knowledge that translation probability functions p(f l |e) tend to have a low entropy, or in other words, that each concept is typically only translated into a very small number of words in each language. \n\nThe joint probability of the common representation and the alignments is given by: \n\nwhere p(c; \u03b1) = CRP(c; \u03b1) and the remaining factors are the same as in Mermer and Sarac \u00b8lar (2011) with the common representation being the \"target language\", except that there is a product across all languages l. Note that since word order is not modeled, p(a|c) is constant.",
            "score": 0.4064586197702927,
            "section_title": "Model",
            "char_start_offset": 4069,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 223,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 615
                },
                {
                    "start": 618,
                    "end": 656
                },
                {
                    "start": 659,
                    "end": 818
                },
                {
                    "start": 821,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1034
                },
                {
                    "start": 1035,
                    "end": 1313
                },
                {
                    "start": 1316,
                    "end": 1398
                },
                {
                    "start": 1401,
                    "end": 1615
                },
                {
                    "start": 1616,
                    "end": 1678
                }
            ],
            "ref_mentions": [
                {
                    "start": 191,
                    "end": 219,
                    "matchedPaperCorpusId": "8216700"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.19482421875
        },
        {
            "corpus_id": "8374684",
            "title": "Parameterized Neural Network Language Models for Information Retrieval",
            "text": "Language models are probabilistic generative models of text -viewed as a sequence of terms. If a text is composed of a sequence of terms t 1...n , where each t i corresponds to a word in a pre-defined vocabulary, we can compute the probability of observing this sequence given the language model M as: \n\nLanguage models are used in IR as a simple yet effective way to compute the relevance of a document to a query [27]. There exists different types of language models for IR, but one of the most standard is to equate the relevance of the document d with the likelihood of the language model generating the query, using the evaluated document language model M d . \n\nwhere M d is the so-called document language model, which is the model within the family of models M that maximizes the probability of observing the document d composed of terms d 1...N , that is: \n\nAmong the different families of generative models, the n-gram multinomial family is the most used in IR, with n usually equal to 1. The multinomial model assumes the independence of terms given the n \u2212 1 previous ones in the sequence. Formally, if M is within this family, then \n\nwhere \u03b8 is a conditional probability table giving the probability of observing the term t i after having observed the sequence t i\u2212n+1...i\u22121 . \n\nFor a given document, the parameters \u03b8 that maximize equation ( 1) can be computed in a closed form formula: \n\nwhere \u2022 correspond to any term of the vocabulary (i.e. t i\u2212n+1...i\u22121 \u2022 corresponds to the sequence t i\u2212n+1 , . . . , t i\u22121 , u where u \u2208 W). With n = 1, we get a simple unigram model that does not consider the context of the term (note that in that case the denominator is equal to the length of the document). \n\nFor instance, in a document about Boston, \"trail\" is more likely to occur after \"freedom\" than in other documents.",
            "score": 0.4064586197702927,
            "section_title": "Background: Language Models for IR",
            "char_start_offset": 11083,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 92,
                    "end": 301
                },
                {
                    "start": 304,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 664
                },
                {
                    "start": 667,
                    "end": 863
                },
                {
                    "start": 866,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1143
                },
                {
                    "start": 1146,
                    "end": 1288
                },
                {
                    "start": 1291,
                    "end": 1399
                },
                {
                    "start": 1402,
                    "end": 1456
                },
                {
                    "start": 1457,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1542
                },
                {
                    "start": 1543,
                    "end": 1712
                },
                {
                    "start": 1715,
                    "end": 1829
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98876953125
        },
        {
            "corpus_id": "202539551",
            "title": "Language Models as Knowledge Bases?",
            "text": "In this section we provide background on language models. Statistics for the models that we include in our investigation are summarized in Table 1.",
            "score": 0.4064586197702927,
            "section_title": "Background",
            "char_start_offset": 4420,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 57
                },
                {
                    "start": 58,
                    "end": 147
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.261962890625
        },
        {
            "corpus_id": "277451826",
            "title": "Bridging Language Models and Financial Analysis",
            "text": "Language models are probabilistic models that assign probabilities to text sequences based on patterns learned from training data (Hiemstra, 2009). After the early efforts in language modeling that focused on rule-based approaches (Weizenbaum, 1966;Bar-Hillel, 1960), statistical methods such as N-gram and Hidden Markov models became prominent, which relied on word frequency and sequence patterns (Brown et al., 1992;Fine et al., 1998;Markov, 2006;Eisenstein, 2019). After the introduction of neural networks, deep learning-based models have achieved remarkable performance in language modeling (Schuster & Paliwal, 1997;Graves & Graves, 2012), greatly enhancing the ability to understand and generate complex text sequences (Li, 2018;Li et al., 2022;O'Mahony et al., 2020;Bengio et al., 2000;Graves, 2013;Wang & Jiang, 2015;Sutskever, 2014;Sundermeyer et al., 2012;Bahdanau, 2014). Among these, models with Transformer (Vaswani, 2017) architecture have been the most widely researched and utilized in recent years, demonstrating exceptional performance in tasks related to both language understanding (Clark, 2020;Devlin, 2018;Joshi et al., 2020;He et al., 2020) and generation (Radford, 2018;Radford et al., 2019;Brown et al., 2020). \n\nBuilding on early work with the Transformer's encoder architecture (Devlin, 2018;Clark, 2020;Liu, 2019) which excels in language understanding, several approaches have adapted this structure and its training methods for applications in the finance domain (Araci, 2019;Liu et al., 2021;Yang et al., 2020;Huang et al., 2023;Shah et al., 2022;Banerjee et al., 2024b).",
            "score": 0.4064586197702927,
            "section_title": "LANGUAGE MODELS FOR DIVERSE FINANCIAL APPLICATIONS",
            "char_start_offset": 8442,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 1237
                },
                {
                    "start": 1240,
                    "end": 1604
                }
            ],
            "ref_mentions": [
                {
                    "start": 231,
                    "end": 249,
                    "matchedPaperCorpusId": "1896290"
                },
                {
                    "start": 249,
                    "end": 266,
                    "matchedPaperCorpusId": "18822284"
                },
                {
                    "start": 399,
                    "end": 419,
                    "matchedPaperCorpusId": "10986188"
                },
                {
                    "start": 419,
                    "end": 437,
                    "matchedPaperCorpusId": "3465810"
                },
                {
                    "start": 437,
                    "end": 450,
                    "matchedPaperCorpusId": "144854176"
                },
                {
                    "start": 597,
                    "end": 623,
                    "matchedPaperCorpusId": "18375389"
                },
                {
                    "start": 727,
                    "end": 737,
                    "matchedPaperCorpusId": "89689892"
                },
                {
                    "start": 737,
                    "end": 753,
                    "matchedPaperCorpusId": "220961531"
                },
                {
                    "start": 753,
                    "end": 775,
                    "matchedPaperCorpusId": "150135520"
                },
                {
                    "start": 775,
                    "end": 795,
                    "matchedPaperCorpusId": "221275765"
                },
                {
                    "start": 827,
                    "end": 843,
                    "matchedPaperCorpusId": "259108190"
                },
                {
                    "start": 843,
                    "end": 868,
                    "matchedPaperCorpusId": "18939716"
                },
                {
                    "start": 922,
                    "end": 937,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1130,
                    "end": 1149,
                    "matchedPaperCorpusId": "198229624"
                },
                {
                    "start": 1196,
                    "end": 1217,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1217,
                    "end": 1236,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51220703125
        },
        {
            "corpus_id": "9435670",
            "title": "Experiments with Generative Models for Dependency Tree Linearization",
            "text": "Here we describe some possible generative processes G which generate subtrees conditioned on a head. These models contain progressively more information about ordering relations among sister dependents.\n\nA common starting point for G is Eisner Model C (Eisner, 1996). In this model, dependents on one side of the head are generated by repeatedly sampling from a categorical distribution until a special stop-symbol is generated. The model only captures the propensity of dependents to appear on the left or right of the head, and does not capture any order constraints between sister dependents on one side of the head.\n\nWe consider a generalization of Eisner Model C which we call Dependent N-gram models. In a Dependent N-gram model, we generate dependents on each side the head by sampling a sequence of dependents from an N-gram model. Each dependent is generated conditional on the N \u2212 1 previously generated dependents from the head outwards. We have two separate Ngram sequence distributions for left and right dependents. Eisner Model C can be seen as a Dependent N-gram model with N = 1.\n\nWe also consider a model which can capture many more ordering relations among sister dependents: given a head h, sample a subtree whose head is h from a Categorical distribution over subtrees. We call this the Observed Orders model because in practice we are simply sampling one of the observed orders from the training data. This generative process has the capacity to capture the most ordering relations between sister dependents.",
            "score": 0.4064586197702927,
            "section_title": "Model Types",
            "char_start_offset": 3127,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 252,
                    "end": 266,
                    "matchedPaperCorpusId": "3262717"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.004055023193359375
        },
        {
            "corpus_id": "263829904",
            "title": "Augmented Embeddings for Custom Retrievals",
            "text": "Retrieval is being increasingly used to build applications that are based on pretrained large language models (LLMs). Embeddings from LLMs Neelakantan et al. (2022b); Touvron et al. ( 2023) have been used extensively to support the retrieval component in such applications. However, some of these new applications of retrieval pose different set of challenges from what were posed by other more traditional uses of retrieval. Motivated by the new applications, we introduce the class of heterogeneous strict retrieval problems where we are given three inputs: \n\n\u2022 A single query q 0 from a domain Q of queries; \n\n\u2022 A corpus C of candidate items; \n\n\u2022 A dataset D containing pairs (q, c), where q \u2208 Q and c \u2208 C. \n\nThe goal is to extract some k > 0 elements from C that are most likely to be related to the query q 0 in the same way that pairs in D are related. The focus of our work is on cases where k is required to be small. We use the term strict retrieval to emphasize the fact that we want the most likely candidate to appear in the top-1 or top-3 or top-5 of the retrieved candidates (as opposed to being in the top-10 or top-50). \n\nIn the applications we are targeting, the corpus C typically contains a few hundreds or a few thousands of elements. Similarly, the dataset D contains a few hundreds or thousands of (q, c) pairs. A key distinction with most earlier work on retrieval is that we are targeting heterogeneous retrievals where the domains Q and C are potentially very distinct. For example, the artifacts in Q may be natural language sentences, whereas those in C may be code snippets, or semi-structured or structured text or data. \n\nWe assume that we have black-box access to a pretrained large language model that can compute embeddings.",
            "score": 0.4064586197702927,
            "section_title": "HETEROGENEOUS STRICT RETRIEVAL",
            "char_start_offset": 7701,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 559
                },
                {
                    "start": 562,
                    "end": 610
                },
                {
                    "start": 613,
                    "end": 645
                },
                {
                    "start": 648,
                    "end": 709
                },
                {
                    "start": 712,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 1135
                },
                {
                    "start": 1138,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1649
                },
                {
                    "start": 1652,
                    "end": 1757
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91943359375
        },
        {
            "corpus_id": "258865437",
            "title": "Privacy Implications of Retrieval-Based Language Models",
            "text": "Retrieval-based language models (Khandelwal et al., 2020;Borgeaud et al., 2022;Izacard et al., 2022;Zhong et al., 2022;Min et al., 2023;Shi et al., 2023) have been widely studied in recent years. These models not only rely on encoder forward running but also leverage a non-parametric component to incorporate more knowledge from an external datastore during inference. The retrieval process starts by using the input as a query, and then retrieving a set of documents (i.e., sequences of tokens) from a corpus. The language model finally incorporates these retrieved documents as additional information to make its final prediction. While the deployment of retrieval-based language models has been shown to lead to improved performance on various NLP tasks, including language modeling and open-domain question answering, it also poses concerns about data privacy.",
            "score": 0.4064586197702927,
            "section_title": "Retrieval-based Language Models",
            "char_start_offset": 29126,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 865
                }
            ],
            "ref_mentions": [
                {
                    "start": 32,
                    "end": 57,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 57,
                    "end": 79,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 100,
                    "end": 119,
                    "matchedPaperCorpusId": "249062699"
                },
                {
                    "start": 119,
                    "end": 136,
                    "matchedPaperCorpusId": "254220735"
                },
                {
                    "start": 136,
                    "end": 153,
                    "matchedPaperCorpusId": "256389797"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.943359375
        },
        {
            "corpus_id": "256389797",
            "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
            "text": "Large language models (LLMs) such as GPT-3 (Brown et al., 2020a) and Codex (Chen et al., 2021a), have demonstrated impressive performance on a wide range of language tasks. These models are typically trained on very large datasets and store a substantial amount of world or domain knowledge implicitly in their parameters. However, they are also prone to hallucination and cannot represent the full long tail of knowledge from the training corpus. Retrieval-augmented language models (Khandelwal et al., 2020;Borgeaud et al., 2022;Izacard et al., 2022b;Yasunaga et al., 2022), in contrast, can retrieve knowledge from an external datastore when needed, potentially reducing hallucination and increasing coverage. Previous approaches of retrieval-augmented language models require access to the internal LM representations (e.g., to train the model (Borgeaud et al., 2022; Figure 1. Different from previous retrieval-augmented approaches (Borgeaud et al., 2022) that enhance a language model with retrieval by updating the LM's parameters, REPLUG treats the language model as a black box and augments it with a frozen or tunable retriever. This black-box assumption makes REPLUG applicable to large LMs (i.e., >100B parameters), which are often served via APIs. Izacard et al., 2022b) or to index the datastore (Khandelwal et al., 2020)), and are thus difficult to be applied to very large LMs. In addition, many best-in-class LLMs can only be accessed through APIs. Internal representations of such models are not exposed and fine-tuning is not supported. \n\nIn this work, we introduce REPLUG (Retrieve and Plug), a new retrieval-augmented LM framework where the language model is viewed as a black box and the retrieval component is added as a tuneable plug-and-play module. Given an input context, REPLUG first retrieves relevant documents from an external corpus using an off-the-shelf retrieval model.",
            "score": 0.4064586197702927,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 1138
                },
                {
                    "start": 1139,
                    "end": 1260
                },
                {
                    "start": 1261,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1555
                },
                {
                    "start": 1558,
                    "end": 1774
                },
                {
                    "start": 1775,
                    "end": 1904
                }
            ],
            "ref_mentions": [
                {
                    "start": 43,
                    "end": 64,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 484,
                    "end": 509,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 509,
                    "end": 531,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 848,
                    "end": 871,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 937,
                    "end": 960,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 1310,
                    "end": 1335,
                    "matchedPaperCorpusId": "207870430"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90869140625
        },
        {
            "corpus_id": "29740950",
            "title": "Book Review: Statistical Language Models for Information Retrieval by ChengXiang Zhai",
            "text": "r classical information retrieval models, from similarity-based models to probabilistic relevance and inference models, and r two retrieval frameworks, the axiomatic and decision-theoretic retrieval frameworks, both co-developed by the author.\n\nThis overview covers the main aspects of these models and frameworks, and allows the author to introduce notions that help position the statistical language model to be presented in the following chapters.\n\nThe remainder of the book is then devoted to the presentation of the statistical language models used in information retrieval, and their application to special tasks. Chapter 3 presents the standard, simple query likelihood retrieval model. After reviewing the basic idea behind this model (at the core of current statistical language models for information retrieval), the author presents the different event models that have been considered: Multinomial, Multiple Bernoulli, and Multiple Poisson. He then explains the strategy for parameter estimation and the different smoothing techniques one can rely on. This presentation is followed by a discussion of the relation between smoothing and tf-idf weighting, which paves the way for the two-stage smoothing method presented in the following section. This chapter is very well written and presents, in a clear yet complete way, the fundamentals of the query likelihood retrieval model.\n\nThe following chapter, entitled Complex Query Likelihood Retrieval Model, is devoted to extensions of the simple query likelihood model of Chapter 3. In particular, the author reviews document-specific smoothing methods, based on document clustering and document expansion, the use of n-gram models and Markov random fields, as well as the full Bayesian query likelihood and the translation models. The intent here is not to provide a detailed description of these elements, but rather to give an overview, and pointers to extensions and models related to the query likelihood retrieval model. A major conceptual drawback of the query likelihood retrieval model lies in the fact that feedback cannot be naturally accommodated. Indeed, in this model, a query is seen as a sample from the document model; adding terms according to a completely different process renders the \"sample view\" inadequate. In order to better accommodate feedback, the Kullback-Leibler divergence retrieval model has been introduced. In this model, a query model (associated with a word probability distribution) and a document model (also associated with a word probability distribution) are compared with the Kull",
            "score": 0.4064586197702927,
            "section_title": "body",
            "char_start_offset": 1,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9921875
        },
        {
            "corpus_id": "256615323",
            "title": "LaMPP: Language Models as Probabilistic Priors for Perception and Action",
            "text": "A language model (LM) is a distribution over natural language strings. LMs trained on sufficiently large text datasets become good models not just of grammatical phenomena, but various kinds of world knowledge (Talmor et al., 2021;Li et al., 2021). Our work proposes a method for extracting probabilistic common-sense priors from language models, which can then be used to supplement and inform arbitrary task-specific models operating over multiple modalities. These priors can be leveraged at multiple stages in the machine learning pipeline: \n\nPrediction: In many learning problems, our ultimate goal is to model a distribution p(y | x) over labels or decisions y given (non-linguistic) observations x. These ys might be structured objects: in Fig. 1(b), x is an image and y is a set of labels for objects in the image. By Bayes' rule, we can write: \n\nwhich factors this decision-making problem into two parts: a prior over labels p(y), and a generative model of observations p(x | y). If we have such a generative model, we may immediately combine it with a representation of the prior p(y) to model the distribution over labels. \n\nLearning: In models with interpretable parameters, we may also leverage knowledge about the distribution of these parameters themselves during learning, before we make any predictions at all. Given a dataset D of examples (x i , y i ) and a predictive model p(y | x; \u03b8), we may write \n\nin this case making it possible to leverage prior knowledge of \u03b8 itself, e.g., when optimizing model parameters or performing full Bayesian inference. \n\nIn structured output spaces, like segmented images, robot trajectories, or high-dimensional parameter vectors, a useful prior contains information about which joint configurations are plausible (e.g., an image might contain sofas and chairs, or showers and sinks, but not sinks and sofas). How can we use an LM to obtain and use distributions p(y) or p(\u03b8)? \n\nApplying LAMPP in a given problem domain involves four steps:",
            "score": 0.4064586197702927,
            "section_title": "Method",
            "char_start_offset": 4141,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 70
                },
                {
                    "start": 71,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 544
                },
                {
                    "start": 547,
                    "end": 705
                },
                {
                    "start": 706,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 852
                },
                {
                    "start": 855,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1133
                },
                {
                    "start": 1136,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1419
                },
                {
                    "start": 1422,
                    "end": 1572
                },
                {
                    "start": 1575,
                    "end": 1864
                },
                {
                    "start": 1865,
                    "end": 1931
                },
                {
                    "start": 1934,
                    "end": 1995
                }
            ],
            "ref_mentions": [
                {
                    "start": 210,
                    "end": 231,
                    "matchedPaperCorpusId": "237263476"
                },
                {
                    "start": 231,
                    "end": 247,
                    "matchedPaperCorpusId": "235294296"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.442138671875
        },
        {
            "corpus_id": "236924453",
            "title": "Structured Multi-modal Feature Embedding and Alignment for Image-Sentence Retrieval",
            "text": "To better understand the effectiveness of our proposed model, we visualize matching results of the sentence retrieval and image retrieval on Flickr30K in Figure 5 and Figure 6. For image retrieval shown in Figure 5, we show the top 3 ranked images for each text Query Sentence: A baseball player attempting to score a run before the catcher tags the player out . query matched by our proposed SMFEA in first column, and followed by CVSE [30] in the second column. The true matches are outlined in green boxes and false matches in red. Furthermore, as shown in Figure 6, we visualize the sentence retrieval results (top-3 retrieved sentences) predicted by SMFEA and CVSE [30], where the mismatches are highlight in red. Examples of failed image retrieval and sentence retrieval are shown in Figure 7. However, in this case the wrong images/sentences have similar semantic or structural content to true matches. We argue that the reason for this phenomenon may be that our current tree structure model is to unify the coarse-grained semantics and structural consistency between the two modalities. It has a good ability to improve the robustness of the model. But there are certain shortcomings in the distinction of similar sets. We will build fine-grained vocabularies to improve future work.",
            "score": 0.4062176641209117,
            "section_title": "Visualization of results",
            "char_start_offset": 28314,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.235107421875
        },
        {
            "corpus_id": "274965713",
            "title": "MRAG: A Modular Retrieval Framework for Time-Sensitive Question Answering",
            "text": "The goal of RAG is to address the limitations in the parametric knowledge of LLMs by incorporating external knowledge. \n\nPassage retrieval and reranking. Retrieval methods are typically categorized into sparse retrieval and dense retrieval. Sparse retrieval methods like BM25 (MacAvaney et al., 2020) rely on lexical matching. In contrast, dense retrieval models (Karpukhin et al., 2020;Zhao et al., 2021;Thakur et al., 2021;Izacard et al., 2021) encode the question q and passage p into low-dimension vectors separately. The semantic similarity is computed using a scoring function (e.g., dot product) as: \n\nHowever, these bi-encoder models lack the ability to capture fine-grained interactions between the query and passage. A common optional3 approach is to have another cross-encoder model to rerank top passages. Cross-encoder models (Khattab and Zaharia, 2020;Wang et al., 2020;Gemma et al., 2024) jointly encode the query q and the passage p together by concatenating them as input into a single model as: \n\n(2) \n\nAnswer generation. Recent reader systems are mainly powered by LLMs with strong reasoning capabilities. With recent advancements in longcontext LLMs (Dubey et al., 2024;Lee et al., 2024b), top documents are typically concatenated with the query as reader input: \n\nTo unlock the reasoning capabilities of LLMs, Chain-of-Thought (CoT) prompting (Wei et al., 2022) introduces intermediate reasoning steps for improved performance. Self-RAG (Asai et al., 2024) critiques the retrieved passages and its own generations. Recent agentic RAG systems dynamically acquire and integrate external knowledge (using either retriever models or search APIs) during the reasoning process (Li et al., 2025;Song et al., 2025;Jin et al., 2025).",
            "score": 0.40468495130815985,
            "section_title": "Retrieval-Augmented Generation",
            "char_start_offset": 5014,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 121,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 606
                },
                {
                    "start": 609,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 1012
                },
                {
                    "start": 1015,
                    "end": 1018
                },
                {
                    "start": 1021,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1124
                },
                {
                    "start": 1125,
                    "end": 1282
                },
                {
                    "start": 1285,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1535
                },
                {
                    "start": 1536,
                    "end": 1745
                }
            ],
            "ref_mentions": [
                {
                    "start": 276,
                    "end": 300,
                    "matchedPaperCorpusId": "216641996"
                },
                {
                    "start": 405,
                    "end": 425,
                    "matchedPaperCorpusId": "223957053"
                },
                {
                    "start": 1364,
                    "end": 1382,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 1458,
                    "end": 1477,
                    "matchedPaperCorpusId": "264288947"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.841796875
        },
        {
            "corpus_id": "252781830",
            "title": "Legal Element-oriented Modeling with Multi-view Contrastive Learning for Legal Case Retrieval",
            "text": "To verify the effectiveness of MVCL, we compare our model with four types of baselines: traditional bag-of-words retrieval models, common deep retrieval models, hierarchical models for long text matching, models built for legal case retrieval. \n\nTraditional bag-of-words retrieval models include: \n\n\u2022 TF-IDF: TF-IDF simply compares terms between query and candidates and also introduces Inverse Document Frequency, which punishes terms that are common in all documents. \u2022 BM25: BM25 is another exact matching-based and highly effective method. Common deep retrieval models include: \n\n\u2022 MatchPLM: MatchPLM [12] uses a PLM to encode both queries and candidates. A dense layer is used to compute similarities between representations of queries and candidates. \u2022 MatchPyramid: MatchPyramid [17] is an interactionfocused model, which utilizes a CNN layer to capture matching patterns on a word-level similarity matrix. \u2022 DRMM: DRMM [8] also builds a word-level similarity matrix and uses a histogram mapping function. All histograms are transformed by a feed forward network and selected by a gating network to produce the final representation. Hierarchical models for long text matching include: \n\n\u2022 SMASH RNN: SMASH RNN [11] adopted attentive RNN components in a hierarchical architecture to model multiple abstraction levels, including words, sentences, and paragraphs. \n\n\u2022 HAN: HAN [31] builds a hierarchical network based on GRU and uses attention-pooling to aggregate word and sentence representations. \u2022 SMITH: SMITH [12] is another hierarchical model that adopts transformers to compute representations in word and sentence levels. Like MatchBERT, we employ a dense layer as a similarity function for SMASH RNN, HAN, and SMITH. Models built for legal case retrieval include: \n\n\u2022 Roformer: Roformer [35] is a transformer-based model, which incorporates explicit relative position and is able to model long sequences. \u2022 BERT-PLI: BERT-PLI [6] breaks queries and candidates into paragraphs and computes the relevance between paragraphs. It aggregates paragraph relevance with a RNN layer. BERT-PLI is a strong baseline, which achieves the state-of-the-art on COLIEE 2018's dataset.",
            "score": 0.40464325626181297,
            "section_title": "B. Baselines",
            "char_start_offset": 22951,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 243
                },
                {
                    "start": 246,
                    "end": 296
                },
                {
                    "start": 299,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 581
                },
                {
                    "start": 584,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1012
                },
                {
                    "start": 1013,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1191
                },
                {
                    "start": 1194,
                    "end": 1367
                },
                {
                    "start": 1370,
                    "end": 1503
                },
                {
                    "start": 1504,
                    "end": 1634
                },
                {
                    "start": 1635,
                    "end": 1730
                },
                {
                    "start": 1731,
                    "end": 1777
                },
                {
                    "start": 1780,
                    "end": 1918
                },
                {
                    "start": 1919,
                    "end": 2036
                },
                {
                    "start": 2037,
                    "end": 2088
                },
                {
                    "start": 2089,
                    "end": 2181
                }
            ],
            "ref_mentions": [
                {
                    "start": 605,
                    "end": 609,
                    "matchedPaperCorpusId": "222327807"
                },
                {
                    "start": 786,
                    "end": 790,
                    "matchedPaperCorpusId": "3993933"
                },
                {
                    "start": 927,
                    "end": 930,
                    "matchedPaperCorpusId": "5688521"
                },
                {
                    "start": 1217,
                    "end": 1221,
                    "matchedPaperCorpusId": "86604509"
                },
                {
                    "start": 1381,
                    "end": 1385,
                    "matchedPaperCorpusId": "6857205"
                },
                {
                    "start": 1519,
                    "end": 1523,
                    "matchedPaperCorpusId": "222327807"
                },
                {
                    "start": 1940,
                    "end": 1943,
                    "matchedPaperCorpusId": "267909336"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94677734375
        },
        {
            "corpus_id": "271270159",
            "title": "Retrieval-Enhanced Machine Learning: Synthesis and Opportunities",
            "text": "To begin an in-depth exploration of retrieval-enhanced machine learning (REML), we commence with reiterating the generalized formal definition of the task set by Zamani et al. [257]. Like all predictive machine learning frameworks, subsequently referred to Fig. 1. Retrieval-enhanced machine learning models should implement three necessary requirements (querying, retrieval, and response utilization) and may implement two optional properties (storing information and providing feedback to the information access model). This results in four categories of REML models presented above. Figure is taken from [257]. \n\nas ML models, REML is tasked with learning a functional relationship that maps an input space X to an output space Y. The simplest form of REML model is depicted in Figure 1(a) by focusing solely on the essential criteria. The second category, illustrated in Figure 1(b), utilizes the first optional property by storing information in a storage for subsequent retrieval. The third category, presented in Figure 1(c), employs the second optional property, offering feedback to the information access models. The final category incorporates all optional properties, as detailed in Figure 1(d). \n\nBased on the requirements, Zamani et al. [257] proposed a comprehensive framework for REML, as illustrated in Figure 2. This framework is structured around two principal components: the prediction model   and the information retrieval models    s. For any given input , the predictive model   has the flexibility to initiate multiple retrieval operations. This could involve dispatching multiple queries, engaging with numerous data repositories, offering feedback to the information retrieval components, or employing a mix of these strategies. It's noteworthy that for certain inputs, the number of retrieval processes might be nil, thereby allowing REML to extend the conventional predictive modeling.",
            "score": 0.40462684785831926,
            "section_title": "Retrieval-Enhanced Machine Learning",
            "char_start_offset": 6398,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 613
                },
                {
                    "start": 616,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 986
                },
                {
                    "start": 987,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1207
                },
                {
                    "start": 1210,
                    "end": 1329
                },
                {
                    "start": 1330,
                    "end": 1457
                },
                {
                    "start": 1458,
                    "end": 1565
                },
                {
                    "start": 1566,
                    "end": 1755
                },
                {
                    "start": 1756,
                    "end": 1914
                }
            ],
            "ref_mentions": [
                {
                    "start": 176,
                    "end": 181,
                    "matchedPaperCorpusId": "248506020"
                },
                {
                    "start": 607,
                    "end": 612,
                    "matchedPaperCorpusId": "248506020"
                },
                {
                    "start": 1251,
                    "end": 1256,
                    "matchedPaperCorpusId": "248506020"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2330322265625
        },
        {
            "corpus_id": "250663813",
            "title": "Improving word embeddings in Portuguese: increasing accuracy while reducing the size of the corpus",
            "text": "Deep learning methods for language processing owe much of their success to neural network language models. Words are represented as dense real-valued vectors and such representation is referred to as word embeddings given that they embed a vocabulary into a relatively low-dimensional linear space. One of the earliest ideas of distributed representations is presented in Hinton (1986) and has been applied to statistical language modeling with considerable success. These word embeddings have shown to improve performance in a variety of natural language processing tasks including automatic speech recognition, information retrieval, document classification, etc. The training is performed over a large corpus, typically in a totally unsupervised manner, using the co-occurrence statistics of words. Word vectors are typically obtained as a product of training neural network language models to predict the probability distribution over the next word. The learned word embeddings explicitly capture many linguistic regularities and patterns, such as semantic and syntactic attributes of words. Therefore, words that appear in similar contexts, or belong to a common topic (e.g., country and city names, animals, etc.), tend to form a cluster. In Joulin et al. (2016), Mikolov et al. (2013) it has been demonstrated that word embeddings created by a recurrent neural network and a related model, the Word2vec, exhibit an additional linear structure that captures the relation between pairs of word. The use of simple vector arithmetic allows solving analogy queries such as ''man is to king as woman is to?'' In this example, ''queen'' happens to be the word whose vector V queen is the closest approximation to the vector V woman -V man + V king (Lee, 2015). Different algorithms and techniques have been developed in recent years to generate word embeddings from text data. All of them rely on the assumption that words that appear in similar context have similar meanings. The resulting models produce word vectors that can also be used to solve analogy queries. State-of-the-art developments have been focusing recently on other languages besides English.",
            "score": 0.40399085711547006,
            "section_title": "RELATED WORK",
            "char_start_offset": 9716,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 466
                },
                {
                    "start": 467,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 953
                },
                {
                    "start": 954,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1499
                },
                {
                    "start": 1500,
                    "end": 1609
                },
                {
                    "start": 1610,
                    "end": 1760
                },
                {
                    "start": 1761,
                    "end": 1876
                },
                {
                    "start": 1877,
                    "end": 1976
                },
                {
                    "start": 1977,
                    "end": 2066
                },
                {
                    "start": 2067,
                    "end": 2160
                }
            ],
            "ref_mentions": [
                {
                    "start": 372,
                    "end": 385,
                    "matchedPaperCorpusId": "265039794"
                },
                {
                    "start": 1270,
                    "end": 1291,
                    "matchedPaperCorpusId": "5959482"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.662109375
        },
        {
            "corpus_id": "263605962",
            "title": "RA-DIT: Retrieval-Augmented Dual Instruction Tuning",
            "text": "Retrieval-Augmented Language Models RALMs fuse language models (LMs) with a retrieval module that explicitly augments the LM with information retrieved from external knowledge stores (Guu et al., 2020;Lewis et al., 2020). One mainstream type of RALM follows the \"retrieveand-read\" paradigm, where the retrieval module supplies external knowledge as additional context which the LM (reader) leverages to produce the final output (Izacard et al., 2022b;Borgeaud et al., 2022;Shi et al., 2023b;Ram et al., 2023). Some existing work focuses on pre-training the LM to better utilize retrieved knowledge. For example, REALM (Guu et al., 2020) and RETRO (Borgeaud et al., 2022) incorporate retrieval from the beginning and conduct end-to-end retrieval-augmented pre-training, whereas ATLAS (Izacard et al., 2022b) continuously pre-trains a T5 LM (Raffel et al., 2020) jointly with a retriever. Others assume black-box access to an LM and combine it with either off-the-shelf or fine-tuned retrievers (Shi et al., 2023b;Ram et al., 2023). Our approach adopts lightweight fine-tuning to effectively retrofit any pre-trained LLM with retrieval capacity. This approach offers efficiency compared to methods involving extensive pre-training and demonstrates superior effectiveness compared to the off-the-shelf fusion approach. \n\nIndependent to our work, Luo et al. (2023) proposes SAIL, an approach that fine-tunes the LM with instructions augmented with retrieved content, and examines it on public instruction following datasets (Taori et al., 2023;Chiang et al., 2023) using a moderately sized model (7B parameters). \n\nIn comparison, RA-DIT conducts parallel retrieval-augmentation by generating distinct prompts for each retrieved passage and subsequently aggregating the outcomes; SAIL, on the other hand, concatenates the top retrieved passages in the augmentation.",
            "score": 0.40346213869427583,
            "section_title": "RELATED WORK",
            "char_start_offset": 22646,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1315
                },
                {
                    "start": 1318,
                    "end": 1608
                },
                {
                    "start": 1611,
                    "end": 1860
                }
            ],
            "ref_mentions": [
                {
                    "start": 183,
                    "end": 201,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 451,
                    "end": 473,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 618,
                    "end": 636,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 647,
                    "end": 670,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 839,
                    "end": 860,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97998046875
        },
        {
            "corpus_id": "7742063",
            "title": "Bridging Lexical Gaps between Queries and Questions on Large Online Q&A Collections with Compact Translation Models",
            "text": "This section introduces the translation-based language modeling approach to retrieval that has been used to bridge the lexical gap between queries and already-answered questions in this paper. \n\nIn the basic language modeling framework for retrieval (Ponte and Croft, 1998), the similarity between a query Q and a document D for ranking may be modeled as the probability of the document language model \n\nAssuming that query words occur independently given a particular document language model, the query-likelihood P (Q|M D ) is calculated as: \n\nwhere q represents a query word. \n\nTo avoid zero probabilities in document language models, a mixture between a document-specific multinomial distribution and a multinomial distribution estimated from the entire document collection is widely used in practice: \n\nwhere 0 < \u03bb < 1 and M C represents a language model built from the entire collection. The probabilities P (w|M D ) and P (w|M C ) are calculated using maximum likelihood estimation. \n\nThe basic language modeling framework does not address the issue of lexical gaps between queries and question. Berger and Lafferty (1999) viewed information retrieval as statistical document-query translation and introduced translation models to map query words to document words. Assuming that a translation model can be represented by a conditional probability distribution of translation T (\u2022|\u2022) between words, we can model P (q|M D ) in Equation 3 as: \n\nwhere w represents a document word. 3 he translation probability T (q|w) virtually represents the degree of relationship between query word q and document word w captured in a different, machine translation setting. Then, in the traditional information retrieval viewpoint, the use of translation models produce an implicit query expansion effect, since query words not in a document are mapped to related words in the document. This implies that translation-based retrieval models would make positive contributions to retrieval performance only when the pre-constructed translation models have reliable translation probability distributions.",
            "score": 0.4024982055363106,
            "section_title": "Translation-based Retrieval Model",
            "char_start_offset": 3746,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 195,
                    "end": 401
                },
                {
                    "start": 404,
                    "end": 543
                },
                {
                    "start": 546,
                    "end": 578
                },
                {
                    "start": 581,
                    "end": 805
                },
                {
                    "start": 808,
                    "end": 893
                },
                {
                    "start": 894,
                    "end": 989
                },
                {
                    "start": 992,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1447
                },
                {
                    "start": 1450,
                    "end": 1487
                },
                {
                    "start": 1488,
                    "end": 1665
                },
                {
                    "start": 1666,
                    "end": 1878
                },
                {
                    "start": 1879,
                    "end": 2092
                }
            ],
            "ref_mentions": [
                {
                    "start": 250,
                    "end": 273,
                    "matchedPaperCorpusId": "2759772"
                },
                {
                    "start": 1103,
                    "end": 1129,
                    "matchedPaperCorpusId": "10837883"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97265625
        },
        {
            "corpus_id": "270620489",
            "title": "Think-then-Act: A Dual-Angle Evaluated Retrieval-Augmented Generation",
            "text": "Large language models (LLMs) have become a cornerstone of natural language processing (NLP) systems due to their impressive capabilities in understanding and generating human language (Brown et al., 2020;Ouyang et al., 2022;OpenAI, 2023).Despite their success, LLMs often suffer from temporal misalignment (R\u00f6ttger and Pierrehumbert, 2021;Luu et al., 2022)or generating hallucinatory content (Ji et al., 2023;Shi et al., 2023;Bang et al., 2023).This impacts the dependability of LLMs and limits their broader practical use, as the alignment between LLM outputs and real-world information still requires further validation.Augmenting LLMs with retrieval mechanisms to fetch relevant information from external sources has emerged as a promising approach to mitigate these issues (Khandelwal et al., 2019;Izacard et al., 2023).\n\nRetrieval-augmented language models (LMs) typically operate using a retrieve-and-generate framework.This process begins by retrieving relevant documents based on the user's input.Subsequently, the model generates a comprehensive response that is conditioned on the information contained within these retrieved documents.This approach leverages the synergy between information retrieval and natural language generation, enhancing the model's ability to provide accurate and contextually relevant answers.(Chen et al., 2017;Guu et al., 2020;Lewis et al., 2021;Izacard and Grave, 2021;Sachan et al., 2021;Lee et al., 2022;Jiang et al., 2022;Izacard et al., 2023;Nakano et al., 2022;Qian et al., 2023;Lazaridou et al., 2022;Shi et al., 2023).\n\nStandard RAG methods often involve a single retrieval step, which can be insufficient for complex problems requiring multi-step reasoning.(Yoran et al., 2024).",
            "score": 0.4024982055363106,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 238
                },
                {
                    "start": 238,
                    "end": 445
                },
                {
                    "start": 445,
                    "end": 622
                },
                {
                    "start": 622,
                    "end": 824
                },
                {
                    "start": 826,
                    "end": 926
                },
                {
                    "start": 926,
                    "end": 1005
                },
                {
                    "start": 1005,
                    "end": 1146
                },
                {
                    "start": 1146,
                    "end": 1329
                },
                {
                    "start": 1329,
                    "end": 1564
                },
                {
                    "start": 1566,
                    "end": 1704
                },
                {
                    "start": 1704,
                    "end": 1725
                }
            ],
            "ref_mentions": [
                {
                    "start": 184,
                    "end": 204,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 204,
                    "end": 224,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 306,
                    "end": 339,
                    "matchedPaperCorpusId": "233289460"
                },
                {
                    "start": 339,
                    "end": 356,
                    "matchedPaperCorpusId": "244117116"
                },
                {
                    "start": 392,
                    "end": 409,
                    "matchedPaperCorpusId": "246652372"
                },
                {
                    "start": 426,
                    "end": 444,
                    "matchedPaperCorpusId": "256662612"
                },
                {
                    "start": 802,
                    "end": 823,
                    "matchedPaperCorpusId": "251371732"
                },
                {
                    "start": 1329,
                    "end": 1348,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 1384,
                    "end": 1408,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 1464,
                    "end": 1485,
                    "matchedPaperCorpusId": "251371732"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7138671875
        },
        {
            "corpus_id": "252846580",
            "title": "Decoupled Context Processing for Context Augmented Language Modeling",
            "text": "For auto-regressive language modeling, we use English C4 [32] version 2.2.1, the same as Retro. We train the language model and prepare the retrieval database using the train split and evaluate the results using validation split. The language model target sequence is a sliding window (chunk)  of s = 64 tokens, with at most n = 448 preceding tokens are used as input sequence. This setup is similar to XLNet [43] and Retro [4]. The target and input sequences that are smaller than the given window size (64 and 448, respectively) are padded with zeros. \n\nTo construct the retrieval database, the same sliding window processing is also used for the context sequences. The database is formed as associated pairs of retrieval embedding and encoder output: \n\n, where context c are the sliding window of 512 tokens with a stride of 64 tokens. We choose our hyper-parameters to be comparable to Retro: chunk size s = 64 and input window size n = 448 (smaller than 2048 that of Retro). This also implies that the number of entries in the database is larger than the number of articles, but smaller than the number of tokens. \n\nOur training corpus is in the form of triplets (x, y, c). x and y are acquired directly by applying sliding window on the train split of C4. Then BM25 [34] is used as a bootstrapping retriever to mine relevant context c to from the database. The first retrieval results with no more than 8 consecutive token overlap with the target is used as context. Figure 2a illustrate the sliding window construction of database as well as sequence served as input and targets. Table 2b gives the exact number of entries in the resulting database used as external context. \n\nWe use mT5 [42] as the backbone architecture for our context augmented Encoder-Decoder, and train our models from scratch. Train split is used both for training and retrieval, while validation split is used for bits-per-byte (Bpb) evaluation. In auto-regressive language model evaluation, due to the fact that text are crawled from web sources, there can be a non-trivial overlap of tokens between the training and validation splits.",
            "score": 0.4024982055363106,
            "section_title": "Experimental Setup",
            "char_start_offset": 12316,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 553
                },
                {
                    "start": 556,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 753
                },
                {
                    "start": 756,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1118
                },
                {
                    "start": 1121,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1472
                },
                {
                    "start": 1473,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1681
                },
                {
                    "start": 1684,
                    "end": 1806
                },
                {
                    "start": 1807,
                    "end": 1926
                },
                {
                    "start": 1927,
                    "end": 2117
                }
            ],
            "ref_mentions": [
                {
                    "start": 57,
                    "end": 61,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 409,
                    "end": 413,
                    "matchedPaperCorpusId": "195069387"
                },
                {
                    "start": 1695,
                    "end": 1699,
                    "matchedPaperCorpusId": "225040574"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82421875
        },
        {
            "corpus_id": "14861523",
            "title": "Discriminative Word Alignment with Conditional Random Fields",
            "text": "CRFs are undirected graphical models which define a conditional distribution over a label sequence given an observation sequence. We use a CRF to model many-to-one word alignments, where each source word is aligned with zero or one target words, and therefore each target word can be aligned with many source words. Each source word is labelled with the index of its aligned target, or the special value null, denoting no alignment. An example word alignment is shown in Figure 1, where the hollow squares and circles indicate the correct alignments. In this example the French words une and autre would both be assigned the index 24 -for the English word another -when French is the source language. When the source language is English, another could be assigned either index 25 or 26; in these ambiguous situations we take the first index. \n\nThe joint probability density of the alignment, a (a vector of target indices), conditioned on the source and target sentences, e and f , is given by: \n\n) where we make a first order Markov assumption over the alignment sequence. Here t ranges over the indices of the source sentence (f ), k ranges over the model's features, and \u039b = {\u03bb k } are the model parameters (weights for their corresponding features). The feature functions h k are predefined real-valued functions over the source and target sentences coupled with the alignment labels over adjacent times (source sentence locations), t. These feature functions are unconstrained, and may represent overlapping and non-independent features of the data. The distribution is globally normalised by the partition function, Z \u039b (e, f ), which sums out the numerator in (1) for every possible alignment: \n\nWe use a linear chain CRF, which is encoded in the feature functions of (1). The parameters of the CRF are usually estimated from a fully observed training sample (word aligned), by maximising the likelihood of these data. I.e. \u039b M L = arg max \u039b p \u039b (D), where D = {(a, e, f )} are the training data.",
            "score": 0.4024982055363106,
            "section_title": "Conditional random fields",
            "char_start_offset": 3363,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 841
                },
                {
                    "start": 844,
                    "end": 994
                },
                {
                    "start": 997,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1554
                },
                {
                    "start": 1555,
                    "end": 1700
                },
                {
                    "start": 1703,
                    "end": 1779
                },
                {
                    "start": 1780,
                    "end": 1925
                },
                {
                    "start": 1926,
                    "end": 1930
                },
                {
                    "start": 1931,
                    "end": 2003
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.444580078125
        },
        {
            "corpus_id": "258426878",
            "title": "Synthetic Cross-language Information Retrieval Training Data",
            "text": "Language models are now tightly integrated with information retrieval systems. These combined systems are used for a broad range of knowledge-intensive problems, including open-domain question answering [20,21], conversational assistants [45,46], fact-checking [49,50], and even improving language modeling itself [6,31]. \n\nAt times these systems are simply combinations of separate processes [15,20,40], while other times they are trained end-toend from retrieval to the downstream task [21,23,30]. Due to the size of LLMs, they are typically used as separate components, with retrieval results passed to the LLM [15,25,47]. A nascent line of work has even proposed ignoring retrieval entirely and using LLMs to generate a relevant document in lieu of search [57]. In contrast to much of the research cited in this section, our work aims to use LLMs to improve IR models, rather than using retrieval to improve LLMs on NLP tasks.",
            "score": 0.4024982055363106,
            "section_title": "LLMs and Retrieval",
            "char_start_offset": 7462,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 79,
                    "end": 321
                },
                {
                    "start": 324,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 930
                }
            ],
            "ref_mentions": [
                {
                    "start": 314,
                    "end": 317,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 317,
                    "end": 320,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8125
        },
        {
            "corpus_id": "273549254",
            "title": "SmartRAG: Jointly Learn RAG-Related Tasks From the Environment Feedback",
            "text": "2.1 RETRIEVAL-AUGMENTED GENERATION Retrieval augmentation techniques have been utilized to acquire external knowledge, which aids language models in achieving superior performance across a broad spectrum of tasks (Guu et al., 2020;Gao et al., 2023;Hu & Lu, 2024). Previous research has primarily concentrated on \"what to retrieve\" (Khandelwal et al., 2019;Ram et al., 2023) and \"how to utilize the retrieved information\" (Khattab et al., 2022). \n\nRecently, some studies have begun to explore when to use retrieval to meet the varying requirements of diverse tasks (Jiang et al., 2023). For instance, Mallen et al. (2023b) assesses the popularity of a query based on the frequency of its entities and recommends activating retrieval modules only when the entity frequency drops below a predetermined threshold. Wang et al. (2023) enhances the model's retrieval performance by using self-knowledge to decide what they know and do not know. Self-Rag (Asai et al., 2024) leverages special tokens to adaptively retrieve external knowledge and confirm the output's relevance or efficacy. Additionally, some researchers incorporated supplementary modules or classifiers to determine whether a query necessitates additional knowledge for resolution (Liu et al., 2024;Jeong et al., 2024). Wang et al. (2024) create a compositional unknown dataset (CuQA), and utilize the confidence words or scores to decide whether to retrieve. \n\nUnlike previous methods, we have implemented an end-to-end approach to jointly optimize all the related tasks such that the whole system can achieve better performance.",
            "score": 0.4024982055363106,
            "section_title": "RELATED WORK",
            "char_start_offset": 4445,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 444
                },
                {
                    "start": 447,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1419
                },
                {
                    "start": 1422,
                    "end": 1590
                }
            ],
            "ref_mentions": [
                {
                    "start": 213,
                    "end": 231,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 331,
                    "end": 356,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 356,
                    "end": 373,
                    "matchedPaperCorpusId": "256459451"
                },
                {
                    "start": 600,
                    "end": 621,
                    "matchedPaperCorpusId": "254877603"
                },
                {
                    "start": 810,
                    "end": 828,
                    "matchedPaperCorpusId": "263828724"
                },
                {
                    "start": 947,
                    "end": 966,
                    "matchedPaperCorpusId": "264288947"
                },
                {
                    "start": 1241,
                    "end": 1259,
                    "matchedPaperCorpusId": "268363612"
                },
                {
                    "start": 1259,
                    "end": 1278,
                    "matchedPaperCorpusId": "268553748"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7314453125
        },
        {
            "corpus_id": "268667440",
            "title": "FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions",
            "text": "Most retrieval models using instructions received roughly one instruction per retrieval dataset, which generally defined the domain (e.g., \"Financial\"), document size (sentence, passage, etc.), and task format.This includes INSTRUCTOR models (Su et al., 2022), the bi-encoder TART model trained from Contriever (Asai et al., 2022), the reranker TART trained from FLAN-T5 (Chung et al., 2022), E5 Mistral-Instruct (Wang et al., 2023a), and GritLM (Muennighoff et al., 2024).We also include BGE models (Xiao et al., 2023) in this category, although they are trained with only one instruction total for each broad task (retrieval, clustering, etc.).",
            "score": 0.4024982055363106,
            "section_title": "Instructions in IR Training",
            "char_start_offset": 16836,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 210
                },
                {
                    "start": 210,
                    "end": 473
                },
                {
                    "start": 473,
                    "end": 646
                }
            ],
            "ref_mentions": [
                {
                    "start": 446,
                    "end": 472,
                    "matchedPaperCorpusId": "260460088"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8154296875
        },
        {
            "corpus_id": "10844118",
            "title": "Cache-Augmented Latent Topic Language Models for Speech Retrieval",
            "text": "Our primary goal in constructing this model is to apply it to language models for speech recognition and retrieval. Given an LVCSR system with a standard N-gram language model (LM), we now describe how we incorporate the inferred topic and cache model parameters of a new document into the base LM for subsequent recognition tasks on that specific document.\n\nWe begin by estimating model parameters on a training corpus: topics \u03c6 (t) , cache proportions \u03ba (d) , and hyperparameters, \u03b1, \u03b2, and \u03bd (the Beta hyperparameter). In our experiments we restrict the training set to the LVCSR acoustic and language model training. This restriction is required by the Babel task, not the model. Using other corpora or text resources certainly should be considered for other tasks.\n\nTo apply the model during KWS, we first decode a new audio document d with the base LM, P L and extract the most likely observed word sequence W for inference. The inference process gives us the es- timates for \u03b8 (d) and \u03ba (d) , which we then use to compute document-specific and cache-augmented language models.\n\nFrom a language modeling perspective we treat the multinomials \u03c6 (t) as unigram LM's and use the inferred topic proportions \u03b8 (d) as a set of mixture weights. From these we compute the documentspecific unigram model for d (Eqn. 1). This serves to capture what we have referred to as the broad topic context.\n\nWe incorporate both P d as well as the cache P c (local context) into the base model P L using linear interpolation of probabilities. Word histories are denoted h i for brevity. For our experiments we first combine P d with the N-gram model (Eqn. 2). We then interpolate with the cache model to get a joint topic and cache language model (Eqn. 4).\n\nWe expect the inferred document cache probability \u03ba (d) to serve as a natural interpolation weight when combining document-specific unigram model P dc and cache. We consider alternatives to perdocument \u03ba (d) as part of the speech retrieval evaluation (Section 6) and can show that our model'",
            "score": 0.4024982055363106,
            "section_title": "Language Modeling",
            "char_start_offset": 7136,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86572265625
        },
        {
            "corpus_id": "7198531",
            "title": "Weak semantic context helps phonetic learning in a model of infant language acquisition",
            "text": "We use Gibbs sampling to infer three sets of variables in the TLD model: assignments to vowel categories in the lexemes, assignments of tokens to Figure 2: TLD model, depicting, from left to right, the IGMM component, the LD lexicon component, the topic-specific lexicons, and finally the token x hi , appearing in document h, with observed vowel formants w hij and frame f hi . The lexeme assignment x hi and the topic assignment z hi are inferred, the latter using the observed documenttopic distribution \u03b8 h . Note that f i is deterministic given the lexeme assignment. Squared nodes depict hyperparameters. \u03bb is the set of hyperparameters used by H L when generating lexical items (see Section 3.2).\n\ntopics, and assignments of tokens to tables (from which the assignment to lexemes can be read off).",
            "score": 0.4024982055363106,
            "section_title": "Inference: Gibbs Sampling",
            "char_start_offset": 17066,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04931640625
        },
        {
            "corpus_id": "29740950",
            "title": "Book Review: Statistical Language Models for Information Retrieval by ChengXiang Zhai",
            "text": "Statistical language models have recently been successfully applied to many information retrieval problems. A great deal of recent work has shown that statistical language models not only achieve superior empirical performance, but also facilitate parameter tuning and provide a more principled way, in general, for modeling various kinds of complex and non-traditional retrieval problems.",
            "score": 0.4024982055363106,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95703125
        },
        {
            "corpus_id": "268248911",
            "title": "Reliable, Adaptable, and Attributable Language Models with Retrieval",
            "text": "Parametric language models (LMs), which are trained on vast amounts of web data, exhibit remarkable flexibility and capability. However, they still face practical challenges such as hallucinations, difficulty in adapting to new data distributions, and a lack of verifiability. In this position paper, we advocate for retrieval-augmented LMs to replace parametric LMs as the next generation of LMs. By incorporating large-scale datastores during inference, retrieval-augmented LMs can be more reliable, adaptable, and attributable. Despite their potential, retrieval-augmented LMs have yet to be widely adopted due to several obstacles: specifically, current retrieval-augmented LMs struggle to leverage helpful text beyond knowledge-intensive tasks such as question answering, have limited interaction between retrieval and LM components, and lack the infrastructure for scaling. To address these, we propose a roadmap for developing general-purpose retrieval-augmented LMs. This involves a reconsideration of datastores and retrievers, the exploration of pipelines with improved retriever-LM interaction, and significant investment in infrastructure for efficient training and inference.",
            "score": 0.4024982055363106,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95703125
        },
        {
            "corpus_id": "9519540",
            "title": "A P\u00f3lya Urn Document Language Model for Improved Information Retrieval",
            "text": "We have introduced a new family of language model (namely SPUD) based on a P\u00f3lya urn process. We have shown that a query likelihood retrieval method based on this model is superior to that of the state-of-the-art multinomial language model. Interestingly, we have shown that the new model can be computed as efficiently as the multinomial language model. Essentially, this means that the SPUD retrieval method can be used in place of the multinomial query likelihood method in many different retrieval applications and domains. \n\nWe have outlined a number of intuitions that help to motivate the new model. For example, we developed a constraint for the verbosity hypothesis and have shown that the most effective SPUD method, the SPUD dir model, adheres to this constraint. Furthermore, we have shown that the free hyperparameter (i.e. \u03c9 = 0.8) in the SPUD dir method is robust across various collections. This essentially reduces the need for experimental tuning. Given the principled nature of the approach developed, it can be used in a variety of IR tasks. We have shown that it is useful for downstream retrieval methods, as we have used it to estimate a pseudo-relevance based model (PURM) that demonstrates improved retrieval effectiveness on test collections when compared to a pseudo-relevance model based on the multinomial (RM3). \n\nFuture work will look to improve retrieval effectiveness by incorporating multiple DCM language models for modelling a document. Furthermore, we aim to investigate the query likelihood method using different generative assumptions for the query. In this work, we assumed a sampling-with-replacement strategy for query generation. However, different sampling strategies, such as those employed by Friedman urn's [Freedman 1965] might better model query generation.",
            "score": 0.4024982055363106,
            "section_title": "CONCLUSION",
            "char_start_offset": 79653,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 527
                },
                {
                    "start": 530,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1341
                },
                {
                    "start": 1344,
                    "end": 1472
                },
                {
                    "start": 1473,
                    "end": 1589
                },
                {
                    "start": 1590,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1807
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96630859375
        },
        {
            "corpus_id": "238253207",
            "title": "Building an Efficient and Effective Retrieval-based Dialogue System via Mutual Learning",
            "text": "Inspired by the recent dense retrieval (Lee, Chang, and Toutanova 2019;Zhan et al. 2020;Karpukhin et al. 2020), we use a bi-encoder architecture to construct a learnable retriever. \n\nThe architecture utilizes a separated pre-trained encoder to cast the input context message and index entries into dense representations in a vector space and relies on fast maximum inner-product search (MIPS) to complete the retrieval. Without loss of generality, we use two BERT (Devlin et al. 2019) models for both encoders, as it is trained on large amounts of unlabelled data and provides strong \"universal representations\" that can be finetuned on task-specific training data to achieve good performance on downstream tasks. Specifically, given a context c = {u 1 , u 2 , . . . , u nc }, where the t-th utterance u t = {w t,1 , . . . , w t,lt } is a sequence with l t words, a response candidate r = {r 1 , r 2 , . . . , r lr } consisting of l r words and a label y \u2208 {0, 1}, we first concatenate all utterances in the context as a consecutive token sequence with special tokens separating them, which can be formulated as is the \"End Of Turn\" tag designed for multi-turn context. For each word of x, token, position and segment embeddings of x are summated and fed into pre-trained transformer layer (a.k.a. BERT), giving us the contextualized embedding sequence. We then project the [CLS] representation to a vector as the context representation following Lee, Chang, and Toutanova (2019). Formally, \n\nwhere BERT c is the context encoder, W c is the projection matrix for the context [CLS] representation, and v c is the final context representation containing dialogue history information. We then follow the same scheme to obtain the response representation for a response candidate r j : \n\nwhere BERT r is the response encoder, W r is the projection matrix for the response [CLS] representation, and v r is the final response representation. Finally, the retrieval score is computed as",
            "score": 0.40157715880644446,
            "section_title": "Pre-Retriever",
            "char_start_offset": 11089,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 183,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1480
                },
                {
                    "start": 1481,
                    "end": 1490
                },
                {
                    "start": 1493,
                    "end": 1681
                },
                {
                    "start": 1682,
                    "end": 1781
                },
                {
                    "start": 1784,
                    "end": 1935
                },
                {
                    "start": 1936,
                    "end": 1979
                }
            ],
            "ref_mentions": [
                {
                    "start": 71,
                    "end": 88,
                    "matchedPaperCorpusId": "195886246"
                },
                {
                    "start": 88,
                    "end": 110,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 464,
                    "end": 483,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1473,
                    "end": 1479,
                    "matchedPaperCorpusId": "202776649"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65087890625
        },
        {
            "corpus_id": "271891894",
            "title": "EasyRec: Simple yet Effective Language Models for Recommendation",
            "text": "Baseline Methods and Settings. For our comparative evaluation, we included a diverse set of language models as text embedders: (i) General Language Models: BERT [4], RoBERTa [21], and BART [14]; (ii) Language Models for Dense Retrieval: SimCSE [7], GTR [23], and BGE [39]; (iii) Pre-trained Language Models for Recommendation: BLaIR [12]. Additionally, we also compared against the state-of-the-art text embedding models provided by OpenAI. \n\nThe detailed descriptions of these baseline models can be found in Appendix A.3. This selection of language models covers a broad spectrum, from general-purpose contextual encoders to specialized models tailored for tasks like dense retrieval and recommendation.",
            "score": 0.4013052374792626,
            "section_title": "4.2.1",
            "char_start_offset": 24150,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 30
                },
                {
                    "start": 31,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 440
                },
                {
                    "start": 443,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 705
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94580078125
        },
        {
            "corpus_id": "267301281",
            "title": "Unearthing Large Scale Domain-Specific Knowledge from Public Corpora",
            "text": "With the development of large language models, public corpora have become increasingly rich, including the Pile, RedPajama, and Common Crawl. To ensure the diversity of data sources for retrieval, we constructed our retrieval corpus based on Common Crawl. Common Crawl is an open-source web crawler project containing all publicly available web pages from 2013 to the present. In theory, it encompasses a significant portion of the information present on the web. Due to hardware cost constraints, we utilized WARC format data from the years 2016 to 2023 to build our retrieval database. We performed extraction, filtering, and cleaning procedures on the data obtained from Common Crawl to ensure the quality of the retrieval dataset. The final retrieval database comprises several billion records, occupying a total of 50TB of disk space.",
            "score": 0.40108905615920426,
            "section_title": "C.1 Public Corpora",
            "char_start_offset": 27099,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 839
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06292724609375
        },
        {
            "corpus_id": "273228028",
            "title": "Enhancing Legal Case Retrieval via Scaling High-quality Synthetic Query-Candidate Pairs",
            "text": "We also conducted experiments on the original LeCaRD dataset under the traditional case-to-case symmetric retrieval setting, and the results are shown in Table 8. Here, we present the results of all baseline models and the models trained on LEAD with different proportions of augmented positive examples. \n\nFrom the results, we can observe that similar to the results on the CAIL2022-LCR dataset, our dataset, LEAD can significantly benefit the performance of traditional case-to-case symmetric retrieval.",
            "score": 0.3999582104084233,
            "section_title": "A.3 Addition Experiment Result",
            "char_start_offset": 26691,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 304
                },
                {
                    "start": 307,
                    "end": 505
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.076416015625
        },
        {
            "corpus_id": "267770477",
            "title": "MORE: Multi-mOdal REtrieval Augmented Generative Commonsense Reasoning",
            "text": "Since commonsense information has been recorded significantly less frequently than its existence, language models pre-trained by text generation have difficulty to learn sufficient commonsense knowledge. Several studies have leveraged text retrieval to augment the models' commonsense ability. Unlike text, images capture commonsense information inherently but little effort has been paid to effectively utilize them. In this work, we propose a novel Multi-mOdal REtrieval (MORE) augmentation framework, to leverage both text and images to enhance the commonsense ability of language models. Extensive experiments on the Common-Gen task have demonstrated the efficacy of MORE based on the pre-trained models of both single and multiple modalities.",
            "score": 0.39954869809477944,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.386962890625
        },
        {
            "corpus_id": "265034153",
            "title": "Divide & Conquer for Entailment-aware Multi-hop Evidence Retrieval",
            "text": "To evaluate and compare the performance of the base models and various ensemble methods, we adopt the standard performance evaluation metrics for retrieval tasks, including precision at different cut-off points (P @k), mean average precision (M AP ), and recall at different cut-off points (R@k).\n\nTable 2 reports the evidence retrieval performance of all models discussed.Three base models that target either semantic equivalence or inference do not yield optimal performance.As expected, the MSMARCO CE achieves the highest performance among the base models, as it is a strong baseline that is commonly used for retrieval tasks.However, it only considers the semantic matching between question and individual candidate sentences, ignoring other important relevance matching characteristics such as exact matching, textual entailment, and relatedness between candidate evidence sentences.\n\nFor the baseline ensemble models, AR performs worse than the MSMARCO CE, while being slightly better than BM25 and the QNLI CE.Its retrieval performance is essentially a compromise among the performances of the three base models, because it directly averages the individual ranking results.In contrast, SimCom4 does take advantage of complementary relevance signals from the base models, so to perform better than any of the individual base model.This suggests that with an appropriate aggregation, the base models cooperate Models P@3 P@5 MAP R@3 R@5 R@10 Base models BM25 0. advantageously to produce a better ranking.However, it fails to deliver the best overall performance because it simply combines the final output scores from the base models without exploiting the interactions between the relevance signals behind.Lastly, our approaches (i.e., EAR and EARnest) not only outperform the base models, but also exceed the order-based and score-based ensemble models on all metrics.They both jointly consider diverse relevance signals simultaneously, and therefore achieve greater improvements on the retrieval performances.EARnest further considers the relatedness between evidence sentences, becoming our best model.It achieves the highest MAP, and higher than the MSMARCO CE by 10%5 .",
            "score": 0.3992938898919103,
            "section_title": "Results",
            "char_start_offset": 20613,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 296
                },
                {
                    "start": 298,
                    "end": 373
                },
                {
                    "start": 373,
                    "end": 477
                },
                {
                    "start": 477,
                    "end": 630
                },
                {
                    "start": 630,
                    "end": 889
                },
                {
                    "start": 891,
                    "end": 1018
                },
                {
                    "start": 1018,
                    "end": 1181
                },
                {
                    "start": 1181,
                    "end": 1338
                },
                {
                    "start": 1338,
                    "end": 1511
                },
                {
                    "start": 1511,
                    "end": 1714
                },
                {
                    "start": 1714,
                    "end": 1877
                },
                {
                    "start": 1877,
                    "end": 2019
                },
                {
                    "start": 2019,
                    "end": 2113
                },
                {
                    "start": 2113,
                    "end": 2182
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82275390625
        },
        {
            "corpus_id": "207863578",
            "title": "BAS: An Answer Selection Method Using BERT Language Model",
            "text": "The proposed language model-based models are models that instead of overcoming the answer-selection task from scratch, use pre-trained language models that have a complete understanding of the language. These models used the pre-trained language models to overcome the answer-selection task in a similar way proposed by Howard et al. (Howard & Ruder, 2018). Yoon et al. (Yoon et al., 2019) developed a model which used language models for answer-selection task. This model used the ELMo language model (Peters et al., 2018) along with techniques such as Latent-Clustering and demonstrated that the combination of these components produced a robust model.",
            "score": 0.39857759446045876,
            "section_title": "Language Model-based models",
            "char_start_offset": 18632,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 654
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82275390625
        },
        {
            "corpus_id": "273707185",
            "title": "Beyond Content Relevance: Evaluating Instruction Following in Retrieval Models",
            "text": "For general large language models, we use two settings: In the point-wise setting, both the query and document are inputs, with the output probabilities of True or False used as similarity scores. In the list-wise setting, following (Pradeep et al., 2023b), a list of documents is provided as a prompt (see Appendix C), and the model returns the ranked document IDs in a list. \n\nBased on Mistral-7B-Instruct-v0.2 (Jiang et al., 2023), we conduct a specialized experiment to evaluate its zero-shot performance in three retrieval settings: dense retrieval, point-wise reranking, and list-wise reranking. As a highly capable instruction-tuned model, Mistral is expected to demonstrate instruction-following abilities in retrieval tasks without fine-tuning. This experiment explores Mistral's potential as a zero-shot retrieval model, assessing whether it can naturally generate strong embeddings or act as an effective reranker to identify instruction-relevant documents from the list of candidates. For the mode of dense retrieval, we use mean pooling to obtain the sentence level representation. \n\nWe include GPT-4o as a strong baseline due to its demonstrated instruction-following capabilities and to set a high performance benchmark for all models. \n\nTable 1 provides a detailed comparison of different retrieval models across six dimensions using nDCG@10, Robustness@10, p-MRR, WISE, and SICR. The table also includes the average rankings of the golden documents in the instruction mode across the three retrieval models. Notably, almost all models achieved relatively high nDCG, indicating that relying solely on nDCG is insufficient to capture the impact of instructions on ranking changes. Although Robustness can be used for model comparison, it is unable to assess the extent of performance changes before and after instructions because the relevant documents corresponding to the three retrieval modes differ. p-MRR can partially reflect the model's responsiveness to different instructions; however, due to the limitations of this metric, the results are not expressed with sufficient accuracy. For instance, according to p-MRR evaluations, the instruction-following performance of bge-large-v.5 and Instructor models is significantly better than that of GPT-4o.",
            "score": 0.39857759446045876,
            "section_title": "EXPERIMENTAL SETUP",
            "char_start_offset": 16703,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 376
                },
                {
                    "start": 379,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1094
                },
                {
                    "start": 1097,
                    "end": 1250
                },
                {
                    "start": 1253,
                    "end": 1396
                },
                {
                    "start": 1397,
                    "end": 1524
                },
                {
                    "start": 1525,
                    "end": 1695
                },
                {
                    "start": 1696,
                    "end": 1918
                },
                {
                    "start": 1919,
                    "end": 2104
                },
                {
                    "start": 2105,
                    "end": 2272
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88720703125
        },
        {
            "corpus_id": "7405607",
            "title": "Learning to Explain Entity Relationships in Knowledge Graphs",
            "text": "We compare the performance of typical document retrieval models and state-of-the-art sentence retrieval models in order to answer RQ1. We consider five sentence retrieval models: Lucene ranking (LC), language modeling with Dirichlet Has one # pairs # sentences Method NDCG@1 NDCG@10 ERR@1 ERR@10 Exc@1 Per@1 fair 0.8150 \u25b2 0.9245 \u25b2 0.7640 \u25b2 0.8518 \u25b2 0.8909 \u25b2 0.7227 \u25b2 Table 3: Results for the best baseline (B5) and the learning to rank method (LTR). \n\nsmoothing (LM), BM25, TFISF, and Recursive TF-ISF (R-TFISF). We follow related work and set \u00b5 = 0.1 for R-TFISF, k = 1 and b = 0 for BM25 and \u00b5 = 250 for LM (Fern\u00e1ndez et al., 2011). \n\nIn our experiments, a query q is constructed using various combinations of surface forms of the two entities e i and e j and the relationship r. Each entity in the entity pair can be represented by its title, the titles of any redirect pages pointing to the entity's article, the n-grams used as anchors in Wikipedia to link to the article of the entity, or the union of them all. The relationship r can be represented by the terms in the relationship, synonyms in wordnet(r), or by phrases in word2vec(r). \n\nFirst, we fix the way we represent r. Baseline B1 does not include any representation of r in the query. B2 includes the relationship terms of r, while B3 includes the relationship terms of r and the synonyms in wordnet(r). B4 includes the terms of r and the phrases in word2vec(r), and B5 includes the relationship terms of r, the synonyms in wordnet(r) and the phrases in word2vec(r). Combining these variations with the entity representations, we find that all combinations that use the titles as representation and R-TFISF as the retrieval function outperform all other combinations. 7",
            "score": 0.39857759446045876,
            "section_title": "Results and Analysis",
            "char_start_offset": 20617,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 449
                },
                {
                    "start": 452,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 634
                },
                {
                    "start": 637,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1143
                },
                {
                    "start": 1146,
                    "end": 1183
                },
                {
                    "start": 1184,
                    "end": 1250
                },
                {
                    "start": 1251,
                    "end": 1369
                },
                {
                    "start": 1370,
                    "end": 1532
                },
                {
                    "start": 1533,
                    "end": 1735
                }
            ],
            "ref_mentions": [
                {
                    "start": 609,
                    "end": 633,
                    "matchedPaperCorpusId": "14940567"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83544921875
        },
        {
            "corpus_id": "7188219",
            "title": "A Translation Model for Sentence Retrieval",
            "text": "We incorporated a machine translation model in two steps: estimation and ranking. In the estimation step, the probability that a term in the sentence \"translates\" to a term in the query is estimated using the implementation of IBM Model 1 (Brown et al., 1990) in GIZA++ (Al-Onaizan et al., 1999) out-of-the-box without alteration. In the ranking step we incorporate the translation probabilities into the querylikelihood framework.\n\nIn Berger and Lafferty (1999), the IBM Model 1 is incorporated thus:\n\nwhere P (q i |s j ) is the probability that term s j in the sentence translates to term q i in the query.\n\nIf the translation probabilities are modified such that P (q i |s j ) = 1 if q i = s j and 0 otherwise, this is Berger and Lafferty's \"Model 0\", and it is exactly the query-likelihood model (described in section 2.2). A major difference between machine translation and sentence retrieval is that machine translation assumes there is little, if any, overlap in the vocabularies of the two languages. In sentence retrieval we depend heavily on the overlap between the two vocabularies. With the Berger and Lafferty formulation in equation 1, the probability of a word translating to itself is estimated as a fraction of the probability of the word translating to all other words. Because the probabilities must sum to one, if there are any other translations for a given word, its self-translation probability will be less than 1.0. To accommodate this monolingual condition, we make the following improvement.\n\nLet t i = 1 if there exists a term in the sentence s j such that q i = s j , and 0 otherwise:\n\nThe translation probabilities still sum to one. We determined empirically that this adjustment improved the results over IBM model 1, and over Berger and Lafferty model 0.",
            "score": 0.39857759446045876,
            "section_title": "Translation Models",
            "char_start_offset": 4000,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 239,
                    "end": 259,
                    "matchedPaperCorpusId": "14386564"
                },
                {
                    "start": 436,
                    "end": 462,
                    "matchedPaperCorpusId": "10837883"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53857421875
        },
        {
            "corpus_id": "10844118",
            "title": "Cache-Augmented Latent Topic Language Models for Speech Retrieval",
            "text": "The use of both repetition and broad topic context have been exploited in a variety of ways by the speech recognition and retrieval communities. Cache-based or adaptive language models were 1 some of the first approaches to incorporate information beyond a short N-gram history (where N is typically 3-4 words).\n\nCache-based models assume the probability of a word in a document d is influenced both by the global frequency of that word and N-gram context as well as by the N-gram frequencies of d (or preceding cache of K words). Although most words are rare at the corpus level, when they do occur, they occur in bursts. Thus a local estimate, from the cache, may be more reliable than the global estimate. Jelinek (1991) and Kuhn (1990) both successfully applied these types of models for speech recognition, and Rosenfeld (1994), using what he referred to as 'trigger pairs', also realized significant gains in WER. More recently, recurrent neural network language models (RNNLMs) have been introduced to capture more of these \"long-term dependencies\" (Mikolov et al., 2010). In terms of speech retrieval, recent efforts have looked at exploiting repeated keywords at search time, without directly modifying the recognizer (Chiu and Rudnicky, 2013;Wintrode, 2014a).\n\nWork within the information retrieval (IR) community connects topicality with retrieval. Hearst and Plaunt (1993) reported that the \"subtopic structuring\" of documents can improve full-document retrieval. Topic models such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003) or Probabilistic Latent Semantic Analysis (PLSA) (Hofmann, 2001) are used to the augment the document-specific language model in probabilistic, language-model based IR (Wei and Croft, 2006;Chen, 2009;Liu and Croft, 2004;Chemudugunta et al., 2007). In all these cases, topic information was helpful in boosting retrieval performance above baseline vector space or N-gram models.\n\nOur proposed model closely resembles that from Chemudugunta et al. (2007), with our notions of broad and local context corresponding to their \"general and specific\"",
            "score": 0.39857759446045876,
            "section_title": "Related Work",
            "char_start_offset": 1988,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 709,
                    "end": 723,
                    "matchedPaperCorpusId": "11601499"
                },
                {
                    "start": 1056,
                    "end": 1078,
                    "matchedPaperCorpusId": "17048224"
                },
                {
                    "start": 1227,
                    "end": 1252,
                    "matchedPaperCorpusId": "15421536"
                },
                {
                    "start": 1252,
                    "end": 1268,
                    "matchedPaperCorpusId": "13528696"
                },
                {
                    "start": 1360,
                    "end": 1384,
                    "matchedPaperCorpusId": "5390627"
                },
                {
                    "start": 1600,
                    "end": 1615,
                    "matchedPaperCorpusId": "7605995"
                },
                {
                    "start": 1719,
                    "end": 1740,
                    "matchedPaperCorpusId": "3343003"
                },
                {
                    "start": 1740,
                    "end": 1751,
                    "matchedPaperCorpusId": "8458087"
                },
                {
                    "start": 1751,
                    "end": 1771,
                    "matchedPaperCorpusId": "15479915"
                },
                {
                    "start": 1771,
                    "end": 1797,
                    "matchedPaperCorpusId": "2447517"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93359375
        },
        {
            "corpus_id": "14530074",
            "title": "Getting Started with Neural Models for Semantic Matching in Web Search",
            "text": "Zuccon et al [93] compute translation probabilities of word pairs using the cosine similarity of Word2Vec vectors in a translation language model for retrieval. The translation model enriched by embedding similarity scores is called the Neural Translation Language Model (NLTM). NLTM is shown to outperform the Dirichlet LM baseline and to be comparable to translation model with Mutual Information. \n\nCosine similarity of Word2Vec embeddings is used in a similar way in the Generalized Language Model (GLM) [29]. In the GLM, the probability of a query term t to be generated by a document or the collection is modelled by generating an intermediate term t \u2032 followed by a noisy channel model that transforms t to t \u2032 . Cosine similarity of word embeddings is used for computing the transformation probabilities between the intermediate term and the actual term. The GLM outperforms LM and LDA-smoothed LM baselines in terms of MAP score on the TREC 6, 7 and 8 and TREC Robust data sets. However, an LDA smoothed LM achieved higher recall scores.",
            "score": 0.39857759446045876,
            "section_title": "Implicit",
            "char_start_offset": 48145,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 399
                },
                {
                    "start": 402,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1046
                }
            ],
            "ref_mentions": [
                {
                    "start": 13,
                    "end": 17,
                    "matchedPaperCorpusId": "11236762"
                },
                {
                    "start": 508,
                    "end": 512,
                    "matchedPaperCorpusId": "18739283"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9189453125
        },
        {
            "corpus_id": "21012910",
            "title": "Modelling Word Burstiness in Natural Language: A Generalised Polya Process for Document Language Models in Information Retrieval",
            "text": "The general model outlined here (Eq. 1) is an intuitive statistical generative model of documents. The vector u i can be seen as storing the state of the model at a particular time i. Both the multinomial and multivariate P\u00f3lya urn (SPUD [5]) language model are specific instances of this model and are instantiated by different settings of M. Given that the SPUD language model significantly improves upon the multinomial model in information retrieval, the further extensions hold the promise of improved performance and of greater theoretical understanding. Furthermore, it is worth noting that the dependencies that the models3 capture, span a greater distance than n-gram models (i.e. a word occurring at the start of a document affects the choice of word at the end of a document). \n\nThe main challenges to implementing the remaining model variants are in estimating M and u 0 from a large background model (document collection D) and subsequently in inferring the initial state of each document model. For large scale collections this a computationally expensive inverse problem. However, the upcoming section will outline some promising initial experiments with regard to the third variant of the general model (i.e. modelling term-specific burstiness for retrieval). The fourth variant is left for future work.",
            "score": 0.39857759446045876,
            "section_title": "Discussion",
            "char_start_offset": 7319,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 787
                },
                {
                    "start": 790,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1086
                },
                {
                    "start": 1087,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1319
                }
            ],
            "ref_mentions": [
                {
                    "start": 238,
                    "end": 241,
                    "matchedPaperCorpusId": "9519540"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9755859375
        },
        {
            "corpus_id": "256389797",
            "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
            "text": "We introduce REPLUG, a retrieval-augmented language modeling framework that treats the language model (LM) as a black box and augments it with a tuneable retrieval model. Unlike prior retrieval-augmented LMs that train language models with special cross-attention mechanisms to encode the retrieved text, REPLUG simply prepends retrieved documents to the input for the frozen black-box LM. This simple design can be easily applied to any existing language models. Furthermore, we show that the LM can be used to supervise the retrieval model, which can then find documents that help the LM make better predictions. Our experiments demonstrate that REPLUG with the tuned retriever significantly improves the performance of GPT-3 (175B) on language modeling by 6.3%, as well as the performance of Codex on five-shot MMLU by 5.1%. Code is publicly released at github.com/swj0419/REPLUG.",
            "score": 0.39857759446045876,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88818359375
        },
        {
            "corpus_id": "270370926",
            "title": "The Factorization Curse: Which Tokens You Predict Underlie the Reversal Curse and More",
            "text": "We now investigate information retrieval capabilities across learning objectives through the lens of different factorizations of the joint sequence probability.Specifically, we compare \u2022 AR: The standard autoregressive causal next-token prediction.Though all models generate tokens autoregressively, we use AR as a shorthand for left-to-right models, in line with Equation (1).\n\n\u2022 AR w/reverse: AR prediction on sequences and their token-level reverse.2\n\n\u2022 MLM r: BERT-like masked language modeling with fixed random masking rate, r.\n\n\u2022 MLM-U: MLM with r \u223c U(0, 1).PLM results are similar, and are reported in the Appendix.\n\nTo ensure a fair comparison and allow each objective to perform optimally, we employ model architectures specifically designed for each objective.For autoregressive (AR) training, we use GPT-2 (Radford et al., 2019) and Mistral (Jiang et al., 2023).For masked language modeling (MLM), we use BERT (Devlin et al., 2019).\n\nFinally, for MLM-U, we employ an encoder-decoder model3 based on the GPT architecture (see Appendix G for details).\n\nWe study these models across increasing levels of complexity and realism, beginning with a controlled retrieval task using synthetic tokens to a retrieval task using natural text from Wikipedia articles.In our evaluation, we find that the degree to which the learning objective factorizes the input reliably explains performance across this wide range of information retrieval tasks.Factorization-agnostic methods show improved knowledge retrieval capabilities.",
            "score": 0.39857759446045876,
            "section_title": "Experiments",
            "char_start_offset": 11176,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 160,
                    "end": 248
                },
                {
                    "start": 248,
                    "end": 377
                },
                {
                    "start": 379,
                    "end": 453
                },
                {
                    "start": 455,
                    "end": 533
                },
                {
                    "start": 535,
                    "end": 565
                },
                {
                    "start": 565,
                    "end": 623
                },
                {
                    "start": 625,
                    "end": 771
                },
                {
                    "start": 771,
                    "end": 874
                },
                {
                    "start": 874,
                    "end": 944
                },
                {
                    "start": 946,
                    "end": 1061
                },
                {
                    "start": 1063,
                    "end": 1266
                },
                {
                    "start": 1266,
                    "end": 1446
                },
                {
                    "start": 1446,
                    "end": 1524
                }
            ],
            "ref_mentions": [
                {
                    "start": 922,
                    "end": 943,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.849609375
        },
        {
            "corpus_id": "218487298",
            "title": "AVA: an Automatic eValuation Approach to Question Answering Systems",
            "text": "Transformer-based architectures have proved to be powerful language models, which can capture complex similarity patterns. Thus, they are suitable methods to improve our basic approach described in the previous section. Following the linear classifier modeling, we propose three different ways to exploit the relations among the members of the tuple (q, r, s, t). \n\nLet B be a pre-trained language model, e.g., the recently proposed BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019), XLNet (Yang et al., 2019), AlBERT (Lan et al., 2020). We use a language model to compute the embedding representation of the tuple members: B (a, a ) \u2192 x \u2208 R d , where (a, a ) is a sentence pair, x is the output representation of the pair, and d is the dimension of the output representations. The classification layer is a standard feedforward network as A (x) = W x + b, where W and b are parameters we learn by fine-tuning the model on a dataset D. \n\nWe describe different designs for A as follows. A 0 : Text-Pair Embedding \n\nWe build a language model representation for pairs of members of the tuple, x = (q, r, t) by simply inputing them to Transformer models B in the standard sentence pair fashion. We consider four different configurations of A 0 , one for each following pair (q, r), (q, t), (r, t), and one for the triplet, (q, r, t), modeled as the concatenation of the previous three. The representation for each pair is produced by a different and independent BERT instance, i.e., B p . More formally, we have the following three models A 0 (B p (p)), \u2200p \u2208 D 0 , where D 0 = {(q, r), (q, t), (r, t)}. Additionally, we design a model over (q, r, t) with A 0 (\u222a p\u2208D 0 B p (p)), where \u222a means concatenation of the representations.",
            "score": 0.39857759446045876,
            "section_title": "Transformer-based models",
            "char_start_offset": 9703,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 363
                },
                {
                    "start": 366,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 782
                },
                {
                    "start": 783,
                    "end": 940
                },
                {
                    "start": 943,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1016
                },
                {
                    "start": 1019,
                    "end": 1195
                },
                {
                    "start": 1196,
                    "end": 1386
                },
                {
                    "start": 1387,
                    "end": 1489
                },
                {
                    "start": 1490,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 1730
                }
            ],
            "ref_mentions": [
                {
                    "start": 523,
                    "end": 541,
                    "matchedPaperCorpusId": "202888986"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70263671875
        },
        {
            "corpus_id": "5764728",
            "title": "Embedding Web-Based Statistical Translation Models in Cross-Language Information Retrieval",
            "text": "In Section 4.1, we have seen that the basic retrieval model measures the cross-entropy between two language models: a language model of the query and a language model of the document. 11 Instead of translating a query before estimating a query model (the external approach), we propose to directly estimate the query model in the target language. We will do this by decomposing the problem into two components that are easier to estimate: \n\nwhere L is the size of the source vocabulary. Thus, P (t i |M Qs ) can be approximated by combining the translation model P (t i |s j ), which we can estimate on the parallel Web corpus, and the familiar P (s j |M Qs ) which can be estimated using relative frequencies. This simplified model, from which we have dropped the dependency of P (t i |s j ) on Q, can be interpreted as a way of mapping the probability distribution function in the source language event space P (s j |M Qs ) onto the event space of the target language vocabulary. Since this probabilistic mapping function involves a summation over all possible translations, mapping the query model from the source language can be implemented as the matrix product of a vector representing the query probability distribution over source language terms with the translation matrix P (t i |s j ). 12 The result is a probability distribution function over the target language vocabulary. \n\nNow we can substitute the query model P (\u03c4 i |M Q ) in formula ( 7) with the target language query model in ( 8) and, after a similar substitution operation for P (\u03c4 i |M C ) , we arrive at CLIR-model QT:",
            "score": 0.39857759446045876,
            "section_title": "Estimating the query model in the target language (QT)",
            "char_start_offset": 62218,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 438
                },
                {
                    "start": 441,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1386
                },
                {
                    "start": 1389,
                    "end": 1593
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92626953125
        },
        {
            "corpus_id": "260900354",
            "title": "RAVEN: In-Context Learning with Retrieval Augmented Encoder-Decoder Language Models",
            "text": "In this section, we first explore in-context learning with retrieval-augmented encoderdecoder language models in the literature. Building upon the analysis, we develop models with enhanced zero-shot performance and improved in-context learning abilities.",
            "score": 0.39857759446045876,
            "section_title": "Methodology",
            "char_start_offset": 6171,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 254
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77197265625
        },
        {
            "corpus_id": "15981382",
            "title": "Combining Open-Source with Research to Re-engineer a Hands-on Introductory NLP Course",
            "text": "For this topic, we have so far restricted ourselves to the textbook (Jurafsky and Martin, 2000); the in-class discussion and programming assignments have been missing a hands-on component. We have written a Python interface to the SRI Language Modeling toolkit (Stolcke, 2002) for use in our research work. This interface uses the Simplified Wrapper & Interface Generator (SWIG) to generate a Python wrapper around our C code that does all the heavy lifting via the SRILM libraries. We are currently working on integrating this module into NLTK which would allow all NLTK users, including our students in the next version of the course, to build and query statistical language models directly inside their Python code. This module, combined with the large real-world corpora, would provide a great opportunity to perform hands-on experiments with language models and to understand the various smoothing methods. In addition, this would also allow a language model to be used in an assignment for any other topic should we need it.\n\n2. Teaching Distributional Similarity. The idea that a language possesses distributional structure-first discussed at length by Harris (1954)-says that one can describe a language in terms of relationships between the occurrences of its elements (words, morphemes, phonemes). The name for the phenomenon is derived from an element's distribution-sets of other elements in particular positions that occur with the element in utterances or sentences. This work led to the concept of distributional similarity-words or phrases that share the same distribution, i.e., the same set of words or in the same context in a corpus, tend to have similar meanings. This is an extremely popular concept in corpus linguistics and forms the basis of a large body of work. We believe that this is an important topic that should be included in the curriculum. We plan to do so in the context of lexical paraphrase acquisition or synonyms automatically from corpora, a task that relies heavily on this notion of distributional similarity. There has been a lot of work in this area in the past years (Pereira et al., 1993;Gasperin et al., 2001;Glickman and Dagan, 2003;Shimohata and Sumita, 2005), much of which can be easily replicated using the Python-NLTK",
            "score": 0.39857759446045876,
            "section_title": "Hands-on Statistical Language Modeling.",
            "char_start_offset": 21602,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 261,
                    "end": 276,
                    "matchedPaperCorpusId": "1988103"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1856689453125
        },
        {
            "corpus_id": "21807753",
            "title": "On the Equivalence of Generative and Discriminative Formulations of the Sequential Dependence Model",
            "text": "The sequential dependence model [11] is a very robust retrieval model that has been shown to outperform or to be on par with many retrieval models [8]. Its robustness comes from an integration of unigram, bigram, and windowed bigram models through the theoretical framework of Markov random fields. The SDM Markov random field is associated with a set of parameters which are learned through the usual parameter estimation techniques for undirected graphical models with training data. Despite its simplicity, the SDM model is a versatile method that provides a reasonable input ranking for further learning-to-rank phases or in as a building block in a larger model [6]. As it is a feature-based learning-to-rank model, it can be extended with additional features, such as in the latent concept model [2,12]. Like all Markov random field models it can be extended with further variables, for instance to incorporate external knowledge, such as entities from an external semantic network. It can also be extended with additional conditional dependencies, such as further term dependencies that are expected to be helpful for the retrieval task, such as in the hypergraph retrieval model [1]. \n\nThe essential idea of the sequential dependence model (SDM) is to combine unigram, bigram, and windowed bigram models so that they mutually compensate each other's shortcomings. The unigram gram model, which is also called the bag-of-words model and which is closely related to the vector-space model, is indifferent to word order. This is an issue for multi-word expressions which are for instance common for entity names such as \"Massachusetts Institute of Technology\" or compound nouns such as \"information retrieval\" which have a different meaning in combination than individually. This shortcoming is compensated for in bigram model which incorporate word-order by modeling the probability of joint occurrence of two subsequent query words q i\u22121 q i or condition the probability of ith word in the query, q i , on seeing the previous word q i\u22121 .",
            "score": 0.3983111096393575,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1191
                },
                {
                    "start": 1194,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1525
                },
                {
                    "start": 1526,
                    "end": 1779
                },
                {
                    "start": 1780,
                    "end": 2045
                }
            ],
            "ref_mentions": [
                {
                    "start": 32,
                    "end": 36,
                    "matchedPaperCorpusId": "1118305"
                },
                {
                    "start": 147,
                    "end": 150,
                    "matchedPaperCorpusId": "3154405"
                },
                {
                    "start": 667,
                    "end": 670,
                    "matchedPaperCorpusId": "9175104"
                },
                {
                    "start": 802,
                    "end": 805,
                    "matchedPaperCorpusId": "207184945"
                },
                {
                    "start": 805,
                    "end": 808,
                    "matchedPaperCorpusId": "14811099"
                },
                {
                    "start": 1187,
                    "end": 1190,
                    "matchedPaperCorpusId": "10501047"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.953125
        },
        {
            "corpus_id": "13892149",
            "title": "Advancing Trace Recovery Evaluation - Applied Information Retrieval in a Software Engineering Context",
            "text": "Two common models are the Binary Independence retrieval Model (BIM) [25] [82] and Probabilistic Inference Networks [92]. A subset of probabilistic retrieval estimate Language Models (LM) for each document. Documents are then ranked based on the probability that a document would generate the terms of a query [78]. A later refinement of simple LMs, topic models, describes documents as a mixture over topics, where each topic is characterized by an LM. Examples include probabilistic latent semantic indexing [45] and Latent Dirichlet Allocation (LDA) [11].",
            "score": 0.3979634658246799,
            "section_title": "Information Retrieval -Satisfying an Information Need",
            "char_start_offset": 13882,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 557
                }
            ],
            "ref_mentions": [
                {
                    "start": 68,
                    "end": 72,
                    "matchedPaperCorpusId": "3252915"
                },
                {
                    "start": 115,
                    "end": 119,
                    "matchedPaperCorpusId": "11025023"
                },
                {
                    "start": 309,
                    "end": 313,
                    "matchedPaperCorpusId": "2759772"
                },
                {
                    "start": 509,
                    "end": 513,
                    "matchedPaperCorpusId": "7605995"
                },
                {
                    "start": 552,
                    "end": 556,
                    "matchedPaperCorpusId": "3177797"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.994140625
        },
        {
            "corpus_id": "277151020",
            "title": "Privacy-Aware RAG: Secure and Isolated Knowledge Retrieval",
            "text": "Retrieval-Augmented Generation (RAG), initially introduced by [35], has rapidly become one of the most prominent methodologies for enhancing the generative capabilities of Large Language Models (LLMs) [11,45,47,51]. This approach significantly improves the accuracy and relevance of generated outputs by mitigating common issues such as \"hallucinations\" in LLMs [20,50]. One of RAG's distinctive features is its flexible architecture, enabling the interchange or update of its core components-the dataset, retriever, and LLM-without necessitating retraining or fine-tuning of the entire system [13,46]. Consequently, RAG has been widely adopted across various practical applications, including personal chatbots and specialized domain experts like medical diagnostic assistants [41]. \n\nThe increasing attention towards LLMs, both in industry and academia, underscores their remarkable ability to facilitate convincing linguistic interactions with humans [29,30,36,60]. However, adapting these models to new knowledge not available at training time poses significant challenges. For instance, in real-world scenarios involving virtual assistants [15,21,33], the knowledge base or tasks may evolve over time, requiring model adaptation through fine-tuning processes [2,17,58]. This can lead to catastrophic forgetting, where previously acquired knowledge is lost [38]. Alternatively, new knowledge can be appended to the input prompt via in-context learning (ICL) without altering the model parameters [6,19,37,53,55], a principle that underpins RAG systems. \n\nIn the context of RAG, a typical system comprises four principal components [45]: (i) a text embedder function , which maps textual information into a high-dimensional embedding space; (ii) a storage mechanism, often referred to as a vector store, that memorizes texts and their embedded representations; (iii) a similarity function, such as cosine similarity, used to evaluate the similarity between pairs of embedded text vectors; and (iv) a generative model, denoted as function  , typically an LLM, that produces output text based on input prompts and retrieved information.",
            "score": 0.3978152759431647,
            "section_title": "Retrieval-Augmented Generation (RAG)",
            "char_start_offset": 11697,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 783
                },
                {
                    "start": 786,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1366
                },
                {
                    "start": 1367,
                    "end": 1556
                },
                {
                    "start": 1559,
                    "end": 2137
                }
            ],
            "ref_mentions": [
                {
                    "start": 62,
                    "end": 66,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 205,
                    "end": 208,
                    "matchedPaperCorpusId": "256459451"
                },
                {
                    "start": 211,
                    "end": 214,
                    "matchedPaperCorpusId": "261822526"
                },
                {
                    "start": 594,
                    "end": 598,
                    "matchedPaperCorpusId": "258479968"
                },
                {
                    "start": 778,
                    "end": 782,
                    "matchedPaperCorpusId": "267112617"
                },
                {
                    "start": 961,
                    "end": 964,
                    "matchedPaperCorpusId": "220961531"
                },
                {
                    "start": 1149,
                    "end": 1152,
                    "matchedPaperCorpusId": "269914863"
                },
                {
                    "start": 1152,
                    "end": 1155,
                    "matchedPaperCorpusId": "257445349"
                },
                {
                    "start": 1500,
                    "end": 1503,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85400390625
        },
        {
            "corpus_id": "219721819",
            "title": "Improving the Community Question Retrieval Performance Using Attention-Based Siamese LSTM",
            "text": "The question retrieval task has been intensively studied over the past decade. Early works were based on the vector space model referred to as VSM to calculate the cosine similarity between a query and archived questions [2]. However, the major limitation of VSM is that it favors short questions, while cQA services can handle a wide variety of questions not limited to facto\u00efd questions. Language Models (LM)s [3] have been also used to model queries as sequences of terms instead of sets of terms. LMs estimate the relative likelihood for each possible successor term taking into account relative positions of terms. Nevertheless, such models might not be effective when there are only few common words between the questions. Further methods exploited the available category information of questions such as in [2]. Wang et al. [15] used a parser to build syntactic trees of questions, and rank them based on the similarity between their syntactic trees. Nonetheless, such an approach requires large training data and existing parsers are still not well-trained to parse informally written questions. Recent works focused on the representation learning for questions, relying on the Word Embedding model for learning distributed representations of words in a lowdimensional vector space. Along with the popularization of word embeddings and its capacity to produce distributed representations of words, advanced NN architectures such as Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN) and LSTM have proven effectiveness in extracting higher-level features from constituting word embeddings. For instance, Dos Santos et al. [5] employed CNN and bag-of-words (BOW) representations of the questions to calculate the similarity scores. Within the same context, Mohtarami et al. [8] developed a bag-of-vectors approach and used CNN and attention-based LSTMs to capture the semantic similarity between the community questions and rank them accordingly. LSTM model was also used in [12], where the weights learned by the attention mechanism were exploited for selecting important segments and enhancing syntactic tree-kernel models. More recently, the question retrieval task was modeled as a binary classification problem in [6] using a combination of LSTM and a contrastive loss function to effectively memorize the long term depend",
            "score": 0.3974576712438988,
            "section_title": "Related Work",
            "char_start_offset": 2709,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 221,
                    "end": 224,
                    "matchedPaperCorpusId": "1025625"
                },
                {
                    "start": 412,
                    "end": 415,
                    "matchedPaperCorpusId": "17838981"
                },
                {
                    "start": 814,
                    "end": 817,
                    "matchedPaperCorpusId": "1025625"
                },
                {
                    "start": 831,
                    "end": 835,
                    "matchedPaperCorpusId": "419321"
                },
                {
                    "start": 1647,
                    "end": 1650,
                    "matchedPaperCorpusId": "15217458"
                },
                {
                    "start": 1999,
                    "end": 2003,
                    "matchedPaperCorpusId": "14491346"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8173828125
        },
        {
            "corpus_id": "267413218",
            "title": "BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation",
            "text": "The most common form of embedding-based retrieval method is dense retrieval (Karpukhin et al., 2020), where the text encoder's outputs are aggregated (e.g., via [CLS] or mean-pooling) to compute the embedding similarity. Another common alternative is known as multi-vecor retrieval (Khattab and Za-haria, 2020;Humeau et al., 2020), which applies fine-grained interactions for the text encoder's outputs to compute the embedding similarity. Finally, the text embeddings can also be transformed into term weights, which facilitates sparse or lexical retrieval (Luan et al., 2021;Dai and Callan, 2020;Lin and Ma, 2021). Typically, the above retrieval methods are realized by different embedding models. To the best of our knowledge, no existing method is able to unify all these functionalities. \n\nDespite the substantial technical advancement, most of the existing text embeddings are developed only for English, where other languages are lagging behind. To mitigate this problem, continual efforts are presented from multiple directions. One is the development of pre-trained multi-lingual text encoders, such as mBERT (Pires et al., 2019), mT5 (Xue et al., 2021), XLM-R (Conneau et al., 2020). Another one is the curation of training and evaluation data for multi-lingual text embeddings, e.g., MIRACL (Zhang et al., 2023c), mMARCO (Bonifacio et al., 2021), Mr. TyDi (Zhang et al., 2021b), MKQA (Longpre et al., 2021). At the same time, the multi-lingual text embeddings are continually developed from the community, e.g., mDPR (Zhang et al., 2023b), mContriever (Izacard et al., 2022), mE5 (Wang et al., 2022), etc. However, the current progress is still far from enough given the notable gap with English models and the huge imbalance between different languages.",
            "score": 0.39666511197948234,
            "section_title": "Related Work",
            "char_start_offset": 6711,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 699
                },
                {
                    "start": 700,
                    "end": 792
                },
                {
                    "start": 795,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1616
                },
                {
                    "start": 1617,
                    "end": 1765
                }
            ],
            "ref_mentions": [
                {
                    "start": 76,
                    "end": 100,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 310,
                    "end": 330,
                    "matchedPaperCorpusId": "210063976"
                },
                {
                    "start": 558,
                    "end": 577,
                    "matchedPaperCorpusId": "218470027"
                },
                {
                    "start": 577,
                    "end": 598,
                    "matchedPaperCorpusId": "220729621"
                },
                {
                    "start": 1118,
                    "end": 1138,
                    "matchedPaperCorpusId": "174798142"
                },
                {
                    "start": 1170,
                    "end": 1192,
                    "matchedPaperCorpusId": "207880568"
                },
                {
                    "start": 1302,
                    "end": 1322,
                    "matchedPaperCorpusId": "261557946"
                },
                {
                    "start": 1367,
                    "end": 1388,
                    "matchedPaperCorpusId": "237213465"
                },
                {
                    "start": 1395,
                    "end": 1417,
                    "matchedPaperCorpusId": "220871404"
                },
                {
                    "start": 1528,
                    "end": 1549,
                    "matchedPaperCorpusId": "247957843"
                },
                {
                    "start": 1563,
                    "end": 1585,
                    "matchedPaperCorpusId": "249097975"
                },
                {
                    "start": 1591,
                    "end": 1610,
                    "matchedPaperCorpusId": "254366618"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89453125
        },
        {
            "corpus_id": "204769467",
            "title": "Enhancing Question Retrieval in Community Question Answering Using Word Embeddings",
            "text": "Recently, along with the flourishing of cQA archives, much attention has been paid to the question retrieval task. Here, we overview some of the recent approaches proposed to address this task. \n\nSeveral works were based on the vector space model referred to as VSM to calculate the cosine similarity between a query and archived questions [6,4]. However, the main limitation of VSM is that it favors short questions, while cQA services can handle a wide variety of questions not limited to facto\u00efd questions. In order to overcome this shortcoming, BM25 has been used for question retrieval to take into consideration the question length [4]. Language Models (LM)s [5] have been also used to model queries as sequences of terms instead of sets of terms. LMs estimate the relative likelihood for each possible successor term taking into account relative positions of terms. Nonetheless, such models might not be effective when there are few common words between the questions. \n\nTo handle the vocabulary mismatch problem faced by LMs, the translation model was employed to learn correlation between words based on parallel corpora and it has obtained significant performance for question retrieval. The basic intuition behind translation-based models is to consider question-answer pairs as parallel texts then, relationships of words can be built by learning word-to-word translation probabilities such as in [18,3]. Within this context, Zhou et al. [22] tried to improve the word-based translation model by adding some contextual information when translating the phrases as a whole, instead of translating separate words. Singh et al. [16] was extended the word-based translation model by incorporating semantic information and explored strategies to learn the translation probabilities between words and concepts using the cQA archives and an entity catalog. Although, the above-mentioned basic models have yielded good results, questions and answers are not parallel in practice, rather they are different from the information they contain [20]. \n\nFurther approaches based on semantic similarity were required to bridge the lexical gap problem in question retrieval toward a deep understanding of short text to detect the equivalent questions. For instance, there were few attempts that have exploited the available category information for question retrieval like in [5,4,23].",
            "score": 0.39661891513047814,
            "section_title": "Related Work",
            "char_start_offset": 3120,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 193
                },
                {
                    "start": 196,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 975
                },
                {
                    "start": 978,
                    "end": 1197
                },
                {
                    "start": 1198,
                    "end": 1416
                },
                {
                    "start": 1417,
                    "end": 1622
                },
                {
                    "start": 1623,
                    "end": 1860
                },
                {
                    "start": 1861,
                    "end": 2048
                },
                {
                    "start": 2051,
                    "end": 2246
                },
                {
                    "start": 2247,
                    "end": 2380
                }
            ],
            "ref_mentions": [
                {
                    "start": 343,
                    "end": 345,
                    "matchedPaperCorpusId": "1025625"
                },
                {
                    "start": 638,
                    "end": 641,
                    "matchedPaperCorpusId": "1025625"
                },
                {
                    "start": 665,
                    "end": 668,
                    "matchedPaperCorpusId": "17838981"
                },
                {
                    "start": 1409,
                    "end": 1413,
                    "matchedPaperCorpusId": "266650254"
                },
                {
                    "start": 1413,
                    "end": 1415,
                    "matchedPaperCorpusId": "15813829"
                },
                {
                    "start": 1450,
                    "end": 1454,
                    "matchedPaperCorpusId": "2726891"
                },
                {
                    "start": 1636,
                    "end": 1640,
                    "matchedPaperCorpusId": "8234375"
                },
                {
                    "start": 2043,
                    "end": 2047,
                    "matchedPaperCorpusId": "5689836"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.875
        },
        {
            "corpus_id": "258564260",
            "title": "NeuroComparatives: Neuro-Symbolic Distillation of Comparative Knowledge",
            "text": "One unique challenge in probing language models for knowledge acquisition, as opposed to extracting pre-existing knowledge from web text, is knowing exactly what to probe LMs for, i.e., concept pairs for comparison. For practical applications, the comparatives are more likely to be useful when they are about entities sharing some common properties, e.g., \"red wine\" and \"white wine\" (Fig. 4), rather than unrelated ones e.g., \"cucumber\" and \"car\". Identifying a vast array of diverse yet relevant concepts for comparison is complex; thus, we developed a systematic process below: retrieval ( \u00a72.1.1), expansion ( \u00a72.1.2), and filtering ( \u00a72.1.3).",
            "score": 0.39650844233309424,
            "section_title": "Collecting Comparable Entities",
            "char_start_offset": 4891,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 648
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.322021484375
        },
        {
            "corpus_id": "208248357",
            "title": "The JDDC Corpus: A Large-Scale Multi-Turn Chinese Dialogue Dataset for E-commerce Customer Service",
            "text": "To further study the responses given by these models, we conduct some statistical analysis on the result sets. In Table 6, we present the response diversity (the portion of unique responses) in the Ground Truth set, BERT-Retrieval result set, and Seq2Seq-Copy result set. The portions of top three most common responses in the result sets are also listed in the table. Our observations can be summarized are as follows: \n\n1. For retrieval-based models, BERT-Retrieval outperforms BM25, which shows the strong ability of pretrained model in semantic matching task. For generative models, Seq2Seq-Copy performs the best, which shows the effectiveness of using attention and copy mechanism. 6), while the retrieval model tends to give responses containing specific information which may not fit the context and are quite different from the ground truth. \n\n3. The retrieval-based model performs much better in response diversity (Dist-1/2 in Table 5 and the response diversity in Table 6). While the performance of the generative model is very poor since it prefers to generate similar generic responses repeatedly (shown in Table 6), which is the common disadvantage of the generative models (Li et al., 2016).",
            "score": 0.39546959316532726,
            "section_title": "BLEU",
            "char_start_offset": 18656,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 419
                },
                {
                    "start": 422,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 850
                },
                {
                    "start": 853,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1207
                }
            ],
            "ref_mentions": [
                {
                    "start": 1189,
                    "end": 1206,
                    "matchedPaperCorpusId": "7287895"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.576171875
        },
        {
            "corpus_id": "235166616",
            "title": "Retrieval Enhanced Model for Commonsense Generation",
            "text": "We frame CommonGen challenge as a sequenceto-sequence task and adopt T5 (Raffel et al., 2020), a powerful pre-trained encoder-decoder model, as our base model. Fan et al. (2020) find conceptsrelated sentences in external corpora can benefit relational reasoning for CommonGen. We extend this idea by proposing retrieval-enhanced T5 (RE-T5) which equips original T5 with a trainable retriever for selecting prototype sentences based on given concepts. Meanwhile, referring to (Zhou et al., 2021), we design a pre-training task for Common-Gen which continue to pre-train RE-T5 on pseudo concept sets extracted from external corpora. We also use a retriever in this pre-training stage. Formally, given a concept set X = {x 1 , x 2 , . . . , x n }, where x i represents the ith concept and n is the number of concepts, our goal is to generate a natural language output of tokens Y = {y 1 , y 2 , . . . , y m }, which describes a common scenario in our daily life, using all given concepts in X.",
            "score": 0.39531918161792645,
            "section_title": "Method",
            "char_start_offset": 1821,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 72,
                    "end": 93,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 160,
                    "end": 177,
                    "matchedPaperCorpusId": "227231215"
                },
                {
                    "start": 475,
                    "end": 494,
                    "matchedPaperCorpusId": "226964491"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71240234375
        },
        {
            "corpus_id": "271097880",
            "title": "Beyond Benchmarks: Evaluating Embedding Model Similarity for Retrieval Augmented Generation Systems",
            "text": "Retrieval-Augmented Generation (RAG) is an emerging paradigm that helps mitigate the problems of factual hallucination [13] and outdated training data [27] of large language models (LLMs) by providing these models with access to an external, non-parametric knowledge source (e.g. a document corpus).Central to the functioning of RAG frameworks is the retrieval step, wherein a small subset of candidate documents is retrieved from the document corpus, specific to the input query or prompt.This retrieval process, known as dense-retrieval, hinges on text embeddings.Typically, the generation of these embeddings is assigned to an LLM, for which there are several options due to the rapid evolution of the field.Consequently, selecting the most suitable embedding model from an array of available choices emerges as a critical aspect in the development of RAG systems.The information to guide this choice is currently primarily limited to architectural details (which are also on occasion scarce due to the prevalence of closed models) and performance benchmarks such as the Massive Text Embedding Benchmark (MTEB) [28].\n\nWe posit that an analysis of the similarity of the embeddings generated by these models would significantly aid this model selection process.Given the large number of candidates and ever increasing scale of the models, a from-scratch empirical evaluation of the embedding quality of these LLMs on a particular task can incur significant costs.This challenge becomes especially pronounced when dealing with large-scale corpora comprising potentially millions of documents.While the relative performance scores of these models on benchmark datasets offer the simplified perspective of comparing a single scalar value on an array of downstream tasks, such a view of model similarity might overlook the nuances of the relative behaviour of the models [15].As an example, the absolute difference in precision@k between two retrieval systems only provides a weak indication of the overlap of retrieved results.We argue that identifying clusters of models with similar behaviour would allow practitioners to construct smaller, yet diverse candidate pools of models to evaluate.Beyond model selection, as highlighted by Klabunde et al., [14], such an analysis also facilitates the identification of common factors contributing to strong performance, easier model ensembling, and detection of potential instances of unauthorized model reuse.",
            "score": 0.39526656368445445,
            "section_title": "MOTIVATION",
            "char_start_offset": 13,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 299
                },
                {
                    "start": 299,
                    "end": 490
                },
                {
                    "start": 490,
                    "end": 566
                },
                {
                    "start": 566,
                    "end": 711
                },
                {
                    "start": 711,
                    "end": 867
                },
                {
                    "start": 867,
                    "end": 1119
                },
                {
                    "start": 1121,
                    "end": 1262
                },
                {
                    "start": 1262,
                    "end": 1464
                },
                {
                    "start": 1464,
                    "end": 1592
                },
                {
                    "start": 1592,
                    "end": 1873
                },
                {
                    "start": 1873,
                    "end": 2025
                },
                {
                    "start": 2025,
                    "end": 2191
                },
                {
                    "start": 2191,
                    "end": 2453
                }
            ],
            "ref_mentions": [
                {
                    "start": 119,
                    "end": 123,
                    "matchedPaperCorpusId": "246652372"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.869140625
        },
        {
            "corpus_id": "276775668",
            "title": "SePer: Measure Retrieval Utility Through The Lens Of Semantic Perplexity Reduction",
            "text": "Throughout this paper, we use the following notations. Let M denote a well-trained language model (the information receiver) capable of generating answers to queries q. The correct answer set to the query q is denoted by A = {a * }. The retrieved result is denoted by D, where D is a set of n atomic information d i , i.e., D={d 1 , d 2 , ..., d n }. We denote by P M (a) the likelihood model M assigns to answer a without retrieval, and by P M (a | D) the likelihood after incorporating D.",
            "score": 0.3946961894953656,
            "section_title": "NOTATIONS",
            "char_start_offset": 8726,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 54
                },
                {
                    "start": 55,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 490
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5947265625
        },
        {
            "corpus_id": "253384615",
            "title": "Suffix Retrieval-Augmented Language Modeling",
            "text": "Improving language model using retrieval technique is not new. [4] employs document retrieval to retrieve relevant documents which are used to create an adaptive language model and interpolating it with the background statistical N-gram language model. [5] employs information retrieval to perform language model adaptation for statistical machine translation. Once the language models are adapted, they are kept fixed during sequence generation. \n\nMemory Networks (MemNNs [6]) are a family of neural networks integrating a memory component which can be updated during training. To avoid large storage of external memory, MemNNs can be trained end-to-end [7] by computing a compatibility score between a query and memory items , similar to an attention mechanism. SUREALM resembles MemNNs as we utilize an external data store as \"memory\". However, SUREALM's \"memory\" contains precomputed embeddings and is retrieved with an embedding retriever for computational efficiency. Like E2E MemNNs [7], we also distinguish between memory and query. We further construct our memory as prefix-suffix pairs enabling a more effective retrieval scheme by matching input query with prefixes. \n\nMost recent development in language modeling is based on transformers [8]. BERT-based Masked language modeling [9,10] exploits bi-directional information of a sentence to predict the word identity of the masked tokens. While BERT is effective in encoding sequences, it is not suitable for sequence generation due to its non-causal nature. Causal language modeling such as GPT2 [11] is uni-directional. Our proposed model attempts to retain the best of the two worlds as autoregressive and simulated bi-directional via augmentation of suffix embeddings during sequence generation. \n\nOne noticeable work for language modeling using embedding retrieval is nearest neighbor language model (KNN-LM) [12]. Their approach store dynamic information in an external knowledge base. During sequence generation, KNN-LM uses the current prefix to retrieve similar prefixes in the data store using embedding retrieval. The output probability distribution is estimated by looking at the corresponding next words in the retrieved prefixes. Such word probability distribution is linearly interpolated with the output word distribution from the causal transformer LM.",
            "score": 0.3946961894953656,
            "section_title": "RELATED WORK",
            "char_start_offset": 3166,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 62
                },
                {
                    "start": 63,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 446
                },
                {
                    "start": 449,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 973
                },
                {
                    "start": 974,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1177
                },
                {
                    "start": 1180,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1398
                },
                {
                    "start": 1399,
                    "end": 1518
                },
                {
                    "start": 1519,
                    "end": 1581
                },
                {
                    "start": 1582,
                    "end": 1759
                },
                {
                    "start": 1762,
                    "end": 1879
                },
                {
                    "start": 1880,
                    "end": 1951
                },
                {
                    "start": 1952,
                    "end": 2084
                },
                {
                    "start": 2085,
                    "end": 2203
                },
                {
                    "start": 2204,
                    "end": 2329
                }
            ],
            "ref_mentions": [
                {
                    "start": 63,
                    "end": 66,
                    "matchedPaperCorpusId": "18439695"
                },
                {
                    "start": 253,
                    "end": 256,
                    "matchedPaperCorpusId": "2375338"
                },
                {
                    "start": 473,
                    "end": 476,
                    "matchedPaperCorpusId": "2926851"
                },
                {
                    "start": 655,
                    "end": 658,
                    "matchedPaperCorpusId": "1399322"
                },
                {
                    "start": 990,
                    "end": 993,
                    "matchedPaperCorpusId": "1399322"
                },
                {
                    "start": 1250,
                    "end": 1253,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1291,
                    "end": 1294,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1294,
                    "end": 1297,
                    "matchedPaperCorpusId": "198953378"
                },
                {
                    "start": 1874,
                    "end": 1878,
                    "matchedPaperCorpusId": "207870430"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95947265625
        },
        {
            "corpus_id": "2862121",
            "title": "Optimizing Language Model Information Retrieval System with Expectation Maximization Algorithm",
            "text": "Statistical language modeling (SLM) has been used in many different domains for decades and has also been applied to information retrieval (IR) recently. Documents retrieved using this approach are ranked according their probability of generating the given query. In this paper, we present a novel approach that employs the generalized Expectation Maximization (EM) algorithm to improve language models by representing their parameters as observation probabilities of Hidden Markov Models (HMM). In the experiments, we demonstrate that our method outperforms standard SLM-based and tf.idf-based methods on TREC 2005 HARD Track data.",
            "score": 0.3946961894953656,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9375
        },
        {
            "corpus_id": "244119685",
            "title": "Character-based PCFG Induction for Modeling the Syntactic Acquisition of Morphologically Rich Languages",
            "text": "The NeuralChar and NeuralWord models are evaluated on ten constituency treebanks, including Arabic (Maamouri et al., 2004), Chinese (Xia et al., 2000), English (Marcus et al., 1993), French (Abeill\u00e9 et al., 2003), German (Skut et al., 1998), Hebrew (Sima'an et al., 2001), Japanese (Alastair et al., 2018), Korean (Han et al., 2006), Polish (Woli\u0144ski et al., 2018) and Vietnamese (Nguyen et al., 2009). 8 Compared induction models include a pure word-based Bayesian PCFG model (DIMI, Jin et al., 2018a); a PCFG induction model that generates independently trained character-based word vectors (Flow, Jin et al., 2019); word-based neural models Compound and Compound-v (Kim et al., 2019), which differ in that Compound-v induces sentence-specific grammars, as well as its extension with lexical dependencies (L-PCFG, Zhu et al., 2020). At least three random initial seeds are used for each model and each language, and the average performance of grammars with the highest likelihoods are reported. Hyperparameters for the NeuralChar and NeuralWord models are reported in Appendix A, and those for other baseline models followed reported values in their respective papers.",
            "score": 0.3946961894953656,
            "section_title": "Procedures",
            "char_start_offset": 18639,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1170
                }
            ],
            "ref_mentions": [
                {
                    "start": 99,
                    "end": 122,
                    "matchedPaperCorpusId": "16205731"
                },
                {
                    "start": 132,
                    "end": 150,
                    "matchedPaperCorpusId": "5371286"
                },
                {
                    "start": 160,
                    "end": 180,
                    "matchedPaperCorpusId": "252796"
                },
                {
                    "start": 221,
                    "end": 239,
                    "matchedPaperCorpusId": "5508021"
                },
                {
                    "start": 341,
                    "end": 364,
                    "matchedPaperCorpusId": "21715487"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.386962890625
        },
        {
            "corpus_id": "238198426",
            "title": "Instance-Based Neural Dependency Parsing",
            "text": "Most of the studies above were published before the current deep learning era. Very recently, instance-based methods have been revisited and combined with neural models in language modeling (Khandelwal et al., 2019), MT (Khandelwal et al., 2020), and question answering (Lewis et al., 2020). They augment a main neural model with a non-parametric sub-module that retrieves auxiliary objects, such as similar tokens and documents. Guu et al. (2020) proposed to parameterize and learn the sub-module for a target task. \n\nThese studies assume a different setting from ours. There is no ground-truth supervision signal for retrieval in their setting, so they adopt nonparametric approaches or indirectly train the submodule to help a main neural model from the supervision signal of the target task. In our setting, the main neural model plays a role in retrieval and is directly trained with ground-truth objects (annotated dependency edges). Thus, our findings and insights are orthogonal to theirs.",
            "score": 0.3946961894953656,
            "section_title": "Instance-Based Neural Methods in NLP",
            "char_start_offset": 7827,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 79,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 516
                },
                {
                    "start": 519,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 997
                }
            ],
            "ref_mentions": [
                {
                    "start": 190,
                    "end": 215,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 430,
                    "end": 447,
                    "matchedPaperCorpusId": "8281592"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7060546875
        },
        {
            "corpus_id": "211003686",
            "title": "Teaching Machines to Converse",
            "text": "Language modeling is a task of predicting which word comes next given the preceding words, which an important concept in natural language processing. The concept of language modeling can be dated back to the epoch-making work (Shannon, 1951) of Claude Shannon, who considered the case in which a string of input symbols is considered one by one, and the uncertainty of the next is measured by counting how difficult it is to guess. 1 More formally, language modeling defines the probability of a sequence (of words) by individually predicting each word within the sequence given all the preceding words (or history): y = y 1 , y 2 , ...., y N , where y t denotes the word token at position t and N denotes the number of tokens in y p(y) = t=N t=1 p(y t |y 1 , y 2 , ..., y t\u22121 )\n\n(2.1)\n\nwhere p(y t |y 1 , y 2 , ..., y t\u22121 ) denotes the conditional probability of seeing word y t given that all its preceding words, i.e., y 1 , y 2 , ..., y t\u22121 . n-gram language models have been widely used, which approximate the history with n \u2212 1 preceding words. The conditional probability is then estimated from relative frequency counts: count the number of times that we see y t\u2212n+1 , ..., y t\u22121 and count the number of times it is followed by y t :\n\np(y t |y 1 , y 2 , ..., y t\u22121 ) = C(y t\u2212n+1 , ..., y t\u22121 , y t ) C(y t\u2212n+1 , ..., y t\u22121 ) (2.2)\n\nA variation of algorithmic variations (e.g., smoothing techniques, model compressing techniques) have been proposed (Kneser and Ney, 1995;Rosenfeld, 2000;Stolcke et al., 2002;Teh, 2006;Federico et al., 2008;Federico, 1996;Chen and Goodman, 1996;Bacchiani et al., 2004;Brants et al., 2007;Church et al., 2007). N-gram language modeling comes with the merit of easy and",
            "score": 0.3946961894953656,
            "section_title": "Language Modeling",
            "char_start_offset": 34300,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 226,
                    "end": 241,
                    "matchedPaperCorpusId": "9101213"
                },
                {
                    "start": 1456,
                    "end": 1478,
                    "matchedPaperCorpusId": "9685476"
                },
                {
                    "start": 1478,
                    "end": 1494,
                    "matchedPaperCorpusId": "10959945"
                },
                {
                    "start": 1494,
                    "end": 1515,
                    "matchedPaperCorpusId": "1988103"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.34912109375
        },
        {
            "corpus_id": "231637215",
            "title": "Evaluating models of robust word recognition with serial reproduction",
            "text": "large corpora or other large-scale datasets that are strongly predictive of human linguistic behavior, such as response times in word naming tasks. Subsequent work in psycholinguistics and cognitive science has demonstrated the utility of a variety of PGLMs in understanding human sentence processing, especially for reading (Demberg & Keller, 2008Fine, Jaeger, Farmer, & Qian, 2013;Fossum & Levy, 2012;Frank & Bod, 2011;Futrell & Levy, 2017;Goodkind & Bicknell, 2018;Smith & Levy, 2011. However, the question of which models best capture human linguistic expectations-using which information sources and via which representations-remains a central open question.\n\nIn addition to the utility of increasingly accurate statistical models of language structure for psychology and cognitive science, these same models are of critical importance for a broad range of speech-related engineering applications, where noise, ambiguity, and speaker variation have been well-known challenges since the 1940s (e.g., Shannon, 1948).\n\nGenerative models of utterance structure have received extensive treatment in the fields of Computational Linguistics, Natural Language Processing, and Automatic Speech\n\nRecognition, where such models are collectively known as \"language models\" (Jurafsky & Martin, 2009). Their probabilistic form allows them to be combined in principled ways with auditory information using Bayesian techniques, or approximations thereof (e.g., Graves, Fern\u00e1ndez, Gomez, & Schmidhuber, 2006). A growing interest in commercial applications in recent years has resulted in a profusion of model architectures, especially ones taking advantage of large-scale deep artificial neural networks (e.g., Devlin, Chang, Lee, & Toutanova, 2018;Dyer, Kuncoro, Ballesteros, & Smith, 2016;Gulordava, Bojanowski, Grave, Linzen, & Baroni, 2018;Hannun et al., 2014;Jozefowicz, Zaremba, & Sutskever, 2015;Peters et al., 2018;Zaremba, Sutskever, & Vinyals, 2014). These neural network models have received significant attention in their ability to capture human linguistic phenomena above and beyond simpler structural models, though the exact nature of such remains an active area of investigation (Futrell",
            "score": 0.3946961894953656,
            "section_title": "Theoretical Background",
            "char_start_offset": 6202,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 325,
                    "end": 348,
                    "matchedPaperCorpusId": "1444973"
                },
                {
                    "start": 348,
                    "end": 383,
                    "matchedPaperCorpusId": "16080178"
                },
                {
                    "start": 383,
                    "end": 403,
                    "matchedPaperCorpusId": "3932763"
                },
                {
                    "start": 403,
                    "end": 421,
                    "matchedPaperCorpusId": "952654"
                },
                {
                    "start": 442,
                    "end": 468,
                    "matchedPaperCorpusId": "30129590"
                },
                {
                    "start": 468,
                    "end": 486,
                    "matchedPaperCorpusId": "6850948"
                },
                {
                    "start": 1004,
                    "end": 1018,
                    "matchedPaperCorpusId": "5747983"
                },
                {
                    "start": 1737,
                    "end": 1779,
                    "matchedPaperCorpusId": "1949831"
                },
                {
                    "start": 1779,
                    "end": 1832,
                    "matchedPaperCorpusId": "4460159"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6123046875
        },
        {
            "corpus_id": "274140975",
            "title": "Mechanism and Emergence of Stacked Attention Heads in Multi-Layer Transformers",
            "text": "Below I provide one complete example for each of the retrieval and conditional retrieval formulations used in the paper. The examples are generated using the same programs used for benchmarking the large language models. Each example consists of a prompt, a correct answer, and acceptable answers. Acceptable answers are used to filter out incoherent answers by repeatedly sampling from the model until an acceptable answer is found. Bob l i v e s i n Amsterdam Henry l i v e s w i t h David Eve l i v e s w i t h C h a r l i e F r a n k l i v e s w i t h A l i c e G r a c e l i v e s w i t h Bob K a t e l i v e s w i t h G r a c e L a r r y l i v e s w i t h F r a n k J a c k l i v e s w i t h Eve I s a b e l l e l i v e s w i t h Henry Mary l i v e s w i t h J a c k O l i v i a l i v e s w i t h I s a b e l l e Nick l i v e s w i t h K a t e P e t e r l i v e s w i t h L a r r y Rose l i v e s w i t h P e t e r Queen l i v e s w i t h Nick Tom l i v e s w i t h O l i v i a Sam l i v e s w i t h Mary",
            "score": 0.3946961894953656,
            "section_title": "A RETRIEVAL AND CONDITIONAL RETRIEVAL PROMPTS",
            "char_start_offset": 23068,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 1010
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.072265625
        },
        {
            "corpus_id": "1967279",
            "title": "Bayesian Unsupervised Topic Segmentation",
            "text": "The Dirichlet compound multinomial integrates over language models, but we must still set the prior \u03b8 0 . We can re-estimate this prior based on the observed data by interleaving gradient-based search in a Viterbi expectation-maximization framework (Gauvain and Lee, 1994). In the E-step, we estimate a segmentation\u1e91 of the dataset, as described in Section 3.3. In the M-step, we maximize p(\u03b8 0 |X,\u1e91) \u221d p(X|\u03b8 0 ,\u1e91)p(\u03b8 0 ). Assuming a non-informative hyperprior p(\u03b8 0 ), we maximize the likelihood in Equation 7 across all documents. The maximization is performed using a gradient-based search; the gradients are dervied by Minka (2003). This procedure is iterated until convergence or a maximum of twenty iterations.",
            "score": 0.3946961894953656,
            "section_title": "Priors",
            "char_start_offset": 15190,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 249,
                    "end": 272,
                    "matchedPaperCorpusId": "2814093"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08038330078125
        },
        {
            "corpus_id": "273653791",
            "title": "AutoMIR: Effective Zero-Shot Medical Information Retrieval without Relevance Labels",
            "text": "Baseline Models To comprehensively evaluate CMIRB, we compare our model against several popular retrieval models. These include lexical retriever BM25 [28]; dense retrieval models such as Text2Vec-Large-Chinese 7 , PEG [39], BGE-Large-zh-v1.5 [40], GTE-Large-zh [15], and Piccolo-Large-zh 8 ; multilingual retrievers like mContriever (masmarco) [10], M3E-Large 9 , mE5 (multilingual-e5large) [35]; and OpenAI's retriever, text-embedding-ada-002 10 . \n\n7 https://github.com/chmlyrical/text2vec-large-chinese 8 https://github.com/timczm/piccolo-large-zh 9 https://github.com/wangyuxinwhy/uniem 10 https://platform.openai.com/docs/guides/embeddings",
            "score": 0.3946961894953656,
            "section_title": "Data Construction",
            "char_start_offset": 28425,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 449
                },
                {
                    "start": 452,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 645
                }
            ],
            "ref_mentions": [
                {
                    "start": 151,
                    "end": 155,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 243,
                    "end": 247,
                    "matchedPaperCorpusId": "271114619"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8251953125
        },
        {
            "corpus_id": "262054264",
            "title": "Enhancing Open-Domain Table Question Answering via Syntax- and Structure-aware Dense Retrieval",
            "text": "As shown in Figure 2, our proposed retrieval model comprises of 1) Syntactical representation module that generates fine-grained syntactical representations for the question; 2) Structural representation module that generates a limited number of structural representations for a table; 3) Syntactical-tostructural aggregator that produces the matching score between the question and a candidate table.",
            "score": 0.3946961894953656,
            "section_title": "Overview",
            "char_start_offset": 3520,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 401
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.458984375
        },
        {
            "corpus_id": "918796",
            "title": "Generalized Expectation Criteria for Bootstrapping Extractors using Record-Text Alignment",
            "text": "learn an accurate language model. The second approach (Sarawagi and Cohen, 2004;Michelson and Knoblock, 2005;Mansuri and Sarawagi, 2006) uses database or dictionary lookups in combination with similarity measures to add features to the text sequence. Although these features are very informative, learning algorithms still require annotated data to make use of them. The final approach heuristically labels texts using matching records and learns extractors from these annotations (Ramakrishnan and Mukherjee, 2004;Bellare and McCallum, 2007;Michelson and Knoblock, 2008). Heuris-tic labeling decisions, however, are made independently without regard for the Markov dependencies among labels in text and are sensitive to subtle changes in text.\n\nHere we propose a method that automatically induces a labeling of an input text sequence using a word alignment with a matching database record. This induced labeling is then used to train a text extractor. Our approach has several advantages over previous methods. First, we are able to model field ordering and context around fields by learning an extractor from annotations of the text itself. Second, a probabilistic model for word alignment can exploit dependencies among alignments, and is also robust to errors, formatting differences, and missing fields in text and the record. Our word alignment model is a conditional random field (CRF) (Lafferty et al., 2001) that generates alignments between tokens of a text sequence and a matching database record. The structure of the graphical model resembles IBM Model 1 (Brown et al., 1993) in which each target (record) word is assigned one or more source (text) words. The alignment is generated conditioned on both the record and text sequence, and therefore supports large sets of rich and nonindependent features of the sequence pairs. Our model is trained without the need for labeled word alignments by using generalized expectation (GE) criteria (Mann and McCallum, 2008) that penalize the divergence of specific model expectations from target expectations. Model parameters are estimated by minimizing this divergence. To limit over-fitting we include a L 2 -regularization term in the objective. The model expectations in GE criteria are taken with respect to a set of alignment latent variables that are either specific to each sequence",
            "score": 0.3946961894953656,
            "section_title": "Introduction",
            "char_start_offset": 1998,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 54,
                    "end": 80,
                    "matchedPaperCorpusId": "4108765"
                },
                {
                    "start": 80,
                    "end": 109,
                    "matchedPaperCorpusId": "10109645"
                },
                {
                    "start": 109,
                    "end": 136,
                    "matchedPaperCorpusId": "13111836"
                },
                {
                    "start": 481,
                    "end": 515,
                    "matchedPaperCorpusId": "45822290"
                },
                {
                    "start": 515,
                    "end": 542,
                    "matchedPaperCorpusId": "8597804"
                },
                {
                    "start": 542,
                    "end": 571,
                    "matchedPaperCorpusId": "1408190"
                },
                {
                    "start": 1393,
                    "end": 1415,
                    "matchedPaperCorpusId": "219683473"
                },
                {
                    "start": 1568,
                    "end": 1588,
                    "matchedPaperCorpusId": "13259913"
                },
                {
                    "start": 1952,
                    "end": 1976,
                    "matchedPaperCorpusId": "9976345"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7333984375
        },
        {
            "corpus_id": "259991664",
            "title": "Embroid: Unsupervised Prediction Smoothing Can Improve Few-Shot Classification",
            "text": "For convenience, we denote \u03bb LLM (\u03c6 i (x)) as \u03bb i (x) or \u03bb i when the x is obvious, and similarly use \u03bb instead of \u03bb(x). We distinguish between two regimes: in the single-prompt regime with m = 1, we have access to a LM prediction for each point x, while in the multi-prompt regime with m > 1, we have access to multiple predictions. \n\nWe assume access to N embedding models E = [E 1 , . . . , E N ], each represented as a fixed mapping E i : X \u2192 Z i from an input x to an embedding vector z. These auxiliary embedding models provide representations of x which encode different types of similarity information. Through model repositories like HuggingFace [62], it is possible to download a number of models which generate representations for text sentences (e.g., BERT or RoBERTa [12,41]). These models have the property that semantically similar sentences are close-by in embedding space [7,27,39]. \n\nWeak supervision background Weak supervision uses a graphical model to combine votes from multiple noisy sources into a single prediction, by estimating the accuracy of each source. It models Pr(y, \u03bb(x)) as a latent variable graphical model and uses \u0177 = argmax y Pr(y|\u03bb(x)) to produce label estimates, where Pr represents the learned model. The graphical model is based on a graph G = (V, E), where V = y \u222a \u03bb and E consists of edges from y to each \u03bb j . We assume no dependencies between sources, although simple extensions can incorporate them [59]. The formal graphical model \n\nwhere Z is the partition function used for normalization, (I) represents a label balance term with parameter \u03b8 y controlling the prior of Pr(y = 1), and (II) represents the source accuracy term where each \u03b8 i is an accuracy parameter for the ith source. Note that from this model, sources are conditionally independent: \u03bb i \u22a5 \u22a5 \u03bb j |y for any i, j \u2208 [m]. Our use of this model has two steps.",
            "score": 0.3946961894953656,
            "section_title": "Problem setup and background",
            "char_start_offset": 8338,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 333
                },
                {
                    "start": 336,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 789
                },
                {
                    "start": 790,
                    "end": 899
                },
                {
                    "start": 902,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1242
                },
                {
                    "start": 1243,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1452
                },
                {
                    "start": 1453,
                    "end": 1479
                },
                {
                    "start": 1482,
                    "end": 1735
                },
                {
                    "start": 1736,
                    "end": 1836
                },
                {
                    "start": 1837,
                    "end": 1873
                }
            ],
            "ref_mentions": [
                {
                    "start": 1447,
                    "end": 1451,
                    "matchedPaperCorpusId": "76663579"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06939697265625
        },
        {
            "corpus_id": "254877165",
            "title": "DimonGen: Diversified Generative Commonsense Reasoning for Explaining Concept Relationships",
            "text": "To better understand the relationships between given concepts, we introduce a retrieval stage to gather context sentences from external corpora C. Given an input concept pair x = {e a , e b }, we aim to retrieve several diversified sets of relational contexts {S 1 , . . . , S n }, where S i = {s i 1 , . . . , s i k } is a set of context sentences containing x.\n\nWe train the retriever models on a binary classification task. Given a candidate sentence from external knowledge corpora s j \u2208 C, we concatenate it with the input x and use it as input:\n\nwhere [CLS] and [SEP] are special tokens in pretrained language models. The model's task is to predict a label y c from [0, 1], indicating the confidence of the candidate sentence being a true relational context for the input concepts. For each input, we use its target output sentences in the dataset as positive examples and randomly sample the same number of negative examples from its retrieved candidate sentences.\n\nTo extract diversified contexts for each input concept pair, we introduce the mixture of experts (MoE) method into the retriever model. Since independently parameterizing each expert may cause an overfitting problem, we follow the weight-sharing schema in Shen et al. (2019) with a unique identifier to solve this issue. To make the MoE models more easily understood by pre-trained language models, for each expert model, we design its unique identifier as latent variables z i = z i 1 , . . . , z i m which is a randomly sampled prefix token sequence in the model vocabulary. Once an expert is chosen, we could train the model by concatenating the latent variable and input concepts with contexts as the final input:\n\nWe apply the hard-EM algorithm (Shen et al., 2019;Yu et al., 2022) to train our mixture of retrievers model. For each iteration, at E-step, we assign the expert model to each input; at M-step, we update all the expert models with the assigned inputs. With this process, the total training loss turns into an expectation form:\n\nHowever, during experiments, we find the binary classification problem has obvious patterns, and simply applying the hard-EM algorithm may lead to a severe overfitting problem",
            "score": 0.3946961894953656,
            "section_title": "Retrieval Stage",
            "char_start_offset": 8858,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1229,
                    "end": 1247,
                    "matchedPaperCorpusId": "67787922"
                },
                {
                    "start": 1723,
                    "end": 1742,
                    "matchedPaperCorpusId": "67787922"
                },
                {
                    "start": 1742,
                    "end": 1758,
                    "matchedPaperCorpusId": "247447167"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56201171875
        },
        {
            "corpus_id": "267094971",
            "title": "DsDm: Model-Aware Dataset Selection with Datamodels",
            "text": "We consider four separate LM target tasks: standard language modeling tasks: LAMBADA (an open-ended cloze task measuring language understanding (Paperno et al., 2016)), CS-Algorithms (an algorithmic problem solving benchmark containing tasks like longest common subsequence identification (Srivastava et al., 2022)), SQuAD (the Stanford Question-Answering Dataset, a reading comprehension dataset of questions about Wikipedia articles (Rajpurkar et al., 2016)), and Jeopardy (trivia questions from the \"Jeopardy!\" game show (MosaicML, 2023)). We consider 0-shot (LAMBADA and CS-Algorithms) and 3-shot (SQuAD and Jeopardy) settings. For more details on these target tasks, including samples and target/holdout splits for evaluating, see Appendix A.2. \n\nWe train models according to the training procedure described in Section A.4, with the hyperparameters described in the \"Section 3\" rows of Table 2. See Appendix A.3 for baseline details, and Appendix A.1 for candidate dataset details.",
            "score": 0.3946961894953656,
            "section_title": "C.1. Experimental setup",
            "char_start_offset": 64988,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 749
                },
                {
                    "start": 752,
                    "end": 987
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.181884765625
        },
        {
            "corpus_id": "271097983",
            "title": "FsPONER: Few-shot Prompt Optimization for Named Entity Recognition in Domain-specific Scenarios",
            "text": "Language modeling [45] stands out as a core technique to advance language intelligence of machines and has received extensive attention over the past decade. The story begins with statistical language models [14], which follow the Markov assumption and predict the next word based on the most recent context. The rise of deep neural architectures opens the stage for neural language models. As a milestone, Bengio et al. [2] introduced the concept of distributed word representations and crafted word prediction functions on aggregated context features. Furthermore, Collobert et al. proposed a unified multi-layer neural architecture by discovering the internal representations of unlabeled datasets [7], while Mikolov et al. [25,24] proposed word2vec, a simplified shallow neural network designed for learning word representations effectively. These studies have initiated the utilization of language models for representation learning, elevating word sequence modeling to a more advanced level. Vaswani et al. [33] proposed the transformer architecture and attention mechanism, which started the generation of pre-trained language models. In alignment with this highly parallelizable architecture, language models were trained to learn context-aware word representations. BERT, a model proposed by Devlin et al. [9] and pretrained on large-scale unlabeled corpora bidirectionally, stands out. The semantic representations obtained in pre-training make BERT approachable for a broad spectrum of downstream tasks in specific domains. Inspired by such \"pre-training\" and \"fine-tuning\" modes, a substantial quantity of follow-up works have been developed over time, e.g. RoBERTa [21] and DistilBERT [30]. \n\nThe research community continues to enhance the performance of language models by scaling up their sizes. Compared with their smaller counterparts, large-scale models demonstrate unseen emergent abilities [36,22] in solving complex tasks, which have provided us with more information-based problem-solving possibilities, such as in-context learning, prompt engineering, step-by-step reasoning and retrieval augmented generation. These techniques have enlightened this work, leveraging few-shot prompting to solve domainspecific NER tasks.",
            "score": 0.3946961894953656,
            "section_title": "Development of Language Models",
            "char_start_offset": 2654,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1534
                },
                {
                    "start": 1535,
                    "end": 1669
                },
                {
                    "start": 1670,
                    "end": 1703
                },
                {
                    "start": 1706,
                    "end": 1811
                },
                {
                    "start": 1812,
                    "end": 2134
                },
                {
                    "start": 2135,
                    "end": 2244
                }
            ],
            "ref_mentions": [
                {
                    "start": 701,
                    "end": 704,
                    "matchedPaperCorpusId": "351666"
                },
                {
                    "start": 727,
                    "end": 731,
                    "matchedPaperCorpusId": "16447573"
                },
                {
                    "start": 1013,
                    "end": 1017,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8564453125
        },
        {
            "corpus_id": "8102576",
            "title": "Beyond Captions: Linking Figures with Abstract Sentences in Biomedical Articles",
            "text": "We model text properties of linked and non-linked instances using probabilistic language models (LM). Our LM approach is motivated by the successful application of similar methods to document retrieval [27][28][29]. For document retrieval the LM approach induces for each document a probability model over all terms in the vocabulary. Then, a document's relevance to a query is defined as the probability of the query under its model. \n\nHiemstra's LM approach [28] uses two kinds of term distributions: a single background distribution b b shared by all documents, and a set of document-specific distributions In a similar way we use language models to predict links between abstract sentences and figures by treating abstract sentences as queries and figures as documents. Let b b(t) and d d k (t) be the probability of term t under the background distribution and figure k's distribution respectively. Then, the probability of abstract sentence j given that it is linked to figure k is. \n\nwhere T T j is sentence j's length V term vector, 0\u0192l\u01921 is the mixing proportion for the background distribution, and j<k denotes that j and k are linked, or equivalently that L(j,k)~1. If j and k are not linked, the background distribution generates all terms in the sentence: \n\nThe LM score matrix S LM for an article holds the log-odds of the sentence terms given linkage for all instances (j,k). For an article with J sentences and K figures, S LM is J-by-K and. \n\nNote that we do not use pseudo-counts here, as smoothing is unnecessary because the background distribution b b is used for terms that have zero probability in the figure-specific model. \n\nAlthough the Pooled method has the advantage of including text from multiple sources, it is limited in that it ignores term origin even though there may be meaningful differences between terms in captions and referencing paragraphs. For example, while text in referencing paragraphs can discuss topics unrelated to the figure, caption content nearly always relates to the figure. Our final figure-specific term distribution, which we call Mixture, distinguishes between caption terms and referencing paragraph terms.",
            "score": 0.3946961894953656,
            "section_title": "Language Models",
            "char_start_offset": 36356,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 434
                },
                {
                    "start": 437,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 988
                },
                {
                    "start": 991,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1268
                },
                {
                    "start": 1271,
                    "end": 1390
                },
                {
                    "start": 1391,
                    "end": 1457
                },
                {
                    "start": 1460,
                    "end": 1646
                },
                {
                    "start": 1649,
                    "end": 1881
                },
                {
                    "start": 1882,
                    "end": 2028
                },
                {
                    "start": 2029,
                    "end": 2165
                }
            ],
            "ref_mentions": [
                {
                    "start": 202,
                    "end": 206,
                    "matchedPaperCorpusId": "61572040"
                },
                {
                    "start": 210,
                    "end": 214,
                    "matchedPaperCorpusId": "2759772"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73486328125
        },
        {
            "corpus_id": "275358357",
            "title": "Knowledge Retrieval Based on Generative AI",
            "text": "Retrieval-Augmented Generation (RAG) [4] overcomes the limitations of Large Language Models (LLMs), which rely on static, pre-trained datasets that can become outdated and lack domain-specific information. This restricts LLMs' ability to generate accurate and up-to-date responses. RAG integrates Information Retrieval (IR) systems with LLMs, enabling them to query external knowledge sources and access real-time, domain-relevant data. In a typical RAG framework, a retriever processes user queries and retrieves relevant documents based on semantic similarity. These documents are then combined with the original query and passed to the LLM to generate a more accurate and comprehensive response. RAG also addresses the issue of model obsolescence by dynamically accessing updated information without retraining. However, balancing precision and recall during retrieval can be challenging, and re-ranking is required to prioritize relevant information, ensuring that LLM responses are accurate and contextually appropriate in complex queries.",
            "score": 0.3946961894953656,
            "section_title": "C. Retrieval-Augmented Generation",
            "char_start_offset": 3961,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 1044
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 40,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91845703125
        },
        {
            "corpus_id": "253510695",
            "title": "Replacing Language Model for Style Transfer",
            "text": "Language Modeling: Language Modeling (LM) is a fundamental task of natural language processing (NLP). Given a natural language sentence X as a length-n sequence of words (x 0 , x 1 , . . . , x n\u22121 ), LM aims to learn the probability of the occurrence of X, i.e., P (X) = P (x 0 , x 1 , . . . , x n\u22121 ). A well-learned language model can be useful in numerous NLP applications, such as text representation learning (Kenton and Toutanova, 2019;Liu et al., 2019b), text generation (Radford et al., 2019;Brown et al., 2020), and model performance evaluation (Brown et al., 1992;Huang et al., 2021). To learn the probability P (X), a commonly used calculation paradigms (Bengio et al., 2000) is: \n\nwhere sub-sequence X i:j = (x i , x i+1 , . . . , x j\u22121 ) if i < j, and X i:j = \u00d8 if i \u2265 j. With the decomposition in equation 1, the model iteratively provides the probability of next-token prediction x i , conditioned on its prefix X 0:i . The calculation scheme in equation 1 is named the autoregressive (AR) language modeling (Brown et al., 2020). \n\nAlthough widely applied in NLP scenarios, AR models have been growingly challenged at their low-efficiency and accumulation errors (Qi et al., 2021;Arora et al., 2022). Hence, many nonautoregressive (NAR) modeling frameworks are recently proposed (Kenton and Toutanova, 2019;Shen et al., 2020). Generally, NAR models have the following form: \n\nwhere X I = {x i |i \u2208 I} and X\u012a = {x i |i / \u2208 I} are selected from index set I and its complement \u012a respectively.",
            "score": 0.3946961894953656,
            "section_title": "BACKGROUND",
            "char_start_offset": 4454,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 690
                },
                {
                    "start": 693,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1044
                },
                {
                    "start": 1047,
                    "end": 1215
                },
                {
                    "start": 1216,
                    "end": 1341
                },
                {
                    "start": 1342,
                    "end": 1388
                },
                {
                    "start": 1391,
                    "end": 1504
                }
            ],
            "ref_mentions": [
                {
                    "start": 478,
                    "end": 500,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 500,
                    "end": 519,
                    "matchedPaperCorpusId": "18511291"
                },
                {
                    "start": 554,
                    "end": 574,
                    "matchedPaperCorpusId": "18511291"
                },
                {
                    "start": 665,
                    "end": 686,
                    "matchedPaperCorpusId": "221275765"
                },
                {
                    "start": 1023,
                    "end": 1043,
                    "matchedPaperCorpusId": "18511291"
                },
                {
                    "start": 1178,
                    "end": 1195,
                    "matchedPaperCorpusId": "229923123"
                },
                {
                    "start": 1195,
                    "end": 1214,
                    "matchedPaperCorpusId": "247939224"
                },
                {
                    "start": 1322,
                    "end": 1340,
                    "matchedPaperCorpusId": "211069634"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61865234375
        },
        {
            "corpus_id": "273969988",
            "title": "Query Optimization for Parametric Knowledge Refinement in Retrieval-Augmented Large Language Models",
            "text": "We introduce the Extract-Refine-Retrieve-Read (ERRR) framework, a novel approach designed to bridge the pre-retrieval information gap in Retrieval-Augmented Generation (RAG) systems through query optimization tailored to meet the specific knowledge requirements of Large Language Models (LLMs). Unlike conventional query optimization techniques used in RAG, the ERRR framework begins by extracting parametric knowledge from LLMs, followed by using a specialized query optimizer for refining these queries. This process ensures the retrieval of only the most pertinent information essential for generating accurate responses. Moreover, to enhance flexibility and reduce computational costs, we propose a trainable scheme for our pipeline that utilizes a smaller, tunable model as the query optimizer, which is refined through knowledge distillation from a larger teacher model. Our evaluations on various question-answering (QA) datasets and with different retrieval systems show that ERRR consistently outperforms existing baselines, proving to be a versatile and cost-effective module for improving the utility and accuracy of RAG systems.",
            "score": 0.39459413804940635,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72119140625
        },
        {
            "corpus_id": "274140975",
            "title": "Mechanism and Emergence of Stacked Attention Heads in Multi-Layer Transformers",
            "text": "The retrieval problem is implicitly present as a subproblem in many common language tasks such as working with relations between persons, tracking the evolution of a concept, solving mathematical and reasoning problems, programming, and many more. Consider the following real-world example from the Wikipedia article on llamas: \n\n\"Llamas are social animals and live with others as a herd. [ . . . ] A cria (from Spanish for 'baby') is the name for a baby llama, alpaca, vicu\u00f1a, or guanaco. Crias are typically born with all the females of the herd gathering around.\" \n\nAn autoregressive language model trying to predict the second occurrence of the word herd (rather than flock, group, or pack) would need to first retrieve the fact that crias are llamas, and then use it to retrieve the fact that llamas live in herds. This process is essentially a retrieval problem with D = 2. \n\nFrom one point of view, the retrieval problem is essentially about working with relations between entities, which is fundamental for language and reasoning. This makes it an ideal testbed for studying the inner workings of large language models.",
            "score": 0.39426776877464076,
            "section_title": "MOTIVATION",
            "char_start_offset": 4101,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 327
                },
                {
                    "start": 330,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 566
                },
                {
                    "start": 569,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 879
                },
                {
                    "start": 882,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1127
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28076171875
        },
        {
            "corpus_id": "275337144",
            "title": "Interactive Information Need Prediction with Intent and Context",
            "text": "To implicitly predict the information need by retrieving an answering passage, we fine-tuned both bi-encoder and cross-encoder language models. We choose these methods because they are commonly used in the retrieval literature, with bi-encoders often used in the initial retrieval stage [25], and cross-encoders often used in the reranking stage [35]. We based both approaches on BERT [11], an encoder-only pre-trained transformer model. We chose this model due to its generality in various applications. To assess performance, we measured the recall at 10 (R@10) and the mean reciprocal rank (MRR) performance on the validation queries. We selected these two metrics due to their common use in information retrieval and their interpretability for use in web retrieval systems (e.g., such as search engines) [3]. We used the MRR at 10 for testing statistical significance. The validation target paragraphs and the training target paragraphs were used as candidates for retrieval. This amounted to 956 validation samples over 8,873 candidate target paragraphs. \n\nFor the bi-encoder model, we used bert-base-uncased, which is publicly available from Hugging Face [13]. We trained the model to maximize the cosine similarity between positive training samples and minimize the cosine similarity between negative samples via the cosine embedding loss [30]. In all of our experiments, we set the margin to 0.5 and set  1 and  2 to be the CLS tokens from the source and target of each sample, respectively. The label indicates if the sample was positive or negative. Moreover, we used a batch size of 16, 50 warm up steps, and stopped training when the validation loss increased by more than 5% of the best-performing model epoch. We fixed the number of negative samples to 10, varied the number of positive samples (when we had a source paragraph) over {1,2}, and varied the learning rate over {5e-5, 1e-5, 5e-6}. Each training run took approximately four hours. \n\nFor the cross-encoder model, we again used Hugging Face's bertbase-uncased but with a classification head.",
            "score": 0.393541376840174,
            "section_title": "Retrieval",
            "char_start_offset": 20476,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 812
                },
                {
                    "start": 813,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1059
                },
                {
                    "start": 1062,
                    "end": 1166
                },
                {
                    "start": 1167,
                    "end": 1351
                },
                {
                    "start": 1352,
                    "end": 1499
                },
                {
                    "start": 1500,
                    "end": 1559
                },
                {
                    "start": 1560,
                    "end": 1723
                },
                {
                    "start": 1724,
                    "end": 1907
                },
                {
                    "start": 1908,
                    "end": 1956
                },
                {
                    "start": 1959,
                    "end": 2065
                }
            ],
            "ref_mentions": [
                {
                    "start": 346,
                    "end": 350,
                    "matchedPaperCorpusId": "201646309"
                },
                {
                    "start": 808,
                    "end": 811,
                    "matchedPaperCorpusId": "16849904"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97509765625
        },
        {
            "corpus_id": "259203489",
            "title": "GLIMMER: generalized late-interaction memory reranker",
            "text": "In particular, our work is very closely based on LUMEN (de Jong et al., 2023). \n\nReranking Like the language model itself, retrieval procedures face a trade-off between expensive ranking with full interaction (Chen et al., 2020) and the more common dual encoder approaches such as DPR (Karpukhin et al., 2020) and GTR (Ni et al., 2021) that scores based on inner product similarity with a corpus of pre-computed passage representations. Often different models for retrieval are applied in a pipeline approach, with an initial cheap scoring model followed by a more powerful and expensive reranker (Mao et al., 2021;Wang et al., 2018;Yu et al., 2022). Many rerankers also make use of late interaction to obtain a good trade-off between ranking quality and speed, such as COLBERT (Khattab and Zaharia, 2020;Santhanam et al., 2022), PreTTR (MacAvaney et al., 2020), SDR (Cohen et al., 2022), and Poly-encoders (Humeau et al., 2020). GLIMMER combines late-interaction memory and reranking into a single model, sharing the pre-computed representations for both use cases. \n\nLearning to retrieve Retrieval models are often trained with supervised data (Karpukhin et al., 2020;Ni et al., 2021), using gold retrievals from datasets such as MS-MARCO (Nguyen et al., 2016) or TREC CAR (Dietz et al., 2018). When selecting passage to use for retrieval-augmented generation, we have an additional signal, namely which passages are most helpful for the reader model. A number of existing works use this signal to improve retrieval (Guu et al., 2020;Sachan et al., 2021;Jiang et al., 2022;Sachan et al., 2021;Izacard et al., 2022). We follow ATLAS (Izacard et al., 2022) and employ perplexity distillation to train our reranker to select passages that help lower reader model perplexity.",
            "score": 0.39351684386532026,
            "section_title": "Related Work",
            "char_start_offset": 19685,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 81,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 929
                },
                {
                    "start": 930,
                    "end": 1066
                },
                {
                    "start": 1069,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1453
                },
                {
                    "start": 1454,
                    "end": 1617
                },
                {
                    "start": 1618,
                    "end": 1773
                }
            ],
            "ref_mentions": [
                {
                    "start": 209,
                    "end": 228,
                    "matchedPaperCorpusId": "228093856"
                },
                {
                    "start": 285,
                    "end": 309,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 597,
                    "end": 615,
                    "matchedPaperCorpusId": "230435683"
                },
                {
                    "start": 633,
                    "end": 649,
                    "matchedPaperCorpusId": "238583601"
                },
                {
                    "start": 778,
                    "end": 805,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 805,
                    "end": 827,
                    "matchedPaperCorpusId": "244799249"
                },
                {
                    "start": 867,
                    "end": 887,
                    "matchedPaperCorpusId": "238353991"
                },
                {
                    "start": 907,
                    "end": 928,
                    "matchedPaperCorpusId": "210063976"
                },
                {
                    "start": 1146,
                    "end": 1170,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1241,
                    "end": 1262,
                    "matchedPaperCorpusId": "1289517"
                },
                {
                    "start": 1275,
                    "end": 1295,
                    "matchedPaperCorpusId": "4987800"
                },
                {
                    "start": 1536,
                    "end": 1556,
                    "matchedPaperCorpusId": "235390519"
                },
                {
                    "start": 1556,
                    "end": 1575,
                    "matchedPaperCorpusId": "254246471"
                },
                {
                    "start": 1575,
                    "end": 1595,
                    "matchedPaperCorpusId": "235390519"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93701171875
        },
        {
            "corpus_id": "259342736",
            "title": "Open-Domain Hierarchical Event Schema Induction by Incremental Prompting and Verification",
            "text": "To make the model more informed of how events are typically depicted in news, we introduce a retrieval component to guide LLMs to focus on scenario-related passages. The key difficulty of schema induction is to generalize from multiple passages and reflect the \"stereotyped sequence of events\" instead of providing concrete and specific answers. We, therefore, retrieve multiple passages each time and ask the model to provide a generalized answer that is suitable for all passages.\n\nTo build a document collection containing typical events of the given scenario, we leverage its Wikipedia category page and retrieve the reference news articles of each Wikipedia article under the category, as detailed in Appendix A. With such a document collection, for each prompt, we are able to use the description of the event as the query and retrieve k = 3 passages based on state-of-theart document retrieval system TCT-ColBERT (Lin et al., 2021b). The input to the LM is structured as follows:",
            "score": 0.391988839836823,
            "section_title": "Retrieval-Augmented Prompting",
            "char_start_offset": 7544,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 920,
                    "end": 939,
                    "matchedPaperCorpusId": "235720578"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.102294921875
        },
        {
            "corpus_id": "269757433",
            "title": "Advanced Natural-based interaction for the ITAlian language: LLaMAntino-3-ANITA",
            "text": "ANITA can be used in a multitude of possible application scenarios.Here we present some of them, among those we consider most useful to the scientic community.For each, we have also provided an example script to be used for future work.\n\nRetrieval-Augmented Generation (RAG).Retrieval-Augmented Generation [18] is an innovative approach in the eld of Information Retrieval, providing a bridge between generative models and external knowledge bases.At its core, RAG enhances the capabilities of LLMs by enabling them to access and incorporate information from vast databases like Wikipedia or specialized corpora during the text generation process1.This integration allows LLMs to produce responses that are not only contextually relevant but also factually grounded, addressing one of the key limitations of traditional LLMs: the generation of plausible but incorrect or \"hallucinated\" information.The technical implementation of RAG involves a two-step process: retrieval and generation.In the retrieval step, the model uses the input prompt to query an external database and retrieve relevant documents or data snippets.These retrieved pieces of information are then fed into the generative component of the model, which synthesizes the external data with its pre-existing knowledge to generate a coherent and informed response.This process signicantly improves the accuracy and reliability of the model's outputs, as it can draw upon the most current and veried information available, making RAG particularly valuable for applications such as question-answering systems and chatbots where precision and trustworthiness are crucial.The model we propose can be used as the backbone of many of the most common RAG frameworks, such as Llamaindex 21 and LangChain 22 .The ability to use 8K as an input data size and the excellent capabilities in terms of understanding Italian language make ANITA suitable for a wide plethora of applications.An example of use in a RAG system is available here 23 .\n\nTopic Modeling.Topic Modeling [1] is a powerful unsupervised machine learning technique used to discover the hidden thematic structure in a large corpus of text.It involves identifying topics, which are recurring patterns of words, within a collection of documents.The goal is to uncover the latent semantic dimensions-topics that permeate the text but are not explicitly labeled.",
            "score": 0.3919442087124258,
            "section_title": "APPLICATION SCENARIOS",
            "char_start_offset": 24271,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 67
                },
                {
                    "start": 67,
                    "end": 159
                },
                {
                    "start": 159,
                    "end": 236
                },
                {
                    "start": 238,
                    "end": 275
                },
                {
                    "start": 275,
                    "end": 448
                },
                {
                    "start": 448,
                    "end": 648
                },
                {
                    "start": 648,
                    "end": 898
                },
                {
                    "start": 898,
                    "end": 988
                },
                {
                    "start": 988,
                    "end": 1122
                },
                {
                    "start": 1122,
                    "end": 1330
                },
                {
                    "start": 1330,
                    "end": 1634
                },
                {
                    "start": 1634,
                    "end": 1766
                },
                {
                    "start": 1766,
                    "end": 1940
                },
                {
                    "start": 1940,
                    "end": 1996
                },
                {
                    "start": 1998,
                    "end": 2013
                },
                {
                    "start": 2013,
                    "end": 2159
                },
                {
                    "start": 2159,
                    "end": 2263
                },
                {
                    "start": 2263,
                    "end": 2378
                }
            ],
            "ref_mentions": [
                {
                    "start": 306,
                    "end": 310,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 2028,
                    "end": 2031,
                    "matchedPaperCorpusId": "252986531"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.320556640625
        },
        {
            "corpus_id": "263134958",
            "title": "A Benchmark for Learning to Translate a New Language from One Grammar Book",
            "text": "We initially experimented with two types of retrieval: longest-common-substring (LCS) and embedding-based retrieval. For the embedding-based retrieval, we computed embeddings using a pretrained sentence transformer model (all-mpnet-base-v2). Note that this embedding model has been trained on English texts, so we also provide a longest-common substring method for comparison. \n\nFor the grammar book passages, retrieval based on longest-common-substring distance slightly outperformed retrieval in embedding space. We believe this may be the case because, although the grammar book is written primarily in English, it contains a large amount of Kalamang text, and this text is disproportionately important when selecting passages for retrieval. Since our embedding model (all-mpnet-base-v2 from the sentence-transformer library) is trained on English text, the Kalamang text is out-of-distribution for the model, which may result in the retrieval of suboptimal passages. By contrast, longest-common-substring should be fairly robust to the inclusion of Kalamang text. All results shown in the main text of the paper are for longest-common-substring. \n\nBoth types of retrieval are not particularly good at finding passages that describe grammatical features present in the given sentence, and there is no reason to expect them to be: they are only designed to find passages that are similar.",
            "score": 0.3918014270148568,
            "section_title": "D EMBEDDING-BASED RETRIEVAL",
            "char_start_offset": 47350,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 376
                },
                {
                    "start": 379,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1149
                },
                {
                    "start": 1152,
                    "end": 1390
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6787109375
        },
        {
            "corpus_id": "644854",
            "title": "A database approach to information retrieval: The remarkable relationship between language models and region models",
            "text": "As said in the introduction, language models form a general approach to define ranking formulas for retrieval applications. A language model is assigned to every document. The language model of the document defines the probability that the document 'generates' the query. Documents are ranked by this probability. The simplest language modeling approach to information retrieval would be defined by Equation 1. \n\nIt defines the probability of a query of length l given a document D as the product of the probabilities of each term T i (1 \u2264 i \u2264 l) given D. A language model that takes a simple product of terms, i.e., a model that assumes that the probability of one term given a document does not depend on other terms, is called a unigram language model. To make this work, we have to define the basic probability measure P (T |D); typically, it would be defined as the number of occurrences of the term T in the document D, divided by the total number of terms in the document D. For a practical query, say, retrieve all documents about \"db\" and \"ir\", we would instantiate Equation 1 as follows: \n\nThe right-hand side of the equation corresponds to the following region expression. \n\n(<doc> CONTAINING db) AND (<doc> CONTAINING ir) \n\nThis can be shown as follows: The region expression (<doc> CONTAINING db) produces all documents ranked according to P (T = db|D), i.e., all regions tagged as <doc>, ranked by the number of occurrences of db in those regions. Similarly, (<doc> CONTAINING ir) produces all documents ranked according to P (T = ir|D). Finally, the operator AND results in the regions tagged as <doc> that are in both operand sets. The score of the result regions is defined as the product of the scores of the same regions in the operands. Here, and in the remaining examples in this section, we assume that <doc> regions do not nest inside each other. We claim that there is a trivial way to rewrite the right-hand side of Equation 2 to Equation 3 while preserving the outcome. This can be shown by simply replacing P (x|y) by (y CONTAINING x), and the multiplication in Equation 2 by AND.",
            "score": 0.39151391669839825,
            "section_title": "The simplest unigram language model",
            "char_start_offset": 15667,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 410
                },
                {
                    "start": 413,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 1097
                },
                {
                    "start": 1100,
                    "end": 1183
                },
                {
                    "start": 1186,
                    "end": 1233
                },
                {
                    "start": 1236,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1647
                },
                {
                    "start": 1648,
                    "end": 1756
                },
                {
                    "start": 1757,
                    "end": 1869
                },
                {
                    "start": 1870,
                    "end": 1995
                },
                {
                    "start": 1996,
                    "end": 2107
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.931640625
        },
        {
            "corpus_id": "276781769",
            "title": "When Claims Evolve: Evaluating and Enhancing the Robustness of Embedding Models Against Misinformation Edits",
            "text": "To evaluate the robustness of various embedding models, we adopt a common multi-stage retrieval pipeline (Nogueira and Cho, 2019;Nogueira et al., 2019a;Ma et al., 2024) consisting of a computationally efficient retriever model to return top-j relevant fact-checks followed by a more computationally intensive reranker that refines the retrieved candidates to improve ranking quality. We describe the models we evaluate for each stage below: \n\nRetrievers In the retrieval stage, we evaluate embedding models with varying dimensions, pre-training approaches, and backbone architectures. \n\nThis includes: BERT-based embedding models (Devlin et al., 2019;Reimers and Gurevych, 2019): all-MiniLM-L12-v2, all-mpnet-base-v2, and all-distilroberta-v1. \n\nT5-based embedding models (Ni et al., 2022;Raffel et al., 2020): \n\nsentence-t5-base, sentence-t5-large, instructor-base, and instructor-large-instruction-finetuned embeddings based on GTR models (Su et al., 2023). Decoder-only LLM-based embeddings: SFR-Embedding-Mistral (Meng et al., 2024) and NV-Embed-v2 (Lee et al., 2025),5 both initialized from Mistral-7B. To study the effect of finetuning, we finetune all-mpnet-base-v2 and sentence-t5-large on the CheckThat22 training set, and denote the finetuned models respectively as all-mpnet-base-v2-ft and sentence-t5-large-ft. We also implement BM25 as a baseline to explore the robustness of lexical-based sparse retrieval approaches. We provide further details on embedding models and finetuning in \u00a7C of the Appendix.",
            "score": 0.3914826634733064,
            "section_title": "Multi-Stage Retrieval Pipeline",
            "char_start_offset": 15261,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 440
                },
                {
                    "start": 443,
                    "end": 584
                },
                {
                    "start": 587,
                    "end": 743
                },
                {
                    "start": 746,
                    "end": 810
                },
                {
                    "start": 813,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1107
                },
                {
                    "start": 1108,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1516
                }
            ],
            "ref_mentions": [
                {
                    "start": 152,
                    "end": 168,
                    "matchedPaperCorpusId": "263908865"
                },
                {
                    "start": 772,
                    "end": 789,
                    "matchedPaperCorpusId": "237260023"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68212890625
        },
        {
            "corpus_id": "274140975",
            "title": "Mechanism and Emergence of Stacked Attention Heads in Multi-Layer Transformers",
            "text": "In this paper, I introduce the retrieval problem, a simple yet common reasoning task that can be solved only by transformers with a minimum number of layers, which grows logarithmically with the input size. I empirically show that large language models can solve the task under different prompting formulations without any fine-tuning. To understand how transformers solve the retrieval problem, I train several transformers on a minimal formulation. Successful learning occurs only under the presence of an implicit curriculum. I uncover the learned mechanisms by studying the attention maps in the trained transformers. I also study the training process, uncovering that attention heads always emerge in a specific sequence guided by the implicit curriculum.",
            "score": 0.3913988811443383,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.40234375
        },
        {
            "corpus_id": "268856885",
            "title": "Transforming LLMs into Cross-modal and Cross-lingual Retrieval Systems",
            "text": "Large language models (LLMs) are trained on text-only data that go far beyond the languages with paired speech and text data. At the same time, Dual Encoder (DE) based retrieval systems project queries and documents into the same embedding space and have demonstrated their success in retrieval and bi-text mining. To match speech and text in many languages, we propose using LLMs to initialize multi-modal DE retrieval systems. Unlike traditional methods, our system doesn\u2019t require speech data during LLM pre-training and can exploit LLM\u2019s multilingual text understanding capabilities to match speech and text in languages unseen during retrieval training. Our multi-modal LLM-based retrieval system is capable of matching speech and text in 102 languages despite only training on 21 languages. Our system outperforms previous systems trained explicitly on all 102 languages. We achieve a 10% absolute improvement in Recall@1 averaged across these languages. Additionally, our model demonstrates cross-lingual speech and text matching, which is further enhanced by readily available machine translation data.",
            "score": 0.3908534054751986,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9208984375
        },
        {
            "corpus_id": "15470672",
            "title": "Empirical Estimates of Adaptation: The chance of Two Noriegas is closer to p/2 than p2",
            "text": "Adaptive language models were introduced to account for repetition. It is well known that the second instance of a word (or ngram) is nmch more likely than the first. But what we find surprising is just how large the effect is. The chance of two Noriegas is closer to p/2 than p 2. \n\nin addition to the magnitude of adaptation, we were also surprised by the shape: while the first instance of a word depends very strongly on frequency, the second does not. Adaptation depends more on content than flequency; adaptation is stronger for content words such as proper nouns, technical terminology and good keywords for information retrieval, and weaker for functioll words, cliches and first nalnes. \n\nThe shape and magnitude of adaptation has implications for psycholinguistics, information retrieval and language modeling. Psycholinguistics has tended to equate word frequency with content, but our results suggest that two words with similar frequency (e.g., \"Kennedy\" and \"except\") can be distinguished on the basis of their adaptation. Information retrieval has tended to use frequency in a similar way, weighting terms by IDF (inverse document frequency), with little attention paid to adaptation. We propose a term weighting method that makes use of adaptation (burstiness) and expansion frequency in a related paper (Umelnura and Church, submitted). \n\nTwo estimation methods were introduced to demonstrate tile magnitude and shape of adaptation. Both methods produce similar results. \n\n\u2022 Pr(+ adapt I ) = Pr(test] hist) \n\n\u2022 Pr(+adapt2)=Pr(k>2]k>_l ) \n\nNeighborhoods were then introduced for words such as \"laid-off\" that were not in the history but were close (\"laid-off\" is related to \"layoff,\" which was in the history). Neighborhoods were defined in terms of query expansion. The history is treated as a query in an information retriewd document-ranking system. Words in the k topranking documents (but not in the history) are called neighbors. Neighbors adapt more dmn other terms, but not as much as words that actually appeared in the history. Better neighbors (larger et) adapt more than worse neighbors (slnaller el).",
            "score": 0.3908534054751986,
            "section_title": "Conclusions",
            "char_start_offset": 17019,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 67
                },
                {
                    "start": 68,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 281
                },
                {
                    "start": 284,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 695
                },
                {
                    "start": 698,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1353
                },
                {
                    "start": 1356,
                    "end": 1449
                },
                {
                    "start": 1450,
                    "end": 1487
                },
                {
                    "start": 1490,
                    "end": 1523
                },
                {
                    "start": 1526,
                    "end": 1553
                },
                {
                    "start": 1556,
                    "end": 1726
                },
                {
                    "start": 1727,
                    "end": 1782
                },
                {
                    "start": 1783,
                    "end": 1868
                },
                {
                    "start": 1869,
                    "end": 1951
                },
                {
                    "start": 1952,
                    "end": 2053
                },
                {
                    "start": 2054,
                    "end": 2129
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.134033203125
        },
        {
            "corpus_id": "251903333",
            "title": "Domain-Specific NER via Retrieving Correlated Samples",
            "text": "Retrieval-augmented models are showing state-ofthe-art performance in many NLP tasks, such as Dialogue (Weston et al., 2018), Neural Machine Translation (Zhang et al., 2018a), Question Answering (Izacard and Grave, 2021), and Language Modeling (Guu et al., 2020;Yao et al., 2022;Borgeaud et al., 2022). Our work aims to model the internal correlation within sub-groups of samples. We first retrieve correlated sample groups for a given input by the off-the-shelf Elasticsearch engine. Then, we propose painlessly calibrating entity type and transformer-based correlation modeling, where the latter one is similar to Wang et al. (2021). Our recent work (Wang et al., 2022) also investigated retrieving knowledge from the Wikipedia, which can augment the context of NER inputs and shows significant improvements in SemEval-2022 Task 11 Multilingual NER. \n\nThis work could be further investigated with some more sophisticated techniques, such as example-based learning (Gao et al., 2021;Lee et al., 2022;Liu et al., 2022). Meanwhile, it also may help the NER task to extend to the low-resource and zero-shot scenarios (Meng et al., 2021;Zhang et al., 2021;Hu et al., 2021;Lu et al., 2022;Hu et al., 2020).",
            "score": 0.3908534054751986,
            "section_title": "Discussion",
            "char_start_offset": 11657,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 851
                },
                {
                    "start": 854,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1202
                }
            ],
            "ref_mentions": [
                {
                    "start": 103,
                    "end": 124,
                    "matchedPaperCorpusId": "52006529"
                },
                {
                    "start": 153,
                    "end": 174,
                    "matchedPaperCorpusId": "4698173"
                },
                {
                    "start": 195,
                    "end": 220,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 244,
                    "end": 262,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 262,
                    "end": 279,
                    "matchedPaperCorpusId": "243847293"
                },
                {
                    "start": 279,
                    "end": 301,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 616,
                    "end": 634,
                    "matchedPaperCorpusId": "234337605"
                },
                {
                    "start": 652,
                    "end": 670,
                    "matchedPaperCorpusId": "247187671"
                },
                {
                    "start": 966,
                    "end": 984,
                    "matchedPaperCorpusId": "229923710"
                },
                {
                    "start": 984,
                    "end": 1001,
                    "matchedPaperCorpusId": "239016858"
                },
                {
                    "start": 1001,
                    "end": 1018,
                    "matchedPaperCorpusId": "248512847"
                },
                {
                    "start": 1134,
                    "end": 1153,
                    "matchedPaperCorpusId": "235253726"
                },
                {
                    "start": 1153,
                    "end": 1169,
                    "matchedPaperCorpusId": "237502820"
                },
                {
                    "start": 1169,
                    "end": 1185,
                    "matchedPaperCorpusId": "247619149"
                },
                {
                    "start": 1185,
                    "end": 1201,
                    "matchedPaperCorpusId": "214802812"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58837890625
        },
        {
            "corpus_id": "257232958",
            "title": "Joint Representations of Text and Knowledge Graphs for Retrieval and Evaluation",
            "text": "We briefly review recent approaches to uni-and cross-modal retrieval, representation learning models, and evaluation metrics for Natural Language Generation (NLG) models. \n\nNatural Language Retrieval Models. For natural language, a first class of retrieval models focuses on retrieving sentences that are similar to some input sentence. BERT (Devlin et al., 2019) has been used as a cross-encoder. Two sentences are given with a separator token, cross-attention applies to all input tokens and the resulting representation is fed into a linear layer to score the match. However, this is computationally inefficient as it is not possible to pre-compute and index such representations. A pre-computable model was proposed by (Reimers and Gurevych, 2019) who used twin encoders pre-trained on Natural Language Inference data (Bowman et al., 2015) to set new state-of-the-art performance on a large set of sentence scoring tasks. Further work (Chen et al., 2020;Humeau et al., 2019) combined cross-and bi-encoders to reach a tradeoff between accuracy and efficiency. We differ from those works in that we focus on cross-modal representation learning. \n\nRepresentation Learning for Knowledge-Bases. \n\nVarious KB embedding models have been proposed to support downstream applications such as KB completion or alignment of different bases. Compositional approaches (Nickel et al., 2011(Nickel et al., , 2016) ) use tensor products to model relations as functions of their argument entities. Translational approaches model relations as translation operations from the subject (head) to object (tail) entity (Bordes et al., 2013;Yang et al., 2014;Trouillon et al., 2016). \n\nNeural models have also leveraged 2-D convolutions over entity embeddings to predict relations (Dettmers et al., 2018) as well as graph convolutional networks (Schlichtkrull et al., 2018). All these approaches focus on representation learning for Knowledge-Bases entities and relations. In contrast, we focus on cross-modal similarity between a text and a KB graph. \n\nCross-Modal Representation Learning and Retrieval.",
            "score": 0.3908534054751986,
            "section_title": "Related Work",
            "char_start_offset": 4020,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 173,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1146
                },
                {
                    "start": 1149,
                    "end": 1193
                },
                {
                    "start": 1196,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1662
                },
                {
                    "start": 1665,
                    "end": 1853
                },
                {
                    "start": 1854,
                    "end": 1951
                },
                {
                    "start": 1952,
                    "end": 2030
                },
                {
                    "start": 2033,
                    "end": 2083
                }
            ],
            "ref_mentions": [
                {
                    "start": 342,
                    "end": 363,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 723,
                    "end": 751,
                    "matchedPaperCorpusId": "201646309"
                },
                {
                    "start": 822,
                    "end": 843,
                    "matchedPaperCorpusId": "14604520"
                },
                {
                    "start": 939,
                    "end": 958,
                    "matchedPaperCorpusId": "222177208"
                },
                {
                    "start": 1358,
                    "end": 1378,
                    "matchedPaperCorpusId": "1157792"
                },
                {
                    "start": 1378,
                    "end": 1403,
                    "matchedPaperCorpusId": "6071257"
                },
                {
                    "start": 1599,
                    "end": 1620,
                    "matchedPaperCorpusId": "14941970"
                },
                {
                    "start": 1638,
                    "end": 1661,
                    "matchedPaperCorpusId": "15150247"
                },
                {
                    "start": 1760,
                    "end": 1783,
                    "matchedPaperCorpusId": "4328400"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9619140625
        },
        {
            "corpus_id": "246210468",
            "title": "Transfer Learning Approaches for Building Cross-Language Dense Retrieval Models",
            "text": "In this section, we briefly review related work on neural retrieval and its extension to cross-lingual settings. For many years, sparse retrieval models such as BM25 [26] and Query Likelihood [24] were the dominant models for ad hoc retrieval tasks. Only in recent years, with the rise of BERT [7] and the availability of large scale retrieval collections such as MSMARCO [2] for training, have neural information retrieval (neural IR) models emerged as the state of the art. Similar to sparse retrieval models, neural IR models take as input the query and documents, and produce a relevance score. For each query and document pair, matching mechanisms, such as DRMM [9], KNRM [6] or PACCR [10], construct the interaction matrix between the distributed term representations of the query and the documents, and aggregate them into a relevance score. Alternatively, the BERT passage pair classification model [7] considers the query and the document as the input pair, and uses the final classification score as the relevance score [35]. CEDR [18] incorporates contextualized embeddings such as ELMo [23] or BERT [7] into the matching, providing significant effectiveness improvements by taking advantage of contextualization. However, due to the high computational cost, these models are used to rerank top-ranked documents from a sparse retrieval system. \n\nColBERT [13] further improves efficiency by keeping separate the querydocument interaction until the end of the neural architecture. This is called late interaction. As opposed to matching mechanisms that require both the query and the document to be present simultaneously, late interaction allows offline encoding of the documents into bags-of-vectors. Document representations are combined with query representations by an efficient MaxSim operator, which significantly reduces computation at inference time. This decoupling enables the documents to be encoded offline and indexed to support approximate nearest neighbor search. Further details are discussed in Section 3. \n\nCross-language transfer learning is important for CLIR. Due to the lack of training data for ad hoc neural retrieval models other than in English, prior work explored zero-shot model transfer to other languages, trained with only English retrieval examples [17,28].",
            "score": 0.3908534054751986,
            "section_title": "Related Work",
            "char_start_offset": 3238,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1354
                },
                {
                    "start": 1357,
                    "end": 1489
                },
                {
                    "start": 1490,
                    "end": 1522
                },
                {
                    "start": 1523,
                    "end": 1711
                },
                {
                    "start": 1712,
                    "end": 1868
                },
                {
                    "start": 1869,
                    "end": 1988
                },
                {
                    "start": 1989,
                    "end": 2032
                },
                {
                    "start": 2035,
                    "end": 2090
                },
                {
                    "start": 2091,
                    "end": 2300
                }
            ],
            "ref_mentions": [
                {
                    "start": 166,
                    "end": 170,
                    "matchedPaperCorpusId": "41563977"
                },
                {
                    "start": 192,
                    "end": 196,
                    "matchedPaperCorpusId": "2759772"
                },
                {
                    "start": 294,
                    "end": 297,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 667,
                    "end": 670,
                    "matchedPaperCorpusId": "5688521"
                },
                {
                    "start": 677,
                    "end": 680,
                    "matchedPaperCorpusId": "33169397"
                },
                {
                    "start": 690,
                    "end": 694,
                    "matchedPaperCorpusId": "6246996"
                },
                {
                    "start": 907,
                    "end": 910,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1041,
                    "end": 1045,
                    "matchedPaperCorpusId": "131776819"
                },
                {
                    "start": 1098,
                    "end": 1102,
                    "matchedPaperCorpusId": "3626819"
                },
                {
                    "start": 1111,
                    "end": 1114,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1365,
                    "end": 1369,
                    "matchedPaperCorpusId": "216553223"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95361328125
        },
        {
            "corpus_id": "273707469",
            "title": "All or None: Identifiable Linear Properties of Next-token Predictors in Language Modeling",
            "text": "Parallel vectors naturally define a one-dimensional subspace \u0393 \u2208 R d that contains all of those vectors. In language model representations, several such subspaces have been identified that appear to encode semantic and syntactic properties, for example translation across languages or the transformation of an adjective into its comparative or superlative form [Mikolov et al., 2013a, Park et al., 2024a]. Our relational formulation of this linear property focuses on whether these subspaces contain the information in the embeddings f (s) which is relevant to predict the reply token to a query q when appended to the context s. E.g., this could happen if the embeddings projected in the subspace \u0393 eng,ita , containing the vector g(\"Rome\")\u2212g(\"Roma\"), encode whether the replies to the query q = \"Is written in English or Italian?\" are more likely to be y = \"English\" or y \u2032 = \"Italian\". We can capture this through the following definition: \n\nDefinition 10 (ls: Relational Linear Subspaces). \n\nA model (f , g) \u2208 \u0398 linearly represents a subspace \u0393 \u2286 SIm(g 0 ) relative to q \u2208 Seq(A) if for all pairs of tokens y i , y j \u2208 A such that g i (y j ) := g(y j ) \u2212 g(y i ) \u2208 \u0393, there exists a vector \u03b3 \u2208 SIm(g 0 ) such that \u2200s \u2208 Seq(A) \n\nWe provide one example of this property in Figure 3. The ls property is implied by relational linearity (Definition 9) in the following sense: \n\nProposition 11 (\u0393lr =\u21d2 ls). Suppose that a model (f , g) \u2208 \u0398 (i) linearly represents q on \u0393 \u2286 SIm(g 0 ), and (ii) \u0393 q \u2286 SIm(g 0 ), then the model (f , g) linearly represents \u0393 q relative to q (Definition 10).",
            "score": 0.3908534054751986,
            "section_title": "Linear subspaces (ls)",
            "char_start_offset": 23627,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 629
                },
                {
                    "start": 630,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 942
                },
                {
                    "start": 945,
                    "end": 993
                },
                {
                    "start": 996,
                    "end": 1229
                },
                {
                    "start": 1232,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1374
                },
                {
                    "start": 1377,
                    "end": 1404
                },
                {
                    "start": 1405,
                    "end": 1585
                }
            ],
            "ref_mentions": [
                {
                    "start": 383,
                    "end": 404,
                    "matchedPaperCorpusId": "265042984"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08038330078125
        },
        {
            "corpus_id": "229349381",
            "title": "Neural methods for effective, efficient, and exposure-aware information retrieval",
            "text": "4][445][446][447][448], we are now witnessing surprisingly huge benefits from applications of deep NLP models in retrieval. These new NLP artifacts, however, are not in the form thesauri or parts of speech tags, but rather in the form of pretrained language models and latent text representations. While, these black box language models may pick up certain linguistic regularities from training on large corpora, it is also possible, if not likely, that these learned latent representations encode relationships and attributes that are very different to our own notion of linguistic properties. By simply modeling observed regularities in unlabeled corpora, a language model may in fact learn that \"duck\" and \"porcupine\" are similar given they appear in similar contexts-such as, \"how much does a duck weigh?\" and \"how much does a porcupine weigh?\". If our goal is to maximize some averaged relevance metrics for a query autocompletion task, it may indeed be reasonable that \"duck\" and \"porcupine\" have similar latent representations. Similarly, the latent space may be able to encode seemingly nonsensical concepts such as a \"duckupine\" even if it has no meaningful counterpart in the real world, except may be in literary fiction (see Figure 9.1).\n\nThis poses an interesting challenge for the research community. While, we are reasonably good at measuring how effective these black box models are at improving retrieval, it is significantly harder to articulate exactly what knowledge and world view these models encode (and do not encode), and even more difficult to quantify the progress the IR community is making with regards to better understanding of retrieval tasks from the application of these models. This is not to imply that the learned latent representations must be perfectly interpretable to qualify as scientific progress, but rather we are making a case for viewing the contributions of neural IR through a much broader lens that encourages its usage to aid the development of new IR theory and improved understanding of retrieval tasks.\n\nOn that note, we conclude this thesis by summarize the contribution of our own work, as described in the earlier chapters, in Section 9.1, and identifying key future challenges and opportunities for the field in Section 9.2.",
            "score": 0.3908534054751986,
            "section_title": "Conclusion",
            "char_start_offset": 204992,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 2,
                    "end": 7,
                    "matchedPaperCorpusId": "27084667"
                },
                {
                    "start": 7,
                    "end": 12,
                    "matchedPaperCorpusId": "1759481"
                },
                {
                    "start": 12,
                    "end": 17,
                    "matchedPaperCorpusId": "9531323"
                },
                {
                    "start": 17,
                    "end": 22,
                    "matchedPaperCorpusId": "38913981"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80419921875
        },
        {
            "corpus_id": "202789367",
            "title": "Do Nuclear Submarines Have Nuclear Captains? A Challenge Dataset for Commonsense Reasoning over Adjectives and Objects",
            "text": "As we want to evaluate strong yet simple preexisting language understanding models on this task, we now describe a method for obtaining the direct prediction described above via conversion to a form suitable for inference in the style of the SNLI and MNLI datasets (Bowman et al., 2015;Williams et al., 2018), which consist of premise and hypothesis sentence pairs. We first form simple hypothesis sentences from the tuples using the fixed template \"The whole 's part is adjective .\" We then retrieve premise sentences that describe a whole adjective . An example for (bicycle, old) is \"He rode an old bicycle and brought fruits and vegetables home from Chinatown.\" We retrieve context sentences from five resources: Project Gutenberg books2 , the Gigaword news corpus (Parker et al., 2011), SNLI, MNLI, and MSCOCO image captions (Lin et al., 2014); premise sentence selection is described fully in Appendix D and examples are shown in Table 1.",
            "score": 0.3908534054751986,
            "section_title": "Adjective projection as NLI",
            "char_start_offset": 11042,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 944
                }
            ],
            "ref_mentions": [
                {
                    "start": 265,
                    "end": 286,
                    "matchedPaperCorpusId": "14604520"
                },
                {
                    "start": 286,
                    "end": 308,
                    "matchedPaperCorpusId": "3432876"
                },
                {
                    "start": 830,
                    "end": 848,
                    "matchedPaperCorpusId": "14113767"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.042083740234375
        },
        {
            "corpus_id": "276421942",
            "title": "HopRAG: Multi-Hop Reasoning for Logic-Aware Retrieval-Augmented Generation",
            "text": "Retrieval-augmented generation (RAG) has become the standard approach for large language models (LLMs) to tackle knowledge-intensive tasks (Guu et al., 2020a;Lewis et al., 2020a;Izacard et al., 2022;Min et al., 2023;Ram et al., 2023;Liang et al., 2025). Not only can it effectively address the inherent knowledge limitations and hallucination issues (Zhang et al., 2023), but it can also enable easy interpretability and provenance tracking (Akyurek et al., 2022). Especially, the efficacy of RAG hinges on its retrieval module for identifying relevant documents from a vast corpus. \n\nCurrently, there are two mainstream types of retrievers: sparse retrievers (Jones, 1973;Robertson and Zaragoza, 2009b) and dense retrievers (Xiao et al., 2024;Wang et al., 2024b;Sturua et al., 2024;Wang et al., 2024c), which focus on lexical similarity and semantic similarity respectively, and are often combined for better retrieval performance (Sawarkar et al., 2024). Despite advancements, the ultimate goal of information retrieval extends beyond lexical and semantic similarity, striving instead for logical relevance. Due to the lack of logic-aware mechanism, the imperfect retrieval remains prominent (Wang et al., 2024a;Shao et al., 2024;Dai et al., 2024;Su et al., 2024a,b). For precision, the retrieval system may return lexically and semantically similar but indirectly relevant passages; regarding recall, it may fail to retrieve all the necessary passages for the user query. \n\nBoth cases eventually lead to inaccurate or incomplete LLM responses (Chen et al., 2024;Xiang et al., 2024;Zou et al., 2024), especially for multihop or multi-document QA tasks requiring multiple relevant passages for the final answer. In contrast, the reasoning capability of generative models is rapidly advancing, with notable examples such as openai-o1 (Jaech et al., 2024) and deepseek-r1 (Guo et al., 2025).",
            "score": 0.3908534054751986,
            "section_title": "-Six Degrees of Separation",
            "char_start_offset": 159,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 582
                },
                {
                    "start": 585,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1269
                },
                {
                    "start": 1270,
                    "end": 1474
                },
                {
                    "start": 1477,
                    "end": 1712
                },
                {
                    "start": 1713,
                    "end": 1890
                }
            ],
            "ref_mentions": [
                {
                    "start": 139,
                    "end": 158,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 158,
                    "end": 178,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 199,
                    "end": 216,
                    "matchedPaperCorpusId": "254220735"
                },
                {
                    "start": 441,
                    "end": 463,
                    "matchedPaperCorpusId": "248987368"
                },
                {
                    "start": 1546,
                    "end": 1565,
                    "matchedPaperCorpusId": "261530434"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89404296875
        },
        {
            "corpus_id": "265294654",
            "title": "Adapt in Contexts: Retrieval-Augmented Domain Adaptation via In-Context Learning",
            "text": "Given an input sentence from the source domain, we first search for semantically similar examples from the unlabeled target domain. This is analogous to retrieval and re-rank given a search query. Retrieval-augmented LM approaches (Guu et al., 2020;Lewis et al., 2020;Asai et al., 2023) apply a parametrized dense retriever to train with the task model. In this paper, we fix the retriever part and use the off-the-shelf scoring language models. For the SA task, we use SimCSE (Gao et al., 2021) which produces semantically meaningful sentence embeddings after being trained with contrastive learning (Chen et al., 2020;He et al., 2020). Here cosine similarity is used to retrieve top-ranked (most similar) examples from the target domain. For NER, we use BERTScore (Zhang et al., 2020;Wang et al., 2021), because it gives a metric for each sentence based on the similarity of token representation, which is more crucial for the task of NER. Specifically, given a source sentence x S paired with label y, we retrieve top-k relevant chunks of texts from the target unlabeled dataset D T . \n\nThe retrieved examples are denoted as \n\nk } which will serve as the contexts to enrich the semantics for the source input.",
            "score": 0.3908534054751986,
            "section_title": "Context Construction with Retrieval",
            "char_start_offset": 7943,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1087
                },
                {
                    "start": 1090,
                    "end": 1127
                },
                {
                    "start": 1130,
                    "end": 1212
                }
            ],
            "ref_mentions": [
                {
                    "start": 231,
                    "end": 249,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 249,
                    "end": 268,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 477,
                    "end": 495,
                    "matchedPaperCorpusId": "233296292"
                },
                {
                    "start": 601,
                    "end": 620,
                    "matchedPaperCorpusId": "211096730"
                },
                {
                    "start": 620,
                    "end": 636,
                    "matchedPaperCorpusId": "207930212"
                },
                {
                    "start": 786,
                    "end": 804,
                    "matchedPaperCorpusId": "235458009"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94677734375
        },
        {
            "corpus_id": "278714952",
            "title": "Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation",
            "text": "Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of natural language processing tasks (Wang et al., 2023;Rozi\u00e8re et al., 2024;Cui et al., 2025;Yasunaga et al., 2022;Liu et al., 2024). However, their tendency to \"hallucinate\", that is, to produce fluent but factually incorrect information, remains a persistent challenge (Li et al., 2024a;Duan et al., 2024;Zhang et al., 2023), particularly in high-stakes domains such as healthcare, law, and finance (Agarwal et al., 2024;Dahl et al., 2024;Kang and Liu, 2023). To address this, Retrieval-Augmented Generation (RAG) has become a popular solution. Instead of relying solely on parametric memory, RAG systems retrieve external documents and condition the model's response on this evidence. \n\nIn practice, retrieval accuracy in RAG is far from flawless. Retrieved documents may be outdated, misleading, or topically adjacent but factually incorrect. These errors can propagate downstream, leading models to blend inaccurate context into fluent but false answers. This is especially concerning in domains such as law, compliance, financial reporting, or medicine, where mistakes can have wide-ranging repercussions. \n\nMost prior work has addressed this issue from the retrieval perspective, focusing on improving retrievers, reranking mechanisms, or applying filtering heuristics (Sawarkar et al., 2024;Dong et al., 2024;Zhou and Chen, 2025). In contrast, relatively little attention has been given to improving the model's ability to resist using the incorrect information. \n\nIn this paper, we introduce Finetune-RAG, a method that directly targets hallucination by finetuning the model with imperfect RAG samples that mimic real-world retrieval scenarios. We con-structed a diverse dataset covering legal documents, scientific literature, books, and web data, each paired with a plausible but fictitious counterpart.",
            "score": 0.3908534054751986,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 779
                },
                {
                    "start": 782,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1203
                },
                {
                    "start": 1206,
                    "end": 1430
                },
                {
                    "start": 1431,
                    "end": 1562
                },
                {
                    "start": 1565,
                    "end": 1745
                },
                {
                    "start": 1746,
                    "end": 1906
                }
            ],
            "ref_mentions": [
                {
                    "start": 207,
                    "end": 224,
                    "matchedPaperCorpusId": "258841126"
                },
                {
                    "start": 515,
                    "end": 533,
                    "matchedPaperCorpusId": "266725450"
                },
                {
                    "start": 1368,
                    "end": 1391,
                    "matchedPaperCorpusId": "269043117"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53662109375
        },
        {
            "corpus_id": "267770347",
            "title": "Retrieval Helps or Hurts? A Deeper Dive into the Efficacy of Retrieval Augmentation to Language Models",
            "text": "We evaluate 10 language models of varying sizes augmented with four different retrieval methods to quantify the recall capability of LMs and the performance of retrievers in isolation and jointly and share the insights.",
            "score": 0.3908534054751986,
            "section_title": "Experiments: Recall or Retrieve",
            "char_start_offset": 14275,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 219
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.751953125
        },
        {
            "corpus_id": "260735813",
            "title": "Answering Unseen Questions With Smaller Language Models Using Rationale Generation and Dense Retrieval",
            "text": "An overview of our approach is provided in Figure 1. In following sections we describe how the two knowledge sources are implemented, how the RR model is constructed, trained and initially evaluated, and how the Reasoning Models are trained. We describe our context combination methods further below in section 3.3 so as to make clear the nomenclature we use in reporting experimental results. \n\nA major assumption is that our system may be asked arbitrary questions from unknown distributions. Therefore we primarily consider our evaluations in the unseen rather than fine-tuned setting. The most relevant comparisons we have available to us are the baselines for StrategyQA (Geva et al., 2021), CommonsenseQA (Talmor et al., 2019), ARC-DA (Bhakthavatsalam et al., 2021), IIRC (Ferguson et al., 2020) and Musique (Trivedi et al., 2022) established for smaller Language Models in unseen settings by Hartill et al. (2023). The datasets cover a diversity of question types requiring diverse reasoning strategies to answer, including commonsense and n-hop factual questions (n \u2264 4) as discussed further in section 3.2. Hence we adopt these datasets for evaluation and use the same definition as Hartill et al. (2023) for \"seen-ness\" whereby an unseen evaluation sample is one from a dataset that is disjoint from any training dataset. In our case we extend this to our LLM generations, ensuring that all examples in few-shot prompts come from our training rather than evaluation datasets, or are manually created by us. \n\nAside from the baseline results, Hartill et al. (2023) also provide their \"Iterator\" n-hop dense retrieval system (where n \u2264 4). In a single-hop retrieval model, samples are processed as (1) Input \u27e8q\u27e9 with an objective of minimizing distance to the vector representing d 0 (hereafter denoted \u27e8q\u27e9 \u2192 d 0 , where q and d t are the input question and the t-th supporting document of q to retrieve respectively). For a two hop system, the second hop is then (2) \u27e8q, d 0 \u27e9 \u2192 d 1 .",
            "score": 0.3908534054751986,
            "section_title": "Method",
            "char_start_offset": 12854,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 52
                },
                {
                    "start": 53,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 393
                },
                {
                    "start": 396,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1516
                },
                {
                    "start": 1519,
                    "end": 1647
                },
                {
                    "start": 1648,
                    "end": 1926
                },
                {
                    "start": 1927,
                    "end": 1993
                }
            ],
            "ref_mentions": [
                {
                    "start": 676,
                    "end": 695,
                    "matchedPaperCorpusId": "230799347"
                },
                {
                    "start": 711,
                    "end": 732,
                    "matchedPaperCorpusId": "53296520"
                },
                {
                    "start": 778,
                    "end": 801,
                    "matchedPaperCorpusId": "226262208"
                },
                {
                    "start": 814,
                    "end": 836,
                    "matchedPaperCorpusId": "236771976"
                },
                {
                    "start": 899,
                    "end": 920,
                    "matchedPaperCorpusId": "260379194"
                },
                {
                    "start": 1192,
                    "end": 1213,
                    "matchedPaperCorpusId": "260379194"
                },
                {
                    "start": 1552,
                    "end": 1573,
                    "matchedPaperCorpusId": "260379194"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75439453125
        },
        {
            "corpus_id": "271213503",
            "title": "MaskMoE: Boosting Token-Level Learning via Routing Mask in Mixture-of-Experts",
            "text": "Language models are statistical models designed to optimize the likelihood of token sequences in training data (Touvron et al., 2023a). Initially, language models relied on n-gram statistics (Bahl et al., 1983;Katz, 1987;Kneser and Ney, 1995). Subsequently, the emphasis shifted to neural networkbased models, particularly Recurrent Neural Networks (Mikolov et al., 2010) and their variants like LSTMs (Graves, 2013). Those models have demonstrated the ability to learn intricate patterns within textual data, achieving significant success in various language modeling applications. In recent times, Transformers have become the predominant architecture for language models. Notable examples include BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), GPT-2 (Radford et al., 2019), UniLM (Dong et al., 2019), and T5 (Raffel et al., 2020). The introduction of GPT-3 (Brown et al., 2020), which boasts 175 billion parameters, marked a significant milestone due to its exceptional performance across numerous downstream tasks. And that then led to a surge in research focusing on large generative language models, including prominent works such as Gopher (Rae et al., 2021), PaLM (Chowdhery et al., 2022), Pythia (Biderman et al., 2023), OPT (Zhang et al., 2022), GLM (Du et al., 2022b;Zeng et al., 2023) and LLaMA (Touvron et al., 2023a,b). Currently, GPT4 (OpenAI, 2023) achieves truly remarkable results. However, as the size of the model grows, the computational demands for both training and inference also increase. MoE models achieve scalability by sparsely activating a portion of the model's parameters, allowing for an increase in model size without significantly raising computational overhead. Consequently, MoE models have been receiving increasing attention recently.",
            "score": 0.3908534054751986,
            "section_title": "Language Models",
            "char_start_offset": 7404,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1342
                },
                {
                    "start": 1343,
                    "end": 1408
                },
                {
                    "start": 1409,
                    "end": 1522
                },
                {
                    "start": 1523,
                    "end": 1706
                },
                {
                    "start": 1707,
                    "end": 1782
                }
            ],
            "ref_mentions": [
                {
                    "start": 191,
                    "end": 210,
                    "matchedPaperCorpusId": "14789841"
                },
                {
                    "start": 210,
                    "end": 221,
                    "matchedPaperCorpusId": "6555412"
                },
                {
                    "start": 221,
                    "end": 242,
                    "matchedPaperCorpusId": "9685476"
                },
                {
                    "start": 349,
                    "end": 371,
                    "matchedPaperCorpusId": "17048224"
                },
                {
                    "start": 705,
                    "end": 726,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 792,
                    "end": 811,
                    "matchedPaperCorpusId": "147704286"
                },
                {
                    "start": 820,
                    "end": 841,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1214,
                    "end": 1237,
                    "matchedPaperCorpusId": "257921893"
                },
                {
                    "start": 1269,
                    "end": 1287,
                    "matchedPaperCorpusId": "247519241"
                },
                {
                    "start": 1287,
                    "end": 1305,
                    "matchedPaperCorpusId": "252715691"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.44384765625
        },
        {
            "corpus_id": "269282809",
            "title": "BIRD: A Trustworthy Bayesian Inference Framework for Large Language Models",
            "text": "Large language models, although they cannot solve some complicated cases, are shown to possess parametric knowledge that enables them to solve more common cases that can be directly found in their memory (Kambhampati et al., 2024;Li et al., 2024;Zhou et al., 2024). Our method for generating the factors follows this similar intuition. For a scenario S, we first do abstractions with LLMs to derive N factors {F j } N j=1 . The most straightforward approach involves utilizing LLMs to directly obtain a list of factors and their potential values. However, direct generation is effective but not sufficiently comprehensive, and we want to fully utilize past similar occurrences that the model memorizes in their parametric knowledge. \n\nTherefore, to encourage models to fully explore the range of alternatives, we divide the task into two stages: 1) generating sentences describing familiar situations that would increase the likelihood of an outcome, and 2) summarizing these sentences into factors with corresponding values. We show the prompts in Appendix Fig. 5 and Fig. 6. We further utilize LLMs to perform classification to assess how the value of each generated factor affect the decision, i.e., deciding the value supports which outcome, so that we can filter out factors that are unrelated to the outcomes. The prompt is in Appendix Fig. 7. In the example depicted in Fig. 2, a factor like \"The color of the vehicle\" will be eliminated since all potential values of this factor consistently support the same outcome, i.e., neutral.",
            "score": 0.3908534054751986,
            "section_title": "ABDUCTIVE FACTOR GENERATION",
            "char_start_offset": 13256,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 732
                },
                {
                    "start": 735,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1349
                },
                {
                    "start": 1350,
                    "end": 1540
                }
            ],
            "ref_mentions": [
                {
                    "start": 204,
                    "end": 230,
                    "matchedPaperCorpusId": "272330427"
                },
                {
                    "start": 230,
                    "end": 246,
                    "matchedPaperCorpusId": "265220880"
                },
                {
                    "start": 246,
                    "end": 264,
                    "matchedPaperCorpusId": "265220880"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.459228515625
        },
        {
            "corpus_id": "17148613",
            "title": "Monolingual Web-based Factoid Question Answering in Chinese, Swedish, English and Japanese",
            "text": "Finding the best answer involves a search over all for the one which maximizes the probability of the above model: \n\nUsing Bayes rule, making further conditional independence assumptions and assuming uniform prior probabilities, which therefore do not affect the optimisation criterion, we obtain the final optimisation criterion (see (Whittaker et al., 2005a) for more details): \n\nThe \u00c8\u00b4 \u00b5 model is essentially a language model which models the probability of an answer sequence given a set of information-bearing features . It models the proximity of to features in . We call this model the retrieval model and examine it further in Section 2.1. \n\nThe \u00c8\u00b4\u00cf \u00b5 model matches an answer with features in the question-type set \u00cf. Roughly speaking this model relates ways of asking a question with classes of valid answers. For example, it associates dates, or days of the week with whentype questions. In general, there are many valid and equiprobable for a given \u00cf so this component can only re-rank candidate answers retrieved by the retrieval model. Consequently, we call it the filter model and examine it further in Section 2.2.",
            "score": 0.3908534054751986,
            "section_title": "EACL 2006 Workshop on Multilingual Question Answering -MLQA06",
            "char_start_offset": 7144,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 117,
                    "end": 379
                },
                {
                    "start": 382,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 647
                },
                {
                    "start": 650,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1129
                }
            ],
            "ref_mentions": [
                {
                    "start": 335,
                    "end": 360,
                    "matchedPaperCorpusId": "788022"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75390625
        },
        {
            "corpus_id": "14699360",
            "title": "A Bayesian Model for Joint Learning of Categories and their Features",
            "text": "In this section we present our Bayesian model of category and feature induction (henceforth, BCF). BCF jointly learns categories, feature types, and their associations. Specifically, it infers one global set of feature types which is shared across categories (e.g., ANIMALS and VEHICLES can be described in terms of colors). However, categories differ in their strength of association with feature types (e.g., the feature type function will be highly associated with TOOLS but less so with ANIMALS). BCF jointly optimizes categories and their featural representation: the learning objective is to obtain a set of meaningful categories, each characterized by relevant and coherent feature types.\n\nThe generative story and plate diagram for the BCF model are shown in Figures 1 and 2, respectively. The input to the model is a collection of stimuli d \u2208 {1..D} extracted from a large text corpus. Each stimulus consists of a target concept c \u2208 {1..L} and its context f \u2208 {1..F}. We adopt a simple representation of context as the set of words making up the sentence c occurs in (except c). The model assigns concepts to categories k \u2208 {1..K} and features to feature types g \u2208 {1..G}. It learns a set of concept clusters (i.e., categories), as well as a clustering over features (i.e., feature types), and a distribution over those feature clusters for each category (i.e., category-feature type associations). Specifically, the occurrences of a concept will be assigned a category, based on how similar the concept's feature types are compared to the feature types of all other potential categories. Simultaneously, upon observing a stimulus (i.e., a concept in context), the model assigns the context to a particular feature type based on its probability under all po- Figure 2: The plate diagram of the BCF model. Shaded nodes indicate observed variables, and dotted nodes indicate hyperparameters.\n\ntential feature types, and the prior probability of observing that feature type with the concept's assigned category.\n\nMore formally, we can describe the model through the generative story given in Figure 1. We assume a global multinomial distribution over categories Mult(\u03b8), drawn from",
            "score": 0.3908534054751986,
            "section_title": "The BCF Model",
            "char_start_offset": 6460,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.041229248046875
        },
        {
            "corpus_id": "248506020",
            "title": "Retrieval-Enhanced Machine Learning",
            "text": "This paper focuses on predictive ML models. Let X be the input (feature) space for the task and Y be the output (prediction) space. Given an input  \u2208 X, a ML model produces a prediction in the output space  \u2208 Y. Supervised learning models are often trained by minimizing an empirical prediction loss (error) over instances in a training set  = {(, ) \u2208 X \u00d7 Y}. \n\nRetrieval-enhanced machine learning (REML) refers to models composed of two coupled components: one model that makes predictions by communicating with  models each mediating access to a repository of information or knowledge. A REML model is defined as   (; \n\n is called the prediction model and    denotes the  th information access model parameterized by   . Thus, to produce , the prediction model can interface with  information access models. Each    includes a collection or repository   that is available-through an information access model-to the prediction model. This repository could be composed of natural language documents-as with text retrieval-or some other indexed representation. As such,   s reflect a large set of parameters available to the model that can be leveraged ad hoc, as with many non-parametric and lazy learning techniques. The goal of retrieval-enhanced supervised learning models is to minimize the empirical risk, \n\nwhere L is a loss function for each training instance.",
            "score": 0.3908534054751986,
            "section_title": "RETRIEVAL-ENHANCED MACHINE LEARNING",
            "char_start_offset": 8201,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 43
                },
                {
                    "start": 44,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 359
                },
                {
                    "start": 362,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 619
                },
                {
                    "start": 622,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1310
                },
                {
                    "start": 1313,
                    "end": 1367
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.455078125
        },
        {
            "corpus_id": "269740933",
            "title": "A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models",
            "text": "Retrieval-augmented generation (RAG), a cutting-edge AI technique, has achieved remarkable success across various applications, including recommendation, molecule generation, protein representation, and software engineering, owing to the potent capabilities of retrieval in providing supplementary information to enhance generation performance.Recently, increasing efforts have been made to alleviate the limitations of large language models (LLMs), such as hallucination and out-of-date internal knowledge, by leveraging retrieval to provide the latest auxiliary information and teaching LLMs to harness the retrieved external knowledge.With the rapid advancements in retrieval-augmented large language models (RA-LLMs), there is a pressing need for a comprehensive and systematic overview.To bridge this gap, in this paper, we comprehensively review the RA-LLMs from the perspectives of morel architecture, training strategy, and application area, providing researchers with an in-depth understanding.Moreover, since the studies of RA-LLMs are still in the early stage, we also discuss the current limitations and several potential research directions for future research.",
            "score": 0.3908534054751986,
            "section_title": "CONCLUSION",
            "char_start_offset": 60360,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 344
                },
                {
                    "start": 344,
                    "end": 638
                },
                {
                    "start": 638,
                    "end": 791
                },
                {
                    "start": 791,
                    "end": 1003
                },
                {
                    "start": 1003,
                    "end": 1174
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.654296875
        },
        {
            "corpus_id": "21012910",
            "title": "Modelling Word Burstiness in Natural Language: A Generalised Polya Process for Document Language Models in Information Retrieval",
            "text": "This paper has introduced a family of statistical language models inspired by a classic urn model. We have shown that it is the replacement matrix M that defines the dynamics of the model. We have implemented a variant of the model which models burstiness in a term-specific manner. We have shown that the parameters of the model can be estimated from data using sampling techniques. Furthermore, we have incorporated the new language model into a retrieval framework and shown that retrieval effectiveness improves significantly over a highly competitive baseline language model. Although our experiments are conducted on small test collections (because parameter estimation is computationally expensive), the results are promising. We believe that this is first paper that deals with term-specific burstiness in such a principled probabilistic manner. \n\non Research and Development in Information Retrieval, SIGIR '96, pages 187-195, New York, NY, USA, 1996. ACM. \n\n[10] Jay M. Ponte and W. Bruce Croft. A language modeling approach to information retrieval.",
            "score": 0.3908534054751986,
            "section_title": "Summary",
            "char_start_offset": 19164,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 853
                },
                {
                    "start": 856,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 965
                },
                {
                    "start": 968,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1060
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.931640625
        },
        {
            "corpus_id": "267069067",
            "title": "In-context Learning with Retrieved Demonstrations for Language Models: A Survey",
            "text": "These include the nature of the retriever-ranging from general off-the-shelf models to finely-tuned, domain-specific variants-the source and diversity of the retrieval corpus, the retriever's objectives (focusing on either similarity or diversity), and the strategies for integrating multiple demonstrations. Over the past two years, numerous and sometimes concurrent works have studied RetICL each with different terminology and with variations in problem definition and subsequent methodologies, making it difficult to comprehend the current state of research and practice in RetICL, especially for newcomers to the field. In this comprehensive survey, we meticulously analyze 22 seminal papers in the field of RetICL, as detailed in Table 1, and provide a categorization of their main building blocks (See Figure 1). Our work not only provides a thorough synthesis of existing research but also underscores the areas where RetICL significantly surpasses previous ICL methods, and illuminates many paths forward for future innovations in this area, thus serving as a critical resource for ICL. \n\n2 Few-shot In-context Learning for Language Models Language models (LMs) (Zhao et al., 2023;Rosenfeld, 2000) are probabilistic models that assign probabilities to sequences of words and are essential components in many tasks. Let s represent a sequence of words (e.g., a sentence) and w 1 , w 2 , . . . , w n represent the tokens in the sequence. Based on the chain rule, the probability p(s) can be decomposed into the following product of probabilities: where each element in the product corresponds to the probability of a token given the previous tokens. Based on the above decomposition, an LM can be constructed by learning the probability of the next token given the previous ones. \n\nEarlier LMs were mostly based on N-gram models which are based on the Markovian assumption that the next token only depends on the recent context (Jurafsky, 2021). Based on this assumption, p(w k | w 1 , . . .",
            "score": 0.3908534054751986,
            "section_title": "Introduction",
            "char_start_offset": 2029,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 1095
                },
                {
                    "start": 1098,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1400
                },
                {
                    "start": 1401,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1786
                },
                {
                    "start": 1789,
                    "end": 1952
                },
                {
                    "start": 1953,
                    "end": 1998
                }
            ],
            "ref_mentions": [
                {
                    "start": 1190,
                    "end": 1206,
                    "matchedPaperCorpusId": "10959945"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9560546875
        },
        {
            "corpus_id": "30316671",
            "title": "Computing Word Semantic Relatedness for Question Retrieval in Community Question Answering",
            "text": "Given a user question q and a collection of answered questions {q 1 , q 2 , . . . , q n }, the task of question retrieval is to rank q i according to the similarity score sim(q, q i ). \n\nThe question retrieval model we use in this paper is based on the language modeling (LM) framework for IR [13]. Under the LM framework, sim(q, q i ) can be modeled as the probability of the document language model D built from q i generating q. If we assume that words occur independently, sim(q, q i ) may be calculated as: \n\nTo avoid zero probabilities, a mixture between a document-specific multinomial distribution and a multinomial distribution estimated from the entire document collection is used in practice: \n\nwhere 0 < \u03bb < 1, and C refers to a language model derived from the entire Q&A archive. P(w|D) and P(w|C) are calculated using maximum likelihood estimation (MLE). \n\nTo handle the word-mismatch problem, we assume that the generation of q follows a Markov process. First, a word t is chosen randomly from an original question q i , according to the distribution estimated from q i . Then, a word in q is generated based on the chosen word t, according to semantic relatedness among the words. Assuming that semantic relatedness can be represented by a conditional probability distribution R(\u2022|\u2022) between words, the probability of choosing a word w as a representative of D is: \n\nThis question retrieval model is basically equivalent to the one used in [6], except that the translation model is replaced with R(w|t).",
            "score": 0.3908534054751986,
            "section_title": "The Question Retrieval Model",
            "char_start_offset": 4864,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 82,
                    "end": 184
                },
                {
                    "start": 187,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 511
                },
                {
                    "start": 514,
                    "end": 703
                },
                {
                    "start": 706,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 868
                },
                {
                    "start": 871,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1086
                },
                {
                    "start": 1087,
                    "end": 1196
                },
                {
                    "start": 1197,
                    "end": 1380
                },
                {
                    "start": 1383,
                    "end": 1519
                }
            ],
            "ref_mentions": [
                {
                    "start": 293,
                    "end": 297,
                    "matchedPaperCorpusId": "2759772"
                },
                {
                    "start": 1456,
                    "end": 1459,
                    "matchedPaperCorpusId": "1208986"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87060546875
        },
        {
            "corpus_id": "269983203",
            "title": "LIRE: listwise reward enhancement for preference alignment",
            "text": "In this section, we reformulate the preference alignment problem and introduce our LIRE framework.Firstly, we assume a set of queries\n\nNext, we omit the superscript of (i) for clarity.During training, we aim to learn a language model parameterized by \u03b8, which generates responses of better alignment with human preferences.First, we define a set of token prediction probabilities conditioned on x as P \u03c0 \u03b8 (y j,k |x) \u2208 R L\u00d7V , where L is the sequence length and V the vocabulary size.The probability of the sentence y j with K tokens takes the form in an autoregressive nature:\n\nTo exploit the diversity present in different responses, we reformulate the response probability distribution against the entire response set A as:\n\n,\n\n(5) where T is a temperature parameter to control the smoothness of the probability distribution.Equation 5 can be regarded as an updated approximation of the underlying response (trajectory) distribution, and we next derive the listwise loss as:\n\nwhere q(\u2022) denotes the distribution of the queries.In practice, we apply softmax to the reward scores of a single query due to its property of translation invariance.By doing so we mitigate the influence of different reward scales and maintain stable training parameter settings.Next, to develop a general perception of what the model learns through the process, we illustrate the gradient of J(\u03b8), whose derivation process can be found in Appendix A.1: (y|x) is the normalized gradient of model predictions, multiplied by a demeaned reward score.These demeaned rewards act as a weighting mechanism that encourages responses with higher scores while depressing those with lower rewards.With Equation 7 exhibiting substantial differences with Equation 2, we have a better view of how the LIRE objective is built upon and improved over the PG theorem in this setting.Difference with traditional PG objective.The LIRE objective is initially constructed under the PG Theorem but the two objectives still exhibit substantial differences as illustrated in Table 1.Please note that one have to perceive LIRE in a listwise/groupwise manner where all the responses for one query are taken as one \"sample\".",
            "score": 0.3908534054751986,
            "section_title": "The construction of the LIRE objective",
            "char_start_offset": 6780,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 98,
                    "end": 133
                },
                {
                    "start": 135,
                    "end": 184
                },
                {
                    "start": 184,
                    "end": 323
                },
                {
                    "start": 323,
                    "end": 484
                },
                {
                    "start": 484,
                    "end": 577
                },
                {
                    "start": 579,
                    "end": 726
                },
                {
                    "start": 728,
                    "end": 729
                },
                {
                    "start": 731,
                    "end": 828
                },
                {
                    "start": 828,
                    "end": 977
                },
                {
                    "start": 979,
                    "end": 1030
                },
                {
                    "start": 1030,
                    "end": 1145
                },
                {
                    "start": 1145,
                    "end": 1258
                },
                {
                    "start": 1258,
                    "end": 1526
                },
                {
                    "start": 1526,
                    "end": 1665
                },
                {
                    "start": 1665,
                    "end": 1844
                },
                {
                    "start": 1844,
                    "end": 1885
                },
                {
                    "start": 1885,
                    "end": 2037
                },
                {
                    "start": 2037,
                    "end": 2175
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.309326171875
        },
        {
            "corpus_id": "7742063",
            "title": "Bridging Lexical Gaps between Queries and Questions on Large Online Q&A Collections with Compact Translation Models",
            "text": "Obviously, we need to build a translation model in advance. Usually the IBM Model 1, developed in the statistical machine translation field (Brown et al., 1993), is used to construct translation models for retrieval purposes in practice. Specifically, given a number of parallel strings, the IBM Model 1 learns the translation probability from a source word s to a target word t as: \n\nwhere \u03bb s is a normalization factor to make the sum of translation probabilities for the word s equal to 1, N is the number of parallel string pairs, and J i is the ith parallel string pair. c(t|s; J i ) is calculated as: \n\nwhere {s 1 , . . . , s n } are words in the source text in J i . f req t,J i and f req s,J i are the number of times that t and s occur in J i , respectively. \n\nGiven the initial values of T (t|s), Equations ( 5) and ( 6) are used to update T (t|s) repeatedly until the probabilities converge, in an EM-based manner. \n\nNote that the IBM Model 1 solely relies on word co-occurrence statistics obtained from parallel strings in order to learn translation probabilities. This implies that if parallel strings have unimportant words, a resulted translation model based on IBM Model 1 may contain unimportant words with nonzero translation probabilities. \n\nWe alleviate this drawback by eliminating unimportant words from parallel strings, avoiding them from being included in the conditional translation probability distribution. This naturally induces the construction of compact translation models.",
            "score": 0.3908534054751986,
            "section_title": "IBM Translation Model 1",
            "char_start_offset": 5866,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 59
                },
                {
                    "start": 60,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 382
                },
                {
                    "start": 385,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 606
                },
                {
                    "start": 609,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 767
                },
                {
                    "start": 770,
                    "end": 925
                },
                {
                    "start": 928,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1258
                },
                {
                    "start": 1261,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1505
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4951171875
        },
        {
            "corpus_id": "237106778",
            "title": "Rethinking search",
            "text": "This paper envisions an ambitious research direction that doubles down on the synthesis between modern IR and NLP to deliver on the long-promised goal of providing human expert quality answers to information needs. Specifically, the paper makes the case for developing retrieval systems that combine the best elements of document retrieval systems and pre-trained language models. To accomplish this, a so-called model-based information retrieval framework is proposed that breaks away from the traditional index-retrieve-then-rank paradigm by encoding the knowledge contained in a corpus in a consolidated model that replaces the indexing, retrieval, and ranking components of traditional systems. It was argued that if successful, such a consolidated model can be used to solve a wide range of tasks (via multi-task learning), can easily adapt to new low resource tasks and corpora (via zero-and few-shot learning), and can be used to synthesize high quality responses that go well beyond what today's search and question answering systems are capable of. \n\nThere are a number of interesting and difficult research and engineering challenges that must be solved before the envisioned system can be realized. These challenges span the IR, NLP, and machine learning research disciplines, and will require interdisciplinary research to be successful. Some of the major challenges include modeling (moving from LMs to corpus model), training (pre-training objectives, fine-tuning task definitions), response generation (authoritativeness, bias mitigation), and scalability (indexing and serving).",
            "score": 0.39071735111572486,
            "section_title": "Conclusions",
            "char_start_offset": 51028,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 1057
                },
                {
                    "start": 1060,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1349
                },
                {
                    "start": 1350,
                    "end": 1594
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7470703125
        },
        {
            "corpus_id": "265033839",
            "title": "CAME: Competitively Learning a Mixture-of-Experts Model for First-stage Retrieval",
            "text": "Motivated by the above, we build an MoE retrieval model consisting of multiple representative matching experts, and propose a novel mechanism to Competitively leArn the MoE model, named as CAME. Specifically, we include lexical, local, and global retrievers in a multi-path network architecture with shared bottom layers and top MoE layers. The shared bottom layers aim to extract common syntactic and semantic representations for all the experts. The MoE layers consist of three experts with different model architectures to capture pattern-specific features. To guide the component experts in CAME to specialize on certain types of samples, we competitively learn the MoE retriever in two phases. First, in the standardized learning stage, each expert is trained equally to develop the relevance matching capability and prepare for specialization. Second, in the specialized learning stage, the component experts compete with each other on every training instance, and they are trained proportionally to their relative performance among all the experts. In this way, each sample is only used to update the experts that perform decently but has no or little impact on other experts. During inference, each expert estimates the relevance score from its perspective to contribute to the final results. Noted that, in contrast to the classical MoE models that pick experts spontaneously for each input sample, our method establishes more explicit connections between the experts and samples via their relative ranking performance. From the above, we can readily urge each expert to fit the case it is skilled at and fully unleash the advantages of different model architectures. \n\nWe evaluate the effectiveness of the proposed model on three representative retrieval benchmark datasets, i.e., MS MARCO [34], TREC Deep Learning Track [3], and Natural Questions [24]. The empirical results show that CAME can outperform all the baselines including various state-of-the-art single-model retrievers and ensemble retrievers significantly. In addition, we conduct comprehensive analyses on how the model components and learning mechanism impact retrieval performance. It shows that employing multi-types of retrieval experts can capture diverse relevance patterns, and the competitive learning strategy is essential to facilitate the experts to learn their designated patterns, which together boost the retrieval performance. To sum up, our contributions include: \n\n\u2022 We propose a Mixture-of-Experts retrieval model that can orchestrate various types of models to capture diverse relevance patterns.",
            "score": 0.3887723875612886,
            "section_title": "Matching Scope: Verbosity Hypothesis vs Scope Hypothesis query treatment of varicose veins in legs doc",
            "char_start_offset": 5290,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1183
                },
                {
                    "start": 1184,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1676
                },
                {
                    "start": 1679,
                    "end": 1863
                },
                {
                    "start": 1864,
                    "end": 2031
                },
                {
                    "start": 2032,
                    "end": 2159
                },
                {
                    "start": 2160,
                    "end": 2417
                },
                {
                    "start": 2418,
                    "end": 2455
                },
                {
                    "start": 2458,
                    "end": 2591
                }
            ],
            "ref_mentions": [
                {
                    "start": 1800,
                    "end": 1804,
                    "matchedPaperCorpusId": "1289517"
                },
                {
                    "start": 1858,
                    "end": 1862,
                    "matchedPaperCorpusId": "86611921"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9404296875
        },
        {
            "corpus_id": "256389465",
            "title": "EmbedDistill: A Geometric Knowledge Distillation for Information Retrieval",
            "text": "Neural models for information retrieval (IR) are increasingly used to model the true ranking function in various applications, including web search [Mitra and Craswell, 2018], recommendation [Zhang et al., 2019], and question-answering (QA) [Chen et al., 2017]. Notably, the recent success of Transformers [Vaswani et al., 2017]-based pre-trained language models [Devlin et al., 2019, Liu et al., 2019, Raffel et al., 2020] on a wide range of natural language understanding tasks has also prompted their utilization in IR to capture query-document relevance [see, e.g., Dai and Callan, 2019b, MacAvaney et al., 2019a, Nogueira and Cho, 2019, Lee et al., 2019, Karpukhin et al., 2020a]. A typical IR system comprises two stages: (1) A retriever first selects a small subset of potentially relevant candidate documents (out of a large collection) for a given query; and (2) A re-ranker then identifies a precise ranking among the candidates provided by the retriever. Dual-encoder (DE) models are the de-facto architecture for retrievers [Lee et al., 2019, Karpukhin et al., 2020a]. Such models independently embed queries and documents into a common space, and capture their relevance by simple operations on these embeddings such as the inner product. This enables offline creation of a document index and supports fast retrieval during inference via efficient maximum inner product search implementations [Guo et al., 2020, Johnson et al., 2021], with online query embedding generation primarily dictating the inference latency. Cross-encoder (CE) models, on the other hand, are preferred as re-rankers, owing to their excellent performance [Nogueira and Cho, 2019, Dai and Callan, 2019a, Yilmaz et al., 2019]. A CE model jointly encodes a query-document pair while enabling early interaction among query and document features. Employing a CE model for retrieval is often infeasible, as it would require processing a given query with every document in the collection at inference time.",
            "score": 0.388552541238673,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1711
                },
                {
                    "start": 1712,
                    "end": 1828
                },
                {
                    "start": 1829,
                    "end": 1986
                }
            ],
            "ref_mentions": [
                {
                    "start": 241,
                    "end": 260,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 306,
                    "end": 328,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 363,
                    "end": 383,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 401,
                    "end": 423,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 591,
                    "end": 616,
                    "matchedPaperCorpusId": "131776819"
                },
                {
                    "start": 658,
                    "end": 684,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1053,
                    "end": 1079,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1406,
                    "end": 1423,
                    "matchedPaperCorpusId": "218614141"
                },
                {
                    "start": 1423,
                    "end": 1446,
                    "matchedPaperCorpusId": "926364"
                },
                {
                    "start": 1665,
                    "end": 1688,
                    "matchedPaperCorpusId": "162168864"
                },
                {
                    "start": 1688,
                    "end": 1710,
                    "matchedPaperCorpusId": "202635721"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9638671875
        },
        {
            "corpus_id": "153311735",
            "title": "Multilingual Factor Analysis",
            "text": "Furthermore, we show how our model extends to more than two languages within the generative framework which is something that previous alignment models are not naturally suited to, instead resorting to combining bilingual models with a pivot as in (Ammar et al., 2016). \n\nAdditionally the general benefit of the probabilistic setup as discussed in (Tipping and Bishop, 1999) is that it offers the potential to extend the scope of conventional alignment methods to model and exploit linguistic structure more accurately. An example of such a benefit could be modelling how corresponding word translations can be generated by more than just a single latent concept. This assumption can be encoded by a mixture of Factor Analysers (Ghahramani et al., 1996) to model word polysemy in a similar fashion to (Athiwaratkun and Wilson, 2017), where mixtures of Gaussians are used to reflect the different meanings of a word. \n\nThe main contribution of this work is the application of a well-studied graphical model to a novel domain, outperforming previous approaches on word and sentence-level translation retrieval tasks. We put the model through a battery of tests, showing it aligns embeddings across languages well, while retaining performance on monolingual word-level and sentence-level tasks. Finally, we apply a natural extension of this model to more languages in order to align three languages into a single common space.",
            "score": 0.38842640639363213,
            "section_title": "Introduction",
            "char_start_offset": 1798,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 269
                },
                {
                    "start": 272,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 915
                },
                {
                    "start": 918,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1423
                }
            ],
            "ref_mentions": [
                {
                    "start": 348,
                    "end": 374,
                    "matchedPaperCorpusId": "15538672"
                },
                {
                    "start": 801,
                    "end": 832,
                    "matchedPaperCorpusId": "20269438"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56201171875
        },
        {
            "corpus_id": "8322221",
            "title": "Dependencies: Formalising Semantic Catenae for Information Retrieval",
            "text": "current modus vivendi of semantic processing in information retrieval is inadequate and should be revised;\n\n(iii) recommendations on designing a new modus operandi that bridges the two extremes of shallow versus deep semantic processing, leading to more accurate and more expressive information retrieval inferences.\n\nThe common underlying objective in the body of work included in this dissertation is to model the dependence of textual constituents on three different levels of semantic analysis: lexical, discourse, and cognitive. For each of the above three levels, the type and strength of the dependence conjoining the constituent parts of text is examined in a principled manner, and different models are developed for processing the corresponding semantic catenae (of words, sentences, or concepts) in the context of information retrieval. These models capture part of the information loss that takes place when reasoning about text computationally by dismantling it into individual words, sentences, or concept representations. Collectively, these models replace the assumption of independence with non-complex representations of lexical, discourse, and cognitive dependence, using principles from graph theory, probability theory, logic and statistics.\n\nA total of nine models of semantic dependence for information retrieval are presented in this dissertation (outlined in Table 1). The results contributed and conclusions drawn by each model are discussed next, separately for lexical, discourse and cognitive semantics.\n\nTab. 1: The nine models of semantic dependence presented in this dissertation. The numbers inside square brackets point to the bibliographic references of the articles where each model was published.",
            "score": 0.3882951502843339,
            "section_title": "Semantic Dependence in Information Retrieval",
            "char_start_offset": 7495,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.31396484375
        },
        {
            "corpus_id": "235097215",
            "title": "Understanding the Impact of Evidence-Aware Sentence Selection for Fact Checking",
            "text": "Pointwise: Our model is similar to the one described in the work of Soleimani et al. (2020). We use a BERT-based model (Devlin et al., 2019) to obtain the representation of the input sentences. \n\nFor training, we use the cross-entropy loss and the input to our model is the claim along with an evidence sentence. The goal of the sentence retrieval component paired with the pointwise loss is to predict whether a candidate evidence sentence is an evidence or not for a given claim. Thus, the problem of sentence retrieval is framed as a binary classification task.",
            "score": 0.38819451270362215,
            "section_title": "Baseline",
            "char_start_offset": 3333,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 93,
                    "end": 193
                },
                {
                    "start": 196,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 564
                }
            ],
            "ref_mentions": [
                {
                    "start": 68,
                    "end": 91,
                    "matchedPaperCorpusId": "203837038"
                },
                {
                    "start": 119,
                    "end": 140,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53466796875
        },
        {
            "corpus_id": "233263378",
            "title": "Developing Software Signature Search Engines Using Paragraph Vector Model: A Triage Approach for Digital Forensics",
            "text": "To design our software signature search engine, we use the paragraph vector model. In traditional information retrieval techniques, each term/word is displayed as a one-hot encoded vector [41]. Each document or query is displayed as a vector in N -dimensional vector space, in which N is the number of terms/words in the corpus of documents. There are several ways to calculate the weight of a term in a document, and one of the most common is the term frequency-inverse document frequency (TF-IDF) [42]. \n\nIn these frequency-based information retrieval methods, the vector of each word is independent of the other words, i.e., the similarity of one-hot encoded vectors is zero. We need representations for word vectors that consider the context and semantic of words, so that word vectors with similar contexts are closer together in the vector space. Mathematically speaking, the cosine of the angle between word vectors with similar context should be close to one. \n\nRecent NLP models learn meaningful vector representations for words. These vectors are often known as word embeddings or distributed word representations [43]. There are neural network solutions for learning word embedding, which preserve the syntactic and semantic relations between words. Two of the most popular neural network models for training word embedding are continuous bag-of-words (CBOW) and skip-gram [44]. These two models are known as word2vec. \n\nThe Skip-gram model tries to predict the context words given a target word. Therefore, given a sequence of training words w 1 , w 2 , \u2022 \u2022 \u2022 , w N , the goal of the Skip-gram model is to maximize the log probability in (1). \n\nwhere N is the number of training words, and c is the size of the sliding window that determines the number of context words. \n\nInversely, the continuous bag-of-words model tries to predict the target word given the context words. More formally, given a sequence of training words w 1 , w 2 , \u2022 \u2022 \u2022 , w N , the goal of the CBOW model is to maximize the average log probability in (2). \n\n( \n\nThe word2vec models formulate the probability p (w O |w I ) using the softmax function represented by (3).",
            "score": 0.3874539122199067,
            "section_title": "III. PRELIMINARIES: PARAGRAPH VECTOR MODEL",
            "char_start_offset": 14236,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 82
                },
                {
                    "start": 83,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 504
                },
                {
                    "start": 507,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 967
                },
                {
                    "start": 970,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1260
                },
                {
                    "start": 1261,
                    "end": 1389
                },
                {
                    "start": 1390,
                    "end": 1429
                },
                {
                    "start": 1432,
                    "end": 1507
                },
                {
                    "start": 1508,
                    "end": 1654
                },
                {
                    "start": 1657,
                    "end": 1782
                },
                {
                    "start": 1785,
                    "end": 1887
                },
                {
                    "start": 1888,
                    "end": 2041
                },
                {
                    "start": 2044,
                    "end": 2045
                },
                {
                    "start": 2048,
                    "end": 2154
                }
            ],
            "ref_mentions": [
                {
                    "start": 1124,
                    "end": 1128,
                    "matchedPaperCorpusId": "53590711"
                },
                {
                    "start": 2037,
                    "end": 2040,
                    "matchedPaperCorpusId": "6537168"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.662109375
        },
        {
            "corpus_id": "276928453",
            "title": "MultiConIR: Towards multi-condition Information Retrieval",
            "text": "In recent years, artificial datasets generated by LLMs have become a common practice for training and evaluating retrieval models (Su et al., 2024;Lee et al., 2024;Weller et al., 2024a). For instance, E5-Mistral (Wang et al., 2024a) rely entirely on LLM-generated datasets for fine-tuning. While this approach can significantly expand training corpora, prior studies have highlighted its potential drawbacks, including introducing inherit linguistic biases of the underlying LLMs (Shumailov et al., 2024), potentially constraining the retrieval model's performance and generalizability. Furthermore, purely artificial data often lacks the contextual richness and complexity found in real-world retrieval scenarios (Li et al., 2023c;Wang et al., 2024b), making it difficult to capture the actual needs of users' queries accurately. \n\nDuring our dataset construction, we observed similar issues. When using LLM-generated transformations to modify positive documents into hard negatives, the model often restructured expressions to fit",
            "score": 0.38736565522485505,
            "section_title": "E Findings In Constructing MultiCon Dataset E.1 The Use Of LLM-generated Data in Retrieval",
            "char_start_offset": 33385,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 830
                },
                {
                    "start": 833,
                    "end": 893
                },
                {
                    "start": 894,
                    "end": 1032
                }
            ],
            "ref_mentions": [
                {
                    "start": 212,
                    "end": 232,
                    "matchedPaperCorpusId": "266693831"
                },
                {
                    "start": 480,
                    "end": 504,
                    "matchedPaperCorpusId": "271448069"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74951171875
        },
        {
            "corpus_id": "269791442",
            "title": "A Tale of Two Languages: Large-Vocabulary Continuous Sign Language Recognition from Spoken Language Supervision",
            "text": "Retrieval performance.As Tab. 4 shows, our joint CSLR 2 model outperforms a standard InfoNCE [56] baseline for retrieval on all reported metrics, with gains in R@1 for both T2V and V2T of almost 10 points.On the more challenging SENT-TEST gallery of 20k video-subtitle pairs, our CSLR 2 model achieves a Top-5 accuracy of 45.2% for T2V retrieval.We observe that for cases where the target sentence is not the Top-1, the top-retrieved results usually exhibit semantic similarities with the correct sentence, with multiple common words (see the qualitative examples in the supp.mat.).",
            "score": 0.38721475119694376,
            "section_title": "Comparison to the state of the art",
            "char_start_offset": 33375,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 22
                },
                {
                    "start": 22,
                    "end": 205
                },
                {
                    "start": 205,
                    "end": 346
                },
                {
                    "start": 346,
                    "end": 576
                },
                {
                    "start": 576,
                    "end": 582
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.42822265625
        },
        {
            "corpus_id": "252735129",
            "title": "Content-based Search for Deep Generative Models",
            "text": "In this section, we develop a retrieval framework for deep generative models. When a user specifies an image, sketch, or text query, we would like to retrieve a model that best matches the query. We denote a collection of  models by { 1 ,  2 , . . . ,   } and the user query by , and we assume a uniform prior over models (i.e.,  \u223c unif{ 1 ,  2 , . . . ,   }). Every model  captures a distribution  ( | ) over images . While prior retrieval methods [Manning et al. 2010;Smeulders et al. 2000] search for single instances, we aim to construct a method for retrieving distributions. \n\nTo achieve this, we introduce a probabilistic formulation for generative model retrieval. Figure 2 shows an overview of our approach. Our formulation is general to different query modalities and various types of generative models, and can be extended to different algorithms. In Section 3.1, we derive our model retrieval formulation based on a Maximum Likelihood Estimation (MLE) objective based on pre-trained deep learning features. In Section 3.2, we further propose a contrastive learning method to adapt features to different query modalities and model search task. We present our model retrieval algorithms for an image, a text, and a sketch query, respectively. In Sections 4 and 5, we discuss our new benchmark and user interface for model search.",
            "score": 0.38704866882156796,
            "section_title": "METHODS",
            "char_start_offset": 9813,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 77
                },
                {
                    "start": 78,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 580
                },
                {
                    "start": 583,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1252
                },
                {
                    "start": 1253,
                    "end": 1339
                }
            ],
            "ref_mentions": [
                {
                    "start": 449,
                    "end": 470,
                    "matchedPaperCorpusId": "262553219"
                },
                {
                    "start": 470,
                    "end": 491,
                    "matchedPaperCorpusId": "262234827"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59326171875
        },
        {
            "corpus_id": "271769496",
            "title": "Retrieval-Augmented Knowledge Integration into Language Models: A Survey",
            "text": "In the following, we briefly introduce retrievalaugmented generation (RAG) and then define and formulate retrieval-augmented knowledge integration (RAKI). Retrieval-augmented generation is first proposed by Lewis et al. (2020), where world knowledge is retrieved from a vector index constructed over Wikipedia articles and then sent to a seq2seq (Sutskever et al., 2014) model for generation. More formally, given an input-output pair (x, y) from a generation task, retrieval-augmented generation aims to generate the target output y conditioned on the input x and an accessible document set Z for reference (Lewis et al., 2020;Yu, 2022). Retrieval-augmented knowledge integration Baek et al. (2023b) uses the term knowledge augmentation to address the practice of retrieving knowledge for language models. In this work, we adopt the term retrieval-augmented knowledge integration (RAKI) for better clarification, since we would like to avoid confusion with non-retrieval based knowledge-integration methods, as mentioned in Section 1, that involve heavy pre-training or fine-tuning. RAKI also follows the first-retrievethen-infer paradigm as in RAG, and we identify the differences as follows: (1) RAG, by its nature, deals with generation tasks, while RAKI is compatible with classification tasks as well, i.e. y being a class label (Yu et al., 2023). (2) RAG typically retrieves general documents for generation, while RAKI further specifies certain knowledge sources (e.g. an external knowledge graph) as retrieval base for better factuality (Baek et al., 2023b). Definition The setting of RAKI can then be formulated as follows: Given a user input x from task T and a specific knowledge source (to be discussed in Section 3), we denote y as target output and K as whole knowledge from the source. RAKI consists of two components (Baek et al., 2023b): \n\n(1) a retriever R which selects a subset K \u2032 from knowledge K: \n\nwhere normally |K \u2032 | \u226a |K| in this retrieval step; \n\n(2) a language model M targeted for task T .",
            "score": 0.38704866882156796,
            "section_title": "Preliminaries",
            "char_start_offset": 3029,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1567
                },
                {
                    "start": 1568,
                    "end": 1801
                },
                {
                    "start": 1802,
                    "end": 1855
                },
                {
                    "start": 1858,
                    "end": 1920
                },
                {
                    "start": 1923,
                    "end": 1974
                },
                {
                    "start": 1977,
                    "end": 2021
                }
            ],
            "ref_mentions": [
                {
                    "start": 207,
                    "end": 226,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 346,
                    "end": 370,
                    "matchedPaperCorpusId": "7961699"
                },
                {
                    "start": 608,
                    "end": 628,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 628,
                    "end": 637,
                    "matchedPaperCorpusId": "250391000"
                },
                {
                    "start": 681,
                    "end": 700,
                    "matchedPaperCorpusId": "264306280"
                },
                {
                    "start": 1335,
                    "end": 1352,
                    "matchedPaperCorpusId": "266176750"
                },
                {
                    "start": 1546,
                    "end": 1566,
                    "matchedPaperCorpusId": "264306280"
                },
                {
                    "start": 1834,
                    "end": 1854,
                    "matchedPaperCorpusId": "264306280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90869140625
        },
        {
            "corpus_id": "64235020",
            "title": "A Generalized Dependency Tree Language Model for SMT",
            "text": "The idea of capturing structure in a language model has been around since the late 90's (Chelba 1997), particularly in the speech recognition community. Such tree-based, or 'structured' language models have mainly considered only limited word histories, such as ancestors (Gubbins and Vlachos 2013) or specific parent/sibling relationships (Shen, Xu, and Weischedel 2008), however more recent attempts have started to define more general syntactic word histories (Sidorov, Velasquez, Stamatatos, Gelbukh, and Chanona-Hern\u00e1ndez 2013). The beginnings of such generalized approaches can be traced back to 'arbori-context' trees (Mori, Nishimura, and Itoh 2001), which are designed to select optimal partial histories. \n\nOther effective syntactic approaches in recent years have included a bilingual language model (Mari\u00f1o, Banchs, Crego, de Gispert, Lambert, Fonollosa, and Costa-juss\u00e0 2006;Niehues, Herrmann, Vogel, and Waibel 2011) enhanced with some dependency information (Garmash and Monz 2014), specifically the POS tags of parent/grandparent and closest left/right siblings, and modeling a generative dependency structure on top of a classic n-gram language model (Ding and Yamamoto 2014). \n\nWhile not directly designed as 'syntax-based' language models, approaches based on neural networks have also been shown to be effective at capturing a more general word history than classic n-gram models. Such approaches include feed-forward (Bengio, Ducharme, Vincent, and Janvin 2003;Schwenk 2007) and recurrent (Mikolov, Karafi\u00e1t, Burget, Cernock\u00fd, and Khudanpur 2010) neural network language models and more recently LSTM-based language models (Sundermeyer, Schl\u00fcter, and Ney 2012). The most recent approach at the time of writing considers a hybrid approach of syntactic and neural network components (Sennrich 2015). \n\nTwo major drawbacks of neural network based approaches are that it can be difficult to 'reverse engineer' the syntactic knowledge learned and that model training can struggle computationally on large data.",
            "score": 0.38704866882156796,
            "section_title": "Related Work",
            "char_start_offset": 3285,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 714
                },
                {
                    "start": 717,
                    "end": 1193
                },
                {
                    "start": 1196,
                    "end": 1400
                },
                {
                    "start": 1401,
                    "end": 1682
                },
                {
                    "start": 1683,
                    "end": 1818
                },
                {
                    "start": 1821,
                    "end": 2026
                }
            ],
            "ref_mentions": [
                {
                    "start": 88,
                    "end": 101,
                    "matchedPaperCorpusId": "3264519"
                },
                {
                    "start": 272,
                    "end": 298,
                    "matchedPaperCorpusId": "9205274"
                },
                {
                    "start": 463,
                    "end": 532,
                    "matchedPaperCorpusId": "723683"
                },
                {
                    "start": 625,
                    "end": 657,
                    "matchedPaperCorpusId": "14489914"
                },
                {
                    "start": 811,
                    "end": 888,
                    "matchedPaperCorpusId": "1821900"
                },
                {
                    "start": 888,
                    "end": 930,
                    "matchedPaperCorpusId": "7801816"
                },
                {
                    "start": 973,
                    "end": 996,
                    "matchedPaperCorpusId": "12428726"
                },
                {
                    "start": 1168,
                    "end": 1192,
                    "matchedPaperCorpusId": "18078648"
                },
                {
                    "start": 1438,
                    "end": 1482,
                    "matchedPaperCorpusId": "221275765"
                },
                {
                    "start": 1482,
                    "end": 1494,
                    "matchedPaperCorpusId": "207041403"
                },
                {
                    "start": 1510,
                    "end": 1567,
                    "matchedPaperCorpusId": "17048224"
                },
                {
                    "start": 1802,
                    "end": 1817,
                    "matchedPaperCorpusId": "15811826"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5537109375
        },
        {
            "corpus_id": "3262970",
            "title": "Factorization of Language Models through Backing-Off Lattices",
            "text": "Factorization of statistical language models is the task that we resolve the most discriminative model into factored models and determine a new model by combining them so as to provide better estimate to the most discriminative model event. For instance, a new model for trigram can be obtained by combining the factored models: a unigram model, a bigram model and a trigram model; a model for PPattachment (Collins & Brooks, 1995) can be obtained by considering both more discriminative models like Pr(1|is, revenue, f rom, research) 1 and less discriminative ones like Pr(1|is, f rom, research); a lexicalized parsing model can be approximated by combining a lexical dependency model and a syntactic structure model (Klein & Manning, 2002). The former two examples are usually called backing-off. \n\nTherefore, factorization of language models should answer two questions: how to factorize, and how to combine. Most of previous works on language modeling (Chen & Goodman, 1998) (Goodman, 2001) focus on sequential model event (such as n-gram), and thus need not to answer the first question because the sequential model event like n-gram gives a natural factorization order: an n-gram has exactly one type of (n-1)-gram to backoff. However, for nonsequential model event, we need to specify them both. \n\nIn this paper, we formulated a framework for language model factorization. We adopt a backing-off lattice to reflect parallel factorization and to define the paths along which a model is resolved into factored models; we use a mixture model to combine parallel paths in the lattice; and generalize Katz's backing-off method to integrate all the mixture models got by traversing the entire lattice. \n\nBased on this framework, we formulate two types of model factorizations that are used in natural language modeling. \n\nThe remainder of this paper is organized as follows, we first introduce the backing-off lattice, then explain the mixture model, next formulate the backing-off formula, next describe two types of model factorizations, and finally draw the conclusions.",
            "score": 0.38704866882156796,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 798
                },
                {
                    "start": 801,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1302
                },
                {
                    "start": 1305,
                    "end": 1379
                },
                {
                    "start": 1380,
                    "end": 1702
                },
                {
                    "start": 1705,
                    "end": 1820
                },
                {
                    "start": 1823,
                    "end": 2074
                }
            ],
            "ref_mentions": [
                {
                    "start": 407,
                    "end": 431,
                    "matchedPaperCorpusId": "543"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1578369140625
        },
        {
            "corpus_id": "254854373",
            "title": "Rethinking Label Smoothing on Multi-hop Question Answering",
            "text": "In the retrieval module, each question Q is typically accompanied by a set of M documents D 1 , D 2 . . . , D M , but only C, |C| << M (two in HotpotQA) are labeled as relevant to the question Q. We model the retrieval process as a binary classification task. Specifically, for each question-document pair, we generate an input by concatenating \"[CLS], question, [SEP], document, [SEP]\" in sequence. We then feed the [CLS] token output from the model into a linear classifier. L retrieve represents the crossentropy between the predicted probability and the gold label. In contrast to S2G (Wu et al., 2021), which employs a complex pairwise learning-to-rank loss, we opt for a simple binary cross-entropy loss, as it maintains high performance while being significantly more efficient. \n\nwhere \u0177retrieve i is the probability predicted by the model and y retrieve i is the ground-truth label. M is the number of provided documents. E means the expectation of all samples. \n\n(2)",
            "score": 0.38704866882156796,
            "section_title": "Retrieval Module",
            "char_start_offset": 9900,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 476
                },
                {
                    "start": 477,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 785
                },
                {
                    "start": 788,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 970
                },
                {
                    "start": 973,
                    "end": 976
                }
            ],
            "ref_mentions": [
                {
                    "start": 589,
                    "end": 606,
                    "matchedPaperCorpusId": "236428903"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.206298828125
        },
        {
            "corpus_id": "277621440",
            "title": "Unleashing the Power of LLMs in Dense Retrieval with Query Likelihood Modeling",
            "text": "Information retrieval (IR) contains two primary stages: retrieval and reranking [13,67]. Retrieval serves as the cornerstone of information retrieval and is critically important. Its objective is to retrieve relevant passages from a large-scale corpus in response to a query, thereby providing candidate passages for the subsequent reranking stage. In the early stages of retrieval, the focus was primarily on lexical matching between the terms of the query and the passage, with methods such as BM25 [34] and query likelihood (QL) model [46]. However, with the advent of pre-trained language models (PLMs) such as BERT [11], representing passages or queries as dense vectors has gradually become the mainstream approach. These methods typically employ two separate encoders to represent the query and passage and are referred to as dual-encoder models. \n\nLarge language models (LLMs) are being widely applied across various fields [27,54,61], and garnering increasing attention for their application in retrieval tasks [25,39,51]. Unlike bidirectional attention mechanisms in encoder-style pre-trained language models (PLMs) such as BERT, LLMs are typically decoder-style models that employ unidirectional attention. The task of next token prediction allows LLMs to ingest large amounts of various types of data and thus gain more powerful semantic understanding capability. However, the unidirectional attention during modeling may lead to insufficient representation of global semantics, which is inferior to encoder-style PLMs. Although leveraging the superior semantic understanding ability of LLMs for retrieval looks appealing, it is challenging to do so. Recent studies have attempted to repeat passages as input during encoding [51], to use a bidirectional attention mechanism for encoding during relevance matching fine-tuning [4], or pre-train LLMs to strengthen the global semantic representation capabilities [33]. \n\nSince LLMs are decoder-based language models (LMs), it is natural to think of adapting them to retrieval according to the classical language modeling approach to IR [46], i.e., modeling the query likelihood (QL) given the LM of a document.",
            "score": 0.38704866882156796,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 853
                },
                {
                    "start": 856,
                    "end": 1031
                },
                {
                    "start": 1032,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1531
                },
                {
                    "start": 1532,
                    "end": 1662
                },
                {
                    "start": 1663,
                    "end": 1927
                },
                {
                    "start": 1930,
                    "end": 2169
                }
            ],
            "ref_mentions": [
                {
                    "start": 80,
                    "end": 84,
                    "matchedPaperCorpusId": "259076101"
                },
                {
                    "start": 84,
                    "end": 87,
                    "matchedPaperCorpusId": "233289894"
                },
                {
                    "start": 501,
                    "end": 505,
                    "matchedPaperCorpusId": "235366815"
                },
                {
                    "start": 538,
                    "end": 542,
                    "matchedPaperCorpusId": "2759772"
                },
                {
                    "start": 1024,
                    "end": 1027,
                    "matchedPaperCorpusId": "263908865"
                },
                {
                    "start": 1922,
                    "end": 1926,
                    "matchedPaperCorpusId": "271885989"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98095703125
        },
        {
            "corpus_id": "259076101",
            "title": "I3 Retriever: Incorporating Implicit Interaction in Pre-trained Language Models for Passage Retrieval",
            "text": "The performance of neural IR models, including retrievers and rerankers, have been significantly boosted by pre-trained language models (PLM), where various ways of leveraging PLM for IR are proposed. As illustrated in Figure 1, PLM-based IR models can be categorized into three types, i.e., dual-encoders, late-interaction encoders and cross-encoders, in terms of the interaction mechanism applied between query and passage. Overall, existing studies indicate that incorporating more interactions between queries and passages in a PLM-based IR method can improve relevance modeling, but it also comes at the cost of extra computational overhead. In the following, we further introduce the detailed structures of these models. Dual-encoder. Dual-encoders employ two PLM-based encoders to respectively encode the query and passage in a latent embedding space. The relevance score  (q, p) between query and passage is formulated as \n\nHere, Aggregate(\u2022) is usually implemented as a simple metric (e.g., dot-product) between query and passage vectors, which is computed by query and passage encoders (i.e., E  and E  ), respectively. The encoders are stacked transformer layers, where we fetch the representation of [CLS] token in the last layer as final query/passage vector. \n\nThe major merit of dual-encoders lies in its high efficiency. As the query and passage are decoupled at encoding, the passages in large corpus G can be pre-computed and cached offline. By doing this, substantial computational resources could be saved during the online inference for fast retrieval. However, the limitation is also apparent. The absence of interaction between the query and passage during their encoding leads to an inability to effectively capture complex relevance [22,25,65]. Cross-encoder. Cross-encoders are considered the most effective PLM-based IR method due to their early incorporation of querypassage interactions. It takes the concatenation of query and passage as input, and computes the relevance score as \n\nwhere \u2295 means the concatenation operation and E , is the PLM encoder. The FC is a fully-connected layer that transforms the [CLS] representation to a relevance score.",
            "score": 0.38704866882156796,
            "section_title": "PLM-based Retriever and Reranker",
            "char_start_offset": 12868,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 929
                },
                {
                    "start": 932,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1272
                },
                {
                    "start": 1275,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1573
                },
                {
                    "start": 1574,
                    "end": 1615
                },
                {
                    "start": 1616,
                    "end": 1769
                },
                {
                    "start": 1770,
                    "end": 1784
                },
                {
                    "start": 1785,
                    "end": 1916
                },
                {
                    "start": 1917,
                    "end": 2010
                },
                {
                    "start": 2013,
                    "end": 2082
                },
                {
                    "start": 2083,
                    "end": 2179
                }
            ],
            "ref_mentions": [
                {
                    "start": 1762,
                    "end": 1765,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 1765,
                    "end": 1768,
                    "matchedPaperCorpusId": "246828631"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91796875
        },
        {
            "corpus_id": "13916016",
            "title": "Modeling Selectional Preferences of Verbs and Nouns in String-to-Tree Machine Translation",
            "text": "Dependency Language Model. \n\nSennrich (2015) proposes a relational dependency language model (RDLM) for string-to-tree machine translation, which he trains using a feedforward neural network. \n\nFor a sentence S with symbols w 1 , w 2 , ...w n and dependency labels l 1 , l 2 , ...l n with l i the label of the incoming arc at position i, RDLM is defined as: P l (i) \u00d7 P w (i) \n\nwhere for each of q siblings and r ancestors of w i , h s and h a are their head words and l s and l a their dependency labels. The P w (i) distribution models similar information as our proposed feature SelAssoc. However we use h a (i) 1 , l i as context and consider only a subset of dependency labels: nsubj, nsubjpass, dobj, iobj, prep. The reduced context alleviates problems of data sparsity and is more reliably extracted at decoding time. The subset of dependency relations identify arguments for which predicates might exhibit selectional preferences. Our feature is different from RDLM \u2212 P w as it quantifies the amount of information that the predicate carries about the argument class filling a particular syntactic function. We hypothesize that such information is useful when translating arguments that appear less frequently in the training data but are prototypical for certain predicates. For example the triples (bus, drive, dobj) and (van, drive, dobj) have the following log posterior probabilities and SelAssoc scores: log P(bus | drive, dobj) = -5.44, log P(van | drive, dobj)= -5.58 and SelAssoc(bus, drive, dobj) = 0.0079, SelAssoc(van, drive, dobj) = 0.0103.",
            "score": 0.38704866882156796,
            "section_title": "Comparison with a Neural Relational",
            "char_start_offset": 12358,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 26
                },
                {
                    "start": 29,
                    "end": 191
                },
                {
                    "start": 194,
                    "end": 375
                },
                {
                    "start": 378,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1451
                },
                {
                    "start": 1452,
                    "end": 1561
                }
            ],
            "ref_mentions": [
                {
                    "start": 38,
                    "end": 44,
                    "matchedPaperCorpusId": "10550488"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2459716796875
        },
        {
            "corpus_id": "258676667",
            "title": "When Giant Language Brains Just Aren't Enough! Domain Pizzazz with Knowledge Sparkle Dust",
            "text": "As the field of artificial intelligence (AI) continues to advance at an unprecedented pace, Large Language Models (LLMs)1 (Brown et al., 2020;Lieber et al., 2021;Rae et al., 2021;Thoppilan et al., 2022;Smith et al., 2022) and Generative Pre-trained Transformers (GPT) (Radford et al., 2018;Brown et al., 2020) have emerged as breakthrough technologies, revolutionizing natural language processing (NLP) tasks. These powerful models have demonstrated remarkable capabilities in various use cases, ranging from AI assistants, chatbots, programming, gaming, question answering, recommendation systems, and retrieval, to knowledge discovery (Sandur et al., 2022). However, while LLMs (we use the same concept of LLMs and GPT as LLMs) offer immense potential, they are not standalone solutions (IBM, 2023), and their full effectiveness can be realized through synergies with other AI-powered technologies (Sandur et al., 2022) and tools (Lu et al., 2023). \n\nIn this paper, we will delve into the compelling use cases of LLMs (Radford et al., 2018;Brown et al., 2020), showcasing their capabilities and impact in different domains. We will highlight their strengths and limitations, and examine how they can be complemented by other cutting-edge AI technologies to achieve comprehensive and powerful solutions. Through detailed firsthand findings from our internal research projects, we will shed light on the synergistic potential of integrating LLMs with complementary AI-powered tools and techniques. \n\nLanguage modeling is a fundamental NLP task that was originally designed for the probability estimation of an input sequence (Brown et al., 1992;Pauls & Klein, 2011;Jing & Xu, 2019). The assumption of LM is based on the probability of the co-occurrence of tokens observed on a huge corpus. Given an input sequence S = {w 1 , w 2 , ..., w n } with n tokens, the probability of S is computed as a sequence of conditional probabilities as P (S) = n i P (w i |w 1 , ..., w i\u22121 ).",
            "score": 0.38704866882156796,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 950
                },
                {
                    "start": 953,
                    "end": 1125
                },
                {
                    "start": 1126,
                    "end": 1304
                },
                {
                    "start": 1305,
                    "end": 1497
                },
                {
                    "start": 1500,
                    "end": 1682
                },
                {
                    "start": 1683,
                    "end": 1789
                },
                {
                    "start": 1790,
                    "end": 1975
                }
            ],
            "ref_mentions": [
                {
                    "start": 122,
                    "end": 142,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 179,
                    "end": 202,
                    "matchedPaperCorpusId": "231802467"
                },
                {
                    "start": 202,
                    "end": 221,
                    "matchedPaperCorpusId": "221703107"
                },
                {
                    "start": 290,
                    "end": 309,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 637,
                    "end": 658,
                    "matchedPaperCorpusId": "246822660"
                },
                {
                    "start": 900,
                    "end": 921,
                    "matchedPaperCorpusId": "246822660"
                },
                {
                    "start": 1042,
                    "end": 1061,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1625,
                    "end": 1645,
                    "matchedPaperCorpusId": "10986188"
                },
                {
                    "start": 1645,
                    "end": 1665,
                    "matchedPaperCorpusId": "10463701"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.23193359375
        },
        {
            "corpus_id": "267707681",
            "title": "Utilizing BERT for Information Retrieval: Survey, Applications, Resources, and Challenges",
            "text": "This means that they cannot access any information beyond their pretraining timeline. To address these limitations, retrieval-augmented generation has drawn a considerable amount of interest [89]. In this approach, LLMs can retrieve information from external corpora to augment the context of the input. This helps them effectively answer even time-sensitive queries. Specifically, external knowledge bases store vector embeddings of different documents. Thus, for a given query, the most relevant documents are retrieved based on the similarity between the query embedding and the document embeddings. Due to their effectiveness, embeddings generated by BERT-based models are widely used to retrieve the relevant information via vector similarity. . \n\nThe relationship between RoBERTa [141], ALBERT [151], ELECTRA [62], MacBERT [138], TK [105,106], and T5 [25], and the original BERT model [44] suggests that all these models are simply modified versions of the original BERT with adjustable objectives. We called these variants BERT-based models/approaches.",
            "score": 0.38704866882156796,
            "section_title": "Improvements to and Extensions of Pretrained Language Models",
            "char_start_offset": 21728,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 85
                },
                {
                    "start": 86,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 750
                },
                {
                    "start": 753,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1059
                }
            ],
            "ref_mentions": [
                {
                    "start": 191,
                    "end": 195,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 815,
                    "end": 819,
                    "matchedPaperCorpusId": "208229926"
                },
                {
                    "start": 829,
                    "end": 834,
                    "matchedPaperCorpusId": "216641856"
                },
                {
                    "start": 891,
                    "end": 895,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4208984375
        },
        {
            "corpus_id": "277621042",
            "title": "Sample, Don't Search: Rethinking Test-Time Alignment for Language Models",
            "text": "Casting language model alignment as posterior inference decouples our target goal from the procedure used to achieve it. While current approaches rely on variational inference to learn a single model q \u03b8 (y | x) (Eq. 3), this strategy faces four limitations. First, it requires expensive model finetuning, which is costly with large models. Second, many models' weights are not openly shared, including those of state-of-the-art models like GPT-4 (OpenAI, 2024) and Gemini (Gemini, 2024). Third, amortizing the approximation across all prompts necessarily sacrifices the quality of approximation for any individual prompt x to achieve good average performance. Fourth, the approach assumes a monolithic notion of human preferences encoded in r(y, x), offering no flexibility to adapt to varying user preferences or contexts at inference time. \n\nThese limitations motivate a shift toward local posterior approximations, where we achieve a better approximation as we increase the compute budget at test time on a single prompt x. As we will describe in ( \u00a73.1), our key insight is that rather than estimating a single parametric approximation, we can use recent advancements in Markov chain Monte Carlo (MCMC) sampling with LMs (Faria et al., 2024) and obtain a sequence of samples S = \u27e8y 0 , y 1 , . . . , y T \u27e9 from \u03c0(y | x) and use them for optimal decision-making. \n\nA principled way to select a final response from the generated candidate set S from \u03c0(y | x) is by selecting the most common element (mode) i.e., via majority voting (MV; Wang et al., 2023b). However, this only works for tasks with well-defined answers, such as mathematical reasoning or multiple-choice questions. For open-ended generation tasks, we apply a generalization of this approach through the minimum Bayes risk (MBR) principle (Kumar & Byrne, 2002;Eikema & Aziz, 2020;Farinhas et al., 2023;Bertsch et al., 2023). The MBR framework selects the output \u0177 that maximizes the expected task-specific utility u(y, y \u2032 ):",
            "score": 0.38704866882156796,
            "section_title": "Test-Time Alignment via MCMC",
            "char_start_offset": 7192,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 488
                },
                {
                    "start": 489,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 842
                },
                {
                    "start": 845,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1366
                },
                {
                    "start": 1369,
                    "end": 1560
                },
                {
                    "start": 1561,
                    "end": 1683
                },
                {
                    "start": 1684,
                    "end": 1892
                },
                {
                    "start": 1893,
                    "end": 1993
                }
            ],
            "ref_mentions": [
                {
                    "start": 1807,
                    "end": 1828,
                    "matchedPaperCorpusId": "12366433"
                },
                {
                    "start": 1828,
                    "end": 1848,
                    "matchedPaperCorpusId": "218763425"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.17578125
        },
        {
            "corpus_id": "261530434",
            "title": "Benchmarking Large Language Models in Retrieval-Augmented Generation",
            "text": "Retrieval-augmented models The knowledge stored in large language models is commonly out-of-date (He, Zhang, and Roth 2022) and they also sometimes generate hallucination (Cao et al. 2020;Raunak, Menezes, and Junczys-Dowmunt 2021;Ji et al. 2023) i.e., they may generate irrelevant or factually incorrect contents. By using external knowledge as guidance, retrieval-augmented models can generate more accurate and reliable responses (Guu et al. 2020;Lewis et al. 2020;Borgeaud et al. 2022;Izacard et al. 2022;Shi et al. 2023;Ren et al. 2023). Retrievalaugmented models have achieved remarkable results in various tasks such as open-domain QA (Izacard and Grave 2021;Trivedi et al. 2023;Li et al. 2023a), dialogue (Cai et al. 2019a,b;Peng et al. 2023), domain-specific question answering (Cui et al. 2023) and code generation (Zhou",
            "score": 0.38704866882156796,
            "section_title": "Related Work",
            "char_start_offset": 7284,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 829
                }
            ],
            "ref_mentions": [
                {
                    "start": 171,
                    "end": 188,
                    "matchedPaperCorpusId": "224706057"
                },
                {
                    "start": 230,
                    "end": 245,
                    "matchedPaperCorpusId": "246652372"
                },
                {
                    "start": 432,
                    "end": 449,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 449,
                    "end": 467,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 508,
                    "end": 524,
                    "matchedPaperCorpusId": "259991467"
                },
                {
                    "start": 524,
                    "end": 539,
                    "matchedPaperCorpusId": "259991467"
                },
                {
                    "start": 641,
                    "end": 665,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 665,
                    "end": 685,
                    "matchedPaperCorpusId": "254877499"
                },
                {
                    "start": 685,
                    "end": 701,
                    "matchedPaperCorpusId": "253420654"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6962890625
        },
        {
            "corpus_id": "263835099",
            "title": "Retrieve Anything To Augment Large Language Models",
            "text": "Large language models (LLMs) face significant challenges stemming from their inherent limitations in knowledge, memory, alignment, and action. These challenges cannot be addressed by LLMs alone, but should rely on assistance from the external world, such as knowledge base, memory store, demonstration examples, and tools. Retrieval augmentation stands as a vital mechanism for bridging the gap between LLMs and the external assistance. However, conventional methods encounter two pressing issues. On the one hand, the general-purpose retrievers are not properly optimized for the retrieval augmentation of LLMs. On the other hand, the task-specific retrievers lack the required versatility, hindering their performance across the diverse retrieval augmentation scenarios. In this work, we present a novel approach, the LLM-Embedder, which comprehensively supports the diverse retrieval augmentation needs of LLMs with one unified embedding model. Training such a unified model is non-trivial, as various retrieval tasks aim to capture distinct semantic relationships, often subject to mutual interference. To address this challenge, we systematically optimize our training methodology. This includes reward formulation based on LLMs' feedback, the stabilization of knowledge distillation, multi-task fine-tuning with explicit instructions, and homogeneous in-batch negative sampling. These optimization strategies contribute to the outstanding empirical performance of the LLM-Embedder. Notably, it yields remarkable enhancements in retrieval augmentation for LLMs, surpassing both general-purpose and task-specific retrievers in various evaluation scenarios. Our checkpoint and source code are publicly available at https://github.com/FlagOpen/FlagEmbedding.",
            "score": 0.38704866882156796,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8896484375
        },
        {
            "corpus_id": "264818780",
            "title": "DIVKNOWQA: Assessing the Reasoning Ability of LLMs via Open-Domain Question Answering over Knowledge Base and Text",
            "text": "Retrieval-Augmented Large Language Models (RALLMs) are semi-parametric models that integrate both model parameters and a non-parametric datastore to make predictions. RALLMs enhance LLMs by updating their knowledge (Izacard et al., 2023;Khandelwal et al., 2020;Yavuz et al., 2022;Mallen et al., 2023), providing citations to support trustworthy conclusions (Menick et al., 2022;Gao et al., 2023). RALLMs can retrieve information in an end-to-end fashion within a latent space (Khandelwal et al., 2020(Khandelwal et al., , 2021;;Min et al., 2023), or they can follow the retrieve-then-read paradigm, leveraging an external retriever to extract information from textual sources (Ram et al., 2023;Khattab et al., 2022). Our approach adheres to the retrievethen-read paradigm, with a specific emphasis on multi-source retrieval, advocating for structured knowledge retrieval through symbolic generation.",
            "score": 0.38704866882156796,
            "section_title": "Retrieval-Augmented LLMs",
            "char_start_offset": 26009,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 899
                }
            ],
            "ref_mentions": [
                {
                    "start": 215,
                    "end": 237,
                    "matchedPaperCorpusId": "251371732"
                },
                {
                    "start": 237,
                    "end": 261,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 261,
                    "end": 280,
                    "matchedPaperCorpusId": "248780279"
                },
                {
                    "start": 280,
                    "end": 300,
                    "matchedPaperCorpusId": "254877603"
                },
                {
                    "start": 476,
                    "end": 500,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 500,
                    "end": 528,
                    "matchedPaperCorpusId": "222125236"
                },
                {
                    "start": 528,
                    "end": 545,
                    "matchedPaperCorpusId": "254220735"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82275390625
        },
        {
            "corpus_id": "256358611",
            "title": "Case-Based Reasoning with Language Models for Classification of Logical Fallacies",
            "text": "CBR [Schank, 1983] is a method that reasons over new cases based on similar past cases with a known label [Aamodt and Plaza, 1994]. Our CBR formulation (Figure 1) consists of three steps: (1) given a new case, retrieve similar cases from the case database, (2) adapt the fetched similar cases based on the current one, and (3) classify the new case based on the adapted exemplars. In this work, we use LMs as key components in the retriever and the adapter. We opt for this choice because of their strong ability to encode and compute similarity for any natural language input. Retriever finds k similar cases S i (i \u2208 {1, ..., k}) to the new case C from a case database. The retriever estimates the similarity between C and S i by encoding each of them with the same LM encoder and computing the cosine similarity of the resulting encodings. The retriever then picks the k cases with top cosine similarities from the database. The new case is concatenated to its similar cases, i.e., S = C \u2295 < SEP > \u2295S 1 \u2295S 2 \u2295...\u2295S k and is passed as input to the CBR adapter. \n\nAdapter aims to prioritize the most relevant information from S for reasoning over the new case C. Based on the second step of the pipeline by [Aamodt and Plaza, 1994], after fetching similar cases, it might be the case that only certain retrieved cases would be useful, and therefore, they should be weighted according to their utility for approaching the new case. The fusion of the current case with its previously seen similar problems would give the model the chance to come up with a better representation of the current problem, as well as better abstractions and generalizations for further uses. \n\nThe adapter consists of two parts: an encoder and an attention component. The encoder is an LM that takes as an input C and S separately, then outputs their respective embedding representations E C and E S . We use the hidden states of the last layer of the LM as the input embedding.",
            "score": 0.38704866882156796,
            "section_title": "Method",
            "char_start_offset": 4502,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 1062
                },
                {
                    "start": 1065,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1669
                },
                {
                    "start": 1672,
                    "end": 1745
                },
                {
                    "start": 1746,
                    "end": 1879
                },
                {
                    "start": 1880,
                    "end": 1956
                }
            ],
            "ref_mentions": [
                {
                    "start": 106,
                    "end": 130,
                    "matchedPaperCorpusId": "7069926"
                },
                {
                    "start": 1208,
                    "end": 1232,
                    "matchedPaperCorpusId": "7069926"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74609375
        },
        {
            "corpus_id": "5372617",
            "title": "Shallow Semantic in Fast Textual Entailment Rule Learners",
            "text": "The cross-pair similarity model (Zanzotto and Moschitti, 2006) proposes a similarity measure aiming at capturing rewrite rules from training examples, computing a cross-pair similarity K S ((T , H ), (T , H )). The rationale is that if two pairs are similar, it is extremely likely that they have the same entailment value. The key point is the use of placeholders to mark the relations between the sentence words. A placeholder co-indexes two substructures in the parse trees of text and hypothesis, indicating that such substructures are related. For example, the sentence pair, \"All companies file annual reports\" implies \"All insurance companies file annual reports\", is represented as follows:\n\nT1 (S (NP: 1 (DT All) (NNS: 1 companies)) (VP: 2 (VBP: 2 file) (NP: 3 (JJ: 3 annual) (NNS: 3 reports)))) H1 (S (NP: 1 (DT All) (NNP Fortune) (CD 50) (NNS: 1 companies)) (VP: 2 (VBP: 2 file) (NP: 3 (JJ: 3 annual) (NNS: 3 reports))))\n\nwhere the placeholders 1 , 2 , and 3 indicate the relations between the structures of T and of H. Placeholders help to determine if two pairs share the same rewriting rule by looking at the subtrees that they have in common. For example, suppose we have to determine if \"In autumn, all leaves fall\" implies \"In autumn, all maple leaves fall\". The related co-indexed representation is: This is the rewrite rule they have in common. Then, E 2 can be likely classified as a valid entailment, as it shares the rule with the valid entailment E 1 . The cross-pair similarity model uses: (1) a tree similarity measure K T (\u03c4 1 , \u03c4 2 ) (Collins and Duffy, 2002) that counts the subtrees that \u03c4 1 and \u03c4 2 have in common; (2) a substitution function t(\u00b7, c) that changes names of the placeholders in a tree according to a set of correspondences between placeholders c",
            "score": 0.38704866882156796,
            "section_title": "Learning entailment rules with syntactic cross-pair similarity",
            "char_start_offset": 2197,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 46,
                    "end": 62,
                    "matchedPaperCorpusId": "6265951"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.016021728515625
        },
        {
            "corpus_id": "269605025",
            "title": "R4: Reinforced Retriever-Reorder-Responder for Retrieval-Augmented Large Language Models",
            "text": "Recently, large language models (LLMs) have attracted extensive attention, which are typically pre-trained on large datasets and implicitly store substantial amounts of world or domain knowledge [33,45]. However, LLMs are also prone to the hallucination problem, and thus, they may generate erroneous responses [49]. In contrast, retrieval-augmented LLMs [17,10,47,35] retrieve knowledge from an external datastore when needed, thereby reducing hallucinations and increasing the knowledge coverage in responses. \n\nIn the literature, there are two major research aspects in this field: \n\n(1) Datastore Indexing [17,10,44,48] and (2) Document Retrieval [35,27]. For Datastore Indexing, these approaches utilize pre-trained models to generate static embeddings for documents, which are viewed as mounted external memory, and they leverage various semantic similarities to enhance indexing. For Document Retrieval, the system initially retrieves a collection of relevant documents based on the semantic relevance between the user query and the documents. Then, the LLMs concatenate these highly related documents in an unordered manner to the prompt input [4], which makes LLMs better at answering factual questions. These methods essentially organize the information related to the user query from the perspective of coarse-grained memory , ignoring the fine-grained relationships between retrieved documents and the knowledge mastery characteristics of LLMs [14,22]. For instance, the ordering of the top-K retrieved documents can be further adjusted to enhance the performance of retrieval-augmented LLMs in answering questions more accurately, as illustrated in Figure 1. \n\nIn this paper, we propose the Reinforced Retriever-Reorder-Responder framework (R 4 ) to formalize a new retrieval-augmented generation (RAG) pipeline. To reorder the retrieved top-K documents and enhance the response effectiveness of the LLMs, we divide the reorder learning process into the following two steps: Document Order Adjustment: Prior research indicates that LLMs have a better recall of information at the beginning and the ending positions of retrieved documents in prompts [14,22].",
            "score": 0.38704866882156796,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 511
                },
                {
                    "start": 514,
                    "end": 584
                },
                {
                    "start": 587,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 1050
                },
                {
                    "start": 1051,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1671
                },
                {
                    "start": 1674,
                    "end": 1825
                },
                {
                    "start": 1826,
                    "end": 2170
                }
            ],
            "ref_mentions": [
                {
                    "start": 199,
                    "end": 202,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 355,
                    "end": 359,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 362,
                    "end": 365,
                    "matchedPaperCorpusId": "253802096"
                },
                {
                    "start": 610,
                    "end": 614,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 617,
                    "end": 620,
                    "matchedPaperCorpusId": "249674500"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94482421875
        },
        {
            "corpus_id": "1038816",
            "title": "Translating Queries into Snippets for Improved Query Expansion",
            "text": "Equation 3 allows for a separation of a language model P (e), and a translation model P (f |e). Och and Ney (2004) reformulate equation 3 as a linear combination of feature functions h m (e, f ) and weights \u03bb m , including feature functions for translation models h i (e, f ) = P (f |e) and language models h j (e) = P (e): \n\nThe translation model used in our approach is based on the sequence of alignment models described in Och and Ney (2003). The relationship of translation model and alignment model for source language string f = f J 1 and target string e = e I 1 is via a hidden variable describing an alignment mapping from source position j to target position a j : \n\nThe alignment a J 1 contains so-called null-word alignments a j = 0 that align source words to the empty word. The different alignment models described in Och and Ney (2003) each parameterize equation 5 differently so as to capture different properties of source and target mappings. All models are based on estimating parameters \u03b8 by maximizing the likelihood of training data consisting of sentence-aligned, but not word-aligned strings {(f s , e s ) : s = 1, . . . , S}. Since each sentence pair is linked by a hidden alignment variable a = a J 1 , the optimal \u03b8 is found using unlabeleddata log-likelihood estimation techniques such as the EM algorithm (Dempster et al., 1977): \n\nThe final translation model is calculated from relative frequencies of phrases, i.e. consecutive sequences of words occurring in text. Phrases are extracted via various heuristics as larger blocks of aligned words from best word alignments, as described in Och and Ney (2004). Language modeling in our approach deploys an n-gram language model that assigns the following probability to a string w L 1 of words (see Brants et al. (2007)): \n\nEstimation of n-gram probabilities is done by counting relative frequencies of n-grams in a corpus of user queries. Remedies against sparse data problems are achieved by various smoothing techniques, as described in Brants et al. (2007).",
            "score": 0.38704866882156796,
            "section_title": "Query-Document Term Correlations",
            "char_start_offset": 7938,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 323
                },
                {
                    "start": 326,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 674
                },
                {
                    "start": 677,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1358
                },
                {
                    "start": 1361,
                    "end": 1495
                },
                {
                    "start": 1496,
                    "end": 1637
                },
                {
                    "start": 1638,
                    "end": 1798
                },
                {
                    "start": 1801,
                    "end": 1916
                },
                {
                    "start": 1917,
                    "end": 2038
                }
            ],
            "ref_mentions": [
                {
                    "start": 96,
                    "end": 114,
                    "matchedPaperCorpusId": "1272090"
                },
                {
                    "start": 427,
                    "end": 445,
                    "matchedPaperCorpusId": "5219389"
                },
                {
                    "start": 832,
                    "end": 850,
                    "matchedPaperCorpusId": "5219389"
                },
                {
                    "start": 1334,
                    "end": 1357,
                    "matchedPaperCorpusId": "4193919"
                },
                {
                    "start": 1618,
                    "end": 1636,
                    "matchedPaperCorpusId": "1272090"
                },
                {
                    "start": 1776,
                    "end": 1796,
                    "matchedPaperCorpusId": "633992"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52685546875
        },
        {
            "corpus_id": "272310589",
            "title": "MemLong: Memory-Augmented Retrieval for Long Text Modeling",
            "text": "Much effort has been made to enhance Retrieval-Augmented Language Modeling (Lewis et al., 2020;Izacard and Grave, 2020;Ram et al., 2023;Yu et al., 2022;Asai et al., 2023). While some approaches use external retrievers, non-parametric information fusion often falls short compared to parametric methods within the model. We concentrate on integrating retrieval concepts directly into the model. REALM (Guu et al., 2020) suggests that relying solely on internal model knowledge is inefficient and advocates for the model to learn to retrieve and comprehend. kNN-LM (Khandelwal et al., 2019) enhances language modeling by blending the LLM's next-word predictions with those from a retrieval-based mechanism. MemTrm (Wu et al., 2022) introduces a memory bank but risks shifting memory distributions due to parameter adjustments. LongMEM (Wang et al., 2024b) mitigates this by training a sub-network, though this adds significant overhead. In contrast, our approach involves a fixed pre-trained model, enhancing it with a frozen retriever that aligns with the model's internal retrieval processes, thus avoiding distribution shifts and architectural changes.",
            "score": 0.38704866882156796,
            "section_title": "Retrieval-Augmented Language Modeling",
            "char_start_offset": 23636,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1153
                }
            ],
            "ref_mentions": [
                {
                    "start": 75,
                    "end": 95,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 119,
                    "end": 136,
                    "matchedPaperCorpusId": "256459451"
                },
                {
                    "start": 833,
                    "end": 853,
                    "matchedPaperCorpusId": "259137816"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96826171875
        },
        {
            "corpus_id": "276482659",
            "title": "How Much Knowledge Can You Pack into a LoRA Adapter without Harming LLM?",
            "text": "Large Language Models (LLMs) have been widely adopted in many applications due to their ability to produce human-like responses to user queries. This is made possible by their ability to generalize information and accumulate a large amount of knowledge during the pre-training phase (Chen et al., 2024). These models can solve various problems, such as summarization, reasoning, and question answering, among others (Bhakthavatsalam et al., 2021;Lin et al., 2022;Hendrycks et al., 2021;Moskovskiy et al., 2024). \u2020 These authors contributed equally to this work. Despite the general capabilities of LLMs, there are still situations that require the incorporation of new knowledge to better meet specific needs. This could be due to changes in general knowledge that occur after the LLM training period, or possibly due to specific knowledge that was intentionally omitted during the training period. To address these issues techniques such as Retrieval-Augmented Generation (RAG) (Lewis et al., 2020;Belikova et al., 2024) or few-shot learning (Brown et al., 2020) can be applied. In general, RAG requires access to an external knowledge base, which may be undesirable in some contexts. With respect to in-context learning, the resulting generation is strongly dependent on the selected few-shot samples (Rubin et al., 2022). In this work, we revisit fine-tuning as one of the most popular approaches for integrating new knowledge into LLMs. \n\nFine-tuning LLMs, which often have hundreds of billions of parameters, is a computationally expensive and time-consuming process. To address these challenges, Parameter-Efficient Fine-Tuning (PEFT) techniques have gained popularity (Han et al., 2024), with Low-Rank Adaptation (LoRA) (Hu et al., 2022) being one of the most effective methods. However, these modified LLMs may suffer from drawbacks, such as catastrophic forgetting (Aleixo et al., 2024;Kirkpatrick et al., 2017) or less severe but still notable loss of previously learned associations (Hoelscher-Obermaier et al., 2023).",
            "score": 0.38704866882156796,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1185
                },
                {
                    "start": 1186,
                    "end": 1324
                },
                {
                    "start": 1325,
                    "end": 1440
                },
                {
                    "start": 1443,
                    "end": 1572
                },
                {
                    "start": 1573,
                    "end": 1785
                },
                {
                    "start": 1786,
                    "end": 2029
                }
            ],
            "ref_mentions": [
                {
                    "start": 283,
                    "end": 302,
                    "matchedPaperCorpusId": "271483842"
                },
                {
                    "start": 446,
                    "end": 463,
                    "matchedPaperCorpusId": "237532606"
                },
                {
                    "start": 463,
                    "end": 486,
                    "matchedPaperCorpusId": "221516475"
                },
                {
                    "start": 486,
                    "end": 510,
                    "matchedPaperCorpusId": "274060353"
                },
                {
                    "start": 979,
                    "end": 999,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 999,
                    "end": 1021,
                    "matchedPaperCorpusId": "271769602"
                },
                {
                    "start": 1303,
                    "end": 1323,
                    "matchedPaperCorpusId": "245218561"
                },
                {
                    "start": 1727,
                    "end": 1744,
                    "matchedPaperCorpusId": "235458009"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58203125
        },
        {
            "corpus_id": "265498565",
            "title": "Contextual Knowledge Pursuit for Faithful Visual Synthesis",
            "text": "Here we describe our dynamic mechanism to retrieve knowledge from pre-trained language models or external databases in a recursive manner. We assume access to an external knowledge base D that contains a large set of facts, and an instruction-tuned language model F agent . Let C be our knowledge-driven context, a set that will consist of our original task prompt x and a list of facts in the end. Initially, at iteration k = 0, the knowledge base is the set of all facts, i.e. D 0 = D, and the context consists of only the task prompt, i.e. C 0 = {x}. Determining Familiarity with Input Prompt. First, to determine whether CKPT augments knowledge facts using external retrieval or parametric elicitation, we propose a \"get help only when needed\" strategy. Our language model agent will generate an integer score p agent = F agent (x) to measure its familiarity with the prompt based on its parametric knowledge. \n\nReturn an integer from 1 to 10 indicating your level of familiarity with the user input x, where 1 means not familiar and 10 means completely familiar. \n\nCaptions with common daily objects (e.g., 'kitchen with stove') will receive high scores and will be augmented with knowledge from the language model, while out-of-domain prompts such as wildlife animals and scientific nomenclatures will receive low scores and will be augmented with the external knowledge. Contextual Retrieval from External Source. If the agent's familiarity p agent with the input is below a manually-set threshold, it will execute CKPT with the external database. At a certain step k, given the context C k and the knowledge base D k , the goal is to find a fact that is most relevant for the downstream generative task. We denote it as the fact d \u2208 D k that maximizes textual relevance between the text embedding of C k and the text embedding of d, i.e.: \n\nwhere E \u03c8 is an encoder that maps text tokens to a numeric embedding vector in R p , R : R p \u00d7 R p \u2192 R + measures the inner-product similarity, and E \u03c8 to be a pre-trained contrastive text encoder [17].",
            "score": 0.38694882740543096,
            "section_title": "Knowledge Pursuit",
            "char_start_offset": 8931,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 913
                },
                {
                    "start": 916,
                    "end": 1067
                },
                {
                    "start": 1070,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1420
                },
                {
                    "start": 1421,
                    "end": 1554
                },
                {
                    "start": 1555,
                    "end": 1711
                },
                {
                    "start": 1712,
                    "end": 1846
                },
                {
                    "start": 1849,
                    "end": 2051
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.339599609375
        },
        {
            "corpus_id": "17784827",
            "title": "On the Linear Algebraic Structure of Distributed Word Representations",
            "text": "Distributed representations of words in a vector space represent each word with a real-valued vector, called a word vector. They are also known as word embeddings because they embed an entire vocabulary into a relatively low-dimensional linear space whose dimensions are latent continuous features. One of the earliest ideas of distributed representations dates back to \uf731\uf739\uf738\uf736 [\uf731\uf734], and has since been applied to statistical language modeling with considerable success. These word vectors have shown to improve performance in a variety of natural language processing tasks including automatic speech recognition [\uf732\uf735], information retrieval [\uf731\uf737], document classification [\uf732\uf736], and parsing [\uf732\uf738].\n\nThe word vectors are trained over large corpora \uf731 typically in a totally unsupervised manner, using the co-occurrence statistics of words. \uf732 Past methods to obtain word embeddings include matrix factorization methods [\uf731\uf730], variants of neural networks [\uf732\uf734, \uf732, \uf733, \uf739, \uf732\uf730, \uf731\uf738, \uf733\uf730], and energybased models [\uf732\uf733, \uf731]. The learned word vectors explicitly capture many linguistic regularities and patterns, such as semantic and syntactic attributes of words. Therefore, words that appear in similar contexts, or belong to a common \"category\" (e.g., country names, composer names, or university names), tend to form a cluster in the projected space.\n\nRecently, Mikolov et al. [\uf733\uf730] demonstrated that word embeddings created by a recurrent neural net (RNN) and by a related energy-based model called word\uf732vec exhibit an additional linear structure which captures the relation between pairs of words, and allows one to solve analogy queries such as \"man:woman::king:??\" using simple vector arithmetics. More specifically, \"queen\" happens to be the word whose vector v queen is the closest approximation to the vector v woman \u2212 v man + v king . Other subsequent works [\uf732\uf733, \uf731\uf736, \uf732\uf731] produced word vectors that can be used to solve analogy queries in the same way. It remains a mystery as to why these radically different embedding methods, including highly non-linear ones, produce vectors exhibiting similar linear structure. A summary of current justifications for this phenomenon is provided in Section \uf732.\uf733. \uf731 A corpus (plural corpora) is a large and structured set of unlabeled texts. \uf732 We say two words co-occur in a corpus if they appear together within a certain (fixed)",
            "score": 0.38661041288437775,
            "section_title": "\uf731.\uf731 Distributed word representations",
            "char_start_offset": 53,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.468994140625
        },
        {
            "corpus_id": "274822983",
            "title": "Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference",
            "text": "Information Retrieval (IR) is one of the most common applications of encoder-only models,8 where they are used to represent documents and queries in semantic search (Karpukhin et al., 2020). This domain has recently seen considerable growth and interest following the spread of LLMs where semantic search powered by lightweight models is used to provide relevant context to LLMs as part of Retrieval-Augmented Generation pipelines. \n\nWe evaluate models in both the single-vector Dense Passage Retrieval (DPR) (Karpukhin et al., 2020) setting and the multi-vector ColBERT (Khattab and Zaharia, 2020) setting. \n\nWe report retrieval results on the popular BEIR evaluation suite (Thakur et al., 2021), the common standard for evaluating retrieval performance across a variety of tasks and domains, using the nDCG@10 metric. For each setting detailed below, we conduct a learning rate sweep based on results over a subset of the BEIR benchmarks to select the final model, detailed in Appendix E.2. \n\nSingle vector retrieval One of the most common approaches to neural retrieval using encoders is DPR (Karpukhin et al., 2020), where a singlevector is used to represent an entire document. The similarity between a query and a document can then be computed through distance operations, such as cosine similarity. Models are finetuned using contrastive learning to create representations which are close if a document is relevant to a query, and distant if not (van den Oord et al., 2018). \n\nWe train every base model using the MS-MARCO (Bajaj et al., 2016) dataset with mined hard negatives (Xuan et al., 2020) on 1.25M samples with a batch size of 16 and learning rate warmup for 5% of the training using sentencetransformers (Reimers and Gurevych, 2019). \n\nMulti vector retrieval Multi-vector retrieval, championed by ColBERT (Khattab and Zaharia, 2020), seeks to mitigate lost information from compressing an entire sequence into a single vector. In multi-vector retrieval, each document is represented by all of its individual token vectors, and the similarity between a query and a document is computed using the MaxSim9 operator.",
            "score": 0.3846365390336228,
            "section_title": "Text Retrieval",
            "char_start_offset": 16759,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 431
                },
                {
                    "start": 434,
                    "end": 607
                },
                {
                    "start": 610,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 992
                },
                {
                    "start": 995,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1305
                },
                {
                    "start": 1306,
                    "end": 1481
                },
                {
                    "start": 1484,
                    "end": 1749
                },
                {
                    "start": 1752,
                    "end": 1942
                },
                {
                    "start": 1943,
                    "end": 2128
                }
            ],
            "ref_mentions": [
                {
                    "start": 165,
                    "end": 189,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 509,
                    "end": 533,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 675,
                    "end": 696,
                    "matchedPaperCorpusId": "233296016"
                },
                {
                    "start": 1095,
                    "end": 1119,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1584,
                    "end": 1603,
                    "matchedPaperCorpusId": "220793249"
                },
                {
                    "start": 1720,
                    "end": 1748,
                    "matchedPaperCorpusId": "201646309"
                },
                {
                    "start": 1821,
                    "end": 1848,
                    "matchedPaperCorpusId": "216553223"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7158203125
        },
        {
            "corpus_id": "250391000",
            "title": "Retrieval-augmented Generation across Heterogeneous Knowledge",
            "text": "Retrieved data are expected to bridge the gap between inputs and outputs of generation models. In other words, retrievers are trained to provide information that is found with the inputs as queries and related to the outputs. Ideally, they find the output-related information just once. However, that may actually take multiple hops of retrieval across knowledge sources. Thus, the second solution is to iteratively retrieve knowledge from different sources. Regarding an entity, encyclopedic knowledge usually contains its attribute information (e.g., age, duration), while commonsense knowledge includes universally recognized facts in human's daily life. For example, the entity \"soup\" in Wikipedia is described as \"a primarily liquid food, generally served warm or hot, made by combining ingredients of meat or vegetables with stock, milk, or water\"; and in the OMCS corpus (Singh et al., 2002), it contains a well-known fact \"soup and salad can be a healthy lunch\". Therefore, to answer the question \"What are the common ingredients in a healthy lunch?\", the encyclopedic corpus and commonsense corpus can provide complementary knowledge that should be both leveraged. \n\nBesides, it also might be necessary to first read a subset of the corpus to extract the useful information, and then further retrieve information from other knowledge sources. For example, given input q, it may take k steps, each step retrieving data d i from source s i \u2208 S with an incremental query q i = q \u2295 d 1 \u2295 \u2022 \u2022 \u2022 \u2295 d i\u22121 (i \u2264 k) until the final d k contains the information that can directly augment the generation of outputs o. Here S includes various sources such as text corpora, tables, and knowledge graphs. To achieve this, however, the primary challenge for training such a multi-hop retriever is that it cannot observe any intermediate document for supervision but only the final output. So, the multi-virtual hops retrieval (MVHL) needs to perform multi-hop retrieval without any intermediate signal. I will discuss two promising designs as below. First, the MVHL approach will dynamically determine when the multihops retrieval finishes. I denote the relevance score between query q i and data d i from source s i by r(d i ; q i , s i ).",
            "score": 0.38446278027742903,
            "section_title": "Multi-virtual Hops Retrieval over Heterogeneous Knowledge",
            "char_start_offset": 12904,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 458
                },
                {
                    "start": 459,
                    "end": 657
                },
                {
                    "start": 658,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1173
                },
                {
                    "start": 1176,
                    "end": 1351
                },
                {
                    "start": 1352,
                    "end": 1614
                },
                {
                    "start": 1615,
                    "end": 1698
                },
                {
                    "start": 1699,
                    "end": 1881
                },
                {
                    "start": 1882,
                    "end": 1995
                },
                {
                    "start": 1996,
                    "end": 2042
                },
                {
                    "start": 2043,
                    "end": 2133
                },
                {
                    "start": 2134,
                    "end": 2233
                }
            ],
            "ref_mentions": [
                {
                    "start": 878,
                    "end": 898,
                    "matchedPaperCorpusId": "7190428"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.155517578125
        },
        {
            "corpus_id": "269430973",
            "title": "Reinforcement Retrieval Leveraging Fine-grained Feedback for Fact Checking News Claims with Black-Box LLM",
            "text": "Recent advancements in large language models (LLMs), such as GPT-4 (OpenAI, 2023) and Palm 2 (Anil et al., 2023), have demonstrated impressive capabilities in generating coherent, informative, and fluent verbal reasoning (Wei et al., 2022;Wang et al., 2022;Zhou et al., 2022).This opens up the possibility for applying LLMs in factchecking related tasks, such as predicting claim veracity and generating explainable rationales (Zhao et al., 2023;Yao et al., 2023;Pan et al., 2023;Bubeck et al., 2023;Zhang and Gao, 2023).However, these tasks heavily rely on up-to-date information and reliable knowledge, and LLMs may fall short in these aspects, potentially affecting their downstream performance with bias and hallucination (OpenAI, 2022;Ye and Durrett, 2022;Bang et al., 2023).\n\nInspired by retrieval-augmented language modeling approach (Lewis et al., 2020;Guu et al., 2020), recent efforts have attempted to incorporate retrieved external knowledge with LLMs (Nakano et al., 2021;Khattab et al., 2022;Press et al., 2023;Yao et al., 2023;Peng et al., 2023).A common idea is to integrate a plug-and-play retrieval model with the LLM for providing relevant context.However, the quality of retrieval results can compromise the effectiveness of LLM-based reasoning and verification (Yao et al., 2023).A key unresolved issue is how to tune the retrieval model for returning the \"smoking gun\" evidence that can assist the LLM in determining the veracity of the claim reliably.\n\nMeanwhile, advanced LLMs typically exhibit a black-box nature and often provide service via APIs (Sun et al., 2022), preventing end-users",
            "score": 0.3836117238959361,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 276
                },
                {
                    "start": 276,
                    "end": 521
                },
                {
                    "start": 521,
                    "end": 780
                },
                {
                    "start": 782,
                    "end": 1061
                },
                {
                    "start": 1061,
                    "end": 1167
                },
                {
                    "start": 1167,
                    "end": 1301
                },
                {
                    "start": 1301,
                    "end": 1474
                },
                {
                    "start": 1476,
                    "end": 1613
                }
            ],
            "ref_mentions": [
                {
                    "start": 93,
                    "end": 112,
                    "matchedPaperCorpusId": "202541363"
                },
                {
                    "start": 221,
                    "end": 239,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 239,
                    "end": 257,
                    "matchedPaperCorpusId": "247595263"
                },
                {
                    "start": 257,
                    "end": 275,
                    "matchedPaperCorpusId": "248986239"
                },
                {
                    "start": 427,
                    "end": 446,
                    "matchedPaperCorpusId": "258547173"
                },
                {
                    "start": 446,
                    "end": 463,
                    "matchedPaperCorpusId": "252762395"
                },
                {
                    "start": 463,
                    "end": 480,
                    "matchedPaperCorpusId": "252595921"
                },
                {
                    "start": 500,
                    "end": 520,
                    "matchedPaperCorpusId": "263334529"
                },
                {
                    "start": 761,
                    "end": 779,
                    "matchedPaperCorpusId": "256662612"
                },
                {
                    "start": 841,
                    "end": 861,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 861,
                    "end": 878,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 1006,
                    "end": 1025,
                    "matchedPaperCorpusId": "252762102"
                },
                {
                    "start": 1025,
                    "end": 1042,
                    "matchedPaperCorpusId": "252762395"
                },
                {
                    "start": 1282,
                    "end": 1300,
                    "matchedPaperCorpusId": "252762395"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.362060546875
        },
        {
            "corpus_id": "270216169",
            "title": "COS-Mix: Cosine Similarity and Distance Fusion for Improved Information Retrieval",
            "text": "Large Language Models (LLMs) have emerged as transformative technologies with excellent performance on a variety of tasks.With the increasing size of LLMs, they can function as very effective knowledge warehouses [1], with facts embedded within their parameters, and models can be improved further through fine-tuning on domain-specific knowledge.However fine-tuning is a difficult task with vast amounts of data [2].A different method, first developed in open domain question answering systems [3], involves organizing vast amounts of text into smaller sections (paragraphs) and storing them in a distinct information retrieval system.This system retrieves relevant information, which is then provided to the LLM alongside the question for context.Researchers have also attempted using keywords to augment information retrieval [4] with reported reduction in latency and cost of retrieval [4].This approach simplifies the process of supplying a system with up-to-date knowledge in a specific domain, while also facilitating easy understanding of where the information comes from.In contrast, the inherent knowledge of LLMs is complex and challenging to trace back to its origin [5].\n\nNevertheless, existing retrieval-augmented approaches also have flaws.Most practices in retrieval augmented generation or RAG, use vector similarity as semantic similarity, but it has been shown that cosine similarity of learned embeddings can yield arbitrary results [6].In this study, we propose a hybrid retrieval strategy that integrates cosine similarity and cosine distance measures to enhance retrieval performance.Traditional cosine similarity measures have been widely utilized to capture the similarity between vectors in high-dimensional spaces.However, in scenarios where the similarity measure fails to adequately capture the semantic relationship between documents or items, cosine distance provides a complementary perspective by quantifying the dissimilarity between vectors.We demonstrate how this approach improves retrieval specifically for sparse data.\n\nOur proposed method towards RAG is experimented on proprietary data, unlike more recent publications [7][8][9] which have used open-source datasets like QuALITY [10], MedQA [11], US SEC Filings etc.",
            "score": 0.3832814172581209,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 122,
                    "end": 347
                },
                {
                    "start": 347,
                    "end": 417
                },
                {
                    "start": 417,
                    "end": 636
                },
                {
                    "start": 636,
                    "end": 749
                },
                {
                    "start": 749,
                    "end": 894
                },
                {
                    "start": 894,
                    "end": 1080
                },
                {
                    "start": 1080,
                    "end": 1183
                },
                {
                    "start": 1185,
                    "end": 1255
                },
                {
                    "start": 1255,
                    "end": 1457
                },
                {
                    "start": 1457,
                    "end": 1607
                },
                {
                    "start": 1607,
                    "end": 1741
                },
                {
                    "start": 1741,
                    "end": 1976
                },
                {
                    "start": 1976,
                    "end": 2057
                },
                {
                    "start": 2059,
                    "end": 2257
                }
            ],
            "ref_mentions": [
                {
                    "start": 413,
                    "end": 416,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 495,
                    "end": 498,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 829,
                    "end": 832,
                    "matchedPaperCorpusId": "263829976"
                },
                {
                    "start": 890,
                    "end": 893,
                    "matchedPaperCorpusId": "263829976"
                },
                {
                    "start": 1179,
                    "end": 1182,
                    "matchedPaperCorpusId": "248987368"
                },
                {
                    "start": 1453,
                    "end": 1456,
                    "matchedPaperCorpusId": "268296965"
                },
                {
                    "start": 2220,
                    "end": 2224,
                    "matchedPaperCorpusId": "245218982"
                },
                {
                    "start": 2232,
                    "end": 2236,
                    "matchedPaperCorpusId": "221970190"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8544921875
        },
        {
            "corpus_id": "12769731",
            "title": "A Provably Correct Learning Algorithm for Latent-Variable PCFGs",
            "text": "We now describe a second set of experiments, on the Saul and Pereira (1997) model for language modeling. Define V to be the set of words in the vocabulary. For any w 1 , w 2 \u2208 V , the Saul and Pereira (1997) model then defines p(w 2 | w 1 ) = m h=1 r(h | w 1 )s(w 2 | h) where r(h | w 1 ) and  (Kneser and Ney, 1995), using the SRILM toolkit.\n\ns(w 2 | h) are parameters of the approach. The conventional approach to estimation of the parameters r(h | w 1 ) and s(w 2 | h) from a corpus is to use the EM algorithm. In this section we compare the EM algorithm to a pivot-based method.\n\nIt is straightforward to represent this model as an L-PCFG, and hence to use our implementation for estimation.\n\nIn this special case, the L-PCFG learning algorithm is equivalent to a simple algorithm, with the following steps: 1) define the matrix Q with entries Q w 1 ,w 2 = count(w 1 , w 2 )/N where count(w 1 , w 2 ) is the number of times that bigram (w 1 , w 2 ) is seen in the data, and N = w 1 ,w 2 count(w 1 , w 2 ). Run the algorithm of section 5.2 on Q to recover estimates\u015d(w 2 | h); 2) estimater(h | w 1 ) using the EM algorithm to optimize the function w 1 ,w 2 Q w 1 ,w 2 log hr (h | w 1 )\u015d(w 2 | h) with respect to ther parameters; this function is concave in these parameters.\n\nWe performed the language modeling experiments for a number of reasons. First, because in this case the L-PCFG algorithm reduces to a simple algorithm, it allows us to evaluate the core ideas in the method very directly. Second, it allows us to test the pivot method on the very large datasets that are available for language modeling.\n\nWe use two corpora for our experiments. The first is the Brown corpus, as used by Bengio et al. (2006) in language modeling experiments. Following Ben",
            "score": 0.3832814172581209,
            "section_title": "Experiments on the Saul and Pereira (1997) Model for Language Modeling",
            "char_start_offset": 24224,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 294,
                    "end": 316,
                    "matchedPaperCorpusId": "9685476"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.446044921875
        },
        {
            "corpus_id": "2248510",
            "title": "Bag-of-Words Forced Decoding for Cross-Lingual Information Retrieval",
            "text": "We combine both objectives by directly modeling the probability of a document d e in target language e given a query q f in source language f , factorized as follows: \n\nApplying the same Viterbi approximation during inference as in (1), we choose the retrieval score of d e to be the score of the highest scoring hypothesis h, score(q f , d e ) = max \n\nwhere the product between both models can be interpreted as a conjunctive operation similar to a product of experts (Hinton, 2002): A high score is achieved if both experts, namely translation and retrieval models, assign high scores to a hypothesis. That is, the model attempts to produce a wellformed translation, but at the same time chooses lexical items present in the bag-of-words representation of the document. Similarly, we can interpret the inclusion of the retrieval component as a constraint to force the decoder to retrieve d e with high probability. By a slight abuse of terminology, we will henceforth call our approach Bag-of-Words Forced Decoding (BOW-FD). 1 he translation term P (h|q f ) is modeled as in (2) for standard hierarchical phrase-based SMT (Chiang, 2007) and left unchanged in our joint model. The retrieval term P (d e |h, q f ) is modeled in a similar form \n\nwhere IR features do not depend on q f (thus allowing us to drop this term) and decompose over derivation terms. This allows a bag-of-word vector representation of documents, and retrieval features are local to single edges in the search space for efficient Viterbi inference. The joint scoring model is defined as follows: \n\nscore(q f , d e ; w) = max h\u2208Eq f e Fsmt(h,qe,q f )+F ir (h,de) , where the weight vector is defined by the vector concatenation w = w smt w ir , and q e refers to the yield that is determined uniquely by derivation h. \n\nFollowing the interpretation of our joint model as forced or constrained decoding, we can view pipeline approaches such as the direct translation baseline as instances of unconstrained decoding.",
            "score": 0.3832814172581209,
            "section_title": "Related Work",
            "char_start_offset": 7153,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 169,
                    "end": 350
                },
                {
                    "start": 353,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1177
                },
                {
                    "start": 1178,
                    "end": 1242
                },
                {
                    "start": 1245,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1521
                },
                {
                    "start": 1522,
                    "end": 1568
                },
                {
                    "start": 1571,
                    "end": 1789
                },
                {
                    "start": 1792,
                    "end": 1986
                }
            ],
            "ref_mentions": [
                {
                    "start": 469,
                    "end": 483,
                    "matchedPaperCorpusId": "207596505"
                },
                {
                    "start": 1124,
                    "end": 1138,
                    "matchedPaperCorpusId": "3505719"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7099609375
        },
        {
            "paperId": "1bab539dd0318fe446fe50574253bdf4600b112a",
            "corpusId": 270702769,
            "title": "On the Role of Long-tail Knowledge in Retrieval Augmented Large Language Models",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2024,
            "referenceCount": 31,
            "citationCount": 5,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.16367, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2257089368",
                    "name": "Dongyang Li"
                },
                {
                    "authorId": "2243406637",
                    "name": "Junbing Yan"
                },
                {
                    "authorId": "2146342371",
                    "name": "Taolin Zhang"
                },
                {
                    "authorId": "121899912",
                    "name": "Chengyu Wang"
                },
                {
                    "authorId": "2257159827",
                    "name": "Xiaofeng He"
                },
                {
                    "authorId": "2292090586",
                    "name": "Longtao Huang"
                },
                {
                    "authorId": "2292128230",
                    "name": "Hui Xue"
                },
                {
                    "authorId": "2272790856",
                    "name": "Junyuan Huang"
                }
            ],
            "abstract": "Retrieval augmented generation (RAG) exhibits outstanding performance in promoting the knowledge capabilities of large language models (LLMs) with retrieved documents related to user queries. However, RAG only focuses on improving the response quality of LLMs via enhancing queries indiscriminately with retrieved information, paying little attention to what type of knowledge LLMs really need to answer original queries more accurately. In this paper, we suggest that long-tail knowledge is crucial for RAG as LLMs have already remembered common world knowledge during large-scale pre-training. Based on our observation, we propose a simple but effective long-tail knowledge detection method for LLMs. Specifically, the novel Generative Expected Calibration Error (GECE) metric is derived to measure the ``long-tailness'' of knowledge based on both statistics and semantics. Hence, we retrieve relevant documents and infuse them into the model for patching knowledge loopholes only when the input query relates to long-tail knowledge. Experiments show that, compared to existing RAG pipelines, our method achieves over 4x speedup in average inference time and consistent performance improvement in downstream tasks.",
            "corpus_id": "270702769",
            "text": "Retrieval augmented generation (RAG) exhibits outstanding performance in promoting the knowledge capabilities of large language models (LLMs) with retrieved documents related to user queries. However, RAG only focuses on improving the response quality of LLMs via enhancing queries indiscriminately with retrieved information, paying little attention to what type of knowledge LLMs really need to answer original queries more accurately. In this paper, we suggest that long-tail knowledge is crucial for RAG as LLMs have already remembered common world knowledge during large-scale pre-training. Based on our observation, we propose a simple but effective long-tail knowledge detection method for LLMs. Specifically, the novel Generative Expected Calibration Error (GECE) metric is derived to measure the ``long-tailness'' of knowledge based on both statistics and semantics. Hence, we retrieve relevant documents and infuse them into the model for patching knowledge loopholes only when the input query relates to long-tail knowledge. Experiments show that, compared to existing RAG pipelines, our method achieves over 4x speedup in average inference time and consistent performance improvement in downstream tasks.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.8291015625
        },
        {
            "paperId": "a6f3b65fba3ceaf80f2965e0358e3367bc6f185d",
            "corpusId": 273502205,
            "title": "Toward Robust RALMs: Revealing the Impact of Imperfect Retrieval on Retrieval-Augmented Language Models",
            "venue": "Transactions of the Association for Computational Linguistics",
            "year": 2024,
            "referenceCount": 52,
            "citationCount": 2,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.15107, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2315713219",
                    "name": "Seong-Il Park"
                },
                {
                    "authorId": "2305624401",
                    "name": "Jay-Yoon Lee"
                }
            ],
            "abstract": "Abstract Retrieval Augmented Language Models (RALMs) have gained significant attention for their ability to generate accurate answers and improve efficiency. However, RALMs are inherently vulnerable to imperfect information due to their reliance on the imperfect retriever or knowledge source. We identify three common scenarios\u2014unanswerable, adversarial, conflicting\u2014where retrieved document sets can confuse RALMs with plausible real-world examples. We present the first comprehensive investigation to assess how well RALMs detect and handle such problematic scenarios. Among these scenarios, to systematically examine adversarial robustness we propose a new adversarial attack method, Generative model-based ADVersarial attack (GenADV) and a novel metric Robustness under Additional Document (RAD). Our findings reveal that RALMs often fail to identify the unanswerability or contradiction of a document set, which frequently leads to hallucinations. Moreover, we show that the addition of an adversary significantly degrades RALM\u2019s performance, with the model becoming even more vulnerable when the two scenarios overlap (adversarial+ unanswerable). Our research identifies critical areas for assessing and enhancing the robustness of RALMs, laying the foundation for the development of more robust models.1",
            "corpus_id": "273502205",
            "text": "Abstract Retrieval Augmented Language Models (RALMs) have gained significant attention for their ability to generate accurate answers and improve efficiency. However, RALMs are inherently vulnerable to imperfect information due to their reliance on the imperfect retriever or knowledge source. We identify three common scenarios\u2014unanswerable, adversarial, conflicting\u2014where retrieved document sets can confuse RALMs with plausible real-world examples. We present the first comprehensive investigation to assess how well RALMs detect and handle such problematic scenarios. Among these scenarios, to systematically examine adversarial robustness we propose a new adversarial attack method, Generative model-based ADVersarial attack (GenADV) and a novel metric Robustness under Additional Document (RAD). Our findings reveal that RALMs often fail to identify the unanswerability or contradiction of a document set, which frequently leads to hallucinations. Moreover, we show that the addition of an adversary significantly degrades RALM\u2019s performance, with the model becoming even more vulnerable when the two scenarios overlap (adversarial+ unanswerable). Our research identifies critical areas for assessing and enhancing the robustness of RALMs, laying the foundation for the development of more robust models.1",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.7060546875
        },
        {
            "paperId": "aa59536123b29599115cb28027d9ddb67fc1c613",
            "corpusId": 268264533,
            "title": "Telecom Language Models: Must They Be Large?",
            "venue": "IEEE International Symposium on Personal, Indoor and Mobile Radio Communications",
            "year": 2024,
            "referenceCount": 21,
            "citationCount": 16,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.04666, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "3393923",
                    "name": "Nicola Piovesan"
                },
                {
                    "authorId": "2261362548",
                    "name": "Antonio De Domenico"
                },
                {
                    "authorId": "70486867",
                    "name": "Fadhel Ayed"
                }
            ],
            "abstract": "The increasing interest in Large Language Models (LLMs) within the telecommunications sector underscores their potential to revolutionize operational efficiency. However, the deployment of these sophisticated models is often hampered by their substantial size and computational demands, raising concerns about their viability in resource-constrained environments. Addressing this challenge, recent advancements have seen the emergence of small language models that surprisingly exhibit performance comparable to their larger counterparts in many tasks, such as coding and common-sense reasoning. Phi-2, a compact yet powerful model, exemplifies this new wave of efficient small language models. This paper conducts a comprehensive evaluation of Phi-2\u2019s intrinsic understanding of the telecommunications domain. Recognizing the scale-related limitations, we enhance Phi-2\u2019s capabilities through a Retrieval-Augmented Generation approach, meticulously integrating an extensive knowledge base specifically curated with telecom standard specifications. The enhanced Phi-2 model demonstrates a profound improvement in accuracy, answering questions about telecom standards with a precision that closely rivals the more resource-intensive GPT-3.5. The paper further explores the refined capabilities of Phi-2 in addressing problem-solving scenarios within the telecom sector, highlighting its potentials and limitations.",
            "corpus_id": "268264533",
            "text": "The increasing interest in Large Language Models (LLMs) within the telecommunications sector underscores their potential to revolutionize operational efficiency. However, the deployment of these sophisticated models is often hampered by their substantial size and computational demands, raising concerns about their viability in resource-constrained environments. Addressing this challenge, recent advancements have seen the emergence of small language models that surprisingly exhibit performance comparable to their larger counterparts in many tasks, such as coding and common-sense reasoning. Phi-2, a compact yet powerful model, exemplifies this new wave of efficient small language models. This paper conducts a comprehensive evaluation of Phi-2\u2019s intrinsic understanding of the telecommunications domain. Recognizing the scale-related limitations, we enhance Phi-2\u2019s capabilities through a Retrieval-Augmented Generation approach, meticulously integrating an extensive knowledge base specifically curated with telecom standard specifications. The enhanced Phi-2 model demonstrates a profound improvement in accuracy, answering questions about telecom standards with a precision that closely rivals the more resource-intensive GPT-3.5. The paper further explores the refined capabilities of Phi-2 in addressing problem-solving scenarios within the telecom sector, highlighting its potentials and limitations.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.275634765625
        },
        {
            "paperId": "a90919e45fb535dc36d6da426d552104e8f2efe1",
            "corpusId": 267751469,
            "title": "Ask Optimal Questions: Aligning Large Language Models with Retriever's Preference in Conversational Search",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2024,
            "referenceCount": 61,
            "citationCount": 14,
            "influentialCitationCount": 5,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.11827, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2284687462",
                    "name": "Chanwoong Yoon"
                },
                {
                    "authorId": "1390543205",
                    "name": "Gangwoo Kim"
                },
                {
                    "authorId": "2261284448",
                    "name": "Byeongguk Jeon"
                },
                {
                    "authorId": "2829848",
                    "name": "Sungdong Kim"
                },
                {
                    "authorId": "2067643024",
                    "name": "Yohan Jo"
                },
                {
                    "authorId": "2261364403",
                    "name": "Jaewoo Kang"
                }
            ],
            "abstract": "Conversational search, unlike single-turn retrieval tasks, requires understanding the current question within a dialogue context. The common approach of rewrite-then-retrieve aims to decontextualize questions to be self-sufficient for off-the-shelf retrievers, but most existing methods produce sub-optimal query rewrites due to the limited ability to incorporate signals from the retrieval results. To overcome this limitation, we present a novel framework RetPO (Retriever's Preference Optimization), which is designed to optimize a language model (LM) for reformulating search queries in line with the preferences of the target retrieval systems. The process begins by prompting a large LM to produce various potential rewrites and then collects retrieval performance for these rewrites as the retrievers' preferences. Through the process, we construct a large-scale dataset called RF collection, containing Retrievers' Feedback on over 410K query rewrites across 12K conversations. Furthermore, we fine-tune a smaller LM on this dataset to align it with the retrievers' feedback. Our resulting model demonstrates superiority on two benchmarks, surpassing the previous state-of-the-art performance of rewrite-then-retrieve approaches.",
            "corpus_id": "267751469",
            "text": "Conversational search, unlike single-turn retrieval tasks, requires understanding the current question within a dialogue context. The common approach of rewrite-then-retrieve aims to decontextualize questions to be self-sufficient for off-the-shelf retrievers, but most existing methods produce sub-optimal query rewrites due to the limited ability to incorporate signals from the retrieval results. To overcome this limitation, we present a novel framework RetPO (Retriever's Preference Optimization), which is designed to optimize a language model (LM) for reformulating search queries in line with the preferences of the target retrieval systems. The process begins by prompting a large LM to produce various potential rewrites and then collects retrieval performance for these rewrites as the retrievers' preferences. Through the process, we construct a large-scale dataset called RF collection, containing Retrievers' Feedback on over 410K query rewrites across 12K conversations. Furthermore, we fine-tune a smaller LM on this dataset to align it with the retrievers' feedback. Our resulting model demonstrates superiority on two benchmarks, surpassing the previous state-of-the-art performance of rewrite-then-retrieve approaches.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.8818359375
        },
        {
            "paperId": "1ab76168a8fc29887ce54f21d518f528fc7d2ba2",
            "corpusId": 273502867,
            "title": "Who's Who: Large Language Models Meet Knowledge Conflicts in Practice",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "referenceCount": 25,
            "citationCount": 9,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.15737, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2326991628",
                    "name": "Quang Hieu Pham"
                },
                {
                    "authorId": "2302407608",
                    "name": "Hoang Ngo"
                },
                {
                    "authorId": "1755919",
                    "name": "A. Luu"
                },
                {
                    "authorId": "2302694602",
                    "name": "Dat Quoc Nguyen"
                }
            ],
            "abstract": "Retrieval-augmented generation (RAG) methods are viable solutions for addressing the static memory limits of pre-trained language models. Nevertheless, encountering conflicting sources of information within the retrieval context is an inevitable practical challenge. In such situations, the language models are recommended to transparently inform users about the conflicts rather than autonomously deciding what to present based on their inherent biases. To analyze how current large language models (LLMs) align with our recommendation, we introduce WhoQA, a public benchmark dataset to examine model's behavior in knowledge conflict situations. We induce conflicts by asking about a common property among entities having the same name, resulting in questions with up to 8 distinctive answers. WhoQA evaluation set includes 5K questions across 13 Wikidata property types and 150K Wikipedia entities. Our experiments show that despite the simplicity of WhoQA questions, knowledge conflicts significantly degrades LLMs' performance in RAG settings.",
            "corpus_id": "273502867",
            "text": "Retrieval-augmented generation (RAG) methods are viable solutions for addressing the static memory limits of pre-trained language models. Nevertheless, encountering conflicting sources of information within the retrieval context is an inevitable practical challenge. In such situations, the language models are recommended to transparently inform users about the conflicts rather than autonomously deciding what to present based on their inherent biases. To analyze how current large language models (LLMs) align with our recommendation, we introduce WhoQA, a public benchmark dataset to examine model's behavior in knowledge conflict situations. We induce conflicts by asking about a common property among entities having the same name, resulting in questions with up to 8 distinctive answers. WhoQA evaluation set includes 5K questions across 13 Wikidata property types and 150K Wikipedia entities. Our experiments show that despite the simplicity of WhoQA questions, knowledge conflicts significantly degrades LLMs' performance in RAG settings.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.603515625
        },
        {
            "paperId": "72af0f04349fe40fed4d98426d5c74637ccee5cd",
            "corpusId": 271098273,
            "title": "LM-PACE: Confidence Estimation by Large Language Models for Effective Root Causing of Cloud Incidents",
            "venue": "SIGSOFT FSE Companion",
            "year": 2024,
            "referenceCount": 34,
            "citationCount": 8,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3663529.3663858?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3663529.3663858, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2239168714",
                    "name": "Dylan Zhang"
                },
                {
                    "authorId": "2239159562",
                    "name": "Xuchao Zhang"
                },
                {
                    "authorId": "2239106685",
                    "name": "Chetan Bansal"
                },
                {
                    "authorId": "1403267039",
                    "name": "P. Las-Casas"
                },
                {
                    "authorId": "2239106331",
                    "name": "Rodrigo Fonseca"
                },
                {
                    "authorId": "148121358",
                    "name": "S. Rajmohan"
                }
            ],
            "abstract": "Major cloud providers have employed advanced AI-based solutions like large language models to aid humans in identifying the root causes of cloud incidents. Even though AI-driven assistants are be- coming more common in the process of analyzing root causes, their usefulness in supporting on-call engineers is limited by their unstable accuracy. This limitation arises from the fundamental challenges of the task, the tendency of language model-based methods to produce hallucinate information, and the difficulty in distinguishing these well-disguised hallucinations. To address this challenge, we propose a novel confidence estimation method to assign reliable confidence scores to root cause recommendations, aiding on-call engineers in deciding whether to trust the model\u2019s predictions. We made re- training-free confidence estimation on out-of-domain tasks possible via retrieval augmentation. To elicit better-calibrated confidence es- timates, we adopt a two-stage prompting procedure and a learnable transformation, which reduces the estimated calibration error (ECE) to 31% of the direct prompting baseline on a dataset comprising over 100,000 incidents from Microsoft. Additionally, we demonstrate that our method is applicable across various root cause prediction models. Our study takes an important move towards reliably and effectively embedding LLMs into cloud incident management systems",
            "corpus_id": "271098273",
            "text": "Major cloud providers have employed advanced AI-based solutions like large language models to aid humans in identifying the root causes of cloud incidents. Even though AI-driven assistants are be- coming more common in the process of analyzing root causes, their usefulness in supporting on-call engineers is limited by their unstable accuracy. This limitation arises from the fundamental challenges of the task, the tendency of language model-based methods to produce hallucinate information, and the difficulty in distinguishing these well-disguised hallucinations. To address this challenge, we propose a novel confidence estimation method to assign reliable confidence scores to root cause recommendations, aiding on-call engineers in deciding whether to trust the model\u2019s predictions. We made re- training-free confidence estimation on out-of-domain tasks possible via retrieval augmentation. To elicit better-calibrated confidence es- timates, we adopt a two-stage prompting procedure and a learnable transformation, which reduces the estimated calibration error (ECE) to 31% of the direct prompting baseline on a dataset comprising over 100,000 incidents from Microsoft. Additionally, we demonstrate that our method is applicable across various root cause prediction models. Our study takes an important move towards reliably and effectively embedding LLMs into cloud incident management systems",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.16943359375
        },
        {
            "paperId": "440b698bd341c7a44f407fa64f954ec06604e80b",
            "corpusId": 251067130,
            "title": "V2L: Leveraging Vision and Vision-language Models into Large-scale Product Retrieval",
            "venue": "arXiv.org",
            "year": 2022,
            "referenceCount": 26,
            "citationCount": 3,
            "influentialCitationCount": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2207.12994",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2207.12994, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2108977186",
                    "name": "Wenhao Wang"
                },
                {
                    "authorId": "2213925",
                    "name": "Yifan Sun"
                },
                {
                    "authorId": "15556978",
                    "name": "Zongxin Yang"
                },
                {
                    "authorId": "2143686789",
                    "name": "Yi Yang"
                }
            ],
            "abstract": "Product retrieval is of great importance in the ecommerce domain. This paper introduces our 1st-place solution in eBay eProduct Visual Search Challenge (FGVC9), which is featured for an ensemble of about 20 models from vision models and vision-language models. While model ensemble is common, we show that combining the vision models and vision-language models brings particular benefits from their complementarity and is a key factor to our superiority. Specifically, for the vision models, we use a two-stage training pipeline which first learns from the coarse labels provided in the training set and then conducts fine-grained self-supervised training, yielding a coarse-to-fine metric learning manner. For the vision-language models, we use the textual description of the training image as the supervision signals for fine-tuning the image-encoder (feature extractor). With these designs, our solution achieves 0.7623 MAR@10, ranking the first place among all the competitors. The code is available at: \\href{https://github.com/WangWenhao0716/V2L}{V$^2$L}.",
            "corpus_id": "251067130",
            "text": "Product retrieval is of great importance in the ecommerce domain. This paper introduces our 1st-place solution in eBay eProduct Visual Search Challenge (FGVC9), which is featured for an ensemble of about 20 models from vision models and vision-language models. While model ensemble is common, we show that combining the vision models and vision-language models brings particular benefits from their complementarity and is a key factor to our superiority. Specifically, for the vision models, we use a two-stage training pipeline which first learns from the coarse labels provided in the training set and then conducts fine-grained self-supervised training, yielding a coarse-to-fine metric learning manner. For the vision-language models, we use the textual description of the training image as the supervision signals for fine-tuning the image-encoder (feature extractor). With these designs, our solution achieves 0.7623 MAR@10, ranking the first place among all the competitors. The code is available at: \\href{https://github.com/WangWenhao0716/V2L}{V$^2$L}.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.344970703125
        },
        {
            "paperId": "dba13e820a3725ad497d3f8d866249f815e9ebac",
            "corpusId": 270357525,
            "title": "DiffuSyn Bench: Evaluating Vision-Language Models on Real-World Complexities with Diffusion-Generated Synthetic Benchmarks",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 15,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.04470, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2305640968",
                    "name": "Haokun Zhou"
                },
                {
                    "authorId": "2305563441",
                    "name": "Yipeng Hong"
                }
            ],
            "abstract": "This study assesses the ability of Large Vision-Language Models (LVLMs) to differentiate between AI-generated and human-generated images. It introduces a new automated benchmark construction method for this evaluation. The experiment compared common LVLMs with human participants using a mixed dataset of AI and human-created images. Results showed that LVLMs could distinguish between the image types to some extent but exhibited a rightward bias, and perform significantly worse compared to humans. To build on these findings, we developed an automated benchmark construction process using AI. This process involved topic retrieval, narrative script generation, error embedding, and image generation, creating a diverse set of text-image pairs with intentional errors. We validated our method through constructing two caparable benchmarks. This study highlights the strengths and weaknesses of LVLMs in real-world understanding and advances benchmark construction techniques, providing a scalable and automatic approach for AI model evaluation.",
            "corpus_id": "270357525",
            "text": "This study assesses the ability of Large Vision-Language Models (LVLMs) to differentiate between AI-generated and human-generated images. It introduces a new automated benchmark construction method for this evaluation. The experiment compared common LVLMs with human participants using a mixed dataset of AI and human-created images. Results showed that LVLMs could distinguish between the image types to some extent but exhibited a rightward bias, and perform significantly worse compared to humans. To build on these findings, we developed an automated benchmark construction process using AI. This process involved topic retrieval, narrative script generation, error embedding, and image generation, creating a diverse set of text-image pairs with intentional errors. We validated our method through constructing two caparable benchmarks. This study highlights the strengths and weaknesses of LVLMs in real-world understanding and advances benchmark construction techniques, providing a scalable and automatic approach for AI model evaluation.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.0640869140625
        },
        {
            "paperId": "5e4e9a9f8564e3e60e6520f602d9373cf7884b4a",
            "corpusId": 278310759,
            "title": "Seeking to Collide: Online Safety-Critical Scenario Generation for Autonomous Driving with Retrieval Augmented Large Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 27,
            "citationCount": 3,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.00972, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2269471021",
                    "name": "Yuewen Mei"
                },
                {
                    "authorId": "94168461",
                    "name": "Tong Nie"
                },
                {
                    "authorId": "2283484643",
                    "name": "Jian Sun"
                },
                {
                    "authorId": "2143699657",
                    "name": "Ye Tian"
                }
            ],
            "abstract": "Simulation-based testing is crucial for validating autonomous vehicles (AVs), yet existing scenario generation methods either overfit to common driving patterns or operate in an offline, non-interactive manner that fails to expose rare, safety-critical corner cases. In this paper, we introduce an online, retrieval-augmented large language model (LLM) framework for generating safety-critical driving scenarios. Our method first employs an LLM-based behavior analyzer to infer the most dangerous intent of the background vehicle from the observed state, then queries additional LLM agents to synthesize feasible adversarial trajectories. To mitigate catastrophic forgetting and accelerate adaptation, we augment the framework with a dynamic memorization and retrieval bank of intent-planner pairs, automatically expanding its behavioral library when novel intents arise. Evaluations using the Waymo Open Motion Dataset demonstrate that our model reduces the mean minimum time-to-collision from 1.62 to 1.08 s and incurs a 75% collision rate, substantially outperforming baselines.",
            "corpus_id": "278310759",
            "text": "Simulation-based testing is crucial for validating autonomous vehicles (AVs), yet existing scenario generation methods either overfit to common driving patterns or operate in an offline, non-interactive manner that fails to expose rare, safety-critical corner cases. In this paper, we introduce an online, retrieval-augmented large language model (LLM) framework for generating safety-critical driving scenarios. Our method first employs an LLM-based behavior analyzer to infer the most dangerous intent of the background vehicle from the observed state, then queries additional LLM agents to synthesize feasible adversarial trajectories. To mitigate catastrophic forgetting and accelerate adaptation, we augment the framework with a dynamic memorization and retrieval bank of intent-planner pairs, automatically expanding its behavioral library when novel intents arise. Evaluations using the Waymo Open Motion Dataset demonstrate that our model reduces the mean minimum time-to-collision from 1.62 to 1.08 s and incurs a 75% collision rate, substantially outperforming baselines.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.423583984375
        },
        {
            "paperId": "e2f1f04f648a8863d11439aa4c80ee65d6caccda",
            "corpusId": 261531214,
            "title": "ModelScope-Agent: Building Your Customizable Agent System with Open-source Large Language Models",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2023,
            "referenceCount": 25,
            "citationCount": 21,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2309.00986",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.00986, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "143971529",
                    "name": "Chenliang Li"
                },
                {
                    "authorId": "123655156",
                    "name": "Hehong Chen"
                },
                {
                    "authorId": "2114009661",
                    "name": "Mingshi Yan"
                },
                {
                    "authorId": "2237809880",
                    "name": "Weizhou Shen"
                },
                {
                    "authorId": "153194420",
                    "name": "Haiyang Xu"
                },
                {
                    "authorId": "2238047773",
                    "name": "Zhikai Wu"
                },
                {
                    "authorId": "2237946970",
                    "name": "Zhicheng Zhang"
                },
                {
                    "authorId": "2237956023",
                    "name": "Wenmeng Zhou"
                },
                {
                    "authorId": "2237827934",
                    "name": "Yingda Chen"
                },
                {
                    "authorId": "2237996616",
                    "name": "Chen Cheng"
                },
                {
                    "authorId": "2238549612",
                    "name": "Hongzhu Shi"
                },
                {
                    "authorId": "2116921824",
                    "name": "Ji Zhang"
                },
                {
                    "authorId": "143857288",
                    "name": "Fei Huang"
                },
                {
                    "authorId": "2237981776",
                    "name": "Jingren Zhou"
                }
            ],
            "abstract": "Large language models (LLMs) have recently demonstrated remarkable capabilities to comprehend human intentions, engage in reasoning, and design planning-like behavior. To further unleash the power of LLMs to accomplish complex tasks, there is a growing trend to build agent framework that equips LLMs, such as ChatGPT, with tool-use abilities to connect with massive external APIs. In this work, we introduce ModelScope-Agent, a general and customizable agent framework for real-world applications, based on open-source LLMs as controllers. It provides a user-friendly system library, with customizable engine design to support model training on multiple open-source LLMs, while also enabling seamless integration with both model APIs and common APIs in a unified way. To equip the LLMs with tool-use abilities, a comprehensive framework has been proposed spanning over tool-use data collection, tool retrieval, tool registration, memory control, customized model training, and evaluation for practical real-world applications. Finally, we showcase ModelScopeGPT, a real-world intelligent assistant of ModelScope Community based on the ModelScope-Agent framework, which is able to connect open-source LLMs with more than 1000 public AI models and localized community knowledge in ModelScope. The ModelScope-Agent library\\footnote{https://github.com/modelscope/modelscope-agent} and online demo\\footnote{https://modelscope.cn/studios/damo/ModelScopeGPT/summary} are now publicly available.",
            "corpus_id": "261531214",
            "text": "Large language models (LLMs) have recently demonstrated remarkable capabilities to comprehend human intentions, engage in reasoning, and design planning-like behavior. To further unleash the power of LLMs to accomplish complex tasks, there is a growing trend to build agent framework that equips LLMs, such as ChatGPT, with tool-use abilities to connect with massive external APIs. In this work, we introduce ModelScope-Agent, a general and customizable agent framework for real-world applications, based on open-source LLMs as controllers. It provides a user-friendly system library, with customizable engine design to support model training on multiple open-source LLMs, while also enabling seamless integration with both model APIs and common APIs in a unified way. To equip the LLMs with tool-use abilities, a comprehensive framework has been proposed spanning over tool-use data collection, tool retrieval, tool registration, memory control, customized model training, and evaluation for practical real-world applications. Finally, we showcase ModelScopeGPT, a real-world intelligent assistant of ModelScope Community based on the ModelScope-Agent framework, which is able to connect open-source LLMs with more than 1000 public AI models and localized community knowledge in ModelScope. The ModelScope-Agent library\\footnote{https://github.com/modelscope/modelscope-agent} and online demo\\footnote{https://modelscope.cn/studios/damo/ModelScopeGPT/summary} are now publicly available.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.089111328125
        },
        {
            "paperId": "4f5a5a7f4f90fde2b23fc496ec7fc8d429d4bc5e",
            "corpusId": 258762424,
            "title": "Learning the Visualness of Text Using Large Vision-Language Models",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2023,
            "referenceCount": 46,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2305.10434",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.10434, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "145816931",
                    "name": "Gaurav Verma"
                },
                {
                    "authorId": "2066337266",
                    "name": "Ryan A. Rossi"
                },
                {
                    "authorId": "67319819",
                    "name": "Chris Tensmeyer"
                },
                {
                    "authorId": "2174964",
                    "name": "Jiuxiang Gu"
                },
                {
                    "authorId": "3115414",
                    "name": "A. Nenkova"
                }
            ],
            "abstract": "Visual text evokes an image in a person's mind, while non-visual text fails to do so. A method to automatically detect visualness in text will enable text-to-image retrieval and generation models to augment text with relevant images. This is particularly challenging with long-form text as text-to-image generation and retrieval models are often triggered for text that is designed to be explicitly visual in nature, whereas long-form text could contain many non-visual sentences. To this end, we curate a dataset of 3,620 English sentences and their visualness scores provided by multiple human annotators. We also propose a fine-tuning strategy that adapts large vision-language models like CLIP by modifying the model's contrastive learning objective to map text identified as non-visual to a common NULL image while matching visual text to their corresponding images in the document. We evaluate the proposed approach on its ability to (i) classify visual and non-visual text accurately, and (ii) attend over words that are identified as visual in psycholinguistic studies. Empirical evaluation indicates that our approach performs better than several heuristics and baseline models for the proposed task. Furthermore, to highlight the importance of modeling the visualness of text, we conduct qualitative analyses of text-to-image generation systems like DALL-E. Project webpage: https://gaurav22verma.github.io/text-visualness/",
            "corpus_id": "258762424",
            "text": "Visual text evokes an image in a person's mind, while non-visual text fails to do so. A method to automatically detect visualness in text will enable text-to-image retrieval and generation models to augment text with relevant images. This is particularly challenging with long-form text as text-to-image generation and retrieval models are often triggered for text that is designed to be explicitly visual in nature, whereas long-form text could contain many non-visual sentences. To this end, we curate a dataset of 3,620 English sentences and their visualness scores provided by multiple human annotators. We also propose a fine-tuning strategy that adapts large vision-language models like CLIP by modifying the model's contrastive learning objective to map text identified as non-visual to a common NULL image while matching visual text to their corresponding images in the document. We evaluate the proposed approach on its ability to (i) classify visual and non-visual text accurately, and (ii) attend over words that are identified as visual in psycholinguistic studies. Empirical evaluation indicates that our approach performs better than several heuristics and baseline models for the proposed task. Furthermore, to highlight the importance of modeling the visualness of text, we conduct qualitative analyses of text-to-image generation systems like DALL-E. Project webpage: https://gaurav22verma.github.io/text-visualness/",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.198486328125
        },
        {
            "paperId": "567469dc08fbb702df9f525637e9a3fc43bb1fbb",
            "corpusId": 225086076,
            "title": "Probing Contextual Language Models for Common Ground with Visual Representations",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2020,
            "referenceCount": 73,
            "citationCount": 25,
            "influentialCitationCount": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2021.naacl-main.422.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2021.naacl-main.422, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2123694087",
                    "name": "Gabriel Ilharco"
                },
                {
                    "authorId": "2545335",
                    "name": "Rowan Zellers"
                },
                {
                    "authorId": "143787583",
                    "name": "Ali Farhadi"
                },
                {
                    "authorId": "2548384",
                    "name": "Hannaneh Hajishirzi"
                }
            ],
            "abstract": "The success of large-scale contextual language models has attracted great interest in probing what is encoded in their representations. In this work, we consider a new question: to what extent contextual representations of concrete nouns are aligned with corresponding visual representations? We design a probing model that evaluates how effective are text-only representations in distinguishing between matching and non-matching visual representations. Our findings show that language representations alone provide a strong signal for retrieving image patches from the correct object categories. Moreover, they are effective in retrieving specific instances of image patches; textual context plays an important role in this process. Visually grounded language models slightly outperform text-only language models in instance retrieval, but greatly under-perform humans. We hope our analyses inspire future research in understanding and improving the visual capabilities of language models.",
            "corpus_id": "225086076",
            "text": "The success of large-scale contextual language models has attracted great interest in probing what is encoded in their representations. In this work, we consider a new question: to what extent contextual representations of concrete nouns are aligned with corresponding visual representations? We design a probing model that evaluates how effective are text-only representations in distinguishing between matching and non-matching visual representations. Our findings show that language representations alone provide a strong signal for retrieving image patches from the correct object categories. Moreover, they are effective in retrieving specific instances of image patches; textual context plays an important role in this process. Visually grounded language models slightly outperform text-only language models in instance retrieval, but greatly under-perform humans. We hope our analyses inspire future research in understanding and improving the visual capabilities of language models.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.51318359375
        },
        {
            "paperId": "9b52afc58ea4326642970e75b8b10d6a97090900",
            "corpusId": 275095102,
            "title": "Evaluation of the integration of retrieval-augmented generation in large language model for breast cancer nursing care responses",
            "venue": "Scientific Reports",
            "year": 2024,
            "referenceCount": 27,
            "citationCount": 3,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.nature.com/articles/s41598-024-81052-3.pdf",
                "status": "GOLD",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11680762, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2338038620",
                    "name": "Ruiyu Xu"
                },
                {
                    "authorId": "2337665557",
                    "name": "Ying Hong"
                },
                {
                    "authorId": "2337647216",
                    "name": "Feifei Zhang"
                },
                {
                    "authorId": "2218072985",
                    "name": "Hongmei Xu"
                }
            ],
            "abstract": "Breast cancer is one of the most common malignant tumors in women worldwide. Although large language models (LLMs) can provide breast cancer nursing care consultation, inherent hallucinations can lead to inaccurate responses. Retrieval-augmented generation (RAG) technology can improve LLM performance, offering a new approach for clinical applications. In the present study, we evaluated the performance of a LLM in breast cancer nursing care using RAG technology. In the control group (GPT-4), questions were answered directly using the GPT-4 model, whereas the experimental group (RAG-GPT) used the GPT-4 model combined with RAG. A knowledge base for breast cancer nursing was created for the RAG-GPT group, and 15 of 200 real-world clinical care questions were answered randomly. The primary endpoint was overall satisfaction, and the secondary endpoints were accuracy and empathy. RAG-GPT included a curated knowledge base related to breast cancer nursing care, including textbooks, guidelines, and traditional Chinese therapy. The RAG-GPT group showed significantly higher overall satisfaction than that of the GPT-4 group (8.4\u2009\u00b1\u20090.84 vs. 5.4\u2009\u00b1\u20091.27, p\u2009<\u20090.01) as well as an improved accuracy of responses (8.6\u2009\u00b1\u20090.69 vs. 5.6\u2009\u00b1\u20090.96, p\u2009<\u20090.01). However, there was no inter-group difference in empathy (8.4\u2009\u00b1\u20090.85 vs. 7.8\u2009\u00b1\u20091.22, p\u2009>\u20090.05). Overall, this study revealed that RAG technology could improve LLM performance significantly, likely because of the increased accuracy of the answers without diminishing empathy. These findings provide a theoretical basis for applying RAG technology to LLMs in clinical nursing practice and education. Supplementary Information The online version contains supplementary material available at 10.1038/s41598-024-81052-3.",
            "corpus_id": "275095102",
            "text": "Breast cancer is one of the most common malignant tumors in women worldwide. Although large language models (LLMs) can provide breast cancer nursing care consultation, inherent hallucinations can lead to inaccurate responses. Retrieval-augmented generation (RAG) technology can improve LLM performance, offering a new approach for clinical applications. In the present study, we evaluated the performance of a LLM in breast cancer nursing care using RAG technology. In the control group (GPT-4), questions were answered directly using the GPT-4 model, whereas the experimental group (RAG-GPT) used the GPT-4 model combined with RAG. A knowledge base for breast cancer nursing was created for the RAG-GPT group, and 15 of 200 real-world clinical care questions were answered randomly. The primary endpoint was overall satisfaction, and the secondary endpoints were accuracy and empathy. RAG-GPT included a curated knowledge base related to breast cancer nursing care, including textbooks, guidelines, and traditional Chinese therapy. The RAG-GPT group showed significantly higher overall satisfaction than that of the GPT-4 group (8.4\u2009\u00b1\u20090.84 vs. 5.4\u2009\u00b1\u20091.27, p\u2009<\u20090.01) as well as an improved accuracy of responses (8.6\u2009\u00b1\u20090.69 vs. 5.6\u2009\u00b1\u20090.96, p\u2009<\u20090.01). However, there was no inter-group difference in empathy (8.4\u2009\u00b1\u20090.85 vs. 7.8\u2009\u00b1\u20091.22, p\u2009>\u20090.05). Overall, this study revealed that RAG technology could improve LLM performance significantly, likely because of the increased accuracy of the answers without diminishing empathy. These findings provide a theoretical basis for applying RAG technology to LLMs in clinical nursing practice and education. Supplementary Information The online version contains supplementary material available at 10.1038/s41598-024-81052-3.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.1539306640625
        },
        {
            "paperId": "f0b46c4a549fc0bae49c774736a9eb6f05ab7c1b",
            "corpusId": 270440778,
            "title": "Enhancing Diagnostic Accuracy in Rare and Common Fundus Diseases with a Knowledge-Rich Vision-Language Model",
            "venue": "",
            "year": 2024,
            "referenceCount": 59,
            "citationCount": 3,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.09317, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2306391683",
                    "name": "Meng Wang"
                },
                {
                    "authorId": "2306763037",
                    "name": "Tian Lin"
                },
                {
                    "authorId": "145494496",
                    "name": "Aidi Lin"
                },
                {
                    "authorId": "2306948052",
                    "name": "Kai Yu"
                },
                {
                    "authorId": "1796227131",
                    "name": "Yuanyuan Peng"
                },
                {
                    "authorId": "2144690783",
                    "name": "Lianyu Wang"
                },
                {
                    "authorId": "2306163543",
                    "name": "Cheng Chen"
                },
                {
                    "authorId": "2306253925",
                    "name": "Ke Zou"
                },
                {
                    "authorId": "2184745935",
                    "name": "Huiyu Liang"
                },
                {
                    "authorId": "2306347486",
                    "name": "Man Chen"
                },
                {
                    "authorId": "2307608222",
                    "name": "Xue Yao"
                },
                {
                    "authorId": "2307582916",
                    "name": "Meiqin Zhang"
                },
                {
                    "authorId": "2310863680",
                    "name": "Binwei Huang"
                },
                {
                    "authorId": "2306182345",
                    "name": "Chaoxin Zheng"
                },
                {
                    "authorId": "2275537613",
                    "name": "Pei-Xi Zhang"
                },
                {
                    "authorId": "2285644300",
                    "name": "Wei Chen"
                },
                {
                    "authorId": "2179921178",
                    "name": "Yi Luo"
                },
                {
                    "authorId": "2289384933",
                    "name": "Yifan Chen"
                },
                {
                    "authorId": "6684751",
                    "name": "Hong-he Xia"
                },
                {
                    "authorId": "13933602",
                    "name": "Tingkun Shi"
                },
                {
                    "authorId": "2267031197",
                    "name": "Qi Zhang"
                },
                {
                    "authorId": "2309202145",
                    "name": "Jinming Guo"
                },
                {
                    "authorId": "2307602954",
                    "name": "Xiaolin Chen"
                },
                {
                    "authorId": "2284930033",
                    "name": "Jingcheng Wang"
                },
                {
                    "authorId": "6412770",
                    "name": "Y. Tham"
                },
                {
                    "authorId": "2287802973",
                    "name": "Dianbo Liu"
                },
                {
                    "authorId": "2279975227",
                    "name": "Wendy Wong"
                },
                {
                    "authorId": "100970072",
                    "name": "Sahil Thakur"
                },
                {
                    "authorId": "11948718",
                    "name": "B. Fenner"
                },
                {
                    "authorId": "2258072062",
                    "name": "D. Fang"
                },
                {
                    "authorId": "2308546420",
                    "name": "Si Liu"
                },
                {
                    "authorId": "2309204182",
                    "name": "Qingyun Liu"
                },
                {
                    "authorId": "2301620734",
                    "name": "Yuqiang Huang"
                },
                {
                    "authorId": "2309685697",
                    "name": "Hongqiang Zeng"
                },
                {
                    "authorId": "2306751693",
                    "name": "Yanda Meng"
                },
                {
                    "authorId": "2300824614",
                    "name": "Yukun Zhou"
                },
                {
                    "authorId": "2278143872",
                    "name": "Zehua Jiang"
                },
                {
                    "authorId": "2338383701",
                    "name": "Min Qiu"
                },
                {
                    "authorId": "2302950428",
                    "name": "Changqing Zhang"
                },
                {
                    "authorId": "2306477326",
                    "name": "Xinjian Chen"
                },
                {
                    "authorId": "2306059715",
                    "name": "Sophia Y. Wang"
                },
                {
                    "authorId": "2306410742",
                    "name": "Cecilia S. Lee"
                },
                {
                    "authorId": "2275571778",
                    "name": "Lucia Sobrin"
                },
                {
                    "authorId": "2238411657",
                    "name": "C. Y. Cheung"
                },
                {
                    "authorId": "2194281056",
                    "name": "Chi Pui Calvin Pang"
                },
                {
                    "authorId": "2240129086",
                    "name": "P. Keane"
                },
                {
                    "authorId": "2287996623",
                    "name": "Ching-Yu Cheng"
                },
                {
                    "authorId": "2279897995",
                    "name": "Haoyu Chen"
                },
                {
                    "authorId": "2302371411",
                    "name": "Huazhu Fu"
                }
            ],
            "abstract": "Previous foundation models for fundus images were pre-trained with limited disease categories and knowledge base. Here we introduce a knowledge-rich vision-language model (RetiZero) that leverages knowledge from more than 400 fundus diseases. For RetiZero's pretraining, we compiled 341,896 fundus images paired with texts, sourced from public datasets, ophthalmic literature, and online resources, encompassing a diverse range of diseases across multiple ethnicities and countries. RetiZero exhibits remarkable performance in several downstream tasks, including zero-shot disease recognition, image-to-image retrieval, AI-assisted clinical diagnosis,few-shot fine-tuning, and internal- and cross-domain disease identification. In zero-shot scenarios, RetiZero achieves Top-5 accuracies of 0.843 for 15 diseases and 0.756 for 52 diseases. For image retrieval, it achieves Top-5 scores of 0.950 and 0.886 for the same sets, respectively. AI-assisted clinical diagnosis results show that RetiZero's Top-3 zero-shot performance surpasses the average of 19 ophthalmologists from Singapore, China, and the United States. RetiZero substantially enhances clinicians' accuracy in diagnosing fundus diseases, in particularly rare ones. These findings underscore the value of integrating the RetiZero into clinical settings, where various fundus diseases are encountered.",
            "corpus_id": "270440778",
            "text": "Previous foundation models for fundus images were pre-trained with limited disease categories and knowledge base. Here we introduce a knowledge-rich vision-language model (RetiZero) that leverages knowledge from more than 400 fundus diseases. For RetiZero's pretraining, we compiled 341,896 fundus images paired with texts, sourced from public datasets, ophthalmic literature, and online resources, encompassing a diverse range of diseases across multiple ethnicities and countries. RetiZero exhibits remarkable performance in several downstream tasks, including zero-shot disease recognition, image-to-image retrieval, AI-assisted clinical diagnosis,few-shot fine-tuning, and internal- and cross-domain disease identification. In zero-shot scenarios, RetiZero achieves Top-5 accuracies of 0.843 for 15 diseases and 0.756 for 52 diseases. For image retrieval, it achieves Top-5 scores of 0.950 and 0.886 for the same sets, respectively. AI-assisted clinical diagnosis results show that RetiZero's Top-3 zero-shot performance surpasses the average of 19 ophthalmologists from Singapore, China, and the United States. RetiZero substantially enhances clinicians' accuracy in diagnosing fundus diseases, in particularly rare ones. These findings underscore the value of integrating the RetiZero into clinical settings, where various fundus diseases are encountered.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.0236968994140625
        },
        {
            "paperId": "bcd8b4954fb94feaf45912b4654ebda31c390eba",
            "corpusId": 254685724,
            "title": "DeepJoin: Joinable Table Discovery with Pre-trained Language Models",
            "venue": "Proceedings of the VLDB Endowment",
            "year": 2022,
            "referenceCount": 67,
            "citationCount": 20,
            "influentialCitationCount": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2212.07588",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2212.07588, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2815918",
                    "name": "Yuyang Dong"
                },
                {
                    "authorId": "153066261",
                    "name": "Chuan Xiao"
                },
                {
                    "authorId": "27728184",
                    "name": "Takuma Nozawa"
                },
                {
                    "authorId": "2052977310",
                    "name": "Masafumi Enomoto"
                },
                {
                    "authorId": "37267314",
                    "name": "M. Oyamada"
                }
            ],
            "abstract": "Due to the usefulness in data enrichment for data analysis tasks, joinable table discovery has become an important operation in data lake management. Existing approaches target equi-joins, the most common way of combining tables for creating a unified view, or semantic joins, which tolerate misspellings and different formats to deliver more join results. They are either exact solutions whose running time is linear in the sizes of query column and target table repository, or approximate solutions lacking precision. In this paper, we propose DeepJoin, a deep learning model for accurate and efficient joinable table discovery. Our solution is an embedding-based retrieval, which employs a pre-trained language model (PLM) and is designed as one framework serving both equi- and semantic (with a similarity condition on word embeddings) joins for textual attributes with fairly small cardinalities. We propose a set of contextualization options to transform column contents to a text sequence. The PLM reads the sequence and is fine-tuned to embed columns to vectors such that columns are expected to be joinable if they are close to each other in the vector space. Since the output of the PLM is fixed in length, the subsequent search procedure becomes independent of the column size. With a state-of-the-art approximate nearest neighbor search algorithm, the search time is sublinear in the repository size. To train the model, we devise the techniques for preparing training data as well as data augmentation. The experiments on real datasets demonstrate that by training on a small subset of a corpus, DeepJoin generalizes to large datasets and its precision consistently outperforms other approximate solutions'. DeepJoin is even more accurate than an exact solution to semantic joins when evaluated with labels from experts. Moreover, when equipped with a GPU, DeepJoin is up to two orders of magnitude faster than existing solutions.",
            "corpus_id": "254685724",
            "text": "Due to the usefulness in data enrichment for data analysis tasks, joinable table discovery has become an important operation in data lake management. Existing approaches target equi-joins, the most common way of combining tables for creating a unified view, or semantic joins, which tolerate misspellings and different formats to deliver more join results. They are either exact solutions whose running time is linear in the sizes of query column and target table repository, or approximate solutions lacking precision. In this paper, we propose DeepJoin, a deep learning model for accurate and efficient joinable table discovery. Our solution is an embedding-based retrieval, which employs a pre-trained language model (PLM) and is designed as one framework serving both equi- and semantic (with a similarity condition on word embeddings) joins for textual attributes with fairly small cardinalities. We propose a set of contextualization options to transform column contents to a text sequence. The PLM reads the sequence and is fine-tuned to embed columns to vectors such that columns are expected to be joinable if they are close to each other in the vector space. Since the output of the PLM is fixed in length, the subsequent search procedure becomes independent of the column size. With a state-of-the-art approximate nearest neighbor search algorithm, the search time is sublinear in the repository size. To train the model, we devise the techniques for preparing training data as well as data augmentation. The experiments on real datasets demonstrate that by training on a small subset of a corpus, DeepJoin generalizes to large datasets and its precision consistently outperforms other approximate solutions'. DeepJoin is even more accurate than an exact solution to semantic joins when evaluated with labels from experts. Moreover, when equipped with a GPU, DeepJoin is up to two orders of magnitude faster than existing solutions.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.8056640625
        },
        {
            "paperId": "66dcfd17ce3553216eab5c4105760e0fd2a21060",
            "corpusId": 276258680,
            "title": "Scaling Pre-training to One Hundred Billion Data for Vision Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 0,
            "citationCount": 5,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.07617, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2290284032",
                    "name": "Xiao Wang"
                },
                {
                    "authorId": "2922782",
                    "name": "Ibrahim M. Alabdulmohsin"
                },
                {
                    "authorId": "3821662",
                    "name": "Daniel M. Salz"
                },
                {
                    "authorId": "2349544211",
                    "name": "Zhe Li"
                },
                {
                    "authorId": "1996199677",
                    "name": "Keran Rong"
                },
                {
                    "authorId": "2045380893",
                    "name": "Xiao-Qi Zhai"
                }
            ],
            "abstract": "We provide an empirical investigation of the potential of pre-training vision-language models on an unprecedented scale: 100 billion examples. We find that model performance tends to saturate at this scale on many common Western-centric classification and retrieval benchmarks, such as COCO Captions. Nevertheless, tasks of cultural diversity achieve more substantial gains from the 100-billion scale web data, thanks to its coverage of long-tail concepts. Furthermore, we analyze the model's multilinguality and show gains in low-resource languages as well. In addition, we observe that reducing the size of the pretraining dataset via quality filters like using CLIP, typically used to enhance performance, may inadvertently reduce the cultural diversity represented even in large-scale datasets. Our results highlight that while traditional benchmarks may not benefit significantly from scaling noisy, raw web data to 100 billion examples, this data scale is vital for building truly inclusive multimodal systems.",
            "corpus_id": "276258680",
            "text": "We provide an empirical investigation of the potential of pre-training vision-language models on an unprecedented scale: 100 billion examples. We find that model performance tends to saturate at this scale on many common Western-centric classification and retrieval benchmarks, such as COCO Captions. Nevertheless, tasks of cultural diversity achieve more substantial gains from the 100-billion scale web data, thanks to its coverage of long-tail concepts. Furthermore, we analyze the model's multilinguality and show gains in low-resource languages as well. In addition, we observe that reducing the size of the pretraining dataset via quality filters like using CLIP, typically used to enhance performance, may inadvertently reduce the cultural diversity represented even in large-scale datasets. Our results highlight that while traditional benchmarks may not benefit significantly from scaling noisy, raw web data to 100 billion examples, this data scale is vital for building truly inclusive multimodal systems.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.08209228515625
        },
        {
            "paperId": "1d79f54ad68408a62f8589828b4b6104189f257a",
            "corpusId": 276408137,
            "title": "Exploring Large Language Models in Healthcare: Insights into Corpora Sources, Customization Strategies, and Evaluation Metrics",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 0,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.11861, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2338396164",
                    "name": "Shuqi Yang"
                },
                {
                    "authorId": "2345816120",
                    "name": "Mingrui Jing"
                },
                {
                    "authorId": "2346357938",
                    "name": "Shuai Wang"
                },
                {
                    "authorId": "2345815877",
                    "name": "Jiaxin Kou"
                },
                {
                    "authorId": "2346647262",
                    "name": "Manfei Shi"
                },
                {
                    "authorId": "49716194",
                    "name": "Weijie Xing"
                },
                {
                    "authorId": "2346257001",
                    "name": "Yan Hu"
                },
                {
                    "authorId": "2318120658",
                    "name": "Zheng Zhu"
                }
            ],
            "abstract": "This study reviewed the use of Large Language Models (LLMs) in healthcare, focusing on their training corpora, customization techniques, and evaluation metrics. A systematic search of studies from 2021 to 2024 identified 61 articles. Four types of corpora were used: clinical resources, literature, open-source datasets, and web-crawled data. Common construction techniques included pre-training, prompt engineering, and retrieval-augmented generation, with 44 studies combining multiple methods. Evaluation metrics were categorized into process, usability, and outcome metrics, with outcome metrics divided into model-based and expert-assessed outcomes. The study identified critical gaps in corpus fairness, which contributed to biases from geographic, cultural, and socio-economic factors. The reliance on unverified or unstructured data highlighted the need for better integration of evidence-based clinical guidelines. Future research should focus on developing a tiered corpus architecture with vetted sources and dynamic weighting, while ensuring model transparency. Additionally, the lack of standardized evaluation frameworks for domain-specific models called for comprehensive validation of LLMs in real-world healthcare settings.",
            "corpus_id": "276408137",
            "text": "This study reviewed the use of Large Language Models (LLMs) in healthcare, focusing on their training corpora, customization techniques, and evaluation metrics. A systematic search of studies from 2021 to 2024 identified 61 articles. Four types of corpora were used: clinical resources, literature, open-source datasets, and web-crawled data. Common construction techniques included pre-training, prompt engineering, and retrieval-augmented generation, with 44 studies combining multiple methods. Evaluation metrics were categorized into process, usability, and outcome metrics, with outcome metrics divided into model-based and expert-assessed outcomes. The study identified critical gaps in corpus fairness, which contributed to biases from geographic, cultural, and socio-economic factors. The reliance on unverified or unstructured data highlighted the need for better integration of evidence-based clinical guidelines. Future research should focus on developing a tiered corpus architecture with vetted sources and dynamic weighting, while ensuring model transparency. Additionally, the lack of standardized evaluation frameworks for domain-specific models called for comprehensive validation of LLMs in real-world healthcare settings.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.2203369140625
        },
        {
            "paperId": "aeccc10dde5d8575c0a3899a82e30f6cd82a9f9c",
            "corpusId": 267100462,
            "title": "Online Health Search Via Multidimensional Information Quality Assessment Based on Deep Language Models: Algorithm Development and Validation",
            "venue": "JMIR AI",
            "year": 2022,
            "referenceCount": 79,
            "citationCount": 2,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.2196/42630",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11099810, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2141935582",
                    "name": "Boya Zhang"
                },
                {
                    "authorId": "33494034",
                    "name": "Nona Naderi"
                },
                {
                    "authorId": "2280382961",
                    "name": "Rahul Mishra"
                },
                {
                    "authorId": "2268099884",
                    "name": "Douglas Teodoro"
                }
            ],
            "abstract": "Background Widespread misinformation in web resources can lead to serious implications for individuals seeking health advice. Despite that, information retrieval models are often focused only on the query-document relevance dimension to rank results. Objective We investigate a multidimensional information quality retrieval model based on deep learning to enhance the effectiveness of online health care information search results. Methods In this study, we simulated online health information search scenarios with a topic set of 32 different health-related inquiries and a corpus containing 1 billion web documents from the April 2019 snapshot of Common Crawl. Using state-of-the-art pretrained language models, we assessed the quality of the retrieved documents according to their usefulness, supportiveness, and credibility dimensions for a given search query on 6030 human-annotated, query-document pairs. We evaluated this approach using transfer learning and more specific domain adaptation techniques. Results In the transfer learning setting, the usefulness model provided the largest distinction between help- and harm-compatible documents, with a difference of +5.6%, leading to a majority of helpful documents in the top 10 retrieved. The supportiveness model achieved the best harm compatibility (+2.4%), while the combination of usefulness, supportiveness, and credibility models achieved the largest distinction between help- and harm-compatibility on helpful topics (+16.9%). In the domain adaptation setting, the linear combination of different models showed robust performance, with help-harm compatibility above +4.4% for all dimensions and going as high as +6.8%. Conclusions These results suggest that integrating automatic ranking models created for specific information quality dimensions can increase the effectiveness of health-related information retrieval. Thus, our approach could be used to enhance searches made by individuals seeking online health information.",
            "corpus_id": "267100462",
            "text": "Background Widespread misinformation in web resources can lead to serious implications for individuals seeking health advice. Despite that, information retrieval models are often focused only on the query-document relevance dimension to rank results. Objective We investigate a multidimensional information quality retrieval model based on deep learning to enhance the effectiveness of online health care information search results. Methods In this study, we simulated online health information search scenarios with a topic set of 32 different health-related inquiries and a corpus containing 1 billion web documents from the April 2019 snapshot of Common Crawl. Using state-of-the-art pretrained language models, we assessed the quality of the retrieved documents according to their usefulness, supportiveness, and credibility dimensions for a given search query on 6030 human-annotated, query-document pairs. We evaluated this approach using transfer learning and more specific domain adaptation techniques. Results In the transfer learning setting, the usefulness model provided the largest distinction between help- and harm-compatible documents, with a difference of +5.6%, leading to a majority of helpful documents in the top 10 retrieved. The supportiveness model achieved the best harm compatibility (+2.4%), while the combination of usefulness, supportiveness, and credibility models achieved the largest distinction between help- and harm-compatibility on helpful topics (+16.9%). In the domain adaptation setting, the linear combination of different models showed robust performance, with help-harm compatibility above +4.4% for all dimensions and going as high as +6.8%. Conclusions These results suggest that integrating automatic ranking models created for specific information quality dimensions can increase the effectiveness of health-related information retrieval. Thus, our approach could be used to enhance searches made by individuals seeking online health information.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.3955078125
        }
    ],
    "quotes": {
        "cost": 0.235005,
        "quotes": [
            {
                "idx": 0,
                "key": "[10844118 | Wintrode | 2015 | Citations: 0]",
                "snippets": "Cache-based models assume the probability of a word in a document d is influenced both by the global frequency of that word and N-gram context as well as by the N-gram frequencies of d (or preceding cache of K words). Although most words are rare at the corpus level, when they do occur, they occur in bursts. Thus a local estimate, from the cache, may be more reliable than the global estimate.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 313,
                        "end": 708,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Cache-based models assume the probability of a word in a document d is influenced both by the global frequency of that word and N-gram context as well as by the N-gram frequencies of d (or preceding cache of K words). Although most words are rare at the corpus level, when they do occur, they occur in bursts. Thus a local estimate, from the cache, may be more reliable than the global estimate."
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[13892149 | Borg | 2016 | Citations: 3]",
                "snippets": "A subset of probabilistic retrieval estimate Language Models (LM) for each document. Documents are then ranked based on the probability that a document would generate the terms of a query (Ponte et al., 1998).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Information Retrieval -Satisfying an Information Need",
                        "pdf_hash": "",
                        "start": 121,
                        "end": 314,
                        "sentence_offsets": [
                            {
                                "start": 121,
                                "end": 205
                            },
                            {
                                "start": 206,
                                "end": 314
                            }
                        ],
                        "ref_mentions": [
                            "2759772"
                        ],
                        "quote": "A subset of probabilistic retrieval estimate Language Models (LM) for each document. Documents are then ranked based on the probability that a document would generate the terms of a query (Ponte et al., 1998)."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[18819434 | Erkan | 2006 | Citations: 44]",
                "snippets": "In the language modeling framework, each document in the database defines a language model. The relevance of a document to a given query is ranked according to the generation probability of the query based on the underlying language model of the document. To induce a (unigram) language model from a document, we start with the maximum likelihood (ML) estimation of the term probabilities...This estimation is often smoothed based on the following general formula...where ! \" # 7 89 @ A is the ML estimation of over an entire corpus which usually is a member of. \n\n3 is the general smoothing parameter that takes different forms in various smoothing methods. Smoothing has two important roles (Zhai et al., 2004). First, it accounts for terms unseen in the document preventing zero probabilities. This is similar to the smoothing effect in NLP problems such as parsing. Second, smoothing has an \u00a4\u00a5\u00a2like effect that accounts for the generation probabilities of the common terms in the corpus. A common smoothing technique is to use Bayesian smoothing with the Dirichlet prior (Zhai et al., 2004)(Liu et al., 2004)",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Language Models",
                        "pdf_hash": "",
                        "start": 187,
                        "end": 575,
                        "sentence_offsets": [
                            {
                                "start": 187,
                                "end": 278
                            },
                            {
                                "start": 279,
                                "end": 442
                            },
                            {
                                "start": 443,
                                "end": 576
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In the language modeling framework, each document in the database defines a language model. The relevance of a document to a given query is ranked according to the generation probability of the query based on the underlying language model of the document. To induce a (unigram) language model from a document, we start with the maximum likelihood (ML) estimation of the term probabilities"
                    },
                    {
                        "section_title": "Language Models",
                        "pdf_hash": "",
                        "start": 731,
                        "end": 804,
                        "sentence_offsets": [
                            {
                                "start": 731,
                                "end": 804
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "This estimation is often smoothed based on the following general formula"
                    },
                    {
                        "section_title": "Language Models",
                        "pdf_hash": "",
                        "start": 807,
                        "end": 1468,
                        "sentence_offsets": [
                            {
                                "start": 807,
                                "end": 814
                            },
                            {
                                "start": 815,
                                "end": 903
                            },
                            {
                                "start": 906,
                                "end": 999
                            },
                            {
                                "start": 1000,
                                "end": 1060
                            },
                            {
                                "start": 1061,
                                "end": 1143
                            },
                            {
                                "start": 1144,
                                "end": 1216
                            },
                            {
                                "start": 1217,
                                "end": 1338
                            },
                            {
                                "start": 1339,
                                "end": 1468
                            }
                        ],
                        "ref_mentions": [
                            "207670589",
                            "207670589",
                            "15479915"
                        ],
                        "quote": "where ! \" # 7 89 @ A is the ML estimation of over an entire corpus which usually is a member of. \n\n3 is the general smoothing parameter that takes different forms in various smoothing methods. Smoothing has two important roles (Zhai et al., 2004). First, it accounts for terms unseen in the document preventing zero probabilities. This is similar to the smoothing effect in NLP problems such as parsing. Second, smoothing has an \u00a4\u00a5\u00a2like effect that accounts for the generation probabilities of the common terms in the corpus. A common smoothing technique is to use Bayesian smoothing with the Dirichlet prior (Zhai et al., 2004)(Liu et al., 2004)"
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[21012910 | Cummins | 2017 | Citations: 3]",
                "snippets": "The general model outlined here (Eq. 1) is an intuitive statistical generative model of documents. The vector u i can be seen as storing the state of the model at a particular time i. Both the multinomial and multivariate P\u00f3lya urn (SPUD (Cummins et al., 2015)) language model are specific instances of this model and are instantiated by different settings of M. Given that the SPUD language model significantly improves upon the multinomial model in information retrieval, the further extensions hold the promise of improved performance and of greater theoretical understanding.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[9519540 | Cummins et al. | 2015 | Citations: 35]": "The multinomial language model has been one of the most effective models of retrieval for more than a decade. However, the multinomial distribution does not model one important linguistic phenomenon relating to term dependency\u2014that is, the tendency of a term to repeat itself within a document (i.e., word burstiness). In this article, we model document generation as a random process with reinforcement (a multivariate P\u00f3lya process) and develop a Dirichlet compound multinomial language model that captures word burstiness directly. We show that the new reinforced language model can be computed as efficiently as current retrieval models, and with experiments on an extensive set of TREC collections, we show that it significantly outperforms the state-of-the-art language model for a number of standard effectiveness metrics. Experiments also show that the tuning parameter in the proposed model is more robust than that in the multinomial language model. Furthermore, we develop a constraint for the verbosity hypothesis and show that the proposed model adheres to the constraint. Finally, we show that the new language model essentially introduces a measure closely related to idf, which gives theoretical justification for combining the term and document event spaces in tf-idf type schemes."
                },
                "metadata": [
                    {
                        "section_title": "Discussion",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 560,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 98
                            },
                            {
                                "start": 99,
                                "end": 183
                            },
                            {
                                "start": 184,
                                "end": 560
                            }
                        ],
                        "ref_mentions": [
                            "9519540"
                        ],
                        "quote": "The general model outlined here (Eq. 1) is an intuitive statistical generative model of documents. The vector u i can be seen as storing the state of the model at a particular time i. Both the multinomial and multivariate P\u00f3lya urn (SPUD (Cummins et al., 2015)) language model are specific instances of this model and are instantiated by different settings of M. Given that the SPUD language model significantly improves upon the multinomial model in information retrieval, the further extensions hold the promise of improved performance and of greater theoretical understanding."
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[246431219 | Alon et al. | 2022 | Citations: 63]",
                "snippets": "Retrieval-based language models (R-LMs) have recently been shown to improve over standard neural models in a variety of tasks such as unconditional language modeling (Guu et al., 2018;He et al., 2020), machine translation (Zhang et al., 2018;Gu et al., 2018;Khandelwal et al., 2021), question answering (Karpukhin et al., 2020;Ram et al., 2021), and code generation (Hayati et al., 2018;Hashimoto et al., 2018). The key ingredient of R-LMs is their ability to utilize training examples at test time without having to rely on the information encoded in the model's weights only...One prominent example of such a retrieval-based model is kNN-LM (Grave et al., 2017;(Khandelwal et al., 2019), which predicts a token by linearly interpolating the base LM's output with a nonparametric nearest neighbor distribution.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[207870430 | Khandelwal et al. | 2019 | Citations: 842]": "We introduce $k$NN-LMs, which extend a pre-trained neural language model (LM) by linearly interpolating it with a $k$-nearest neighbors ($k$NN) model. The nearest neighbors are computed according to distance in the pre-trained LM embedding space, and can be drawn from any text collection, including the original LM training data. Applying this augmentation to a strong Wikitext-103 LM, with neighbors drawn from the original training set, our $k$NN-LM achieves a new state-of-the-art perplexity of 15.79 - a 2.9 point improvement with no additional training. We also show that this approach has implications for efficiently scaling up to larger training sets and allows for effective domain adaptation, by simply varying the nearest neighbor datastore, again without further training. Qualitatively, the model is particularly helpful in predicting rare patterns, such as factual knowledge. Together, these results strongly suggest that learning similarity between sequences of text is easier than predicting the next word, and that nearest neighbor search is an effective approach for language modeling in the long tail."
                },
                "metadata": [
                    {
                        "quote": "Retrieval-based language models (R-LMs) have recently been shown to improve over standard neural models in a variety of tasks such as unconditional language modeling (Guu et al., 2018;He et al., 2020), machine translation (Zhang et al., 2018;Gu et al., 2018;Khandelwal et al., 2021), question answering (Karpukhin et al., 2020;Ram et al., 2021), and code generation (Hayati et al., 2018;Hashimoto et al., 2018). The key ingredient of R-LMs is their ability to utilize training examples at test time without having to rely on the information encoded in the model's weights only",
                        "pdf_hash": ""
                    },
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1525,
                        "end": 1757,
                        "sentence_offsets": [
                            {
                                "start": 1490,
                                "end": 1756
                            }
                        ],
                        "ref_mentions": [
                            "207870430"
                        ],
                        "quote": "One prominent example of such a retrieval-based model is kNN-LM (Grave et al., 2017;(Khandelwal et al., 2019), which predicts a token by linearly interpolating the base LM's output with a nonparametric nearest neighbor distribution."
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[252781830 | Wang | 2022 | Citations: 12]",
                "snippets": "Common deep retrieval models include: \n\n\u2022 MatchPLM: MatchPLM (Yang et al., 2020) uses a PLM to encode both queries and candidates. A dense layer is used to compute similarities between representations of queries and candidates. \u2022 MatchPyramid: MatchPyramid (Pang et al., 2016) is an interactionfocused model, which utilizes a CNN layer to capture matching patterns on a word-level similarity matrix. \u2022 DRMM: DRMM (Guo et al., 2016) also builds a word-level similarity matrix and uses a histogram mapping function. All histograms are transformed by a feed forward network and selected by a gating network to produce the final representation.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[222327807 | Yang et al. | 2020 | Citations: 89]": "Many natural language processing and information retrieval problems can be formalized as the task of semantic matching. Existing work in this area has been largely focused on matching between short texts (e.g., question answering), or between a short and a long text (e.g., ad-hoc retrieval). Semantic matching between long-form documents, which has many important applications like news recommendation, related article recommendation and document clustering, is relatively less explored and needs more research effort. In recent years, self-attention based models like Transformers and BERT have achieved state-of-the-art performance in the task of text matching. These models, however, are still limited to short text like a few sentences or one paragraph due to the quadratic computational complexity of self-attention with respect to input text length. In this paper, we address the issue by proposing the Siamese Multi-depth Transformer-based Hierarchical (SMITH) Encoder for long-form document matching. Our model contains several innovations to adapt self-attention models for longer text input. We propose a transformer based hierarchical encoder to capture the document structure information. In order to better capture sentence level semantic relations within a document, we pre-train the model with a novel masked sentence block language modeling task in addition to the masked word language modeling task used by BERT. Our experimental results on several benchmark data sets for long-form document matching show that our proposed SMITH model outperforms the previous state-of-the-art models including hierarchical attention, multi-depth attention-based hierarchical recurrent neural network, and BERT. Comparing to BERT based baselines, our model is able to increase maximum input text length from 512 to 2048. We will open source a Wikipedia based benchmark data set, code and a pre-trained model to accelerate future research on long-form document matching.",
                    "[3993933 | Pang et al. | 2016 | Citations: 558]": "\n \n Matching two texts is a fundamental problem in many natural language processing tasks. An effective way is to extract meaningful matching patterns from words, phrases, and sentences to produce the matching score. Inspired by the success of convolutional neural network in image recognition, where neurons can capture many complicated patterns based on the extracted elementary visual patterns such as oriented edges and corners, we propose to model text matching as the problem of image recognition. Firstly, a matching matrix whose entries represent the similarities between words is constructed and viewed as an image. Then a convolutional neural network is utilized to capture rich matching patterns in a layer-by-layer way. We show that by resembling the compositional hierarchies of patterns in image recognition, our model can successfully identify salient signals such as n-gram and n-term matchings. Experimental results demonstrate its superiority against the baselines.\n \n",
                    "[5688521 | Guo et al. | 2016 | Citations: 880]": "In recent years, deep neural networks have led to exciting breakthroughs in speech recognition, computer vision, and natural language processing (NLP) tasks. However, there have been few positive results of deep models on ad-hoc retrieval tasks. This is partially due to the fact that many important characteristics of the ad-hoc retrieval task have not been well addressed in deep models yet. Typically, the ad-hoc retrieval task is formalized as a matching problem between two pieces of text in existing work using deep models, and treated equivalent to many NLP tasks such as paraphrase identification, question answering and automatic conversation. However, we argue that the ad-hoc retrieval task is mainly about relevance matching while most NLP matching tasks concern semantic matching, and there are some fundamental differences between these two matching tasks. Successful relevance matching requires proper handling of the exact matching signals, query term importance, and diverse matching requirements. In this paper, we propose a novel deep relevance matching model (DRMM) for ad-hoc retrieval. Specifically, our model employs a joint deep architecture at the query term level for relevance matching. By using matching histogram mapping, a feed forward matching network, and a term gating network, we can effectively deal with the three relevance matching factors mentioned above. Experimental results on two representative benchmark collections show that our model can significantly outperform some well-known retrieval models as well as state-of-the-art deep matching models."
                },
                "metadata": [
                    {
                        "section_title": "B. Baselines",
                        "pdf_hash": "",
                        "start": 544,
                        "end": 1139,
                        "sentence_offsets": [
                            {
                                "start": 544,
                                "end": 581
                            },
                            {
                                "start": 584,
                                "end": 659
                            },
                            {
                                "start": 660,
                                "end": 756
                            },
                            {
                                "start": 757,
                                "end": 913
                            },
                            {
                                "start": 914,
                                "end": 1012
                            },
                            {
                                "start": 1013,
                                "end": 1139
                            }
                        ],
                        "ref_mentions": [
                            "222327807",
                            "3993933",
                            "5688521"
                        ],
                        "quote": "Common deep retrieval models include: \n\n\u2022 MatchPLM: MatchPLM (Yang et al., 2020) uses a PLM to encode both queries and candidates. A dense layer is used to compute similarities between representations of queries and candidates. \u2022 MatchPyramid: MatchPyramid (Pang et al., 2016) is an interactionfocused model, which utilizes a CNN layer to capture matching patterns on a word-level similarity matrix. \u2022 DRMM: DRMM (Guo et al., 2016) also builds a word-level similarity matrix and uses a histogram mapping function. All histograms are transformed by a feed forward network and selected by a gating network to produce the final representation."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[253384615 | Wang et al. | 2022 | Citations: 1]",
                "snippets": "Most recent development in language modeling is based on transformers (Vaswani et al., 2017). BERT-based Masked language modeling (Devlin et al., 2019)(Liu et al., 2019) exploits bi-directional information of a sentence to predict the word identity of the masked tokens. While BERT is effective in encoding sequences, it is not suitable for sequence generation due to its non-causal nature. Causal language modeling such as GPT2 [11] is uni-directional. Our proposed model attempts to retain the best of the two worlds as autoregressive and simulated bi-directional via augmentation of suffix embeddings during sequence generation.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[13756489 | Vaswani et al. | 2017 | Citations: 132444]": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                    "[198953378 | Liu et al. | 2019 | Citations: 24556]": "Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.",
                    "[52967399 | Devlin et al. | 2019 | Citations: 95215]": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
                },
                "metadata": [
                    {
                        "section_title": "RELATED WORK",
                        "pdf_hash": "",
                        "start": 1180,
                        "end": 1759,
                        "sentence_offsets": [
                            {
                                "start": 1180,
                                "end": 1254
                            },
                            {
                                "start": 1255,
                                "end": 1398
                            },
                            {
                                "start": 1399,
                                "end": 1518
                            },
                            {
                                "start": 1519,
                                "end": 1581
                            },
                            {
                                "start": 1582,
                                "end": 1759
                            }
                        ],
                        "ref_mentions": [
                            "13756489",
                            "52967399",
                            "198953378"
                        ],
                        "quote": "Most recent development in language modeling is based on transformers (Vaswani et al., 2017). BERT-based Masked language modeling (Devlin et al., 2019)(Liu et al., 2019) exploits bi-directional information of a sentence to predict the word identity of the masked tokens. While BERT is effective in encoding sequences, it is not suitable for sequence generation due to its non-causal nature. Causal language modeling such as GPT2 [11] is uni-directional. Our proposed model attempts to retain the best of the two worlds as autoregressive and simulated bi-directional via augmentation of suffix embeddings during sequence generation."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[256389465 | Kim et al. | 2023 | Citations: 7]",
                "snippets": "Neural models for information retrieval (IR) are increasingly used to model the true ranking function in various applications, including web search [Mitra and Craswell, 2018], recommendation [Zhang et al., 2019], and question-answering (QA) (Chen et al., 2017). Notably, the recent success of Transformers (Vaswani et al., 2017)-based pre-trained language models (Devlin et al., 2019), Liu et al., 2019(Raffel et al., 2019) on a wide range of natural language understanding tasks has also prompted their utilization in IR to capture query-document relevance [see, e.g., Dai and Callan, 2019b(MacAvaney et al., 2019), Nogueira and Cho, 2019, Lee et al., 2019(Karpukhin et al., 2020). A typical IR system comprises two stages: (1) A retriever first selects a small subset of potentially relevant candidate documents (out of a large collection) for a given query; and (2) A re-ranker then identifies a precise ranking among the candidates provided by the retriever. Dual-encoder (DE) models are the de-facto architecture for retrievers [Lee et al., 2019(Karpukhin et al., 2020). Such models independently embed queries and documents into a common space, and capture their relevance by simple operations on these embeddings such as the inner product.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[131776819 | MacAvaney et al. | 2019 | Citations: 335]": "Although considerable attention has been given to neural ranking architectures recently, far less attention has been paid to the term representations that are used as input to these models. In this work, we investigate how two pretrained contextualized language models (ELMo and BERT) can be utilized for ad-hoc document ranking. Through experiments on TREC benchmarks, we find that several ex-sting neural ranking architectures can benefit from the additional context provided by contextualized language models. Furthermore, we propose a joint approach that incorporates BERT's classification vector into existing neural models and show that it outperforms state-of-the-art ad-hoc ranking baselines. We call this joint approach CEDR (Contextualized Embeddings for Document Ranking). We also address practical challenges in using these models for ranking, including the maximum input length imposed by BERT and runtime performance impacts of contextualized language models.",
                    "[13756489 | Vaswani et al. | 2017 | Citations: 132444]": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                    "[204838007 | Raffel et al. | 2019 | Citations: 20336]": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.",
                    "[215737187 | Karpukhin et al. | 2020 | Citations: 3794]": "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.",
                    "[3618568 | Chen et al. | 2017 | Citations: 2019]": "This paper proposes to tackle open-domain question answering using Wikipedia as the unique knowledge source: the answer to any factoid question is a text span in a Wikipedia article. This task of machine reading at scale combines the challenges of document retrieval (finding the relevant articles) with that of machine comprehension of text (identifying the answer spans from those articles). Our approach combines a search component based on bigram hashing and TF-IDF matching with a multi-layer recurrent neural network model trained to detect answers in Wikipedia paragraphs. Our experiments on multiple existing QA datasets indicate that (1) both modules are highly competitive with respect to existing counterparts and (2) multitask learning using distant supervision on their combination is an effective complete system on this challenging task.",
                    "[52967399 | Devlin et al. | 2019 | Citations: 95215]": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1251,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 261
                            },
                            {
                                "start": 262,
                                "end": 685
                            },
                            {
                                "start": 686,
                                "end": 965
                            },
                            {
                                "start": 966,
                                "end": 1080
                            },
                            {
                                "start": 1081,
                                "end": 1251
                            }
                        ],
                        "ref_mentions": [
                            "3618568",
                            "13756489",
                            "52967399",
                            "204838007",
                            "131776819",
                            "215737187",
                            "215737187"
                        ],
                        "quote": "Neural models for information retrieval (IR) are increasingly used to model the true ranking function in various applications, including web search [Mitra and Craswell, 2018], recommendation [Zhang et al., 2019], and question-answering (QA) (Chen et al., 2017). Notably, the recent success of Transformers (Vaswani et al., 2017)-based pre-trained language models (Devlin et al., 2019), Liu et al., 2019(Raffel et al., 2019) on a wide range of natural language understanding tasks has also prompted their utilization in IR to capture query-document relevance [see, e.g., Dai and Callan, 2019b(MacAvaney et al., 2019), Nogueira and Cho, 2019, Lee et al., 2019(Karpukhin et al., 2020). A typical IR system comprises two stages: (1) A retriever first selects a small subset of potentially relevant candidate documents (out of a large collection) for a given query; and (2) A re-ranker then identifies a precise ranking among the candidates provided by the retriever. Dual-encoder (DE) models are the de-facto architecture for retrievers [Lee et al., 2019(Karpukhin et al., 2020). Such models independently embed queries and documents into a common space, and capture their relevance by simple operations on these embeddings such as the inner product."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[257232958 | Scao et al. | 2023 | Citations: 2]",
                "snippets": "Natural Language Retrieval Models. For natural language, a first class of retrieval models focuses on retrieving sentences that are similar to some input sentence. BERT (Devlin et al., 2019) has been used as a cross-encoder. Two sentences are given with a separator token, cross-attention applies to all input tokens and the resulting representation is fed into a linear layer to score the match. However, this is computationally inefficient as it is not possible to pre-compute and index such representations. A pre-computable model was proposed by (Reimers et al., 2019) who used twin encoders pre-trained on Natural Language Inference data (Bowman et al., 2015) to set new state-of-the-art performance on a large set of sentence scoring tasks. Further work (Chen et al., 2020)Humeau et al., 2019) combined cross-and bi-encoders to reach a tradeoff between accuracy and efficiency.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[14604520 | Bowman et al. | 2015 | Citations: 4296]": "Understanding entailment and contradiction is fundamental to understanding natural language, and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations. However, machine learning research in this area has been dramatically limited by the lack of large-scale resources. To address this, we introduce the Stanford Natural Language Inference corpus, a new, freely available collection of labeled sentence pairs, written by humans doing a novel grounded task based on image captioning. At 570K pairs, it is two orders of magnitude larger than all other resources of its type. This increase in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time.",
                    "[201646309 | Reimers et al. | 2019 | Citations: 12316]": "BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.",
                    "[222177208 | Chen et al. | 2020 | Citations: 29]": "Pre-trained models like BERT ((Devlin et al., 2018) have dominated NLP / IR applications such as single sentence classification, text pair classification, and question answering. However, deploying these models in real systems is highly non-trivial due to their exorbitant computational costs. A common remedy to this is knowledge distillation (Hinton et al., 2015), leading to faster inference. However \u2013 as we show here \u2013 existing works are not optimized for dealing with pairs (or tuples) of texts. Consequently, they are either not scalable or demonstrate subpar performance. In this work, we propose DiPair \u2014 a novel framework for distilling fast and accurate models on text pair tasks. Coupled with an end-to-end training strategy, DiPair is both highly scalable and offers improved quality-speed tradeoffs. Empirical studies conducted on both academic and real-world e-commerce benchmarks demonstrate the efficacy of the proposed approach with speedups of over 350x and minimal quality drop relative to the cross-attention teacher BERT model.",
                    "[52967399 | Devlin et al. | 2019 | Citations: 95215]": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 173,
                        "end": 1062,
                        "sentence_offsets": [
                            {
                                "start": 173,
                                "end": 207
                            },
                            {
                                "start": 208,
                                "end": 336
                            },
                            {
                                "start": 337,
                                "end": 397
                            },
                            {
                                "start": 398,
                                "end": 569
                            },
                            {
                                "start": 570,
                                "end": 683
                            },
                            {
                                "start": 684,
                                "end": 925
                            },
                            {
                                "start": 926,
                                "end": 1062
                            }
                        ],
                        "ref_mentions": [
                            "52967399",
                            "201646309",
                            "14604520",
                            "222177208"
                        ],
                        "quote": "Natural Language Retrieval Models. For natural language, a first class of retrieval models focuses on retrieving sentences that are similar to some input sentence. BERT (Devlin et al., 2019) has been used as a cross-encoder. Two sentences are given with a separator token, cross-attention applies to all input tokens and the resulting representation is fed into a linear layer to score the match. However, this is computationally inefficient as it is not possible to pre-compute and index such representations. A pre-computable model was proposed by (Reimers et al., 2019) who used twin encoders pre-trained on Natural Language Inference data (Bowman et al., 2015) to set new state-of-the-art performance on a large set of sentence scoring tasks. Further work (Chen et al., 2020)Humeau et al., 2019) combined cross-and bi-encoders to reach a tradeoff between accuracy and efficiency."
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[258676146 | Weller et al. | 2023 | Citations: 20]",
                "snippets": "We evaluate a wide variety of models in order to show a comprehensive evaluation across common neural IR model types. We note that although there are other models we do not use (as well as many different strategies for model training), all the major types of retrieval models are accounted for here. We evaluate on the following IR model categories: \n\nSparse We evaluate sparse IR models that use the bag-of-words representation during retrieval. This includes TF-IDF (the only non-neural IR method, here as a baseline), and two variants of SPLADE v2++ (Formal et al., 2022(Formal et al., , 2021;;Lassance and Clinchant, 2022), the ensemble distillation and selfdistillation methods.\n\nLate Interaction Late interaction models like ColBERT (Khattab and Zaharia, 2020;Santhanam et al., 2022b) embed documents and queries into one vector for each sub-word token. At inference time, these models need to compute a MaxSim operation between query vectors and document vectors to determine similarity. We use both ColBERT v1 and v2 in our experiments.\n\ni-Encoders Another common category of IR models are bi-encoders, which embed both documents and queries into a single vector representation. At inference time the similarity is computed via a simple dot product or cosine similarity. Due to the popularity of this category, we include a broad spectrum: models from Sen-tenceTransformer (Reimers and Gurevych, 2019) trained on MSMarco and/or Natural Questions, DPR (Karpukhin et al., 2020), CoCondenser (Gao and Callan, 2022), and RocketQA (Qu et al., 2021;Ren et al., 2021).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "We evaluate a wide variety of models in order to show a comprehensive evaluation across common neural IR model types. We note that although there are other models we do not use (as well as many different strategies for model training), all the major types of retrieval models are accounted for here. We evaluate on the following IR model categories: \n\nSparse We evaluate sparse IR models that use the bag-of-words representation during retrieval. This includes TF-IDF (the only non-neural IR method, here as a baseline), and two variants of SPLADE v2++ (Formal et al., 2022(Formal et al., , 2021;;Lassance and Clinchant, 2022), the ensemble distillation and selfdistillation methods.\n\nLate Interaction Late interaction models like ColBERT (Khattab and Zaharia, 2020;Santhanam et al., 2022b) embed documents and queries into one vector for each sub-word token. At inference time, these models need to compute a MaxSim operation between query vectors and document vectors to determine similarity. We use both ColBERT v1 and v2 in our experiments.\n\ni-Encoders Another common category of IR models are bi-encoders, which embed both documents and queries into a single vector representation. At inference time the similarity is computed via a simple dot product or cosine similarity. Due to the popularity of this category, we include a broad spectrum: models from Sen-tenceTransformer (Reimers and Gurevych, 2019) trained on MSMarco and/or Natural Questions, DPR (Karpukhin et al., 2020), CoCondenser (Gao and Callan, 2022), and RocketQA (Qu et al., 2021;Ren et al., 2021).",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[258865437 | Huang et al. | 2023 | Citations: 30]",
                "snippets": "Retrieval-based language models (Khandelwal et al., 2019)(Borgeaud et al., 2021)Izacard et al., 2022;(Zhong et al., 2022)(Min et al., 2022)(Shi et al., 2023) have been widely studied in recent years. These models not only rely on encoder forward running but also leverage a non-parametric component to incorporate more knowledge from an external datastore during inference.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[256389797 | Shi et al. | 2023 | Citations: 641]": "We introduce REPLUG, a retrieval-augmented language modeling framework that treats the language model (LM) as a black box and augments it with a tuneable retrieval model. Unlike prior retrieval-augmented LMs that train language models with special cross-attention mechanisms to encode the retrieved text, REPLUG simply prepends retrieved documents to the input for the frozen black-box LM. This simple design can be easily applied to any existing language models. Furthermore, we show that the LM can be used to supervise the retrieval model, which can then find documents that help the LM make better predictions. Our experiments demonstrate that REPLUG with the tuned retriever significantly improves the performance of GPT-3 (175B) on language modeling by 6.3%, as well as the performance of Codex on five-shot MMLU by 5.1%. Code is publicly released at github.com/swj0419/REPLUG.",
                    "[207870430 | Khandelwal et al. | 2019 | Citations: 842]": "We introduce $k$NN-LMs, which extend a pre-trained neural language model (LM) by linearly interpolating it with a $k$-nearest neighbors ($k$NN) model. The nearest neighbors are computed according to distance in the pre-trained LM embedding space, and can be drawn from any text collection, including the original LM training data. Applying this augmentation to a strong Wikitext-103 LM, with neighbors drawn from the original training set, our $k$NN-LM achieves a new state-of-the-art perplexity of 15.79 - a 2.9 point improvement with no additional training. We also show that this approach has implications for efficiently scaling up to larger training sets and allows for effective domain adaptation, by simply varying the nearest neighbor datastore, again without further training. Qualitatively, the model is particularly helpful in predicting rare patterns, such as factual knowledge. Together, these results strongly suggest that learning similarity between sequences of text is easier than predicting the next word, and that nearest neighbor search is an effective approach for language modeling in the long tail.",
                    "[244954723 | Borgeaud et al. | 2021 | Citations: 1100]": "We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a $2$ trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25$\\times$ fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale.",
                    "[249062699 | Zhong et al. | 2022 | Citations: 132]": "Recent work has improved language models (LMs) remarkably by equipping them with a non-parametric memory component. However, most existing approaches only introduce mem-ories at testing time or represent them using a separately trained encoder, resulting in suboptimal training of the language model. In this work, we present TRIME, a novel yet simple training approach designed for training LMs with memory augmentation. Our approach uses a training objective that directly takes in-batch examples as accessible memory. We also present new methods for memory construction and data batching, which are used for adapting to different sets of memories\u2014local, long-term, and external memory\u2014at testing time. We evaluate TRIME on multiple language modeling and machine translation benchmarks and show that it is able to achieve significant improvements across all the settings. Concretely, TRIME reduces the perplexity from 18.70 to 15.37 on WIKITEXT-103, by effectively leveraging a large memory set from the training corpus. Compared to standard LM training, TRIME adds negligible computational overhead and is compatible with different neural architectures, making it a versatile solution for training memory-augmented LMs.",
                    "[254220735 | Min et al. | 2022 | Citations: 50]": "Existing language models (LMs) predict tokens with a softmax over a finite vocabulary, which can make it difficult to predict rare tokens or phrases. We introduce NPM, the first nonparametric masked language model that replaces this softmax with a nonparametric distribution over every phrase in a reference corpus. NPM fills in the [MASK] solely from retrieving a token from a text corpus. We show that NPM can be efficiently trained with a contrastive objective and an in-batch approximation to full corpus retrieval. Zero-shot evaluation on 16 tasks including classification, fact probing and question answering demonstrates that NPM outperforms significantly larger parametric models, either with or without a retrieve-and-generate approach. It is particularly better at dealing with rare patterns (word senses or facts) and predicting rare or nearly unseen words (e.g., non-Latin script). We release the model and code at github.com/facebookresearch/NPM."
                },
                "metadata": [
                    {
                        "section_title": "Retrieval-based Language Models",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 369,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 195
                            },
                            {
                                "start": 196,
                                "end": 369
                            }
                        ],
                        "ref_mentions": [
                            "207870430",
                            "244954723",
                            "249062699",
                            "254220735",
                            "256389797"
                        ],
                        "quote": "Retrieval-based language models (Khandelwal et al., 2019)(Borgeaud et al., 2021)Izacard et al., 2022;(Zhong et al., 2022)(Min et al., 2022)(Shi et al., 2023) have been widely studied in recent years. These models not only rely on encoder forward running but also leverage a non-parametric component to incorporate more knowledge from an external datastore during inference."
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[259203489 | Jong et al. | 2023 | Citations: 5]",
                "snippets": "retrieval procedures face a trade-off between expensive ranking with full interaction (Chen et al., 2020) and the more common dual encoder approaches such as DPR (Karpukhin et al., 2020) and GTR (Ni et al., 2021) that scores based on inner product similarity with a corpus of pre-computed passage representations.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[215737187 | Karpukhin et al. | 2020 | Citations: 3794]": "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.",
                    "[228093856 | Chen et al. | 2020 | Citations: 12]": "The task of Cross-lingual Passage Re-ranking (XPR) aims to rank a list of candidate passages in multiple languages given a query, which is generally challenged by two main issues: (1) the query and passages to be ranked are often in different languages, which requires strong cross-lingual alignment, and (2) the lack of annotated data for model training and evaluation. In this article, we propose a two-stage approach to address these issues. At the first stage, we introduce the task of Cross-lingual Paraphrase Identification (XPI) as an extra pre-training to augment the alignment by leveraging a large unsupervised parallel corpus. This task aims to identify whether two sentences, which may be from different languages, have the same meaning. At the second stage, we introduce and compare three effective strategies for cross-lingual training. To verify the effectiveness of our method, we construct an XPR dataset by assembling and modifying two monolingual datasets. Experimental results show that our augmented pre-training contributes significantly to the XPR task. Besides, we directly transfer the trained model to test on out-domain data which are constructed by modifying three multi-lingual Question Answering (QA) datasets. The results demonstrate the cross-domain robustness of the proposed approach."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 123,
                        "end": 436,
                        "sentence_offsets": [
                            {
                                "start": 81,
                                "end": 436
                            }
                        ],
                        "ref_mentions": [
                            "228093856",
                            "215737187"
                        ],
                        "quote": "retrieval procedures face a trade-off between expensive ranking with full interaction (Chen et al., 2020) and the more common dual encoder approaches such as DPR (Karpukhin et al., 2020) and GTR (Ni et al., 2021) that scores based on inner product similarity with a corpus of pre-computed passage representations."
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[260900354 | Huang et al. | 2023 | Citations: 35]",
                "snippets": "Retrieval-augmented language models are a class of language models designed to enhance their performance by incorporating external knowledge. These models typically employ an information retrieval mechanism to access relevant information from a large corpus, which is then integrated into the model's prediction process. Retrieval-augmented LMs can be based on both encoder-decoder (Izacard et al., 2022)(Lewis et al., 2020) and decoderonly (Khandelwal et al., 2019)Borgeaud et al., 2022;(Shi et al., 2022) architectures.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[207870430 | Khandelwal et al. | 2019 | Citations: 842]": "We introduce $k$NN-LMs, which extend a pre-trained neural language model (LM) by linearly interpolating it with a $k$-nearest neighbors ($k$NN) model. The nearest neighbors are computed according to distance in the pre-trained LM embedding space, and can be drawn from any text collection, including the original LM training data. Applying this augmentation to a strong Wikitext-103 LM, with neighbors drawn from the original training set, our $k$NN-LM achieves a new state-of-the-art perplexity of 15.79 - a 2.9 point improvement with no additional training. We also show that this approach has implications for efficiently scaling up to larger training sets and allows for effective domain adaptation, by simply varying the nearest neighbor datastore, again without further training. Qualitatively, the model is particularly helpful in predicting rare patterns, such as factual knowledge. Together, these results strongly suggest that learning similarity between sequences of text is easier than predicting the next word, and that nearest neighbor search is an effective approach for language modeling in the long tail.",
                    "[218869575 | Lewis et al. | 2020 | Citations: 6476]": "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",
                    "[249152130 | Shi et al. | 2022 | Citations: 32]": "Retrieval-augmented language models (LMs) use non-parametric memory to substantially outperform their non-retrieval counterparts on perplexity-based evaluations, but it is an open question whether they achieve similar gains in few- and zero-shot end-task accuracy. We extensively study one such model, the k-nearest neighbor LM (kNN-LM), showing that the gains marginally transfer. The main challenge is to achieve coverage of the verbalizer tokens that define the different end-task class labels. To address this challenge, we also introduce kNN-Prompt, a simple and effective kNN-LM with automatically expanded fuzzy verbalizers (e.g. to expand \u201cterrible\u201d to also include \u201csilly\u201d and other task-specific synonyms for sentiment classification). Across nine diverse end-tasks, using kNN-Prompt with GPT-2 large yields significant performance boosts over strong zeroshot baselines (13.4% absolute improvement over the base LM on average). We also show that other advantages of non-parametric augmentation hold for end tasks; kNN-Prompt is effective for domain adaptation with no further training, and gains increase with the size of the retrieval model.",
                    "[251371732 | Izacard et al. | 2022 | Citations: 783]": "Large language models have shown impressive few-shot results on a wide range of tasks. However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter counts to store knowledge seem to be needed. Retrieval augmented models are known to excel at knowledge intensive tasks without the need for as many parameters, but it is unclear whether they work in few-shot settings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including MMLU, KILT and NaturalQuestions, and study the impact of the content of the document index, showing that it can easily be updated. Notably, Atlas reaches over 42% accuracy on Natural Questions using only 64 examples, outperforming a 540B parameters model by 3% despite having 50x fewer parameters."
                },
                "metadata": [
                    {
                        "section_title": "Background and Related Work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 519,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 141
                            },
                            {
                                "start": 142,
                                "end": 320
                            },
                            {
                                "start": 321,
                                "end": 519
                            }
                        ],
                        "ref_mentions": [
                            "251371732",
                            "218869575",
                            "207870430",
                            "249152130"
                        ],
                        "quote": "Retrieval-augmented language models are a class of language models designed to enhance their performance by incorporating external knowledge. These models typically employ an information retrieval mechanism to access relevant information from a large corpus, which is then integrated into the model's prediction process. Retrieval-augmented LMs can be based on both encoder-decoder (Izacard et al., 2022)(Lewis et al., 2020) and decoderonly (Khandelwal et al., 2019)Borgeaud et al., 2022;(Shi et al., 2022) architectures."
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[263605962 | Lin et al. | 2023 | Citations: 153]",
                "snippets": "Retrieval-Augmented Language Models RALMs fuse language models (LMs) with a retrieval module that explicitly augments the LM with external knowledge stores (Guu et al., 2020;Lewis et al., 2020). One mainstream type of RALM follows the \"retrieveand-read\" paradigm, where the retrieval module supplies external knowledge as additional context which the LM (reader) leverages to produce the final output (Izacard et al., 2022b;Borgeaud et al., 2022;Shi et al., 2023b;Ram et al., 2023).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Retrieval-Augmented Language Models RALMs fuse language models (LMs) with a retrieval module that explicitly augments the LM with external knowledge stores (Guu et al., 2020;Lewis et al., 2020). One mainstream type of RALM follows the \"retrieveand-read\" paradigm, where the retrieval module supplies external knowledge as additional context which the LM (reader) leverages to produce the final output (Izacard et al., 2022b;Borgeaud et al., 2022;Shi et al., 2023b;Ram et al., 2023).",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[263835099 | Zhang et al. | 2023 | Citations: 63]",
                "snippets": "In practice, there are two common types of retrievers. One is to leverage the general purpose retrievers, such as sparse models like BM25 [69], and dense models, like DPR [37], contriever [30], E5 [81], BGE [89], OpenAI text embedding [56]. The other option is develop task-specific retriever, e.g., AAR for knowledge enhancement [96], LLM-R [85] for in-context learning. The general purpose methods are praised for their generality and simplicity for usage, but may suffer from an inferior retrieval quality.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "RELATED WORKS",
                        "pdf_hash": "",
                        "start": 1710,
                        "end": 2219,
                        "sentence_offsets": [
                            {
                                "start": 1710,
                                "end": 1764
                            },
                            {
                                "start": 1765,
                                "end": 1950
                            },
                            {
                                "start": 1951,
                                "end": 2081
                            },
                            {
                                "start": 2082,
                                "end": 2219
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In practice, there are two common types of retrievers. One is to leverage the general purpose retrievers, such as sparse models like BM25 [69], and dense models, like DPR [37], contriever [30], E5 [81], BGE [89], OpenAI text embedding [56]. The other option is develop task-specific retriever, e.g., AAR for knowledge enhancement [96], LLM-R [85] for in-context learning. The general purpose methods are praised for their generality and simplicity for usage, but may suffer from an inferior retrieval quality."
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[263866951 | Asai et al. | 2023 | Citations: 88]",
                "snippets": "Retrieval-based language models (LMs) have shown impressive performance on diverse NLP tasks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 93,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Retrieval-based language models (LMs) have shown impressive performance on diverse NLP tasks."
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[265294654 | Long et al. | 2023 | Citations: 14]",
                "snippets": "Retrieval-augmented LM approaches (Guu et al., 2020)(Lewis et al., 2020)Asai et al., 2023) apply a parametrized dense retriever to train with the task model. In this paper, we fix the retriever part and use the off-the-shelf scoring language models. For the SA task, we use SimCSE (Gao et al., 2021) which produces semantically meaningful sentence embeddings after being trained with contrastive learning (Chen et al., 2020)(He et al., 2019). Here cosine similarity is used to retrieve top-ranked (most similar) examples from the target domain. For NER, we use BERTScore (Zhang et al., 2020;(Hu et al., 2021), because it gives a metric for each sentence based on the similarity of token representation, which is more crucial for the task of NER.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[207930212 | He et al. | 2019 | Citations: 12131]": "We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.",
                    "[211096730 | Chen et al. | 2020 | Citations: 18878]": "This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.",
                    "[211204736 | Guu et al. | 2020 | Citations: 2119]": "Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. \nTo capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. \nWe demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity.",
                    "[218869575 | Lewis et al. | 2020 | Citations: 6476]": "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",
                    "[233296292 | Gao et al. | 2021 | Citations: 3413]": "This paper presents SimCSE, a simple contrastive learning framework that greatly advances the state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We find that dropout acts as minimal data augmentation and removing it leads to a representation collapse. Then, we propose a supervised approach, which incorporates annotated pairs from natural language inference datasets into our contrastive learning framework, by using \u201centailment\u201d pairs as positives and \u201ccontradiction\u201d pairs as hard negatives. We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERT base achieve an average of 76.3% and 81.6% Spearman\u2019s correlation respectively, a 4.2% and 2.2% improvement compared to previous best results. We also show\u2014both theoretically and empirically\u2014that contrastive learning objective regularizes pre-trained embeddings\u2019 anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are available.",
                    "[235458009 | Hu et al. | 2021 | Citations: 10511]": "An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA."
                },
                "metadata": [
                    {
                        "section_title": "Context Construction with Retrieval",
                        "pdf_hash": "",
                        "start": 197,
                        "end": 941,
                        "sentence_offsets": [
                            {
                                "start": 197,
                                "end": 353
                            },
                            {
                                "start": 354,
                                "end": 445
                            },
                            {
                                "start": 446,
                                "end": 637
                            },
                            {
                                "start": 638,
                                "end": 739
                            },
                            {
                                "start": 740,
                                "end": 941
                            }
                        ],
                        "ref_mentions": [
                            "211204736",
                            "218869575",
                            "233296292",
                            "211096730",
                            "207930212",
                            "235458009"
                        ],
                        "quote": "Retrieval-augmented LM approaches (Guu et al., 2020)(Lewis et al., 2020)Asai et al., 2023) apply a parametrized dense retriever to train with the task model. In this paper, we fix the retriever part and use the off-the-shelf scoring language models. For the SA task, we use SimCSE (Gao et al., 2021) which produces semantically meaningful sentence embeddings after being trained with contrastive learning (Chen et al., 2020)(He et al., 2019). Here cosine similarity is used to retrieve top-ranked (most similar) examples from the target domain. For NER, we use BERTScore (Zhang et al., 2020;(Hu et al., 2021), because it gives a metric for each sentence based on the similarity of token representation, which is more crucial for the task of NER."
                    }
                ]
            },
            {
                "idx": 17,
                "key": "[267377589 | Liu et al. | 2024 | Citations: 7]",
                "snippets": "Neural Dense Retrieval Models use dense vectors to represent queries and documents. Two widely adopted architectures of dense retrieval models are cross-encoders and bi-encoders.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[235792467 | Formal et al. | 2021 | Citations: 325]": "In neural Information Retrieval, ongoing research is directed towards improving the first retriever in ranking pipelines. Learning dense embeddings to conduct retrieval using efficient approximate nearest neighbors methods has proven to work well. Meanwhile, there has been a growing interest in learning sparse representations for documents and queries, that could inherit from the desirable properties of bag-of-words models such as the exact matching of terms and the efficiency of inverted indexes. In this work, we present a new first-stage ranker based on explicit sparsity regularization and a log-saturation effect on term weights, leading to highly sparse representations and competitive results with respect to state-of-the-art dense and sparse methods. Our approach is simple, trained end-to-end in a single stage. We also explore the trade-off between effectiveness and efficiency, by controlling the contribution of the sparsity regularization."
                },
                "metadata": [
                    {
                        "section_title": "Neural Retrieval Models",
                        "pdf_hash": "",
                        "start": 1242,
                        "end": 1420,
                        "sentence_offsets": [
                            {
                                "start": 1167,
                                "end": 1333
                            },
                            {
                                "start": 1333,
                                "end": 1494
                            }
                        ],
                        "ref_mentions": [
                            "235792467"
                        ],
                        "quote": "Neural Dense Retrieval Models use dense vectors to represent queries and documents. Two widely adopted architectures of dense retrieval models are cross-encoders and bi-encoders."
                    }
                ]
            },
            {
                "idx": 18,
                "key": "[267412330 | Kang et al. | 2024 | Citations: 22]",
                "snippets": "To quantify the quality of retrieval models, we introduce the concept of V rag -retrieval model, where V rag measures the variance of the contrastive loss of the retrieval model. A small V rag implies a well-trained low-variance retrieval model and can be theoretically linked to the retrieval quality, which is measured by the number of retrieved positive examples with respect to the query text. \n\nDefinition 1 (V rag -retrieval model). Consider a retrieval model with similarity measurement s \u03b8r (\u2022, \u2022) parameterized with \u03b8 r and trained with contrastive loss L cont . Let x + , x \u2212 be positive and negative samples to sample x. Consider common contrastive loss L cont = \u2212 log (\u03c3 sig (exp{s \u03b8 (x, x \u2212 ) \u2212 exp{s \u03b8 (x, x + ))), where \u03c3 sig (\u2022) is the sigmoid function.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Retrieval quality analysis",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 769,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 178
                            },
                            {
                                "start": 179,
                                "end": 397
                            },
                            {
                                "start": 400,
                                "end": 438
                            },
                            {
                                "start": 439,
                                "end": 571
                            },
                            {
                                "start": 572,
                                "end": 631
                            },
                            {
                                "start": 632,
                                "end": 769
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "To quantify the quality of retrieval models, we introduce the concept of V rag -retrieval model, where V rag measures the variance of the contrastive loss of the retrieval model. A small V rag implies a well-trained low-variance retrieval model and can be theoretically linked to the retrieval quality, which is measured by the number of retrieved positive examples with respect to the query text. \n\nDefinition 1 (V rag -retrieval model). Consider a retrieval model with similarity measurement s \u03b8r (\u2022, \u2022) parameterized with \u03b8 r and trained with contrastive loss L cont . Let x + , x \u2212 be positive and negative samples to sample x. Consider common contrastive loss L cont = \u2212 log (\u03c3 sig (exp{s \u03b8 (x, x \u2212 ) \u2212 exp{s \u03b8 (x, x + ))), where \u03c3 sig (\u2022) is the sigmoid function."
                    }
                ]
            },
            {
                "idx": 19,
                "key": "[268856885 | Gomez et al. | 2024 | Citations: 4]",
                "snippets": "Dual Encoder (DE) based retrieval systems project queries and documents into the same embedding space and have demonstrated their success in retrieval and bi-text mining.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 144,
                        "end": 314,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Dual Encoder (DE) based retrieval systems project queries and documents into the same embedding space and have demonstrated their success in retrieval and bi-text mining."
                    }
                ]
            },
            {
                "idx": 20,
                "key": "[269302879 | Zeng et al. | 2024 | Citations: 16]",
                "snippets": "With the emergence of large language models (LLMs) [14,(Devlin et al., 2019)39,43,(Raffel et al., 2019) and large-scale information retrieval datasets [4,(Kwiatkowski et al., 2019), neural-based IR models have demonstrated superior results over the traditional lexical-matching models, such as BM25 (Robertson et al., 2009).In general, these IR models can fall into three categories: (1) cross-encoder models [42,48,(Zhuang et al., 2022), (2) dense retrieval models [22,(Hofst\u00e4tter et al., 2021)(Karpukhin et al., 2020)(Khattab et al., 2020)38,(Zeng et al., 2022), and (3) sparse retrieval models [12](Choi et al., 2022)18,(Formal et al., 2021).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[204838007 | Raffel et al. | 2019 | Citations: 20336]": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.",
                    "[207178704 | Robertson et al. | 2009 | Citations: 3760]": "The Probabilistic Relevance Framework (PRF) is a formal framework for document retrieval, grounded in work done in the 1970\u20141980s, which led to the development of one of the most successful text-retrieval algorithms, BM25. In recent years, research in the PRF has yielded new retrieval models capable of taking into account document meta-data (especially structure and link-graph information). Again, this has led to one of the most successful Web-search and corporate-search algorithms, BM25F. This work presents the PRF from a conceptual point of view, describing the probabilistic modelling assumptions behind the framework and the different ranking algorithms that result from its application: the binary independence model, relevance feedback models, BM25 and BM25F. It also discusses the relation between the PRF and other statistical models for IR, and covers some related topics, such as the use of non-textual features, and parameter optimisation for models with free parameters.",
                    "[215737187 | Karpukhin et al. | 2020 | Citations: 3794]": "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.",
                    "[216553223 | Khattab et al. | 2020 | Citations: 1377]": "Recent progress in Natural Language Understanding (NLU) is driving fast-paced advances in Information Retrieval (IR), largely owed to fine-tuning deep language models (LMs) for document ranking. While remarkably effective, the ranking models based on these LMs increase computational cost by orders of magnitude over prior approaches, particularly as they must feed each query-document pair through a massive neural network to compute a single relevance score. To tackle this, we present ColBERT, a novel ranking model that adapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT introduces a late interaction architecture that independently encodes the query and the document using BERT and then employs a cheap yet powerful interaction step that models their fine-grained similarity. By delaying and yet retaining this fine-granular interaction, ColBERT can leverage the expressiveness of deep LMs while simultaneously gaining the ability to pre-compute document representations offline, considerably speeding up query processing. Crucially, ColBERT's pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from millions of documents. We extensively evaluate ColBERT using two recent passage search datasets. Results show that ColBERT's effectiveness is competitive with existing BERT-based models (and outperforms every non-BERT baseline), while executing two orders-of-magnitude faster and requiring up to four orders-of-magnitude fewer FLOPs per query.",
                    "[233231706 | Hofstatter et al. | 2021 | Citations: 402]": "A vital step towards the widespread adoption of neural retrieval models is their resource efficiency throughout the training, indexing and query workflows. The neural IR community made great advancements in training effective dual-encoder dense retrieval (DR) models recently. A dense text retrieval model uses a single vector representation per query and passage to score a match, which enables low-latency first-stage retrieval with a nearest neighbor search. Increasingly common, training approaches require enormous compute power, as they either conduct negative passage sampling out of a continuously updating refreshing index or require very large batch sizes. Instead of relying on more compute capability, we introduce an efficient topic-aware query and balanced margin sampling technique, called TAS-Balanced. We cluster queries once before training and sample queries out of a cluster per batch. We train our lightweight 6-layer DR model with a novel dual-teacher supervision that combines pairwise and in-batch negative teachers. Our method is trainable on a single consumer-grade GPU in under 48 hours. We show that our TAS-Balanced training method achieves state-of-the-art low-latency (64ms per query) results on two TREC Deep Learning Track query sets. Evaluated on NDCG@10, we outperform BM25 by 44%, a plainly trained DR by 19%, docT5query by 11%, and the previous best DR model by 5%. Additionally, TAS-Balanced produces the first dense retriever that outperforms every other method on recall at any cutoff on TREC-DL and allows more resource intensive re-ranking models to operate on fewer passages to improve results further.",
                    "[235792467 | Formal et al. | 2021 | Citations: 325]": "In neural Information Retrieval, ongoing research is directed towards improving the first retriever in ranking pipelines. Learning dense embeddings to conduct retrieval using efficient approximate nearest neighbors methods has proven to work well. Meanwhile, there has been a growing interest in learning sparse representations for documents and queries, that could inherit from the desirable properties of bag-of-words models such as the exact matching of terms and the efficiency of inverted indexes. In this work, we present a new first-stage ranker based on explicit sparsity regularization and a log-saturation effect on term weights, leading to highly sparse representations and competitive results with respect to state-of-the-art dense and sparse methods. Our approach is simple, trained end-to-end in a single stage. We also explore the trade-off between effectiveness and efficiency, by controlling the contribution of the sparsity regularization.",
                    "[248426770 | Zeng et al. | 2022 | Citations: 51]": "Recent work has shown that more effective dense retrieval models can be obtained by distilling ranking knowledge from an existing base re-ranking model. In this paper, we propose a generic curriculum learning based optimization framework called CL-DRD that controls the difficulty level of training data produced by the re-ranking (teacher) model. CL-DRD iteratively optimizes the dense retrieval (student) model by increasing the difficulty of the knowledge distillation data made available to it. In more detail, we initially provide the student model coarse-grained preference pairs between documents in the teacher's ranking, and progressively move towards finer-grained pairwise document ordering requirements. In our experiments, we apply a simple implementation of the CL-DRD framework to enhance two state-of-the-art dense retrieval models. Experiments on three public passage retrieval datasets demonstrate the effectiveness of our proposed framework.",
                    "[252212320 | Choi et al. | 2022 | Citations: 17]": "Sparse document representations have been widely used to retrieve relevant documents via exact lexical matching. Owing to the pre-computed inverted index, it supports fast ad-hoc search but incurs the vocabulary mismatch problem. Although recent neural ranking models using pre-trained language models can address this problem, they usually require expensive query inference costs, implying the trade-off between effectiveness and efficiency. Tackling the trade-off, we propose a novel uni-encoder ranking model, Sparse retriever using a Dual document Encoder (SpaDE), learning document representation via the dual encoder. Each encoder plays a central role in (i) adjusting the importance of terms to improve lexical matching and (ii) expanding additional terms to support semantic matching. Furthermore, our co-training strategy trains the dual encoder effectively and avoids unnecessary intervention in training each other. Experimental results on several benchmarks show that SpaDE outperforms existing uni-encoder ranking models.",
                    "[252993059 | Zhuang et al. | 2022 | Citations: 140]": "Pretrained language models such as BERT have been shown to be exceptionally effective for text ranking. However, there are limited studies on how to leverage more powerful sequence-to-sequence models such as T5. Existing attempts usually formulate text ranking as a classification problem and rely on postprocessing to obtain a ranked list. In this paper, we propose RankT5 and study two T5-based ranking model structures, an encoder-decoder and an encoder-only one, so that they not only can directly output ranking scores for each query-document pair, but also can be fine-tuned with pairwise or listwise ranking losses to optimize ranking performance. Our experiments show that the proposed models with ranking losses can achieve substantial ranking performance gains on different public text ranking data sets. Moreover, ranking models fine-tuned with listwise ranking losses have better zero-shot ranking performance on out-of-domain data than models fine-tuned with classification losses.",
                    "[52967399 | Devlin et al. | 2019 | Citations: 95215]": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                    "[86611921 | Kwiatkowski et al. | 2019 | Citations: 3361]": "We present the Natural Questions corpus, a question answering data set. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature."
                },
                "metadata": [
                    {
                        "section_title": "RELATED WORK",
                        "pdf_hash": "",
                        "start": 26,
                        "end": 462,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 271
                            },
                            {
                                "start": 271,
                                "end": 462
                            }
                        ],
                        "ref_mentions": [
                            "52967399",
                            "204838007",
                            "86611921",
                            "207178704",
                            "252993059",
                            "233231706",
                            "215737187",
                            "216553223",
                            "248426770",
                            "252212320",
                            "235792467"
                        ],
                        "quote": "With the emergence of large language models (LLMs) [14,(Devlin et al., 2019)39,43,(Raffel et al., 2019) and large-scale information retrieval datasets [4,(Kwiatkowski et al., 2019), neural-based IR models have demonstrated superior results over the traditional lexical-matching models, such as BM25 (Robertson et al., 2009).In general, these IR models can fall into three categories: (1) cross-encoder models [42,48,(Zhuang et al., 2022), (2) dense retrieval models [22,(Hofst\u00e4tter et al., 2021)(Karpukhin et al., 2020)(Khattab et al., 2020)38,(Zeng et al., 2022), and (3) sparse retrieval models [12](Choi et al., 2022)18,(Formal et al., 2021)."
                    }
                ]
            },
            {
                "idx": 21,
                "key": "[269605025 | Zhang et al. | 2024 | Citations: 0]",
                "snippets": "In the literature, there are two major research aspects in this field: \n\n(1) Datastore Indexing (Khandelwal et al., 2019)10,(Wei et al., 2022)[48] and (2) Document Retrieval [35,27]. For Datastore Indexing, these approaches utilize pre-trained models to generate static embeddings for documents, which are viewed as mounted external memory, and they leverage various semantic similarities to enhance indexing. For Document Retrieval, the system initially retrieves a collection of relevant documents based on the semantic relevance between the user query and the documents. Then, the LLMs concatenate these highly related documents in an unordered manner to the prompt input [4], which makes LLMs better at answering factual questions.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[207870430 | Khandelwal et al. | 2019 | Citations: 842]": "We introduce $k$NN-LMs, which extend a pre-trained neural language model (LM) by linearly interpolating it with a $k$-nearest neighbors ($k$NN) model. The nearest neighbors are computed according to distance in the pre-trained LM embedding space, and can be drawn from any text collection, including the original LM training data. Applying this augmentation to a strong Wikitext-103 LM, with neighbors drawn from the original training set, our $k$NN-LM achieves a new state-of-the-art perplexity of 15.79 - a 2.9 point improvement with no additional training. We also show that this approach has implications for efficiently scaling up to larger training sets and allows for effective domain adaptation, by simply varying the nearest neighbor datastore, again without further training. Qualitatively, the model is particularly helpful in predicting rare patterns, such as factual knowledge. Together, these results strongly suggest that learning similarity between sequences of text is easier than predicting the next word, and that nearest neighbor search is an effective approach for language modeling in the long tail.",
                    "[249674500 | Wei et al. | 2022 | Citations: 2516]": "Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 514,
                        "end": 1212,
                        "sentence_offsets": [
                            {
                                "start": 514,
                                "end": 584
                            },
                            {
                                "start": 587,
                                "end": 659
                            },
                            {
                                "start": 660,
                                "end": 886
                            },
                            {
                                "start": 887,
                                "end": 1050
                            },
                            {
                                "start": 1051,
                                "end": 1212
                            }
                        ],
                        "ref_mentions": [
                            "207870430",
                            "249674500"
                        ],
                        "quote": "In the literature, there are two major research aspects in this field: \n\n(1) Datastore Indexing (Khandelwal et al., 2019)10,(Wei et al., 2022)[48] and (2) Document Retrieval [35,27]. For Datastore Indexing, these approaches utilize pre-trained models to generate static embeddings for documents, which are viewed as mounted external memory, and they leverage various semantic similarities to enhance indexing. For Document Retrieval, the system initially retrieves a collection of relevant documents based on the semantic relevance between the user query and the documents. Then, the LLMs concatenate these highly related documents in an unordered manner to the prompt input [4], which makes LLMs better at answering factual questions."
                    }
                ]
            },
            {
                "idx": 22,
                "key": "[269921622 | He et al. | 2024 | Citations: 7]",
                "snippets": "The transformer model has shown proficiency in retrieval tasks, encoding item information within its parameters, which is a method termed Differentiable Search Index (DSI) (Tay et al., 2022).DSI involves two key training tasks for pre-trained language models: Learn to Index (L2I) and Learn to Retrieve (L2R), which can be used to train a model jointly or in a sequential order...Compared to common two-tower models, DSI models require only a single model for item recommendations, by indexing item information into its parameters (Tay et al., 2022).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[246863488 | Tay et al. | 2022 | Citations: 286]": "In this paper, we demonstrate that information retrieval can be accomplished with a single Transformer, in which all information about the corpus is encoded in the parameters of the model. To this end, we introduce the Differentiable Search Index (DSI), a new paradigm that learns a text-to-text model that maps string queries directly to relevant docids; in other words, a DSI model answers queries directly using only its parameters, dramatically simplifying the whole retrieval process. We study variations in how documents and their identifiers are represented, variations in training procedures, and the interplay between models and corpus sizes. Experiments demonstrate that given appropriate design choices, DSI significantly outperforms strong baselines such as dual encoder models. Moreover, DSI demonstrates strong generalization capabilities, outperforming a BM25 baseline in a zero-shot setup."
                },
                "metadata": [
                    {
                        "section_title": "Differentiable Search Index (DSI)",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 377,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 191
                            },
                            {
                                "start": 191,
                                "end": 378
                            }
                        ],
                        "ref_mentions": [
                            "246863488"
                        ],
                        "quote": "The transformer model has shown proficiency in retrieval tasks, encoding item information within its parameters, which is a method termed Differentiable Search Index (DSI) (Tay et al., 2022).DSI involves two key training tasks for pre-trained language models: Learn to Index (L2I) and Learn to Retrieve (L2R), which can be used to train a model jointly or in a sequential order"
                    },
                    {
                        "section_title": "Differentiable Search Index (DSI)",
                        "pdf_hash": "",
                        "start": 1148,
                        "end": 1318,
                        "sentence_offsets": [
                            {
                                "start": 1148,
                                "end": 1318
                            }
                        ],
                        "ref_mentions": [
                            "246863488"
                        ],
                        "quote": "Compared to common two-tower models, DSI models require only a single model for item recommendations, by indexing item information into its parameters (Tay et al., 2022)."
                    }
                ]
            },
            {
                "idx": 23,
                "key": "[271891894 | Ren et al. | 2024 | Citations: 5]",
                "snippets": "General Language Models: BERT [4], RoBERTa [21], and BART [14]; (ii) Language Models for Dense Retrieval: SimCSE [7], GTR [23], and BGE [39]; (iii) Pre-trained Language Models for Recommendation: BLaIR [12].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "4.2.1",
                        "pdf_hash": "",
                        "start": 131,
                        "end": 338,
                        "sentence_offsets": [
                            {
                                "start": 31,
                                "end": 338
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "General Language Models: BERT [4], RoBERTa [21], and BART [14]; (ii) Language Models for Dense Retrieval: SimCSE [7], GTR [23], and BGE [39]; (iii) Pre-trained Language Models for Recommendation: BLaIR [12]."
                    }
                ]
            },
            {
                "idx": 24,
                "key": "[272310589 | Liu et al. | 2024 | Citations: 3]",
                "snippets": "Much effort has been made to enhance Retrieval-Augmented Language Modeling (Lewis et al., 2020)Izacard and Grave, 2020;(Ram et al., 2023)Yu et al., 2022;Asai et al., 2023). While some approaches use external retrievers, non-parametric information fusion often falls short compared to parametric methods within the model. We concentrate on integrating retrieval concepts directly into the model. REALM (Guu et al., 2020) suggests that relying solely on internal model knowledge is inefficient and advocates for the model to learn to retrieve and comprehend. kNN-LM (Khandelwal et al., 2019) enhances language modeling by blending the LLM's next-word predictions with those from a retrieval-based mechanism. MemTrm (Wu et al., 2022) introduces a memory bank but risks shifting memory distributions due to parameter adjustments. LongMEM (Wang et al., 2023) mitigates this by training a sub-network, though this adds significant overhead.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[218869575 | Lewis et al. | 2020 | Citations: 6476]": "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",
                    "[256459451 | Ram et al. | 2023 | Citations: 605]": "Abstract Retrieval-Augmented Language Modeling (RALM) methods, which condition a language model (LM) on relevant documents from a grounding corpus during generation, were shown to significantly improve language modeling performance. In addition, they can mitigate the problem of factually inaccurate text generation and provide natural source attribution mechanism. Existing RALM approaches focus on modifying the LM architecture in order to facilitate the incorporation of external information, significantly complicating deployment. This paper considers a simple alternative, which we dub In-Context RALM: leaving the LM architecture unchanged and prepending grounding documents to the input, without any further training of the LM. We show that In-Context RALM that builds on off-the-shelf general purpose retrievers provides surprisingly large LM gains across model sizes and diverse corpora. We also demonstrate that the document retrieval and ranking mechanism can be specialized to the RALM setting to further boost performance. We conclude that In-Context RALM has considerable potential to increase the prevalence of LM grounding, particularly in settings where a pretrained LM must be used without modification or even via API access.1",
                    "[259137816 | Wang et al. | 2023 | Citations: 94]": "Existing large language models (LLMs) can only afford fix-sized inputs due to the input length limit, preventing them from utilizing rich long-context information from past inputs. To address this, we propose a framework, Language Models Augmented with Long-Term Memory (LongMem), which enables LLMs to memorize long history. We design a novel decoupled network architecture with the original backbone LLM frozen as a memory encoder and an adaptive residual side-network as a memory retriever and reader. Such a decoupled memory design can easily cache and update long-term past contexts for memory retrieval without suffering from memory staleness. Enhanced with memory-augmented adaptation training, LongMem can thus memorize long past context and use long-term memory for language modeling. The proposed memory retrieval module can handle unlimited-length context in its memory bank to benefit various downstream tasks. Typically, LongMem can enlarge the long-form memory to 65k tokens and thus cache many-shot extra demonstration examples as long-form memory for in-context learning. Experiments show that our method outperforms strong long-context models on ChapterBreak, a challenging long-context modeling benchmark, and achieves remarkable improvements on memory-augmented in-context learning over LLMs. The results demonstrate that the proposed method is effective in helping language models to memorize and utilize long-form contents. Our code is open-sourced at https://aka.ms/LongMem."
                },
                "metadata": [
                    {
                        "section_title": "Retrieval-Augmented Language Modeling",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 934,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 171
                            },
                            {
                                "start": 172,
                                "end": 319
                            },
                            {
                                "start": 320,
                                "end": 393
                            },
                            {
                                "start": 394,
                                "end": 555
                            },
                            {
                                "start": 556,
                                "end": 704
                            },
                            {
                                "start": 705,
                                "end": 824
                            },
                            {
                                "start": 825,
                                "end": 934
                            }
                        ],
                        "ref_mentions": [
                            "218869575",
                            "256459451",
                            "259137816"
                        ],
                        "quote": "Much effort has been made to enhance Retrieval-Augmented Language Modeling (Lewis et al., 2020)Izacard and Grave, 2020;(Ram et al., 2023)Yu et al., 2022;Asai et al., 2023). While some approaches use external retrievers, non-parametric information fusion often falls short compared to parametric methods within the model. We concentrate on integrating retrieval concepts directly into the model. REALM (Guu et al., 2020) suggests that relying solely on internal model knowledge is inefficient and advocates for the model to learn to retrieve and comprehend. kNN-LM (Khandelwal et al., 2019) enhances language modeling by blending the LLM's next-word predictions with those from a retrieval-based mechanism. MemTrm (Wu et al., 2022) introduces a memory bank but risks shifting memory distributions due to parameter adjustments. LongMEM (Wang et al., 2023) mitigates this by training a sub-network, though this adds significant overhead."
                    }
                ]
            },
            {
                "idx": 25,
                "key": "[272330251 | Monath et al. | 2024 | Citations: 0]",
                "snippets": "Retrieval augmented language models (RLMs) typically consist of two major architectural components, a retriever model (e.g., a dual-encoder) and a generative language model or reader model (Guu et al., 2020)Izacard & Grave, 2021;Izacard et al., 2022). The input to a retrieval augmented language model is a natural language text sequence, x. This input text will be encoded using a dual-encoder retrieval model, f (x). Retrieval will be performed over a corpus of targets, Y, returning k targets relevant to x, denoted S x (Y). The reader model takes as input the retrieved targets, S x (Y), and the text x, and generates text.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[211204736 | Guu et al. | 2020 | Citations: 2119]": "Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. \nTo capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. \nWe demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity."
                },
                "metadata": [
                    {
                        "section_title": "Latent Variables in Retrieval Augmented Models",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 627,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 251
                            },
                            {
                                "start": 252,
                                "end": 341
                            },
                            {
                                "start": 342,
                                "end": 418
                            },
                            {
                                "start": 419,
                                "end": 527
                            },
                            {
                                "start": 528,
                                "end": 627
                            }
                        ],
                        "ref_mentions": [
                            "211204736"
                        ],
                        "quote": "Retrieval augmented language models (RLMs) typically consist of two major architectural components, a retriever model (e.g., a dual-encoder) and a generative language model or reader model (Guu et al., 2020)Izacard & Grave, 2021;Izacard et al., 2022). The input to a retrieval augmented language model is a natural language text sequence, x. This input text will be encoded using a dual-encoder retrieval model, f (x). Retrieval will be performed over a corpus of targets, Y, returning k targets relevant to x, denoted S x (Y). The reader model takes as input the retrieved targets, S x (Y), and the text x, and generates text."
                    }
                ]
            },
            {
                "idx": 26,
                "key": "[273654180 | Tejaswi et al. | 2024 | Citations: 1]",
                "snippets": "Retrieval Large language models pre-trained with autoregressive setups (Jiang et al., 2023;Dubey et al., 2024) have shown remarkable performance when adapted to retrieval tasks (Wang et al., 2024b;BehnamGhader et al., 2024), outperforming encoder-style retrievers (Izacard et al., 2021)Wang et al., 2024a).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[249097975 | Izacard et al. | 2021 | Citations: 924]": "Recently, information retrieval has seen the emergence of dense retrievers, using neural networks, as an alternative to classical sparse methods based on term-frequency. These models have obtained state-of-the-art results on datasets and tasks where large training sets are available. However, they do not transfer well to new applications with no training data, and are outperformed by unsupervised term-frequency methods such as BM25. In this work, we explore the limits of contrastive learning as a way to train unsupervised dense retrievers and show that it leads to strong performance in various retrieval settings. On the BEIR benchmark our unsupervised model outperforms BM25 on 11 out of 15 datasets for the Recall@100. When used as pre-training before fine-tuning, either on a few thousands in-domain examples or on the large MS~MARCO dataset, our contrastive model leads to improvements on the BEIR benchmark. Finally, we evaluate our approach for multi-lingual retrieval, where training data is even scarcer than for English, and show that our approach leads to strong unsupervised performance. Our model also exhibits strong cross-lingual transfer when fine-tuned on supervised English data only and evaluated on low resources language such as Swahili. We show that our unsupervised models can perform cross-lingual retrieval between different scripts, such as retrieving English documents from Arabic queries, which would not be possible with term matching methods."
                },
                "metadata": [
                    {
                        "section_title": "RELATED WORK",
                        "pdf_hash": "",
                        "start": 164,
                        "end": 470,
                        "sentence_offsets": [
                            {
                                "start": 164,
                                "end": 470
                            }
                        ],
                        "ref_mentions": [
                            "249097975"
                        ],
                        "quote": "Retrieval Large language models pre-trained with autoregressive setups (Jiang et al., 2023;Dubey et al., 2024) have shown remarkable performance when adapted to retrieval tasks (Wang et al., 2024b;BehnamGhader et al., 2024), outperforming encoder-style retrievers (Izacard et al., 2021)Wang et al., 2024a)."
                    }
                ]
            },
            {
                "idx": 27,
                "key": "[274982275 | Zayyad et al. | 2024 | Citations: 0]",
                "snippets": "RAG is a sophisticated framework designed to enhance language models by coupling them with external retrieval systems, addressing limitations inherent in static, solely parameter-based language models. RAG integrates a dual-component architecture where a retriever dynamically searches a structured external corpus for relevant information based on the input query, and a generator LLM uses the retrieved content as context to generate accurate and contextually enriched responses [Gao et al., 2023, Mialon et al., 2023]...RAGs can employ two main types of retrieval mechanisms: dense and sparse [Mialon et al., 2023]. Sparse retrievers rely on bag-of-words representations, excelling at finding documents with high term overlap to the query, while dense retrievers utilize neural network embeddings to capture semantic similarities, enhancing the model's comprehension of related concepts.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "A. Retrieval-Augmented Language Models",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 520,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 201
                            },
                            {
                                "start": 202,
                                "end": 521
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "RAG is a sophisticated framework designed to enhance language models by coupling them with external retrieval systems, addressing limitations inherent in static, solely parameter-based language models. RAG integrates a dual-component architecture where a retriever dynamically searches a structured external corpus for relevant information based on the input query, and a generator LLM uses the retrieved content as context to generate accurate and contextually enriched responses [Gao et al., 2023, Mialon et al., 2023]"
                    },
                    {
                        "section_title": "A. Retrieval-Augmented Language Models",
                        "pdf_hash": "",
                        "start": 1031,
                        "end": 1399,
                        "sentence_offsets": [
                            {
                                "start": 1031,
                                "end": 1126
                            },
                            {
                                "start": 1127,
                                "end": 1398
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "RAGs can employ two main types of retrieval mechanisms: dense and sparse [Mialon et al., 2023]. Sparse retrievers rely on bag-of-words representations, excelling at finding documents with high term overlap to the query, while dense retrievers utilize neural network embeddings to capture semantic similarities, enhancing the model's comprehension of related concepts."
                    }
                ]
            },
            {
                "idx": 28,
                "key": "[275337144 | Ros et al. | 2025 | Citations: 0]",
                "snippets": "We choose these methods because they are commonly used in the retrieval literature, with bi-encoders often used in the initial retrieval stage [25], and cross-encoders often used in the reranking stage (Reimers et al., 2019).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[201646309 | Reimers et al. | 2019 | Citations: 12316]": "BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods."
                },
                "metadata": [
                    {
                        "section_title": "Retrieval",
                        "pdf_hash": "",
                        "start": 144,
                        "end": 351,
                        "sentence_offsets": [
                            {
                                "start": 144,
                                "end": 351
                            }
                        ],
                        "ref_mentions": [
                            "201646309"
                        ],
                        "quote": "We choose these methods because they are commonly used in the retrieval literature, with bi-encoders often used in the initial retrieval stage [25], and cross-encoders often used in the reranking stage (Reimers et al., 2019)."
                    }
                ]
            },
            {
                "idx": 29,
                "key": "[276421942 | Liu et al. | 2025 | Citations: 4]",
                "snippets": "Retrieval models have evolved from early sparse retrievers, such as TF-IDF (Jones, 1973) and BM25 (Robertson and Zaragoza, 2009b), which rely on word statistics and inverted indices, to dense retrievers (Lewis et al., 2020b) that utilize neural representations for semantic matching.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 306,
                        "end": 589,
                        "sentence_offsets": [
                            {
                                "start": 306,
                                "end": 589
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Retrieval models have evolved from early sparse retrievers, such as TF-IDF (Jones, 1973) and BM25 (Robertson and Zaragoza, 2009b), which rely on word statistics and inverted indices, to dense retrievers (Lewis et al., 2020b) that utilize neural representations for semantic matching."
                    }
                ]
            },
            {
                "idx": 30,
                "key": "[277621440 | Zhang et al. | 2025 | Citations: 0]",
                "snippets": "Large language models (LLMs) are being widely applied across various fields [27,54,61], and garnering increasing attention for their application in retrieval tasks [25,(Ma et al., 2023)[51]. Unlike bidirectional attention mechanisms in encoder-style pre-trained language models (PLMs) such as BERT, LLMs are typically decoder-style models that employ unidirectional attention.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[263908865 | Ma et al. | 2023 | Citations: 223]": "While large language models (LLMs) have shown impressive NLP capabilities, existing IR applications mainly focus on prompting LLMs to generate query expansions or generating permutations for listwise reranking. In this study, we leverage LLMs directly to serve as components in the widely used multi-stage text ranking pipeline. Specifically, we fine-tune the open-source LLaMA-2 model as a dense retriever (repLLaMA) and a pointwise reranker (rankLLaMA). This is performed for both passage and document retrieval tasks using the MS MARCO training data. Our study shows that finetuned LLM retrieval models outperform smaller models. They are more effective and exhibit greater generalizability, requiring only a straightforward training strategy. Moreover, our pipeline allows for the fine-tuning of LLMs at each stage of a multi-stage retrieval pipeline. This demonstrates the strong potential for optimizing LLMs to enhance a variety of retrieval tasks. Furthermore, as LLMs are naturally pre-trained with longer contexts, they can directly represent longer documents. This eliminates the need for heuristic segmenting and pooling strategies to rank long documents. On the MS MARCO and BEIR datasets, our repLLaMA-rankLLaMA pipeline demonstrates a high level of effectiveness."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 856,
                        "end": 1217,
                        "sentence_offsets": [
                            {
                                "start": 856,
                                "end": 1031
                            },
                            {
                                "start": 1032,
                                "end": 1217
                            }
                        ],
                        "ref_mentions": [
                            "263908865"
                        ],
                        "quote": "Large language models (LLMs) are being widely applied across various fields [27,54,61], and garnering increasing attention for their application in retrieval tasks [25,(Ma et al., 2023)[51]. Unlike bidirectional attention mechanisms in encoder-style pre-trained language models (PLMs) such as BERT, LLMs are typically decoder-style models that employ unidirectional attention."
                    }
                ]
            },
            {
                "idx": 31,
                "key": "[2862121 | Chiu et al. | 2009 | Citations: 0]",
                "snippets": "Statistical language modeling (SLM) has been used in many different domains for decades and has also been applied to information retrieval (IR) recently. Documents retrieved using this approach are ranked according their probability of generating the given query.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 263,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Statistical language modeling (SLM) has been used in many different domains for decades and has also been applied to information retrieval (IR) recently. Documents retrieved using this approach are ranked according their probability of generating the given query."
                    }
                ]
            },
            {
                "idx": 32,
                "key": "[5764728 | Kraaij et al. | 2003 | Citations: 123]",
                "snippets": "Instead of translating a query before estimating a query model (the external approach), we propose to directly estimate the query model in the target language. We will do this by decomposing the problem into two components that are easier to estimate: \n\nwhere L is the size of the source vocabulary. Thus, P (t i |M Qs ) can be approximated by combining the translation model P (t i |s j ), which we can estimate on the parallel Web corpus, and the familiar P (s j |M Qs ) which can be estimated using relative frequencies. This simplified model, from which we have dropped the dependency of P (t i |s j ) on Q, can be interpreted as a way of mapping the probability distribution function in the source language event space P (s j |M Qs ) onto the event space of the target language vocabulary.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Estimating the query model in the target language (QT)",
                        "pdf_hash": "",
                        "start": 187,
                        "end": 981,
                        "sentence_offsets": [
                            {
                                "start": 187,
                                "end": 346
                            },
                            {
                                "start": 347,
                                "end": 438
                            },
                            {
                                "start": 441,
                                "end": 486
                            },
                            {
                                "start": 487,
                                "end": 710
                            },
                            {
                                "start": 711,
                                "end": 981
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Instead of translating a query before estimating a query model (the external approach), we propose to directly estimate the query model in the target language. We will do this by decomposing the problem into two components that are easier to estimate: \n\nwhere L is the size of the source vocabulary. Thus, P (t i |M Qs ) can be approximated by combining the translation model P (t i |s j ), which we can estimate on the parallel Web corpus, and the familiar P (s j |M Qs ) which can be estimated using relative frequencies. This simplified model, from which we have dropped the dependency of P (t i |s j ) on Q, can be interpreted as a way of mapping the probability distribution function in the source language event space P (s j |M Qs ) onto the event space of the target language vocabulary."
                    }
                ]
            },
            {
                "idx": 33,
                "key": "[644854 | Hiemstra et al. | 2005 | Citations: 8]",
                "snippets": "The right-hand side of the equation corresponds to the following region expression. \n\n(<doc> CONTAINING db) AND (<doc> CONTAINING ir) \n\nThis can be shown as follows: The region expression (<doc> CONTAINING db) produces all documents ranked according to P (T = db|D), i.e., all regions tagged as <doc>, ranked by the number of occurrences of db in those regions. Similarly, (<doc> CONTAINING ir) produces all documents ranked according to P (T = ir|D). Finally, the operator AND results in the regions tagged as <doc> that are in both operand sets. The score of the result regions is defined as the product of the scores of the same regions in the operands.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "The simplest unigram language model",
                        "pdf_hash": "",
                        "start": 1100,
                        "end": 1756,
                        "sentence_offsets": [
                            {
                                "start": 1100,
                                "end": 1183
                            },
                            {
                                "start": 1186,
                                "end": 1233
                            },
                            {
                                "start": 1236,
                                "end": 1461
                            },
                            {
                                "start": 1462,
                                "end": 1551
                            },
                            {
                                "start": 1552,
                                "end": 1647
                            },
                            {
                                "start": 1648,
                                "end": 1756
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The right-hand side of the equation corresponds to the following region expression. \n\n(<doc> CONTAINING db) AND (<doc> CONTAINING ir) \n\nThis can be shown as follows: The region expression (<doc> CONTAINING db) produces all documents ranked according to P (T = db|D), i.e., all regions tagged as <doc>, ranked by the number of occurrences of db in those regions. Similarly, (<doc> CONTAINING ir) produces all documents ranked according to P (T = ir|D). Finally, the operator AND results in the regions tagged as <doc> that are in both operand sets. The score of the result regions is defined as the product of the scores of the same regions in the operands."
                    }
                ]
            },
            {
                "idx": 34,
                "key": "[7742063 | Lee et al. | 2008 | Citations: 68]",
                "snippets": "The basic language modeling framework does not address the issue of lexical gaps between queries and question. (Berger et al., 1999) viewed information retrieval as statistical document-query translation and introduced translation models to map query words to document words. Assuming that a translation model can be represented by a conditional probability distribution of translation T (\u2022|\u2022) between words, we can model P (q|M D ) in Equation 3 as: \n\nwhere w represents a document word. 3 he translation probability T (q|w) virtually represents the degree of relationship between query word q and document word w captured in a different, machine translation setting. Then, in the traditional information retrieval viewpoint, the use of translation models produce an implicit query expansion effect, since query words not in a document are mapped to related words in the document. This implies that translation-based retrieval models would make positive contributions to retrieval performance only when the pre-constructed translation models have reliable translation probability distributions.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Translation-based Retrieval Model",
                        "pdf_hash": "",
                        "start": 992,
                        "end": 2092,
                        "sentence_offsets": [
                            {
                                "start": 992,
                                "end": 1102
                            },
                            {
                                "start": 1103,
                                "end": 1272
                            },
                            {
                                "start": 1273,
                                "end": 1447
                            },
                            {
                                "start": 1450,
                                "end": 1487
                            },
                            {
                                "start": 1488,
                                "end": 1665
                            },
                            {
                                "start": 1666,
                                "end": 1878
                            },
                            {
                                "start": 1879,
                                "end": 2092
                            }
                        ],
                        "ref_mentions": [
                            "10837883"
                        ],
                        "quote": "The basic language modeling framework does not address the issue of lexical gaps between queries and question. (Berger et al., 1999) viewed information retrieval as statistical document-query translation and introduced translation models to map query words to document words. Assuming that a translation model can be represented by a conditional probability distribution of translation T (\u2022|\u2022) between words, we can model P (q|M D ) in Equation 3 as: \n\nwhere w represents a document word. 3 he translation probability T (q|w) virtually represents the degree of relationship between query word q and document word w captured in a different, machine translation setting. Then, in the traditional information retrieval viewpoint, the use of translation models produce an implicit query expansion effect, since query words not in a document are mapped to related words in the document. This implies that translation-based retrieval models would make positive contributions to retrieval performance only when the pre-constructed translation models have reliable translation probability distributions."
                    }
                ]
            },
            {
                "idx": 35,
                "key": "[8486003 | Tan et al. | 2016 | Citations: 29]",
                "snippets": "To model the relationship between image and language, some works associate both modalities by embedding their representations into a common space (Hodosh et al., 2013)(Frome et al., 2013)(Socher et al., 2014)(Karpathy et al., 2014). First, they obtain the image features using a visual model like CNN (Frome et al., 2013)(Socher et al., 2014), as well as the representation of sentence with a language model such as recursive neural network (Socher et al., 2014). Then, both of them are embedded into a common multimodal space and the whole model is learned with ranking objective for image and sentence retrieval task. This framework was also tested at object level by Karpathy et al. (Karpathy et al., 2014) and proved to yield better results for the image and sentence bi-directional retrieval task.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[2315434 | Karpathy et al. | 2014 | Citations: 937]": "We introduce a model for bidirectional retrieval of images and sentences through a deep, multi-modal embedding of visual and natural language data. Unlike previous models that directly map images or sentences into a common embedding space, our model works on a finer level and embeds fragments of images (objects) and fragments of sentences (typed dependency tree relations) into a common space. We then introduce a structured max-margin objective that allows our model to explicitly associate these fragments across modalities. Extensive experimental evaluation shows that reasoning on both the global level of images and sentences and the finer level of their respective fragments improves performance on image-sentence retrieval tasks. Additionally, our model provides interpretable predictions for the image-sentence retrieval task since the inferred inter-modal alignment of fragments is explicit.",
                    "[2317858 | Socher et al. | 2014 | Citations: 897]": "Previous work on Recursive Neural Networks (RNNs) shows that these models can produce compositional feature vectors for accurately representing and classifying sentences or images. However, the sentence vectors of previous models cannot accurately represent visually grounded meaning. We introduce the DT-RNN model which uses dependency trees to embed sentences into a vector space in order to retrieve images that are described by those sentences. Unlike previous RNN-based models which use constituency trees, DT-RNNs naturally focus on the action and agents in a sentence. They are better able to abstract from the details of word order and syntactic expression. DT-RNNs outperform other recursive and recurrent neural networks, kernelized CCA and a bag-of-words baseline on the tasks of finding an image that fits a sentence description and vice versa. They also give more similar representations to sentences that describe the same image.",
                    "[928608 | Hodosh et al. | 2013 | Citations: 1325]": "The ability to associate images with natural language sentences that describe what is depicted in them is a hallmark of image understanding, and a prerequisite for applications such as sentence-based image search. In analogy to image search, we propose to frame sentence-based image annotation as the task of ranking a given pool of captions. We introduce a new benchmark collection for sentence-based image description and search, consisting of 8,000 images that are each paired with five different captions which provide clear descriptions of the salient entities and events. We introduce a number of systems that perform quite well on this task, even though they are only based on features that can be obtained with minimal supervision. Our results clearly indicate the importance of training on multiple captions per image, and of capturing syntactic (word order-based) and semantic features of these captions. We also perform an in-depth comparison of human and automatic evaluation metrics for this task, and propose strategies for collecting human judgments cheaply and on a very large scale, allowing us to augment our collection with additional relevance judgments of which captions describe which image. Our analysis shows that metrics that consider the ranked list of results for each query image or sentence are significantly more robust than metrics that are based on a single response per query. Moreover, our study suggests that the evaluation of ranking-based image description systems may be fully automated."
                },
                "metadata": [
                    {
                        "section_title": "Multimodal Representation and Transition",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 663,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 163
                            },
                            {
                                "start": 164,
                                "end": 343
                            },
                            {
                                "start": 344,
                                "end": 499
                            },
                            {
                                "start": 500,
                                "end": 663
                            }
                        ],
                        "ref_mentions": [
                            "928608",
                            "261138",
                            "2317858",
                            "2315434",
                            "261138",
                            "2317858",
                            "2317858",
                            "2315434"
                        ],
                        "quote": "To model the relationship between image and language, some works associate both modalities by embedding their representations into a common space (Hodosh et al., 2013)(Frome et al., 2013)(Socher et al., 2014)(Karpathy et al., 2014). First, they obtain the image features using a visual model like CNN (Frome et al., 2013)(Socher et al., 2014), as well as the representation of sentence with a language model such as recursive neural network (Socher et al., 2014). Then, both of them are embedded into a common multimodal space and the whole model is learned with ranking objective for image and sentence retrieval task. This framework was also tested at object level by Karpathy et al. (Karpathy et al., 2014) and proved to yield better results for the image and sentence bi-directional retrieval task."
                    }
                ]
            },
            {
                "idx": 36,
                "key": "[90238494 | Petersen | 2019 | Citations: 0]",
                "snippets": "A language model (LM) is, in its most basic form, a probability distribution over terms in a language where each term, t is assigned a non-zero probability denoting its probability of occurrence in the \"language\". A \"language\" here is defined as a non-empty finite sequence of symbols or terms. Given a query q and document d \u2208C for some collection C , d 's LM, \u03b8 d , is a probabilistic model that estimates the probability that q was generated by d . In other words, each document is viewed as a sample from the language, and its relevance to q is estimated as the probability that q was generated from this sample.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Language Modelling",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 616,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "A language model (LM) is, in its most basic form, a probability distribution over terms in a language where each term, t is assigned a non-zero probability denoting its probability of occurrence in the \"language\". A \"language\" here is defined as a non-empty finite sequence of symbols or terms. Given a query q and document d \u2208C for some collection C , d 's LM, \u03b8 d , is a probabilistic model that estimates the probability that q was generated by d . In other words, each document is viewed as a sample from the language, and its relevance to q is estimated as the probability that q was generated from this sample."
                    }
                ]
            },
            {
                "idx": 37,
                "key": "[9519540 | Cummins et al. | 2015 | Citations: 35]",
                "snippets": "The multinomial language model has been one of the most effective models of retrieval for more than a decade. However, the multinomial distribution does not model one important linguistic phenomenon relating to term dependency\u2014that is, the tendency of a term to repeat itself within a document (i.e., word burstiness).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "The multinomial language model has been one of the most effective models of retrieval for more than a decade. However, the multinomial distribution does not model one important linguistic phenomenon relating to term dependency\u2014that is, the tendency of a term to repeat itself within a document (i.e., word burstiness).",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.030498,
        "cot": "For the query \"are the common model retrieval language models\", I need to identify the key dimensions that will help provide a comprehensive answer. \n\nThe query is asking about common types or models of retrieval language models (RLMs). Based on my analysis of the provided quotes, I can identify several key dimensions:\n\n1. \"Introduction to Retrieval Language Models\": This should be a synthesis section that provides a foundational understanding of what retrieval language models are, their purpose, and basic concepts. This will help frame the rest of the answer.\n\n2. \"Types of Retrieval Models\": Many quotes discuss various categories and architectures of retrieval models. This should be a list format to clearly enumerate the different types of models mentioned across the quotes.\n\n3. \"Architectural Components of Retrieval Language Models\": Multiple quotes discuss the components and structure of RLMs, like retrievers and generators. This should be a synthesis to explain how these components work together.\n\n4. \"Applications and Performance\": Several quotes mention how these models are applied in various tasks and their performance benefits. A synthesis format works best here to discuss the broader impact and effectiveness.\n\n5. \"Evolution of Retrieval Models\": Some quotes discuss how retrieval models have evolved over time, from traditional to neural approaches. This context is important and should be presented as a synthesis.\n\nThese dimensions provide a comprehensive framework to answer the query about common retrieval language models, covering what they are, their types, components, applications, and evolution.",
        "plan": {
            "Introduction to Retrieval Language Models (synthesis)": [
                4,
                10,
                12,
                13,
                15,
                24,
                25,
                27
            ],
            "Types of Retrieval Models (list)": [
                5,
                9,
                14,
                17,
                19,
                20,
                28,
                29
            ],
            "Architectural Components of Retrieval Language Models (synthesis)": [
                1,
                2,
                3,
                7,
                8,
                11,
                18,
                22,
                26,
                30
            ],
            "Applications and Performance (synthesis)": [
                6,
                16,
                21,
                23,
                35
            ],
            "Evolution of Retrieval Models (synthesis)": [
                0,
                31,
                32,
                33,
                34,
                36,
                37
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction to Retrieval Language Models",
                "tldr": "Retrieval Language Models (RLMs) enhance traditional language models by incorporating external knowledge through retrieval mechanisms. They combine parametric neural networks with non-parametric retrieval components to access relevant information during inference, addressing limitations of purely parameter-based models. (12 sources)",
                "text": "\nRetrieval Language Models (RLMs) represent a significant advancement in natural language processing by augmenting traditional language models with external knowledge retrieval capabilities. These models have demonstrated substantial improvements across various tasks including language modeling, machine translation, question answering, and code generation <Paper corpusId=\"246431219\" paperTitle=\"(Alon et al., 2022)\" isShortName></Paper>. The fundamental innovation of RLMs lies in their ability to leverage training examples at test time, rather than relying solely on information encoded in the model's parameters <Paper corpusId=\"246431219\" paperTitle=\"(Alon et al., 2022)\" isShortName></Paper>.\n\nThe architecture of RLMs typically consists of two main components: a retriever model that searches for relevant information from an external corpus, and a reader or generator model that incorporates this retrieved information to produce outputs <Paper corpusId=\"272330251\" paperTitle=\"(Monath et al., 2024)\" isShortName></Paper> <Paper corpusId=\"211204736\" paperTitle=\"(Guu et al., 2020)\" isShortName></Paper>. This dual-component design allows RLMs to dynamically access and utilize contextually relevant information during inference <Paper corpusId=\"274982275\" paperTitle=\"(Zayyad et al., 2024)\" isShortName></Paper>.\n\nOne prominent example of a retrieval-based model is kNN-LM, which extends pre-trained neural language models by linearly interpolating their output with a k-nearest neighbors distribution <Paper corpusId=\"246431219\" paperTitle=\"(Alon et al., 2022)\" isShortName></Paper> <Paper corpusId=\"207870430\" paperTitle=\"(Khandelwal et al., 2019)\" isShortName></Paper>. This approach has achieved state-of-the-art results in language modeling tasks, with particular effectiveness in predicting rare patterns and factual knowledge <Paper corpusId=\"207870430\" paperTitle=\"(Khandelwal et al., 2019)\" isShortName></Paper>.\n\nRetrieval-augmented language models can be implemented using both encoder-decoder architectures, as seen in models like RAG <Paper corpusId=\"260900354\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"218869575\" paperTitle=\"(Lewis et al., 2020)\" isShortName></Paper>, and decoder-only architectures, as demonstrated by kNN-LM and RETRO <Paper corpusId=\"260900354\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"207870430\" paperTitle=\"(Khandelwal et al., 2019)\" isShortName></Paper> <Paper corpusId=\"244954723\" paperTitle=\"(Borgeaud et al., 2021)\" isShortName></Paper>. These models follow different paradigms for incorporating external knowledge, with some using a \"retrieve-and-read\" approach where retrieved documents are provided as additional context <Paper corpusId=\"263605962\" paperTitle=\"(Lin et al., 2023)\" isShortName></Paper>.\n\nThe retrieval mechanisms in these models can be categorized as either dense or sparse <Paper corpusId=\"274982275\" paperTitle=\"(Zayyad et al., 2024)\" isShortName></Paper>. Sparse retrievers utilize bag-of-words representations to find documents with high term overlap with the query, while dense retrievers employ neural network embeddings to capture semantic similarities <Paper corpusId=\"274982275\" paperTitle=\"(Zayyad et al., 2024)\" isShortName></Paper>.\n\nRecent advances have also introduced more flexible approaches such as \"In-Context RALM,\" which simply prepends retrieved documents to the input without modifying the language model architecture <Paper corpusId=\"272310589\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"256459451\" paperTitle=\"(Ram et al., 2023)\" isShortName></Paper>. This method has shown surprising effectiveness while simplifying deployment <Paper corpusId=\"256459451\" paperTitle=\"(Ram et al., 2023)\" isShortName></Paper>.\n\nOverall, retrieval-augmented language models have demonstrated impressive performance across diverse NLP tasks <Paper corpusId=\"263866951\" paperTitle=\"(Asai et al., 2023)\" isShortName></Paper>, particularly excelling in knowledge-intensive applications where access to external information is crucial for generating accurate and contextually appropriate responses.",
                "citations": [
                    {
                        "id": "(Alon et al., 2022)",
                        "snippets": [
                            "Retrieval-based language models (R-LMs) have recently been shown to improve over standard neural models in a variety of tasks such as unconditional language modeling (Guu et al., 2018;He et al., 2020), machine translation (Zhang et al., 2018;Gu et al., 2018;Khandelwal et al., 2021), question answering (Karpukhin et al., 2020;Ram et al., 2021), and code generation (Hayati et al., 2018;Hashimoto et al., 2018). The key ingredient of R-LMs is their ability to utilize training examples at test time without having to rely on the information encoded in the model's weights only",
                            "One prominent example of such a retrieval-based model is kNN-LM (Grave et al., 2017;(Khandelwal et al., 2019), which predicts a token by linearly interpolating the base LM's output with a nonparametric nearest neighbor distribution."
                        ],
                        "paper": {
                            "corpus_id": 246431219,
                            "title": "Neuro-Symbolic Language Modeling with Automaton-augmented Retrieval",
                            "authors": [
                                {
                                    "authorId": "47051926",
                                    "name": "Uri Alon"
                                },
                                {
                                    "authorId": "40027632",
                                    "name": "Frank F. Xu"
                                },
                                {
                                    "authorId": "6215698",
                                    "name": "Junxian He"
                                },
                                {
                                    "authorId": "2072419570",
                                    "name": "Sudipta Sengupta"
                                },
                                {
                                    "authorId": "144590225",
                                    "name": "D. Roth"
                                },
                                {
                                    "authorId": "1700325",
                                    "name": "Graham Neubig"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 63
                        },
                        "score": 0.97412109375
                    },
                    {
                        "id": "(Monath et al., 2024)",
                        "snippets": [
                            "Retrieval augmented language models (RLMs) typically consist of two major architectural components, a retriever model (e.g., a dual-encoder) and a generative language model or reader model (Guu et al., 2020)Izacard & Grave, 2021;Izacard et al., 2022). The input to a retrieval augmented language model is a natural language text sequence, x. This input text will be encoded using a dual-encoder retrieval model, f (x). Retrieval will be performed over a corpus of targets, Y, returning k targets relevant to x, denoted S x (Y). The reader model takes as input the retrieved targets, S x (Y), and the text x, and generates text."
                        ],
                        "paper": {
                            "corpus_id": 272330251,
                            "title": "A Fresh Take on Stale Embeddings: Improving Dense Retriever Training with Corrector Networks",
                            "authors": [
                                {
                                    "authorId": "2121348263",
                                    "name": "Nicholas Monath"
                                },
                                {
                                    "authorId": "2319130233",
                                    "name": "Will Sussman Grathwohl"
                                },
                                {
                                    "authorId": "51020741",
                                    "name": "Michael Boratko"
                                },
                                {
                                    "authorId": "2300098510",
                                    "name": "Rob Fergus"
                                },
                                {
                                    "authorId": "2286335051",
                                    "name": "Andrew McCallum"
                                },
                                {
                                    "authorId": "1771307",
                                    "name": "M. Zaheer"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 0
                        },
                        "score": 0.98388671875
                    },
                    {
                        "id": "(Guu et al., 2020)",
                        "snippets": [
                            "Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. \nTo capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. \nWe demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity."
                        ],
                        "paper": {
                            "corpus_id": 211204736,
                            "title": "REALM: Retrieval-Augmented Language Model Pre-Training",
                            "authors": [
                                {
                                    "authorId": "2091768",
                                    "name": "Kelvin Guu"
                                },
                                {
                                    "authorId": "2544107",
                                    "name": "Kenton Lee"
                                },
                                {
                                    "authorId": "9941702",
                                    "name": "Zora Tung"
                                },
                                {
                                    "authorId": "2616463",
                                    "name": "Panupong Pasupat"
                                },
                                {
                                    "authorId": "1744179",
                                    "name": "Ming-Wei Chang"
                                }
                            ],
                            "year": 2020,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 2119
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zayyad et al., 2024)",
                        "snippets": [
                            "RAG is a sophisticated framework designed to enhance language models by coupling them with external retrieval systems, addressing limitations inherent in static, solely parameter-based language models. RAG integrates a dual-component architecture where a retriever dynamically searches a structured external corpus for relevant information based on the input query, and a generator LLM uses the retrieved content as context to generate accurate and contextually enriched responses [Gao et al., 2023, Mialon et al., 2023]",
                            "RAGs can employ two main types of retrieval mechanisms: dense and sparse [Mialon et al., 2023]. Sparse retrievers rely on bag-of-words representations, excelling at finding documents with high term overlap to the query, while dense retrievers utilize neural network embeddings to capture semantic similarities, enhancing the model's comprehension of related concepts."
                        ],
                        "paper": {
                            "corpus_id": 274982275,
                            "title": "Formal Language Knowledge Corpus for Retrieval Augmented Generation",
                            "authors": [
                                {
                                    "authorId": "2336913948",
                                    "name": "Majd Zayyad"
                                },
                                {
                                    "authorId": "2727584",
                                    "name": "Yossi Adi"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.9755859375
                    },
                    {
                        "id": "(Khandelwal et al., 2019)",
                        "snippets": [
                            "We introduce $k$NN-LMs, which extend a pre-trained neural language model (LM) by linearly interpolating it with a $k$-nearest neighbors ($k$NN) model. The nearest neighbors are computed according to distance in the pre-trained LM embedding space, and can be drawn from any text collection, including the original LM training data. Applying this augmentation to a strong Wikitext-103 LM, with neighbors drawn from the original training set, our $k$NN-LM achieves a new state-of-the-art perplexity of 15.79 - a 2.9 point improvement with no additional training. We also show that this approach has implications for efficiently scaling up to larger training sets and allows for effective domain adaptation, by simply varying the nearest neighbor datastore, again without further training. Qualitatively, the model is particularly helpful in predicting rare patterns, such as factual knowledge. Together, these results strongly suggest that learning similarity between sequences of text is easier than predicting the next word, and that nearest neighbor search is an effective approach for language modeling in the long tail."
                        ],
                        "paper": {
                            "corpus_id": 207870430,
                            "title": "Generalization through Memorization: Nearest Neighbor Language Models",
                            "authors": [
                                {
                                    "authorId": "3030219",
                                    "name": "Urvashi Khandelwal"
                                },
                                {
                                    "authorId": "39455775",
                                    "name": "Omer Levy"
                                },
                                {
                                    "authorId": "1746807",
                                    "name": "Dan Jurafsky"
                                },
                                {
                                    "authorId": "1982950",
                                    "name": "Luke Zettlemoyer"
                                },
                                {
                                    "authorId": "35084211",
                                    "name": "M. Lewis"
                                }
                            ],
                            "year": 2019,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 842
                        },
                        "score": 0
                    },
                    {
                        "id": "(Huang et al., 2023)",
                        "snippets": [
                            "Retrieval-augmented language models are a class of language models designed to enhance their performance by incorporating external knowledge. These models typically employ an information retrieval mechanism to access relevant information from a large corpus, which is then integrated into the model's prediction process. Retrieval-augmented LMs can be based on both encoder-decoder (Izacard et al., 2022)(Lewis et al., 2020) and decoderonly (Khandelwal et al., 2019)Borgeaud et al., 2022;(Shi et al., 2022) architectures."
                        ],
                        "paper": {
                            "corpus_id": 260900354,
                            "title": "RAVEN: In-Context Learning with Retrieval Augmented Encoder-Decoder Language Models",
                            "authors": [
                                {
                                    "authorId": "1490651934",
                                    "name": "Jie Huang"
                                },
                                {
                                    "authorId": "2056440915",
                                    "name": "Wei Ping"
                                },
                                {
                                    "authorId": "145011005",
                                    "name": "Peng Xu"
                                },
                                {
                                    "authorId": "1911755",
                                    "name": "M. Shoeybi"
                                },
                                {
                                    "authorId": "143922493",
                                    "name": "K. Chang"
                                },
                                {
                                    "authorId": "2301680",
                                    "name": "Bryan Catanzaro"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 35
                        },
                        "score": 0.95361328125
                    },
                    {
                        "id": "(Lewis et al., 2020)",
                        "snippets": [
                            "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline."
                        ],
                        "paper": {
                            "corpus_id": 218869575,
                            "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
                            "authors": [
                                {
                                    "authorId": "145222654",
                                    "name": "Patrick Lewis"
                                },
                                {
                                    "authorId": "3439053",
                                    "name": "Ethan Perez"
                                },
                                {
                                    "authorId": "1716179427",
                                    "name": "Aleksandara Piktus"
                                },
                                {
                                    "authorId": "40052301",
                                    "name": "F. Petroni"
                                },
                                {
                                    "authorId": "2067091563",
                                    "name": "Vladimir Karpukhin"
                                },
                                {
                                    "authorId": "39589154",
                                    "name": "Naman Goyal"
                                },
                                {
                                    "authorId": "103131985",
                                    "name": "Heinrich Kuttler"
                                },
                                {
                                    "authorId": "35084211",
                                    "name": "M. Lewis"
                                },
                                {
                                    "authorId": "144105277",
                                    "name": "Wen-tau Yih"
                                },
                                {
                                    "authorId": "2620211",
                                    "name": "Tim Rockt\u00e4schel"
                                },
                                {
                                    "authorId": "48662861",
                                    "name": "Sebastian Riedel"
                                },
                                {
                                    "authorId": "1743722",
                                    "name": "Douwe Kiela"
                                }
                            ],
                            "year": 2020,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 6476
                        },
                        "score": 0
                    },
                    {
                        "id": "(Borgeaud et al., 2021)",
                        "snippets": [
                            "We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a $2$ trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25$\\times$ fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale."
                        ],
                        "paper": {
                            "corpus_id": 244954723,
                            "title": "Improving language models by retrieving from trillions of tokens",
                            "authors": [
                                {
                                    "authorId": "148016269",
                                    "name": "Sebastian Borgeaud"
                                },
                                {
                                    "authorId": "1697879",
                                    "name": "A. Mensch"
                                },
                                {
                                    "authorId": "46616544",
                                    "name": "Jordan Hoffmann"
                                },
                                {
                                    "authorId": "2072572294",
                                    "name": "Trevor Cai"
                                },
                                {
                                    "authorId": "2143538252",
                                    "name": "Eliza Rutherford"
                                },
                                {
                                    "authorId": "2143434227",
                                    "name": "Katie Millican"
                                },
                                {
                                    "authorId": "47568983",
                                    "name": "George van den Driessche"
                                },
                                {
                                    "authorId": "143783339",
                                    "name": "Jean-Baptiste Lespiau"
                                },
                                {
                                    "authorId": "2143374656",
                                    "name": "Bogdan Damoc"
                                },
                                {
                                    "authorId": "31993415",
                                    "name": "Aidan Clark"
                                },
                                {
                                    "authorId": "40550616",
                                    "name": "Diego de Las Casas"
                                },
                                {
                                    "authorId": "40895205",
                                    "name": "Aurelia Guy"
                                },
                                {
                                    "authorId": "10698483",
                                    "name": "Jacob Menick"
                                },
                                {
                                    "authorId": "81387328",
                                    "name": "Roman Ring"
                                },
                                {
                                    "authorId": "4629007",
                                    "name": "T. Hennigan"
                                },
                                {
                                    "authorId": "2148653469",
                                    "name": "Saffron Huang"
                                },
                                {
                                    "authorId": "108173905",
                                    "name": "Lorenzo Maggiore"
                                },
                                {
                                    "authorId": "2115601070",
                                    "name": "Chris Jones"
                                },
                                {
                                    "authorId": "51042571",
                                    "name": "Albin Cassirer"
                                },
                                {
                                    "authorId": "2065040422",
                                    "name": "Andy Brock"
                                },
                                {
                                    "authorId": "35550664",
                                    "name": "Michela Paganini"
                                },
                                {
                                    "authorId": "2060655766",
                                    "name": "G. Irving"
                                },
                                {
                                    "authorId": "1689108",
                                    "name": "O. Vinyals"
                                },
                                {
                                    "authorId": "2217144",
                                    "name": "Simon Osindero"
                                },
                                {
                                    "authorId": "34838386",
                                    "name": "K. Simonyan"
                                },
                                {
                                    "authorId": "34269227",
                                    "name": "Jack W. Rae"
                                },
                                {
                                    "authorId": "152585800",
                                    "name": "Erich Elsen"
                                },
                                {
                                    "authorId": "2175946",
                                    "name": "L. Sifre"
                                }
                            ],
                            "year": 2021,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 1100
                        },
                        "score": 0
                    },
                    {
                        "id": "(Lin et al., 2023)",
                        "snippets": [
                            "Retrieval-Augmented Language Models RALMs fuse language models (LMs) with a retrieval module that explicitly augments the LM with external knowledge stores (Guu et al., 2020;Lewis et al., 2020). One mainstream type of RALM follows the \"retrieveand-read\" paradigm, where the retrieval module supplies external knowledge as additional context which the LM (reader) leverages to produce the final output (Izacard et al., 2022b;Borgeaud et al., 2022;Shi et al., 2023b;Ram et al., 2023)."
                        ],
                        "paper": {
                            "corpus_id": 263605962,
                            "title": "RA-DIT: Retrieval-Augmented Dual Instruction Tuning",
                            "authors": [
                                {
                                    "authorId": "2255374957",
                                    "name": "Xi Victoria Lin"
                                },
                                {
                                    "authorId": "1769736",
                                    "name": "Xilun Chen"
                                },
                                {
                                    "authorId": "46221498",
                                    "name": "Mingda Chen"
                                },
                                {
                                    "authorId": "2254168373",
                                    "name": "Weijia Shi"
                                },
                                {
                                    "authorId": "2253400960",
                                    "name": "Maria Lomeli"
                                },
                                {
                                    "authorId": "2191899140",
                                    "name": "Rich James"
                                },
                                {
                                    "authorId": "2253404757",
                                    "name": "Pedro Rodriguez"
                                },
                                {
                                    "authorId": "2253401183",
                                    "name": "Jacob Kahn"
                                },
                                {
                                    "authorId": "2253402270",
                                    "name": "Gergely Szilvasy"
                                },
                                {
                                    "authorId": "2253417398",
                                    "name": "Mike Lewis"
                                },
                                {
                                    "authorId": "2137813791",
                                    "name": "Luke S. Zettlemoyer"
                                },
                                {
                                    "authorId": "2253400757",
                                    "name": "Scott Yih"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 153
                        },
                        "score": 0.97998046875
                    },
                    {
                        "id": "(Liu et al., 2024)",
                        "snippets": [
                            "Much effort has been made to enhance Retrieval-Augmented Language Modeling (Lewis et al., 2020)Izacard and Grave, 2020;(Ram et al., 2023)Yu et al., 2022;Asai et al., 2023). While some approaches use external retrievers, non-parametric information fusion often falls short compared to parametric methods within the model. We concentrate on integrating retrieval concepts directly into the model. REALM (Guu et al., 2020) suggests that relying solely on internal model knowledge is inefficient and advocates for the model to learn to retrieve and comprehend. kNN-LM (Khandelwal et al., 2019) enhances language modeling by blending the LLM's next-word predictions with those from a retrieval-based mechanism. MemTrm (Wu et al., 2022) introduces a memory bank but risks shifting memory distributions due to parameter adjustments. LongMEM (Wang et al., 2023) mitigates this by training a sub-network, though this adds significant overhead."
                        ],
                        "paper": {
                            "corpus_id": 272310589,
                            "title": "MemLong: Memory-Augmented Retrieval for Long Text Modeling",
                            "authors": [
                                {
                                    "authorId": "2319081035",
                                    "name": "Weijie Liu"
                                },
                                {
                                    "authorId": "1576234850",
                                    "name": "Zecheng Tang"
                                },
                                {
                                    "authorId": "2257093356",
                                    "name": "Juntao Li"
                                },
                                {
                                    "authorId": "2266796043",
                                    "name": "Kehai Chen"
                                },
                                {
                                    "authorId": "2258690229",
                                    "name": "Min Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.96826171875
                    },
                    {
                        "id": "(Ram et al., 2023)",
                        "snippets": [
                            "Abstract Retrieval-Augmented Language Modeling (RALM) methods, which condition a language model (LM) on relevant documents from a grounding corpus during generation, were shown to significantly improve language modeling performance. In addition, they can mitigate the problem of factually inaccurate text generation and provide natural source attribution mechanism. Existing RALM approaches focus on modifying the LM architecture in order to facilitate the incorporation of external information, significantly complicating deployment. This paper considers a simple alternative, which we dub In-Context RALM: leaving the LM architecture unchanged and prepending grounding documents to the input, without any further training of the LM. We show that In-Context RALM that builds on off-the-shelf general purpose retrievers provides surprisingly large LM gains across model sizes and diverse corpora. We also demonstrate that the document retrieval and ranking mechanism can be specialized to the RALM setting to further boost performance. We conclude that In-Context RALM has considerable potential to increase the prevalence of LM grounding, particularly in settings where a pretrained LM must be used without modification or even via API access.1"
                        ],
                        "paper": {
                            "corpus_id": 256459451,
                            "title": "In-Context Retrieval-Augmented Language Models",
                            "authors": [
                                {
                                    "authorId": "73775461",
                                    "name": "Ori Ram"
                                },
                                {
                                    "authorId": "152754428",
                                    "name": "Yoav Levine"
                                },
                                {
                                    "authorId": "1491822146",
                                    "name": "Itay Dalmedigos"
                                },
                                {
                                    "authorId": "51918041",
                                    "name": "Dor Muhlgay"
                                },
                                {
                                    "authorId": "3140335",
                                    "name": "A. Shashua"
                                },
                                {
                                    "authorId": "2066411743",
                                    "name": "Kevin Leyton-Brown"
                                },
                                {
                                    "authorId": "1701353",
                                    "name": "Y. Shoham"
                                }
                            ],
                            "year": 2023,
                            "venue": "Transactions of the Association for Computational Linguistics",
                            "n_citations": 605
                        },
                        "score": 0
                    },
                    {
                        "id": "(Asai et al., 2023)",
                        "snippets": [
                            "Retrieval-based language models (LMs) have shown impressive performance on diverse NLP tasks."
                        ],
                        "paper": {
                            "corpus_id": 263866951,
                            "title": "Retrieval-based Language Models and Applications",
                            "authors": [
                                {
                                    "authorId": "2290402940",
                                    "name": "Akari Asai"
                                },
                                {
                                    "authorId": "48872685",
                                    "name": "Sewon Min"
                                },
                                {
                                    "authorId": "49164966",
                                    "name": "Zexuan Zhong"
                                },
                                {
                                    "authorId": "2286629648",
                                    "name": "Danqi Chen"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 88
                        },
                        "score": 0.990234375
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Types of Retrieval Models",
                "tldr": "Retrieval models can be categorized into sparse, dense, and hybrid approaches. Sparse models rely on term-matching and inverted indices, while dense models use neural embeddings to capture semantic relationships, with specific architectures including bi-encoders, cross-encoders, and late interaction models. (17 sources)",
                "text": "\nRetrieval models have evolved significantly over time, transitioning from traditional lexical-matching approaches to neural-based methods. These models can be broadly categorized into several types:\n\n## Sparse Retrieval Models\n* **Traditional Sparse Models**: Classic approaches like TF-IDF and BM25 utilize bag-of-words representations and rely on exact lexical matching between query and document terms <Paper corpusId=\"276421942\" paperTitle=\"(Liu et al., 2025)\" isShortName></Paper> <Paper corpusId=\"269302879\" paperTitle=\"(Zeng et al., 2024)\" isShortName></Paper> <Paper corpusId=\"207178704\" paperTitle=\"(Robertson et al., 2009)\" isShortName></Paper>.\n* **Neural Sparse Models**: More recent sparse retrieval models like SPLADE enhance traditional approaches by applying neural techniques while maintaining the efficiency of inverted indexes <Paper corpusId=\"258676146\" paperTitle=\"(Weller et al., 2023)\" isShortName></Paper> <Paper corpusId=\"235792467\" paperTitle=\"(Formal et al., 2021)\" isShortName></Paper> <Paper corpusId=\"252212320\" paperTitle=\"(Choi et al., 2022)\" isShortName></Paper>.\n\n## Dense Retrieval Models\nDense retrievers represent documents and queries as continuous vector representations, capturing semantic relationships beyond exact matching. These models can be further divided into several architectures:\n\n* **Bi-Encoders (Dual Encoders)**: These models independently encode queries and documents into single vector representations, comparing them using simple similarity metrics like dot product or cosine similarity <Paper corpusId=\"267377589\" paperTitle=\"(Liu et al._1, 2024)\" isShortName></Paper> <Paper corpusId=\"258676146\" paperTitle=\"(Weller et al., 2023)\" isShortName></Paper> <Paper corpusId=\"268856885\" paperTitle=\"(Gomez et al., 2024)\" isShortName></Paper>. Examples include DPR <Paper corpusId=\"215737187\" paperTitle=\"(Karpukhin et al., 2020)\" isShortName></Paper>, SentenceTransformer <Paper corpusId=\"201646309\" paperTitle=\"(Reimers et al., 2019)\" isShortName></Paper>, and general-purpose embeddings like E5 and Contriever <Paper corpusId=\"263835099\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper>.\n\n* **Cross-Encoders**: These models process the query and document together through a single encoder, enabling more complex interaction patterns but at higher computational cost <Paper corpusId=\"269302879\" paperTitle=\"(Zeng et al., 2024)\" isShortName></Paper> <Paper corpusId=\"275337144\" paperTitle=\"(Ros et al., 2025)\" isShortName></Paper> <Paper corpusId=\"252993059\" paperTitle=\"(Zhuang et al., 2022)\" isShortName></Paper>.\n\n* **Late Interaction Models**: Models like ColBERT represent a middle ground, encoding queries and documents into multiple vectors (one per token) and computing fine-grained interactions between them at query time <Paper corpusId=\"258676146\" paperTitle=\"(Weller et al., 2023)\" isShortName></Paper> <Paper corpusId=\"216553223\" paperTitle=\"(Khattab et al., 2020)\" isShortName></Paper>.\n\n## Other Retrieval Model Types\n* **Matching-focused Models**: Models like MatchPLM, MatchPyramid, and DRMM focus specifically on capturing matching patterns between queries and documents <Paper corpusId=\"252781830\" paperTitle=\"(Wang, 2022)\" isShortName></Paper>. For example, DRMM builds word-level similarity matrices and uses histogram mapping functions <Paper corpusId=\"5688521\" paperTitle=\"(Guo et al., 2016)\" isShortName></Paper>, while MatchPyramid views the matching matrix as an image and applies convolutional neural networks to identify matching patterns <Paper corpusId=\"3993933\" paperTitle=\"(Pang et al., 2016)\" isShortName></Paper>.\n\n* **Task-Specific Retrievers**: Some retrievers are designed for specific tasks, such as knowledge enhancement (AAR) or in-context learning (LLM-R), optimizing for particular use cases rather than general-purpose retrieval <Paper corpusId=\"263835099\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper>.\n\nEach model type offers different trade-offs between effectiveness, efficiency, and computational requirements. While dense retrievers have shown superior performance in capturing semantic relationships, sparse models maintain advantages in efficiency and interpretability, leading to ongoing research in both areas and hybrid approaches.",
                "citations": [
                    {
                        "id": "(Liu et al., 2025)",
                        "snippets": [
                            "Retrieval models have evolved from early sparse retrievers, such as TF-IDF (Jones, 1973) and BM25 (Robertson and Zaragoza, 2009b), which rely on word statistics and inverted indices, to dense retrievers (Lewis et al., 2020b) that utilize neural representations for semantic matching."
                        ],
                        "paper": {
                            "corpus_id": 276421942,
                            "title": "HopRAG: Multi-Hop Reasoning for Logic-Aware Retrieval-Augmented Generation",
                            "authors": [
                                {
                                    "authorId": "2345972557",
                                    "name": "Hao Liu"
                                },
                                {
                                    "authorId": "2288675277",
                                    "name": "Zhengren Wang"
                                },
                                {
                                    "authorId": "2346461266",
                                    "name": "Xi Chen"
                                },
                                {
                                    "authorId": "2268429641",
                                    "name": "Zhiyu Li"
                                },
                                {
                                    "authorId": "2268399953",
                                    "name": "Feiyu Xiong"
                                },
                                {
                                    "authorId": "2289597580",
                                    "name": "Qinhan Yu"
                                },
                                {
                                    "authorId": "2344098350",
                                    "name": "Wentao Zhang"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 4
                        },
                        "score": 0.92578125
                    },
                    {
                        "id": "(Zeng et al., 2024)",
                        "snippets": [
                            "With the emergence of large language models (LLMs) [14,(Devlin et al., 2019)39,43,(Raffel et al., 2019) and large-scale information retrieval datasets [4,(Kwiatkowski et al., 2019), neural-based IR models have demonstrated superior results over the traditional lexical-matching models, such as BM25 (Robertson et al., 2009).In general, these IR models can fall into three categories: (1) cross-encoder models [42,48,(Zhuang et al., 2022), (2) dense retrieval models [22,(Hofst\u00e4tter et al., 2021)(Karpukhin et al., 2020)(Khattab et al., 2020)38,(Zeng et al., 2022), and (3) sparse retrieval models [12](Choi et al., 2022)18,(Formal et al., 2021)."
                        ],
                        "paper": {
                            "corpus_id": 269302879,
                            "title": "Planning Ahead in Generative Retrieval: Guiding Autoregressive Generation through Simultaneous Decoding",
                            "authors": [
                                {
                                    "authorId": "2029235362",
                                    "name": "Hansi Zeng"
                                },
                                {
                                    "authorId": "2294680636",
                                    "name": "Chen Luo"
                                },
                                {
                                    "authorId": "2257027392",
                                    "name": "Hamed Zamani"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                            "n_citations": 16
                        },
                        "score": 0.9921875
                    },
                    {
                        "id": "(Robertson et al., 2009)",
                        "snippets": [
                            "The Probabilistic Relevance Framework (PRF) is a formal framework for document retrieval, grounded in work done in the 1970\u20141980s, which led to the development of one of the most successful text-retrieval algorithms, BM25. In recent years, research in the PRF has yielded new retrieval models capable of taking into account document meta-data (especially structure and link-graph information). Again, this has led to one of the most successful Web-search and corporate-search algorithms, BM25F. This work presents the PRF from a conceptual point of view, describing the probabilistic modelling assumptions behind the framework and the different ranking algorithms that result from its application: the binary independence model, relevance feedback models, BM25 and BM25F. It also discusses the relation between the PRF and other statistical models for IR, and covers some related topics, such as the use of non-textual features, and parameter optimisation for models with free parameters."
                        ],
                        "paper": {
                            "corpus_id": 207178704,
                            "title": "The Probabilistic Relevance Framework: BM25 and Beyond",
                            "authors": [
                                {
                                    "authorId": "144430625",
                                    "name": "S. Robertson"
                                },
                                {
                                    "authorId": "2833561",
                                    "name": "H. Zaragoza"
                                }
                            ],
                            "year": 2009,
                            "venue": "Foundations and Trends in Information Retrieval",
                            "n_citations": 3760
                        },
                        "score": 0
                    },
                    {
                        "id": "(Weller et al., 2023)",
                        "snippets": [
                            "We evaluate a wide variety of models in order to show a comprehensive evaluation across common neural IR model types. We note that although there are other models we do not use (as well as many different strategies for model training), all the major types of retrieval models are accounted for here. We evaluate on the following IR model categories: \n\nSparse We evaluate sparse IR models that use the bag-of-words representation during retrieval. This includes TF-IDF (the only non-neural IR method, here as a baseline), and two variants of SPLADE v2++ (Formal et al., 2022(Formal et al., , 2021;;Lassance and Clinchant, 2022), the ensemble distillation and selfdistillation methods.\n\nLate Interaction Late interaction models like ColBERT (Khattab and Zaharia, 2020;Santhanam et al., 2022b) embed documents and queries into one vector for each sub-word token. At inference time, these models need to compute a MaxSim operation between query vectors and document vectors to determine similarity. We use both ColBERT v1 and v2 in our experiments.\n\ni-Encoders Another common category of IR models are bi-encoders, which embed both documents and queries into a single vector representation. At inference time the similarity is computed via a simple dot product or cosine similarity. Due to the popularity of this category, we include a broad spectrum: models from Sen-tenceTransformer (Reimers and Gurevych, 2019) trained on MSMarco and/or Natural Questions, DPR (Karpukhin et al., 2020), CoCondenser (Gao and Callan, 2022), and RocketQA (Qu et al., 2021;Ren et al., 2021)."
                        ],
                        "paper": {
                            "corpus_id": 258676146,
                            "title": "NevIR: Negation in Neural Information Retrieval",
                            "authors": [
                                {
                                    "authorId": "47433471",
                                    "name": "Orion Weller"
                                },
                                {
                                    "authorId": "2539674",
                                    "name": "Dawn J Lawrie"
                                },
                                {
                                    "authorId": "7536576",
                                    "name": "Benjamin Van Durme"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
                            "n_citations": 20
                        },
                        "score": 0.92138671875
                    },
                    {
                        "id": "(Formal et al., 2021)",
                        "snippets": [
                            "In neural Information Retrieval, ongoing research is directed towards improving the first retriever in ranking pipelines. Learning dense embeddings to conduct retrieval using efficient approximate nearest neighbors methods has proven to work well. Meanwhile, there has been a growing interest in learning sparse representations for documents and queries, that could inherit from the desirable properties of bag-of-words models such as the exact matching of terms and the efficiency of inverted indexes. In this work, we present a new first-stage ranker based on explicit sparsity regularization and a log-saturation effect on term weights, leading to highly sparse representations and competitive results with respect to state-of-the-art dense and sparse methods. Our approach is simple, trained end-to-end in a single stage. We also explore the trade-off between effectiveness and efficiency, by controlling the contribution of the sparsity regularization."
                        ],
                        "paper": {
                            "corpus_id": 235792467,
                            "title": "SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking",
                            "authors": [
                                {
                                    "authorId": "1630412772",
                                    "name": "Thibault Formal"
                                },
                                {
                                    "authorId": "1703777",
                                    "name": "Benjamin Piwowarski"
                                },
                                {
                                    "authorId": "2207074",
                                    "name": "S. Clinchant"
                                }
                            ],
                            "year": 2021,
                            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                            "n_citations": 325
                        },
                        "score": 0
                    },
                    {
                        "id": "(Choi et al., 2022)",
                        "snippets": [
                            "Sparse document representations have been widely used to retrieve relevant documents via exact lexical matching. Owing to the pre-computed inverted index, it supports fast ad-hoc search but incurs the vocabulary mismatch problem. Although recent neural ranking models using pre-trained language models can address this problem, they usually require expensive query inference costs, implying the trade-off between effectiveness and efficiency. Tackling the trade-off, we propose a novel uni-encoder ranking model, Sparse retriever using a Dual document Encoder (SpaDE), learning document representation via the dual encoder. Each encoder plays a central role in (i) adjusting the importance of terms to improve lexical matching and (ii) expanding additional terms to support semantic matching. Furthermore, our co-training strategy trains the dual encoder effectively and avoids unnecessary intervention in training each other. Experimental results on several benchmarks show that SpaDE outperforms existing uni-encoder ranking models."
                        ],
                        "paper": {
                            "corpus_id": 252212320,
                            "title": "SpaDE: Improving Sparse Representations using a Dual Document Encoder for First-stage Retrieval",
                            "authors": [
                                {
                                    "authorId": "115153451",
                                    "name": "Eunseong Choi"
                                },
                                {
                                    "authorId": "2154280101",
                                    "name": "Sunkyung Lee"
                                },
                                {
                                    "authorId": "2111881386",
                                    "name": "Minjin Choi"
                                },
                                {
                                    "authorId": "2184725617",
                                    "name": "Hyeseon Ko"
                                },
                                {
                                    "authorId": "1693485",
                                    "name": "Young-In Song"
                                },
                                {
                                    "authorId": "1865093",
                                    "name": "Jongwuk Lee"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Conference on Information and Knowledge Management",
                            "n_citations": 17
                        },
                        "score": 0
                    },
                    {
                        "id": "(Liu et al._1, 2024)",
                        "snippets": [
                            "Neural Dense Retrieval Models use dense vectors to represent queries and documents. Two widely adopted architectures of dense retrieval models are cross-encoders and bi-encoders."
                        ],
                        "paper": {
                            "corpus_id": 267377589,
                            "title": "An Analysis on Matching Mechanisms and Token Pruning for Late-interaction Models",
                            "authors": [
                                {
                                    "authorId": "2260822142",
                                    "name": "Qi Liu"
                                },
                                {
                                    "authorId": "2163425408",
                                    "name": "Gang Guo"
                                },
                                {
                                    "authorId": "2265811336",
                                    "name": "Jiaxin Mao"
                                },
                                {
                                    "authorId": "2273086037",
                                    "name": "Zhicheng Dou"
                                },
                                {
                                    "authorId": "2260701602",
                                    "name": "Ji-Rong Wen"
                                },
                                {
                                    "authorId": "2279223883",
                                    "name": "Hao Jiang"
                                },
                                {
                                    "authorId": "2282242196",
                                    "name": "Xinyu Zhang"
                                },
                                {
                                    "authorId": "2282520819",
                                    "name": "Zhao Cao"
                                }
                            ],
                            "year": 2024,
                            "venue": "ACM Trans. Inf. Syst.",
                            "n_citations": 7
                        },
                        "score": 0.97900390625
                    },
                    {
                        "id": "(Gomez et al., 2024)",
                        "snippets": [
                            "Dual Encoder (DE) based retrieval systems project queries and documents into the same embedding space and have demonstrated their success in retrieval and bi-text mining."
                        ],
                        "paper": {
                            "corpus_id": 268856885,
                            "title": "Transforming LLMs into Cross-modal and Cross-lingual Retrieval Systems",
                            "authors": [
                                {
                                    "authorId": "2294562390",
                                    "name": "Frank Palma Gomez"
                                },
                                {
                                    "authorId": "2294572013",
                                    "name": "Ramon Sanabria"
                                },
                                {
                                    "authorId": "2294570198",
                                    "name": "Yun-hsuan Sung"
                                },
                                {
                                    "authorId": "2266238595",
                                    "name": "Daniel Cer"
                                },
                                {
                                    "authorId": "35186886",
                                    "name": "Siddharth Dalmia"
                                },
                                {
                                    "authorId": "2124016663",
                                    "name": "Gustavo Hern\u00e1ndez Abrego"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Workshop on Spoken Language Translation",
                            "n_citations": 4
                        },
                        "score": 0.9208984375
                    },
                    {
                        "id": "(Karpukhin et al., 2020)",
                        "snippets": [
                            "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks."
                        ],
                        "paper": {
                            "corpus_id": 215737187,
                            "title": "Dense Passage Retrieval for Open-Domain Question Answering",
                            "authors": [
                                {
                                    "authorId": "2067091563",
                                    "name": "Vladimir Karpukhin"
                                },
                                {
                                    "authorId": "9185192",
                                    "name": "Barlas O\u011fuz"
                                },
                                {
                                    "authorId": "48872685",
                                    "name": "Sewon Min"
                                },
                                {
                                    "authorId": "145222654",
                                    "name": "Patrick Lewis"
                                },
                                {
                                    "authorId": "51183248",
                                    "name": "Ledell Yu Wu"
                                },
                                {
                                    "authorId": "2068070",
                                    "name": "Sergey Edunov"
                                },
                                {
                                    "authorId": "50536468",
                                    "name": "Danqi Chen"
                                },
                                {
                                    "authorId": "144105277",
                                    "name": "Wen-tau Yih"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 3794
                        },
                        "score": 0
                    },
                    {
                        "id": "(Reimers et al., 2019)",
                        "snippets": [
                            "BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods."
                        ],
                        "paper": {
                            "corpus_id": 201646309,
                            "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
                            "authors": [
                                {
                                    "authorId": "2959414",
                                    "name": "Nils Reimers"
                                },
                                {
                                    "authorId": "1730400",
                                    "name": "Iryna Gurevych"
                                }
                            ],
                            "year": 2019,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 12316
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhang et al., 2023)",
                        "snippets": [
                            "In practice, there are two common types of retrievers. One is to leverage the general purpose retrievers, such as sparse models like BM25 [69], and dense models, like DPR [37], contriever [30], E5 [81], BGE [89], OpenAI text embedding [56]. The other option is develop task-specific retriever, e.g., AAR for knowledge enhancement [96], LLM-R [85] for in-context learning. The general purpose methods are praised for their generality and simplicity for usage, but may suffer from an inferior retrieval quality."
                        ],
                        "paper": {
                            "corpus_id": 263835099,
                            "title": "Retrieve Anything To Augment Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2153419738",
                                    "name": "Peitian Zhang"
                                },
                                {
                                    "authorId": "2051175765",
                                    "name": "Shitao Xiao"
                                },
                                {
                                    "authorId": "2240687341",
                                    "name": "Zheng Liu"
                                },
                                {
                                    "authorId": "2257039188",
                                    "name": "Zhicheng Dou"
                                },
                                {
                                    "authorId": "2261086903",
                                    "name": "Jian-Yun Nie"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 63
                        },
                        "score": 0.94189453125
                    },
                    {
                        "id": "(Ros et al., 2025)",
                        "snippets": [
                            "We choose these methods because they are commonly used in the retrieval literature, with bi-encoders often used in the initial retrieval stage [25], and cross-encoders often used in the reranking stage (Reimers et al., 2019)."
                        ],
                        "paper": {
                            "corpus_id": 275337144,
                            "title": "Interactive Information Need Prediction with Intent and Context",
                            "authors": [
                                {
                                    "authorId": "79769007",
                                    "name": "Kevin Ros"
                                },
                                {
                                    "authorId": "2338832917",
                                    "name": "Dhyey Pandya"
                                },
                                {
                                    "authorId": "2253607011",
                                    "name": "ChengXiang Zhai"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.97509765625
                    },
                    {
                        "id": "(Zhuang et al., 2022)",
                        "snippets": [
                            "Pretrained language models such as BERT have been shown to be exceptionally effective for text ranking. However, there are limited studies on how to leverage more powerful sequence-to-sequence models such as T5. Existing attempts usually formulate text ranking as a classification problem and rely on postprocessing to obtain a ranked list. In this paper, we propose RankT5 and study two T5-based ranking model structures, an encoder-decoder and an encoder-only one, so that they not only can directly output ranking scores for each query-document pair, but also can be fine-tuned with pairwise or listwise ranking losses to optimize ranking performance. Our experiments show that the proposed models with ranking losses can achieve substantial ranking performance gains on different public text ranking data sets. Moreover, ranking models fine-tuned with listwise ranking losses have better zero-shot ranking performance on out-of-domain data than models fine-tuned with classification losses."
                        ],
                        "paper": {
                            "corpus_id": 252993059,
                            "title": "RankT5: Fine-Tuning T5 for Text Ranking with Ranking Losses",
                            "authors": [
                                {
                                    "authorId": "39371343",
                                    "name": "Honglei Zhuang"
                                },
                                {
                                    "authorId": "2099586642",
                                    "name": "Zhen Qin"
                                },
                                {
                                    "authorId": "1886219",
                                    "name": "R. Jagerman"
                                },
                                {
                                    "authorId": "47214884",
                                    "name": "Kai Hui"
                                },
                                {
                                    "authorId": "2109919783",
                                    "name": "Ji Ma"
                                },
                                {
                                    "authorId": "2115404379",
                                    "name": "Jing Lu"
                                },
                                {
                                    "authorId": "2148023",
                                    "name": "Jianmo Ni"
                                },
                                {
                                    "authorId": "1526973500",
                                    "name": "Xuanhui Wang"
                                },
                                {
                                    "authorId": "1815447",
                                    "name": "Michael Bendersky"
                                }
                            ],
                            "year": 2022,
                            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                            "n_citations": 140
                        },
                        "score": 0
                    },
                    {
                        "id": "(Khattab et al., 2020)",
                        "snippets": [
                            "Recent progress in Natural Language Understanding (NLU) is driving fast-paced advances in Information Retrieval (IR), largely owed to fine-tuning deep language models (LMs) for document ranking. While remarkably effective, the ranking models based on these LMs increase computational cost by orders of magnitude over prior approaches, particularly as they must feed each query-document pair through a massive neural network to compute a single relevance score. To tackle this, we present ColBERT, a novel ranking model that adapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT introduces a late interaction architecture that independently encodes the query and the document using BERT and then employs a cheap yet powerful interaction step that models their fine-grained similarity. By delaying and yet retaining this fine-granular interaction, ColBERT can leverage the expressiveness of deep LMs while simultaneously gaining the ability to pre-compute document representations offline, considerably speeding up query processing. Crucially, ColBERT's pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from millions of documents. We extensively evaluate ColBERT using two recent passage search datasets. Results show that ColBERT's effectiveness is competitive with existing BERT-based models (and outperforms every non-BERT baseline), while executing two orders-of-magnitude faster and requiring up to four orders-of-magnitude fewer FLOPs per query."
                        ],
                        "paper": {
                            "corpus_id": 216553223,
                            "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT",
                            "authors": [
                                {
                                    "authorId": "144112155",
                                    "name": "O. Khattab"
                                },
                                {
                                    "authorId": "143834867",
                                    "name": "M. Zaharia"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                            "n_citations": 1377
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wang, 2022)",
                        "snippets": [
                            "Common deep retrieval models include: \n\n\u2022 MatchPLM: MatchPLM (Yang et al., 2020) uses a PLM to encode both queries and candidates. A dense layer is used to compute similarities between representations of queries and candidates. \u2022 MatchPyramid: MatchPyramid (Pang et al., 2016) is an interactionfocused model, which utilizes a CNN layer to capture matching patterns on a word-level similarity matrix. \u2022 DRMM: DRMM (Guo et al., 2016) also builds a word-level similarity matrix and uses a histogram mapping function. All histograms are transformed by a feed forward network and selected by a gating network to produce the final representation."
                        ],
                        "paper": {
                            "corpus_id": 252781830,
                            "title": "Legal Element-oriented Modeling with Multi-view Contrastive Learning for Legal Case Retrieval",
                            "authors": [
                                {
                                    "authorId": "2144715839",
                                    "name": "Zhaowei Wang"
                                }
                            ],
                            "year": 2022,
                            "venue": "IEEE International Joint Conference on Neural Network",
                            "n_citations": 12
                        },
                        "score": 0.94677734375
                    },
                    {
                        "id": "(Guo et al., 2016)",
                        "snippets": [
                            "In recent years, deep neural networks have led to exciting breakthroughs in speech recognition, computer vision, and natural language processing (NLP) tasks. However, there have been few positive results of deep models on ad-hoc retrieval tasks. This is partially due to the fact that many important characteristics of the ad-hoc retrieval task have not been well addressed in deep models yet. Typically, the ad-hoc retrieval task is formalized as a matching problem between two pieces of text in existing work using deep models, and treated equivalent to many NLP tasks such as paraphrase identification, question answering and automatic conversation. However, we argue that the ad-hoc retrieval task is mainly about relevance matching while most NLP matching tasks concern semantic matching, and there are some fundamental differences between these two matching tasks. Successful relevance matching requires proper handling of the exact matching signals, query term importance, and diverse matching requirements. In this paper, we propose a novel deep relevance matching model (DRMM) for ad-hoc retrieval. Specifically, our model employs a joint deep architecture at the query term level for relevance matching. By using matching histogram mapping, a feed forward matching network, and a term gating network, we can effectively deal with the three relevance matching factors mentioned above. Experimental results on two representative benchmark collections show that our model can significantly outperform some well-known retrieval models as well as state-of-the-art deep matching models."
                        ],
                        "paper": {
                            "corpus_id": 5688521,
                            "title": "A Deep Relevance Matching Model for Ad-hoc Retrieval",
                            "authors": [
                                {
                                    "authorId": "1777025",
                                    "name": "J. Guo"
                                },
                                {
                                    "authorId": "7888704",
                                    "name": "Yixing Fan"
                                },
                                {
                                    "authorId": "144922928",
                                    "name": "Qingyao Ai"
                                },
                                {
                                    "authorId": "144456145",
                                    "name": "W. Bruce Croft"
                                }
                            ],
                            "year": 2016,
                            "venue": "International Conference on Information and Knowledge Management",
                            "n_citations": 880
                        },
                        "score": 0
                    },
                    {
                        "id": "(Pang et al., 2016)",
                        "snippets": [
                            "Matching two texts is a fundamental problem in many natural language processing tasks. An effective way is to extract meaningful matching patterns from words, phrases, and sentences to produce the matching score. Inspired by the success of convolutional neural network in image recognition, where neurons can capture many complicated patterns based on the extracted elementary visual patterns such as oriented edges and corners, we propose to model text matching as the problem of image recognition. Firstly, a matching matrix whose entries represent the similarities between words is constructed and viewed as an image. Then a convolutional neural network is utilized to capture rich matching patterns in a layer-by-layer way. We show that by resembling the compositional hierarchies of patterns in image recognition, our model can successfully identify salient signals such as n-gram and n-term matchings. Experimental results demonstrate its superiority against the baselines."
                        ],
                        "paper": {
                            "corpus_id": 3993933,
                            "title": "Text Matching as Image Recognition",
                            "authors": [
                                {
                                    "authorId": "48537499",
                                    "name": "Liang Pang"
                                },
                                {
                                    "authorId": "37510256",
                                    "name": "Yanyan Lan"
                                },
                                {
                                    "authorId": "1777025",
                                    "name": "J. Guo"
                                },
                                {
                                    "authorId": "39474114",
                                    "name": "Jun Xu"
                                },
                                {
                                    "authorId": "2019211",
                                    "name": "Shengxian Wan"
                                },
                                {
                                    "authorId": "1717004",
                                    "name": "Xueqi Cheng"
                                }
                            ],
                            "year": 2016,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 558
                        },
                        "score": 0
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Architectural Components of Retrieval Language Models",
                "tldr": "Retrieval Language Models comprise essential components including language modeling frameworks, retrieval mechanisms, and neural architectures that work together to enable effective information retrieval. These components have evolved from traditional probabilistic approaches to transformer-based architectures that support various query-document matching paradigms. (16 sources)",
                "text": "\nRetrieval Language Models (RLMs) integrate several key architectural components that enable effective information retrieval across various applications. At their foundation, many retrieval models utilize language modeling frameworks where documents are represented as probabilistic language models, and relevance is determined by the likelihood that a document would generate the terms of a query <Paper corpusId=\"13892149\" paperTitle=\"(Borg, 2016)\" isShortName></Paper>.\n\n## Language Modeling Components\n\nTraditional language modeling approaches for retrieval often start with maximum likelihood estimation of term probabilities, which are then smoothed to account for unseen terms and adjust for common terms in the corpus <Paper corpusId=\"18819434\" paperTitle=\"(Erkan, 2006)\" isShortName></Paper>. Smoothing techniques, such as Bayesian smoothing with Dirichlet prior, play a crucial role in preventing zero probabilities for unseen terms and providing TF-IDF-like effects <Paper corpusId=\"18819434\" paperTitle=\"(Erkan, 2006)\" isShortName></Paper>.\n\nMore advanced statistical generative models have been developed to better capture linguistic phenomena. For example, the multivariate P\u00f3lya urn model (SPUD) improves upon traditional multinomial language models by accounting for term dependency and word burstiness\u2014the tendency of terms to repeat within documents <Paper corpusId=\"21012910\" paperTitle=\"(Cummins, 2017)\" isShortName></Paper> <Paper corpusId=\"9519540\" paperTitle=\"(Cummins et al., 2015)\" isShortName></Paper>.\n\n## Neural Architectures for Retrieval\n\nModern retrieval models have increasingly incorporated neural architectures, particularly transformer-based pre-trained language models, to better capture query-document relevance <Paper corpusId=\"256389465\" paperTitle=\"(Kim et al., 2023)\" isShortName></Paper>. These architectures can be categorized into several types based on how they process queries and documents:\n\n1. **Cross-Encoders**: These models process query and document pairs together with full cross-attention between all tokens, providing high accuracy but at a computational cost that makes indexing challenging <Paper corpusId=\"257232958\" paperTitle=\"(Scao et al., 2023)\" isShortName></Paper> <Paper corpusId=\"52967399\" paperTitle=\"(Devlin et al., 2019)\" isShortName></Paper>.\n\n2. **Dual/Bi-Encoders**: This architecture independently encodes queries and documents into vector representations, enabling efficient retrieval through vector similarity computations like inner products <Paper corpusId=\"256389465\" paperTitle=\"(Kim et al., 2023)\" isShortName></Paper> <Paper corpusId=\"215737187\" paperTitle=\"(Karpukhin et al., 2020)\" isShortName></Paper>. These models form the foundation for dense retrieval systems that allow pre-computation and indexing of document representations.\n\n3. **Hybrid Approaches**: Some systems combine cross-encoders and bi-encoders to achieve a balance between accuracy and efficiency <Paper corpusId=\"257232958\" paperTitle=\"(Scao et al., 2023)\" isShortName></Paper> <Paper corpusId=\"222177208\" paperTitle=\"(Chen et al., 2020)\" isShortName></Paper>.\n\n## Training and Optimization Components\n\nRetrieval models are typically trained using contrastive learning techniques, where the objective is to minimize the distance between query and relevant document representations while maximizing the distance to irrelevant documents <Paper corpusId=\"267412330\" paperTitle=\"(Kang et al., 2024)\" isShortName></Paper>. The quality of a retrieval model can be measured by the variance of this contrastive loss, with lower variance indicating better retrieval performance <Paper corpusId=\"267412330\" paperTitle=\"(Kang et al., 2024)\" isShortName></Paper>.\n\nRecent innovations include the Differentiable Search Index (DSI) paradigm, which encodes corpus information directly within the parameters of a transformer model <Paper corpusId=\"269921622\" paperTitle=\"(He et al., 2024)\" isShortName></Paper> <Paper corpusId=\"246863488\" paperTitle=\"(Tay et al., 2022)\" isShortName></Paper>. This approach uses \"Learn to Index\" and \"Learn to Retrieve\" training tasks, potentially simplifying the retrieval process by eliminating the need for separate indexing structures <Paper corpusId=\"269921622\" paperTitle=\"(He et al., 2024)\" isShortName></Paper>.\n\n## Model Types and Attention Mechanisms\n\nWhile traditional retrieval models often utilize bidirectional attention mechanisms like those in BERT, newer approaches are exploring decoder-style large language models (LLMs) with unidirectional attention for retrieval tasks <Paper corpusId=\"277621440\" paperTitle=\"(Zhang et al., 2025)\" isShortName></Paper> <Paper corpusId=\"263908865\" paperTitle=\"(Ma et al., 2023)\" isShortName></Paper>. These LLM-based retrievers have shown strong performance and generalizability across various retrieval tasks <Paper corpusId=\"273654180\" paperTitle=\"(Tejaswi et al., 2024)\" isShortName></Paper>.\n\nA significant architectural consideration is the trade-off between expensive full-interaction ranking models and more efficient dual encoder approaches <Paper corpusId=\"259203489\" paperTitle=\"(Jong et al., 2023)\" isShortName></Paper>. Full-interaction models provide more comprehensive query-document matching but at higher computational costs, while dual encoders offer efficiency through pre-computed representations but may sacrifice some relevance precision <Paper corpusId=\"259203489\" paperTitle=\"(Jong et al., 2023)\" isShortName></Paper> <Paper corpusId=\"215737187\" paperTitle=\"(Karpukhin et al., 2020)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Borg, 2016)",
                        "snippets": [
                            "A subset of probabilistic retrieval estimate Language Models (LM) for each document. Documents are then ranked based on the probability that a document would generate the terms of a query (Ponte et al., 1998)."
                        ],
                        "paper": {
                            "corpus_id": 13892149,
                            "title": "Advancing Trace Recovery Evaluation - Applied Information Retrieval in a Software Engineering Context",
                            "authors": [
                                {
                                    "authorId": "145654045",
                                    "name": "Markus Borg"
                                }
                            ],
                            "year": 2016,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.994140625
                    },
                    {
                        "id": "(Erkan, 2006)",
                        "snippets": [
                            "In the language modeling framework, each document in the database defines a language model. The relevance of a document to a given query is ranked according to the generation probability of the query based on the underlying language model of the document. To induce a (unigram) language model from a document, we start with the maximum likelihood (ML) estimation of the term probabilities",
                            "This estimation is often smoothed based on the following general formula",
                            "where ! \" # 7 89 @ A is the ML estimation of over an entire corpus which usually is a member of. \n\n3 is the general smoothing parameter that takes different forms in various smoothing methods. Smoothing has two important roles (Zhai et al., 2004). First, it accounts for terms unseen in the document preventing zero probabilities. This is similar to the smoothing effect in NLP problems such as parsing. Second, smoothing has an \u00a4\u00a5\u00a2like effect that accounts for the generation probabilities of the common terms in the corpus. A common smoothing technique is to use Bayesian smoothing with the Dirichlet prior (Zhai et al., 2004)(Liu et al., 2004)"
                        ],
                        "paper": {
                            "corpus_id": 18819434,
                            "title": "Language Model-Based Document Clustering Using Random Walks",
                            "authors": [
                                {
                                    "authorId": "2158159",
                                    "name": "G\u00fcnes Erkan"
                                }
                            ],
                            "year": 2006,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 44
                        },
                        "score": 0.97265625
                    },
                    {
                        "id": "(Cummins, 2017)",
                        "snippets": [
                            "The general model outlined here (Eq. 1) is an intuitive statistical generative model of documents. The vector u i can be seen as storing the state of the model at a particular time i. Both the multinomial and multivariate P\u00f3lya urn (SPUD (Cummins et al., 2015)) language model are specific instances of this model and are instantiated by different settings of M. Given that the SPUD language model significantly improves upon the multinomial model in information retrieval, the further extensions hold the promise of improved performance and of greater theoretical understanding."
                        ],
                        "paper": {
                            "corpus_id": 21012910,
                            "title": "Modelling Word Burstiness in Natural Language: A Generalised Polya Process for Document Language Models in Information Retrieval",
                            "authors": [
                                {
                                    "authorId": "3161572",
                                    "name": "Ronan Cummins"
                                }
                            ],
                            "year": 2017,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.9755859375
                    },
                    {
                        "id": "(Cummins et al., 2015)",
                        "snippets": [
                            "The multinomial language model has been one of the most effective models of retrieval for more than a decade. However, the multinomial distribution does not model one important linguistic phenomenon relating to term dependency\u2014that is, the tendency of a term to repeat itself within a document (i.e., word burstiness)."
                        ],
                        "paper": {
                            "corpus_id": 9519540,
                            "title": "A P\u00f3lya Urn Document Language Model for Improved Information Retrieval",
                            "authors": [
                                {
                                    "authorId": "3161572",
                                    "name": "Ronan Cummins"
                                },
                                {
                                    "authorId": "2071527",
                                    "name": "Jiaul H. Paik"
                                },
                                {
                                    "authorId": "40282698",
                                    "name": "Yuanhua Lv"
                                }
                            ],
                            "year": 2015,
                            "venue": "ACM Trans. Inf. Syst.",
                            "n_citations": 35
                        },
                        "score": 0.96630859375
                    },
                    {
                        "id": "(Kim et al., 2023)",
                        "snippets": [
                            "Neural models for information retrieval (IR) are increasingly used to model the true ranking function in various applications, including web search [Mitra and Craswell, 2018], recommendation [Zhang et al., 2019], and question-answering (QA) (Chen et al., 2017). Notably, the recent success of Transformers (Vaswani et al., 2017)-based pre-trained language models (Devlin et al., 2019), Liu et al., 2019(Raffel et al., 2019) on a wide range of natural language understanding tasks has also prompted their utilization in IR to capture query-document relevance [see, e.g., Dai and Callan, 2019b(MacAvaney et al., 2019), Nogueira and Cho, 2019, Lee et al., 2019(Karpukhin et al., 2020). A typical IR system comprises two stages: (1) A retriever first selects a small subset of potentially relevant candidate documents (out of a large collection) for a given query; and (2) A re-ranker then identifies a precise ranking among the candidates provided by the retriever. Dual-encoder (DE) models are the de-facto architecture for retrievers [Lee et al., 2019(Karpukhin et al., 2020). Such models independently embed queries and documents into a common space, and capture their relevance by simple operations on these embeddings such as the inner product."
                        ],
                        "paper": {
                            "corpus_id": 256389465,
                            "title": "EmbedDistill: A Geometric Knowledge Distillation for Information Retrieval",
                            "authors": [
                                {
                                    "authorId": "2109548913",
                                    "name": "Seungyeon Kim"
                                },
                                {
                                    "authorId": "2241094",
                                    "name": "A. Rawat"
                                },
                                {
                                    "authorId": "1771307",
                                    "name": "M. Zaheer"
                                },
                                {
                                    "authorId": "3078751",
                                    "name": "Sadeep Jayasumana"
                                },
                                {
                                    "authorId": "3148908",
                                    "name": "Veeranjaneyulu Sadhanala"
                                },
                                {
                                    "authorId": "2141405",
                                    "name": "Wittawat Jitkrittum"
                                },
                                {
                                    "authorId": "2844480",
                                    "name": "A. Menon"
                                },
                                {
                                    "authorId": "2276554",
                                    "name": "R. Fergus"
                                },
                                {
                                    "authorId": "49596260",
                                    "name": "Surinder Kumar"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 7
                        },
                        "score": 0.9638671875
                    },
                    {
                        "id": "(Scao et al., 2023)",
                        "snippets": [
                            "Natural Language Retrieval Models. For natural language, a first class of retrieval models focuses on retrieving sentences that are similar to some input sentence. BERT (Devlin et al., 2019) has been used as a cross-encoder. Two sentences are given with a separator token, cross-attention applies to all input tokens and the resulting representation is fed into a linear layer to score the match. However, this is computationally inefficient as it is not possible to pre-compute and index such representations. A pre-computable model was proposed by (Reimers et al., 2019) who used twin encoders pre-trained on Natural Language Inference data (Bowman et al., 2015) to set new state-of-the-art performance on a large set of sentence scoring tasks. Further work (Chen et al., 2020)Humeau et al., 2019) combined cross-and bi-encoders to reach a tradeoff between accuracy and efficiency."
                        ],
                        "paper": {
                            "corpus_id": 257232958,
                            "title": "Joint Representations of Text and Knowledge Graphs for Retrieval and Evaluation",
                            "authors": [
                                {
                                    "authorId": "1379806208",
                                    "name": "Teven Le Scao"
                                },
                                {
                                    "authorId": "2065132524",
                                    "name": "Claire Gardent"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Joint Conference on Natural Language Processing",
                            "n_citations": 2
                        },
                        "score": 0.9619140625
                    },
                    {
                        "id": "(Devlin et al., 2019)",
                        "snippets": [
                            "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
                        ],
                        "paper": {
                            "corpus_id": 52967399,
                            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                            "authors": [
                                {
                                    "authorId": "39172707",
                                    "name": "Jacob Devlin"
                                },
                                {
                                    "authorId": "1744179",
                                    "name": "Ming-Wei Chang"
                                },
                                {
                                    "authorId": "2544107",
                                    "name": "Kenton Lee"
                                },
                                {
                                    "authorId": "3259253",
                                    "name": "Kristina Toutanova"
                                }
                            ],
                            "year": 2019,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 95215
                        },
                        "score": 0
                    },
                    {
                        "id": "(Karpukhin et al., 2020)",
                        "snippets": [
                            "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks."
                        ],
                        "paper": {
                            "corpus_id": 215737187,
                            "title": "Dense Passage Retrieval for Open-Domain Question Answering",
                            "authors": [
                                {
                                    "authorId": "2067091563",
                                    "name": "Vladimir Karpukhin"
                                },
                                {
                                    "authorId": "9185192",
                                    "name": "Barlas O\u011fuz"
                                },
                                {
                                    "authorId": "48872685",
                                    "name": "Sewon Min"
                                },
                                {
                                    "authorId": "145222654",
                                    "name": "Patrick Lewis"
                                },
                                {
                                    "authorId": "51183248",
                                    "name": "Ledell Yu Wu"
                                },
                                {
                                    "authorId": "2068070",
                                    "name": "Sergey Edunov"
                                },
                                {
                                    "authorId": "50536468",
                                    "name": "Danqi Chen"
                                },
                                {
                                    "authorId": "144105277",
                                    "name": "Wen-tau Yih"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 3794
                        },
                        "score": 0
                    },
                    {
                        "id": "(Chen et al., 2020)",
                        "snippets": [
                            "Pre-trained models like BERT ((Devlin et al., 2018) have dominated NLP / IR applications such as single sentence classification, text pair classification, and question answering. However, deploying these models in real systems is highly non-trivial due to their exorbitant computational costs. A common remedy to this is knowledge distillation (Hinton et al., 2015), leading to faster inference. However \u2013 as we show here \u2013 existing works are not optimized for dealing with pairs (or tuples) of texts. Consequently, they are either not scalable or demonstrate subpar performance. In this work, we propose DiPair \u2014 a novel framework for distilling fast and accurate models on text pair tasks. Coupled with an end-to-end training strategy, DiPair is both highly scalable and offers improved quality-speed tradeoffs. Empirical studies conducted on both academic and real-world e-commerce benchmarks demonstrate the efficacy of the proposed approach with speedups of over 350x and minimal quality drop relative to the cross-attention teacher BERT model."
                        ],
                        "paper": {
                            "corpus_id": 222177208,
                            "title": "DiPair: Fast and Accurate Distillation for Trillion-ScaleText Matching and Pair Modeling",
                            "authors": [
                                {
                                    "authorId": "2809410",
                                    "name": "Jiecao Chen"
                                },
                                {
                                    "authorId": "1988888746",
                                    "name": "Liu Yang"
                                },
                                {
                                    "authorId": "2062947723",
                                    "name": "K. Raman"
                                },
                                {
                                    "authorId": "1815447",
                                    "name": "Michael Bendersky"
                                },
                                {
                                    "authorId": "34727699",
                                    "name": "Jung-Jung Yeh"
                                },
                                {
                                    "authorId": "2118116642",
                                    "name": "Yun Zhou"
                                },
                                {
                                    "authorId": "1763978",
                                    "name": "Marc Najork"
                                },
                                {
                                    "authorId": "1833259140",
                                    "name": "Danyang Cai"
                                },
                                {
                                    "authorId": "2465392",
                                    "name": "Ehsan Emadzadeh"
                                }
                            ],
                            "year": 2020,
                            "venue": "Findings",
                            "n_citations": 29
                        },
                        "score": 0
                    },
                    {
                        "id": "(Kang et al., 2024)",
                        "snippets": [
                            "To quantify the quality of retrieval models, we introduce the concept of V rag -retrieval model, where V rag measures the variance of the contrastive loss of the retrieval model. A small V rag implies a well-trained low-variance retrieval model and can be theoretically linked to the retrieval quality, which is measured by the number of retrieved positive examples with respect to the query text. \n\nDefinition 1 (V rag -retrieval model). Consider a retrieval model with similarity measurement s \u03b8r (\u2022, \u2022) parameterized with \u03b8 r and trained with contrastive loss L cont . Let x + , x \u2212 be positive and negative samples to sample x. Consider common contrastive loss L cont = \u2212 log (\u03c3 sig (exp{s \u03b8 (x, x \u2212 ) \u2212 exp{s \u03b8 (x, x + ))), where \u03c3 sig (\u2022) is the sigmoid function."
                        ],
                        "paper": {
                            "corpus_id": 267412330,
                            "title": "C-RAG: Certified Generation Risks for Retrieval-Augmented Language Models",
                            "authors": [
                                {
                                    "authorId": "2153110066",
                                    "name": "Mintong Kang"
                                },
                                {
                                    "authorId": "51274710",
                                    "name": "Nezihe Merve Gurel"
                                },
                                {
                                    "authorId": "2282538184",
                                    "name": "Ning Yu"
                                },
                                {
                                    "authorId": "2242706269",
                                    "name": "D. Song"
                                },
                                {
                                    "authorId": "2267398406",
                                    "name": "Bo Li"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 22
                        },
                        "score": 0.9248046875
                    },
                    {
                        "id": "(He et al., 2024)",
                        "snippets": [
                            "The transformer model has shown proficiency in retrieval tasks, encoding item information within its parameters, which is a method termed Differentiable Search Index (DSI) (Tay et al., 2022).DSI involves two key training tasks for pre-trained language models: Learn to Index (L2I) and Learn to Retrieve (L2R), which can be used to train a model jointly or in a sequential order",
                            "Compared to common two-tower models, DSI models require only a single model for item recommendations, by indexing item information into its parameters (Tay et al., 2022)."
                        ],
                        "paper": {
                            "corpus_id": 269921622,
                            "title": "Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation",
                            "authors": [
                                {
                                    "authorId": "51002202",
                                    "name": "Zhankui He"
                                },
                                {
                                    "authorId": "2153722757",
                                    "name": "Zhouhang Xie"
                                },
                                {
                                    "authorId": "2290487114",
                                    "name": "Harald Steck"
                                },
                                {
                                    "authorId": "2261449339",
                                    "name": "Dawen Liang"
                                },
                                {
                                    "authorId": "2302321905",
                                    "name": "Rahul Jha"
                                },
                                {
                                    "authorId": "3174388",
                                    "name": "Nathan Kallus"
                                },
                                {
                                    "authorId": "2258552056",
                                    "name": "Julian McAuley"
                                }
                            ],
                            "year": 2024,
                            "venue": "Web Search and Data Mining",
                            "n_citations": 7
                        },
                        "score": 0.92626953125
                    },
                    {
                        "id": "(Tay et al., 2022)",
                        "snippets": [
                            "In this paper, we demonstrate that information retrieval can be accomplished with a single Transformer, in which all information about the corpus is encoded in the parameters of the model. To this end, we introduce the Differentiable Search Index (DSI), a new paradigm that learns a text-to-text model that maps string queries directly to relevant docids; in other words, a DSI model answers queries directly using only its parameters, dramatically simplifying the whole retrieval process. We study variations in how documents and their identifiers are represented, variations in training procedures, and the interplay between models and corpus sizes. Experiments demonstrate that given appropriate design choices, DSI significantly outperforms strong baselines such as dual encoder models. Moreover, DSI demonstrates strong generalization capabilities, outperforming a BM25 baseline in a zero-shot setup."
                        ],
                        "paper": {
                            "corpus_id": 246863488,
                            "title": "Transformer Memory as a Differentiable Search Index",
                            "authors": [
                                {
                                    "authorId": "144447820",
                                    "name": "Yi Tay"
                                },
                                {
                                    "authorId": "2057663102",
                                    "name": "Vinh Q. Tran"
                                },
                                {
                                    "authorId": "3226635",
                                    "name": "Mostafa Dehghani"
                                },
                                {
                                    "authorId": "2148023",
                                    "name": "Jianmo Ni"
                                },
                                {
                                    "authorId": "2119725651",
                                    "name": "Dara Bahri"
                                },
                                {
                                    "authorId": "18138802",
                                    "name": "Harsh Mehta"
                                },
                                {
                                    "authorId": "145144957",
                                    "name": "Zhen Qin"
                                },
                                {
                                    "authorId": "47214884",
                                    "name": "Kai Hui"
                                },
                                {
                                    "authorId": "48634137",
                                    "name": "Zhe Zhao"
                                },
                                {
                                    "authorId": "143702064",
                                    "name": "Jai Gupta"
                                },
                                {
                                    "authorId": "32303439",
                                    "name": "Tal Schuster"
                                },
                                {
                                    "authorId": "50056360",
                                    "name": "William W. Cohen"
                                },
                                {
                                    "authorId": "1680617",
                                    "name": "Donald Metzler"
                                }
                            ],
                            "year": 2022,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 286
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhang et al., 2025)",
                        "snippets": [
                            "Large language models (LLMs) are being widely applied across various fields [27,54,61], and garnering increasing attention for their application in retrieval tasks [25,(Ma et al., 2023)[51]. Unlike bidirectional attention mechanisms in encoder-style pre-trained language models (PLMs) such as BERT, LLMs are typically decoder-style models that employ unidirectional attention."
                        ],
                        "paper": {
                            "corpus_id": 277621440,
                            "title": "Unleashing the Power of LLMs in Dense Retrieval with Query Likelihood Modeling",
                            "authors": [
                                {
                                    "authorId": "2260353683",
                                    "name": "Hengran Zhang"
                                },
                                {
                                    "authorId": "2249759496",
                                    "name": "Keping Bi"
                                },
                                {
                                    "authorId": "2316783018",
                                    "name": "Jiafeng Guo"
                                },
                                {
                                    "authorId": "2269761948",
                                    "name": "Xiaojie Sun"
                                },
                                {
                                    "authorId": "2310512864",
                                    "name": "Shihao Liu"
                                },
                                {
                                    "authorId": "2104450297",
                                    "name": "Daiting Shi"
                                },
                                {
                                    "authorId": "2310342915",
                                    "name": "Dawei Yin"
                                },
                                {
                                    "authorId": "2244825947",
                                    "name": "Xueqi Cheng"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.98095703125
                    },
                    {
                        "id": "(Ma et al., 2023)",
                        "snippets": [
                            "While large language models (LLMs) have shown impressive NLP capabilities, existing IR applications mainly focus on prompting LLMs to generate query expansions or generating permutations for listwise reranking. In this study, we leverage LLMs directly to serve as components in the widely used multi-stage text ranking pipeline. Specifically, we fine-tune the open-source LLaMA-2 model as a dense retriever (repLLaMA) and a pointwise reranker (rankLLaMA). This is performed for both passage and document retrieval tasks using the MS MARCO training data. Our study shows that finetuned LLM retrieval models outperform smaller models. They are more effective and exhibit greater generalizability, requiring only a straightforward training strategy. Moreover, our pipeline allows for the fine-tuning of LLMs at each stage of a multi-stage retrieval pipeline. This demonstrates the strong potential for optimizing LLMs to enhance a variety of retrieval tasks. Furthermore, as LLMs are naturally pre-trained with longer contexts, they can directly represent longer documents. This eliminates the need for heuristic segmenting and pooling strategies to rank long documents. On the MS MARCO and BEIR datasets, our repLLaMA-rankLLaMA pipeline demonstrates a high level of effectiveness."
                        ],
                        "paper": {
                            "corpus_id": 263908865,
                            "title": "Fine-Tuning LLaMA for Multi-Stage Text Retrieval",
                            "authors": [
                                {
                                    "authorId": "2461713",
                                    "name": "Xueguang Ma"
                                },
                                {
                                    "authorId": "145769448",
                                    "name": "Liang Wang"
                                },
                                {
                                    "authorId": "2242947624",
                                    "name": "Nan Yang"
                                },
                                {
                                    "authorId": "2257346447",
                                    "name": "Furu Wei"
                                },
                                {
                                    "authorId": "2257085301",
                                    "name": "Jimmy Lin"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                            "n_citations": 223
                        },
                        "score": 0
                    },
                    {
                        "id": "(Tejaswi et al., 2024)",
                        "snippets": [
                            "Retrieval Large language models pre-trained with autoregressive setups (Jiang et al., 2023;Dubey et al., 2024) have shown remarkable performance when adapted to retrieval tasks (Wang et al., 2024b;BehnamGhader et al., 2024), outperforming encoder-style retrievers (Izacard et al., 2021)Wang et al., 2024a)."
                        ],
                        "paper": {
                            "corpus_id": 273654180,
                            "title": "RARe: Retrieval Augmented Retrieval with In-Context Examples",
                            "authors": [
                                {
                                    "authorId": "2303846034",
                                    "name": "Atula Tejaswi"
                                },
                                {
                                    "authorId": "2269690359",
                                    "name": "Yoonsang Lee"
                                },
                                {
                                    "authorId": "2303847572",
                                    "name": "Sujay Sanghavi"
                                },
                                {
                                    "authorId": "2304136439",
                                    "name": "Eunsol Choi"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.9208984375
                    },
                    {
                        "id": "(Jong et al., 2023)",
                        "snippets": [
                            "retrieval procedures face a trade-off between expensive ranking with full interaction (Chen et al., 2020) and the more common dual encoder approaches such as DPR (Karpukhin et al., 2020) and GTR (Ni et al., 2021) that scores based on inner product similarity with a corpus of pre-computed passage representations."
                        ],
                        "paper": {
                            "corpus_id": 259203489,
                            "title": "GLIMMER: generalized late-interaction memory reranker",
                            "authors": [
                                {
                                    "authorId": "21379393",
                                    "name": "Michiel de Jong"
                                },
                                {
                                    "authorId": "2220287479",
                                    "name": "Yury Zemlyanskiy"
                                },
                                {
                                    "authorId": "143883142",
                                    "name": "Nicholas FitzGerald"
                                },
                                {
                                    "authorId": "144074891",
                                    "name": "Sumit K. Sanghai"
                                },
                                {
                                    "authorId": "50056360",
                                    "name": "William W. Cohen"
                                },
                                {
                                    "authorId": "1643737606",
                                    "name": "J. Ainslie"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.93701171875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Applications and Performance",
                "tldr": "Retrieval Language Models have demonstrated strong performance across various NLP applications including question answering, language generation, semantic textual similarity, and information retrieval. Their effectiveness stems from combining transformer-based architectures with retrieval mechanisms that enhance factual accuracy and domain adaptation capabilities. (16 sources)",
                "text": "\nRetrieval Language Models (RLMs) have been successfully applied to a wide range of natural language processing tasks, showcasing their versatility and effectiveness in different application domains.\n\n## Question Answering and Knowledge-Intensive Tasks\n\nRetrieval-augmented approaches have achieved particularly impressive results on open-domain question answering tasks. Models like REALM have demonstrated how unsupervised pre-training of knowledge retrievers, using masked language modeling as the learning signal, can significantly improve performance on challenging question answering benchmarks <Paper corpusId=\"211204736\" paperTitle=\"(Guu et al., 2020)\" isShortName></Paper>. Similarly, Retrieval-Augmented Generation (RAG) models have set state-of-the-art results on multiple open-domain QA tasks by combining pre-trained parametric and non-parametric memory, outperforming both traditional parametric sequence-to-sequence models and task-specific retrieve-and-extract architectures <Paper corpusId=\"218869575\" paperTitle=\"(Lewis et al., 2020)\" isShortName></Paper>.\n\n## Language Modeling and Text Generation\n\nIn language modeling, kNN-LM has established new state-of-the-art performance by extending pre-trained neural language models with k-nearest neighbors interpolation <Paper corpusId=\"207870430\" paperTitle=\"(Khandelwal et al., 2019)\" isShortName></Paper>. This approach has been particularly effective for predicting rare patterns and factual knowledge, suggesting that similarity-based retrieval provides significant advantages for language modeling in the long tail of data distributions.\n\nTransformer-based architectures form the foundation of most modern RLMs, with different variants optimized for specific tasks. BERT-based masked language modeling exploits bidirectional information for effective sequence encoding, while causal language models like GPT-2 are unidirectional and better suited for text generation <Paper corpusId=\"253384615\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper> <Paper corpusId=\"52967399\" paperTitle=\"(Devlin et al., 2019)\" isShortName></Paper> <Paper corpusId=\"198953378\" paperTitle=\"(Liu et al., 2019)\" isShortName></Paper>. Some innovative approaches attempt to combine the strengths of both paradigms through techniques such as augmenting suffix embeddings during sequence generation <Paper corpusId=\"253384615\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper>.\n\n## Semantic Understanding and Similarity Tasks\n\nFor semantic understanding tasks, retrieval models frequently utilize specialized pre-trained encoders. SimCSE has emerged as an effective model for producing semantically meaningful sentence embeddings through contrastive learning approaches <Paper corpusId=\"233296292\" paperTitle=\"(Gao et al., 2021)\" isShortName></Paper> <Paper corpusId=\"211096730\" paperTitle=\"(Chen et al._1, 2020)\" isShortName></Paper> <Paper corpusId=\"207930212\" paperTitle=\"(He et al., 2019)\" isShortName></Paper>. These embeddings enable high-quality retrieval based on semantic similarity rather than just lexical matching, which is crucial for tasks like semantic textual similarity and information retrieval.\n\n## Multimodal Applications\n\nRetrieval models have also been extended to multimodal domains, particularly for tasks involving images and text. Several approaches associate visual and language modalities by embedding their representations into a common space, enabling bidirectional retrieval between images and sentences <Paper corpusId=\"8486003\" paperTitle=\"(Tan et al., 2016)\" isShortName></Paper>. These methods typically use CNNs to extract image features and language models to represent sentences, embedding both into a shared multimodal space and optimizing the model with ranking objectives <Paper corpusId=\"2315434\" paperTitle=\"(Karpathy et al., 2014)\" isShortName></Paper> <Paper corpusId=\"2317858\" paperTitle=\"(Socher et al., 2014)\" isShortName></Paper> <Paper corpusId=\"928608\" paperTitle=\"(Hodosh et al., 2013)\" isShortName></Paper>.\n\n## Domain Adaptation and Specialization\n\nOne of the key advantages of retrieval-augmented models is their ability to adapt to specific domains without extensive retraining. By simply varying the nearest neighbor datastore, kNN-LM has demonstrated effective domain adaptation capabilities without requiring further training <Paper corpusId=\"207870430\" paperTitle=\"(Khandelwal et al., 2019)\" isShortName></Paper>. This flexibility makes RLMs particularly valuable for specialized applications where domain-specific knowledge is crucial.\n\nFor specialized domains like recommendation systems, dedicated pre-trained language models such as BLaIR have been developed <Paper corpusId=\"271891894\" paperTitle=\"(Ren et al., 2024)\" isShortName></Paper>, while general-purpose language models like BERT, RoBERTa, and BART continue to serve as strong foundations for retrieval tasks across domains.\n\n## Document Retrieval and Factual Answers\n\nDocument retrieval approaches have shown that large language models can provide more accurate factual answers when augmented with retrieved content. These systems retrieve relevant documents based on semantic relevance between user queries and documents, then concatenate these documents to the prompt input, enabling LLMs to better answer factual questions <Paper corpusId=\"269605025\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>. This approach has become increasingly important as language models scale up, with larger models demonstrating emergent abilities that cannot be predicted by extrapolating from smaller models <Paper corpusId=\"249674500\" paperTitle=\"(Wei et al., 2022)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Guu et al., 2020)",
                        "snippets": [
                            "Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. \nTo capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. \nWe demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity."
                        ],
                        "paper": {
                            "corpus_id": 211204736,
                            "title": "REALM: Retrieval-Augmented Language Model Pre-Training",
                            "authors": [
                                {
                                    "authorId": "2091768",
                                    "name": "Kelvin Guu"
                                },
                                {
                                    "authorId": "2544107",
                                    "name": "Kenton Lee"
                                },
                                {
                                    "authorId": "9941702",
                                    "name": "Zora Tung"
                                },
                                {
                                    "authorId": "2616463",
                                    "name": "Panupong Pasupat"
                                },
                                {
                                    "authorId": "1744179",
                                    "name": "Ming-Wei Chang"
                                }
                            ],
                            "year": 2020,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 2119
                        },
                        "score": 0
                    },
                    {
                        "id": "(Lewis et al., 2020)",
                        "snippets": [
                            "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline."
                        ],
                        "paper": {
                            "corpus_id": 218869575,
                            "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
                            "authors": [
                                {
                                    "authorId": "145222654",
                                    "name": "Patrick Lewis"
                                },
                                {
                                    "authorId": "3439053",
                                    "name": "Ethan Perez"
                                },
                                {
                                    "authorId": "1716179427",
                                    "name": "Aleksandara Piktus"
                                },
                                {
                                    "authorId": "40052301",
                                    "name": "F. Petroni"
                                },
                                {
                                    "authorId": "2067091563",
                                    "name": "Vladimir Karpukhin"
                                },
                                {
                                    "authorId": "39589154",
                                    "name": "Naman Goyal"
                                },
                                {
                                    "authorId": "103131985",
                                    "name": "Heinrich Kuttler"
                                },
                                {
                                    "authorId": "35084211",
                                    "name": "M. Lewis"
                                },
                                {
                                    "authorId": "144105277",
                                    "name": "Wen-tau Yih"
                                },
                                {
                                    "authorId": "2620211",
                                    "name": "Tim Rockt\u00e4schel"
                                },
                                {
                                    "authorId": "48662861",
                                    "name": "Sebastian Riedel"
                                },
                                {
                                    "authorId": "1743722",
                                    "name": "Douwe Kiela"
                                }
                            ],
                            "year": 2020,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 6476
                        },
                        "score": 0
                    },
                    {
                        "id": "(Khandelwal et al., 2019)",
                        "snippets": [
                            "We introduce $k$NN-LMs, which extend a pre-trained neural language model (LM) by linearly interpolating it with a $k$-nearest neighbors ($k$NN) model. The nearest neighbors are computed according to distance in the pre-trained LM embedding space, and can be drawn from any text collection, including the original LM training data. Applying this augmentation to a strong Wikitext-103 LM, with neighbors drawn from the original training set, our $k$NN-LM achieves a new state-of-the-art perplexity of 15.79 - a 2.9 point improvement with no additional training. We also show that this approach has implications for efficiently scaling up to larger training sets and allows for effective domain adaptation, by simply varying the nearest neighbor datastore, again without further training. Qualitatively, the model is particularly helpful in predicting rare patterns, such as factual knowledge. Together, these results strongly suggest that learning similarity between sequences of text is easier than predicting the next word, and that nearest neighbor search is an effective approach for language modeling in the long tail."
                        ],
                        "paper": {
                            "corpus_id": 207870430,
                            "title": "Generalization through Memorization: Nearest Neighbor Language Models",
                            "authors": [
                                {
                                    "authorId": "3030219",
                                    "name": "Urvashi Khandelwal"
                                },
                                {
                                    "authorId": "39455775",
                                    "name": "Omer Levy"
                                },
                                {
                                    "authorId": "1746807",
                                    "name": "Dan Jurafsky"
                                },
                                {
                                    "authorId": "1982950",
                                    "name": "Luke Zettlemoyer"
                                },
                                {
                                    "authorId": "35084211",
                                    "name": "M. Lewis"
                                }
                            ],
                            "year": 2019,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 842
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wang et al., 2022)",
                        "snippets": [
                            "Most recent development in language modeling is based on transformers (Vaswani et al., 2017). BERT-based Masked language modeling (Devlin et al., 2019)(Liu et al., 2019) exploits bi-directional information of a sentence to predict the word identity of the masked tokens. While BERT is effective in encoding sequences, it is not suitable for sequence generation due to its non-causal nature. Causal language modeling such as GPT2 [11] is uni-directional. Our proposed model attempts to retain the best of the two worlds as autoregressive and simulated bi-directional via augmentation of suffix embeddings during sequence generation."
                        ],
                        "paper": {
                            "corpus_id": 253384615,
                            "title": "Suffix Retrieval-Augmented Language Modeling",
                            "authors": [
                                {
                                    "authorId": "70452651",
                                    "name": "Zecheng Wang"
                                },
                                {
                                    "authorId": "1789138",
                                    "name": "Yik-Cheung Tam"
                                }
                            ],
                            "year": 2022,
                            "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
                            "n_citations": 1
                        },
                        "score": 0.95947265625
                    },
                    {
                        "id": "(Devlin et al., 2019)",
                        "snippets": [
                            "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
                        ],
                        "paper": {
                            "corpus_id": 52967399,
                            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                            "authors": [
                                {
                                    "authorId": "39172707",
                                    "name": "Jacob Devlin"
                                },
                                {
                                    "authorId": "1744179",
                                    "name": "Ming-Wei Chang"
                                },
                                {
                                    "authorId": "2544107",
                                    "name": "Kenton Lee"
                                },
                                {
                                    "authorId": "3259253",
                                    "name": "Kristina Toutanova"
                                }
                            ],
                            "year": 2019,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 95215
                        },
                        "score": 0
                    },
                    {
                        "id": "(Liu et al., 2019)",
                        "snippets": [
                            "Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code."
                        ],
                        "paper": {
                            "corpus_id": 198953378,
                            "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
                            "authors": [
                                {
                                    "authorId": "11323179",
                                    "name": "Yinhan Liu"
                                },
                                {
                                    "authorId": "40511414",
                                    "name": "Myle Ott"
                                },
                                {
                                    "authorId": "39589154",
                                    "name": "Naman Goyal"
                                },
                                {
                                    "authorId": "3048577",
                                    "name": "Jingfei Du"
                                },
                                {
                                    "authorId": "144863691",
                                    "name": "Mandar Joshi"
                                },
                                {
                                    "authorId": "50536468",
                                    "name": "Danqi Chen"
                                },
                                {
                                    "authorId": "39455775",
                                    "name": "Omer Levy"
                                },
                                {
                                    "authorId": "35084211",
                                    "name": "M. Lewis"
                                },
                                {
                                    "authorId": "1982950",
                                    "name": "Luke Zettlemoyer"
                                },
                                {
                                    "authorId": "1759422",
                                    "name": "Veselin Stoyanov"
                                }
                            ],
                            "year": 2019,
                            "venue": "arXiv.org",
                            "n_citations": 24556
                        },
                        "score": 0
                    },
                    {
                        "id": "(Gao et al., 2021)",
                        "snippets": [
                            "This paper presents SimCSE, a simple contrastive learning framework that greatly advances the state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We find that dropout acts as minimal data augmentation and removing it leads to a representation collapse. Then, we propose a supervised approach, which incorporates annotated pairs from natural language inference datasets into our contrastive learning framework, by using \"entailment\" pairs as positives and \"contradiction\" pairs as hard negatives. We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERT base achieve an average of 76.3% and 81.6% Spearman\u2019s correlation respectively, a 4.2% and 2.2% improvement compared to previous best results. We also show\u2014both theoretically and empirically\u2014that contrastive learning objective regularizes pre-trained embeddings\u2019 anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are available."
                        ],
                        "paper": {
                            "corpus_id": 233296292,
                            "title": "SimCSE: Simple Contrastive Learning of Sentence Embeddings",
                            "authors": [
                                {
                                    "authorId": "4800645",
                                    "name": "Tianyu Gao"
                                },
                                {
                                    "authorId": "2087141625",
                                    "name": "Xingcheng Yao"
                                },
                                {
                                    "authorId": "50536468",
                                    "name": "Danqi Chen"
                                }
                            ],
                            "year": 2021,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 3413
                        },
                        "score": 0
                    },
                    {
                        "id": "(Chen et al._1, 2020)",
                        "snippets": [
                            "This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels."
                        ],
                        "paper": {
                            "corpus_id": 211096730,
                            "title": "A Simple Framework for Contrastive Learning of Visual Representations",
                            "authors": [
                                {
                                    "authorId": "145358498",
                                    "name": "Ting Chen"
                                },
                                {
                                    "authorId": "40464924",
                                    "name": "Simon Kornblith"
                                },
                                {
                                    "authorId": "144739074",
                                    "name": "Mohammad Norouzi"
                                },
                                {
                                    "authorId": "1695689",
                                    "name": "Geoffrey E. Hinton"
                                }
                            ],
                            "year": 2020,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 18878
                        },
                        "score": 0
                    },
                    {
                        "id": "(He et al., 2019)",
                        "snippets": [
                            "We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks."
                        ],
                        "paper": {
                            "corpus_id": 207930212,
                            "title": "Momentum Contrast for Unsupervised Visual Representation Learning",
                            "authors": [
                                {
                                    "authorId": "39353098",
                                    "name": "Kaiming He"
                                },
                                {
                                    "authorId": "146884473",
                                    "name": "Haoqi Fan"
                                },
                                {
                                    "authorId": "98264506",
                                    "name": "Yuxin Wu"
                                },
                                {
                                    "authorId": "1817030",
                                    "name": "Saining Xie"
                                },
                                {
                                    "authorId": "2983898",
                                    "name": "Ross B. Girshick"
                                }
                            ],
                            "year": 2019,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 12131
                        },
                        "score": 0
                    },
                    {
                        "id": "(Tan et al., 2016)",
                        "snippets": [
                            "To model the relationship between image and language, some works associate both modalities by embedding their representations into a common space (Hodosh et al., 2013)(Frome et al., 2013)(Socher et al., 2014)(Karpathy et al., 2014). First, they obtain the image features using a visual model like CNN (Frome et al., 2013)(Socher et al., 2014), as well as the representation of sentence with a language model such as recursive neural network (Socher et al., 2014). Then, both of them are embedded into a common multimodal space and the whole model is learned with ranking objective for image and sentence retrieval task. This framework was also tested at object level by Karpathy et al. (Karpathy et al., 2014) and proved to yield better results for the image and sentence bi-directional retrieval task."
                        ],
                        "paper": {
                            "corpus_id": 8486003,
                            "title": "phi-LSTM: A Phrase-Based Hierarchical LSTM Model for Image Captioning",
                            "authors": [
                                {
                                    "authorId": "143692915",
                                    "name": "Y. Tan"
                                },
                                {
                                    "authorId": "2863960",
                                    "name": "Chee Seng Chan"
                                }
                            ],
                            "year": 2016,
                            "venue": "Asian Conference on Computer Vision",
                            "n_citations": 29
                        },
                        "score": 0.93896484375
                    },
                    {
                        "id": "(Karpathy et al., 2014)",
                        "snippets": [
                            "We introduce a model for bidirectional retrieval of images and sentences through a deep, multi-modal embedding of visual and natural language data. Unlike previous models that directly map images or sentences into a common embedding space, our model works on a finer level and embeds fragments of images (objects) and fragments of sentences (typed dependency tree relations) into a common space. We then introduce a structured max-margin objective that allows our model to explicitly associate these fragments across modalities. Extensive experimental evaluation shows that reasoning on both the global level of images and sentences and the finer level of their respective fragments improves performance on image-sentence retrieval tasks. Additionally, our model provides interpretable predictions for the image-sentence retrieval task since the inferred inter-modal alignment of fragments is explicit."
                        ],
                        "paper": {
                            "corpus_id": 2315434,
                            "title": "Deep Fragment Embeddings for Bidirectional Image Sentence Mapping",
                            "authors": [
                                {
                                    "authorId": "2354728",
                                    "name": "A. Karpathy"
                                },
                                {
                                    "authorId": "2319608",
                                    "name": "Armand Joulin"
                                },
                                {
                                    "authorId": "48004138",
                                    "name": "Li Fei-Fei"
                                }
                            ],
                            "year": 2014,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 937
                        },
                        "score": 0
                    },
                    {
                        "id": "(Socher et al., 2014)",
                        "snippets": [
                            "Previous work on Recursive Neural Networks (RNNs) shows that these models can produce compositional feature vectors for accurately representing and classifying sentences or images. However, the sentence vectors of previous models cannot accurately represent visually grounded meaning. We introduce the DT-RNN model which uses dependency trees to embed sentences into a vector space in order to retrieve images that are described by those sentences. Unlike previous RNN-based models which use constituency trees, DT-RNNs naturally focus on the action and agents in a sentence. They are better able to abstract from the details of word order and syntactic expression. DT-RNNs outperform other recursive and recurrent neural networks, kernelized CCA and a bag-of-words baseline on the tasks of finding an image that fits a sentence description and vice versa. They also give more similar representations to sentences that describe the same image."
                        ],
                        "paper": {
                            "corpus_id": 2317858,
                            "title": "Grounded Compositional Semantics for Finding and Describing Images with Sentences",
                            "authors": [
                                {
                                    "authorId": "2166511",
                                    "name": "R. Socher"
                                },
                                {
                                    "authorId": "2354728",
                                    "name": "A. Karpathy"
                                },
                                {
                                    "authorId": "2827616",
                                    "name": "Quoc V. Le"
                                },
                                {
                                    "authorId": "144783904",
                                    "name": "Christopher D. Manning"
                                },
                                {
                                    "authorId": "34699434",
                                    "name": "A. Ng"
                                }
                            ],
                            "year": 2014,
                            "venue": "Transactions of the Association for Computational Linguistics",
                            "n_citations": 897
                        },
                        "score": 0
                    },
                    {
                        "id": "(Hodosh et al., 2013)",
                        "snippets": [
                            "The ability to associate images with natural language sentences that describe what is depicted in them is a hallmark of image understanding, and a prerequisite for applications such as sentence-based image search. In analogy to image search, we propose to frame sentence-based image annotation as the task of ranking a given pool of captions. We introduce a new benchmark collection for sentence-based image description and search, consisting of 8,000 images that are each paired with five different captions which provide clear descriptions of the salient entities and events. We introduce a number of systems that perform quite well on this task, even though they are only based on features that can be obtained with minimal supervision. Our results clearly indicate the importance of training on multiple captions per image, and of capturing syntactic (word order-based) and semantic features of these captions. We also perform an in-depth comparison of human and automatic evaluation metrics for this task, and propose strategies for collecting human judgments cheaply and on a very large scale, allowing us to augment our collection with additional relevance judgments of which captions describe which image. Our analysis shows that metrics that consider the ranked list of results for each query image or sentence are significantly more robust than metrics that are based on a single response per query. Moreover, our study suggests that the evaluation of ranking-based image description systems may be fully automated."
                        ],
                        "paper": {
                            "corpus_id": 928608,
                            "title": "Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics",
                            "authors": [
                                {
                                    "authorId": "2170746",
                                    "name": "Micah Hodosh"
                                },
                                {
                                    "authorId": "2052690705",
                                    "name": "Peter Young"
                                },
                                {
                                    "authorId": "3118681",
                                    "name": "J. Hockenmaier"
                                }
                            ],
                            "year": 2013,
                            "venue": "Journal of Artificial Intelligence Research",
                            "n_citations": 1325
                        },
                        "score": 0
                    },
                    {
                        "id": "(Ren et al., 2024)",
                        "snippets": [
                            "General Language Models: BERT [4], RoBERTa [21], and BART [14]; (ii) Language Models for Dense Retrieval: SimCSE [7], GTR [23], and BGE [39]; (iii) Pre-trained Language Models for Recommendation: BLaIR [12]."
                        ],
                        "paper": {
                            "corpus_id": 271891894,
                            "title": "EasyRec: Simple yet Effective Language Models for Recommendation",
                            "authors": [
                                {
                                    "authorId": "2163180478",
                                    "name": "Xubin Ren"
                                },
                                {
                                    "authorId": "2305137970",
                                    "name": "Chao Huang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.94580078125
                    },
                    {
                        "id": "(Zhang et al., 2024)",
                        "snippets": [
                            "In the literature, there are two major research aspects in this field: \n\n(1) Datastore Indexing (Khandelwal et al., 2019)10,(Wei et al., 2022)[48] and (2) Document Retrieval [35,27]. For Datastore Indexing, these approaches utilize pre-trained models to generate static embeddings for documents, which are viewed as mounted external memory, and they leverage various semantic similarities to enhance indexing. For Document Retrieval, the system initially retrieves a collection of relevant documents based on the semantic relevance between the user query and the documents. Then, the LLMs concatenate these highly related documents in an unordered manner to the prompt input [4], which makes LLMs better at answering factual questions."
                        ],
                        "paper": {
                            "corpus_id": 269605025,
                            "title": "R4: Reinforced Retriever-Reorder-Responder for Retrieval-Augmented Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2146342371",
                                    "name": "Taolin Zhang"
                                },
                                {
                                    "authorId": "2257089368",
                                    "name": "Dongyang Li"
                                },
                                {
                                    "authorId": "2300139675",
                                    "name": "Qizhou Chen"
                                },
                                {
                                    "authorId": "50097294",
                                    "name": "Chengyu Wang"
                                },
                                {
                                    "authorId": "2292090586",
                                    "name": "Longtao Huang"
                                },
                                {
                                    "authorId": "2292128230",
                                    "name": "Hui Xue"
                                },
                                {
                                    "authorId": "2257159827",
                                    "name": "Xiaofeng He"
                                },
                                {
                                    "authorId": "2272790856",
                                    "name": "Junyuan Huang"
                                }
                            ],
                            "year": 2024,
                            "venue": "European Conference on Artificial Intelligence",
                            "n_citations": 0
                        },
                        "score": 0.94482421875
                    },
                    {
                        "id": "(Wei et al., 2022)",
                        "snippets": [
                            "Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models."
                        ],
                        "paper": {
                            "corpus_id": 249674500,
                            "title": "Emergent Abilities of Large Language Models",
                            "authors": [
                                {
                                    "authorId": "119640649",
                                    "name": "Jason Wei"
                                },
                                {
                                    "authorId": "144447820",
                                    "name": "Yi Tay"
                                },
                                {
                                    "authorId": "150272855",
                                    "name": "Rishi Bommasani"
                                },
                                {
                                    "authorId": "2402716",
                                    "name": "Colin Raffel"
                                },
                                {
                                    "authorId": "2368067",
                                    "name": "Barret Zoph"
                                },
                                {
                                    "authorId": "148016269",
                                    "name": "Sebastian Borgeaud"
                                },
                                {
                                    "authorId": "1755465",
                                    "name": "Dani Yogatama"
                                },
                                {
                                    "authorId": "40377863",
                                    "name": "Maarten Bosma"
                                },
                                {
                                    "authorId": "65855107",
                                    "name": "Denny Zhou"
                                },
                                {
                                    "authorId": "1680617",
                                    "name": "Donald Metzler"
                                },
                                {
                                    "authorId": "2226805",
                                    "name": "Ed H. Chi"
                                },
                                {
                                    "authorId": "2117567142",
                                    "name": "Tatsunori Hashimoto"
                                },
                                {
                                    "authorId": "1689108",
                                    "name": "O. Vinyals"
                                },
                                {
                                    "authorId": "2075292388",
                                    "name": "P. Liang"
                                },
                                {
                                    "authorId": "48448318",
                                    "name": "J. Dean"
                                },
                                {
                                    "authorId": "26958176",
                                    "name": "W. Fedus"
                                }
                            ],
                            "year": 2022,
                            "venue": "Trans. Mach. Learn. Res.",
                            "n_citations": 2516
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Evolution of Retrieval Models",
                "tldr": "Retrieval models have evolved from statistical language modeling approaches to sophisticated neural architectures. This progression includes advancements from basic probabilistic document-query generation models to cache-based techniques accounting for word burstiness, and translation-based methods addressing lexical gaps between queries and documents. (7 sources)",
                "text": "\nThe evolution of retrieval models has its roots in statistical language modeling (SLM), which has been applied to information retrieval for decades. In early approaches, documents were ranked according to their probability of generating a given query, establishing a fundamental probabilistic framework for retrieval systems <Paper corpusId=\"2862121\" paperTitle=\"(Chiu et al., 2009)\" isShortName></Paper>. These basic language models assigned non-zero probabilities to terms in a language, with a document's relevance estimated as the probability that a query was generated from that document <Paper corpusId=\"90238494\" paperTitle=\"(Petersen, 2019)\" isShortName></Paper>.\n\nAs retrieval models developed, researchers identified limitations in the basic multinomial language model, particularly its inability to account for term dependency phenomena such as word burstiness\u2014the tendency of terms to repeat within documents <Paper corpusId=\"9519540\" paperTitle=\"(Cummins et al., 2015)\" isShortName></Paper>. This led to the development of cache-based models that recognized how word probability in a document is influenced not only by global frequency and N-gram context but also by local document frequencies. These models leveraged the observation that while most words are rare at the corpus level, they tend to occur in bursts when they do appear, making local estimates from a cache potentially more reliable than global estimates <Paper corpusId=\"10844118\" paperTitle=\"(Wintrode, 2015)\" isShortName></Paper>.\n\nTo improve retrieval capabilities, researchers also explored structured query representations through region expressions. These approaches used logical operators to combine probability assessments, such as producing documents containing specific terms and ranking them according to term occurrence probabilities <Paper corpusId=\"644854\" paperTitle=\"(Hiemstra et al., 2005)\" isShortName></Paper>. This introduced greater flexibility in query formulation and document matching.\n\nAnother significant advancement came through translation-based retrieval models that addressed lexical gaps between queries and documents. By viewing information retrieval as statistical document-query translation, these models introduced conditional probability distributions to map query words to document words <Paper corpusId=\"7742063\" paperTitle=\"(Lee et al., 2008)\" isShortName></Paper>. This approach effectively produced an implicit query expansion effect, allowing query words not present in a document to be mapped to related words in the document, thereby improving retrieval performance when reliable translation probability distributions were available.\n\nCross-language information retrieval further extended these concepts by decomposing the retrieval problem into components that were easier to estimate. Rather than translating a query before estimating a query model, some approaches directly estimated the query model in the target language by combining translation models with familiar probability distributions <Paper corpusId=\"5764728\" paperTitle=\"(Kraaij et al., 2003)\" isShortName></Paper>. This method effectively mapped probability distribution functions from the source language event space onto the target language vocabulary.\n\nThis evolutionary trajectory from basic statistical language models to sophisticated translation-based approaches established the foundation for the modern neural retrieval architectures discussed in previous sections, demonstrating how innovations in handling term dependencies, lexical gaps, and cross-language mapping have contributed to the current state of retrieval language models.",
                "citations": [
                    {
                        "id": "(Chiu et al., 2009)",
                        "snippets": [
                            "Statistical language modeling (SLM) has been used in many different domains for decades and has also been applied to information retrieval (IR) recently. Documents retrieved using this approach are ranked according their probability of generating the given query."
                        ],
                        "paper": {
                            "corpus_id": 2862121,
                            "title": "Optimizing Language Model Information Retrieval System with Expectation Maximization Algorithm",
                            "authors": [
                                {
                                    "authorId": "2990581",
                                    "name": "J. Chiu"
                                },
                                {
                                    "authorId": "2684491",
                                    "name": "Jyun-Wei Huang"
                                }
                            ],
                            "year": 2009,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 0
                        },
                        "score": 0.9375
                    },
                    {
                        "id": "(Petersen, 2019)",
                        "snippets": [
                            "A language model (LM) is, in its most basic form, a probability distribution over terms in a language where each term, t is assigned a non-zero probability denoting its probability of occurrence in the \"language\". A \"language\" here is defined as a non-empty finite sequence of symbols or terms. Given a query q and document d \u2208C for some collection C , d 's LM, \u03b8 d , is a probabilistic model that estimates the probability that q was generated by d . In other words, each document is viewed as a sample from the language, and its relevance to q is estimated as the probability that q was generated from this sample."
                        ],
                        "paper": {
                            "corpus_id": 90238494,
                            "title": "On the Estimation and Use of Statistical Modelling in Information Retrieval",
                            "authors": [
                                {
                                    "authorId": "8304471",
                                    "name": "Casper Petersen"
                                }
                            ],
                            "year": 2019,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.98828125
                    },
                    {
                        "id": "(Cummins et al., 2015)",
                        "snippets": [
                            "The multinomial language model has been one of the most effective models of retrieval for more than a decade. However, the multinomial distribution does not model one important linguistic phenomenon relating to term dependency\u2014that is, the tendency of a term to repeat itself within a document (i.e., word burstiness)."
                        ],
                        "paper": {
                            "corpus_id": 9519540,
                            "title": "A P\u00f3lya Urn Document Language Model for Improved Information Retrieval",
                            "authors": [
                                {
                                    "authorId": "3161572",
                                    "name": "Ronan Cummins"
                                },
                                {
                                    "authorId": "2071527",
                                    "name": "Jiaul H. Paik"
                                },
                                {
                                    "authorId": "40282698",
                                    "name": "Yuanhua Lv"
                                }
                            ],
                            "year": 2015,
                            "venue": "ACM Trans. Inf. Syst.",
                            "n_citations": 35
                        },
                        "score": 0.96630859375
                    },
                    {
                        "id": "(Wintrode, 2015)",
                        "snippets": [
                            "Cache-based models assume the probability of a word in a document d is influenced both by the global frequency of that word and N-gram context as well as by the N-gram frequencies of d (or preceding cache of K words). Although most words are rare at the corpus level, when they do occur, they occur in bursts. Thus a local estimate, from the cache, may be more reliable than the global estimate."
                        ],
                        "paper": {
                            "corpus_id": 10844118,
                            "title": "Cache-Augmented Latent Topic Language Models for Speech Retrieval",
                            "authors": [
                                {
                                    "authorId": "3045640",
                                    "name": "Jonathan Wintrode"
                                }
                            ],
                            "year": 2015,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 0
                        },
                        "score": 0.93359375
                    },
                    {
                        "id": "(Hiemstra et al., 2005)",
                        "snippets": [
                            "The right-hand side of the equation corresponds to the following region expression. \n\n(<doc> CONTAINING db) AND (<doc> CONTAINING ir) \n\nThis can be shown as follows: The region expression (<doc> CONTAINING db) produces all documents ranked according to P (T = db|D), i.e., all regions tagged as <doc>, ranked by the number of occurrences of db in those regions. Similarly, (<doc> CONTAINING ir) produces all documents ranked according to P (T = ir|D). Finally, the operator AND results in the regions tagged as <doc> that are in both operand sets. The score of the result regions is defined as the product of the scores of the same regions in the operands."
                        ],
                        "paper": {
                            "corpus_id": 644854,
                            "title": "A database approach to information retrieval: The remarkable relationship between language models and region models",
                            "authors": [
                                {
                                    "authorId": "1691929",
                                    "name": "D. Hiemstra"
                                },
                                {
                                    "authorId": "144779604",
                                    "name": "V. Mihajlovi\u0107"
                                }
                            ],
                            "year": 2005,
                            "venue": "arXiv.org",
                            "n_citations": 8
                        },
                        "score": 0.931640625
                    },
                    {
                        "id": "(Lee et al., 2008)",
                        "snippets": [
                            "The basic language modeling framework does not address the issue of lexical gaps between queries and question. (Berger et al., 1999) viewed information retrieval as statistical document-query translation and introduced translation models to map query words to document words. Assuming that a translation model can be represented by a conditional probability distribution of translation T (\u2022|\u2022) between words, we can model P (q|M D ) in Equation 3 as: \n\nwhere w represents a document word. 3 he translation probability T (q|w) virtually represents the degree of relationship between query word q and document word w captured in a different, machine translation setting. Then, in the traditional information retrieval viewpoint, the use of translation models produce an implicit query expansion effect, since query words not in a document are mapped to related words in the document. This implies that translation-based retrieval models would make positive contributions to retrieval performance only when the pre-constructed translation models have reliable translation probability distributions."
                        ],
                        "paper": {
                            "corpus_id": 7742063,
                            "title": "Bridging Lexical Gaps between Queries and Questions on Large Online Q&A Collections with Compact Translation Models",
                            "authors": [
                                {
                                    "authorId": "143703455",
                                    "name": "Jung-Tae Lee"
                                },
                                {
                                    "authorId": "2109561184",
                                    "name": "Sang-Bum Kim"
                                },
                                {
                                    "authorId": "1693485",
                                    "name": "Young-In Song"
                                },
                                {
                                    "authorId": "2326357",
                                    "name": "Hae-Chang Rim"
                                }
                            ],
                            "year": 2008,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 68
                        },
                        "score": 0.97265625
                    },
                    {
                        "id": "(Kraaij et al., 2003)",
                        "snippets": [
                            "Instead of translating a query before estimating a query model (the external approach), we propose to directly estimate the query model in the target language. We will do this by decomposing the problem into two components that are easier to estimate: \n\nwhere L is the size of the source vocabulary. Thus, P (t i |M Qs ) can be approximated by combining the translation model P (t i |s j ), which we can estimate on the parallel Web corpus, and the familiar P (s j |M Qs ) which can be estimated using relative frequencies. This simplified model, from which we have dropped the dependency of P (t i |s j ) on Q, can be interpreted as a way of mapping the probability distribution function in the source language event space P (s j |M Qs ) onto the event space of the target language vocabulary."
                        ],
                        "paper": {
                            "corpus_id": 5764728,
                            "title": "Embedding Web-Based Statistical Translation Models in Cross-Language Information Retrieval",
                            "authors": [
                                {
                                    "authorId": "1740640",
                                    "name": "Wessel Kraaij"
                                },
                                {
                                    "authorId": "143619007",
                                    "name": "Jian-Yun Nie"
                                },
                                {
                                    "authorId": "144866028",
                                    "name": "Michel Simard"
                                }
                            ],
                            "year": 2003,
                            "venue": "International Conference on Computational Logic",
                            "n_citations": 123
                        },
                        "score": 0.92626953125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.185337
    }
}