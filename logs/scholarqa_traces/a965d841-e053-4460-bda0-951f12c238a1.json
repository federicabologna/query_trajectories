{
    "query": "What are the most effective post-training mitigation techniques for reducing racial and gender bias in large language models, and how do they compare in terms of performance trade-offs?",
    "user_id": "lib_user",
    "task_id": "a965d841-e053-4460-bda0-951f12c238a1",
    "timestamp": "2025-06-24T00:52:36.975751",
    "n_retrieval": 256,
    "n_retrieved": 263,
    "n_candidates": 48,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.47095499999999996,
    "decomposed_query": {
        "rewritten_query": "Effective post-training mitigation techniques for reducing racial and gender bias in large language models, and how they compare in terms of performance trade-offs.",
        "keyword_query": "post-training mitigation techniques racial gender bias large language models performance trade-offs",
        "search_filters": {
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.010176,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Bias and Fairness in Large Language Models: Evaluation and Mitigation Techniques",
            "venue": "International Journal of Academic and Industrial Research Innovations(IJAIRI)",
            "year": 2025,
            "reference_count": 0,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.62311/nesx/rphcr6?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.62311/nesx/rphcr6, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2295364655",
                    "name": "Murali Krishna Pasupuleti"
                }
            ],
            "abstract": "Abstract:\nLarge Language Models (LLMs) such as GPT, BERT, and LLaMA have transformed natural language processing, yet they exhibit social biases that can reinforce unfair outcomes. This paper systematically evaluates bias and fairness in LLMs across gender, race, and socioeconomic dimensions using benchmark datasets and fairness metrics. We assess bias through template-based probing, stereotype score measurement, and downstream task performance. We implement mitigation strategies including adversarial training, counterfactual data augmentation, and fairness-aware loss functions. Regression and predictive analysis reveal that token frequency and representation distance significantly correlate with bias scores. Post-mitigation analysis shows up to a 48% reduction in bias indicators with minimal accuracy trade-offs.\n\nKeywords:\nLarge Language Models, Bias, Fairness, Evaluation, Mitigation, GPT, BERT, Counterfactual Augmentation, SHAP, LIME",
            "corpus_id": 278939773,
            "sentences": [],
            "relevance_judgement": 0.97119140625,
            "relevance_judgment_input_expanded": "# Title: Bias and Fairness in Large Language Models: Evaluation and Mitigation Techniques\n# Venue: International Journal of Academic and Industrial Research Innovations(IJAIRI)\n# Authors: Murali Krishna Pasupuleti\n## Abstract\nAbstract:\nLarge Language Models (LLMs) such as GPT, BERT, and LLaMA have transformed natural language processing, yet they exhibit social biases that can reinforce unfair outcomes. This paper systematically evaluates bias and fairness in LLMs across gender, race, and socioeconomic dimensions using benchmark datasets and fairness metrics. We assess bias through template-based probing, stereotype score measurement, and downstream task performance. We implement mitigation strategies including adversarial training, counterfactual data augmentation, and fairness-aware loss functions. Regression and predictive analysis reveal that token frequency and representation distance significantly correlate with bias scores. Post-mitigation analysis shows up to a 48% reduction in bias indicators with minimal accuracy trade-offs.\n\nKeywords:\nLarge Language Models, Bias, Fairness, Evaluation, Mitigation, GPT, BERT, Counterfactual Augmentation, SHAP, LIME\n",
            "reference_string": "[278939773 | Pasupuleti | 2025 | Citations: 0]"
        },
        {
            "title": "Attention Pruning: Automated Fairness Repair of Language Models via Surrogate Simulated Annealing",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 76,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.15815, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "152123953",
                    "name": "Vishnu Asutosh Dasu"
                },
                {
                    "authorId": "2150249269",
                    "name": "Md. Rafi Ur Rashid"
                },
                {
                    "authorId": "2110652561",
                    "name": "Vipul Gupta"
                },
                {
                    "authorId": "1404887808",
                    "name": "Saeid Tizpaz-Niari"
                },
                {
                    "authorId": "2309172099",
                    "name": "Gang Tan"
                }
            ],
            "abstract": "This paper explores pruning attention heads as a post-processing bias mitigation method for large language models (LLMs). Modern AI systems such as LLMs are expanding into sensitive social contexts where fairness concerns become especially crucial. Since LLMs develop decision-making patterns by training on massive datasets of human-generated content, they naturally encode and perpetuate societal biases. While modifying training datasets and algorithms is expensive and requires significant resources; post-processing techniques-such as selectively deactivating neurons and attention heads in pre-trained LLMs-can provide feasible and effective approaches to improve fairness. However, identifying the optimal subset of parameters to prune presents a combinatorial challenge within LLMs' immense parameter space, requiring solutions that efficiently balance competing objectives across the frontiers of model fairness and utility. To address the computational challenges, we explore a search-based program repair approach via randomized simulated annealing. Given the prohibitive evaluation costs in billion-parameter LLMs, we develop surrogate deep neural networks that efficiently model the relationship between attention head states (active/inactive) and their corresponding fairness/utility metrics. This allows us to perform optimization over the surrogate models and efficiently identify optimal subsets of attention heads for selective pruning rather than directly searching through the LLM parameter space. This paper introduces Attention Pruning, a fairness-aware surrogate simulated annealing approach to prune attention heads in LLMs that disproportionately contribute to bias while minimally impacting overall model utility. Our experiments show that Attention Pruning achieves up to $40\\%$ reduction in gender bias and outperforms the state-of-the-art bias mitigation strategies.",
            "corpus_id": 277150560,
            "sentences": [
                {
                    "corpus_id": "277150560",
                    "title": "Attention Pruning: Automated Fairness Repair of Language Models via Surrogate Simulated Annealing",
                    "text": "Recent works have started using expert models for bias reduction. Orgad and Belinkov [49] predicted biased samples with an auxiliary model and reweighted them during pre-training. Jeon et al. [35] employed binary classifiers, or bias experts, to pinpoint biased examples in specific classes. \n\nIn-processing methods modify the model's parameters and optimization process. Methods such as ADELE [36] introduce adapter layers for parameter-efficient fine-tuning to reduce bias. Regularization techniques can be applied to penalize the model for producing biased outputs [47]. Additionally, debiasing word embeddings can help reduce bias in the model's representation of words by reducing gendered or racial stereotypes [7,25,68]. Several loss functions, such as declustering loss [23] and contrastive loss [34,41] are also shown to be effective for mitigating bias. Zhang et al. [71] leveraged gradient-based explanations to target sensitive attributes and adjust training for balanced fairness and performance. Apart from that, Dhingra et al. [16] used style transfer to replace harmful language while maintaining the original meaning. \n\nPost-processing techniques modify the model's inference behavior after complete training. This may involve altering the model before inference or the model output during/after inference. For instance, Gehman et al. [24] proposed token-blocking methods during decoding to prevent the generation of harmful or biased terms. \n\nHauzenberger et al. [33] introduced sparse debiasing subnetworks that are trained separately and can be applied to the model at inference time. Qian et al. [53] performed keyword-based distillation to remove bias during inference. Tokpo and Calders [61] identify biased tokens and replace them with less stereotypical terms. Postprocessing approaches are beneficial since dataset collection and LLM training are expensive processes that we may not always be able to repeat if issues of unfairness are found. Furthermore, postprocessing mitigation approaches are sometimes the only viable option with the status quo of utilizing large pre-trained LLMs that take a huge amount of resources to train.",
                    "score": 0.7231611812219216,
                    "section_title": "Related Works",
                    "char_start_offset": 39048,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 65
                        },
                        {
                            "start": 66,
                            "end": 179
                        },
                        {
                            "start": 180,
                            "end": 291
                        },
                        {
                            "start": 294,
                            "end": 371
                        },
                        {
                            "start": 372,
                            "end": 475
                        },
                        {
                            "start": 476,
                            "end": 573
                        },
                        {
                            "start": 574,
                            "end": 727
                        },
                        {
                            "start": 728,
                            "end": 863
                        },
                        {
                            "start": 864,
                            "end": 1009
                        },
                        {
                            "start": 1010,
                            "end": 1134
                        },
                        {
                            "start": 1137,
                            "end": 1226
                        },
                        {
                            "start": 1227,
                            "end": 1323
                        },
                        {
                            "start": 1324,
                            "end": 1458
                        },
                        {
                            "start": 1461,
                            "end": 1604
                        },
                        {
                            "start": 1605,
                            "end": 1691
                        },
                        {
                            "start": 1692,
                            "end": 1785
                        },
                        {
                            "start": 1786,
                            "end": 1968
                        },
                        {
                            "start": 1969,
                            "end": 2158
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 85,
                            "end": 89,
                            "matchedPaperCorpusId": "259138920"
                        },
                        {
                            "start": 568,
                            "end": 572,
                            "matchedPaperCorpusId": "221136077"
                        },
                        {
                            "start": 778,
                            "end": 782,
                            "matchedPaperCorpusId": "236477795"
                        },
                        {
                            "start": 804,
                            "end": 808,
                            "matchedPaperCorpusId": "253157517"
                        },
                        {
                            "start": 808,
                            "end": 811,
                            "matchedPaperCorpusId": "233864681"
                        },
                        {
                            "start": 877,
                            "end": 881,
                            "matchedPaperCorpusId": "258833564"
                        },
                        {
                            "start": 1617,
                            "end": 1621,
                            "matchedPaperCorpusId": "236459953"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9658203125
                }
            ],
            "relevance_judgement": 0.9658203125,
            "relevance_judgment_input_expanded": "# Title: Attention Pruning: Automated Fairness Repair of Language Models via Surrogate Simulated Annealing\n# Venue: arXiv.org\n# Authors: Vishnu Asutosh Dasu, Md. Rafi Ur Rashid, Vipul Gupta, Saeid Tizpaz-Niari, Gang Tan\n## Abstract\nThis paper explores pruning attention heads as a post-processing bias mitigation method for large language models (LLMs). Modern AI systems such as LLMs are expanding into sensitive social contexts where fairness concerns become especially crucial. Since LLMs develop decision-making patterns by training on massive datasets of human-generated content, they naturally encode and perpetuate societal biases. While modifying training datasets and algorithms is expensive and requires significant resources; post-processing techniques-such as selectively deactivating neurons and attention heads in pre-trained LLMs-can provide feasible and effective approaches to improve fairness. However, identifying the optimal subset of parameters to prune presents a combinatorial challenge within LLMs' immense parameter space, requiring solutions that efficiently balance competing objectives across the frontiers of model fairness and utility. To address the computational challenges, we explore a search-based program repair approach via randomized simulated annealing. Given the prohibitive evaluation costs in billion-parameter LLMs, we develop surrogate deep neural networks that efficiently model the relationship between attention head states (active/inactive) and their corresponding fairness/utility metrics. This allows us to perform optimization over the surrogate models and efficiently identify optimal subsets of attention heads for selective pruning rather than directly searching through the LLM parameter space. This paper introduces Attention Pruning, a fairness-aware surrogate simulated annealing approach to prune attention heads in LLMs that disproportionately contribute to bias while minimally impacting overall model utility. Our experiments show that Attention Pruning achieves up to $40\\%$ reduction in gender bias and outperforms the state-of-the-art bias mitigation strategies.\n## Related Works\nRecent works have started using expert models for bias reduction. Orgad and Belinkov [49] predicted biased samples with an auxiliary model and reweighted them during pre-training. Jeon et al. [35] employed binary classifiers, or bias experts, to pinpoint biased examples in specific classes. \n\nIn-processing methods modify the model's parameters and optimization process. Methods such as ADELE [36] introduce adapter layers for parameter-efficient fine-tuning to reduce bias. Regularization techniques can be applied to penalize the model for producing biased outputs [47]. Additionally, debiasing word embeddings can help reduce bias in the model's representation of words by reducing gendered or racial stereotypes [7,25,68]. Several loss functions, such as declustering loss [23] and contrastive loss [34,41] are also shown to be effective for mitigating bias. Zhang et al. [71] leveraged gradient-based explanations to target sensitive attributes and adjust training for balanced fairness and performance. Apart from that, Dhingra et al. [16] used style transfer to replace harmful language while maintaining the original meaning. \n\nPost-processing techniques modify the model's inference behavior after complete training. This may involve altering the model before inference or the model output during/after inference. For instance, Gehman et al. [24] proposed token-blocking methods during decoding to prevent the generation of harmful or biased terms. \n\nHauzenberger et al. [33] introduced sparse debiasing subnetworks that are trained separately and can be applied to the model at inference time. Qian et al. [53] performed keyword-based distillation to remove bias during inference. Tokpo and Calders [61] identify biased tokens and replace them with less stereotypical terms. Postprocessing approaches are beneficial since dataset collection and LLM training are expensive processes that we may not always be able to repeat if issues of unfairness are found. Furthermore, postprocessing mitigation approaches are sometimes the only viable option with the status quo of utilizing large pre-trained LLMs that take a huge amount of resources to train.",
            "reference_string": "[277150560 | Dasu et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Demographic-Aware Language Model Fine-tuning as a Bias Mitigation Technique",
            "venue": "AACL",
            "year": 2022,
            "reference_count": 27,
            "citation_count": 21,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2022.aacl-short.38, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "31099365",
                    "name": "Aparna Garimella"
                },
                {
                    "authorId": "2105984203",
                    "name": "Rada Mihalcea"
                },
                {
                    "authorId": "2121347719",
                    "name": "Akhash Amarnath"
                }
            ],
            "abstract": "BERT-like language models (LMs), when exposed to large unstructured datasets, are known to learn and sometimes even amplify the biases present in such data. These biases generally reflect social stereotypes with respect to gender, race, age, and others. In this paper, we analyze the variations in gender and racial biases in BERT, a large pre-trained LM, when exposed to different demographic groups. Specifically, we investigate the effect of fine-tuning BERT on text authored by historically disadvantaged demographic groups in comparison to that by advantaged groups. We show that simply by fine-tuning BERT-like LMs on text authored by certain demographic groups can result in the mitigation of social biases in these LMs against various target groups.",
            "corpus_id": 253762006,
            "sentences": [
                {
                    "corpus_id": "253762006",
                    "title": "Demographic-Aware Language Model Fine-tuning as a Bias Mitigation Technique",
                    "text": "BERT-like language models (LMs), when exposed to large unstructured datasets, are known to learn and sometimes even amplify the biases present in such data. These biases generally reflect social stereotypes with respect to gender, race, age, and others. In this paper, we analyze the variations in gender and racial biases in BERT, a large pre-trained LM, when exposed to different demographic groups. Specifically, we investigate the effect of fine-tuning BERT on text authored by historically disadvantaged demographic groups in comparison to that by advantaged groups. We show that simply by fine-tuning BERT-like LMs on text authored by certain demographic groups can result in the mitigation of social biases in these LMs against various target groups.",
                    "score": 0.5121247568369862,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96240234375
                }
            ],
            "relevance_judgement": 0.96240234375,
            "relevance_judgment_input_expanded": "# Title: Demographic-Aware Language Model Fine-tuning as a Bias Mitigation Technique\n# Venue: AACL\n# Authors: Aparna Garimella, Rada Mihalcea, Akhash Amarnath\n## Abstract\nBERT-like language models (LMs), when exposed to large unstructured datasets, are known to learn and sometimes even amplify the biases present in such data. These biases generally reflect social stereotypes with respect to gender, race, age, and others. In this paper, we analyze the variations in gender and racial biases in BERT, a large pre-trained LM, when exposed to different demographic groups. Specifically, we investigate the effect of fine-tuning BERT on text authored by historically disadvantaged demographic groups in comparison to that by advantaged groups. We show that simply by fine-tuning BERT-like LMs on text authored by certain demographic groups can result in the mitigation of social biases in these LMs against various target groups.\n",
            "reference_string": "[253762006 | Garimella et al. | 2022 | Citations: 21]"
        },
        {
            "title": "Biases Mitigation and Expressiveness Preservation in Language Models: A Comprehensive Pipeline (Student Abstract)",
            "venue": "AAAI Conference on Artificial Intelligence",
            "year": 2024,
            "reference_count": 2,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ojs.aaai.org/index.php/AAAI/article/download/30532/32688",
                "status": "GOLD",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v38i21.30532?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v38i21.30532, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2171005187",
                    "name": "Liu Yu"
                },
                {
                    "authorId": "2293569654",
                    "name": "Ludie Guo"
                },
                {
                    "authorId": "2162996325",
                    "name": "Ping Kuang"
                },
                {
                    "authorId": "2294149576",
                    "name": "Fan Zhou"
                }
            ],
            "abstract": "Pre-trained language models (PLMs) have greatly transformed various downstream tasks, yet frequently display social biases from training data, raising fairness concerns. Recent efforts to debias PLMs come with limitations: they either fine-tune the entire parameters in PLMs, which is time-consuming and disregards the expressiveness of PLMs, or ignore the reintroducing biases from downstream tasks when applying debiased models to them. Hence, we propose a two-stage pipeline to mitigate biases from both internal and downstream contexts while preserving expressiveness in language models. Specifically, for the debiasing procedure, we resort to continuous prefix-tuning, not fully fine-tuning the PLM, in which we design a debiasing term for optimization and an alignment term to keep words\u2019 relative distances and ensure the model's expressiveness. For downstream tasks, we perform causal intervention across different demographic groups for invariant predictions. Results on three GLUE tasks show our method alleviates biases from internal and downstream contexts, while keeping PLM expressiveness intact.",
            "corpus_id": 268709624,
            "sentences": [
                {
                    "corpus_id": "268709624",
                    "title": "Biases Mitigation and Expressiveness Preservation in Language Models: A Comprehensive Pipeline (Student Abstract)",
                    "text": "Pre-trained Language Models (PLMs) excel in diverse natural language tasks due to their training on extensive data. However, prior studies have revealed that PLMs inadvertently encode and propagate social biases from their unfiltered pre-training data. Take gender bias as an example: the PLM is more inclined towards associating male (female) attributes with programmers (nurses). Several solutions for mitigating the social biases have been proposed, including: (1) Post-hoc-based method add a post-training step to these sentence representations before applied to downstream tasks, including removing the estimated genderdirection subspace from sentence representation (Liang et al. 2020), or use pre-defined word tuples combine specific techniques to debias text encoder for a fair sentence representation (Cheng et al. 2021). (2) Fine-tuning-based models use specific loss terms to guide a PLM to remove biases, including distribution alignment loss for debiasing embedding space (Guo, Yang, and Abbasi 2022); orthogonal loss aims to promote irrelevance between stereotyped words and gender-specific words (Kaneko and Bollegala 2021), etc. Current debiasing methods for PLMs have shown promise but grapple with notable challenges: (1) demanding timeconsuming to fine-tune entire parameters in PLMs; (2) disregarding the expressiveness of PLMs, which could potentially disrupt PLM's computational structure and undermine the benefits of pre-training; (3) reintroducing biases from downstream tasks into PLMs when applying debiased models to those tasks. Hence, we present a new two-stage pipeline that aims to simultaneously preserve the PLMs' expressiveness and mitigate biases from both internal and downstream contexts. As shown in Figure 1, in first stage, we keep PLM's parameter frozen, and only train the continuous prefix to reduce the magnitude of trainable parameters, towards mitigating internal bias and meanwhile preserving expressiveness. In second stage, we perform causal interventions on different demographic groups to eliminate the biases from downstream contexts.",
                    "score": 0.5695316491216322,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 115
                        },
                        {
                            "start": 116,
                            "end": 252
                        },
                        {
                            "start": 253,
                            "end": 381
                        },
                        {
                            "start": 382,
                            "end": 830
                        },
                        {
                            "start": 831,
                            "end": 1144
                        },
                        {
                            "start": 1145,
                            "end": 1557
                        },
                        {
                            "start": 1558,
                            "end": 1726
                        },
                        {
                            "start": 1727,
                            "end": 1956
                        },
                        {
                            "start": 1957,
                            "end": 2087
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 672,
                            "end": 691,
                            "matchedPaperCorpusId": "207996257"
                        },
                        {
                            "start": 810,
                            "end": 829,
                            "matchedPaperCorpusId": "232185104"
                        },
                        {
                            "start": 985,
                            "end": 1013,
                            "matchedPaperCorpusId": "248780440"
                        },
                        {
                            "start": 1111,
                            "end": 1138,
                            "matchedPaperCorpusId": "231698657"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9609375
                }
            ],
            "relevance_judgement": 0.9609375,
            "relevance_judgment_input_expanded": "# Title: Biases Mitigation and Expressiveness Preservation in Language Models: A Comprehensive Pipeline (Student Abstract)\n# Venue: AAAI Conference on Artificial Intelligence\n# Authors: Liu Yu, Ludie Guo, Ping Kuang, Fan Zhou\n## Abstract\nPre-trained language models (PLMs) have greatly transformed various downstream tasks, yet frequently display social biases from training data, raising fairness concerns. Recent efforts to debias PLMs come with limitations: they either fine-tune the entire parameters in PLMs, which is time-consuming and disregards the expressiveness of PLMs, or ignore the reintroducing biases from downstream tasks when applying debiased models to them. Hence, we propose a two-stage pipeline to mitigate biases from both internal and downstream contexts while preserving expressiveness in language models. Specifically, for the debiasing procedure, we resort to continuous prefix-tuning, not fully fine-tuning the PLM, in which we design a debiasing term for optimization and an alignment term to keep words\u2019 relative distances and ensure the model's expressiveness. For downstream tasks, we perform causal intervention across different demographic groups for invariant predictions. Results on three GLUE tasks show our method alleviates biases from internal and downstream contexts, while keeping PLM expressiveness intact.\n## Introduction\nPre-trained Language Models (PLMs) excel in diverse natural language tasks due to their training on extensive data. However, prior studies have revealed that PLMs inadvertently encode and propagate social biases from their unfiltered pre-training data. Take gender bias as an example: the PLM is more inclined towards associating male (female) attributes with programmers (nurses). Several solutions for mitigating the social biases have been proposed, including: (1) Post-hoc-based method add a post-training step to these sentence representations before applied to downstream tasks, including removing the estimated genderdirection subspace from sentence representation (Liang et al. 2020), or use pre-defined word tuples combine specific techniques to debias text encoder for a fair sentence representation (Cheng et al. 2021). (2) Fine-tuning-based models use specific loss terms to guide a PLM to remove biases, including distribution alignment loss for debiasing embedding space (Guo, Yang, and Abbasi 2022); orthogonal loss aims to promote irrelevance between stereotyped words and gender-specific words (Kaneko and Bollegala 2021), etc. Current debiasing methods for PLMs have shown promise but grapple with notable challenges: (1) demanding timeconsuming to fine-tune entire parameters in PLMs; (2) disregarding the expressiveness of PLMs, which could potentially disrupt PLM's computational structure and undermine the benefits of pre-training; (3) reintroducing biases from downstream tasks into PLMs when applying debiased models to those tasks. Hence, we present a new two-stage pipeline that aims to simultaneously preserve the PLMs' expressiveness and mitigate biases from both internal and downstream contexts. As shown in Figure 1, in first stage, we keep PLM's parameter frozen, and only train the continuous prefix to reduce the magnitude of trainable parameters, towards mitigating internal bias and meanwhile preserving expressiveness. In second stage, we perform causal interventions on different demographic groups to eliminate the biases from downstream contexts.",
            "reference_string": "[268709624 | Yu et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Causality for Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 124,
            "citation_count": 9,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.15319, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1748975704",
                    "name": "Anpeng Wu"
                },
                {
                    "authorId": "2315590986",
                    "name": "Kun Kuang"
                },
                {
                    "authorId": "2292675528",
                    "name": "Minqin Zhu"
                },
                {
                    "authorId": "2290028937",
                    "name": "Yingrong Wang"
                },
                {
                    "authorId": "2309464222",
                    "name": "Yujia Zheng"
                },
                {
                    "authorId": "2315988838",
                    "name": "Kairong Han"
                },
                {
                    "authorId": "2313166010",
                    "name": "Baohong Li"
                },
                {
                    "authorId": "2155315836",
                    "name": "Guan-Hong Chen"
                },
                {
                    "authorId": "2290035617",
                    "name": "Fei Wu"
                },
                {
                    "authorId": "2316094914",
                    "name": "Kun Zhang"
                }
            ],
            "abstract": "Recent breakthroughs in artificial intelligence have driven a paradigm shift, where large language models (LLMs) with billions or trillions of parameters are trained on vast datasets, achieving unprecedented success across a series of language tasks. However, despite these successes, LLMs still rely on probabilistic modeling, which often captures spurious correlations rooted in linguistic patterns and social stereotypes, rather than the true causal relationships between entities and events. This limitation renders LLMs vulnerable to issues such as demographic biases, social stereotypes, and LLM hallucinations. These challenges highlight the urgent need to integrate causality into LLMs, moving beyond correlation-driven paradigms to build more reliable and ethically aligned AI systems. While many existing surveys and studies focus on utilizing prompt engineering to activate LLMs for causal knowledge or developing benchmarks to assess their causal reasoning abilities, most of these efforts rely on human intervention to activate pre-trained models. How to embed causality into the training process of LLMs and build more general and intelligent models remains unexplored. Recent research highlights that LLMs function as causal parrots, capable of reciting causal knowledge without truly understanding or applying it. These prompt-based methods are still limited to human interventional improvements. This survey aims to address this gap by exploring how causality can enhance LLMs at every stage of their lifecycle-from token embedding learning and foundation model training to fine-tuning, alignment, inference, and evaluation-paving the way for more interpretable, reliable, and causally-informed models. Additionally, we further outline six promising future directions to advance LLM development, enhance their causal reasoning capabilities, and address the current limitations these models face.",
            "corpus_id": 273501862,
            "sentences": [
                {
                    "corpus_id": "273501862",
                    "title": "Causality for Large Language Models",
                    "text": "Dropout is applied during training to reduce overfitting and disrupt gendered associations in the model's attention mechanisms. CDA involves augmenting training data by swapping gendered terms (e.g., replacing \"he\" with \"she\") to teach the model to be neutral to such correlations. Both techniques were found to reduce gender bias while maintaining overall model performance. \n\nCausal-Debias [31]. The Causal-Debias framework offers a pioneering solution to mitigating biases in pretrained language models (PLMs) by integrating causal learning principles into the fine-tuning process, which could be directly applied to the pre-training processes. Unlike conventional methods that separate bias mitigation and task performance optimization, Causal-Debias merges these goals through the use of causal interventions and invariant risk minimization (IRM). The framework effectively distinguishes between causal (label-relevant) and non-causal (bias-related) factors embedded in token representations, addressing spurious correlations resulting from demographic biases and social stereotypes. By generating counterfactual data-modifying bias-related attributes such as gendered or racial terms-and training the model with an invariant loss across diverse environments, Causal-Debias ensures that the model generalizes well across tasks while mitigating biases. This approach directly tackles the challenge of bias resurgence, where previously mitigated biases reappear during fine-tuning, a limitation observed in many existing debiasing techniques. \n\nBy unifying the debiasing process with token embedding learning, the framework provides a robust and practical solution for building fair and accountable NLP models. However, its reliance on external corpora and predefined bias-related word lists poses challenges, particularly in extending its application to broader or more nuanced demographic groups and biases. This dependence may limit its scalability or effectiveness in handling more complex, intersectional biases that extend beyond predefined categories. Nonetheless, Causal-Debias represents a significant advancement in the field, emphasizing the importance of causal learning in achieving meaningful debiasing across various environments.",
                    "score": 0.5823273394493663,
                    "section_title": "Debiased Token Embedding",
                    "char_start_offset": 36075,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 127
                        },
                        {
                            "start": 128,
                            "end": 281
                        },
                        {
                            "start": 282,
                            "end": 375
                        },
                        {
                            "start": 378,
                            "end": 397
                        },
                        {
                            "start": 398,
                            "end": 647
                        },
                        {
                            "start": 648,
                            "end": 852
                        },
                        {
                            "start": 853,
                            "end": 1088
                        },
                        {
                            "start": 1089,
                            "end": 1356
                        },
                        {
                            "start": 1357,
                            "end": 1545
                        },
                        {
                            "start": 1548,
                            "end": 1713
                        },
                        {
                            "start": 1714,
                            "end": 1912
                        },
                        {
                            "start": 1913,
                            "end": 2061
                        },
                        {
                            "start": 2062,
                            "end": 2248
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 392,
                            "end": 396,
                            "matchedPaperCorpusId": "259370743"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95703125
                }
            ],
            "relevance_judgement": 0.95703125,
            "relevance_judgment_input_expanded": "# Title: Causality for Large Language Models\n# Venue: arXiv.org\n# Authors: Anpeng Wu, Kun Kuang, Minqin Zhu, Yingrong Wang, Yujia Zheng, Kairong Han, Baohong Li, Guan-Hong Chen, Fei Wu, Kun Zhang\n## Abstract\nRecent breakthroughs in artificial intelligence have driven a paradigm shift, where large language models (LLMs) with billions or trillions of parameters are trained on vast datasets, achieving unprecedented success across a series of language tasks. However, despite these successes, LLMs still rely on probabilistic modeling, which often captures spurious correlations rooted in linguistic patterns and social stereotypes, rather than the true causal relationships between entities and events. This limitation renders LLMs vulnerable to issues such as demographic biases, social stereotypes, and LLM hallucinations. These challenges highlight the urgent need to integrate causality into LLMs, moving beyond correlation-driven paradigms to build more reliable and ethically aligned AI systems. While many existing surveys and studies focus on utilizing prompt engineering to activate LLMs for causal knowledge or developing benchmarks to assess their causal reasoning abilities, most of these efforts rely on human intervention to activate pre-trained models. How to embed causality into the training process of LLMs and build more general and intelligent models remains unexplored. Recent research highlights that LLMs function as causal parrots, capable of reciting causal knowledge without truly understanding or applying it. These prompt-based methods are still limited to human interventional improvements. This survey aims to address this gap by exploring how causality can enhance LLMs at every stage of their lifecycle-from token embedding learning and foundation model training to fine-tuning, alignment, inference, and evaluation-paving the way for more interpretable, reliable, and causally-informed models. Additionally, we further outline six promising future directions to advance LLM development, enhance their causal reasoning capabilities, and address the current limitations these models face.\n## Debiased Token Embedding\nDropout is applied during training to reduce overfitting and disrupt gendered associations in the model's attention mechanisms. CDA involves augmenting training data by swapping gendered terms (e.g., replacing \"he\" with \"she\") to teach the model to be neutral to such correlations. Both techniques were found to reduce gender bias while maintaining overall model performance. \n\nCausal-Debias [31]. The Causal-Debias framework offers a pioneering solution to mitigating biases in pretrained language models (PLMs) by integrating causal learning principles into the fine-tuning process, which could be directly applied to the pre-training processes. Unlike conventional methods that separate bias mitigation and task performance optimization, Causal-Debias merges these goals through the use of causal interventions and invariant risk minimization (IRM). The framework effectively distinguishes between causal (label-relevant) and non-causal (bias-related) factors embedded in token representations, addressing spurious correlations resulting from demographic biases and social stereotypes. By generating counterfactual data-modifying bias-related attributes such as gendered or racial terms-and training the model with an invariant loss across diverse environments, Causal-Debias ensures that the model generalizes well across tasks while mitigating biases. This approach directly tackles the challenge of bias resurgence, where previously mitigated biases reappear during fine-tuning, a limitation observed in many existing debiasing techniques. \n\nBy unifying the debiasing process with token embedding learning, the framework provides a robust and practical solution for building fair and accountable NLP models. However, its reliance on external corpora and predefined bias-related word lists poses challenges, particularly in extending its application to broader or more nuanced demographic groups and biases. This dependence may limit its scalability or effectiveness in handling more complex, intersectional biases that extend beyond predefined categories. Nonetheless, Causal-Debias represents a significant advancement in the field, emphasizing the importance of causal learning in achieving meaningful debiasing across various environments.",
            "reference_string": "[273501862 | Wu et al. | 2024 | Citations: 9]"
        },
        {
            "title": "Securing Large Language Models: Addressing Bias, Misinformation, and Prompt Attacks",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 159,
            "citation_count": 15,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.08087, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2319606006",
                    "name": "Benji Peng"
                },
                {
                    "authorId": "2319648854",
                    "name": "Keyu Chen"
                },
                {
                    "authorId": "2321058649",
                    "name": "Ming Li"
                },
                {
                    "authorId": "2319607771",
                    "name": "Pohsun Feng"
                },
                {
                    "authorId": "2319608188",
                    "name": "Ziqian Bi"
                },
                {
                    "authorId": "2319615017",
                    "name": "Junyu Liu"
                },
                {
                    "authorId": "2319611972",
                    "name": "Qian Niu"
                }
            ],
            "abstract": "Large Language Models (LLMs) demonstrate impressive capabilities across various fields, yet their increasing use raises critical security concerns. This article reviews recent literature addressing key issues in LLM security, with a focus on accuracy, bias, content detection, and vulnerability to attacks. Issues related to inaccurate or misleading outputs from LLMs is discussed, with emphasis on the implementation from fact-checking methodologies to enhance response reliability. Inherent biases within LLMs are critically examined through diverse evaluation techniques, including controlled input studies and red teaming exercises. A comprehensive analysis of bias mitigation strategies is presented, including approaches from pre-processing interventions to in-training adjustments and post-processing refinements. The article also probes the complexity of distinguishing LLM-generated content from human-produced text, introducing detection mechanisms like DetectGPT and watermarking techniques while noting the limitations of machine learning enabled classifiers under intricate circumstances. Moreover, LLM vulnerabilities, including jailbreak attacks and prompt injection exploits, are analyzed by looking into different case studies and large-scale competitions like HackAPrompt. This review is concluded by retrospecting defense mechanisms to safeguard LLMs, accentuating the need for more extensive research into the LLM security field.",
            "corpus_id": 272599907,
            "sentences": [
                {
                    "corpus_id": "272599907",
                    "title": "Securing Large Language Models: Addressing Bias, Misinformation, and Prompt Attacks",
                    "text": "Bias mitigation can be achieved during four stages: pre-processing, in-training, intra-processing, and post-processing. Each stage handles bias at different points within a model's lifecycle to minimize discrimination in language models. \n\nAt the pre-processing stage, data augmentation, such as Counterfactual Data Augmentation (CDA), balances datasets by substituting attributes related to gender, race, or other protected groups. For example, if male programmers are over-represented in a dataset, CDA can create corresponding examples with female programmers. CDA approach was further improved by Counterfactual Data Substitution (CDS), which randomly replaces attributes to mitigate bias [134]. Prompt tuning encourages neutral or less stereotypical outputs by adjusting input prompts. Hard prompts use static templates, while soft prompts [135] generate embeddings dynamically during interactions with the model. \n\nBias for models in-training is addressed by modifying the learning process. Iterative Null Space Projection (INLP) removes bias by projecting targeted attributes into a space where they do not influence the model's outputs [136]. \n\nCausal Regularization ensures that models rely on meaningful, causal relationships rather than biased correlations in the data [137]. Auxiliary modules, such as the Adapter-based Debiasing (ADELE) uses additional modules to address bias without retraining the entire model [138]. And, GEnder Equality Prompt (GEEP), has been proposed to help overcome catastrophic forgetting improve gender fairness by freezing the pre-trained model and let the model learn gender-related prompts with gender-neutral data [139]. \n\nDuring intra-processing, models are tweaked at the inference stage without retraining. Model editing enables targeted updates to model behavior, ensuring that biases in specific areas are corrected without affecting overall model performance [140,141]. Decoding modification like DEXPERTS directly affects text generation by adjusting token probabilities. DEXPERTS uses two models, one to promote non-toxic text and another to discourage harmful content, to improving output fairness [142]. \n\nPost-processing methods focus on modifying the model's outputs.",
                    "score": 0.6319894525142066,
                    "section_title": "Bias Mitigation Strategies",
                    "char_start_offset": 15183,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 119
                        },
                        {
                            "start": 120,
                            "end": 237
                        },
                        {
                            "start": 240,
                            "end": 432
                        },
                        {
                            "start": 433,
                            "end": 563
                        },
                        {
                            "start": 564,
                            "end": 699
                        },
                        {
                            "start": 700,
                            "end": 790
                        },
                        {
                            "start": 791,
                            "end": 918
                        },
                        {
                            "start": 921,
                            "end": 996
                        },
                        {
                            "start": 997,
                            "end": 1150
                        },
                        {
                            "start": 1153,
                            "end": 1286
                        },
                        {
                            "start": 1287,
                            "end": 1432
                        },
                        {
                            "start": 1433,
                            "end": 1664
                        },
                        {
                            "start": 1667,
                            "end": 1753
                        },
                        {
                            "start": 1754,
                            "end": 1919
                        },
                        {
                            "start": 1920,
                            "end": 2022
                        },
                        {
                            "start": 2023,
                            "end": 2157
                        },
                        {
                            "start": 2160,
                            "end": 2223
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1909,
                            "end": 1914,
                            "matchedPaperCorpusId": "249642147"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.955078125
                }
            ],
            "relevance_judgement": 0.955078125,
            "relevance_judgment_input_expanded": "# Title: Securing Large Language Models: Addressing Bias, Misinformation, and Prompt Attacks\n# Venue: arXiv.org\n# Authors: Benji Peng, Keyu Chen, Ming Li, Pohsun Feng, Ziqian Bi, Junyu Liu, Qian Niu\n## Abstract\nLarge Language Models (LLMs) demonstrate impressive capabilities across various fields, yet their increasing use raises critical security concerns. This article reviews recent literature addressing key issues in LLM security, with a focus on accuracy, bias, content detection, and vulnerability to attacks. Issues related to inaccurate or misleading outputs from LLMs is discussed, with emphasis on the implementation from fact-checking methodologies to enhance response reliability. Inherent biases within LLMs are critically examined through diverse evaluation techniques, including controlled input studies and red teaming exercises. A comprehensive analysis of bias mitigation strategies is presented, including approaches from pre-processing interventions to in-training adjustments and post-processing refinements. The article also probes the complexity of distinguishing LLM-generated content from human-produced text, introducing detection mechanisms like DetectGPT and watermarking techniques while noting the limitations of machine learning enabled classifiers under intricate circumstances. Moreover, LLM vulnerabilities, including jailbreak attacks and prompt injection exploits, are analyzed by looking into different case studies and large-scale competitions like HackAPrompt. This review is concluded by retrospecting defense mechanisms to safeguard LLMs, accentuating the need for more extensive research into the LLM security field.\n## Bias Mitigation Strategies\nBias mitigation can be achieved during four stages: pre-processing, in-training, intra-processing, and post-processing. Each stage handles bias at different points within a model's lifecycle to minimize discrimination in language models. \n\nAt the pre-processing stage, data augmentation, such as Counterfactual Data Augmentation (CDA), balances datasets by substituting attributes related to gender, race, or other protected groups. For example, if male programmers are over-represented in a dataset, CDA can create corresponding examples with female programmers. CDA approach was further improved by Counterfactual Data Substitution (CDS), which randomly replaces attributes to mitigate bias [134]. Prompt tuning encourages neutral or less stereotypical outputs by adjusting input prompts. Hard prompts use static templates, while soft prompts [135] generate embeddings dynamically during interactions with the model. \n\nBias for models in-training is addressed by modifying the learning process. Iterative Null Space Projection (INLP) removes bias by projecting targeted attributes into a space where they do not influence the model's outputs [136]. \n\nCausal Regularization ensures that models rely on meaningful, causal relationships rather than biased correlations in the data [137]. Auxiliary modules, such as the Adapter-based Debiasing (ADELE) uses additional modules to address bias without retraining the entire model [138]. And, GEnder Equality Prompt (GEEP), has been proposed to help overcome catastrophic forgetting improve gender fairness by freezing the pre-trained model and let the model learn gender-related prompts with gender-neutral data [139]. \n\nDuring intra-processing, models are tweaked at the inference stage without retraining. Model editing enables targeted updates to model behavior, ensuring that biases in specific areas are corrected without affecting overall model performance [140,141]. Decoding modification like DEXPERTS directly affects text generation by adjusting token probabilities. DEXPERTS uses two models, one to promote non-toxic text and another to discourage harmful content, to improving output fairness [142]. \n\nPost-processing methods focus on modifying the model's outputs.",
            "reference_string": "[272599907 | Peng et al. | 2024 | Citations: 15]"
        },
        {
            "title": "Sustainable Modular Debiasing of Language Models",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2021,
            "reference_count": 58,
            "citation_count": 122,
            "influential_citation_count": 13,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2021.findings-emnlp.411.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2109.03646, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "29891652",
                    "name": "Anne Lauscher"
                },
                {
                    "authorId": "2127511334",
                    "name": "Tobias L\u00fcken"
                },
                {
                    "authorId": "2472657",
                    "name": "Goran Glavas"
                }
            ],
            "abstract": "Unfair stereotypical biases (e.g., gender, racial, or religious biases) encoded in modern pretrained language models (PLMs) have negative ethical implications for widespread adoption of state-of-the-art language technology. To remedy for this, a wide range of debiasing techniques have recently been introduced to remove such stereotypical biases from PLMs. Existing debiasing methods, however, directly modify all of the PLMs parameters, which -- besides being computationally expensive -- comes with the inherent risk of (catastrophic) forgetting of useful language knowledge acquired in pretraining. In this work, we propose a more sustainable modular debiasing approach based on dedicated debiasing adapters, dubbed ADELE. Concretely, we (1) inject adapter modules into the original PLM layers and (2) update only the adapters (i.e., we keep the original PLM parameters frozen) via language modeling training on a counterfactually augmented corpus. We showcase ADELE, in gender debiasing of BERT: our extensive evaluation, encompassing three intrinsic and two extrinsic bias measures, renders ADELE, very effective in bias mitigation. We further show that -- due to its modular nature -- ADELE, coupled with task adapters, retains fairness even after large-scale downstream training. Finally, by means of multilingual BERT, we successfully transfer ADELE, to six target languages.",
            "corpus_id": 237440429,
            "sentences": [
                {
                    "corpus_id": "237440429",
                    "title": "Sustainable Modular Debiasing of Language Models",
                    "text": "Unfair stereotypical biases (e.g., gender, racial, or religious biases) encoded in modern pretrained language models (PLMs) have negative ethical implications for widespread adoption of state-of-the-art language technology. To remedy for this, a wide range of debiasing techniques have recently been introduced to remove such stereotypical biases from PLMs. Existing debiasing methods, however, directly modify all of the PLMs parameters, which -- besides being computationally expensive -- comes with the inherent risk of (catastrophic) forgetting of useful language knowledge acquired in pretraining. In this work, we propose a more sustainable modular debiasing approach based on dedicated debiasing adapters, dubbed ADELE. Concretely, we (1) inject adapter modules into the original PLM layers and (2) update only the adapters (i.e., we keep the original PLM parameters frozen) via language modeling training on a counterfactually augmented corpus. We showcase ADELE, in gender debiasing of BERT: our extensive evaluation, encompassing three intrinsic and two extrinsic bias measures, renders ADELE, very effective in bias mitigation. We further show that -- due to its modular nature -- ADELE, coupled with task adapters, retains fairness even after large-scale downstream training. Finally, by means of multilingual BERT, we successfully transfer ADELE, to six target languages.",
                    "score": 0.5591747269355064,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95068359375
                }
            ],
            "relevance_judgement": 0.95068359375,
            "relevance_judgment_input_expanded": "# Title: Sustainable Modular Debiasing of Language Models\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Anne Lauscher, Tobias L\u00fcken, Goran Glavas\n## Abstract\nUnfair stereotypical biases (e.g., gender, racial, or religious biases) encoded in modern pretrained language models (PLMs) have negative ethical implications for widespread adoption of state-of-the-art language technology. To remedy for this, a wide range of debiasing techniques have recently been introduced to remove such stereotypical biases from PLMs. Existing debiasing methods, however, directly modify all of the PLMs parameters, which -- besides being computationally expensive -- comes with the inherent risk of (catastrophic) forgetting of useful language knowledge acquired in pretraining. In this work, we propose a more sustainable modular debiasing approach based on dedicated debiasing adapters, dubbed ADELE. Concretely, we (1) inject adapter modules into the original PLM layers and (2) update only the adapters (i.e., we keep the original PLM parameters frozen) via language modeling training on a counterfactually augmented corpus. We showcase ADELE, in gender debiasing of BERT: our extensive evaluation, encompassing three intrinsic and two extrinsic bias measures, renders ADELE, very effective in bias mitigation. We further show that -- due to its modular nature -- ADELE, coupled with task adapters, retains fairness even after large-scale downstream training. Finally, by means of multilingual BERT, we successfully transfer ADELE, to six target languages.\n",
            "reference_string": "[237440429 | Lauscher et al. | 2021 | Citations: 122]"
        },
        {
            "title": "Auto-Debias: Debiasing Masked Language Models with Automated Biased Prompts",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2022,
            "reference_count": 56,
            "citation_count": 167,
            "influential_citation_count": 14,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2022.acl-long.72.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2022.acl-long.72, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": null,
                    "name": "Yue Guo"
                },
                {
                    "authorId": "46285693",
                    "name": "Yi Yang"
                },
                {
                    "authorId": "144849629",
                    "name": "A. Abbasi"
                }
            ],
            "abstract": "Human-like biases and undesired social stereotypes exist in large pretrained language models. Given the wide adoption of these models in real-world applications, mitigating such biases has become an emerging and important task. In this paper, we propose an automatic method to mitigate the biases in pretrained language models. Different from previous debiasing work that uses external corpora to fine-tune the pretrained models, we instead directly probe the biases encoded in pretrained models through prompts. Specifically, we propose a variant of the beam search method to automatically search for biased prompts such that the cloze-style completions are the most different with respect to different demographic groups. Given the identified biased prompts, we then propose a distribution alignment loss to mitigate the biases. Experiment results on standard datasets and metrics show that our proposed Auto-Debias approach can significantly reduce biases, including gender and racial bias, in pretrained language models such as BERT, RoBERTa and ALBERT. Moreover, the improvement in fairness does not decrease the language models\u2019 understanding abilities, as shown using the GLUE benchmark.",
            "corpus_id": 248780440,
            "sentences": [
                {
                    "corpus_id": "248780440",
                    "title": "Auto-Debias: Debiasing Masked Language Models with Automated Biased Prompts",
                    "text": "In this work, we propose Auto-Debias, a framework and method for automatically mitigating the biases and stereotypes encoded in PLMs. Compared to previous efforts that rely on external corpora to obtain context-dependent word embeddings, our approach automatically searches for biased prompts in the PLMs. Therefore, our approach is effective, efficient, and is perhaps also more objective than prior methods that rely heavily on manually crafted lists of stereotype words. Experimental results on standard benchmarks show that Auto-Debias reduces gender and race biases more effectively than prior efforts. Moreover, the debiased models also maintain good language modeling capability. Bias in NLP systems can stem from different aspects such as training data, pretrained embeddings, or through amplification when finetuning the machine learning models. We believe this work contributes to the emerging literature that sheds light on practical and effective debiasing techniques.",
                    "score": 0.5320837315818048,
                    "section_title": "Conclusion",
                    "char_start_offset": 26822,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 133
                        },
                        {
                            "start": 134,
                            "end": 305
                        },
                        {
                            "start": 306,
                            "end": 473
                        },
                        {
                            "start": 474,
                            "end": 607
                        },
                        {
                            "start": 608,
                            "end": 686
                        },
                        {
                            "start": 687,
                            "end": 854
                        },
                        {
                            "start": 855,
                            "end": 980
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9501953125
                },
                {
                    "corpus_id": "248780440",
                    "title": "Auto-Debias: Debiasing Masked Language Models with Automated Biased Prompts",
                    "text": "Human-like biases and undesired social stereotypes exist in large pretrained language models. Given the wide adoption of these models in real-world applications, mitigating such biases has become an emerging and important task. In this paper, we propose an automatic method to mitigate the biases in pretrained language models. Different from previous debiasing work that uses external corpora to fine-tune the pretrained models, we instead directly probe the biases encoded in pretrained models through prompts. Specifically, we propose a variant of the beam search method to automatically search for biased prompts such that the cloze-style completions are the most different with respect to different demographic groups. Given the identified biased prompts, we then propose a distribution alignment loss to mitigate the biases. Experiment results on standard datasets and metrics show that our proposed Auto-Debias approach can significantly reduce biases, including gender and racial bias, in pretrained language models such as BERT, RoBERTa and ALBERT. Moreover, the improvement in fairness does not decrease the language models\u2019 understanding abilities, as shown using the GLUE benchmark.",
                    "score": 0.5393247525121996,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91552734375
                }
            ],
            "relevance_judgement": 0.9501953125,
            "relevance_judgment_input_expanded": "# Title: Auto-Debias: Debiasing Masked Language Models with Automated Biased Prompts\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Yue Guo, Yi Yang, A. Abbasi\n## Abstract\nHuman-like biases and undesired social stereotypes exist in large pretrained language models. Given the wide adoption of these models in real-world applications, mitigating such biases has become an emerging and important task. In this paper, we propose an automatic method to mitigate the biases in pretrained language models. Different from previous debiasing work that uses external corpora to fine-tune the pretrained models, we instead directly probe the biases encoded in pretrained models through prompts. Specifically, we propose a variant of the beam search method to automatically search for biased prompts such that the cloze-style completions are the most different with respect to different demographic groups. Given the identified biased prompts, we then propose a distribution alignment loss to mitigate the biases. Experiment results on standard datasets and metrics show that our proposed Auto-Debias approach can significantly reduce biases, including gender and racial bias, in pretrained language models such as BERT, RoBERTa and ALBERT. Moreover, the improvement in fairness does not decrease the language models\u2019 understanding abilities, as shown using the GLUE benchmark.\n## Conclusion\nIn this work, we propose Auto-Debias, a framework and method for automatically mitigating the biases and stereotypes encoded in PLMs. Compared to previous efforts that rely on external corpora to obtain context-dependent word embeddings, our approach automatically searches for biased prompts in the PLMs. Therefore, our approach is effective, efficient, and is perhaps also more objective than prior methods that rely heavily on manually crafted lists of stereotype words. Experimental results on standard benchmarks show that Auto-Debias reduces gender and race biases more effectively than prior efforts. Moreover, the debiased models also maintain good language modeling capability. Bias in NLP systems can stem from different aspects such as training data, pretrained embeddings, or through amplification when finetuning the machine learning models. We believe this work contributes to the emerging literature that sheds light on practical and effective debiasing techniques.",
            "reference_string": "[248780440 | Guo et al. | 2022 | Citations: 167]"
        },
        {
            "title": "Multi-objective search for gender-fair and semantically correct word embeddings",
            "venue": "Applied Soft Computing",
            "year": 2022,
            "reference_count": 96,
            "citation_count": 8,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1016/j.asoc.2022.109916",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.asoc.2022.109916?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.asoc.2022.109916, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2091097171",
                    "name": "Max Hort"
                },
                {
                    "authorId": "23105293",
                    "name": "Rebecca Moussa"
                },
                {
                    "authorId": "2260686724",
                    "name": "Federica Sarro"
                }
            ],
            "abstract": null,
            "corpus_id": 264309222,
            "sentences": [
                {
                    "corpus_id": "264309222",
                    "title": "Multi-objective search for gender-fair and semantically correct word embeddings",
                    "text": "In order to improve algorithm fairness, three types of approaches can be applied: pre-processing, in-processing and postprocessing. We refer the reader to the literature review by Sun et al. [58] for further details on techniques including the debiasing of training corpora (pre-processing) and debiasing by adjusting algorithms (in-processing). In the following, we focus on postprocessing debiasing methods for word embeddings as our work belongs to this category. \n\nPost-processing bias methods mitigate bias after a model has been trained. To reduce gender bias, Bolukbasi et al. [20] proposed the following Hard Debiasing (HD) post-processing method: They first identified a gender subspace and then proceeded to neutralise it, ensuring that the performance on the evaluation tasks is maintained. To determine the gender subspace, they identified a gender direction g \u2208 R d by combining several directions (e.g., \n\n. This approach has however been criticised to have several shortcomings [59] and alternative approaches have been sought to overcome them. To this end, Dev and Phillips [29] debiased word embeddings by using a linear projection along the gender direction, whereas, Lauscher et al. [31] adjusted the linear projection by using an alternative projecting approach and a neural network to learn word vectors transformation. Instead, Kaneko and Bollegala [60] proposed the use of an autoencoder to remove biases from pre-trained word embeddings. Shin et al. [61] proposed a latent disentanglement method to obtain gender-neutralised word embeddings. Ravfogel et al. [30] presented an Iterative Null-space Projection (INLP) method. Instead of specifying a gender direction, as done by Bolukbasi et al. [20], INLP learns this direction with a linear classifier and removes it by iteratively projecting the word embeddings on their null-space. Kaneko and Bollegala [62] debiased pre-trained word embeddings with the use of dictionaries. \n\nIn this work, we take a different route to mitigate bias in word embeddings.",
                    "score": 0.5428220667327114,
                    "section_title": "Debiasing word embeddings",
                    "char_start_offset": 12055,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 131
                        },
                        {
                            "start": 132,
                            "end": 345
                        },
                        {
                            "start": 346,
                            "end": 466
                        },
                        {
                            "start": 469,
                            "end": 543
                        },
                        {
                            "start": 544,
                            "end": 801
                        },
                        {
                            "start": 802,
                            "end": 917
                        },
                        {
                            "start": 920,
                            "end": 1059
                        },
                        {
                            "start": 1060,
                            "end": 1340
                        },
                        {
                            "start": 1341,
                            "end": 1461
                        },
                        {
                            "start": 1462,
                            "end": 1565
                        },
                        {
                            "start": 1566,
                            "end": 1646
                        },
                        {
                            "start": 1647,
                            "end": 1856
                        },
                        {
                            "start": 1857,
                            "end": 1949
                        },
                        {
                            "start": 1952,
                            "end": 2028
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 584,
                            "end": 588,
                            "matchedPaperCorpusId": "1704893"
                        },
                        {
                            "start": 1202,
                            "end": 1206,
                            "matchedPaperCorpusId": "202572693"
                        },
                        {
                            "start": 1717,
                            "end": 1721,
                            "matchedPaperCorpusId": "1704893"
                        },
                        {
                            "start": 1878,
                            "end": 1882,
                            "matchedPaperCorpusId": "231698423"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9384765625
                }
            ],
            "relevance_judgement": 0.9384765625,
            "relevance_judgment_input_expanded": "# Title: Multi-objective search for gender-fair and semantically correct word embeddings\n# Venue: Applied Soft Computing\n# Authors: Max Hort, Rebecca Moussa, Federica Sarro\n## Abstract\nNone\n## Debiasing word embeddings\nIn order to improve algorithm fairness, three types of approaches can be applied: pre-processing, in-processing and postprocessing. We refer the reader to the literature review by Sun et al. [58] for further details on techniques including the debiasing of training corpora (pre-processing) and debiasing by adjusting algorithms (in-processing). In the following, we focus on postprocessing debiasing methods for word embeddings as our work belongs to this category. \n\nPost-processing bias methods mitigate bias after a model has been trained. To reduce gender bias, Bolukbasi et al. [20] proposed the following Hard Debiasing (HD) post-processing method: They first identified a gender subspace and then proceeded to neutralise it, ensuring that the performance on the evaluation tasks is maintained. To determine the gender subspace, they identified a gender direction g \u2208 R d by combining several directions (e.g., \n\n. This approach has however been criticised to have several shortcomings [59] and alternative approaches have been sought to overcome them. To this end, Dev and Phillips [29] debiased word embeddings by using a linear projection along the gender direction, whereas, Lauscher et al. [31] adjusted the linear projection by using an alternative projecting approach and a neural network to learn word vectors transformation. Instead, Kaneko and Bollegala [60] proposed the use of an autoencoder to remove biases from pre-trained word embeddings. Shin et al. [61] proposed a latent disentanglement method to obtain gender-neutralised word embeddings. Ravfogel et al. [30] presented an Iterative Null-space Projection (INLP) method. Instead of specifying a gender direction, as done by Bolukbasi et al. [20], INLP learns this direction with a linear classifier and removes it by iteratively projecting the word embeddings on their null-space. Kaneko and Bollegala [62] debiased pre-trained word embeddings with the use of dictionaries. \n\nIn this work, we take a different route to mitigate bias in word embeddings.",
            "reference_string": "[264309222 | Hort et al. | 2022 | Citations: 8]"
        },
        {
            "title": "Modular and On-demand Bias Mitigation with Attribute-Removal Subnetworks",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2022,
            "reference_count": 60,
            "citation_count": 18,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2023.findings-acl.386.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2205.15171, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2167029006",
                    "name": "Lukas Hauzenberger"
                },
                {
                    "authorId": "2184114298",
                    "name": "Shahed Masoudian"
                },
                {
                    "authorId": "2116415191",
                    "name": "Deepak Kumar"
                },
                {
                    "authorId": "144125621",
                    "name": "M. Schedl"
                },
                {
                    "authorId": "2844293",
                    "name": "Navid Rekabsaz"
                }
            ],
            "abstract": "Societal biases are reflected in large pre-trained language models and their fine-tuned versions on downstream tasks. Common in-processing bias mitigation approaches, such as adversarial training and mutual information removal, introduce additional optimization criteria, and update the model to reach a new debiased state. However, in practice, end-users and practitioners might prefer to switch back to the original model, or apply debiasing only on a specific subset of protected attributes. To enable this, we propose a novel modular bias mitigation approach, consisting of stand-alone highly sparse debiasing subnetworks, where each debiasing module can be integrated into the core model on-demand at inference time. Our approach draws from the concept of \\emph{diff} pruning, and proposes a novel training regime adaptable to various representation disentanglement optimizations. We conduct experiments on three classification tasks with gender, race, and age as protected attributes. The results show that our modular approach, while maintaining task performance, improves (or at least remains on-par with) the effectiveness of bias mitigation in comparison with baseline finetuning. Particularly on a two-attribute dataset, our approach with separately learned debiasing subnetworks shows effective utilization of either or both the subnetworks for selective bias mitigation.",
            "corpus_id": 258461365,
            "sentences": [
                {
                    "corpus_id": "258461365",
                    "title": "Modular and On-demand Bias Mitigation with Attribute-Removal Subnetworks",
                    "text": "Societal biases are reflected in large pre-trained language models and their fine-tuned versions on downstream tasks. Common in-processing bias mitigation approaches, such as adversarial training and mutual information removal, introduce additional optimization criteria, and update the model to reach a new debiased state. However, in practice, end-users and practitioners might prefer to switch back to the original model, or apply debiasing only on a specific subset of protected attributes. To enable this, we propose a novel modular bias mitigation approach, consisting of stand-alone highly sparse debiasing subnetworks, where each debiasing module can be integrated into the core model on-demand at inference time. Our approach draws from the concept of \\emph{diff} pruning, and proposes a novel training regime adaptable to various representation disentanglement optimizations. We conduct experiments on three classification tasks with gender, race, and age as protected attributes. The results show that our modular approach, while maintaining task performance, improves (or at least remains on-par with) the effectiveness of bias mitigation in comparison with baseline finetuning. Particularly on a two-attribute dataset, our approach with separately learned debiasing subnetworks shows effective utilization of either or both the subnetworks for selective bias mitigation.",
                    "score": 0.5216367262408728,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93701171875
                }
            ],
            "relevance_judgement": 0.93701171875,
            "relevance_judgment_input_expanded": "# Title: Modular and On-demand Bias Mitigation with Attribute-Removal Subnetworks\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Lukas Hauzenberger, Shahed Masoudian, Deepak Kumar, M. Schedl, Navid Rekabsaz\n## Abstract\nSocietal biases are reflected in large pre-trained language models and their fine-tuned versions on downstream tasks. Common in-processing bias mitigation approaches, such as adversarial training and mutual information removal, introduce additional optimization criteria, and update the model to reach a new debiased state. However, in practice, end-users and practitioners might prefer to switch back to the original model, or apply debiasing only on a specific subset of protected attributes. To enable this, we propose a novel modular bias mitigation approach, consisting of stand-alone highly sparse debiasing subnetworks, where each debiasing module can be integrated into the core model on-demand at inference time. Our approach draws from the concept of \\emph{diff} pruning, and proposes a novel training regime adaptable to various representation disentanglement optimizations. We conduct experiments on three classification tasks with gender, race, and age as protected attributes. The results show that our modular approach, while maintaining task performance, improves (or at least remains on-par with) the effectiveness of bias mitigation in comparison with baseline finetuning. Particularly on a two-attribute dataset, our approach with separately learned debiasing subnetworks shows effective utilization of either or both the subnetworks for selective bias mitigation.\n",
            "reference_string": "[258461365 | Hauzenberger et al. | 2022 | Citations: 18]"
        },
        {
            "title": "Detecting Bias in Large Language Models: Fine-tuned KcBERT",
            "venue": "International Conferences on Pattern Recognition and Artificial Intelligence",
            "year": 2024,
            "reference_count": 33,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.10774, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2292166326",
                    "name": "J. K. Lee"
                },
                {
                    "authorId": "2292024530",
                    "name": "T. M. Chung"
                }
            ],
            "abstract": "The rapid advancement of large language models (LLMs) has enabled natural language processing capabilities similar to those of humans, and LLMs are being widely utilized across various societal domains such as education and healthcare. While the versatility of these models has increased, they have the potential to generate subjective and normative language, leading to discriminatory treatment or outcomes among social groups, especially due to online offensive language. In this paper, we define such harm as societal bias and assess ethnic, gender, and racial biases in a model fine-tuned with Korean comments using Bidirectional Encoder Representations from Transformers (KcBERT) and KOLD data through template-based Masked Language Modeling (MLM). To quantitatively evaluate biases, we employ LPBS and CBS metrics. Compared to KcBERT, the fine-tuned model shows a reduction in ethnic bias but demonstrates significant changes in gender and racial biases. Based on these results, we propose two methods to mitigate societal bias. Firstly, a data balancing approach during the pre-training phase adjusts the uniformity of data by aligning the distribution of the occurrences of specific words and converting surrounding harmful words into non-harmful words. Secondly, during the in-training phase, we apply Debiasing Regularization by adjusting dropout and regularization, confirming a decrease in training loss. Our contribution lies in demonstrating that societal bias exists in Korean language models due to language-dependent characteristics.",
            "corpus_id": 268512691,
            "sentences": [
                {
                    "corpus_id": "268512691",
                    "title": "Detecting Bias in Large Language Models: Fine-tuned KcBERT",
                    "text": "To alleviate the bias in LLMs, mitigation methods are categorized into preprocessing, in-training, and post-processing based on the pre-training and finetuned processes of LLMs [15].\n\nPre-processing mitigation methods aim to remove social bias from the initial input of the model, such as data or prompts.Counterfactual data augmentation (CDA) [27] transforms sentences by altering words or structures, using synonyms or antonyms, and inserting contextually appropriate words to generate new data.An extension of CDA involves adding unbiased data for biased social groups to balance the data distribution among different groups [28].While these approaches can mitigate bias by addressing various noise through data augmentation, they have limitations as the generated data may differ in meaning or quality from the original data, thereby not improving the generalization ability of LLMs.\n\nIn-training mitigation methods involve training an adversarial classifier, evaluating whether bias occurs during the training process by adding an adversarial loss function.Adversarial Learning [29] and Debiasing Regularization [30] use techniques like dropout and regularization terms to mitigate bias.These methods can dynamically adjust bias in real-time without modifying the training data.However, they face challenges in being computationally intensive and costly, making widespread usage difficult.\n\nPost-processing mitigation methods adjust the probability distribution during the decoding phase to select tokens with less bias, using methods such as adjusting, filtering, or inserting tokens [31].Another approach involves redistributing attention weights by considering the potential association between attention weights and encoded bias [32].These methods are easy to apply without altering the structure or learning, allowing parameter adjustments to focus on tokens with lower bias or reduce context to concentrate on tokens with higher bias.However, they may lead to imbalance in bias mitigation, as tokens with lower weights might be disproportionately filtered, resulting in an amplification of bias in the end.\n\nWe propose two effective bias mitigation methods tailored for Korean.",
                    "score": 0.5951018098202758,
                    "section_title": "Methods of mitigating social bias",
                    "char_start_offset": 7687,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 182
                        },
                        {
                            "start": 184,
                            "end": 305
                        },
                        {
                            "start": 305,
                            "end": 497
                        },
                        {
                            "start": 497,
                            "end": 633
                        },
                        {
                            "start": 633,
                            "end": 887
                        },
                        {
                            "start": 889,
                            "end": 1062
                        },
                        {
                            "start": 1062,
                            "end": 1192
                        },
                        {
                            "start": 1192,
                            "end": 1283
                        },
                        {
                            "start": 1283,
                            "end": 1394
                        },
                        {
                            "start": 1396,
                            "end": 1595
                        },
                        {
                            "start": 1595,
                            "end": 1743
                        },
                        {
                            "start": 1743,
                            "end": 1945
                        },
                        {
                            "start": 1945,
                            "end": 2117
                        },
                        {
                            "start": 2119,
                            "end": 2188
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 344,
                            "end": 348,
                            "matchedPaperCorpusId": "225080424"
                        },
                        {
                            "start": 628,
                            "end": 632,
                            "matchedPaperCorpusId": "54997157"
                        },
                        {
                            "start": 1083,
                            "end": 1087,
                            "matchedPaperCorpusId": "9424845"
                        },
                        {
                            "start": 1590,
                            "end": 1594,
                            "matchedPaperCorpusId": "253734850"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92822265625
                }
            ],
            "relevance_judgement": 0.92822265625,
            "relevance_judgment_input_expanded": "# Title: Detecting Bias in Large Language Models: Fine-tuned KcBERT\n# Venue: International Conferences on Pattern Recognition and Artificial Intelligence\n# Authors: J. K. Lee, T. M. Chung\n## Abstract\nThe rapid advancement of large language models (LLMs) has enabled natural language processing capabilities similar to those of humans, and LLMs are being widely utilized across various societal domains such as education and healthcare. While the versatility of these models has increased, they have the potential to generate subjective and normative language, leading to discriminatory treatment or outcomes among social groups, especially due to online offensive language. In this paper, we define such harm as societal bias and assess ethnic, gender, and racial biases in a model fine-tuned with Korean comments using Bidirectional Encoder Representations from Transformers (KcBERT) and KOLD data through template-based Masked Language Modeling (MLM). To quantitatively evaluate biases, we employ LPBS and CBS metrics. Compared to KcBERT, the fine-tuned model shows a reduction in ethnic bias but demonstrates significant changes in gender and racial biases. Based on these results, we propose two methods to mitigate societal bias. Firstly, a data balancing approach during the pre-training phase adjusts the uniformity of data by aligning the distribution of the occurrences of specific words and converting surrounding harmful words into non-harmful words. Secondly, during the in-training phase, we apply Debiasing Regularization by adjusting dropout and regularization, confirming a decrease in training loss. Our contribution lies in demonstrating that societal bias exists in Korean language models due to language-dependent characteristics.\n## Methods of mitigating social bias\nTo alleviate the bias in LLMs, mitigation methods are categorized into preprocessing, in-training, and post-processing based on the pre-training and finetuned processes of LLMs [15].\n\nPre-processing mitigation methods aim to remove social bias from the initial input of the model, such as data or prompts.Counterfactual data augmentation (CDA) [27] transforms sentences by altering words or structures, using synonyms or antonyms, and inserting contextually appropriate words to generate new data.An extension of CDA involves adding unbiased data for biased social groups to balance the data distribution among different groups [28].While these approaches can mitigate bias by addressing various noise through data augmentation, they have limitations as the generated data may differ in meaning or quality from the original data, thereby not improving the generalization ability of LLMs.\n\nIn-training mitigation methods involve training an adversarial classifier, evaluating whether bias occurs during the training process by adding an adversarial loss function.Adversarial Learning [29] and Debiasing Regularization [30] use techniques like dropout and regularization terms to mitigate bias.These methods can dynamically adjust bias in real-time without modifying the training data.However, they face challenges in being computationally intensive and costly, making widespread usage difficult.\n\nPost-processing mitigation methods adjust the probability distribution during the decoding phase to select tokens with less bias, using methods such as adjusting, filtering, or inserting tokens [31].Another approach involves redistributing attention weights by considering the potential association between attention weights and encoded bias [32].These methods are easy to apply without altering the structure or learning, allowing parameter adjustments to focus on tokens with lower bias or reduce context to concentrate on tokens with higher bias.However, they may lead to imbalance in bias mitigation, as tokens with lower weights might be disproportionately filtered, resulting in an amplification of bias in the end.\n\nWe propose two effective bias mitigation methods tailored for Korean.",
            "reference_string": "[268512691 | Lee et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Can a Prediction\u2019s Rank Offer a More Accurate Quantification of Bias? A Case Study Measuring Sexism in Debiased Language Models",
            "venue": "EVAL4NLP",
            "year": 2023,
            "reference_count": 23,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2023.eval4nlp-1.9.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2023.eval4nlp-1.9, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1724523030",
                    "name": "Jad Doughman"
                },
                {
                    "authorId": "38510157",
                    "name": "Shady Shehata"
                },
                {
                    "authorId": "1452348515",
                    "name": "L. A. Qadi"
                },
                {
                    "authorId": "2292591921",
                    "name": "Youssef Nafea"
                },
                {
                    "authorId": "1696863",
                    "name": "F. Karray"
                }
            ],
            "abstract": "Pre-trained language models are known to inherit a plethora of contextual biases from their training data. These biases have proven to be projected onto a variety of downstream applications, making their detection and mitigation imminent. Limited research has been conducted to quantify specific bias types, such as benevolent sexism, which may be subtly present within the inferred connotations of a sentence. To this extent, our work aims to: (1) provide a benchmark of sexism sentences; (2) adapt two bias metrics: mean probability score and mean normalized rank; (3) conduct a case study to quantify and analyze sexism in base and de-biased masked language models. We find that debiasing, even in its most effective form (Auto-Debias), solely nullifies the probability score of biasing tokens, while retaining them in high ranks. Auto-Debias illustrates a 90%-96% reduction in mean probability scores from base to debiased models, while only a 3%-16% reduction in mean normalized ranks. Similar to the application of non-parametric statistical tests for data that does not follow a normal distribution, operating on the ranks of predictions rather than their probability scores offers a more representative bias measure.",
            "corpus_id": 268064051,
            "sentences": [
                {
                    "corpus_id": "268064051",
                    "title": "Can a Prediction\u2019s Rank Offer a More Accurate Quantification of Bias? A Case Study Measuring Sexism in Debiased Language Models",
                    "text": "According to the literature and to the best of our knowledge, we outline below the four prominent debiasing techniques. \n\nContext-Debias. Context-Debias (Kaneko and Bollegala, 2019) is a technique for debiasing pretrained contextualized word embeddings in a finetuning setting that both (a) preserves the semantic information in the pre-trained contextualized word embedding model, and (b) removes discriminative gender-related biases via an orthogonal projection in the intermediate (hidden) layers by operating at token or sentence-levels. \n\nAuto-Debias. Auto-Debias (Guo et al., 2022) is a debiasing technique for masked language models that does not entail referencing external corpora. Auto-Debias contains two stages: First, automatically crafting biased prompts, such that the clozestyle completions have the highest disagreement in generating stereotype words with respect to demographic groups. Second, debiasing the language model by a distribution alignment loss. \n\nCounterfactual Data Augmentation. Counterfactual Data Augmentation (CDA) (Zmigrod et al., 2019) is a data augmentation technique that involves generating new instances by modifying existing observations. This technique has been employed to mitigate gender bias in models by interchanging masculine-inflected nouns with feminineinflected nouns, and vice versa, thereby generating additional data points that promote model generalization. \n\nDropout. Dropout (Webster et al., 2020) is a regularization technique typically used to reduce overfitting in models, it is also effective for reducing gendered bias problems. By randomly deactivating a portion of the neurons during training/finetuning, dropout can mitigate the influence of gender-specific features, contributing to a more equitable and unbiased model.",
                    "score": 0.5287457250466373,
                    "section_title": "Debiasing Techniques",
                    "char_start_offset": 15110,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 119
                        },
                        {
                            "start": 122,
                            "end": 137
                        },
                        {
                            "start": 138,
                            "end": 541
                        },
                        {
                            "start": 544,
                            "end": 556
                        },
                        {
                            "start": 557,
                            "end": 690
                        },
                        {
                            "start": 691,
                            "end": 903
                        },
                        {
                            "start": 904,
                            "end": 974
                        },
                        {
                            "start": 977,
                            "end": 1010
                        },
                        {
                            "start": 1011,
                            "end": 1180
                        },
                        {
                            "start": 1181,
                            "end": 1413
                        },
                        {
                            "start": 1416,
                            "end": 1424
                        },
                        {
                            "start": 1425,
                            "end": 1591
                        },
                        {
                            "start": 1592,
                            "end": 1786
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 153,
                            "end": 180,
                            "matchedPaperCorpusId": "173991106"
                        },
                        {
                            "start": 569,
                            "end": 586,
                            "matchedPaperCorpusId": "248780440"
                        },
                        {
                            "start": 1050,
                            "end": 1072,
                            "matchedPaperCorpusId": "184486914"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9267578125
                }
            ],
            "relevance_judgement": 0.9267578125,
            "relevance_judgment_input_expanded": "# Title: Can a Prediction\u2019s Rank Offer a More Accurate Quantification of Bias? A Case Study Measuring Sexism in Debiased Language Models\n# Venue: EVAL4NLP\n# Authors: Jad Doughman, Shady Shehata, L. A. Qadi, Youssef Nafea, F. Karray\n## Abstract\nPre-trained language models are known to inherit a plethora of contextual biases from their training data. These biases have proven to be projected onto a variety of downstream applications, making their detection and mitigation imminent. Limited research has been conducted to quantify specific bias types, such as benevolent sexism, which may be subtly present within the inferred connotations of a sentence. To this extent, our work aims to: (1) provide a benchmark of sexism sentences; (2) adapt two bias metrics: mean probability score and mean normalized rank; (3) conduct a case study to quantify and analyze sexism in base and de-biased masked language models. We find that debiasing, even in its most effective form (Auto-Debias), solely nullifies the probability score of biasing tokens, while retaining them in high ranks. Auto-Debias illustrates a 90%-96% reduction in mean probability scores from base to debiased models, while only a 3%-16% reduction in mean normalized ranks. Similar to the application of non-parametric statistical tests for data that does not follow a normal distribution, operating on the ranks of predictions rather than their probability scores offers a more representative bias measure.\n## Debiasing Techniques\nAccording to the literature and to the best of our knowledge, we outline below the four prominent debiasing techniques. \n\nContext-Debias. Context-Debias (Kaneko and Bollegala, 2019) is a technique for debiasing pretrained contextualized word embeddings in a finetuning setting that both (a) preserves the semantic information in the pre-trained contextualized word embedding model, and (b) removes discriminative gender-related biases via an orthogonal projection in the intermediate (hidden) layers by operating at token or sentence-levels. \n\nAuto-Debias. Auto-Debias (Guo et al., 2022) is a debiasing technique for masked language models that does not entail referencing external corpora. Auto-Debias contains two stages: First, automatically crafting biased prompts, such that the clozestyle completions have the highest disagreement in generating stereotype words with respect to demographic groups. Second, debiasing the language model by a distribution alignment loss. \n\nCounterfactual Data Augmentation. Counterfactual Data Augmentation (CDA) (Zmigrod et al., 2019) is a data augmentation technique that involves generating new instances by modifying existing observations. This technique has been employed to mitigate gender bias in models by interchanging masculine-inflected nouns with feminineinflected nouns, and vice versa, thereby generating additional data points that promote model generalization. \n\nDropout. Dropout (Webster et al., 2020) is a regularization technique typically used to reduce overfitting in models, it is also effective for reducing gendered bias problems. By randomly deactivating a portion of the neurons during training/finetuning, dropout can mitigate the influence of gender-specific features, contributing to a more equitable and unbiased model.",
            "reference_string": "[268064051 | Doughman et al. | 2023 | Citations: 0]"
        },
        {
            "title": "An Empirical Analysis of Parameter-Efficient Methods for Debiasing Pre-Trained Language Models",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2023,
            "reference_count": 48,
            "citation_count": 12,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2306.04067",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.04067, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "123639116",
                    "name": "Zhongbin Xie"
                },
                {
                    "authorId": "1690572",
                    "name": "Thomas Lukasiewicz"
                }
            ],
            "abstract": "The increasingly large size of modern pre-trained language models not only makes them inherit more human-like biases from the training corpora, but also makes it computationally expensive to mitigate such biases. In this paper, we investigate recent parameter-efficient methods in combination with counterfactual data augmentation (CDA) for bias mitigation. We conduct extensive experiments with prefix tuning, prompt tuning, and adapter tuning on different language models and bias types to evaluate their debiasing performance and abilities to preserve the internal knowledge of a pre-trained model. We find that the parameter-efficient methods (i) are effective in mitigating gender bias, where adapter tuning is consistently the most effective one and prompt tuning is more suitable for GPT-2 than BERT, (ii) areless effective when it comes to racial and religious bias, which may be attributed to the limitations of CDA, and (iii) can perform similarly to or sometimes better than full fine-tuning with improved time and memory efficiency, as well as maintain the internal knowledge in BERT and GPT-2, evaluated via fact retrieval and downstream fine-tuning.",
            "corpus_id": 259095584,
            "sentences": [
                {
                    "corpus_id": "259095584",
                    "title": "An Empirical Analysis of Parameter-Efficient Methods for Debiasing Pre-Trained Language Models",
                    "text": "The increasingly large size of modern pre-trained language models not only makes them inherit more human-like biases from the training corpora, but also makes it computationally expensive to mitigate such biases. In this paper, we investigate recent parameter-efficient methods in combination with counterfactual data augmentation (CDA) for bias mitigation. We conduct extensive experiments with prefix tuning, prompt tuning, and adapter tuning on different language models and bias types to evaluate their debiasing performance and abilities to preserve the internal knowledge of a pre-trained model. We find that the parameter-efficient methods (i) are effective in mitigating gender bias, where adapter tuning is consistently the most effective one and prompt tuning is more suitable for GPT-2 than BERT, (ii) areless effective when it comes to racial and religious bias, which may be attributed to the limitations of CDA, and (iii) can perform similarly to or sometimes better than full fine-tuning with improved time and memory efficiency, as well as maintain the internal knowledge in BERT and GPT-2, evaluated via fact retrieval and downstream fine-tuning.",
                    "score": 0.5331046326806994,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9248046875
                }
            ],
            "relevance_judgement": 0.9248046875,
            "relevance_judgment_input_expanded": "# Title: An Empirical Analysis of Parameter-Efficient Methods for Debiasing Pre-Trained Language Models\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Zhongbin Xie, Thomas Lukasiewicz\n## Abstract\nThe increasingly large size of modern pre-trained language models not only makes them inherit more human-like biases from the training corpora, but also makes it computationally expensive to mitigate such biases. In this paper, we investigate recent parameter-efficient methods in combination with counterfactual data augmentation (CDA) for bias mitigation. We conduct extensive experiments with prefix tuning, prompt tuning, and adapter tuning on different language models and bias types to evaluate their debiasing performance and abilities to preserve the internal knowledge of a pre-trained model. We find that the parameter-efficient methods (i) are effective in mitigating gender bias, where adapter tuning is consistently the most effective one and prompt tuning is more suitable for GPT-2 than BERT, (ii) areless effective when it comes to racial and religious bias, which may be attributed to the limitations of CDA, and (iii) can perform similarly to or sometimes better than full fine-tuning with improved time and memory efficiency, as well as maintain the internal knowledge in BERT and GPT-2, evaluated via fact retrieval and downstream fine-tuning.\n",
            "reference_string": "[259095584 | Xie et al. | 2023 | Citations: 12]"
        },
        {
            "title": "Towards Transfer Unlearning: Empirical Evidence of Cross-Domain Bias Mitigation",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 18,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.16951, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2312881526",
                    "name": "Huimin Lu"
                },
                {
                    "authorId": "24905917",
                    "name": "Masaru Isonuma"
                },
                {
                    "authorId": "49010536",
                    "name": "Junichiro Mori"
                },
                {
                    "authorId": "2265001653",
                    "name": "Ichiro Sakata"
                }
            ],
            "abstract": "Large language models (LLMs) often inherit biases from vast amounts of training corpora. Traditional debiasing methods, while effective to some extent, do not completely eliminate memorized biases and toxicity in LLMs. In this paper, we study an unlearning-based approach to debiasing in LLMs by performing gradient ascent on hate speech against minority groups, i.e., minimizing the likelihood of biased or toxic content. Specifically, we propose a mask language modeling unlearning technique, which unlearns the harmful part of the text. This method enables LLMs to selectively forget and disassociate from biased and harmful content. Experimental results demonstrate the effectiveness of our approach in diminishing bias while maintaining the language modeling abilities. Surprisingly, the results also unveil an unexpected potential for cross-domain transfer unlearning: debiasing in one bias form (e.g. gender) may contribute to mitigating others (e.g. race and religion).",
            "corpus_id": 271404523,
            "sentences": [
                {
                    "corpus_id": "271404523",
                    "title": "Towards Transfer Unlearning: Empirical Evidence of Cross-Domain Bias Mitigation",
                    "text": "In recent years, the natural language processing (NLP) field has experienced a transformative shift with the introduction of Large Language Models (LLMs).The remarkable capabilities of LLMs are largely attributed to scaling laws (Kaplan et al., 2020), which suggest their capability heavily depends on the model size and training dataset size.However, training on massive corpus often results in LLMs inadvertently acquiring social biases present in their training datasets (Webster et al., 2021;Nangia et al., 2020;Nadeem et al., 2021).Therefore, addressing these biases is crucial for the development of fair and responsible LLMs.\n\nNumerous studies have been conducted to mitigate the bias and toxicity inherent in LLMs (Zhao et al., 2018;Barikeri et al., 2021;Liang et al., 2020;Ravfogel et al., 2020;Schick et al., 2021).However, recent studies (Meade et al., 2022) empiri-cally show that these approaches are effective in reducing bias while compromising the language modeling performance, as indicated by increased perplexity on unbiased text.Notably, existing posthoc techniques manage to retain language modeling performance, but they fail to detect more subtle and implicit toxic content.\n\nTo overcome this challenge, our study explores an unlearning-based approach that makes LLMs forget biased and toxic content.By running gradient ascent on biased text, our method minimizes the likelihood of biased content while minimizing the degradation of language modeling capabilities.Our debiasing method is inspired by the successes of prior work (Chen and Yang, 2023;Jang et al., 2023), which demonstrated the technique's efficacy in unlearning privacy-sensitive data.\n\nThis study explores Mask Language Modeling (MLM) unlearning, which selectively unlearns harmful content within the text by forgetting only toxic or biased tokens.Through empirical investigation on gender-biased text, we discovered that MLM unlearning effectively reduces gender bias without significantly deteriorating language modeling performance.Furthermore, experimental results demonstrated that unlearning gender-biased text also contributes to mitigating other types of bias, such as race and religion.",
                    "score": 0.572140928632923,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 154
                        },
                        {
                            "start": 154,
                            "end": 343
                        },
                        {
                            "start": 343,
                            "end": 537
                        },
                        {
                            "start": 537,
                            "end": 632
                        },
                        {
                            "start": 634,
                            "end": 825
                        },
                        {
                            "start": 825,
                            "end": 1049
                        },
                        {
                            "start": 1049,
                            "end": 1197
                        },
                        {
                            "start": 1199,
                            "end": 1323
                        },
                        {
                            "start": 1323,
                            "end": 1487
                        },
                        {
                            "start": 1487,
                            "end": 1673
                        },
                        {
                            "start": 1675,
                            "end": 1837
                        },
                        {
                            "start": 1837,
                            "end": 2024
                        },
                        {
                            "start": 2024,
                            "end": 2184
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 496,
                            "end": 516,
                            "matchedPaperCorpusId": "222090785"
                        },
                        {
                            "start": 516,
                            "end": 536,
                            "matchedPaperCorpusId": "215828184"
                        },
                        {
                            "start": 722,
                            "end": 741,
                            "matchedPaperCorpusId": "4952494"
                        },
                        {
                            "start": 741,
                            "end": 763,
                            "matchedPaperCorpusId": "235358955"
                        },
                        {
                            "start": 763,
                            "end": 782,
                            "matchedPaperCorpusId": "207996257"
                        },
                        {
                            "start": 782,
                            "end": 804,
                            "matchedPaperCorpusId": "215786522"
                        },
                        {
                            "start": 804,
                            "end": 824,
                            "matchedPaperCorpusId": "232075876"
                        },
                        {
                            "start": 849,
                            "end": 869,
                            "matchedPaperCorpusId": "239015827"
                        },
                        {
                            "start": 1551,
                            "end": 1572,
                            "matchedPaperCorpusId": "264828972"
                        },
                        {
                            "start": 1572,
                            "end": 1590,
                            "matchedPaperCorpusId": "252693065"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92041015625
                }
            ],
            "relevance_judgement": 0.92041015625,
            "relevance_judgment_input_expanded": "# Title: Towards Transfer Unlearning: Empirical Evidence of Cross-Domain Bias Mitigation\n# Venue: arXiv.org\n# Authors: Huimin Lu, Masaru Isonuma, Junichiro Mori, Ichiro Sakata\n## Abstract\nLarge language models (LLMs) often inherit biases from vast amounts of training corpora. Traditional debiasing methods, while effective to some extent, do not completely eliminate memorized biases and toxicity in LLMs. In this paper, we study an unlearning-based approach to debiasing in LLMs by performing gradient ascent on hate speech against minority groups, i.e., minimizing the likelihood of biased or toxic content. Specifically, we propose a mask language modeling unlearning technique, which unlearns the harmful part of the text. This method enables LLMs to selectively forget and disassociate from biased and harmful content. Experimental results demonstrate the effectiveness of our approach in diminishing bias while maintaining the language modeling abilities. Surprisingly, the results also unveil an unexpected potential for cross-domain transfer unlearning: debiasing in one bias form (e.g. gender) may contribute to mitigating others (e.g. race and religion).\n## Introduction\nIn recent years, the natural language processing (NLP) field has experienced a transformative shift with the introduction of Large Language Models (LLMs).The remarkable capabilities of LLMs are largely attributed to scaling laws (Kaplan et al., 2020), which suggest their capability heavily depends on the model size and training dataset size.However, training on massive corpus often results in LLMs inadvertently acquiring social biases present in their training datasets (Webster et al., 2021;Nangia et al., 2020;Nadeem et al., 2021).Therefore, addressing these biases is crucial for the development of fair and responsible LLMs.\n\nNumerous studies have been conducted to mitigate the bias and toxicity inherent in LLMs (Zhao et al., 2018;Barikeri et al., 2021;Liang et al., 2020;Ravfogel et al., 2020;Schick et al., 2021).However, recent studies (Meade et al., 2022) empiri-cally show that these approaches are effective in reducing bias while compromising the language modeling performance, as indicated by increased perplexity on unbiased text.Notably, existing posthoc techniques manage to retain language modeling performance, but they fail to detect more subtle and implicit toxic content.\n\nTo overcome this challenge, our study explores an unlearning-based approach that makes LLMs forget biased and toxic content.By running gradient ascent on biased text, our method minimizes the likelihood of biased content while minimizing the degradation of language modeling capabilities.Our debiasing method is inspired by the successes of prior work (Chen and Yang, 2023;Jang et al., 2023), which demonstrated the technique's efficacy in unlearning privacy-sensitive data.\n\nThis study explores Mask Language Modeling (MLM) unlearning, which selectively unlearns harmful content within the text by forgetting only toxic or biased tokens.Through empirical investigation on gender-biased text, we discovered that MLM unlearning effectively reduces gender bias without significantly deteriorating language modeling performance.Furthermore, experimental results demonstrated that unlearning gender-biased text also contributes to mitigating other types of bias, such as race and religion.",
            "reference_string": "[271404523 | Lu et al. | 2024 | Citations: 1]"
        },
        {
            "title": "De-amplifying Bias from Differential Privacy in Language Model Fine-tuning",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 32,
            "citation_count": 2,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.04489, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2283141928",
                    "name": "Sanjari Srivastava"
                },
                {
                    "authorId": "3251561",
                    "name": "Piotr (Peter) Mardziel"
                },
                {
                    "authorId": "2283145557",
                    "name": "Zhikhun Zhang"
                },
                {
                    "authorId": "2283137127",
                    "name": "Archana Ahlawat"
                },
                {
                    "authorId": "33374965",
                    "name": "Anupam Datta"
                },
                {
                    "authorId": "2283264477",
                    "name": "John C. Mitchell"
                }
            ],
            "abstract": "Fairness and privacy are two important values machine learning (ML) practitioners often seek to operationalize in models. Fairness aims to reduce model bias for social/demographic sub-groups. Privacy via differential privacy (DP) mechanisms, on the other hand, limits the impact of any individual's training data on the resulting model. The trade-offs between privacy and fairness goals of trustworthy ML pose a challenge to those wishing to address both. We show that DP amplifies gender, racial, and religious bias when fine-tuning large language models (LLMs), producing models more biased than ones fine-tuned without DP. We find the cause of the amplification to be a disparity in convergence of gradients across sub-groups. Through the case of binary gender bias, we demonstrate that Counterfactual Data Augmentation (CDA), a known method for addressing bias, also mitigates bias amplification by DP. As a consequence, DP and CDA together can be used to fine-tune models while maintaining both fairness and privacy.",
            "corpus_id": 267523377,
            "sentences": [],
            "relevance_judgement": 0.91748046875,
            "relevance_judgment_input_expanded": "# Title: De-amplifying Bias from Differential Privacy in Language Model Fine-tuning\n# Venue: arXiv.org\n# Authors: Sanjari Srivastava, Piotr (Peter) Mardziel, Zhikhun Zhang, Archana Ahlawat, Anupam Datta, John C. Mitchell\n## Abstract\nFairness and privacy are two important values machine learning (ML) practitioners often seek to operationalize in models. Fairness aims to reduce model bias for social/demographic sub-groups. Privacy via differential privacy (DP) mechanisms, on the other hand, limits the impact of any individual's training data on the resulting model. The trade-offs between privacy and fairness goals of trustworthy ML pose a challenge to those wishing to address both. We show that DP amplifies gender, racial, and religious bias when fine-tuning large language models (LLMs), producing models more biased than ones fine-tuned without DP. We find the cause of the amplification to be a disparity in convergence of gradients across sub-groups. Through the case of binary gender bias, we demonstrate that Counterfactual Data Augmentation (CDA), a known method for addressing bias, also mitigates bias amplification by DP. As a consequence, DP and CDA together can be used to fine-tune models while maintaining both fairness and privacy.\n",
            "reference_string": "[267523377 | Srivastava et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Enforcing Consistency and Fairness in Multi-level Hierarchical Classification with a Mask-based Output Layer",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 45,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.15566, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2301225724",
                    "name": "Shijing Chen"
                },
                {
                    "authorId": "38797620",
                    "name": "Shoaib Jameel"
                },
                {
                    "authorId": "3080469",
                    "name": "Mohamed Reda Bouadjenek"
                },
                {
                    "authorId": "2338268075",
                    "name": "Feilong Tang"
                },
                {
                    "authorId": "2301210198",
                    "name": "Usman Naseem"
                },
                {
                    "authorId": "2260341001",
                    "name": "Basem Suleiman"
                },
                {
                    "authorId": "2296782788",
                    "name": "Hakim Hacid"
                },
                {
                    "authorId": "2253483164",
                    "name": "Flora D. Salim"
                },
                {
                    "authorId": "2240180680",
                    "name": "Imran Razzak"
                }
            ],
            "abstract": "Traditional Multi-level Hierarchical Classification (MLHC) classifiers often rely on backbone models with $n$ independent output layers. This structure tends to overlook the hierarchical relationships between classes, leading to inconsistent predictions that violate the underlying taxonomy. Additionally, once a backbone architecture for an MLHC classifier is selected, adapting the model to accommodate new tasks can be challenging. For example, incorporating fairness to protect sensitive attributes within a hierarchical classifier necessitates complex adjustments to maintain the class hierarchy while enforcing fairness constraints. In this paper, we extend this concept to hierarchical classification by introducing a fair, model-agnostic layer designed to enforce taxonomy and optimize specific objectives, including consistency, fairness, and exact match. Our evaluations demonstrate that the proposed layer not only improves the fairness of predictions but also enforces the taxonomy, resulting in consistent predictions and superior performance. Compared to Large Language Models (LLMs) employing in-processing de-biasing techniques and models without any bias correction, our approach achieves better outcomes in both fairness and accuracy, making it particularly valuable in sectors like e-commerce, healthcare, and education, where predictive reliability is crucial.",
            "corpus_id": 277150794,
            "sentences": [
                {
                    "corpus_id": "277150794",
                    "title": "Enforcing Consistency and Fairness in Multi-level Hierarchical Classification with a Mask-based Output Layer",
                    "text": "Recent work has focused on mitigating intrinsic bias during pretraining and in-processing stages, using various techniques such as Counterfactual Data Augmentation (CDA), Context-debias, and Sent-debias. For instance, CDA balances representation by swapping demographic-specific terms (e.g., \"he\" and \"she\") in the training data, though it is resource-intensive due to the need for retraining (Zmigrod et al., 2019;Webster et al., 2020). In-processing methods like Context-debias attempt to remove bias by ensuring that embeddings of stereotypical terms are orthogonal to gender-related terms, but they depend heavily on predefined word lists, limiting their generalizability (Kaneko and Bollegala, 2021). Post-processing methods such as Sent-debias work by removing gender bias from pretrained model embeddings, though research suggests that these methods often obscure rather than fully eliminate bias (Liang et al., 2020;Gonen and Goldberg, 2019). \n\nDespite significant advancements in integrating deep learning techniques for tasks involving multilevel taxonomies, challenges persist in scaling models to handle large, complex hierarchies consistently and fairly. Existing methods often struggle to maintain consistency across deep hierarchies. Also, upstream debiasing techniques frequently do not translate into improved fairness in downstream tasks-especially in complex scenarios like MLHC where preserving class hierarchy is crucial (Steed et al., 2022). To tackle these issues, we propose the Debiased Taxonomy-based Transitional Classifier (D-TTC), which embeds hierarchical information directly into the classification process, leverages LLMs for better contextual understanding, and uses downstream post-processing debiasing through dynamic reweighting which adjusts the importance of different samples during training. As a modelagnostic layer, D-TTC enhances both flexibility and performance across various backbone models, providing a more consistent solution for complex hierarchies. Additionally, it enables the model to address bias more effectively within specific application domains, ensuring improved fairness along-side high performance.",
                    "score": 0.5312744870847811,
                    "section_title": "Related Work",
                    "char_start_offset": 11542,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 203
                        },
                        {
                            "start": 204,
                            "end": 437
                        },
                        {
                            "start": 438,
                            "end": 705
                        },
                        {
                            "start": 706,
                            "end": 950
                        },
                        {
                            "start": 953,
                            "end": 1167
                        },
                        {
                            "start": 1168,
                            "end": 1248
                        },
                        {
                            "start": 1249,
                            "end": 1463
                        },
                        {
                            "start": 1464,
                            "end": 1832
                        },
                        {
                            "start": 1833,
                            "end": 2000
                        },
                        {
                            "start": 2001,
                            "end": 2161
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1442,
                            "end": 1462,
                            "matchedPaperCorpusId": "248780439"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9091796875
                }
            ],
            "relevance_judgement": 0.9091796875,
            "relevance_judgment_input_expanded": "# Title: Enforcing Consistency and Fairness in Multi-level Hierarchical Classification with a Mask-based Output Layer\n# Venue: arXiv.org\n# Authors: Shijing Chen, Shoaib Jameel, Mohamed Reda Bouadjenek, Feilong Tang, Usman Naseem, Basem Suleiman, Hakim Hacid, Flora D. Salim, Imran Razzak\n## Abstract\nTraditional Multi-level Hierarchical Classification (MLHC) classifiers often rely on backbone models with $n$ independent output layers. This structure tends to overlook the hierarchical relationships between classes, leading to inconsistent predictions that violate the underlying taxonomy. Additionally, once a backbone architecture for an MLHC classifier is selected, adapting the model to accommodate new tasks can be challenging. For example, incorporating fairness to protect sensitive attributes within a hierarchical classifier necessitates complex adjustments to maintain the class hierarchy while enforcing fairness constraints. In this paper, we extend this concept to hierarchical classification by introducing a fair, model-agnostic layer designed to enforce taxonomy and optimize specific objectives, including consistency, fairness, and exact match. Our evaluations demonstrate that the proposed layer not only improves the fairness of predictions but also enforces the taxonomy, resulting in consistent predictions and superior performance. Compared to Large Language Models (LLMs) employing in-processing de-biasing techniques and models without any bias correction, our approach achieves better outcomes in both fairness and accuracy, making it particularly valuable in sectors like e-commerce, healthcare, and education, where predictive reliability is crucial.\n## Related Work\nRecent work has focused on mitigating intrinsic bias during pretraining and in-processing stages, using various techniques such as Counterfactual Data Augmentation (CDA), Context-debias, and Sent-debias. For instance, CDA balances representation by swapping demographic-specific terms (e.g., \"he\" and \"she\") in the training data, though it is resource-intensive due to the need for retraining (Zmigrod et al., 2019;Webster et al., 2020). In-processing methods like Context-debias attempt to remove bias by ensuring that embeddings of stereotypical terms are orthogonal to gender-related terms, but they depend heavily on predefined word lists, limiting their generalizability (Kaneko and Bollegala, 2021). Post-processing methods such as Sent-debias work by removing gender bias from pretrained model embeddings, though research suggests that these methods often obscure rather than fully eliminate bias (Liang et al., 2020;Gonen and Goldberg, 2019). \n\nDespite significant advancements in integrating deep learning techniques for tasks involving multilevel taxonomies, challenges persist in scaling models to handle large, complex hierarchies consistently and fairly. Existing methods often struggle to maintain consistency across deep hierarchies. Also, upstream debiasing techniques frequently do not translate into improved fairness in downstream tasks-especially in complex scenarios like MLHC where preserving class hierarchy is crucial (Steed et al., 2022). To tackle these issues, we propose the Debiased Taxonomy-based Transitional Classifier (D-TTC), which embeds hierarchical information directly into the classification process, leverages LLMs for better contextual understanding, and uses downstream post-processing debiasing through dynamic reweighting which adjusts the importance of different samples during training. As a modelagnostic layer, D-TTC enhances both flexibility and performance across various backbone models, providing a more consistent solution for complex hierarchies. Additionally, it enables the model to address bias more effectively within specific application domains, ensuring improved fairness along-side high performance.",
            "reference_string": "[277150794 | Chen et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Prompt Tuning Pushes Farther, Contrastive Learning Pulls Closer: A Two-Stage Approach to Mitigate Social Biases",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2023,
            "reference_count": 46,
            "citation_count": 30,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2307.01595",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2307.01595, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": null,
                    "name": "Yingji Li"
                },
                {
                    "authorId": "3432460",
                    "name": "Mengnan Du"
                },
                {
                    "authorId": "2153687737",
                    "name": "Xin Wang"
                },
                {
                    "authorId": "49416173",
                    "name": "Y. Wang"
                }
            ],
            "abstract": "As the representation capability of Pre-trained Language Models (PLMs) improve, there is growing concern that they will inherit social biases from unprocessed corpora. Most previous debiasing techniques used Counterfactual Data Augmentation (CDA) to balance the training corpus. However, CDA slightly modifies the original corpus, limiting the representation distance between different demographic groups to a narrow range. As a result, the debiasing model easily fits the differences between counterfactual pairs, which affects its debiasing performance with limited text resources. In this paper, we propose an adversarial training-inspired two-stage debiasing model using Contrastive learning with Continuous Prompt Augmentation (named CCPA) to mitigate social biases in PLMs\u2019 encoding. In the first stage, we propose a data augmentation method based on continuous prompt tuning to push farther the representation distance between sample pairs along different demographic groups. In the second stage, we utilize contrastive learning to pull closer the representation distance between the augmented sample pairs and then fine-tune PLMs\u2019 parameters to get debiased encoding. Our approach guides the model to achieve stronger debiasing performance by adding difficulty to the training process. Extensive experiments show that CCPA outperforms baselines in terms of debiasing performance. Meanwhile, experimental results on the GLUE benchmark show that CCPA retains the language modeling capability of PLMs.",
            "corpus_id": 259342087,
            "sentences": [
                {
                    "corpus_id": "259342087",
                    "title": "Prompt Tuning Pushes Farther, Contrastive Learning Pulls Closer: A Two-Stage Approach to Mitigate Social Biases",
                    "text": "Pre-trained Language Models (PLMs) have demonstrated outstanding performance in recent years and have been widely used in natural language understanding tasks (Peters et al., 2018;Delobelle et al., 2022). However, the powerful language modeling capability enables PLMs to learn good representations from large-scale training corpora while capturing human-like social biases. Recent studies have demonstrated that the representations encoded by PLMs learn social biases specific to demographic groups (e.g., gender, race, religion) and can be amplified to downstream tasks, leading to unfair outcomes and adverse social effects (Zhao et al., 2019;Webster et al., 2020). As a result, mitigating social biases in PLMs' encoding can improve the fairness of NLP systems significantly (Bolukbasi et al., 2016;Bender and Friedman, 2018).\n\nMost existing debiasing techniques first need to construct sample pairs using Counterfactual Data Augmentation (CDA) (Zmigrod et al., 2019;Wang et al., 2022) to balance the training corpora. The general approach of CDA is to replace the original corpus with attribute words (e.g., he/she, man/woman) specific to different demographic groups. For example, RCDA (Chen et al., 2021) uses a generator to generate a large number of antisense sentences and then uses a discriminator to evaluate the quality of the original and antisense samples jointly. FairFil (Cheng et al., 2021) obtains a pair of positive sample sentences by replacing the attribute words in the training corpora with the antonyms and then uses contrastive learning to train a filter for debiasing. Auto-Debias (Guo et al., 2022) uses pairs of attribute words as training corpora, amplifies the bias between sample pairs by searching biased prompt texts in the Wikipedia vocabulary, and then performs semantic alignment using Jensen-Shannon divergence. These methods aim to mitigate social biases between different demographic groups by narrowing the representation distance between sample pairs. However, CDA slightly modifies the original corpus, limiting the representation distance between different demographic groups to a narrow range. As a result, the debiasing model is easy to overfit the",
                    "score": 0.5514526048520503,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 159,
                            "end": 180,
                            "matchedPaperCorpusId": "3626819"
                        },
                        {
                            "start": 180,
                            "end": 203,
                            "matchedPaperCorpusId": "250390561"
                        },
                        {
                            "start": 627,
                            "end": 646,
                            "matchedPaperCorpusId": "102352962"
                        },
                        {
                            "start": 779,
                            "end": 803,
                            "matchedPaperCorpusId": "1704893"
                        },
                        {
                            "start": 803,
                            "end": 829,
                            "matchedPaperCorpusId": "52255687"
                        },
                        {
                            "start": 949,
                            "end": 971,
                            "matchedPaperCorpusId": "184486914"
                        },
                        {
                            "start": 971,
                            "end": 989,
                            "matchedPaperCorpusId": "247155039"
                        },
                        {
                            "start": 1388,
                            "end": 1408,
                            "matchedPaperCorpusId": "232185104"
                        },
                        {
                            "start": 1608,
                            "end": 1626,
                            "matchedPaperCorpusId": "248780440"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9072265625
                }
            ],
            "relevance_judgement": 0.9072265625,
            "relevance_judgment_input_expanded": "# Title: Prompt Tuning Pushes Farther, Contrastive Learning Pulls Closer: A Two-Stage Approach to Mitigate Social Biases\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Yingji Li, Mengnan Du, Xin Wang, Y. Wang\n## Abstract\nAs the representation capability of Pre-trained Language Models (PLMs) improve, there is growing concern that they will inherit social biases from unprocessed corpora. Most previous debiasing techniques used Counterfactual Data Augmentation (CDA) to balance the training corpus. However, CDA slightly modifies the original corpus, limiting the representation distance between different demographic groups to a narrow range. As a result, the debiasing model easily fits the differences between counterfactual pairs, which affects its debiasing performance with limited text resources. In this paper, we propose an adversarial training-inspired two-stage debiasing model using Contrastive learning with Continuous Prompt Augmentation (named CCPA) to mitigate social biases in PLMs\u2019 encoding. In the first stage, we propose a data augmentation method based on continuous prompt tuning to push farther the representation distance between sample pairs along different demographic groups. In the second stage, we utilize contrastive learning to pull closer the representation distance between the augmented sample pairs and then fine-tune PLMs\u2019 parameters to get debiased encoding. Our approach guides the model to achieve stronger debiasing performance by adding difficulty to the training process. Extensive experiments show that CCPA outperforms baselines in terms of debiasing performance. Meanwhile, experimental results on the GLUE benchmark show that CCPA retains the language modeling capability of PLMs.\n## Introduction\nPre-trained Language Models (PLMs) have demonstrated outstanding performance in recent years and have been widely used in natural language understanding tasks (Peters et al., 2018;Delobelle et al., 2022). However, the powerful language modeling capability enables PLMs to learn good representations from large-scale training corpora while capturing human-like social biases. Recent studies have demonstrated that the representations encoded by PLMs learn social biases specific to demographic groups (e.g., gender, race, religion) and can be amplified to downstream tasks, leading to unfair outcomes and adverse social effects (Zhao et al., 2019;Webster et al., 2020). As a result, mitigating social biases in PLMs' encoding can improve the fairness of NLP systems significantly (Bolukbasi et al., 2016;Bender and Friedman, 2018).\n\nMost existing debiasing techniques first need to construct sample pairs using Counterfactual Data Augmentation (CDA) (Zmigrod et al., 2019;Wang et al., 2022) to balance the training corpora. The general approach of CDA is to replace the original corpus with attribute words (e.g., he/she, man/woman) specific to different demographic groups. For example, RCDA (Chen et al., 2021) uses a generator to generate a large number of antisense sentences and then uses a discriminator to evaluate the quality of the original and antisense samples jointly. FairFil (Cheng et al., 2021) obtains a pair of positive sample sentences by replacing the attribute words in the training corpora with the antonyms and then uses contrastive learning to train a filter for debiasing. Auto-Debias (Guo et al., 2022) uses pairs of attribute words as training corpora, amplifies the bias between sample pairs by searching biased prompt texts in the Wikipedia vocabulary, and then performs semantic alignment using Jensen-Shannon divergence. These methods aim to mitigate social biases between different demographic groups by narrowing the representation distance between sample pairs. However, CDA slightly modifies the original corpus, limiting the representation distance between different demographic groups to a narrow range. As a result, the debiasing model is easy to overfit the",
            "reference_string": "[259342087 | Li et al. | 2023 | Citations: 30]"
        },
        {
            "title": "Survey of Social Bias in Vision-Language Models",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 203,
            "citation_count": 18,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2309.14381",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.14381, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "40221187",
                    "name": "Nayeon Lee"
                },
                {
                    "authorId": "23672613",
                    "name": "Yejin Bang"
                },
                {
                    "authorId": "116344405",
                    "name": "Holy Lovenia"
                },
                {
                    "authorId": "66986482",
                    "name": "Samuel Cahyawijaya"
                },
                {
                    "authorId": "47653392",
                    "name": "Wenliang Dai"
                },
                {
                    "authorId": "1683412",
                    "name": "Pascale Fung"
                }
            ],
            "abstract": "In recent years, the rapid advancement of machine learning (ML) models, particularly transformer-based pre-trained models, has revolutionized Natural Language Processing (NLP) and Computer Vision (CV) fields. However, researchers have discovered that these models can inadvertently capture and reinforce social biases present in their training datasets, leading to potential social harms, such as uneven resource allocation and unfair representation of specific social groups. Addressing these biases and ensuring fairness in artificial intelligence (AI) systems has become a critical concern in the ML community. The recent introduction of pre-trained vision-and-language (VL) models in the emerging multimodal field demands attention to the potential social biases present in these models as well. Although VL models are susceptible to social bias, there is a limited understanding compared to the extensive discussions on bias in NLP and CV. This survey aims to provide researchers with a high-level insight into the similarities and differences of social bias studies in pre-trained models across NLP, CV, and VL. By examining these perspectives, the survey aims to offer valuable guidelines on how to approach and mitigate social bias in both unimodal and multimodal settings. The findings and recommendations presented here can benefit the ML community, fostering the development of fairer and non-biased AI models in various applications and research endeavors.",
            "corpus_id": 262828449,
            "sentences": [
                {
                    "corpus_id": "262828449",
                    "title": "Survey of Social Bias in Vision-Language Models",
                    "text": "Post-processing Methods. \n\n\u2022 Feature Clipping. Wang et al. [174] propose a post-processing mitigation method that clips/prunes the dimensions of feature embeddings that are highly correlated with gender information. This idea is motivated by the fact that an unbiased retrieve implies the independence between the covariates (active features) and sensitive attributes (gender). However, the limitation of this approach is that it results in unavoidable performance degradation in the main task performance. \u2022 Instruction/Prompt Engineering. Friedrich et al. [62] propose a post-processing mitigation approach, Fair Diffusion, that is inspired by advances in instructing AI systems based on human feedback. Fair Diffusion \"instructs\" the pre-trained diffusion models to be fair during the deployment stage; it enables precise guidance to reduce biases in model outcomes based on pre-defined instructions stored in a lookup table. \u2022 Bias vector projection. Chuang et al. [38] de-bias VL foundation models by projecting out biased directions (gender bias direction, racial bias direction) in the text embedding. These bias directions are obtained from embeddings of spurious prompts such as \"a photo of a [irrelevant attribute]\". The authors also calibrate the projection matrix by introducing one additional regularization constraint that ensures the debiased prompt representation still has the same semantic meaning after the projection. To elaborate, \"a photo of a [class name] with [spurious attribute]\" should still have the same semantic meaning as \"a photo of a [class name]\". The biggest advantage of this method is that it is computationally lightweight and simple, as it only manipulates text embedding. However, we believe further exploration should be conducted to ensure that such an unimodal mitigation method can truly mitigate multimodal bias. \n\n4.5 Future Direction 4.5.1 Bias Evaluation and Analysis. \n\n\u2022 Choice of VLP architecture. As explained in Section 4.2, there are different architectural choices (i.e., unified, dual-stream, and encoder-decoder) that have their own strengths and weaknesses. It would be insightful to analyze the impact of VLP architectural design on social bias.",
                    "score": 0.5099980368940413,
                    "section_title": "4.4.2",
                    "char_start_offset": 67608,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 24
                        },
                        {
                            "start": 27,
                            "end": 46
                        },
                        {
                            "start": 47,
                            "end": 215
                        },
                        {
                            "start": 216,
                            "end": 377
                        },
                        {
                            "start": 378,
                            "end": 506
                        },
                        {
                            "start": 507,
                            "end": 540
                        },
                        {
                            "start": 541,
                            "end": 705
                        },
                        {
                            "start": 706,
                            "end": 928
                        },
                        {
                            "start": 929,
                            "end": 954
                        },
                        {
                            "start": 955,
                            "end": 1108
                        },
                        {
                            "start": 1109,
                            "end": 1226
                        },
                        {
                            "start": 1227,
                            "end": 1437
                        },
                        {
                            "start": 1438,
                            "end": 1581
                        },
                        {
                            "start": 1582,
                            "end": 1711
                        },
                        {
                            "start": 1712,
                            "end": 1857
                        },
                        {
                            "start": 1860,
                            "end": 1916
                        },
                        {
                            "start": 1919,
                            "end": 1948
                        },
                        {
                            "start": 1949,
                            "end": 2115
                        },
                        {
                            "start": 2116,
                            "end": 2204
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 59,
                            "end": 64,
                            "matchedPaperCorpusId": "237490811"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90625
                }
            ],
            "relevance_judgement": 0.90625,
            "relevance_judgment_input_expanded": "# Title: Survey of Social Bias in Vision-Language Models\n# Venue: arXiv.org\n# Authors: Nayeon Lee, Yejin Bang, Holy Lovenia, Samuel Cahyawijaya, Wenliang Dai, Pascale Fung\n## Abstract\nIn recent years, the rapid advancement of machine learning (ML) models, particularly transformer-based pre-trained models, has revolutionized Natural Language Processing (NLP) and Computer Vision (CV) fields. However, researchers have discovered that these models can inadvertently capture and reinforce social biases present in their training datasets, leading to potential social harms, such as uneven resource allocation and unfair representation of specific social groups. Addressing these biases and ensuring fairness in artificial intelligence (AI) systems has become a critical concern in the ML community. The recent introduction of pre-trained vision-and-language (VL) models in the emerging multimodal field demands attention to the potential social biases present in these models as well. Although VL models are susceptible to social bias, there is a limited understanding compared to the extensive discussions on bias in NLP and CV. This survey aims to provide researchers with a high-level insight into the similarities and differences of social bias studies in pre-trained models across NLP, CV, and VL. By examining these perspectives, the survey aims to offer valuable guidelines on how to approach and mitigate social bias in both unimodal and multimodal settings. The findings and recommendations presented here can benefit the ML community, fostering the development of fairer and non-biased AI models in various applications and research endeavors.\n## 4.4.2\nPost-processing Methods. \n\n\u2022 Feature Clipping. Wang et al. [174] propose a post-processing mitigation method that clips/prunes the dimensions of feature embeddings that are highly correlated with gender information. This idea is motivated by the fact that an unbiased retrieve implies the independence between the covariates (active features) and sensitive attributes (gender). However, the limitation of this approach is that it results in unavoidable performance degradation in the main task performance. \u2022 Instruction/Prompt Engineering. Friedrich et al. [62] propose a post-processing mitigation approach, Fair Diffusion, that is inspired by advances in instructing AI systems based on human feedback. Fair Diffusion \"instructs\" the pre-trained diffusion models to be fair during the deployment stage; it enables precise guidance to reduce biases in model outcomes based on pre-defined instructions stored in a lookup table. \u2022 Bias vector projection. Chuang et al. [38] de-bias VL foundation models by projecting out biased directions (gender bias direction, racial bias direction) in the text embedding. These bias directions are obtained from embeddings of spurious prompts such as \"a photo of a [irrelevant attribute]\". The authors also calibrate the projection matrix by introducing one additional regularization constraint that ensures the debiased prompt representation still has the same semantic meaning after the projection. To elaborate, \"a photo of a [class name] with [spurious attribute]\" should still have the same semantic meaning as \"a photo of a [class name]\". The biggest advantage of this method is that it is computationally lightweight and simple, as it only manipulates text embedding. However, we believe further exploration should be conducted to ensure that such an unimodal mitigation method can truly mitigate multimodal bias. \n\n4.5 Future Direction 4.5.1 Bias Evaluation and Analysis. \n\n\u2022 Choice of VLP architecture. As explained in Section 4.2, there are different architectural choices (i.e., unified, dual-stream, and encoder-decoder) that have their own strengths and weaknesses. It would be insightful to analyze the impact of VLP architectural design on social bias.",
            "reference_string": "[262828449 | Lee et al. | 2023 | Citations: 18]"
        },
        {
            "title": "Potential and Challenges of Model Editing for Social Debiasing",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 55,
            "citation_count": 9,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.13462, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "134233854",
                    "name": "Jianhao Yan"
                },
                {
                    "authorId": "2285582246",
                    "name": "Futing Wang"
                },
                {
                    "authorId": "2110450452",
                    "name": "Yafu Li"
                },
                {
                    "authorId": "2249762135",
                    "name": "Yue Zhang"
                }
            ],
            "abstract": "Large language models (LLMs) trained on vast corpora suffer from inevitable stereotype biases. Mitigating these biases with fine-tuning could be both costly and data-hungry. Model editing methods, which focus on modifying LLMs in a post-hoc manner, are of great potential to address debiasing. However, it lacks a comprehensive study that facilitates both internal and external model editing methods, supports various bias types, as well as understands the pros and cons of applying editing methods to stereotypical debiasing. To mitigate this gap, we carefully formulate social debiasing into an editing problem and benchmark seven existing model editing algorithms on stereotypical debiasing, i.e., debias editing. Our findings in three scenarios reveal both the potential and challenges of debias editing: (1) Existing model editing methods can effectively preserve knowledge and mitigate biases, while the generalization of debias effect from edited sentences to semantically equivalent sentences is limited.(2) Sequential editing highlights the robustness of SERAC (Mitchell et al. 2022b), while internal editing methods degenerate with the number of edits. (3) Model editing algorithms achieve generalization towards unseen biases both within the same type and from different types. In light of these findings, we further propose two simple but effective methods to improve debias editing, and experimentally show the effectiveness of the proposed methods.",
            "corpus_id": 267770177,
            "sentences": [
                {
                    "corpus_id": "267770177",
                    "title": "Potential and Challenges of Model Editing for Social Debiasing",
                    "text": "Debiasing Addressing social bias in language models is an ongoing challenge that has received significant attention. Strategies for mitigating bias in language models can be classified based on different stages of the model workflow: Preprocessing techniques aim to detect and eliminate bias and unfairness early on, either within the dataset (Zmigrod et al., 2019;Dinan et al., 2020;Abid et al., 2021;Qian et al., 2022a;Ghanbarzadeh et al., 2023) or prompt (Mattern et al., 2022;Fatemi et al., 2023;Yang et al., 2023). In-training bias mitigation techniques focus on reducing bias and unfairness during model training, by adjusting model architecture (Bartl et al., 2020;Han et al., 2022), modifying loss functions (Liu et al., 2020;Webster et al., 2020;Ouyang et al., 2022;Woo et al., 2023;Park et al., 2023;Zhou et al., 2023;Li et al., 2023), or selectively updating parameters (Qian et al., 2022b;Ranaldi et al., 2023;Yu et al., 2023). \n\nIntraprocessing approaches alter decoding behavior (Saunders et al., 2022;Meade et al., 2023;Kim et al., 2023;Chung et al., 2023;Hallinan et al., 2023) without additional training or fine-tuning. \n\nPost-processing techniques primarily adjust model outputs to address bias and unfairness, without directly accessing the model itself (He et al., 2021;Tokpo and Calders, 2022;Majumder et al., 2022;Dhingra et al., 2023). However, effectively modifying bias in pre-trained large language models while minimizing disruption to the model's capabilities remains largely unexplored. \n\nModel Editing To address inaccuracies and biases in Large Language Models, various model editing techniques have been developed for efficient post-training adjustments.",
                    "score": 0.5585385263552101,
                    "section_title": "Related Work",
                    "char_start_offset": 4897,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 116
                        },
                        {
                            "start": 117,
                            "end": 519
                        },
                        {
                            "start": 520,
                            "end": 939
                        },
                        {
                            "start": 942,
                            "end": 1137
                        },
                        {
                            "start": 1140,
                            "end": 1359
                        },
                        {
                            "start": 1360,
                            "end": 1516
                        },
                        {
                            "start": 1519,
                            "end": 1687
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 343,
                            "end": 365,
                            "matchedPaperCorpusId": "184486914"
                        },
                        {
                            "start": 365,
                            "end": 384,
                            "matchedPaperCorpusId": "207852875"
                        },
                        {
                            "start": 402,
                            "end": 421,
                            "matchedPaperCorpusId": "249062690"
                        },
                        {
                            "start": 421,
                            "end": 447,
                            "matchedPaperCorpusId": "259859044"
                        },
                        {
                            "start": 480,
                            "end": 500,
                            "matchedPaperCorpusId": "238582879"
                        },
                        {
                            "start": 500,
                            "end": 518,
                            "matchedPaperCorpusId": "253446867"
                        },
                        {
                            "start": 652,
                            "end": 672,
                            "matchedPaperCorpusId": "225094152"
                        },
                        {
                            "start": 672,
                            "end": 689,
                            "matchedPaperCorpusId": "247694107"
                        },
                        {
                            "start": 716,
                            "end": 734,
                            "matchedPaperCorpusId": "204838020"
                        },
                        {
                            "start": 755,
                            "end": 775,
                            "matchedPaperCorpusId": "246426909"
                        },
                        {
                            "start": 775,
                            "end": 792,
                            "matchedPaperCorpusId": "258537309"
                        },
                        {
                            "start": 792,
                            "end": 810,
                            "matchedPaperCorpusId": "257079734"
                        },
                        {
                            "start": 810,
                            "end": 828,
                            "matchedPaperCorpusId": "259370743"
                        },
                        {
                            "start": 828,
                            "end": 844,
                            "matchedPaperCorpusId": "259342087"
                        },
                        {
                            "start": 881,
                            "end": 901,
                            "matchedPaperCorpusId": "249062690"
                        },
                        {
                            "start": 922,
                            "end": 938,
                            "matchedPaperCorpusId": "259859034"
                        },
                        {
                            "start": 993,
                            "end": 1016,
                            "matchedPaperCorpusId": "233240748"
                        },
                        {
                            "start": 1035,
                            "end": 1052,
                            "matchedPaperCorpusId": "254926596"
                        },
                        {
                            "start": 1052,
                            "end": 1071,
                            "matchedPaperCorpusId": "259096160"
                        },
                        {
                            "start": 1071,
                            "end": 1093,
                            "matchedPaperCorpusId": "252734135"
                        },
                        {
                            "start": 1274,
                            "end": 1291,
                            "matchedPaperCorpusId": "237634972"
                        },
                        {
                            "start": 1291,
                            "end": 1315,
                            "matchedPaperCorpusId": "246210255"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90380859375
                }
            ],
            "relevance_judgement": 0.90380859375,
            "relevance_judgment_input_expanded": "# Title: Potential and Challenges of Model Editing for Social Debiasing\n# Venue: arXiv.org\n# Authors: Jianhao Yan, Futing Wang, Yafu Li, Yue Zhang\n## Abstract\nLarge language models (LLMs) trained on vast corpora suffer from inevitable stereotype biases. Mitigating these biases with fine-tuning could be both costly and data-hungry. Model editing methods, which focus on modifying LLMs in a post-hoc manner, are of great potential to address debiasing. However, it lacks a comprehensive study that facilitates both internal and external model editing methods, supports various bias types, as well as understands the pros and cons of applying editing methods to stereotypical debiasing. To mitigate this gap, we carefully formulate social debiasing into an editing problem and benchmark seven existing model editing algorithms on stereotypical debiasing, i.e., debias editing. Our findings in three scenarios reveal both the potential and challenges of debias editing: (1) Existing model editing methods can effectively preserve knowledge and mitigate biases, while the generalization of debias effect from edited sentences to semantically equivalent sentences is limited.(2) Sequential editing highlights the robustness of SERAC (Mitchell et al. 2022b), while internal editing methods degenerate with the number of edits. (3) Model editing algorithms achieve generalization towards unseen biases both within the same type and from different types. In light of these findings, we further propose two simple but effective methods to improve debias editing, and experimentally show the effectiveness of the proposed methods.\n## Related Work\nDebiasing Addressing social bias in language models is an ongoing challenge that has received significant attention. Strategies for mitigating bias in language models can be classified based on different stages of the model workflow: Preprocessing techniques aim to detect and eliminate bias and unfairness early on, either within the dataset (Zmigrod et al., 2019;Dinan et al., 2020;Abid et al., 2021;Qian et al., 2022a;Ghanbarzadeh et al., 2023) or prompt (Mattern et al., 2022;Fatemi et al., 2023;Yang et al., 2023). In-training bias mitigation techniques focus on reducing bias and unfairness during model training, by adjusting model architecture (Bartl et al., 2020;Han et al., 2022), modifying loss functions (Liu et al., 2020;Webster et al., 2020;Ouyang et al., 2022;Woo et al., 2023;Park et al., 2023;Zhou et al., 2023;Li et al., 2023), or selectively updating parameters (Qian et al., 2022b;Ranaldi et al., 2023;Yu et al., 2023). \n\nIntraprocessing approaches alter decoding behavior (Saunders et al., 2022;Meade et al., 2023;Kim et al., 2023;Chung et al., 2023;Hallinan et al., 2023) without additional training or fine-tuning. \n\nPost-processing techniques primarily adjust model outputs to address bias and unfairness, without directly accessing the model itself (He et al., 2021;Tokpo and Calders, 2022;Majumder et al., 2022;Dhingra et al., 2023). However, effectively modifying bias in pre-trained large language models while minimizing disruption to the model's capabilities remains largely unexplored. \n\nModel Editing To address inaccuracies and biases in Large Language Models, various model editing techniques have been developed for efficient post-training adjustments.",
            "reference_string": "[267770177 | Yan et al. | 2024 | Citations: 9]"
        },
        {
            "title": "Deconstructing The Ethics of Large Language Models from Long-standing Issues to New-emerging Dilemmas",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 344,
            "citation_count": 24,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.05392, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2307568934",
                    "name": "Chengyuan Deng"
                },
                {
                    "authorId": "2305729796",
                    "name": "Yiqun Duan"
                },
                {
                    "authorId": "2308077589",
                    "name": "Xin Jin"
                },
                {
                    "authorId": "2305625227",
                    "name": "Heng Chang"
                },
                {
                    "authorId": "46879986",
                    "name": "Yijun Tian"
                },
                {
                    "authorId": "2305875824",
                    "name": "Han Liu"
                },
                {
                    "authorId": "2261285492",
                    "name": "Henry Peng Zou"
                },
                {
                    "authorId": "2087723977",
                    "name": "Yiqiao Jin"
                },
                {
                    "authorId": "95289709",
                    "name": "Yijia Xiao"
                },
                {
                    "authorId": "2305636181",
                    "name": "Yichen Wang"
                },
                {
                    "authorId": "2305627329",
                    "name": "Shenghao Wu"
                },
                {
                    "authorId": "2307409629",
                    "name": "Zongxing Xie"
                },
                {
                    "authorId": "2345701402",
                    "name": "Kuofeng Gao"
                },
                {
                    "authorId": "2309117237",
                    "name": "Sihong He"
                },
                {
                    "authorId": "2305613890",
                    "name": "Jun Zhuang"
                },
                {
                    "authorId": "2305654768",
                    "name": "Lu Cheng"
                },
                {
                    "authorId": "2305650253",
                    "name": "Haohan Wang"
                }
            ],
            "abstract": "Large Language Models (LLMs) have achieved unparalleled success across diverse language modeling tasks in recent years. However, this progress has also intensified ethical concerns, impacting the deployment of LLMs in everyday contexts. This paper provides a comprehensive survey of ethical challenges associated with LLMs, from longstanding issues such as copyright infringement, systematic bias, and data privacy, to emerging problems like truthfulness and social norms. We critically analyze existing research aimed at understanding, examining, and mitigating these ethical risks. Our survey underscores integrating ethical standards and societal values into the development of LLMs, thereby guiding the development of responsible and ethically aligned language models.",
            "corpus_id": 270370977,
            "sentences": [
                {
                    "corpus_id": "270370977",
                    "title": "Deconstructing The Ethics of Large Language Models from Long-standing Issues to New-emerging Dilemmas",
                    "text": "The third method focuses on creating new examples adhering to specific characteristics, like collecting high-quality examples to steer the model towards desired output [255; 141], and generating word lists associated with social groups [93].The fourth method performs instruction tuning by adding textual instructions [209], static tokens [180], or trained prefixes [155; 174] to reduce bias in the data.There is also one line of work involving altering contextualized embeddings to remove bias [236; 123].In-training Mitigation.These mitigation techniques focus on modifying the training procedure to reduce bias.The first method of this category focuses on altering the model's structure (i.e., integrating debiasing adapter modules [116]), and using demographic-specific encoder [97].The second method focuses on disrupting the association between social groups and stereotypical words.This is typically achieved by modifying the loss function applied on various model layers like the embedding layer [171; 219], attention layers [76; 13], and token generation stage [229; 108].Additionally, new training paradigms are proposed, such as contrastive learning [215; 159], adversarial learning [96; 238], and reinforcement learning [172; 18].The last method focuses on efficient fine-tuning procedures that freeze most pre-trained model parameters, and only update those potentially related to bias [317; 286; 280; 288].Intra-processing Mitigation.These approaches modify a trained model's behavior without additional training to generate debiased predictions during inference.There are mainly four types of methods.The first method adds restrictions during token search decoding to prevent biased outputs [245; 194].The second method adjusts token distributions to enhance output diversity or sample less biased outputs [58; 95].The third method redistributes the model's attention to less stereotypical aspects [321].The last method implements standalone networks with original models for specific debiasing tasks, such as reducing gender or racial biases [100].\n\nPost-processing Mitigation.The techniques address bias in generated outputs, especially relevant for black-box models with inaccessible training data or internal processes.",
                    "score": 0.6228053544611492,
                    "section_title": "Mitigation Strategy",
                    "char_start_offset": 32746,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 241
                        },
                        {
                            "start": 241,
                            "end": 404
                        },
                        {
                            "start": 404,
                            "end": 506
                        },
                        {
                            "start": 506,
                            "end": 529
                        },
                        {
                            "start": 529,
                            "end": 614
                        },
                        {
                            "start": 614,
                            "end": 787
                        },
                        {
                            "start": 787,
                            "end": 889
                        },
                        {
                            "start": 889,
                            "end": 1081
                        },
                        {
                            "start": 1081,
                            "end": 1242
                        },
                        {
                            "start": 1242,
                            "end": 1420
                        },
                        {
                            "start": 1420,
                            "end": 1448
                        },
                        {
                            "start": 1448,
                            "end": 1577
                        },
                        {
                            "start": 1577,
                            "end": 1616
                        },
                        {
                            "start": 1616,
                            "end": 1717
                        },
                        {
                            "start": 1717,
                            "end": 1830
                        },
                        {
                            "start": 1830,
                            "end": 1919
                        },
                        {
                            "start": 1919,
                            "end": 2064
                        },
                        {
                            "start": 2066,
                            "end": 2093
                        },
                        {
                            "start": 2093,
                            "end": 2238
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 236,
                            "end": 240,
                            "matchedPaperCorpusId": "247619104"
                        },
                        {
                            "start": 318,
                            "end": 323,
                            "matchedPaperCorpusId": "256616034"
                        },
                        {
                            "start": 735,
                            "end": 740,
                            "matchedPaperCorpusId": "59599816"
                        },
                        {
                            "start": 782,
                            "end": 786,
                            "matchedPaperCorpusId": "247694107"
                        },
                        {
                            "start": 2058,
                            "end": 2063,
                            "matchedPaperCorpusId": "258461365"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90283203125
                }
            ],
            "relevance_judgement": 0.90283203125,
            "relevance_judgment_input_expanded": "# Title: Deconstructing The Ethics of Large Language Models from Long-standing Issues to New-emerging Dilemmas\n# Venue: arXiv.org\n# Authors: Chengyuan Deng, Yiqun Duan, Xin Jin, Heng Chang, Yijun Tian, Han Liu, Henry Peng Zou, Yiqiao Jin, Yijia Xiao, Yichen Wang, Shenghao Wu, Zongxing Xie, Kuofeng Gao, Sihong He, Jun Zhuang, Lu Cheng, Haohan Wang\n## Abstract\nLarge Language Models (LLMs) have achieved unparalleled success across diverse language modeling tasks in recent years. However, this progress has also intensified ethical concerns, impacting the deployment of LLMs in everyday contexts. This paper provides a comprehensive survey of ethical challenges associated with LLMs, from longstanding issues such as copyright infringement, systematic bias, and data privacy, to emerging problems like truthfulness and social norms. We critically analyze existing research aimed at understanding, examining, and mitigating these ethical risks. Our survey underscores integrating ethical standards and societal values into the development of LLMs, thereby guiding the development of responsible and ethically aligned language models.\n## Mitigation Strategy\nThe third method focuses on creating new examples adhering to specific characteristics, like collecting high-quality examples to steer the model towards desired output [255; 141], and generating word lists associated with social groups [93].The fourth method performs instruction tuning by adding textual instructions [209], static tokens [180], or trained prefixes [155; 174] to reduce bias in the data.There is also one line of work involving altering contextualized embeddings to remove bias [236; 123].In-training Mitigation.These mitigation techniques focus on modifying the training procedure to reduce bias.The first method of this category focuses on altering the model's structure (i.e., integrating debiasing adapter modules [116]), and using demographic-specific encoder [97].The second method focuses on disrupting the association between social groups and stereotypical words.This is typically achieved by modifying the loss function applied on various model layers like the embedding layer [171; 219], attention layers [76; 13], and token generation stage [229; 108].Additionally, new training paradigms are proposed, such as contrastive learning [215; 159], adversarial learning [96; 238], and reinforcement learning [172; 18].The last method focuses on efficient fine-tuning procedures that freeze most pre-trained model parameters, and only update those potentially related to bias [317; 286; 280; 288].Intra-processing Mitigation.These approaches modify a trained model's behavior without additional training to generate debiased predictions during inference.There are mainly four types of methods.The first method adds restrictions during token search decoding to prevent biased outputs [245; 194].The second method adjusts token distributions to enhance output diversity or sample less biased outputs [58; 95].The third method redistributes the model's attention to less stereotypical aspects [321].The last method implements standalone networks with original models for specific debiasing tasks, such as reducing gender or racial biases [100].\n\nPost-processing Mitigation.The techniques address bias in generated outputs, especially relevant for black-box models with inaccessible training data or internal processes.",
            "reference_string": "[270370977 | Deng et al. | 2024 | Citations: 24]"
        },
        {
            "title": "Deep Learning on a Healthy Data Diet: Finding Important Examples for Fairness",
            "venue": "AAAI Conference on Artificial Intelligence",
            "year": 2022,
            "reference_count": 59,
            "citation_count": 22,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2211.11109",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2211.11109, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2077390471",
                    "name": "A. Zayed"
                },
                {
                    "authorId": "32899078",
                    "name": "Prasanna Parthasarathi"
                },
                {
                    "authorId": "24039720",
                    "name": "Gon\u00e7alo Mordido"
                },
                {
                    "authorId": "2542427",
                    "name": "Hamid Palangi"
                },
                {
                    "authorId": "3197429",
                    "name": "S. Shabanian"
                },
                {
                    "authorId": "123607932",
                    "name": "Sarath Chandar"
                }
            ],
            "abstract": "Data-driven predictive solutions predominant in commercial applications tend to suffer from biases and stereotypes, which raises equity concerns. Prediction models may discover, use, or amplify spurious correlations based on gender or other protected personal characteristics, thus discriminating against marginalized groups. Mitigating gender bias has become an important research focus in natural language processing (NLP) and is an area where annotated corpora are available. Data augmentation reduces gender bias by adding counterfactual examples to the training dataset. In this work, we show that some of the examples in the augmented dataset can be not important or even harmful to fairness. We hence propose a general method for pruning both the factual and counterfactual examples to maximize the model\u2019s fairness as measured by the demographic parity, equality of opportunity, and equality of odds. The fairness achieved by our method surpasses that of data augmentation on three text classification datasets, using no more than half of the examples in the augmented dataset. Our experiments are conducted using models of varying sizes and pre-training settings. WARNING: This work uses language that is offensive in nature.",
            "corpus_id": 253734850,
            "sentences": [
                {
                    "corpus_id": "253734850",
                    "title": "Deep Learning on a Healthy Data Diet: Finding Important Examples for Fairness",
                    "text": "A biased model should not replace humans in certain tasks (e.g. resume filtering or loan eligibility prediction), regardless of how accurate it is if it achieves high accuracy on a test set that is not representative of the population. For example, if most test examples refer to men, this could hide the model's poor performance on examples that reference women. Hence, relying solely on metrics such as accuracy and F 1 might be misleading. \n\nTo mitigate gender bias in language models, several techniques have been proposed. These methods can be broadly classified into three main categories: data-based methods (Lu et al. 2020;Hall Maudslay et al. 2019;De-Arteaga et al. 2019); regularization-based methods (Gupta et al. 2021;Garg et al. 2019); and adversarial-based methods (Song et al. 2019;Madras et al. 2018;Jaiswal et al. 2020). Data-based methods, which are our focus in this work, change the training data to balance the bias through targeted data augmentation (Lu et al. 2020;Hall Maudslay et al. 2019), constructing counterfactual examples (Garg et al. 2019), or removing protected attributes from the input (De-Arteaga et al. 2019) to disallow models from learning any correlation between labels and gender words. Regularization-based methods add an auxiliary loss term to the objective function that reduces the amount of bias in the model. Adversarial-based methods use adversarial learning for bias mitigation. \n\nAlthough data-based methods are effective in theory, they are also demanding in different ways. First, some methods such as counterfactual data augmentation (CDA) (Lu et al. 2020) add counterfactual examples to the training data, which substantially increases the training time. Second, data balancing methods (Dixon et al. 2018) manually collect more examples for the under-represented groups, which requires human intervention. Third, the performance may degrade on the main downstream task (Zhang et al. 2020;Meade, Poole-Dayan, and Reddy 2022).",
                    "score": 0.6091733580708276,
                    "section_title": "Introduction",
                    "char_start_offset": 1623,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 63
                        },
                        {
                            "start": 64,
                            "end": 235
                        },
                        {
                            "start": 236,
                            "end": 363
                        },
                        {
                            "start": 364,
                            "end": 442
                        },
                        {
                            "start": 445,
                            "end": 527
                        },
                        {
                            "start": 528,
                            "end": 837
                        },
                        {
                            "start": 838,
                            "end": 1227
                        },
                        {
                            "start": 1228,
                            "end": 1355
                        },
                        {
                            "start": 1356,
                            "end": 1427
                        },
                        {
                            "start": 1430,
                            "end": 1525
                        },
                        {
                            "start": 1526,
                            "end": 1708
                        },
                        {
                            "start": 1709,
                            "end": 1859
                        },
                        {
                            "start": 1860,
                            "end": 1978
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 615,
                            "end": 631,
                            "matchedPaperCorpusId": "51888520"
                        },
                        {
                            "start": 631,
                            "end": 657,
                            "matchedPaperCorpusId": "202541569"
                        },
                        {
                            "start": 657,
                            "end": 680,
                            "matchedPaperCorpusId": "58006082"
                        },
                        {
                            "start": 711,
                            "end": 730,
                            "matchedPaperCorpusId": "202541569"
                        },
                        {
                            "start": 730,
                            "end": 747,
                            "matchedPaperCorpusId": "52880735"
                        },
                        {
                            "start": 797,
                            "end": 816,
                            "matchedPaperCorpusId": "102350941"
                        },
                        {
                            "start": 816,
                            "end": 836,
                            "matchedPaperCorpusId": "5147501"
                        },
                        {
                            "start": 972,
                            "end": 988,
                            "matchedPaperCorpusId": "51888520"
                        },
                        {
                            "start": 988,
                            "end": 1014,
                            "matchedPaperCorpusId": "202541569"
                        },
                        {
                            "start": 1053,
                            "end": 1071,
                            "matchedPaperCorpusId": "52880735"
                        },
                        {
                            "start": 1121,
                            "end": 1145,
                            "matchedPaperCorpusId": "58006082"
                        },
                        {
                            "start": 1593,
                            "end": 1609,
                            "matchedPaperCorpusId": "51888520"
                        },
                        {
                            "start": 1740,
                            "end": 1758,
                            "matchedPaperCorpusId": "54997157"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.900390625
                }
            ],
            "relevance_judgement": 0.900390625,
            "relevance_judgment_input_expanded": "# Title: Deep Learning on a Healthy Data Diet: Finding Important Examples for Fairness\n# Venue: AAAI Conference on Artificial Intelligence\n# Authors: A. Zayed, Prasanna Parthasarathi, Gon\u00e7alo Mordido, Hamid Palangi, S. Shabanian, Sarath Chandar\n## Abstract\nData-driven predictive solutions predominant in commercial applications tend to suffer from biases and stereotypes, which raises equity concerns. Prediction models may discover, use, or amplify spurious correlations based on gender or other protected personal characteristics, thus discriminating against marginalized groups. Mitigating gender bias has become an important research focus in natural language processing (NLP) and is an area where annotated corpora are available. Data augmentation reduces gender bias by adding counterfactual examples to the training dataset. In this work, we show that some of the examples in the augmented dataset can be not important or even harmful to fairness. We hence propose a general method for pruning both the factual and counterfactual examples to maximize the model\u2019s fairness as measured by the demographic parity, equality of opportunity, and equality of odds. The fairness achieved by our method surpasses that of data augmentation on three text classification datasets, using no more than half of the examples in the augmented dataset. Our experiments are conducted using models of varying sizes and pre-training settings. WARNING: This work uses language that is offensive in nature.\n## Introduction\nA biased model should not replace humans in certain tasks (e.g. resume filtering or loan eligibility prediction), regardless of how accurate it is if it achieves high accuracy on a test set that is not representative of the population. For example, if most test examples refer to men, this could hide the model's poor performance on examples that reference women. Hence, relying solely on metrics such as accuracy and F 1 might be misleading. \n\nTo mitigate gender bias in language models, several techniques have been proposed. These methods can be broadly classified into three main categories: data-based methods (Lu et al. 2020;Hall Maudslay et al. 2019;De-Arteaga et al. 2019); regularization-based methods (Gupta et al. 2021;Garg et al. 2019); and adversarial-based methods (Song et al. 2019;Madras et al. 2018;Jaiswal et al. 2020). Data-based methods, which are our focus in this work, change the training data to balance the bias through targeted data augmentation (Lu et al. 2020;Hall Maudslay et al. 2019), constructing counterfactual examples (Garg et al. 2019), or removing protected attributes from the input (De-Arteaga et al. 2019) to disallow models from learning any correlation between labels and gender words. Regularization-based methods add an auxiliary loss term to the objective function that reduces the amount of bias in the model. Adversarial-based methods use adversarial learning for bias mitigation. \n\nAlthough data-based methods are effective in theory, they are also demanding in different ways. First, some methods such as counterfactual data augmentation (CDA) (Lu et al. 2020) add counterfactual examples to the training data, which substantially increases the training time. Second, data balancing methods (Dixon et al. 2018) manually collect more examples for the under-represented groups, which requires human intervention. Third, the performance may degrade on the main downstream task (Zhang et al. 2020;Meade, Poole-Dayan, and Reddy 2022).",
            "reference_string": "[253734850 | Zayed et al. | 2022 | Citations: 22]"
        },
        {
            "title": "Debiasing Pre-Trained Language Models via Efficient Fine-Tuning",
            "venue": "LTEDI",
            "year": 2022,
            "reference_count": 48,
            "citation_count": 44,
            "influential_citation_count": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2022.ltedi-1.8.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2022.ltedi-1.8, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2165226185",
                    "name": "Michael Gira"
                },
                {
                    "authorId": "2165321339",
                    "name": "Ruisu Zhang"
                },
                {
                    "authorId": "2115495251",
                    "name": "Kangwook Lee"
                }
            ],
            "abstract": "An explosion in the popularity of transformer-based language models (such as GPT-3, BERT, RoBERTa, and ALBERT) has opened the doors to new machine learning applications involving language modeling, text generation, and more. However, recent scrutiny reveals that these language models contain inherent biases towards certain demographics reflected in their training data. While research has tried mitigating this problem, existing approaches either fail to remove the bias completely, degrade performance (\u201ccatastrophic forgetting\u201d), or are costly to execute. This work examines how to reduce gender bias in a GPT-2 language model by fine-tuning less than 1% of its parameters. Through quantitative benchmarks, we show that this is a viable way to reduce prejudice in pre-trained language models while remaining cost-effective at scale.",
            "corpus_id": 248780268,
            "sentences": [
                {
                    "corpus_id": "248780268",
                    "title": "Debiasing Pre-Trained Language Models via Efficient Fine-Tuning",
                    "text": "An explosion in the popularity of transformer-based language models (such as GPT-3, BERT, RoBERTa, and ALBERT) has opened the doors to new machine learning applications involving language modeling, text generation, and more. However, recent scrutiny reveals that these language models contain inherent biases towards certain demographics reflected in their training data. While research has tried mitigating this problem, existing approaches either fail to remove the bias completely, degrade performance (\u201ccatastrophic forgetting\u201d), or are costly to execute. This work examines how to reduce gender bias in a GPT-2 language model by fine-tuning less than 1% of its parameters. Through quantitative benchmarks, we show that this is a viable way to reduce prejudice in pre-trained language models while remaining cost-effective at scale.",
                    "score": 0.5782167040824414,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.900390625
                }
            ],
            "relevance_judgement": 0.900390625,
            "relevance_judgment_input_expanded": "# Title: Debiasing Pre-Trained Language Models via Efficient Fine-Tuning\n# Venue: LTEDI\n# Authors: Michael Gira, Ruisu Zhang, Kangwook Lee\n## Abstract\nAn explosion in the popularity of transformer-based language models (such as GPT-3, BERT, RoBERTa, and ALBERT) has opened the doors to new machine learning applications involving language modeling, text generation, and more. However, recent scrutiny reveals that these language models contain inherent biases towards certain demographics reflected in their training data. While research has tried mitigating this problem, existing approaches either fail to remove the bias completely, degrade performance (\u201ccatastrophic forgetting\u201d), or are costly to execute. This work examines how to reduce gender bias in a GPT-2 language model by fine-tuning less than 1% of its parameters. Through quantitative benchmarks, we show that this is a viable way to reduce prejudice in pre-trained language models while remaining cost-effective at scale.\n",
            "reference_string": "[248780268 | Gira et al. | 2022 | Citations: 44]"
        },
        {
            "title": "BiasWipe: Mitigating Unintended Bias in Text Classifiers through Model Interpretability",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "reference_count": 55,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2024.emnlp-main.1172, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "50783005",
                    "name": "Mamta Mamta"
                },
                {
                    "authorId": "2329735916",
                    "name": "Rishikant Chigrupaatii"
                },
                {
                    "authorId": "2254291732",
                    "name": "Asif Ekbal"
                }
            ],
            "abstract": "Toxic content detection plays a vital role in addressing the misuse of social media platforms to harm people or groups due to their race, gender or ethnicity. However, due to the nature of the datasets, systems develop an unintended bias due to the over-generalization of the model to the training data. This compromises the fairness of the systems, which can impact certain groups due to their race, gender, etc.Existing methods mitigate bias using data augmentation, adversarial learning, etc., which require re-training and adding extra parameters to the model.In this work, we present a robust and generalizable technique BiasWipe to mitigate unintended bias in language models. BiasWipe utilizes model interpretability using Shapley values, which achieve fairness by pruning the neuron weights responsible for unintended bias. It first identifies the neuron weights responsible for unintended bias and then achieves fairness by pruning them without loss of original performance. It does not require re-training or adding extra parameters to the model. To show the effectiveness of our proposed technique for bias unlearning, we perform extensive experiments for Toxic content detection for BERT, RoBERTa, and GPT models. .",
            "corpus_id": 273901329,
            "sentences": [
                {
                    "corpus_id": "273901329",
                    "title": "BiasWipe: Mitigating Unintended Bias in Text Classifiers through Model Interpretability",
                    "text": "More recently, Goldfarb-Tarrant et al. (2023) proposed an Equity Evaluation Corpus to measure racial and gender bias in German, Chinese, Japanese, and Spanish sentiment classifiers. \n\nAccording to Garg et al. (2023), bias mitigation techniques can be classified into two categories, viz., data correction and filtering (pre-processing) or debiased training of downstream models (inprocessing). Data correction and filtering involve upsampling (Dixon et al., 2018;Zhao et al., 2017), use of wordnet hypernym-tree (Badjatiya et al., 2019), or data filtering approaches to obtain the training samples that will lead to better generalizability which reduce bias as a by-product (Zhou et al., 2021). Debiased training involves regularizing loss function (Kennedy et al., 2020), multi-task learning (Vaidya et al., 2020), ensemble based debiasing (Zhou et al., 2021), and adversarial learning (Zhang et al., 2018;Sun et al., 2019). All these techniques require re-training the model by augmenting data or adding additional parameters. A recent attempt has been made by Baldini et al. (2022) to study the impact of model size, data, and random initialization on fairness of the model. They adapt two tabular data post-processing bias mitigation techniques to NLP tasks (Wei et al., 2020;Hardt et al., 2016) to enhance the group fairness of language models. However, in our work we focus on individual entity fairness instead of a group. \n\nThere are attempts to mitigate bias using postprocessing methods, particularly in tabular datasets. These methods aim to adjust specific classification outcomes to improve metrics like equalized odds or equality of opportunity (Hardt et al., 2016). Madras et al. (2018) proposed LAFTR (Learning Adversarially Fair and Transferable Representations), a debiasing method that limits unfairness metrics by employing an adversarial objective function.",
                    "score": 0.5850290840786437,
                    "section_title": "Bias Detection and Mitigation",
                    "char_start_offset": 7438,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 181
                        },
                        {
                            "start": 184,
                            "end": 393
                        },
                        {
                            "start": 394,
                            "end": 694
                        },
                        {
                            "start": 695,
                            "end": 925
                        },
                        {
                            "start": 926,
                            "end": 1028
                        },
                        {
                            "start": 1029,
                            "end": 1177
                        },
                        {
                            "start": 1178,
                            "end": 1349
                        },
                        {
                            "start": 1350,
                            "end": 1429
                        },
                        {
                            "start": 1432,
                            "end": 1531
                        },
                        {
                            "start": 1532,
                            "end": 1680
                        },
                        {
                            "start": 1681,
                            "end": 1878
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 197,
                            "end": 215,
                            "matchedPaperCorpusId": "246441998"
                        },
                        {
                            "start": 443,
                            "end": 463,
                            "matchedPaperCorpusId": "54997157"
                        },
                        {
                            "start": 512,
                            "end": 536,
                            "matchedPaperCorpusId": "86446136"
                        },
                        {
                            "start": 793,
                            "end": 814,
                            "matchedPaperCorpusId": "219561187"
                        },
                        {
                            "start": 887,
                            "end": 907,
                            "matchedPaperCorpusId": "9424845"
                        },
                        {
                            "start": 1063,
                            "end": 1084,
                            "matchedPaperCorpusId": "248177981"
                        },
                        {
                            "start": 1262,
                            "end": 1280,
                            "matchedPaperCorpusId": "173991023"
                        },
                        {
                            "start": 1280,
                            "end": 1299,
                            "matchedPaperCorpusId": "7567061"
                        },
                        {
                            "start": 1659,
                            "end": 1679,
                            "matchedPaperCorpusId": "7567061"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8974609375
                }
            ],
            "relevance_judgement": 0.8974609375,
            "relevance_judgment_input_expanded": "# Title: BiasWipe: Mitigating Unintended Bias in Text Classifiers through Model Interpretability\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Mamta Mamta, Rishikant Chigrupaatii, Asif Ekbal\n## Abstract\nToxic content detection plays a vital role in addressing the misuse of social media platforms to harm people or groups due to their race, gender or ethnicity. However, due to the nature of the datasets, systems develop an unintended bias due to the over-generalization of the model to the training data. This compromises the fairness of the systems, which can impact certain groups due to their race, gender, etc.Existing methods mitigate bias using data augmentation, adversarial learning, etc., which require re-training and adding extra parameters to the model.In this work, we present a robust and generalizable technique BiasWipe to mitigate unintended bias in language models. BiasWipe utilizes model interpretability using Shapley values, which achieve fairness by pruning the neuron weights responsible for unintended bias. It first identifies the neuron weights responsible for unintended bias and then achieves fairness by pruning them without loss of original performance. It does not require re-training or adding extra parameters to the model. To show the effectiveness of our proposed technique for bias unlearning, we perform extensive experiments for Toxic content detection for BERT, RoBERTa, and GPT models. .\n## Bias Detection and Mitigation\nMore recently, Goldfarb-Tarrant et al. (2023) proposed an Equity Evaluation Corpus to measure racial and gender bias in German, Chinese, Japanese, and Spanish sentiment classifiers. \n\nAccording to Garg et al. (2023), bias mitigation techniques can be classified into two categories, viz., data correction and filtering (pre-processing) or debiased training of downstream models (inprocessing). Data correction and filtering involve upsampling (Dixon et al., 2018;Zhao et al., 2017), use of wordnet hypernym-tree (Badjatiya et al., 2019), or data filtering approaches to obtain the training samples that will lead to better generalizability which reduce bias as a by-product (Zhou et al., 2021). Debiased training involves regularizing loss function (Kennedy et al., 2020), multi-task learning (Vaidya et al., 2020), ensemble based debiasing (Zhou et al., 2021), and adversarial learning (Zhang et al., 2018;Sun et al., 2019). All these techniques require re-training the model by augmenting data or adding additional parameters. A recent attempt has been made by Baldini et al. (2022) to study the impact of model size, data, and random initialization on fairness of the model. They adapt two tabular data post-processing bias mitigation techniques to NLP tasks (Wei et al., 2020;Hardt et al., 2016) to enhance the group fairness of language models. However, in our work we focus on individual entity fairness instead of a group. \n\nThere are attempts to mitigate bias using postprocessing methods, particularly in tabular datasets. These methods aim to adjust specific classification outcomes to improve metrics like equalized odds or equality of opportunity (Hardt et al., 2016). Madras et al. (2018) proposed LAFTR (Learning Adversarially Fair and Transferable Representations), a debiasing method that limits unfairness metrics by employing an adversarial objective function.",
            "reference_string": "[273901329 | Mamta et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Understanding and Mitigating the Bias Inheritance in LLM-based Data Augmentation on Downstream Tasks",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 78,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.04419, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2344641665",
                    "name": "Miaomiao Li"
                },
                {
                    "authorId": "2261741520",
                    "name": "Hao Chen"
                },
                {
                    "authorId": "2344936960",
                    "name": "Yang Wang"
                },
                {
                    "authorId": "2331767016",
                    "name": "Tingyuan Zhu"
                },
                {
                    "authorId": "2344790620",
                    "name": "Weijia Zhang"
                },
                {
                    "authorId": "2313725635",
                    "name": "Kaijie Zhu"
                },
                {
                    "authorId": "2345348279",
                    "name": "Kam-Fai Wong"
                },
                {
                    "authorId": "2285254341",
                    "name": "Jindong Wang"
                }
            ],
            "abstract": "Generating synthetic datasets via large language models (LLMs) themselves has emerged as a promising approach to improve LLM performance. However, LLMs inherently reflect biases present in their training data, leading to a critical challenge: when these models generate synthetic data for training, they may propagate and amplify their inherent biases that can significantly impact model fairness and robustness on downstream tasks--a phenomenon we term bias inheritance. This work presents the first systematic investigation in understanding, analyzing, and mitigating bias inheritance. We study this problem by fine-tuning LLMs with a combined dataset consisting of original and LLM-augmented data, where bias ratio represents the proportion of augmented data. Through systematic experiments across 10 classification and generation tasks, we analyze how 6 different types of biases manifest at varying bias ratios. Our results reveal that bias inheritance has nuanced effects on downstream tasks, influencing both classification tasks and generation tasks differently. Then, our analysis identifies three key misalignment factors: misalignment of values, group data, and data distributions. Based on these insights, we propose three mitigation strategies: token-based, mask-based, and loss-based approaches. Experiments demonstrate that these strategies also work differently on various tasks and bias, indicating the substantial challenges to fully mitigate bias inheritance. We hope this work can provide valuable insights to the research of LLM data augmentation.",
            "corpus_id": 276235453,
            "sentences": [
                {
                    "corpus_id": "276235453",
                    "title": "Understanding and Mitigating the Bias Inheritance in LLM-based Data Augmentation on Downstream Tasks",
                    "text": "The average mitigation results of severe negative influences for each task and bias are shown in Figure 9. A detailed analysis of the mitigation effects on gender and cultural biases across different tasks, as well as the results on GPT-4o-mini, are provided in Appendix E.2. Our key conclusion is that, similar to the nuanced effects on downstream tasks, the effectiveness of different mitigation approaches is also nuanced, depending on various factors such as different types of bias, downstream tasks, bias ratios, and more, highlighting the difficulty in devising a unified mitigation solution. Specifically, token-based mitigation provides implicit cues and works best with simple biases and tasks, as it depends on the model's own understanding. It is more effective for gender bias than complex cultural bias and performs better in classification tasks than in detailed generation tasks. For gender generation tasks, salary recommendation benefits more than complex hiring recommendation. More augmentation data (50%) further enhance its performance. \n\nMask-based mitigation shows noticeable effects at lower bias ratios (5%), especially in tasks like cultural classification. As at this ratio, explicit biased terms have a more direct and pronounced impact on the model's performance. By masking these terms, the model is less likely to rely on biased information or features that could skew its decision-making, thereby reducing bias inheritance. However, as bias ratios increase, the influence of more subtle and implicit biases grows, necessitating more complex mitigation strategies. It also proves effective in contrastive explicit bias, where direct bias information is more clearly identifiable. \n\nLoss-based mitigation primarily depends on the distribution distance between the augmentation and the original data, showing significant effectiveness when the distance is large. It is more suitable for coarse-grained tasks. For example, it works well in classification tasks where decisions often rely on broader patterns. In generation tasks, coarse-grained contexts like salary recommendations perform better than finer-grained ones like hiring recommendation. Additionally, smaller proportions (e.g., 5% and 10%) of augmentation data yield better results, by introducing subtle shifts that avoid overfitting to the augmented distribution while effectively influencing the original bias.",
                    "score": 0.520546208813335,
                    "section_title": "Mitigation Results",
                    "char_start_offset": 31739,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 106
                        },
                        {
                            "start": 107,
                            "end": 275
                        },
                        {
                            "start": 276,
                            "end": 599
                        },
                        {
                            "start": 600,
                            "end": 752
                        },
                        {
                            "start": 753,
                            "end": 895
                        },
                        {
                            "start": 896,
                            "end": 996
                        },
                        {
                            "start": 997,
                            "end": 1058
                        },
                        {
                            "start": 1061,
                            "end": 1184
                        },
                        {
                            "start": 1185,
                            "end": 1293
                        },
                        {
                            "start": 1294,
                            "end": 1456
                        },
                        {
                            "start": 1457,
                            "end": 1596
                        },
                        {
                            "start": 1597,
                            "end": 1711
                        },
                        {
                            "start": 1714,
                            "end": 1892
                        },
                        {
                            "start": 1893,
                            "end": 1938
                        },
                        {
                            "start": 1939,
                            "end": 2037
                        },
                        {
                            "start": 2038,
                            "end": 2177
                        },
                        {
                            "start": 2178,
                            "end": 2404
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8974609375
                }
            ],
            "relevance_judgement": 0.8974609375,
            "relevance_judgment_input_expanded": "# Title: Understanding and Mitigating the Bias Inheritance in LLM-based Data Augmentation on Downstream Tasks\n# Venue: arXiv.org\n# Authors: Miaomiao Li, Hao Chen, Yang Wang, Tingyuan Zhu, Weijia Zhang, Kaijie Zhu, Kam-Fai Wong, Jindong Wang\n## Abstract\nGenerating synthetic datasets via large language models (LLMs) themselves has emerged as a promising approach to improve LLM performance. However, LLMs inherently reflect biases present in their training data, leading to a critical challenge: when these models generate synthetic data for training, they may propagate and amplify their inherent biases that can significantly impact model fairness and robustness on downstream tasks--a phenomenon we term bias inheritance. This work presents the first systematic investigation in understanding, analyzing, and mitigating bias inheritance. We study this problem by fine-tuning LLMs with a combined dataset consisting of original and LLM-augmented data, where bias ratio represents the proportion of augmented data. Through systematic experiments across 10 classification and generation tasks, we analyze how 6 different types of biases manifest at varying bias ratios. Our results reveal that bias inheritance has nuanced effects on downstream tasks, influencing both classification tasks and generation tasks differently. Then, our analysis identifies three key misalignment factors: misalignment of values, group data, and data distributions. Based on these insights, we propose three mitigation strategies: token-based, mask-based, and loss-based approaches. Experiments demonstrate that these strategies also work differently on various tasks and bias, indicating the substantial challenges to fully mitigate bias inheritance. We hope this work can provide valuable insights to the research of LLM data augmentation.\n## Mitigation Results\nThe average mitigation results of severe negative influences for each task and bias are shown in Figure 9. A detailed analysis of the mitigation effects on gender and cultural biases across different tasks, as well as the results on GPT-4o-mini, are provided in Appendix E.2. Our key conclusion is that, similar to the nuanced effects on downstream tasks, the effectiveness of different mitigation approaches is also nuanced, depending on various factors such as different types of bias, downstream tasks, bias ratios, and more, highlighting the difficulty in devising a unified mitigation solution. Specifically, token-based mitigation provides implicit cues and works best with simple biases and tasks, as it depends on the model's own understanding. It is more effective for gender bias than complex cultural bias and performs better in classification tasks than in detailed generation tasks. For gender generation tasks, salary recommendation benefits more than complex hiring recommendation. More augmentation data (50%) further enhance its performance. \n\nMask-based mitigation shows noticeable effects at lower bias ratios (5%), especially in tasks like cultural classification. As at this ratio, explicit biased terms have a more direct and pronounced impact on the model's performance. By masking these terms, the model is less likely to rely on biased information or features that could skew its decision-making, thereby reducing bias inheritance. However, as bias ratios increase, the influence of more subtle and implicit biases grows, necessitating more complex mitigation strategies. It also proves effective in contrastive explicit bias, where direct bias information is more clearly identifiable. \n\nLoss-based mitigation primarily depends on the distribution distance between the augmentation and the original data, showing significant effectiveness when the distance is large. It is more suitable for coarse-grained tasks. For example, it works well in classification tasks where decisions often rely on broader patterns. In generation tasks, coarse-grained contexts like salary recommendations perform better than finer-grained ones like hiring recommendation. Additionally, smaller proportions (e.g., 5% and 10%) of augmentation data yield better results, by introducing subtle shifts that avoid overfitting to the augmented distribution while effectively influencing the original bias.",
            "reference_string": "[276235453 | Li et al. | 2025 | Citations: 4]"
        },
        {
            "title": "Intersectional Bias in Causal Language Models",
            "venue": "arXiv.org",
            "year": 2021,
            "reference_count": 55,
            "citation_count": 33,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2107.07691, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2733075",
                    "name": "L. Magee"
                },
                {
                    "authorId": "9557084",
                    "name": "Lida Ghahremanlou"
                },
                {
                    "authorId": "13714096",
                    "name": "K. Soldati\u0107"
                },
                {
                    "authorId": "97868921",
                    "name": "S. Robertson"
                }
            ],
            "abstract": "To examine whether intersectional bias can be observed in language generation, we examine \\emph{GPT-2} and \\emph{GPT-NEO} models, ranging in size from 124 million to ~2.7 billion parameters. We conduct an experiment combining up to three social categories - gender, religion and disability - into unconditional or zero-shot prompts used to generate sentences that are then analysed for sentiment. Our results confirm earlier tests conducted with auto-regressive causal models, including the \\emph{GPT} family of models. We also illustrate why bias may be resistant to techniques that target single categories (e.g. gender, religion and race), as it can also manifest, in often subtle ways, in texts prompted by concatenated social categories. To address these difficulties, we suggest technical and community-based approaches need to combine to acknowledge and address complex and intersectional language model bias.",
            "corpus_id": 236034024,
            "sentences": [
                {
                    "corpus_id": "236034024",
                    "title": "Intersectional Bias in Causal Language Models",
                    "text": "Many approaches to mitigating or addressing bias have been proposed. Broadly, these can be distinguished as data-driven, model-driven or use-driven. Discussing gender bias, [33] suggest 'data augmentation', 'gender tagging' and 'bias fine tuning' as examples of datadriven approaches; removing subspaces (e.g. gender) or isolating category-specific information in word embeddings as examples of model-driven approaches; and constraining or censoring predictions as examples of use-driven approaches. \n\nModel design has increasingly sought to address bias using various data-driven approaches, either during training or via fine-tuning. Both GPT-2 and GPT-3 [31,36] use criteria such as Reddit popularity and weights to de-bias training data, while 'The Pile', developed in part to train GPT-NEO, has sought to incorporate a wider range of text [37]. As [37] also point out, careful curation of training data goes some way towards addressing at least more conspicuous bias. [51] suggests a mitigation approach based on (1) training a de-biased upstream model and then (2) fine-tuning a downstream model. \n\nModel-driven approaches treat the language model post-training. For models involving word embeddings, these embeddings can be inspected, and following a debiasing heuristic, modified directly. [10] for example conducts a component analysis to identify bias, and then manipulates embedding values, to either increase or decrease cosine values between category terms in accordance with the analysis. Such techniques appear more difficult to apply to transformer models with contextual embeddings. [35] for example examines both data augmentation and model neutralisation, and finds model neutralisation less effective. \n\nUse-driven approaches constrain or modify inputs. [48] shows for instance how inclusion of modifiers can produce radically different predictions for terms (like 'Muslim' in their study) showing strong negative bias in GPT-3. More generally, generalised pre-trained models make possible 'one-shot' or 'few-shot' training [36], padding inputs with a small number of prior examples, that can selectively condition predictions.",
                    "score": 0.6584737480994318,
                    "section_title": "Bias Mitigation",
                    "char_start_offset": 6467,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 68
                        },
                        {
                            "start": 69,
                            "end": 148
                        },
                        {
                            "start": 149,
                            "end": 309
                        },
                        {
                            "start": 310,
                            "end": 499
                        },
                        {
                            "start": 502,
                            "end": 635
                        },
                        {
                            "start": 636,
                            "end": 849
                        },
                        {
                            "start": 850,
                            "end": 972
                        },
                        {
                            "start": 973,
                            "end": 1102
                        },
                        {
                            "start": 1105,
                            "end": 1168
                        },
                        {
                            "start": 1169,
                            "end": 1297
                        },
                        {
                            "start": 1298,
                            "end": 1502
                        },
                        {
                            "start": 1503,
                            "end": 1599
                        },
                        {
                            "start": 1600,
                            "end": 1721
                        },
                        {
                            "start": 1724,
                            "end": 1773
                        },
                        {
                            "start": 1774,
                            "end": 1948
                        },
                        {
                            "start": 1949,
                            "end": 2147
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 657,
                            "end": 661,
                            "matchedPaperCorpusId": "160025533"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.89697265625
                }
            ],
            "relevance_judgement": 0.89697265625,
            "relevance_judgment_input_expanded": "# Title: Intersectional Bias in Causal Language Models\n# Venue: arXiv.org\n# Authors: L. Magee, Lida Ghahremanlou, K. Soldati\u0107, S. Robertson\n## Abstract\nTo examine whether intersectional bias can be observed in language generation, we examine \\emph{GPT-2} and \\emph{GPT-NEO} models, ranging in size from 124 million to ~2.7 billion parameters. We conduct an experiment combining up to three social categories - gender, religion and disability - into unconditional or zero-shot prompts used to generate sentences that are then analysed for sentiment. Our results confirm earlier tests conducted with auto-regressive causal models, including the \\emph{GPT} family of models. We also illustrate why bias may be resistant to techniques that target single categories (e.g. gender, religion and race), as it can also manifest, in often subtle ways, in texts prompted by concatenated social categories. To address these difficulties, we suggest technical and community-based approaches need to combine to acknowledge and address complex and intersectional language model bias.\n## Bias Mitigation\nMany approaches to mitigating or addressing bias have been proposed. Broadly, these can be distinguished as data-driven, model-driven or use-driven. Discussing gender bias, [33] suggest 'data augmentation', 'gender tagging' and 'bias fine tuning' as examples of datadriven approaches; removing subspaces (e.g. gender) or isolating category-specific information in word embeddings as examples of model-driven approaches; and constraining or censoring predictions as examples of use-driven approaches. \n\nModel design has increasingly sought to address bias using various data-driven approaches, either during training or via fine-tuning. Both GPT-2 and GPT-3 [31,36] use criteria such as Reddit popularity and weights to de-bias training data, while 'The Pile', developed in part to train GPT-NEO, has sought to incorporate a wider range of text [37]. As [37] also point out, careful curation of training data goes some way towards addressing at least more conspicuous bias. [51] suggests a mitigation approach based on (1) training a de-biased upstream model and then (2) fine-tuning a downstream model. \n\nModel-driven approaches treat the language model post-training. For models involving word embeddings, these embeddings can be inspected, and following a debiasing heuristic, modified directly. [10] for example conducts a component analysis to identify bias, and then manipulates embedding values, to either increase or decrease cosine values between category terms in accordance with the analysis. Such techniques appear more difficult to apply to transformer models with contextual embeddings. [35] for example examines both data augmentation and model neutralisation, and finds model neutralisation less effective. \n\nUse-driven approaches constrain or modify inputs. [48] shows for instance how inclusion of modifiers can produce radically different predictions for terms (like 'Muslim' in their study) showing strong negative bias in GPT-3. More generally, generalised pre-trained models make possible 'one-shot' or 'few-shot' training [36], padding inputs with a small number of prior examples, that can selectively condition predictions.",
            "reference_string": "[236034024 | Magee et al. | 2021 | Citations: 33]"
        },
        {
            "title": "Reducing Gender Bias in Word-Level Language Models with a Gender-Equalizing Loss Function",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2019,
            "reference_count": 11,
            "citation_count": 95,
            "influential_citation_count": 7,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/P19-2031.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1905.12801, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "134894281",
                    "name": "Yusu Qian"
                },
                {
                    "authorId": "3375824",
                    "name": "Urwa Muaz"
                },
                {
                    "authorId": null,
                    "name": "Ben Zhang"
                },
                {
                    "authorId": "2058870034",
                    "name": "J. Hyun"
                }
            ],
            "abstract": "Gender bias exists in natural language datasets, which neural language models tend to learn, resulting in biased text generation. In this research, we propose a debiasing approach based on the loss function modification. We introduce a new term to the loss function which attempts to equalize the probabilities of male and female words in the output. Using an array of bias evaluation metrics, we provide empirical evidence that our approach successfully mitigates gender bias in language models without increasing perplexity. In comparison to existing debiasing strategies, data augmentation, and word embedding debiasing, our method performs better in several aspects, especially in reducing gender bias in occupation words. Finally, we introduce a combination of data augmentation and our approach and show that it outperforms existing strategies in all bias evaluation metrics.",
            "corpus_id": 170078973,
            "sentences": [
                {
                    "corpus_id": "170078973",
                    "title": "Reducing Gender Bias in Word-Level Language Models with a Gender-Equalizing Loss Function",
                    "text": "In this research, we propose a new approach for mitigating gender bias in neural language models and empirically show its effectiveness in reducing bias as measured with different evaluation metrics. Our research also highlights the fact that debiasing the model with bias penalties in the loss function is an effective method. We emphasize that loss function based debiasing is powerful and generalizable to other downstream NLP applications. The research also reinforces the idea that geometric debiasing of the word embedding is not a complete solution for debiasing the downstream applications but encourages end-to-end approaches to debiasing. \n\nAll the debiasing techniques experimented in this paper rely on a predefined set of gender pairs in some way. CDA used gender pairs for flipping, REG uses it for gender space definition and our technique uses them for computing loss. This reliance on pre-defined set of gender pairs can be considered a limitation of these methods. It also results in another concern. There are gender associated words which do not have pairs, like pregnant. These words are not treated properly by techniques relying on gender pairs. Future work includes designing a context-aware version of our loss function which can distinguish between the unbiased and biased mentions of the gendered words and only penalize the biased version. Another interesting direction is exploring the application of this method in mitigating racial bias which brings more challenges.",
                    "score": 0.5553034853052237,
                    "section_title": "Conclusion and Discussion",
                    "char_start_offset": 15731,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 199
                        },
                        {
                            "start": 200,
                            "end": 327
                        },
                        {
                            "start": 328,
                            "end": 443
                        },
                        {
                            "start": 444,
                            "end": 648
                        },
                        {
                            "start": 651,
                            "end": 760
                        },
                        {
                            "start": 761,
                            "end": 884
                        },
                        {
                            "start": 885,
                            "end": 982
                        },
                        {
                            "start": 983,
                            "end": 1018
                        },
                        {
                            "start": 1019,
                            "end": 1092
                        },
                        {
                            "start": 1093,
                            "end": 1168
                        },
                        {
                            "start": 1169,
                            "end": 1367
                        },
                        {
                            "start": 1368,
                            "end": 1497
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.89599609375
                }
            ],
            "relevance_judgement": 0.89599609375,
            "relevance_judgment_input_expanded": "# Title: Reducing Gender Bias in Word-Level Language Models with a Gender-Equalizing Loss Function\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Yusu Qian, Urwa Muaz, Ben Zhang, J. Hyun\n## Abstract\nGender bias exists in natural language datasets, which neural language models tend to learn, resulting in biased text generation. In this research, we propose a debiasing approach based on the loss function modification. We introduce a new term to the loss function which attempts to equalize the probabilities of male and female words in the output. Using an array of bias evaluation metrics, we provide empirical evidence that our approach successfully mitigates gender bias in language models without increasing perplexity. In comparison to existing debiasing strategies, data augmentation, and word embedding debiasing, our method performs better in several aspects, especially in reducing gender bias in occupation words. Finally, we introduce a combination of data augmentation and our approach and show that it outperforms existing strategies in all bias evaluation metrics.\n## Conclusion and Discussion\nIn this research, we propose a new approach for mitigating gender bias in neural language models and empirically show its effectiveness in reducing bias as measured with different evaluation metrics. Our research also highlights the fact that debiasing the model with bias penalties in the loss function is an effective method. We emphasize that loss function based debiasing is powerful and generalizable to other downstream NLP applications. The research also reinforces the idea that geometric debiasing of the word embedding is not a complete solution for debiasing the downstream applications but encourages end-to-end approaches to debiasing. \n\nAll the debiasing techniques experimented in this paper rely on a predefined set of gender pairs in some way. CDA used gender pairs for flipping, REG uses it for gender space definition and our technique uses them for computing loss. This reliance on pre-defined set of gender pairs can be considered a limitation of these methods. It also results in another concern. There are gender associated words which do not have pairs, like pregnant. These words are not treated properly by techniques relying on gender pairs. Future work includes designing a context-aware version of our loss function which can distinguish between the unbiased and biased mentions of the gendered words and only penalize the biased version. Another interesting direction is exploring the application of this method in mitigating racial bias which brings more challenges.",
            "reference_string": "[170078973 | Qian et al. | 2019 | Citations: 95]"
        },
        {
            "title": "Co$^2$PT: Mitigating Bias in Pre-trained Language Models through Counterfactual Contrastive Prompt Tuning",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2023,
            "reference_count": 43,
            "citation_count": 11,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.12490, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1716200686",
                    "name": "Xiangjue Dong"
                },
                {
                    "authorId": "9725200",
                    "name": "Ziwei Zhu"
                },
                {
                    "authorId": "96309563",
                    "name": "Zhuoer Wang"
                },
                {
                    "authorId": "121003684",
                    "name": "Maria Teleki"
                },
                {
                    "authorId": "1697232",
                    "name": "James Caverlee"
                }
            ],
            "abstract": "Pre-trained Language Models are widely used in many important real-world applications. However, recent studies show that these models can encode social biases from large pre-training corpora and even amplify biases in downstream applications. To address this challenge, we propose Co$^2$PT, an efficient and effective debias-while-prompt tuning method for mitigating biases via counterfactual contrastive prompt tuning on downstream tasks. Our experiments conducted on three extrinsic bias benchmarks demonstrate the effectiveness of Co$^2$PT on bias mitigation during the prompt tuning process and its adaptability to existing upstream debiased language models. These findings indicate the strength of Co$^2$PT and provide promising avenues for further enhancement in bias mitigation on downstream tasks.",
            "corpus_id": 264305744,
            "sentences": [
                {
                    "corpus_id": "264305744",
                    "title": "Co$^2$PT: Mitigating Bias in Pre-trained Language Models through Counterfactual Contrastive Prompt Tuning",
                    "text": "We propose Co 2 PT, an efficient and effective debiasing method for mitigating bias in downstream tasks. We evaluate its effectiveness on bias mitigation and applicability to existing debiased upstream models, and investigate how the design of each component and the selection of hyperparameters impact both its bias reduction capabilities and downstream task performance. \n\nMitigating non-gender and intersectional bias. \n\nMitigating non-gender biases is challenging as some debiasing methods work well on reducing gender biases but show poor generalization capabilities in addressing biases beyond gender (Meade et al., 2022). Without re-training the model, Co 2 PT is flexible to apply in order to mitigate different bias types in downstream applications. One can train different debiasing prompts to tackle different bias dimensions such as gender, race, and religion. Furthermore, these debiasing prompts can be applied to mitigate intersectional bias by simply combining the corresponding prompts in downstream tasks.",
                    "score": 0.5118659949069039,
                    "section_title": "Conclusion and Future Work",
                    "char_start_offset": 26498,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 104
                        },
                        {
                            "start": 105,
                            "end": 372
                        },
                        {
                            "start": 375,
                            "end": 421
                        },
                        {
                            "start": 424,
                            "end": 628
                        },
                        {
                            "start": 629,
                            "end": 758
                        },
                        {
                            "start": 759,
                            "end": 872
                        },
                        {
                            "start": 873,
                            "end": 1023
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 607,
                            "end": 627,
                            "matchedPaperCorpusId": "239015827"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.89501953125
                }
            ],
            "relevance_judgement": 0.89501953125,
            "relevance_judgment_input_expanded": "# Title: Co$^2$PT: Mitigating Bias in Pre-trained Language Models through Counterfactual Contrastive Prompt Tuning\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Xiangjue Dong, Ziwei Zhu, Zhuoer Wang, Maria Teleki, James Caverlee\n## Abstract\nPre-trained Language Models are widely used in many important real-world applications. However, recent studies show that these models can encode social biases from large pre-training corpora and even amplify biases in downstream applications. To address this challenge, we propose Co$^2$PT, an efficient and effective debias-while-prompt tuning method for mitigating biases via counterfactual contrastive prompt tuning on downstream tasks. Our experiments conducted on three extrinsic bias benchmarks demonstrate the effectiveness of Co$^2$PT on bias mitigation during the prompt tuning process and its adaptability to existing upstream debiased language models. These findings indicate the strength of Co$^2$PT and provide promising avenues for further enhancement in bias mitigation on downstream tasks.\n## Conclusion and Future Work\nWe propose Co 2 PT, an efficient and effective debiasing method for mitigating bias in downstream tasks. We evaluate its effectiveness on bias mitigation and applicability to existing debiased upstream models, and investigate how the design of each component and the selection of hyperparameters impact both its bias reduction capabilities and downstream task performance. \n\nMitigating non-gender and intersectional bias. \n\nMitigating non-gender biases is challenging as some debiasing methods work well on reducing gender biases but show poor generalization capabilities in addressing biases beyond gender (Meade et al., 2022). Without re-training the model, Co 2 PT is flexible to apply in order to mitigate different bias types in downstream applications. One can train different debiasing prompts to tackle different bias dimensions such as gender, race, and religion. Furthermore, these debiasing prompts can be applied to mitigate intersectional bias by simply combining the corresponding prompts in downstream tasks.",
            "reference_string": "[264305744 | Dong et al. | 2023 | Citations: 11]"
        },
        {
            "title": "Hi, my name is Martha: Using names to measure and mitigate bias in generative dialogue models",
            "venue": "arXiv.org",
            "year": 2021,
            "reference_count": 103,
            "citation_count": 28,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2109.03300, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "51324296",
                    "name": "Eric Michael Smith"
                },
                {
                    "authorId": "81840293",
                    "name": "Adina Williams"
                }
            ],
            "abstract": "All AI models are susceptible to learning biases in data that they are trained on. For generative dialogue models, being trained on real human conversations containing unbalanced gender and race/ethnicity references can lead to models that display learned biases, which we define here broadly as any measurable differences in the distributions of words or semantic content of conversations based on demographic groups. We measure the strength of such biases by producing artificial conversations between two copies of a dialogue model, conditioning one conversational partner to state a name commonly associated with a certain gender and/or race/ethnicity. We find that larger capacity models tend to exhibit more gender bias and greater stereotyping of occupations by gender. We show that several methods of tuning these dialogue models, specifically name scrambling, controlled generation, and unlikelihood training, are effective in reducing bias in conversation, including on a downstream conversational task. Name scrambling is also effective in lowering differences in token usage across conversations where partners have names associated with different genders or races/ethnicities.",
            "corpus_id": 237442178,
            "sentences": [
                {
                    "corpus_id": "237442178",
                    "title": "Hi, my name is Martha: Using names to measure and mitigate bias in generative dialogue models",
                    "text": "We investigate several sources of bias in large dialogue models, and employ a few methods to reduce bias by gender and race/ethnicity in Blender-Bot while maintaining similar levels of performance. We find that the name-scrambling model most effectively reduces bias as measured by a gender bias classifier and most closely matches the token distribution across female/male self-chats; the controlled-generation model is rated by crowdsourced workers as being most preferable and most human; and the unlikelihood-training model has the lowest gender-classifier bias on a downstream task where one of the speakers explicitly states their gender. \n\nPractitioners should choose mitigation techniques that match their use case: the namescrambling method might be most applicable for a model where invariance to names is of most essential importance (e.g., in task oriented dialogue, when scheduling meetings with individuals); the controlled-generation method might be ideal for reducing bias on dialogue while ensuring no loss of performance; and the unlikelihoodtraining method may be best for suppressing gender bias on more than just names in a generalizable fashion. Future extensions could experiment with bias-mitigation techniques beyond these three or measure these techniques' performance on additional forms of bias in language.",
                    "score": 0.5579249913172559,
                    "section_title": "Conclusion",
                    "char_start_offset": 31686,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 197
                        },
                        {
                            "start": 198,
                            "end": 644
                        },
                        {
                            "start": 647,
                            "end": 1167
                        },
                        {
                            "start": 1168,
                            "end": 1335
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.89208984375
                }
            ],
            "relevance_judgement": 0.89208984375,
            "relevance_judgment_input_expanded": "# Title: Hi, my name is Martha: Using names to measure and mitigate bias in generative dialogue models\n# Venue: arXiv.org\n# Authors: Eric Michael Smith, Adina Williams\n## Abstract\nAll AI models are susceptible to learning biases in data that they are trained on. For generative dialogue models, being trained on real human conversations containing unbalanced gender and race/ethnicity references can lead to models that display learned biases, which we define here broadly as any measurable differences in the distributions of words or semantic content of conversations based on demographic groups. We measure the strength of such biases by producing artificial conversations between two copies of a dialogue model, conditioning one conversational partner to state a name commonly associated with a certain gender and/or race/ethnicity. We find that larger capacity models tend to exhibit more gender bias and greater stereotyping of occupations by gender. We show that several methods of tuning these dialogue models, specifically name scrambling, controlled generation, and unlikelihood training, are effective in reducing bias in conversation, including on a downstream conversational task. Name scrambling is also effective in lowering differences in token usage across conversations where partners have names associated with different genders or races/ethnicities.\n## Conclusion\nWe investigate several sources of bias in large dialogue models, and employ a few methods to reduce bias by gender and race/ethnicity in Blender-Bot while maintaining similar levels of performance. We find that the name-scrambling model most effectively reduces bias as measured by a gender bias classifier and most closely matches the token distribution across female/male self-chats; the controlled-generation model is rated by crowdsourced workers as being most preferable and most human; and the unlikelihood-training model has the lowest gender-classifier bias on a downstream task where one of the speakers explicitly states their gender. \n\nPractitioners should choose mitigation techniques that match their use case: the namescrambling method might be most applicable for a model where invariance to names is of most essential importance (e.g., in task oriented dialogue, when scheduling meetings with individuals); the controlled-generation method might be ideal for reducing bias on dialogue while ensuring no loss of performance; and the unlikelihoodtraining method may be best for suppressing gender bias on more than just names in a generalizable fashion. Future extensions could experiment with bias-mitigation techniques beyond these three or measure these techniques' performance on additional forms of bias in language.",
            "reference_string": "[237442178 | Smith et al. | 2021 | Citations: 28]"
        },
        {
            "title": "An Empirical Survey of the Effectiveness of Debiasing Techniques for Pre-trained Language Models",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2021,
            "reference_count": 48,
            "citation_count": 128,
            "influential_citation_count": 21,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2022.acl-long.132.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2110.08527, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "150247363",
                    "name": "Nicholas Meade"
                },
                {
                    "authorId": "2133330526",
                    "name": "Elinor Poole-Dayan"
                },
                {
                    "authorId": "145732771",
                    "name": "Siva Reddy"
                }
            ],
            "abstract": "Recent work has shown pre-trained language models capture social biases from the large amounts of text they are trained on. This has attracted attention to developing techniques that mitigate such biases. In this work, we perform an empirical survey of five recently proposed bias mitigation techniques: Counterfactual Data Augmentation (CDA), Dropout, Iterative Nullspace Projection, Self-Debias, and SentenceDebias. We quantify the effectiveness of each technique using three intrinsic bias benchmarks while also measuring the impact of these techniques on a model\u2019s language modeling ability, as well as its performance on downstream NLU tasks. We experimentally find that: (1) Self-Debias is the strongest debiasing technique, obtaining improved scores on all bias benchmarks; (2) Current debiasing techniques perform less consistently when mitigating non-gender biases; And (3) improvements on bias benchmarks such as StereoSet and CrowS-Pairs by using debiasing strategies are often accompanied by a decrease in language modeling ability, making it difficult to determine whether the bias mitigation was effective.",
            "corpus_id": 239015827,
            "sentences": [
                {
                    "corpus_id": "239015827",
                    "title": "An Empirical Survey of the Effectiveness of Debiasing Techniques for Pre-trained Language Models",
                    "text": "Recent work has shown pre-trained language models capture social biases from the large amounts of text they are trained on. This has attracted attention to developing techniques that mitigate such biases. In this work, we perform an empirical survey of five recently proposed bias mitigation techniques: Counterfactual Data Augmentation (CDA), Dropout, Iterative Nullspace Projection, Self-Debias, and SentenceDebias. We quantify the effectiveness of each technique using three intrinsic bias benchmarks while also measuring the impact of these techniques on a model\u2019s language modeling ability, as well as its performance on downstream NLU tasks. We experimentally find that: (1) Self-Debias is the strongest debiasing technique, obtaining improved scores on all bias benchmarks; (2) Current debiasing techniques perform less consistently when mitigating non-gender biases; And (3) improvements on bias benchmarks such as StereoSet and CrowS-Pairs by using debiasing strategies are often accompanied by a decrease in language modeling ability, making it difficult to determine whether the bias mitigation was effective.",
                    "score": 0.6504556481541399,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88671875
                },
                {
                    "corpus_id": "239015827",
                    "title": "An Empirical Survey of the Effectiveness of Debiasing Techniques for Pre-trained Language Models",
                    "text": "To the best of our knowledge, we have performed the first large scale evaluation of multiple debiasing techniques for pre-trained language models. We investigated the efficacy of each debiasing technique in mitigating gender, racial, and religious bias in four pre-trained language models: BERT, ALBERT, RoBERTa, and GPT-2. We used three intrinsic bias benchmarks to evaluate the effectiveness of each debiasing technique in mitigating bias and also investigated how debiasing impacts language modeling and downstream NLU task performance. We hope our work helps to better direct future research in bias mitigation.",
                    "score": 0.6975828528348689,
                    "section_title": "Conclusion",
                    "char_start_offset": 25377,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8642578125
                }
            ],
            "relevance_judgement": 0.88671875,
            "relevance_judgment_input_expanded": "# Title: An Empirical Survey of the Effectiveness of Debiasing Techniques for Pre-trained Language Models\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Nicholas Meade, Elinor Poole-Dayan, Siva Reddy\n## Abstract\nRecent work has shown pre-trained language models capture social biases from the large amounts of text they are trained on. This has attracted attention to developing techniques that mitigate such biases. In this work, we perform an empirical survey of five recently proposed bias mitigation techniques: Counterfactual Data Augmentation (CDA), Dropout, Iterative Nullspace Projection, Self-Debias, and SentenceDebias. We quantify the effectiveness of each technique using three intrinsic bias benchmarks while also measuring the impact of these techniques on a model\u2019s language modeling ability, as well as its performance on downstream NLU tasks. We experimentally find that: (1) Self-Debias is the strongest debiasing technique, obtaining improved scores on all bias benchmarks; (2) Current debiasing techniques perform less consistently when mitigating non-gender biases; And (3) improvements on bias benchmarks such as StereoSet and CrowS-Pairs by using debiasing strategies are often accompanied by a decrease in language modeling ability, making it difficult to determine whether the bias mitigation was effective.\n## Conclusion\nTo the best of our knowledge, we have performed the first large scale evaluation of multiple debiasing techniques for pre-trained language models. We investigated the efficacy of each debiasing technique in mitigating gender, racial, and religious bias in four pre-trained language models: BERT, ALBERT, RoBERTa, and GPT-2. We used three intrinsic bias benchmarks to evaluate the effectiveness of each debiasing technique in mitigating bias and also investigated how debiasing impacts language modeling and downstream NLU task performance. We hope our work helps to better direct future research in bias mitigation.",
            "reference_string": "[239015827 | Meade et al. | 2021 | Citations: 128]"
        },
        {
            "title": "Identifying and Adapting Transformer-Components Responsible for Gender Bias in an English Language Model",
            "venue": "BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP",
            "year": 2023,
            "reference_count": 66,
            "citation_count": 18,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.12611, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2260342327",
                    "name": "Abhijith Chintam"
                },
                {
                    "authorId": "2260341585",
                    "name": "Rahel Beloch"
                },
                {
                    "authorId": "2254288138",
                    "name": "Willem Zuidema"
                },
                {
                    "authorId": "2140766524",
                    "name": "Michael Hanna"
                },
                {
                    "authorId": "1986356851",
                    "name": "Oskar van der Wal"
                }
            ],
            "abstract": "Language models (LMs) exhibit and amplify many types of undesirable biases learned from the training data, including gender bias. However, we lack tools for effectively and efficiently changing this behavior without hurting general language modeling performance. In this paper, we study three methods for identifying causal relations between LM components and particular output: causal mediation analysis, automated circuit discovery and our novel, efficient method called DiffMask+ based on differential masking. We apply the methods to GPT-2 small and the problem of gender bias, and use the discovered sets of components to perform parameter-efficient fine-tuning for bias mitigation. Our results show significant overlap in the identified components (despite huge differences in the computational requirements of the methods) as well as success in mitigating gender bias, with less damage to general language modeling compared to full model fine-tuning. However, our work also underscores the difficulty of defining and measuring bias, and the sensitivity of causal discovery procedures to dataset choice. We hope our work can contribute to more attention for dataset development, and lead to more effective mitigation strategies for other types of bias.",
            "corpus_id": 264306211,
            "sentences": [
                {
                    "corpus_id": "264306211",
                    "title": "Identifying and Adapting Transformer-Components Responsible for Gender Bias in an English Language Model",
                    "text": "Language models (LMs) exhibit and amplify many types of undesirable biases learned from the training data, including gender bias. However, we lack tools for effectively and efficiently changing this behavior without hurting general language modeling performance. In this paper, we study three methods for identifying causal relations between LM components and particular output: causal mediation analysis, automated circuit discovery and our novel, efficient method called DiffMask+ based on differential masking. We apply the methods to GPT-2 small and the problem of gender bias, and use the discovered sets of components to perform parameter-efficient fine-tuning for bias mitigation. Our results show significant overlap in the identified components (despite huge differences in the computational requirements of the methods) as well as success in mitigating gender bias, with less damage to general language modeling compared to full model fine-tuning. However, our work also underscores the difficulty of defining and measuring bias, and the sensitivity of causal discovery procedures to dataset choice. We hope our work can contribute to more attention for dataset development, and lead to more effective mitigation strategies for other types of bias.",
                    "score": 0.5415145753198071,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88525390625
                }
            ],
            "relevance_judgement": 0.88525390625,
            "relevance_judgment_input_expanded": "# Title: Identifying and Adapting Transformer-Components Responsible for Gender Bias in an English Language Model\n# Venue: BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP\n# Authors: Abhijith Chintam, Rahel Beloch, Willem Zuidema, Michael Hanna, Oskar van der Wal\n## Abstract\nLanguage models (LMs) exhibit and amplify many types of undesirable biases learned from the training data, including gender bias. However, we lack tools for effectively and efficiently changing this behavior without hurting general language modeling performance. In this paper, we study three methods for identifying causal relations between LM components and particular output: causal mediation analysis, automated circuit discovery and our novel, efficient method called DiffMask+ based on differential masking. We apply the methods to GPT-2 small and the problem of gender bias, and use the discovered sets of components to perform parameter-efficient fine-tuning for bias mitigation. Our results show significant overlap in the identified components (despite huge differences in the computational requirements of the methods) as well as success in mitigating gender bias, with less damage to general language modeling compared to full model fine-tuning. However, our work also underscores the difficulty of defining and measuring bias, and the sensitivity of causal discovery procedures to dataset choice. We hope our work can contribute to more attention for dataset development, and lead to more effective mitigation strategies for other types of bias.\n",
            "reference_string": "[264306211 | Chintam et al. | 2023 | Citations: 18]"
        },
        {
            "title": "Should We Attend More or Less? Modulating Attention for Fairness",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 76,
            "citation_count": 10,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2305.13088",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.13088, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2077390471",
                    "name": "A. Zayed"
                },
                {
                    "authorId": "24039720",
                    "name": "Gon\u00e7alo Mordido"
                },
                {
                    "authorId": "3197429",
                    "name": "S. Shabanian"
                },
                {
                    "authorId": "123607932",
                    "name": "Sarath Chandar"
                }
            ],
            "abstract": "The advances in natural language processing (NLP) pose both opportunities and challenges. While recent progress enables the development of high-performing models for a variety of tasks, it also poses the risk of models learning harmful biases from the data, such as gender stereotypes. In this work, we investigate the role of attention, a widely-used technique in current state-of-the-art NLP models, in the propagation of social biases. Specifically, we study the relationship between the entropy of the attention distribution and the model's performance and fairness. We then propose a novel method for modulating attention weights to improve model fairness after training. Since our method is only applied post-training and pre-inference, it is an intra-processing method and is, therefore, less computationally expensive than existing in-processing and pre-processing approaches. Our results show an increase in fairness and minimal performance loss on different text classification and generation tasks using language models of varying sizes. WARNING: This work uses language that is offensive.",
            "corpus_id": 258832694,
            "sentences": [
                {
                    "corpus_id": "258832694",
                    "title": "Should We Attend More or Less? Modulating Attention for Fairness",
                    "text": "Gender bias mitigation methods may be broadly classified into two categories: intrinsic and extrinsic approaches. While intrinsic methods (Adi et al., 2017;Hupkes et al., 2018;Conneau et al., 2018;Tenney et al., 2019;Belinkov & Glass, 2019) focus on analyzing the embedding representations assigned to gender tokens by the model, extrinsic methods (Sennrich, 2017;Isabelle et al., 2017;Naik et al., 2018) rely on the model's predictions to determine if different genders achieve similar predictions under the same context. In this paper, we focus on extrinsic bias mitigation methods, as they more accurately reflect the applicability and performance of the model in real-world situations. \n\nPre-processing methods for mitigating gender bias involve modifying the training data to improve model fairness. One common method is counterfactual data augmentation (CDA) (Lu et al., 2020) (Dixon et al., 2018) adds new examples only for under-represented groups in the dataset. \n\nIn-processing bias mitigation methods aim to reduce bias during training by adding auxiliary loss terms to the model. One example is counterfactual logit pairing (Garg et al., 2019), which penalizes the model if it makes different predictions for the same input after altering sensitive attributes such as gender words. Another method by Kennedy et al. (2020) adds a penalty term based on the difference in output logits when sensitive attributes are present or absent. Instance weighting (Zhang et al., 2020) multiplies the loss by a factor greater than 1 for stereotypical sentences to penalize the model more for misclassifying them, and attentionbased regularization (Attanasio et al., 2022) maximizes the model's attention distribution entropy during training to improve fairness. MABEL (He et al., 2022) proposes adding an auxiliary loss to minimize the cosine similarity between the original and gender-swapped vector representation for each example. \n\nWhile relatively less explored, post-processing bias mitigation methods modify the predictions of a biased model and generate a new set of less biased predictions.",
                    "score": 0.5106003029695988,
                    "section_title": "Gender bias mitigation methods",
                    "char_start_offset": 4908,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 113
                        },
                        {
                            "start": 114,
                            "end": 522
                        },
                        {
                            "start": 523,
                            "end": 689
                        },
                        {
                            "start": 692,
                            "end": 804
                        },
                        {
                            "start": 805,
                            "end": 971
                        },
                        {
                            "start": 974,
                            "end": 1091
                        },
                        {
                            "start": 1092,
                            "end": 1293
                        },
                        {
                            "start": 1294,
                            "end": 1443
                        },
                        {
                            "start": 1444,
                            "end": 1759
                        },
                        {
                            "start": 1760,
                            "end": 1931
                        },
                        {
                            "start": 1934,
                            "end": 2097
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 138,
                            "end": 156,
                            "matchedPaperCorpusId": "6771196"
                        },
                        {
                            "start": 156,
                            "end": 176,
                            "matchedPaperCorpusId": "5013113"
                        },
                        {
                            "start": 176,
                            "end": 197,
                            "matchedPaperCorpusId": "24461982"
                        },
                        {
                            "start": 197,
                            "end": 217,
                            "matchedPaperCorpusId": "155092004"
                        },
                        {
                            "start": 386,
                            "end": 404,
                            "matchedPaperCorpusId": "46932607"
                        },
                        {
                            "start": 865,
                            "end": 882,
                            "matchedPaperCorpusId": "51888520"
                        },
                        {
                            "start": 883,
                            "end": 903,
                            "matchedPaperCorpusId": "54997157"
                        },
                        {
                            "start": 1136,
                            "end": 1155,
                            "matchedPaperCorpusId": "52880735"
                        },
                        {
                            "start": 1312,
                            "end": 1333,
                            "matchedPaperCorpusId": "218517088"
                        },
                        {
                            "start": 1463,
                            "end": 1483,
                            "matchedPaperCorpusId": "216641659"
                        },
                        {
                            "start": 1645,
                            "end": 1669,
                            "matchedPaperCorpusId": "247519021"
                        },
                        {
                            "start": 1766,
                            "end": 1783,
                            "matchedPaperCorpusId": "253157517"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88232421875
                }
            ],
            "relevance_judgement": 0.88232421875,
            "relevance_judgment_input_expanded": "# Title: Should We Attend More or Less? Modulating Attention for Fairness\n# Venue: arXiv.org\n# Authors: A. Zayed, Gon\u00e7alo Mordido, S. Shabanian, Sarath Chandar\n## Abstract\nThe advances in natural language processing (NLP) pose both opportunities and challenges. While recent progress enables the development of high-performing models for a variety of tasks, it also poses the risk of models learning harmful biases from the data, such as gender stereotypes. In this work, we investigate the role of attention, a widely-used technique in current state-of-the-art NLP models, in the propagation of social biases. Specifically, we study the relationship between the entropy of the attention distribution and the model's performance and fairness. We then propose a novel method for modulating attention weights to improve model fairness after training. Since our method is only applied post-training and pre-inference, it is an intra-processing method and is, therefore, less computationally expensive than existing in-processing and pre-processing approaches. Our results show an increase in fairness and minimal performance loss on different text classification and generation tasks using language models of varying sizes. WARNING: This work uses language that is offensive.\n## Gender bias mitigation methods\nGender bias mitigation methods may be broadly classified into two categories: intrinsic and extrinsic approaches. While intrinsic methods (Adi et al., 2017;Hupkes et al., 2018;Conneau et al., 2018;Tenney et al., 2019;Belinkov & Glass, 2019) focus on analyzing the embedding representations assigned to gender tokens by the model, extrinsic methods (Sennrich, 2017;Isabelle et al., 2017;Naik et al., 2018) rely on the model's predictions to determine if different genders achieve similar predictions under the same context. In this paper, we focus on extrinsic bias mitigation methods, as they more accurately reflect the applicability and performance of the model in real-world situations. \n\nPre-processing methods for mitigating gender bias involve modifying the training data to improve model fairness. One common method is counterfactual data augmentation (CDA) (Lu et al., 2020) (Dixon et al., 2018) adds new examples only for under-represented groups in the dataset. \n\nIn-processing bias mitigation methods aim to reduce bias during training by adding auxiliary loss terms to the model. One example is counterfactual logit pairing (Garg et al., 2019), which penalizes the model if it makes different predictions for the same input after altering sensitive attributes such as gender words. Another method by Kennedy et al. (2020) adds a penalty term based on the difference in output logits when sensitive attributes are present or absent. Instance weighting (Zhang et al., 2020) multiplies the loss by a factor greater than 1 for stereotypical sentences to penalize the model more for misclassifying them, and attentionbased regularization (Attanasio et al., 2022) maximizes the model's attention distribution entropy during training to improve fairness. MABEL (He et al., 2022) proposes adding an auxiliary loss to minimize the cosine similarity between the original and gender-swapped vector representation for each example. \n\nWhile relatively less explored, post-processing bias mitigation methods modify the predictions of a biased model and generate a new set of less biased predictions.",
            "reference_string": "[258832694 | Zayed et al. | 2023 | Citations: 10]"
        },
        {
            "title": "DiFair: A Benchmark for Disentangled Assessment of Gender Knowledge and Bias",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2023,
            "reference_count": 0,
            "citation_count": 7,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.14329, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2261278375",
                    "name": "Mahdi Zakizadeh"
                },
                {
                    "authorId": "2261278011",
                    "name": "Kaveh Eskandari Miandoab"
                },
                {
                    "authorId": "1717641",
                    "name": "Mohammad Taher Pilehvar"
                }
            ],
            "abstract": "Numerous debiasing techniques have been proposed to mitigate the gender bias that is prevalent in pretrained language models. These are often evaluated on datasets that check the extent to which the model is gender-neutral in its predictions. Importantly, this evaluation protocol overlooks the possible adverse impact of bias mitigation on useful gender knowledge. To fill this gap, we propose DiFair, a manually curated dataset based on masked language modeling objectives. DiFair allows us to introduce a unified metric, gender invariance score, that not only quantifies a model's biased behavior, but also checks if useful gender knowledge is preserved. We use DiFair as a benchmark for a number of widely-used pretained language models and debiasing techniques. Experimental results corroborate previous findings on the existing gender biases, while also demonstrating that although debiasing techniques ameliorate the issue of gender bias, this improvement usually comes at the price of lowering useful gender knowledge of the model.",
            "corpus_id": 264425921,
            "sentences": [
                {
                    "corpus_id": "264425921",
                    "title": "DiFair: A Benchmark for Disentangled Assessment of Gender Knowledge and Bias",
                    "text": "It is widely acknowledged that pre-trained language models may demonstrate biased behavior against underrepresented demographic groups, such as women (Silva et al., 2021) or racial minorities (Field et al., 2021). Given the broad adoption of these models across various use cases, it is imperative for social good to understand these biases and strive to mitigate them while retaining factual gender information that is required to make meaningful gender-based predictions. \n\nIn recent years, numerous studies have attempted to address the biased behaviour of language models, either by manipulating the training data (Webster et al., 2020), altering the training objective (Kaneko and Bollegala, 2021), or by modifying the architecture (Lauscher et al., 2021). Although \u2020 Work done as a Master's student at TeIAS. Figure 1: Ideally, a language model is expected not to favor a gender in a sentence that does not explicitly specify one (top example), while it should prefer genderspecific words when the gender is explicitly specified in the sentence (bottom example). Cf. Section 2.1 for task formulation. \n\ncurrent debiasing techniques, such as counterfactual augmentation (Zhao et al., 2018) and dropout techniques (Webster et al., 2020), are effective in removing biased information from model representations, recent studies have shown that such debiasing can damage a model's useful gender knowledge (Limisiewicz and Mare\u010dek, 2022). This suggests the need for more robust metrics for measuring bias in NLP models that simultaneously consider both performance and fairness. \n\nIn this study, we try to fill this evaluation gap by presenting DIFAIR, a benchmark for evaluating gender bias in language models via a masked lan-guage modeling (MLM) objective, while also considering the preservation of relevant gender data. We test several widely-used pretrained models on this dataset, along with recent debiasing techniques. Our findings echo prior research, indicating that these models struggle to discern when to differentiate between genders. This is emphasized by their performance lag compared to human upper bound on our dataset. We also note that while debiasing techniques enhance gender fairness, they typically compromise the model's ability to retain factual gender information.",
                    "score": 0.5756771013267912,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 213
                        },
                        {
                            "start": 214,
                            "end": 473
                        },
                        {
                            "start": 476,
                            "end": 761
                        },
                        {
                            "start": 762,
                            "end": 1068
                        },
                        {
                            "start": 1069,
                            "end": 1072
                        },
                        {
                            "start": 1073,
                            "end": 1106
                        },
                        {
                            "start": 1109,
                            "end": 1438
                        },
                        {
                            "start": 1439,
                            "end": 1578
                        },
                        {
                            "start": 1581,
                            "end": 1824
                        },
                        {
                            "start": 1825,
                            "end": 1927
                        },
                        {
                            "start": 1928,
                            "end": 2049
                        },
                        {
                            "start": 2050,
                            "end": 2139
                        },
                        {
                            "start": 2140,
                            "end": 2293
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 192,
                            "end": 212,
                            "matchedPaperCorpusId": "235593087"
                        },
                        {
                            "start": 674,
                            "end": 702,
                            "matchedPaperCorpusId": "231698657"
                        },
                        {
                            "start": 737,
                            "end": 760,
                            "matchedPaperCorpusId": "237440429"
                        },
                        {
                            "start": 1175,
                            "end": 1194,
                            "matchedPaperCorpusId": "4952494"
                        },
                        {
                            "start": 1406,
                            "end": 1437,
                            "matchedPaperCorpusId": "248980783"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8818359375
                }
            ],
            "relevance_judgement": 0.8818359375,
            "relevance_judgment_input_expanded": "# Title: DiFair: A Benchmark for Disentangled Assessment of Gender Knowledge and Bias\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Mahdi Zakizadeh, Kaveh Eskandari Miandoab, Mohammad Taher Pilehvar\n## Abstract\nNumerous debiasing techniques have been proposed to mitigate the gender bias that is prevalent in pretrained language models. These are often evaluated on datasets that check the extent to which the model is gender-neutral in its predictions. Importantly, this evaluation protocol overlooks the possible adverse impact of bias mitigation on useful gender knowledge. To fill this gap, we propose DiFair, a manually curated dataset based on masked language modeling objectives. DiFair allows us to introduce a unified metric, gender invariance score, that not only quantifies a model's biased behavior, but also checks if useful gender knowledge is preserved. We use DiFair as a benchmark for a number of widely-used pretained language models and debiasing techniques. Experimental results corroborate previous findings on the existing gender biases, while also demonstrating that although debiasing techniques ameliorate the issue of gender bias, this improvement usually comes at the price of lowering useful gender knowledge of the model.\n## Introduction\nIt is widely acknowledged that pre-trained language models may demonstrate biased behavior against underrepresented demographic groups, such as women (Silva et al., 2021) or racial minorities (Field et al., 2021). Given the broad adoption of these models across various use cases, it is imperative for social good to understand these biases and strive to mitigate them while retaining factual gender information that is required to make meaningful gender-based predictions. \n\nIn recent years, numerous studies have attempted to address the biased behaviour of language models, either by manipulating the training data (Webster et al., 2020), altering the training objective (Kaneko and Bollegala, 2021), or by modifying the architecture (Lauscher et al., 2021). Although \u2020 Work done as a Master's student at TeIAS. Figure 1: Ideally, a language model is expected not to favor a gender in a sentence that does not explicitly specify one (top example), while it should prefer genderspecific words when the gender is explicitly specified in the sentence (bottom example). Cf. Section 2.1 for task formulation. \n\ncurrent debiasing techniques, such as counterfactual augmentation (Zhao et al., 2018) and dropout techniques (Webster et al., 2020), are effective in removing biased information from model representations, recent studies have shown that such debiasing can damage a model's useful gender knowledge (Limisiewicz and Mare\u010dek, 2022). This suggests the need for more robust metrics for measuring bias in NLP models that simultaneously consider both performance and fairness. \n\nIn this study, we try to fill this evaluation gap by presenting DIFAIR, a benchmark for evaluating gender bias in language models via a masked lan-guage modeling (MLM) objective, while also considering the preservation of relevant gender data. We test several widely-used pretrained models on this dataset, along with recent debiasing techniques. Our findings echo prior research, indicating that these models struggle to discern when to differentiate between genders. This is emphasized by their performance lag compared to human upper bound on our dataset. We also note that while debiasing techniques enhance gender fairness, they typically compromise the model's ability to retain factual gender information.",
            "reference_string": "[264425921 | Zakizadeh et al. | 2023 | Citations: 7]"
        },
        {
            "title": "Reducing Gender Bias in Abusive Language Detection",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2018,
            "reference_count": 20,
            "citation_count": 341,
            "influential_citation_count": 28,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/D18-1302.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1808.07231, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2116023415",
                    "name": "Ji Ho Park"
                },
                {
                    "authorId": "51228826",
                    "name": "Jamin Shin"
                },
                {
                    "authorId": "1683412",
                    "name": "Pascale Fung"
                }
            ],
            "abstract": "Abusive language detection models tend to have a problem of being biased toward identity words of a certain group of people because of imbalanced training datasets. For example, \u201cYou are a good woman\u201d was considered \u201csexist\u201d when trained on an existing dataset. Such model bias is an obstacle for models to be robust enough for practical use. In this work, we measure them on models trained with different datasets, while analyzing the effect of different pre-trained word embeddings and model architectures. We also experiment with three mitigation methods: (1) debiased word embeddings, (2) gender swap data augmentation, and (3) fine-tuning with a larger corpus. These methods can effectively reduce model bias by 90-98% and can be extended to correct model bias in other scenarios.",
            "corpus_id": 52070035,
            "sentences": [
                {
                    "corpus_id": "52070035",
                    "title": "Reducing Gender Bias in Abusive Language Detection",
                    "text": "Table 6 shows the results of experiments using the three methods proposed. The first rows are the baselines without any method applied. We can see from the second rows of each section that debiased word embeddings alone do not effectively correct the bias of the whole system that well, while gender swapping significantly reduced both the equality difference scores. Meanwhile, fine-tuning bias with a larger, less biased source dataset helped to decrease the equality difference scores and greatly improve the AUC scores from the generated unbiased test set. The latter improvement shows that the model significantly reduced errors on the unbiased set in general. \n\nTo our surprise, the most effective method was applying both debiased embedding and gender swap to GRU, which reduced the equality differences by 98% & 89% while losing only 1.5% of the original performance. We assume that this may be related to the influence of \"attending\" model architectures on biases as discussed in Section 4.3. On the other hand, using the three methods together improved both generated unbiased set performance and equality differences, but had the largest decrease in the original performance. \n\nAll methods involved some performance loss when gender biases were reduced. Especially, fine-tuning had the largest decrease in original test set performance. This could be attributed to the difference in the source and target tasks (abusive & sexist). However, the decrease was marginal (less than 4%), while the drop in bias was significant. We assume the performance loss happens because mitigation methods modify the data or the model in a way that sometimes deters the models from discriminating important \"unbiased\" features.",
                    "score": 0.5229092986533025,
                    "section_title": "Results & Discussion",
                    "char_start_offset": 12092,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 74
                        },
                        {
                            "start": 75,
                            "end": 135
                        },
                        {
                            "start": 136,
                            "end": 367
                        },
                        {
                            "start": 368,
                            "end": 560
                        },
                        {
                            "start": 561,
                            "end": 665
                        },
                        {
                            "start": 668,
                            "end": 875
                        },
                        {
                            "start": 876,
                            "end": 1001
                        },
                        {
                            "start": 1002,
                            "end": 1186
                        },
                        {
                            "start": 1189,
                            "end": 1264
                        },
                        {
                            "start": 1265,
                            "end": 1347
                        },
                        {
                            "start": 1348,
                            "end": 1441
                        },
                        {
                            "start": 1442,
                            "end": 1532
                        },
                        {
                            "start": 1533,
                            "end": 1720
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8779296875
                }
            ],
            "relevance_judgement": 0.8779296875,
            "relevance_judgment_input_expanded": "# Title: Reducing Gender Bias in Abusive Language Detection\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Ji Ho Park, Jamin Shin, Pascale Fung\n## Abstract\nAbusive language detection models tend to have a problem of being biased toward identity words of a certain group of people because of imbalanced training datasets. For example, \u201cYou are a good woman\u201d was considered \u201csexist\u201d when trained on an existing dataset. Such model bias is an obstacle for models to be robust enough for practical use. In this work, we measure them on models trained with different datasets, while analyzing the effect of different pre-trained word embeddings and model architectures. We also experiment with three mitigation methods: (1) debiased word embeddings, (2) gender swap data augmentation, and (3) fine-tuning with a larger corpus. These methods can effectively reduce model bias by 90-98% and can be extended to correct model bias in other scenarios.\n## Results & Discussion\nTable 6 shows the results of experiments using the three methods proposed. The first rows are the baselines without any method applied. We can see from the second rows of each section that debiased word embeddings alone do not effectively correct the bias of the whole system that well, while gender swapping significantly reduced both the equality difference scores. Meanwhile, fine-tuning bias with a larger, less biased source dataset helped to decrease the equality difference scores and greatly improve the AUC scores from the generated unbiased test set. The latter improvement shows that the model significantly reduced errors on the unbiased set in general. \n\nTo our surprise, the most effective method was applying both debiased embedding and gender swap to GRU, which reduced the equality differences by 98% & 89% while losing only 1.5% of the original performance. We assume that this may be related to the influence of \"attending\" model architectures on biases as discussed in Section 4.3. On the other hand, using the three methods together improved both generated unbiased set performance and equality differences, but had the largest decrease in the original performance. \n\nAll methods involved some performance loss when gender biases were reduced. Especially, fine-tuning had the largest decrease in original test set performance. This could be attributed to the difference in the source and target tasks (abusive & sexist). However, the decrease was marginal (less than 4%), while the drop in bias was significant. We assume the performance loss happens because mitigation methods modify the data or the model in a way that sometimes deters the models from discriminating important \"unbiased\" features.",
            "reference_string": "[52070035 | Park et al. | 2018 | Citations: 341]"
        },
        {
            "title": "BiasDPO: Mitigating Bias in Language Models through Direct Preference Optimization",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2024,
            "reference_count": 19,
            "citation_count": 10,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.13928, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2312204915",
                    "name": "Ahmed Allam"
                }
            ],
            "abstract": "Large Language Models (LLMs) have become pivotal in advancing natural language processing, yet their potential to perpetuate biases poses significant concerns. This paper introduces a new framework employing Direct Preference Optimization (DPO) to mitigate gender, racial, and religious biases in LLM-generated English text. By developing a loss function that favors less biased over biased completions, our approach cultivates a preference for respectful and non-discriminatory language in LLMs. We also contribute a manually designed dataset for training LLMs to recognize and correct biases. This dataset encompasses a diverse range of prompts paired with both biased and unbiased completions. Implementing this approach on the Microsoft Phi-2 model, we demonstrate substantial reductions in biased outputs as our model outperforms the baseline model on almost all bias benchmarks. Our model also achieves better performance compared to other open-source models on most benchmarks. By reducing biases in the language generated by the model, our study marks a significant step towards developing more ethical and socially responsible LLMs. We publicly release BiasDPO dataset on HuggingFace.",
            "corpus_id": 271310069,
            "sentences": [
                {
                    "corpus_id": "271310069",
                    "title": "BiasDPO: Mitigating Bias in Language Models through Direct Preference Optimization",
                    "text": "Large Language Models (LLMs) have become pivotal in advancing natural language processing, yet their potential to perpetuate biases poses significant concerns. This paper introduces a new framework employing Direct Preference Optimization (DPO) to mitigate gender, racial, and religious biases in LLM-generated English text. By developing a loss function that favors less biased over biased completions, our approach cultivates a preference for respectful and non-discriminatory language in LLMs. We also contribute a manually designed dataset for training LLMs to recognize and correct biases. This dataset encompasses a diverse range of prompts paired with both biased and unbiased completions. Implementing this approach on the Microsoft Phi-2 model, we demonstrate substantial reductions in biased outputs as our model outperforms the baseline model on almost all bias benchmarks. Our model also achieves better performance compared to other open-source models on most benchmarks. By reducing biases in the language generated by the model, our study marks a significant step towards developing more ethical and socially responsible LLMs. We publicly release BiasDPO dataset on HuggingFace.",
                    "score": 0.5688024999059451,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.87744140625
                }
            ],
            "relevance_judgement": 0.87744140625,
            "relevance_judgment_input_expanded": "# Title: BiasDPO: Mitigating Bias in Language Models through Direct Preference Optimization\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Ahmed Allam\n## Abstract\nLarge Language Models (LLMs) have become pivotal in advancing natural language processing, yet their potential to perpetuate biases poses significant concerns. This paper introduces a new framework employing Direct Preference Optimization (DPO) to mitigate gender, racial, and religious biases in LLM-generated English text. By developing a loss function that favors less biased over biased completions, our approach cultivates a preference for respectful and non-discriminatory language in LLMs. We also contribute a manually designed dataset for training LLMs to recognize and correct biases. This dataset encompasses a diverse range of prompts paired with both biased and unbiased completions. Implementing this approach on the Microsoft Phi-2 model, we demonstrate substantial reductions in biased outputs as our model outperforms the baseline model on almost all bias benchmarks. Our model also achieves better performance compared to other open-source models on most benchmarks. By reducing biases in the language generated by the model, our study marks a significant step towards developing more ethical and socially responsible LLMs. We publicly release BiasDPO dataset on HuggingFace.\n",
            "reference_string": "[271310069 | Allam | 2024 | Citations: 10]"
        },
        {
            "title": "How Far Can It Go? On Intrinsic Gender Bias Mitigation for Text Classification",
            "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
            "year": 2023,
            "reference_count": 35,
            "citation_count": 8,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2301.12855",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2301.12855, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2145259446",
                    "name": "E. Tokpo"
                },
                {
                    "authorId": "150258834",
                    "name": "Pieter Delobelle"
                },
                {
                    "authorId": "2990203",
                    "name": "Bettina Berendt"
                },
                {
                    "authorId": "1709830",
                    "name": "T. Calders"
                }
            ],
            "abstract": "To mitigate gender bias in contextualized language models, different intrinsic mitigation strategies have been proposed, alongside many bias metrics. Considering that the end use of these language models is for downstream tasks like text classification, it is important to understand how these intrinsic bias mitigation strategies actually translate to fairness in downstream tasks and the extent of this.In this work, we design a probe to investigate the effects that some of the major intrinsic gender bias mitigation strategies have on downstream text classification tasks. We discover that instead of resolving gender bias, intrinsic mitigation techniques and metrics are able to hide it in such a way that significant gender information is retained in the embeddings. Furthermore, we show that each mitigation technique is able to hide the bias from some of the intrinsic bias measures but not all, and each intrinsic bias measure can be fooled by some mitigation techniques, but not all. We confirm experimentally, that none of the intrinsic mitigation techniques used without any other fairness intervention is able to consistently impact extrinsic bias. We recommend that intrinsic bias mitigation techniques should be combined with other fairness interventions for downstream tasks.",
            "corpus_id": 256390197,
            "sentences": [
                {
                    "corpus_id": "256390197",
                    "title": "How Far Can It Go? On Intrinsic Gender Bias Mitigation for Text Classification",
                    "text": "Intrinsic gender bias mitigation methods target either the pretraining data, the pretraining procedure, or the pretrained model's output, which we refer to as pre-processing, in-processing, and postprocessing respectively (Friedler et al., 2019). We select three popular mitigation methods to represent all three types, namely Counterfactual Data Augmentation (CDA), Context-debias, and Sentdebias. Notice that these methods create debiased pretrained language models, as is illustrated in Figure 1. These models still need to be finetuned on a downstream task.\n\nCDA pretraining. The idea behind counterfactual data augmentation (Zmigrod et al., 2019;Lu et al., 2020) is to generate a counterfactual for each example in the training corpus by replacing attribute terms with their complimentary equivalent from the other demographic classes. For example, she will map to he in the case of binary gender. To mitigate intrinsic bias, this counterfactual augmentation has to be done as a pretraining step. Since CDA involves retraining the model, it is more resource-intensive compared to Sent-debias and Context-debias. We use the pretrained CDA models based on BERT and ALBERT from Webster et al. (2020) for our implementation. (2021) introduce a debiasing method that involves retraining the language model with a constraint to make the embeddings of stereotype terms 4 (such as doctor, nurse) orthogonal to embeddings of attribute terms 5 (such as gender pronouns like she, he and gender nouns like woman, man). Given the dynamic nature of contextualized word embeddings which causes a word to have different embeddings, they define fixed word embeddings for each attribute token by averaging the contextual embeddings of a word in all sentences it appears in. Training is done so that the embeddings of all stereotype terms are made orthogonal to all fixed attribute embeddings. They add a regularizer that constrains the debiased embedding to retain as much information by ensuring that they are as similar to the original em",
                    "score": 0.50965774548655,
                    "section_title": "Intrinsic bias mitigation techniques",
                    "char_start_offset": 4439,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8740234375
                }
            ],
            "relevance_judgement": 0.8740234375,
            "relevance_judgment_input_expanded": "# Title: How Far Can It Go? On Intrinsic Gender Bias Mitigation for Text Classification\n# Venue: Conference of the European Chapter of the Association for Computational Linguistics\n# Authors: E. Tokpo, Pieter Delobelle, Bettina Berendt, T. Calders\n## Abstract\nTo mitigate gender bias in contextualized language models, different intrinsic mitigation strategies have been proposed, alongside many bias metrics. Considering that the end use of these language models is for downstream tasks like text classification, it is important to understand how these intrinsic bias mitigation strategies actually translate to fairness in downstream tasks and the extent of this.In this work, we design a probe to investigate the effects that some of the major intrinsic gender bias mitigation strategies have on downstream text classification tasks. We discover that instead of resolving gender bias, intrinsic mitigation techniques and metrics are able to hide it in such a way that significant gender information is retained in the embeddings. Furthermore, we show that each mitigation technique is able to hide the bias from some of the intrinsic bias measures but not all, and each intrinsic bias measure can be fooled by some mitigation techniques, but not all. We confirm experimentally, that none of the intrinsic mitigation techniques used without any other fairness intervention is able to consistently impact extrinsic bias. We recommend that intrinsic bias mitigation techniques should be combined with other fairness interventions for downstream tasks.\n## Intrinsic bias mitigation techniques\nIntrinsic gender bias mitigation methods target either the pretraining data, the pretraining procedure, or the pretrained model's output, which we refer to as pre-processing, in-processing, and postprocessing respectively (Friedler et al., 2019). We select three popular mitigation methods to represent all three types, namely Counterfactual Data Augmentation (CDA), Context-debias, and Sentdebias. Notice that these methods create debiased pretrained language models, as is illustrated in Figure 1. These models still need to be finetuned on a downstream task.\n\nCDA pretraining. The idea behind counterfactual data augmentation (Zmigrod et al., 2019;Lu et al., 2020) is to generate a counterfactual for each example in the training corpus by replacing attribute terms with their complimentary equivalent from the other demographic classes. For example, she will map to he in the case of binary gender. To mitigate intrinsic bias, this counterfactual augmentation has to be done as a pretraining step. Since CDA involves retraining the model, it is more resource-intensive compared to Sent-debias and Context-debias. We use the pretrained CDA models based on BERT and ALBERT from Webster et al. (2020) for our implementation. (2021) introduce a debiasing method that involves retraining the language model with a constraint to make the embeddings of stereotype terms 4 (such as doctor, nurse) orthogonal to embeddings of attribute terms 5 (such as gender pronouns like she, he and gender nouns like woman, man). Given the dynamic nature of contextualized word embeddings which causes a word to have different embeddings, they define fixed word embeddings for each attribute token by averaging the contextual embeddings of a word in all sentences it appears in. Training is done so that the embeddings of all stereotype terms are made orthogonal to all fixed attribute embeddings. They add a regularizer that constrains the debiased embedding to retain as much information by ensuring that they are as similar to the original em",
            "reference_string": "[256390197 | Tokpo et al. | 2023 | Citations: 8]"
        },
        {
            "title": "The African Woman is Rhythmic and Soulful: An Investigation of Implicit Biases in LLM Open-ended Text Generation",
            "venue": "",
            "year": 2024,
            "reference_count": 0,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.01270, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2309199058",
                    "name": "Serene Lim"
                },
                {
                    "authorId": "2323507494",
                    "name": "Mar'ia P'erez-Ortiz"
                }
            ],
            "abstract": "This paper investigates the subtle and often concealed biases present in Large Language Models (LLMs), focusing on implicit biases that may remain despite passing explicit bias tests. Implicit biases are significant because they influence the decisions made by these systems, potentially perpetuating stereotypes and discrimination, even when LLMs appear to function fairly. Traditionally, explicit bias tests or embedding-based methods are employed to detect bias, but these approaches can overlook more nuanced, implicit forms of bias. To address this, we introduce two novel psychological-inspired methodologies: the LLM Implicit Association Test (IAT) Bias and the LLM Decision Bias, designed to reveal and measure implicit biases through prompt-based and decision-making tasks. Additionally, open-ended generation tasks with thematic analysis of word generations and storytelling provide qualitative insights into the model's behavior. Our findings demonstrate that the LLM IAT Bias correlates with traditional methods and more effectively predicts downstream behaviors, as measured by the LLM Decision Bias, offering a more comprehensive framework for detecting subtle biases in AI systems. This research advances the field of AI ethics by proposing new methods to continually assess and mitigate biases in LLMs, highlighting the importance of qualitative and decision-focused evaluations to address challenges that previous approaches have not fully captured.",
            "corpus_id": 270869465,
            "sentences": [
                {
                    "corpus_id": "270869465",
                    "title": "The African Woman is Rhythmic and Soulful: An Investigation of Implicit Biases in LLM Open-ended Text Generation",
                    "text": "Additionally, the application of fairness constraints and regularization techniques within the training process alters the model's objective function to discourage biases, promoting the creation of more balanced and inclusive AI systems. \n\nWe therefore developed a scoring methodology used to evaluate bias mitigation techniques in Large Language Models (LLMs) to offer a comparative analysis that highlights both the strengths and limitations of each approach. Each technique was assessed across multiple dimensions -transparency, cultural sensitivity, and effectiveness. The scores for these dimensions were derived through a combination of literature review, empirical findings, and qualitative evaluations of how these techniques perform in practical settings. For example, adversarial training received a high score for its effectiveness in reducing certain explicit biases but was marked lower for transparency due to the complexity and opacity of its implementation. In contrast, model auditing scored higher for transparency but displayed limitations in direct bias mitigation, particularly in subtle or implicit bias contexts. The novelty of this scoring framework lies in its interdisciplinary approach, which integrates technical assessments typically found in computer science with insights from social sciences, such as digital anthropology and ethics. \n\nInstead of focusing solely on the computational effectiveness of each technique, the scoring system accounts for how these techniques perform when contextualized within broader societal and cultural frameworks. By including dimensions like cultural sensitivity and transparency, the scoring system acknowledges the importance of not just technical solutions but also their ethical and societal impact. This approach offers a more holistic evaluation, emphasizing that successful bias mitigation must go beyond optimizing model performance and incorporate a deeper understanding of the human contexts in which these technologies are applied. More information on the exact scores can be found in the appendix. Transparency, Direct Mitigation, and Cultural Sensitivity. This structured approach facilitates a systematic comparison that underscores distinct capabilities and limitations inherent to each method. \n\n\u2022 The Transparency Score is attributed based on the extent to which a technique facilitates understanding and visibility into the AI's decision-making processes. Techniques that allow stakeholders to readily observe and understand how decisions are made by the AI, including the provenance of data and the mechanics of algorithmic decisions, receive higher scores. This dimension is particularly influenced by the degree to which methods disclose information about training data, algorithmic design, and operational mechanisms.",
                    "score": 0.5439837016372195,
                    "section_title": "BACKGROUND AND LITERATURE REVIEW",
                    "char_start_offset": 11537,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 237
                        },
                        {
                            "start": 240,
                            "end": 461
                        },
                        {
                            "start": 462,
                            "end": 572
                        },
                        {
                            "start": 573,
                            "end": 764
                        },
                        {
                            "start": 765,
                            "end": 973
                        },
                        {
                            "start": 974,
                            "end": 1135
                        },
                        {
                            "start": 1136,
                            "end": 1365
                        },
                        {
                            "start": 1368,
                            "end": 1578
                        },
                        {
                            "start": 1579,
                            "end": 1769
                        },
                        {
                            "start": 1770,
                            "end": 2008
                        },
                        {
                            "start": 2009,
                            "end": 2075
                        },
                        {
                            "start": 2076,
                            "end": 2134
                        },
                        {
                            "start": 2135,
                            "end": 2275
                        },
                        {
                            "start": 2278,
                            "end": 2439
                        },
                        {
                            "start": 2440,
                            "end": 2642
                        },
                        {
                            "start": 2643,
                            "end": 2805
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.87353515625
                }
            ],
            "relevance_judgement": 0.87353515625,
            "relevance_judgment_input_expanded": "# Title: The African Woman is Rhythmic and Soulful: An Investigation of Implicit Biases in LLM Open-ended Text Generation\n# Venue: \n# Authors: Serene Lim, Mar'ia P'erez-Ortiz\n## Abstract\nThis paper investigates the subtle and often concealed biases present in Large Language Models (LLMs), focusing on implicit biases that may remain despite passing explicit bias tests. Implicit biases are significant because they influence the decisions made by these systems, potentially perpetuating stereotypes and discrimination, even when LLMs appear to function fairly. Traditionally, explicit bias tests or embedding-based methods are employed to detect bias, but these approaches can overlook more nuanced, implicit forms of bias. To address this, we introduce two novel psychological-inspired methodologies: the LLM Implicit Association Test (IAT) Bias and the LLM Decision Bias, designed to reveal and measure implicit biases through prompt-based and decision-making tasks. Additionally, open-ended generation tasks with thematic analysis of word generations and storytelling provide qualitative insights into the model's behavior. Our findings demonstrate that the LLM IAT Bias correlates with traditional methods and more effectively predicts downstream behaviors, as measured by the LLM Decision Bias, offering a more comprehensive framework for detecting subtle biases in AI systems. This research advances the field of AI ethics by proposing new methods to continually assess and mitigate biases in LLMs, highlighting the importance of qualitative and decision-focused evaluations to address challenges that previous approaches have not fully captured.\n## BACKGROUND AND LITERATURE REVIEW\nAdditionally, the application of fairness constraints and regularization techniques within the training process alters the model's objective function to discourage biases, promoting the creation of more balanced and inclusive AI systems. \n\nWe therefore developed a scoring methodology used to evaluate bias mitigation techniques in Large Language Models (LLMs) to offer a comparative analysis that highlights both the strengths and limitations of each approach. Each technique was assessed across multiple dimensions -transparency, cultural sensitivity, and effectiveness. The scores for these dimensions were derived through a combination of literature review, empirical findings, and qualitative evaluations of how these techniques perform in practical settings. For example, adversarial training received a high score for its effectiveness in reducing certain explicit biases but was marked lower for transparency due to the complexity and opacity of its implementation. In contrast, model auditing scored higher for transparency but displayed limitations in direct bias mitigation, particularly in subtle or implicit bias contexts. The novelty of this scoring framework lies in its interdisciplinary approach, which integrates technical assessments typically found in computer science with insights from social sciences, such as digital anthropology and ethics. \n\nInstead of focusing solely on the computational effectiveness of each technique, the scoring system accounts for how these techniques perform when contextualized within broader societal and cultural frameworks. By including dimensions like cultural sensitivity and transparency, the scoring system acknowledges the importance of not just technical solutions but also their ethical and societal impact. This approach offers a more holistic evaluation, emphasizing that successful bias mitigation must go beyond optimizing model performance and incorporate a deeper understanding of the human contexts in which these technologies are applied. More information on the exact scores can be found in the appendix. Transparency, Direct Mitigation, and Cultural Sensitivity. This structured approach facilitates a systematic comparison that underscores distinct capabilities and limitations inherent to each method. \n\n\u2022 The Transparency Score is attributed based on the extent to which a technique facilitates understanding and visibility into the AI's decision-making processes. Techniques that allow stakeholders to readily observe and understand how decisions are made by the AI, including the provenance of data and the mechanics of algorithmic decisions, receive higher scores. This dimension is particularly influenced by the degree to which methods disclose information about training data, algorithmic design, and operational mechanisms.",
            "reference_string": "[270869465 | Lim et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Locating and Mitigating Gender Bias in Large Language Models",
            "venue": "International Conference on Intelligent Computing",
            "year": 2024,
            "reference_count": 38,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.14409, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2292681106",
                    "name": "Yuchen Cai"
                },
                {
                    "authorId": "2237226610",
                    "name": "Ding Cao"
                },
                {
                    "authorId": "2237261799",
                    "name": "Rongxi Guo"
                },
                {
                    "authorId": "2293239741",
                    "name": "Yaqin Wen"
                },
                {
                    "authorId": "2237403462",
                    "name": "Guiquan Liu"
                },
                {
                    "authorId": "2292390890",
                    "name": "Enhong Chen"
                }
            ],
            "abstract": "Large language models(LLM) are pre-trained on extensive corpora to learn facts and human cognition which contain human preferences. However, this process can inadvertently lead to these models acquiring biases and stereotypes prevalent in society. Prior research has typically tackled the issue of bias through a one-dimensional perspective, concentrating either on locating or mitigating it. This limited perspective has created obstacles in facilitating research on bias to synergistically complement and progressively build upon one another. In this study, we integrate the processes of locating and mitigating bias within a unified framework. Initially, we use causal mediation analysis to trace the causal effects of different components' activation within a large language model. Building on this, we propose the LSDM (Least Square Debias Method), a knowledge-editing based method for mitigating gender bias in occupational pronouns, and compare it against two baselines on three gender bias datasets and seven knowledge competency test datasets. The experimental results indicate that the primary contributors to gender bias are the bottom MLP modules acting on the last token of occupational pronouns and the top attention module acting on the final word in the sentence. Furthermore, LSDM mitigates gender bias in the model more effectively than the other baselines, while fully preserving the model's capabilities in all other aspects.",
            "corpus_id": 268553687,
            "sentences": [
                {
                    "corpus_id": "268553687",
                    "title": "Locating and Mitigating Gender Bias in Large Language Models",
                    "text": "To further validate the influence of the bottom MLP modules on the generation of gender bias, we have adapted a knowledge editing technique [19] to mitigation of gender bias within the model, called LSDM (Least Square Debias Method).LSDM modifies parameters by solving a matrix equation with constraint terms, enabling us to minimize interference with other aspects of the model while specifically mitigating gender bias associated with certain occupation words.LSDM overcomes the catastrophic forgetting problem that exists in all other debiasing methods and stands out by avoiding additional reinforcement learning or human annotations, basing its approach on causal trace conclusions rather than just black-box fine-tuning.Our main contributions are as follows:\n\n\u2022 We trace the causal effects of different components' activation within a large language model using causal mediation analysis to measure the magnitude of the impact of different components of the model on gender bias and reveal the flow process of biased information.\u2022 We propose Least Square Debias Method to modify parameters to mitigate gender bias in models.This is a more interpretable debiasing algorithm.\n\nResults confirm that LSDM serves as an efficient debias method that overcomes the catastrophic forgetting problem that exists in all other debiasing methods.\u2022 To our knowledge, this study represents the first endeavour to incorporate both the location and mitigation of gender bias into a unified framework.\u2022 We are the first to transfer knowledge editing methods to the domain of debias and validate their feasibility, providing a viable solution for eliminating various biases present in large language models.\n\n2 Interventions on Activations for Tracing Bias Information Flow",
                    "score": 0.5319553103729349,
                    "section_title": "Introduction",
                    "char_start_offset": 6353,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 233
                        },
                        {
                            "start": 233,
                            "end": 462
                        },
                        {
                            "start": 462,
                            "end": 726
                        },
                        {
                            "start": 726,
                            "end": 764
                        },
                        {
                            "start": 766,
                            "end": 1035
                        },
                        {
                            "start": 1035,
                            "end": 1130
                        },
                        {
                            "start": 1130,
                            "end": 1179
                        },
                        {
                            "start": 1181,
                            "end": 1338
                        },
                        {
                            "start": 1338,
                            "end": 1488
                        },
                        {
                            "start": 1488,
                            "end": 1693
                        },
                        {
                            "start": 1695,
                            "end": 1759
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.87158203125
                }
            ],
            "relevance_judgement": 0.87158203125,
            "relevance_judgment_input_expanded": "# Title: Locating and Mitigating Gender Bias in Large Language Models\n# Venue: International Conference on Intelligent Computing\n# Authors: Yuchen Cai, Ding Cao, Rongxi Guo, Yaqin Wen, Guiquan Liu, Enhong Chen\n## Abstract\nLarge language models(LLM) are pre-trained on extensive corpora to learn facts and human cognition which contain human preferences. However, this process can inadvertently lead to these models acquiring biases and stereotypes prevalent in society. Prior research has typically tackled the issue of bias through a one-dimensional perspective, concentrating either on locating or mitigating it. This limited perspective has created obstacles in facilitating research on bias to synergistically complement and progressively build upon one another. In this study, we integrate the processes of locating and mitigating bias within a unified framework. Initially, we use causal mediation analysis to trace the causal effects of different components' activation within a large language model. Building on this, we propose the LSDM (Least Square Debias Method), a knowledge-editing based method for mitigating gender bias in occupational pronouns, and compare it against two baselines on three gender bias datasets and seven knowledge competency test datasets. The experimental results indicate that the primary contributors to gender bias are the bottom MLP modules acting on the last token of occupational pronouns and the top attention module acting on the final word in the sentence. Furthermore, LSDM mitigates gender bias in the model more effectively than the other baselines, while fully preserving the model's capabilities in all other aspects.\n## Introduction\nTo further validate the influence of the bottom MLP modules on the generation of gender bias, we have adapted a knowledge editing technique [19] to mitigation of gender bias within the model, called LSDM (Least Square Debias Method).LSDM modifies parameters by solving a matrix equation with constraint terms, enabling us to minimize interference with other aspects of the model while specifically mitigating gender bias associated with certain occupation words.LSDM overcomes the catastrophic forgetting problem that exists in all other debiasing methods and stands out by avoiding additional reinforcement learning or human annotations, basing its approach on causal trace conclusions rather than just black-box fine-tuning.Our main contributions are as follows:\n\n\u2022 We trace the causal effects of different components' activation within a large language model using causal mediation analysis to measure the magnitude of the impact of different components of the model on gender bias and reveal the flow process of biased information.\u2022 We propose Least Square Debias Method to modify parameters to mitigate gender bias in models.This is a more interpretable debiasing algorithm.\n\nResults confirm that LSDM serves as an efficient debias method that overcomes the catastrophic forgetting problem that exists in all other debiasing methods.\u2022 To our knowledge, this study represents the first endeavour to incorporate both the location and mitigation of gender bias into a unified framework.\u2022 We are the first to transfer knowledge editing methods to the domain of debias and validate their feasibility, providing a viable solution for eliminating various biases present in large language models.\n\n2 Interventions on Activations for Tracing Bias Information Flow",
            "reference_string": "[268553687 | Cai et al. | 2024 | Citations: 5]"
        },
        {
            "title": "Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2023,
            "reference_count": 31,
            "citation_count": 37,
            "influential_citation_count": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2306.04597",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.04597, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2221493995",
                    "name": "Himanshu Thakur"
                },
                {
                    "authorId": "1819271266",
                    "name": "Atishay Jain"
                },
                {
                    "authorId": "2127734657",
                    "name": "Praneetha Vaddamanu"
                },
                {
                    "authorId": "28130078",
                    "name": "Paul Pu Liang"
                },
                {
                    "authorId": "49933077",
                    "name": "Louis-philippe Morency"
                }
            ],
            "abstract": "Societal biases present in pre-trained large language models are a critical issue as these models have been shown to propagate biases in countless downstream applications, rendering them unfair towards specific groups of people. Since large-scale retraining of these models from scratch is both time and compute-expensive, a variety of approaches have been previously proposed that de-bias a pre-trained model. While the majority of current state-of-the-art debiasing methods focus on changes to the training regime, in this paper, we propose data intervention strategies as a powerful yet simple technique to reduce gender bias in pre-trained models. Specifically, we empirically show that by fine-tuning a pre-trained model on only 10 debiased (intervened) training examples, the tendency to favor any gender is significantly reduced. Since our proposed method only needs a few training examples, we argue that our few-shot de-biasing approach is highly feasible and practical. Through extensive experimentation, we show that our de-biasing technique performs better than competitive state-of-the-art baselines with minimal loss in language modeling ability.",
            "corpus_id": 259095603,
            "sentences": [
                {
                    "corpus_id": "259095603",
                    "title": "Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions",
                    "text": "Societal biases present in pre-trained large language models are a critical issue as these models have been shown to propagate biases in countless downstream applications, rendering them unfair towards specific groups of people. Since large-scale retraining of these models from scratch is both time and compute-expensive, a variety of approaches have been previously proposed that de-bias a pre-trained model. While the majority of current state-of-the-art debiasing methods focus on changes to the training regime, in this paper, we propose data intervention strategies as a powerful yet simple technique to reduce gender bias in pre-trained models. Specifically, we empirically show that by fine-tuning a pre-trained model on only 10 debiased (intervened) training examples, the tendency to favor any gender is significantly reduced. Since our proposed method only needs a few training examples, we argue that our few-shot de-biasing approach is highly feasible and practical. Through extensive experimentation, we show that our de-biasing technique performs better than competitive state-of-the-art baselines with minimal loss in language modeling ability.",
                    "score": 0.5101122198941723,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.87109375
                }
            ],
            "relevance_judgement": 0.87109375,
            "relevance_judgment_input_expanded": "# Title: Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Himanshu Thakur, Atishay Jain, Praneetha Vaddamanu, Paul Pu Liang, Louis-philippe Morency\n## Abstract\nSocietal biases present in pre-trained large language models are a critical issue as these models have been shown to propagate biases in countless downstream applications, rendering them unfair towards specific groups of people. Since large-scale retraining of these models from scratch is both time and compute-expensive, a variety of approaches have been previously proposed that de-bias a pre-trained model. While the majority of current state-of-the-art debiasing methods focus on changes to the training regime, in this paper, we propose data intervention strategies as a powerful yet simple technique to reduce gender bias in pre-trained models. Specifically, we empirically show that by fine-tuning a pre-trained model on only 10 debiased (intervened) training examples, the tendency to favor any gender is significantly reduced. Since our proposed method only needs a few training examples, we argue that our few-shot de-biasing approach is highly feasible and practical. Through extensive experimentation, we show that our de-biasing technique performs better than competitive state-of-the-art baselines with minimal loss in language modeling ability.\n",
            "reference_string": "[259095603 | Thakur et al. | 2023 | Citations: 37]"
        },
        {
            "title": "A review of causality-based fairness machine learning",
            "venue": "Intelligence &amp; Robotics",
            "year": 2022,
            "reference_count": 121,
            "citation_count": 11,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://intellrobot.com/article/download/5055",
                "status": "GOLD",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.20517/ir.2022.17?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.20517/ir.2022.17, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2152082208",
                    "name": "Cong Su"
                },
                {
                    "authorId": "46546894",
                    "name": "Guoxian Yu"
                },
                {
                    "authorId": "66063792",
                    "name": "J. Wang"
                },
                {
                    "authorId": "3047447",
                    "name": "Zhongmin Yan"
                },
                {
                    "authorId": "101457473",
                    "name": "Li-zhen Cui"
                }
            ],
            "abstract": "With the wide application of machine learning driven automated decisions (e.g., education, loan approval, and hiring) in daily life, it is critical to address the problem of discriminatory behavior toward certain individuals or groups. Early studies focused on defining the correlation/association-based notions, such as statistical parity, equalized odds, etc. However, recent studies reflect that it is necessary to use causality to address the problem of fairness. This review provides an exhaustive overview of notions and methods for detecting and eliminating algorithmic discrimination from a causality perspective. The review begins by introducing the common causality-based definitions and measures for fairness. We then review causality-based fairness-enhancing methods from the perspective of pre-processing, in-processing and post-processing mechanisms, and conduct a comprehensive analysis of the advantages, disadvantages, and applicability of these mechanisms. In addition, this review also examines other domains where researchers have observed unfair outcomes and the ways they have tried to address them. There are still many challenges that hinder the practical application of causality-based fairness notions, specifically the difficulty of acquiring causal graphs and identifiability of causal effects. One of the main purposes of this review is to spark more researchers to tackle these challenges in the near future.",
            "corpus_id": 252129484,
            "sentences": [
                {
                    "corpus_id": "252129484",
                    "title": "A review of causality-based fairness machine learning",
                    "text": "As such, the word embedding generated by this method can strike a balance between gender debiasing and semantic information preserving. Yang and Feng [79] presented a causality-based post-processing approach for eliminating the gender bias in word embeddings. Specifically, their method was based on statistical correlation and half-sibling regression, which leverages the statistical dependency between gender-biased word vectors and gender-definition word vectors to learn the counterfactual gender information of an individual through causal inference. The learned spurious gender information is then subtracted from the gender-biased word vectors to remove the gender bias. Lu et al. [80] proposed a method called CDA to eliminate gender bias through counterfactual data augmentation. The main idea of CDA is to augment the corpus by exchanging gender word pairs in the corpus and constructing matching gender word pairs with causal interventions. As such, CDA breaks associations between gendered and gender-neutral words and alleviates the problem that gender bias increases as loss decreases when training with gradient descent. \n\nThere exists a certain degree of bias and fairness issues in word embedding, machine translation, sentiment analysis, language models, and dialog generation in NLP. At present, most studies only focus on a single bias (such as gender bias), and there is a lack of research results on other biases or eliminating multiple biases at the same time. Therefore, how should we analyze and evaluate the mechanism and impact of multi-bias in word embedding and machine learning algorithms? Establishing effective techniques for eliminating various biases in word embedding and machine learning algorithms requires further research which needs to be carried out for fair NLP.",
                    "score": 0.5149632657768765,
                    "section_title": "Fair natural language processing",
                    "char_start_offset": 51525,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 135
                        },
                        {
                            "start": 136,
                            "end": 259
                        },
                        {
                            "start": 260,
                            "end": 555
                        },
                        {
                            "start": 556,
                            "end": 677
                        },
                        {
                            "start": 678,
                            "end": 788
                        },
                        {
                            "start": 789,
                            "end": 951
                        },
                        {
                            "start": 952,
                            "end": 1135
                        },
                        {
                            "start": 1138,
                            "end": 1302
                        },
                        {
                            "start": 1303,
                            "end": 1483
                        },
                        {
                            "start": 1484,
                            "end": 1619
                        },
                        {
                            "start": 1620,
                            "end": 1804
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 150,
                            "end": 154,
                            "matchedPaperCorpusId": "208267842"
                        },
                        {
                            "start": 688,
                            "end": 692,
                            "matchedPaperCorpusId": "51888520"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.869140625
                }
            ],
            "relevance_judgement": 0.869140625,
            "relevance_judgment_input_expanded": "# Title: A review of causality-based fairness machine learning\n# Venue: Intelligence &amp; Robotics\n# Authors: Cong Su, Guoxian Yu, J. Wang, Zhongmin Yan, Li-zhen Cui\n## Abstract\nWith the wide application of machine learning driven automated decisions (e.g., education, loan approval, and hiring) in daily life, it is critical to address the problem of discriminatory behavior toward certain individuals or groups. Early studies focused on defining the correlation/association-based notions, such as statistical parity, equalized odds, etc. However, recent studies reflect that it is necessary to use causality to address the problem of fairness. This review provides an exhaustive overview of notions and methods for detecting and eliminating algorithmic discrimination from a causality perspective. The review begins by introducing the common causality-based definitions and measures for fairness. We then review causality-based fairness-enhancing methods from the perspective of pre-processing, in-processing and post-processing mechanisms, and conduct a comprehensive analysis of the advantages, disadvantages, and applicability of these mechanisms. In addition, this review also examines other domains where researchers have observed unfair outcomes and the ways they have tried to address them. There are still many challenges that hinder the practical application of causality-based fairness notions, specifically the difficulty of acquiring causal graphs and identifiability of causal effects. One of the main purposes of this review is to spark more researchers to tackle these challenges in the near future.\n## Fair natural language processing\nAs such, the word embedding generated by this method can strike a balance between gender debiasing and semantic information preserving. Yang and Feng [79] presented a causality-based post-processing approach for eliminating the gender bias in word embeddings. Specifically, their method was based on statistical correlation and half-sibling regression, which leverages the statistical dependency between gender-biased word vectors and gender-definition word vectors to learn the counterfactual gender information of an individual through causal inference. The learned spurious gender information is then subtracted from the gender-biased word vectors to remove the gender bias. Lu et al. [80] proposed a method called CDA to eliminate gender bias through counterfactual data augmentation. The main idea of CDA is to augment the corpus by exchanging gender word pairs in the corpus and constructing matching gender word pairs with causal interventions. As such, CDA breaks associations between gendered and gender-neutral words and alleviates the problem that gender bias increases as loss decreases when training with gradient descent. \n\nThere exists a certain degree of bias and fairness issues in word embedding, machine translation, sentiment analysis, language models, and dialog generation in NLP. At present, most studies only focus on a single bias (such as gender bias), and there is a lack of research results on other biases or eliminating multiple biases at the same time. Therefore, how should we analyze and evaluate the mechanism and impact of multi-bias in word embedding and machine learning algorithms? Establishing effective techniques for eliminating various biases in word embedding and machine learning algorithms requires further research which needs to be carried out for fair NLP.",
            "reference_string": "[252129484 | Su et al. | 2022 | Citations: 11]"
        },
        {
            "title": "Evaluating and Mitigating Gender Bias in Generative Large Language Models",
            "venue": "International Journal of Computers Communications & Control",
            "year": 2024,
            "reference_count": 0,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.15837/ijccc.2024.6.6853",
                "status": "GOLD",
                "license": "CCBYNC",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.15837/ijccc.2024.6.6853?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.15837/ijccc.2024.6.6853, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2329139543",
                    "name": "Hanqing Zhou"
                },
                {
                    "authorId": "2326494123",
                    "name": "Diana Inkpen"
                },
                {
                    "authorId": "2497479",
                    "name": "B. Kantarci"
                }
            ],
            "abstract": "The examination of gender bias, alongside other demographic biases like race, nationality, and religion, within generative large language models (LLMs), is increasingly capturing the attention of both the scientific community and industry stakeholders. These biases often affect generative LLMs, influencing popular products and potentially compromising user experiences. A growing body of research is dedicated to enhancing gender representations in natural language processing (NLP) across a spectrum of generative LLMs. This paper explores the current research focused on identifying and evaluating gender bias in generative LLMs. A comprehensive investigation is conducted to evaluate and mitigate gender bias across five distinct generative LLMs. The mitigation strategies implemented yield significant improvements in gender bias scores, with performance enhancements of up to 46% compared to zero-shot text generation approaches. Additionally, we explore how different levels of LLM precision and quantization impact gender bias, providing insights into how technical factors influence bias mitigation strategies. By tackling these challenges and suggesting areas for future research, we aim to contribute to the ongoing discussion about gender bias in language technologies, promoting more equitable and inclusive NLP systems.",
            "corpus_id": 273782730,
            "sentences": [
                {
                    "corpus_id": "273782730",
                    "title": "Evaluating and Mitigating Gender Bias in Generative Large Language Models",
                    "text": "The examination of gender bias, alongside other demographic biases like race, nationality, and religion, within generative large language models (LLMs), is increasingly capturing the attention of both the scientific community and industry stakeholders. These biases often affect generative LLMs, influencing popular products and potentially compromising user experiences. A growing body of research is dedicated to enhancing gender representations in natural language processing (NLP) across a spectrum of generative LLMs. This paper explores the current research focused on identifying and evaluating gender bias in generative LLMs. A comprehensive investigation is conducted to evaluate and mitigate gender bias across five distinct generative LLMs. The mitigation strategies implemented yield significant improvements in gender bias scores, with performance enhancements of up to 46% compared to zero-shot text generation approaches. Additionally, we explore how different levels of LLM precision and quantization impact gender bias, providing insights into how technical factors influence bias mitigation strategies. By tackling these challenges and suggesting areas for future research, we aim to contribute to the ongoing discussion about gender bias in language technologies, promoting more equitable and inclusive NLP systems.",
                    "score": 0.5341872947758152,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.86865234375
                }
            ],
            "relevance_judgement": 0.86865234375,
            "relevance_judgment_input_expanded": "# Title: Evaluating and Mitigating Gender Bias in Generative Large Language Models\n# Venue: International Journal of Computers Communications & Control\n# Authors: Hanqing Zhou, Diana Inkpen, B. Kantarci\n## Abstract\nThe examination of gender bias, alongside other demographic biases like race, nationality, and religion, within generative large language models (LLMs), is increasingly capturing the attention of both the scientific community and industry stakeholders. These biases often affect generative LLMs, influencing popular products and potentially compromising user experiences. A growing body of research is dedicated to enhancing gender representations in natural language processing (NLP) across a spectrum of generative LLMs. This paper explores the current research focused on identifying and evaluating gender bias in generative LLMs. A comprehensive investigation is conducted to evaluate and mitigate gender bias across five distinct generative LLMs. The mitigation strategies implemented yield significant improvements in gender bias scores, with performance enhancements of up to 46% compared to zero-shot text generation approaches. Additionally, we explore how different levels of LLM precision and quantization impact gender bias, providing insights into how technical factors influence bias mitigation strategies. By tackling these challenges and suggesting areas for future research, we aim to contribute to the ongoing discussion about gender bias in language technologies, promoting more equitable and inclusive NLP systems.\n",
            "reference_string": "[273782730 | Zhou et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Causal-Debias: Unifying Debiasing in Pretrained Language Models and Fine-tuning via Causal Invariant Learning",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2023,
            "reference_count": 47,
            "citation_count": 41,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2023.acl-long.232.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2023.acl-long.232, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "144315735",
                    "name": "Fan Zhou"
                },
                {
                    "authorId": "2221367541",
                    "name": "Yuzhou Mao"
                },
                {
                    "authorId": "38057162",
                    "name": "Liu Yu"
                },
                {
                    "authorId": "2143686211",
                    "name": "Yi Yang"
                },
                {
                    "authorId": "46456474",
                    "name": "Ting Zhong"
                }
            ],
            "abstract": "Demographic biases and social stereotypes are common in pretrained language models (PLMs), and a burgeoning body of literature focuses on removing the unwanted stereotypical associations from PLMs. However, when fine-tuning these bias-mitigated PLMs in downstream natural language processing (NLP) applications, such as sentiment classification, the unwanted stereotypical associations resurface or even get amplified. Since pretrain&fine-tune is a major paradigm in NLP applications, separating the debiasing procedure of PLMs from fine-tuning would eventually harm the actual downstream utility. In this paper, we propose a unified debiasing framework Causal-Debias to remove unwanted stereotypical associations in PLMs during fine-tuning. Specifically, CausalDebias mitigates bias from a causal invariant perspective by leveraging the specific downstream task to identify bias-relevant and labelrelevant factors. We propose that bias-relevant factors are non-causal as they should have little impact on downstream tasks, while labelrelevant factors are causal. We perform interventions on non-causal factors in different demographic groups and design an invariant risk minimization loss to mitigate bias while maintaining task performance. Experimental results on three downstream tasks show that our proposed method can remarkably reduce unwanted stereotypical associations after PLMs are finetuned, while simultaneously minimizing the impact on PLMs and downstream applications.",
            "corpus_id": 259370743,
            "sentences": [
                {
                    "corpus_id": "259370743",
                    "title": "Causal-Debias: Unifying Debiasing in Pretrained Language Models and Fine-tuning via Causal Invariant Learning",
                    "text": "Racial debiasing refers to examining the association difference between European-American/African American names/terms and the stereotype words (pleasant vs. unpleasant) (Caliskan et al., 2017). Unlike gender debiasing, few prior studies investigated the racial debiasing problem, due to the difficulty of mitigating racial bias (Meade et al., 2022). A critical challenge is the potential word ambiguity (e.g., white, black) in various contexts (Guo et al., 2022). Table 4 reports the performance of Causal-Debias and Auto-Debias -the state-of-the-art racial debiasing model. Causal-Debias substantially decreases the racial biases on PLMs after fine-tuning, while obtaining comparable downstream performance. Auto-Debias, in contrast, still suffers from bias recurrence issue. Compared to Auto-Debias, Causal-Debias is more effective as it exploits downstream datasets for debiasing, which allows us to alleviate the influence of ambiguous words. \n\nBesides, the causal invariant learning in Causal-Debias encourages the model to learn consistent representations and clear meanings of ambiguous words so as to avoid bias-related associations.",
                    "score": 0.5228945681080214,
                    "section_title": "Results on Mitigating Racial Bias",
                    "char_start_offset": 25744,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 194
                        },
                        {
                            "start": 195,
                            "end": 350
                        },
                        {
                            "start": 351,
                            "end": 464
                        },
                        {
                            "start": 465,
                            "end": 575
                        },
                        {
                            "start": 576,
                            "end": 709
                        },
                        {
                            "start": 710,
                            "end": 777
                        },
                        {
                            "start": 778,
                            "end": 947
                        },
                        {
                            "start": 950,
                            "end": 1142
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 170,
                            "end": 193,
                            "matchedPaperCorpusId": "23163324"
                        },
                        {
                            "start": 329,
                            "end": 349,
                            "matchedPaperCorpusId": "239015827"
                        },
                        {
                            "start": 445,
                            "end": 463,
                            "matchedPaperCorpusId": "248780440"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.86865234375
                }
            ],
            "relevance_judgement": 0.86865234375,
            "relevance_judgment_input_expanded": "# Title: Causal-Debias: Unifying Debiasing in Pretrained Language Models and Fine-tuning via Causal Invariant Learning\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Fan Zhou, Yuzhou Mao, Liu Yu, Yi Yang, Ting Zhong\n## Abstract\nDemographic biases and social stereotypes are common in pretrained language models (PLMs), and a burgeoning body of literature focuses on removing the unwanted stereotypical associations from PLMs. However, when fine-tuning these bias-mitigated PLMs in downstream natural language processing (NLP) applications, such as sentiment classification, the unwanted stereotypical associations resurface or even get amplified. Since pretrain&fine-tune is a major paradigm in NLP applications, separating the debiasing procedure of PLMs from fine-tuning would eventually harm the actual downstream utility. In this paper, we propose a unified debiasing framework Causal-Debias to remove unwanted stereotypical associations in PLMs during fine-tuning. Specifically, CausalDebias mitigates bias from a causal invariant perspective by leveraging the specific downstream task to identify bias-relevant and labelrelevant factors. We propose that bias-relevant factors are non-causal as they should have little impact on downstream tasks, while labelrelevant factors are causal. We perform interventions on non-causal factors in different demographic groups and design an invariant risk minimization loss to mitigate bias while maintaining task performance. Experimental results on three downstream tasks show that our proposed method can remarkably reduce unwanted stereotypical associations after PLMs are finetuned, while simultaneously minimizing the impact on PLMs and downstream applications.\n## Results on Mitigating Racial Bias\nRacial debiasing refers to examining the association difference between European-American/African American names/terms and the stereotype words (pleasant vs. unpleasant) (Caliskan et al., 2017). Unlike gender debiasing, few prior studies investigated the racial debiasing problem, due to the difficulty of mitigating racial bias (Meade et al., 2022). A critical challenge is the potential word ambiguity (e.g., white, black) in various contexts (Guo et al., 2022). Table 4 reports the performance of Causal-Debias and Auto-Debias -the state-of-the-art racial debiasing model. Causal-Debias substantially decreases the racial biases on PLMs after fine-tuning, while obtaining comparable downstream performance. Auto-Debias, in contrast, still suffers from bias recurrence issue. Compared to Auto-Debias, Causal-Debias is more effective as it exploits downstream datasets for debiasing, which allows us to alleviate the influence of ambiguous words. \n\nBesides, the causal invariant learning in Causal-Debias encourages the model to learn consistent representations and clear meanings of ambiguous words so as to avoid bias-related associations.",
            "reference_string": "[259370743 | Zhou et al. | 2023 | Citations: 41]"
        },
        {
            "title": "FairSISA: Ensemble Post-Processing to Improve Fairness of Unlearning in LLMs",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 45,
            "citation_count": 7,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.07420, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1686542",
                    "name": "S. Kadhe"
                },
                {
                    "authorId": "2288315382",
                    "name": "Anisa Halimi"
                },
                {
                    "authorId": "22261698",
                    "name": "Ambrish Rawat"
                },
                {
                    "authorId": "2478882",
                    "name": "Nathalie Baracaldo"
                }
            ],
            "abstract": "Training large language models (LLMs) is a costly endeavour in terms of time and computational resources. The large amount of training data used during the unsupervised pre-training phase makes it difficult to verify all data and, unfortunately, undesirable data may be ingested during training. Re-training from scratch is impractical and has led to the creation of the 'unlearning' discipline where models are modified to\"unlearn\"undesirable information without retraining. However, any modification can alter the behaviour of LLMs, especially on key dimensions such as fairness. This is the first work that examines this interplay between unlearning and fairness for LLMs. In particular, we focus on a popular unlearning framework known as SISA [Bourtoule et al., 2021], which creates an ensemble of models trained on disjoint shards. We evaluate the performance-fairness trade-off for SISA, and empirically demsontrate that SISA can indeed reduce fairness in LLMs. To remedy this, we propose post-processing bias mitigation techniques for ensemble models produced by SISA. We adapt the post-processing fairness improvement technique from [Hardt et al., 2016] to design three methods that can handle model ensembles, and prove that one of the methods is an optimal fair predictor for ensemble of models. Through experimental results, we demonstrate the efficacy of our post-processing framework called 'FairSISA'.",
            "corpus_id": 266174259,
            "sentences": [],
            "relevance_judgement": 0.8671875,
            "relevance_judgment_input_expanded": "# Title: FairSISA: Ensemble Post-Processing to Improve Fairness of Unlearning in LLMs\n# Venue: arXiv.org\n# Authors: S. Kadhe, Anisa Halimi, Ambrish Rawat, Nathalie Baracaldo\n## Abstract\nTraining large language models (LLMs) is a costly endeavour in terms of time and computational resources. The large amount of training data used during the unsupervised pre-training phase makes it difficult to verify all data and, unfortunately, undesirable data may be ingested during training. Re-training from scratch is impractical and has led to the creation of the 'unlearning' discipline where models are modified to\"unlearn\"undesirable information without retraining. However, any modification can alter the behaviour of LLMs, especially on key dimensions such as fairness. This is the first work that examines this interplay between unlearning and fairness for LLMs. In particular, we focus on a popular unlearning framework known as SISA [Bourtoule et al., 2021], which creates an ensemble of models trained on disjoint shards. We evaluate the performance-fairness trade-off for SISA, and empirically demsontrate that SISA can indeed reduce fairness in LLMs. To remedy this, we propose post-processing bias mitigation techniques for ensemble models produced by SISA. We adapt the post-processing fairness improvement technique from [Hardt et al., 2016] to design three methods that can handle model ensembles, and prove that one of the methods is an optimal fair predictor for ensemble of models. Through experimental results, we demonstrate the efficacy of our post-processing framework called 'FairSISA'.\n",
            "reference_string": "[266174259 | Kadhe et al. | 2023 | Citations: 7]"
        },
        {
            "title": "NLPGuard: A Framework for Mitigating the Use of Protected Attributes by NLP Classifiers",
            "venue": "Proc. ACM Hum. Comput. Interact.",
            "year": 2024,
            "reference_count": 86,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3686924",
                "status": "HYBRID",
                "license": "other-oa",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.01697, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2149942497",
                    "name": "Salvatore Greco"
                },
                {
                    "authorId": "2310191590",
                    "name": "Ke Zhou"
                },
                {
                    "authorId": "47809306",
                    "name": "L. Capra"
                },
                {
                    "authorId": "2203091524",
                    "name": "Tania Cerquitelli"
                },
                {
                    "authorId": "144041798",
                    "name": "D. Quercia"
                }
            ],
            "abstract": "AI regulations are expected to prohibit machine learning models from using sensitive attributes during training. However, the latest Natural Language Processing (NLP) classifiers, which rely on deep learning, operate as black-box systems, complicating the detection and remediation of such misuse. Traditional bias mitigation methods in NLP aim for comparable performance across different groups based on attributes like gender or race but fail to address the underlying issue of reliance on protected attributes. To partly fix that, we introduce NLPGuard, a framework for mitigating the reliance on protected attributes in NLP classifiers. NLPGuard takes an unlabeled dataset, an existing NLP classifier, and its training data as input, producing a modified training dataset that significantly reduces dependence on protected attributes without compromising accuracy. NLPGuard is applied to three classification tasks: identifying toxic language, sentiment analysis, and occupation classification. Our evaluation shows that current NLP classifiers heavily depend on protected attributes, with up to 23% of the most predictive words associated with these attributes. However, NLPGuard effectively reduces this reliance by up to 79%, while slightly improving accuracy.",
            "corpus_id": 270878797,
            "sentences": [
                {
                    "corpus_id": "270878797",
                    "title": "NLPGuard: A Framework for Mitigating the Use of Protected Attributes by NLP Classifiers",
                    "text": "Bias in NLP decision-making has manifested itself in several ways, including dialogue generation [19], text classification [20], and machine translation [66].It usually arises from training data [22,67].For instance, pre-trained models and word embedding can inherit biases and stereotypes present in the large training corpora [7,9,25,58].When quantifying bias, existing works generally highlight disparities between demographic groups, with differences in performance or selection bias on protected attributes such as race, gender, religion, and sexual orientation [22,29,37,67].\n\nTo address biases in NLP, techniques can be developed that act at the three main stages of the NLP pipeline [38,75]: pre-processing (modifying training data), in-processing (imposing fairness constraints during model training), and post-processing (adjusting classifier predictions based on fairness metrics).Most existing works focus on the first two stages, exploiting data augmentation and modified model training techniques [4,6,20,52,58,64,77]. Furthermore, most of those studies focus on one protected category at a time.For example, Badjatiya et al. [6] proposed the identification of protected attributes, such as gender, by creating a manual list of words, measuring the skewed occurrence of words across classes or predicted class probability distribution of words.Park et al. [52] introduced gender swapping to equalize the number of male and female entities in the training data.Dixon et al. [20] proposed dataset augmentation strategies that generate new sentences using templates or replace protected attributes with generic tags, such as part-of-speech or named-entity tags.Zhang et al. [77] proposed mitigating biases in the training data by assuming a non-discrimination distribution and then reconstructing the distribution using instance weighting.Ravfogel et al. [58] proposed removing information from neural representations concerning gender or race for debiasing word embedding for NLP classification.",
                    "score": 0.6372560143640364,
                    "section_title": "Bias mitigation for NLP",
                    "char_start_offset": 6613,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 158
                        },
                        {
                            "start": 158,
                            "end": 203
                        },
                        {
                            "start": 203,
                            "end": 340
                        },
                        {
                            "start": 340,
                            "end": 581
                        },
                        {
                            "start": 583,
                            "end": 892
                        },
                        {
                            "start": 892,
                            "end": 1110
                        },
                        {
                            "start": 1110,
                            "end": 1358
                        },
                        {
                            "start": 1358,
                            "end": 1474
                        },
                        {
                            "start": 1474,
                            "end": 1672
                        },
                        {
                            "start": 1672,
                            "end": 1850
                        },
                        {
                            "start": 1850,
                            "end": 2007
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 123,
                            "end": 127,
                            "matchedPaperCorpusId": "54997157"
                        },
                        {
                            "start": 153,
                            "end": 157,
                            "matchedPaperCorpusId": "173991101"
                        },
                        {
                            "start": 331,
                            "end": 333,
                            "matchedPaperCorpusId": "23163324"
                        },
                        {
                            "start": 336,
                            "end": 339,
                            "matchedPaperCorpusId": "215786522"
                        },
                        {
                            "start": 571,
                            "end": 574,
                            "matchedPaperCorpusId": "237940142"
                        },
                        {
                            "start": 574,
                            "end": 577,
                            "matchedPaperCorpusId": "247222214"
                        },
                        {
                            "start": 691,
                            "end": 695,
                            "matchedPaperCorpusId": "232092280"
                        },
                        {
                            "start": 1014,
                            "end": 1016,
                            "matchedPaperCorpusId": "86446136"
                        },
                        {
                            "start": 1016,
                            "end": 1019,
                            "matchedPaperCorpusId": "54997157"
                        },
                        {
                            "start": 1019,
                            "end": 1022,
                            "matchedPaperCorpusId": "52070035"
                        },
                        {
                            "start": 1022,
                            "end": 1025,
                            "matchedPaperCorpusId": "215786522"
                        },
                        {
                            "start": 1025,
                            "end": 1028,
                            "matchedPaperCorpusId": "248572126"
                        },
                        {
                            "start": 1140,
                            "end": 1143,
                            "matchedPaperCorpusId": "86446136"
                        },
                        {
                            "start": 1370,
                            "end": 1374,
                            "matchedPaperCorpusId": "52070035"
                        },
                        {
                            "start": 1487,
                            "end": 1491,
                            "matchedPaperCorpusId": "54997157"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.865234375
                }
            ],
            "relevance_judgement": 0.865234375,
            "relevance_judgment_input_expanded": "# Title: NLPGuard: A Framework for Mitigating the Use of Protected Attributes by NLP Classifiers\n# Venue: Proc. ACM Hum. Comput. Interact.\n# Authors: Salvatore Greco, Ke Zhou, L. Capra, Tania Cerquitelli, D. Quercia\n## Abstract\nAI regulations are expected to prohibit machine learning models from using sensitive attributes during training. However, the latest Natural Language Processing (NLP) classifiers, which rely on deep learning, operate as black-box systems, complicating the detection and remediation of such misuse. Traditional bias mitigation methods in NLP aim for comparable performance across different groups based on attributes like gender or race but fail to address the underlying issue of reliance on protected attributes. To partly fix that, we introduce NLPGuard, a framework for mitigating the reliance on protected attributes in NLP classifiers. NLPGuard takes an unlabeled dataset, an existing NLP classifier, and its training data as input, producing a modified training dataset that significantly reduces dependence on protected attributes without compromising accuracy. NLPGuard is applied to three classification tasks: identifying toxic language, sentiment analysis, and occupation classification. Our evaluation shows that current NLP classifiers heavily depend on protected attributes, with up to 23% of the most predictive words associated with these attributes. However, NLPGuard effectively reduces this reliance by up to 79%, while slightly improving accuracy.\n## Bias mitigation for NLP\nBias in NLP decision-making has manifested itself in several ways, including dialogue generation [19], text classification [20], and machine translation [66].It usually arises from training data [22,67].For instance, pre-trained models and word embedding can inherit biases and stereotypes present in the large training corpora [7,9,25,58].When quantifying bias, existing works generally highlight disparities between demographic groups, with differences in performance or selection bias on protected attributes such as race, gender, religion, and sexual orientation [22,29,37,67].\n\nTo address biases in NLP, techniques can be developed that act at the three main stages of the NLP pipeline [38,75]: pre-processing (modifying training data), in-processing (imposing fairness constraints during model training), and post-processing (adjusting classifier predictions based on fairness metrics).Most existing works focus on the first two stages, exploiting data augmentation and modified model training techniques [4,6,20,52,58,64,77]. Furthermore, most of those studies focus on one protected category at a time.For example, Badjatiya et al. [6] proposed the identification of protected attributes, such as gender, by creating a manual list of words, measuring the skewed occurrence of words across classes or predicted class probability distribution of words.Park et al. [52] introduced gender swapping to equalize the number of male and female entities in the training data.Dixon et al. [20] proposed dataset augmentation strategies that generate new sentences using templates or replace protected attributes with generic tags, such as part-of-speech or named-entity tags.Zhang et al. [77] proposed mitigating biases in the training data by assuming a non-discrimination distribution and then reconstructing the distribution using instance weighting.Ravfogel et al. [58] proposed removing information from neural representations concerning gender or race for debiasing word embedding for NLP classification.",
            "reference_string": "[270878797 | Greco et al. | 2024 | Citations: 2]"
        },
        {
            "title": "A Parameter-Efficient Multi-Objective Approach to Mitigate Stereotypical Bias in Language Models",
            "venue": "GEBNLP",
            "year": 2024,
            "reference_count": 58,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2024.gebnlp-1.1, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2293409512",
                    "name": "Yifan Wang"
                },
                {
                    "authorId": "2293393732",
                    "name": "Vera Demberg"
                }
            ],
            "abstract": "Pre-trained language models have shown impressive abilities of understanding and generating natural languages. However, they typically inherit undesired human-like bias and stereotypes from training data, which raises concerns about putting these models into use in real-world scenarios. Although prior research has proposed to reduce bias using different fairness objectives, they usually fail to capture different representations of bias and, therefore, struggle with fully debiasing models. In this work, we introduce a multi-objective probability alignment approach to overcome current challenges by incorporating multiple debiasing losses to locate and penalize bias in different forms. Compared to existing methods, our proposed method can more effectively and comprehensively reduce stereotypical bias, and maintains the language ability of pre-trained models at the same time. Besides, we adopt prefix-tuning to optimize fairness objectives, and results show that it can achieve better bias removal than full fine-tuning while requiring much fewer computational resources. Our code and data are available at https://github.com/Ewanwong/debias_NLG.",
            "corpus_id": 271769656,
            "sentences": [
                {
                    "corpus_id": "271769656",
                    "title": "A Parameter-Efficient Multi-Objective Approach to Mitigate Stereotypical Bias in Language Models",
                    "text": "We evaluate our approach's performance of mitigating stereotypical bias in a GPT-2 small model on multiple benchmarks and compare its performance to various existing debiasing methods. \n\nBenchmark methods Benchmark methods we consider fall into the following categories depending on which stages they are applied to: \n\n\u2022 Pre-training: CDA (Zhao et al., 2019;Zmigrod et al., 2019;Lu et al., 2020) is a commonly used data augmentation method that augments the original biased dataset with synthetic gender-swapped sentences for fairer model pre-training. Dropout dissociates attributes and targets by increasing dropout rate in model pre-training (Webster et al., 2020). \n\n\u2022 Fine-tuning: Here we extend the concept of fine-tuning to include both full fine-tuning and parameter-efficient fine-tuning. Context-Debias (Kaneko and Bollegala, 2021) is a projection-based full fine-tuning method that encourages models to encode attribute and target words orthogonally to each other. Controllable-Bias (Sheng et al., 2020) mitigates bias by learning a discrete prompt that reduces negative regards for both genders. \n\n\u2022 Post-hoc: Iterative null-space projection (INLP) (Ravfogel et al., 2020) trains a set of linear classifiers to predict genders from embeddings and then projects embeddings to the null-space of learned classifiers. Self-Debias (Schick et al., 2021) adjusts next token probabilities at each step according to model's prediction to what extent the next token is biased. \n\nAs baselines, we also report the performance of vanilla GPT-2 and GPT-2 with randomly initialized prefix. Evaluation Metrics We adopt various different metrics to comprehensively evaluate the performance of our approach and benchmark methods.",
                    "score": 0.6200989658614015,
                    "section_title": "Experiments",
                    "char_start_offset": 16798,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 184
                        },
                        {
                            "start": 187,
                            "end": 316
                        },
                        {
                            "start": 319,
                            "end": 552
                        },
                        {
                            "start": 553,
                            "end": 668
                        },
                        {
                            "start": 671,
                            "end": 797
                        },
                        {
                            "start": 798,
                            "end": 975
                        },
                        {
                            "start": 976,
                            "end": 1107
                        },
                        {
                            "start": 1110,
                            "end": 1325
                        },
                        {
                            "start": 1326,
                            "end": 1478
                        },
                        {
                            "start": 1481,
                            "end": 1586
                        },
                        {
                            "start": 1587,
                            "end": 1723
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 339,
                            "end": 358,
                            "matchedPaperCorpusId": "102352962"
                        },
                        {
                            "start": 358,
                            "end": 379,
                            "matchedPaperCorpusId": "184486914"
                        },
                        {
                            "start": 813,
                            "end": 840,
                            "matchedPaperCorpusId": "231698657"
                        },
                        {
                            "start": 994,
                            "end": 1014,
                            "matchedPaperCorpusId": "218470535"
                        },
                        {
                            "start": 1161,
                            "end": 1184,
                            "matchedPaperCorpusId": "215786522"
                        },
                        {
                            "start": 1338,
                            "end": 1359,
                            "matchedPaperCorpusId": "232075876"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8642578125
                }
            ],
            "relevance_judgement": 0.8642578125,
            "relevance_judgment_input_expanded": "# Title: A Parameter-Efficient Multi-Objective Approach to Mitigate Stereotypical Bias in Language Models\n# Venue: GEBNLP\n# Authors: Yifan Wang, Vera Demberg\n## Abstract\nPre-trained language models have shown impressive abilities of understanding and generating natural languages. However, they typically inherit undesired human-like bias and stereotypes from training data, which raises concerns about putting these models into use in real-world scenarios. Although prior research has proposed to reduce bias using different fairness objectives, they usually fail to capture different representations of bias and, therefore, struggle with fully debiasing models. In this work, we introduce a multi-objective probability alignment approach to overcome current challenges by incorporating multiple debiasing losses to locate and penalize bias in different forms. Compared to existing methods, our proposed method can more effectively and comprehensively reduce stereotypical bias, and maintains the language ability of pre-trained models at the same time. Besides, we adopt prefix-tuning to optimize fairness objectives, and results show that it can achieve better bias removal than full fine-tuning while requiring much fewer computational resources. Our code and data are available at https://github.com/Ewanwong/debias_NLG.\n## Experiments\nWe evaluate our approach's performance of mitigating stereotypical bias in a GPT-2 small model on multiple benchmarks and compare its performance to various existing debiasing methods. \n\nBenchmark methods Benchmark methods we consider fall into the following categories depending on which stages they are applied to: \n\n\u2022 Pre-training: CDA (Zhao et al., 2019;Zmigrod et al., 2019;Lu et al., 2020) is a commonly used data augmentation method that augments the original biased dataset with synthetic gender-swapped sentences for fairer model pre-training. Dropout dissociates attributes and targets by increasing dropout rate in model pre-training (Webster et al., 2020). \n\n\u2022 Fine-tuning: Here we extend the concept of fine-tuning to include both full fine-tuning and parameter-efficient fine-tuning. Context-Debias (Kaneko and Bollegala, 2021) is a projection-based full fine-tuning method that encourages models to encode attribute and target words orthogonally to each other. Controllable-Bias (Sheng et al., 2020) mitigates bias by learning a discrete prompt that reduces negative regards for both genders. \n\n\u2022 Post-hoc: Iterative null-space projection (INLP) (Ravfogel et al., 2020) trains a set of linear classifiers to predict genders from embeddings and then projects embeddings to the null-space of learned classifiers. Self-Debias (Schick et al., 2021) adjusts next token probabilities at each step according to model's prediction to what extent the next token is biased. \n\nAs baselines, we also report the performance of vanilla GPT-2 and GPT-2 with randomly initialized prefix. Evaluation Metrics We adopt various different metrics to comprehensively evaluate the performance of our approach and benchmark methods.",
            "reference_string": "[271769656 | Wang et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Bias A-head? Analyzing Bias in Transformer-Based Language Model Attention Heads",
            "venue": "Proceedings of the 5th Workshop on Trustworthy NLP (TrustNLP 2025)",
            "year": 2023,
            "reference_count": 65,
            "citation_count": 8,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.10395, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2246043972",
                    "name": "Yi Yang"
                },
                {
                    "authorId": "2146477013",
                    "name": "Hanyu Duan"
                },
                {
                    "authorId": "2267242940",
                    "name": "Ahmed Abbasi"
                },
                {
                    "authorId": "9051130",
                    "name": "John P. Lalor"
                },
                {
                    "authorId": "1805674",
                    "name": "K. Tam"
                }
            ],
            "abstract": "Transformer-based pretrained large language models (PLM) such as BERT and GPT have achieved remarkable success in NLP tasks. However, PLMs are prone to encoding stereotypical biases. Although a burgeoning literature has emerged on stereotypical bias mitigation in PLMs, such as work on debiasing gender and racial stereotyping, how such biases manifest and behave internally within PLMs remains largely unknown. Understanding the internal stereotyping mechanisms may allow better assessment of model fairness and guide the development of effective mitigation strategies. In this work, we focus on attention heads, a major component of the Transformer architecture, and propose a bias analysis framework to explore and identify a small set of biased heads that are found to contribute to a PLM's stereotypical bias. We conduct extensive experiments to validate the existence of these biased heads and to better understand how they behave. We investigate gender and racial bias in the English language in two types of Transformer-based PLMs: the encoder-based BERT model and the decoder-based autoregressive GPT model. Overall, the results shed light on understanding the bias behavior in pretrained language models.",
            "corpus_id": 265281304,
            "sentences": [
                {
                    "corpus_id": "265281304",
                    "title": "Bias A-head? Analyzing Bias in Transformer-Based Language Model Attention Heads",
                    "text": "Transformer-based pretrained large language models (PLM) such as BERT and GPT have achieved remarkable success in NLP tasks. However, PLMs are prone to encoding stereotypical biases. Although a burgeoning literature has emerged on stereotypical bias mitigation in PLMs, such as work on debiasing gender and racial stereotyping, how such biases manifest and behave internally within PLMs remains largely unknown. Understanding the internal stereotyping mechanisms may allow better assessment of model fairness and guide the development of effective mitigation strategies. In this work, we focus on attention heads, a major component of the Transformer architecture, and propose a bias analysis framework to explore and identify a small set of biased heads that are found to contribute to a PLM's stereotypical bias. We conduct extensive experiments to validate the existence of these biased heads and to better understand how they behave. We investigate gender and racial bias in the English language in two types of Transformer-based PLMs: the encoder-based BERT model and the decoder-based autoregressive GPT model. Overall, the results shed light on understanding the bias behavior in pretrained language models.",
                    "score": 0.5443953293320843,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.86328125
                }
            ],
            "relevance_judgement": 0.86328125,
            "relevance_judgment_input_expanded": "# Title: Bias A-head? Analyzing Bias in Transformer-Based Language Model Attention Heads\n# Venue: Proceedings of the 5th Workshop on Trustworthy NLP (TrustNLP 2025)\n# Authors: Yi Yang, Hanyu Duan, Ahmed Abbasi, John P. Lalor, K. Tam\n## Abstract\nTransformer-based pretrained large language models (PLM) such as BERT and GPT have achieved remarkable success in NLP tasks. However, PLMs are prone to encoding stereotypical biases. Although a burgeoning literature has emerged on stereotypical bias mitigation in PLMs, such as work on debiasing gender and racial stereotyping, how such biases manifest and behave internally within PLMs remains largely unknown. Understanding the internal stereotyping mechanisms may allow better assessment of model fairness and guide the development of effective mitigation strategies. In this work, we focus on attention heads, a major component of the Transformer architecture, and propose a bias analysis framework to explore and identify a small set of biased heads that are found to contribute to a PLM's stereotypical bias. We conduct extensive experiments to validate the existence of these biased heads and to better understand how they behave. We investigate gender and racial bias in the English language in two types of Transformer-based PLMs: the encoder-based BERT model and the decoder-based autoregressive GPT model. Overall, the results shed light on understanding the bias behavior in pretrained language models.\n",
            "reference_string": "[265281304 | Yang et al. | 2023 | Citations: 8]"
        },
        {
            "title": "Improving Commonsense Bias Classification by Mitigating the Influence of Demographic Terms",
            "venue": "IEEE Access",
            "year": 2024,
            "reference_count": 28,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1109/access.2024.3477599",
                "status": "GOLD",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.07229, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2305677491",
                    "name": "JinKyu Lee"
                },
                {
                    "authorId": "2306835658",
                    "name": "Jihie Kim"
                }
            ],
            "abstract": "Understanding commonsense knowledge is crucial in the field of Natural Language Processing (NLP). However, the presence of demographic terms in commonsense knowledge poses a potential risk of compromising the performance of NLP models. This study aims to investigate and propose methods for enhancing the performance and effectiveness of a commonsense polarization classifier by mitigating the influence of demographic terms. Three methods are introduced in this paper: (1) hierarchical generalization of demographic terms (2) threshold-based augmentation and (3) integration of hierarchical generalization and threshold-based augmentation methods(IHTA). The first method involves replacing demographic terms with more general ones based on a term hierarchy ontology, aiming to mitigate the influence of specific terms. To address the limited bias-related information, the second method measures the polarization of demographic terms by comparing the changes in the model\u2019s predictions when these terms are masked versus unmasked. This method augments commonsense sentences containing terms with high polarization values by replacing their predicates with synonyms generated by ChatGPT. The third method combines the two approaches, starting with threshold-based augmentation followed by hierarchical generalization. The experiments show that the first method increases the accuracy over the baseline by 2.33%, and the second one by 0.96% over standard augmentation methods. The IHTA techniques yielded an 8.82% and 9.96% higher accuracy than threshold-based and standard augmentation methods, respectively.",
            "corpus_id": 270380297,
            "sentences": [
                {
                    "corpus_id": "270380297",
                    "title": "Improving Commonsense Bias Classification by Mitigating the Influence of Demographic Terms",
                    "text": "Extensive research in Natural Language Processing (NLP) has focused on investigating biases in language models and developing strategies to understand and mitigate them.A study exploring the relationship between the size of language models and their biases was conducted, finding that larger models exhibit more nuanced biases and an increased potential for bias in specific categories [5].Researchers have also explored bias evaluation in language models, employing extrinsic evaluation methods and debiasing techniques to identify and address bias in real world applications and downstream tasks [7,8].Intrinsic evaluation methods, such as analyzing word or sentence embeddings, contribute to a deeper understanding of biases within language models [9,10,11].Additionally, It has been shown that systematic differences in bias measurement can be revealed through the unification of extrinsic evaluation methods, attributed to parameter choices [12].Moreover, these methods have been used to identify biased decisions from models in specific tasks [13].Recent studies have drawn significant attention to the bias caused by demographic terms in language models.Various techniques have been proposed to address and mitigate demographic bias, aiming to promote fairness, equality, and inclusivity.These techniques include debiasing models trained on human-annotated examples [10], fairness-aware neural language models [14], bias-mitigating transformer architectures [15], and debiasing frameworks to identify and correct demographic bias [16].Additionally, post-hoc debiasing techniques have been explored, where a debiasing step is added to sentence representations after initial training, prior to their utilization in downstream tasks [10,17].While preceding investigations focused on biases in language models linked to particular categories like origin and gender, our study adopts a more extensive methodology, assessing biases across a wider range of demographic terms.Building upon previous studies, our research proposes novel approaches to mitigate the impact of demographic terms and enhance the performance of the commonsense polarization classifier in knowledge models.This approach distinguishes itself from prior research by employing augmentation to substitute the predicate segment of a sentence with a synonym while retaining the original semantic content.Furthermore, to address bias stemming from demographic terms, our methodology involves substituting specific demographic terms with broader alternatives.",
                    "score": 0.6077283508324098,
                    "section_title": "II. RELATED WORK",
                    "char_start_offset": 3511,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 169
                        },
                        {
                            "start": 169,
                            "end": 390
                        },
                        {
                            "start": 390,
                            "end": 604
                        },
                        {
                            "start": 604,
                            "end": 761
                        },
                        {
                            "start": 761,
                            "end": 951
                        },
                        {
                            "start": 951,
                            "end": 1054
                        },
                        {
                            "start": 1054,
                            "end": 1161
                        },
                        {
                            "start": 1161,
                            "end": 1295
                        },
                        {
                            "start": 1295,
                            "end": 1542
                        },
                        {
                            "start": 1542,
                            "end": 1745
                        },
                        {
                            "start": 1745,
                            "end": 1975
                        },
                        {
                            "start": 1975,
                            "end": 2181
                        },
                        {
                            "start": 2181,
                            "end": 2373
                        },
                        {
                            "start": 2373,
                            "end": 2526
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 386,
                            "end": 389,
                            "matchedPaperCorpusId": "248649722"
                        },
                        {
                            "start": 598,
                            "end": 601,
                            "matchedPaperCorpusId": "58006082"
                        },
                        {
                            "start": 751,
                            "end": 754,
                            "matchedPaperCorpusId": "23163324"
                        },
                        {
                            "start": 754,
                            "end": 757,
                            "matchedPaperCorpusId": "1704893"
                        },
                        {
                            "start": 757,
                            "end": 760,
                            "matchedPaperCorpusId": "219530686"
                        },
                        {
                            "start": 946,
                            "end": 950,
                            "matchedPaperCorpusId": "235658325"
                        },
                        {
                            "start": 1049,
                            "end": 1053,
                            "matchedPaperCorpusId": "44090948"
                        },
                        {
                            "start": 1373,
                            "end": 1377,
                            "matchedPaperCorpusId": "1704893"
                        },
                        {
                            "start": 1417,
                            "end": 1421,
                            "matchedPaperCorpusId": "19117312"
                        },
                        {
                            "start": 1465,
                            "end": 1469,
                            "matchedPaperCorpusId": "198968250"
                        },
                        {
                            "start": 1737,
                            "end": 1741,
                            "matchedPaperCorpusId": "1704893"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.86279296875
                }
            ],
            "relevance_judgement": 0.86279296875,
            "relevance_judgment_input_expanded": "# Title: Improving Commonsense Bias Classification by Mitigating the Influence of Demographic Terms\n# Venue: IEEE Access\n# Authors: JinKyu Lee, Jihie Kim\n## Abstract\nUnderstanding commonsense knowledge is crucial in the field of Natural Language Processing (NLP). However, the presence of demographic terms in commonsense knowledge poses a potential risk of compromising the performance of NLP models. This study aims to investigate and propose methods for enhancing the performance and effectiveness of a commonsense polarization classifier by mitigating the influence of demographic terms. Three methods are introduced in this paper: (1) hierarchical generalization of demographic terms (2) threshold-based augmentation and (3) integration of hierarchical generalization and threshold-based augmentation methods(IHTA). The first method involves replacing demographic terms with more general ones based on a term hierarchy ontology, aiming to mitigate the influence of specific terms. To address the limited bias-related information, the second method measures the polarization of demographic terms by comparing the changes in the model\u2019s predictions when these terms are masked versus unmasked. This method augments commonsense sentences containing terms with high polarization values by replacing their predicates with synonyms generated by ChatGPT. The third method combines the two approaches, starting with threshold-based augmentation followed by hierarchical generalization. The experiments show that the first method increases the accuracy over the baseline by 2.33%, and the second one by 0.96% over standard augmentation methods. The IHTA techniques yielded an 8.82% and 9.96% higher accuracy than threshold-based and standard augmentation methods, respectively.\n## II. RELATED WORK\nExtensive research in Natural Language Processing (NLP) has focused on investigating biases in language models and developing strategies to understand and mitigate them.A study exploring the relationship between the size of language models and their biases was conducted, finding that larger models exhibit more nuanced biases and an increased potential for bias in specific categories [5].Researchers have also explored bias evaluation in language models, employing extrinsic evaluation methods and debiasing techniques to identify and address bias in real world applications and downstream tasks [7,8].Intrinsic evaluation methods, such as analyzing word or sentence embeddings, contribute to a deeper understanding of biases within language models [9,10,11].Additionally, It has been shown that systematic differences in bias measurement can be revealed through the unification of extrinsic evaluation methods, attributed to parameter choices [12].Moreover, these methods have been used to identify biased decisions from models in specific tasks [13].Recent studies have drawn significant attention to the bias caused by demographic terms in language models.Various techniques have been proposed to address and mitigate demographic bias, aiming to promote fairness, equality, and inclusivity.These techniques include debiasing models trained on human-annotated examples [10], fairness-aware neural language models [14], bias-mitigating transformer architectures [15], and debiasing frameworks to identify and correct demographic bias [16].Additionally, post-hoc debiasing techniques have been explored, where a debiasing step is added to sentence representations after initial training, prior to their utilization in downstream tasks [10,17].While preceding investigations focused on biases in language models linked to particular categories like origin and gender, our study adopts a more extensive methodology, assessing biases across a wider range of demographic terms.Building upon previous studies, our research proposes novel approaches to mitigate the impact of demographic terms and enhance the performance of the commonsense polarization classifier in knowledge models.This approach distinguishes itself from prior research by employing augmentation to substitute the predicate segment of a sentence with a synonym while retaining the original semantic content.Furthermore, to address bias stemming from demographic terms, our methodology involves substituting specific demographic terms with broader alternatives.",
            "reference_string": "[270380297 | Lee et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Fairness-Aware Structured Pruning in Transformers",
            "venue": "AAAI Conference on Artificial Intelligence",
            "year": 2023,
            "reference_count": 47,
            "citation_count": 17,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.15398, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2077390471",
                    "name": "A. Zayed"
                },
                {
                    "authorId": "2267925411",
                    "name": "Gon\u00e7alo Mordido"
                },
                {
                    "authorId": "3197429",
                    "name": "S. Shabanian"
                },
                {
                    "authorId": "2276424514",
                    "name": "Ioana Baldini"
                },
                {
                    "authorId": "123607932",
                    "name": "Sarath Chandar"
                }
            ],
            "abstract": "The increasing size of large language models (LLMs) has introduced challenges in their training and inference. Removing model components is perceived as a solution to tackle the large model sizes, however, existing pruning methods solely focus on performance, without considering an essential aspect for the responsible use of LLMs: model fairness. It is crucial to address the fairness of LLMs towards diverse groups, such as women, Black people, LGBTQ+, Jewish communities, among others, as they are being deployed and available to a wide audience. In this work, first, we investigate how attention heads impact fairness and performance in pre-trained transformer-based language models. We then propose a novel method to prune the attention heads that negatively impact fairness while retaining the heads critical for performance, i.e. language modeling capabilities. Our approach is practical in terms of time and resources, as it does not require fine-tuning the final pruned, and fairer, model. Our findings demonstrate a reduction in gender bias by 19%, 19.5%, 39.5%, 34.7%, 23%, and 8% for DistilGPT-2, GPT-2, GPT-Neo of two different sizes, GPT-J, and Llama 2 models, respectively, in comparison to the biased model, with only a slight decrease in performance. WARNING: This work uses language that is offensive in nature.",
            "corpus_id": 266550954,
            "sentences": [
                {
                    "corpus_id": "266550954",
                    "title": "Fairness-Aware Structured Pruning in Transformers",
                    "text": "The increasing size of large language models (LLMs) has introduced challenges in their training and inference. Removing model components is perceived as a solution to tackle the large model sizes, however, existing pruning methods solely focus on performance, without considering an essential aspect for the responsible use of LLMs: model fairness. It is crucial to address the fairness of LLMs towards diverse groups, such as women, Black people, LGBTQ+, Jewish communities, among others, as they are being deployed and available to a wide audience. In this work, first, we investigate how attention heads impact fairness and performance in pre-trained transformer-based language models. We then propose a novel method to prune the attention heads that negatively impact fairness while retaining the heads critical for performance, i.e. language modeling capabilities. Our approach is practical in terms of time and resources, as it does not require fine-tuning the final pruned, and fairer, model. Our findings demonstrate a reduction in gender bias by 19%, 19.5%, 39.5%, 34.7%, 23%, and 8% for DistilGPT-2, GPT-2, GPT-Neo of two different sizes, GPT-J, and Llama 2 models, respectively, in comparison to the biased model, with only a slight decrease in performance. WARNING: This work uses language that is offensive in nature.",
                    "score": 0.5115676371870865,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.86083984375
                }
            ],
            "relevance_judgement": 0.86083984375,
            "relevance_judgment_input_expanded": "# Title: Fairness-Aware Structured Pruning in Transformers\n# Venue: AAAI Conference on Artificial Intelligence\n# Authors: A. Zayed, Gon\u00e7alo Mordido, S. Shabanian, Ioana Baldini, Sarath Chandar\n## Abstract\nThe increasing size of large language models (LLMs) has introduced challenges in their training and inference. Removing model components is perceived as a solution to tackle the large model sizes, however, existing pruning methods solely focus on performance, without considering an essential aspect for the responsible use of LLMs: model fairness. It is crucial to address the fairness of LLMs towards diverse groups, such as women, Black people, LGBTQ+, Jewish communities, among others, as they are being deployed and available to a wide audience. In this work, first, we investigate how attention heads impact fairness and performance in pre-trained transformer-based language models. We then propose a novel method to prune the attention heads that negatively impact fairness while retaining the heads critical for performance, i.e. language modeling capabilities. Our approach is practical in terms of time and resources, as it does not require fine-tuning the final pruned, and fairer, model. Our findings demonstrate a reduction in gender bias by 19%, 19.5%, 39.5%, 34.7%, 23%, and 8% for DistilGPT-2, GPT-2, GPT-Neo of two different sizes, GPT-J, and Llama 2 models, respectively, in comparison to the biased model, with only a slight decrease in performance. WARNING: This work uses language that is offensive in nature.\n",
            "reference_string": "[266550954 | Zayed et al. | 2023 | Citations: 17]"
        },
        {
            "title": "Gender Encoding Patterns in Pretrained Language Model Representations",
            "venue": "Proceedings of the 5th Workshop on Trustworthy NLP (TrustNLP 2025)",
            "year": 2025,
            "reference_count": 49,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.06734, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2261278375",
                    "name": "Mahdi Zakizadeh"
                },
                {
                    "authorId": "1717641",
                    "name": "Mohammad Taher Pilehvar"
                }
            ],
            "abstract": "Gender bias in pretrained language models (PLMs) poses significant social and ethical challenges. Despite growing awareness, there is a lack of comprehensive investigation into how different models internally represent and propagate such biases. This study adopts an information-theoretic approach to analyze how gender biases are encoded within various encoder-based architectures. We focus on three key aspects: identifying how models encode gender information and biases, examining the impact of bias mitigation techniques and fine-tuning on the encoded biases and their effectiveness, and exploring how model design differences influence the encoding of biases. Through rigorous and systematic investigation, our findings reveal a consistent pattern of gender encoding across diverse models. Surprisingly, debiasing techniques often exhibit limited efficacy, sometimes inadvertently increasing the encoded bias in internal representations while reducing bias in model output distributions. This highlights a disconnect between mitigating bias in output distributions and addressing its internal representations. This work provides valuable guidance for advancing bias mitigation strategies and fostering the development of more equitable language models.",
            "corpus_id": 276902427,
            "sentences": [
                {
                    "corpus_id": "276902427",
                    "title": "Gender Encoding Patterns in Pretrained Language Model Representations",
                    "text": "Bias mitigation in language models seeks to address both overt biases in model outputs and the subtler, systemic biases embedded within the model's internal representations. Effective techniques should suppress these encoded biases while maintaining model utility. In this section, we investigate the impact of various debiasing methods on compression values, used as a measure of encoded gender information, and evaluate their effectiveness across different experimental setups and models.",
                    "score": 0.647677692621207,
                    "section_title": "Impact of Bias Mitigation",
                    "char_start_offset": 19298,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 173
                        },
                        {
                            "start": 174,
                            "end": 264
                        },
                        {
                            "start": 265,
                            "end": 490
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.85791015625
                }
            ],
            "relevance_judgement": 0.85791015625,
            "relevance_judgment_input_expanded": "# Title: Gender Encoding Patterns in Pretrained Language Model Representations\n# Venue: Proceedings of the 5th Workshop on Trustworthy NLP (TrustNLP 2025)\n# Authors: Mahdi Zakizadeh, Mohammad Taher Pilehvar\n## Abstract\nGender bias in pretrained language models (PLMs) poses significant social and ethical challenges. Despite growing awareness, there is a lack of comprehensive investigation into how different models internally represent and propagate such biases. This study adopts an information-theoretic approach to analyze how gender biases are encoded within various encoder-based architectures. We focus on three key aspects: identifying how models encode gender information and biases, examining the impact of bias mitigation techniques and fine-tuning on the encoded biases and their effectiveness, and exploring how model design differences influence the encoding of biases. Through rigorous and systematic investigation, our findings reveal a consistent pattern of gender encoding across diverse models. Surprisingly, debiasing techniques often exhibit limited efficacy, sometimes inadvertently increasing the encoded bias in internal representations while reducing bias in model output distributions. This highlights a disconnect between mitigating bias in output distributions and addressing its internal representations. This work provides valuable guidance for advancing bias mitigation strategies and fostering the development of more equitable language models.\n## Impact of Bias Mitigation\nBias mitigation in language models seeks to address both overt biases in model outputs and the subtler, systemic biases embedded within the model's internal representations. Effective techniques should suppress these encoded biases while maintaining model utility. In this section, we investigate the impact of various debiasing methods on compression values, used as a measure of encoded gender information, and evaluate their effectiveness across different experimental setups and models.",
            "reference_string": "[276902427 | Zakizadeh et al. | 2025 | Citations: 0]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "277150560",
            "title": "Attention Pruning: Automated Fairness Repair of Language Models via Surrogate Simulated Annealing",
            "text": "Recent works have started using expert models for bias reduction. Orgad and Belinkov [49] predicted biased samples with an auxiliary model and reweighted them during pre-training. Jeon et al. [35] employed binary classifiers, or bias experts, to pinpoint biased examples in specific classes. \n\nIn-processing methods modify the model's parameters and optimization process. Methods such as ADELE [36] introduce adapter layers for parameter-efficient fine-tuning to reduce bias. Regularization techniques can be applied to penalize the model for producing biased outputs [47]. Additionally, debiasing word embeddings can help reduce bias in the model's representation of words by reducing gendered or racial stereotypes [7,25,68]. Several loss functions, such as declustering loss [23] and contrastive loss [34,41] are also shown to be effective for mitigating bias. Zhang et al. [71] leveraged gradient-based explanations to target sensitive attributes and adjust training for balanced fairness and performance. Apart from that, Dhingra et al. [16] used style transfer to replace harmful language while maintaining the original meaning. \n\nPost-processing techniques modify the model's inference behavior after complete training. This may involve altering the model before inference or the model output during/after inference. For instance, Gehman et al. [24] proposed token-blocking methods during decoding to prevent the generation of harmful or biased terms. \n\nHauzenberger et al. [33] introduced sparse debiasing subnetworks that are trained separately and can be applied to the model at inference time. Qian et al. [53] performed keyword-based distillation to remove bias during inference. Tokpo and Calders [61] identify biased tokens and replace them with less stereotypical terms. Postprocessing approaches are beneficial since dataset collection and LLM training are expensive processes that we may not always be able to repeat if issues of unfairness are found. Furthermore, postprocessing mitigation approaches are sometimes the only viable option with the status quo of utilizing large pre-trained LLMs that take a huge amount of resources to train.",
            "score": 0.7231611812219216,
            "section_title": "Related Works",
            "char_start_offset": 39048,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 65
                },
                {
                    "start": 66,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 291
                },
                {
                    "start": 294,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 1009
                },
                {
                    "start": 1010,
                    "end": 1134
                },
                {
                    "start": 1137,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1458
                },
                {
                    "start": 1461,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1691
                },
                {
                    "start": 1692,
                    "end": 1785
                },
                {
                    "start": 1786,
                    "end": 1968
                },
                {
                    "start": 1969,
                    "end": 2158
                }
            ],
            "ref_mentions": [
                {
                    "start": 85,
                    "end": 89,
                    "matchedPaperCorpusId": "259138920"
                },
                {
                    "start": 568,
                    "end": 572,
                    "matchedPaperCorpusId": "221136077"
                },
                {
                    "start": 778,
                    "end": 782,
                    "matchedPaperCorpusId": "236477795"
                },
                {
                    "start": 804,
                    "end": 808,
                    "matchedPaperCorpusId": "253157517"
                },
                {
                    "start": 808,
                    "end": 811,
                    "matchedPaperCorpusId": "233864681"
                },
                {
                    "start": 877,
                    "end": 881,
                    "matchedPaperCorpusId": "258833564"
                },
                {
                    "start": 1617,
                    "end": 1621,
                    "matchedPaperCorpusId": "236459953"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9658203125
        },
        {
            "corpus_id": "272826949",
            "title": "Evaluating Gender, Racial, and Age Biases in Large Language Models: A Comparative Analysis of Occupational and Crime Scenarios",
            "text": "Recent advancements in Large Language Models(LLMs) have been notable, yet widespread enterprise adoption remains limited due to various constraints. This paper examines bias in LLMs-a crucial issue affecting their usability, reliability, and fairness. Researchers are developing strategies to mitigate bias, including debiasing layers, specialized reference datasets like Winogender and Winobias, and reinforcement learning with human feedback (RLHF). These techniques have been integrated into the latest LLMs. Our study evaluates gender bias in occupational scenarios and gender, age, and racial bias in crime scenarios across four leading LLMs released in 2024: Gemini 1.5 Pro, Llama 3 70B, Claude 3 Opus, and GPT-4o. Findings reveal that LLMs often depict female characters more frequently than male ones in various occupations, showing a 37% deviation from US BLS data. In crime scenarios, deviations from US FBI data are 54% for gender, 28% for race, and 17% for age. We observe that efforts to reduce gender and racial bias often lead to outcomes that may over-index one sub-class, potentially exacerbating the issue. These results highlight the limitations of current bias mitigation techniques and underscore the need for more effective approaches.",
            "score": 0.720422795403263,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7841796875
        },
        {
            "corpus_id": "239015827",
            "title": "An Empirical Survey of the Effectiveness of Debiasing Techniques for Pre-trained Language Models",
            "text": "To the best of our knowledge, we have performed the first large scale evaluation of multiple debiasing techniques for pre-trained language models. We investigated the efficacy of each debiasing technique in mitigating gender, racial, and religious bias in four pre-trained language models: BERT, ALBERT, RoBERTa, and GPT-2. We used three intrinsic bias benchmarks to evaluate the effectiveness of each debiasing technique in mitigating bias and also investigated how debiasing impacts language modeling and downstream NLU task performance. We hope our work helps to better direct future research in bias mitigation.",
            "score": 0.6975828528348689,
            "section_title": "Conclusion",
            "char_start_offset": 25377,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8642578125
        },
        {
            "corpus_id": "259095603",
            "title": "Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions",
            "text": "Recently, there has been a surge of interest in pretrained large language models (LLM) in natural language processing (NLP). It has been shown that the pre-training + finetuning of a model drastically improves its performance on downstream tasks as the knowledge captured by the pre-training on a large corpus is transferred to the downstream application when finetuning the model. However, this also leads to societal biases like gender bias that were implicitly learned by the pre-trained models being transferred to crucial downstream applications like job recommendation engines (Zhao et al., 2019; * Equal Contribution Barocas et al., 2017;Kurita et al., 2019). Analyzing and mitigating bias without requiring significant re-training or compute resources is crucial to the widespread adoption of LLMs in downstream applications.\n\nPrevious work (Nadeem et al., 2021), (Nangia et al., 2020a), (Cer et al., 2018) has attempted to quantify bias, and others such as Ravfogel et al. (2020) and Liang et al. (2021) have attempted to remove it algorithmically from the models. Closer to our work are data-manipulative techniques such as Zmigrod et al. (2019) and Maudslay et al. (2019) that modify the dataset and further fine-tune the model. In this paper, we propose simple data intervention strategies and show that they can mitigate gender bias in pre-trained models with the help of few-shot fine-tuning. Moreover, taking inspiration from Schick et al. (2021), we find that by utilizing a biased pre-trained LLM for mining for most gender-biased samples in a dataset, our methods can mitigate gender bias with very few training samples. Finally, we perform an extensive evaluation of our debiasing technique on two recent bias benchmarks (Nadeem et al., 2021) and show that our method outperforms three existing state-of-the-art techniques and performs comparably to the other two. Our main contributions are the following:\n\n\u2022 We propose simple data intervention techniques that can be used to reduce gender bias in a pre-trained LLM with few training examples (few-shot), thus making human-in-theloop",
            "score": 0.69026183276903,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 896,
                    "end": 914,
                    "matchedPaperCorpusId": "53245704"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78759765625
        },
        {
            "corpus_id": "239015827",
            "title": "An Empirical Survey of the Effectiveness of Debiasing Techniques for Pre-trained Language Models",
            "text": "Large pre-trained language models have proven effective across a variety of tasks in natural language processing, often obtaining state of the art performance (Peters et al., 2018;Devlin et al., 2019;Radford et al., 2019;Brown et al., 2020). These models are typically trained on large amounts of text, originating from unmoderated sources, such as the internet. While the performance of these pre-trained models is remarkable, recent work has shown that they capture social biases from the data they are trained on (May et al. 2019;Kurita et al. 2019;Webster et al. 2020;Nangia et al. 2020;Nadeem et al. 2021, inter alia). Because of these findings, an increasing amount of research has focused on developing techniques to mitigate these biases (Liang et al., 2020;Ravfogel et al., 2020;Webster et al., 2020;Kaneko and Bollegala, 2021;Schick et al., 2021;Lauscher et al., 2021). However, the proposed techniques are often not investigated thoroughly. For instance, much work focuses only on mitigating gender bias despite pre-trained language models being plagued by other social biases (e.g., racial or religious bias). Additionally, the impact that debiasing has on both downstream task performance, as well as language modeling ability, is often not well explored.\n\nIn this paper, we perform an empirical survey of the effectiveness of five recently proposed debiasing techniques for pre-trained language models: 2 Counterfactual Data Augmentation (CDA; Zmigrod et al. 2019;Webster et al. 2020), Dropout (Webster et al., 2020), Iterative Nullspace Projection (INLP; Ravfogel et al. 2020), Self-Debias (Schick et al., 2021), and SentenceDebias (Liang et al., 2020). Following the taxonomy described by Blodgett et al. (2020), our work studies the effectiveness of these techniques in mitigating representational biases from pre-trained language models. More specifically, we investigate mitigating gender, racial, and religious biases in three masked language models (",
            "score": 0.6867862101879415,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83740234375
        },
        {
            "corpus_id": "256390197",
            "title": "How Far Can It Go? On Intrinsic Gender Bias Mitigation for Text Classification",
            "text": "the discovery that results from different metrics for intrinsic bias in many cases do not correlate (Delobelle et al., 2022), we find this probe effective as an extra step in evaluating the efficacy of these mitigation strategies. We realize from this study that how intrinsic bias has been measured and the choice of bias mitigation strategies explored by some existing works have not been ideal. We further use this probe to investigate if some proposed mitigation strategies superficially conceal bias.\n\nIn this work, when we refer to bias in a language model, we mean stereotyping bias as defined by Garrido-Mu\u00f1oz et al. (2021) as \"the undesired variation of the [association] of certain words in that language model according to certain prior words in a given domain\". We focus our experiments on gender bias for two primary reasons: its intuitive nature making it easy to analyze and discuss, and the accessibility of resources and datasets regarding gender. For the same reason, we narrow our definition of bias in our experiments and analysis to binary gender bias 1 . This paper considers the primary goal of mitigating intrinsic bias to ensure fairness in downstream tasks.\n\nWe consider one case of binary classification and one multiclass classification case, all on English language corpora using the BERT-large (Devlin et al., 2019) and the ALBERT-large (Lan et al., 2019) pretrained models for each task.\n\nIn summary, we develop an extensive probe 2 to uncover intrinsic bias in pertained contextualized language models, and seek to answer three key research questions: RQ1: Do different intrinsic bias metrics respond differently to different bias mitigation techniques? ( \u00a7 4.1). RQ2: Can intrinsic bias mitigation techniques hide bias instead of resolving it? ( \u00a7 5.1). RQ3: Do intrinsic bias mitigation techniques in language models improve fairness in downstream tasks? ( \u00a7 5.2) 2 Measuring and mitigating bias Since many techniques for measuring and mitigating gender bias have been introduced for both intrinsic and extrinsic bias, we only discuss techniques we use in experiments in this paper.\n\nBias mitigation techniques can be applied to pretrained or finetuned language models, or both. Figure 1 illustrates these interactions with both training settings and this section",
            "score": 0.6756414479379669,
            "section_title": "Introduction",
            "char_start_offset": 1941,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6201171875
        },
        {
            "corpus_id": "248780440",
            "title": "Auto-Debias: Debiasing Masked Language Models with Automated Biased Prompts",
            "text": "Mitigating non-gender biases is a challenging task in debiasing research. Meade et al. (2021) empirically show that some of the debiasing techniques considered in our benchmarks generalize poorly in racial debiasing. One of the challenges could be the ambiguity of words (white, black) in different contexts. Therefore, the counterfactual dataaugmentation approach or the fine-tuning approach relying on external corpora may be less effective. \n\nIn this experiment, we evaluate Auto-Debias's performance in mitigating racial biases in the PLMs and evaluate the performance using SEAT 3,3b,4,5,and 5b tests. We can see from Table 4 that Auto-Debias can meaningfully mitigate the racial biases in terms of the SEAT metric. Note that the racial SEAT test examines any association difference between European-American/African American names/terms and the stereotype words (pleasant vs. unpleasant). For example, on BERT, Auto-Debias considerably mitigates the racial bias in 4 out of 5 SEAT sub-tests, and the overall score is reduced from 0.23 to 0.18. On ALBERT, Auto-Debias also significantly mitigates the bias in all subsets. \n\n5 Does Auto-Debias affect downstream NLP tasks? Meade et al. (2021) find that the previous debiasing techniques often come at a price of worsened performance in downstream NLP tasks, which implies that prior work might over-debias. Our work instead directly probes the bias encoded in PLM, alleviating the concern of over-debias. \n\nIn this section, we evaluate the gender debiased BERT/ALBERT/RoBERTa on the General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019), to examine the capabilities of the language models. The results are reported in Table 5. The racial-debiased PLM models achieve similar GLUE scores. Auto-Debias performs on par with the base models on most natural language understanding tasks. There is only one exception: CoLA dataset. CoLA evaluates linguistic acceptability, judging whether a sentence is grammatically correct.",
            "score": 0.6750622683457599,
            "section_title": "Mitigating racial bias",
            "char_start_offset": 22414,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 73
                },
                {
                    "start": 74,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 443
                },
                {
                    "start": 446,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 1049
                },
                {
                    "start": 1050,
                    "end": 1126
                },
                {
                    "start": 1129,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1458
                },
                {
                    "start": 1461,
                    "end": 1668
                },
                {
                    "start": 1669,
                    "end": 1705
                },
                {
                    "start": 1706,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 1860
                },
                {
                    "start": 1861,
                    "end": 1903
                },
                {
                    "start": 1904,
                    "end": 1997
                }
            ],
            "ref_mentions": [
                {
                    "start": 1596,
                    "end": 1615,
                    "matchedPaperCorpusId": "5034059"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81201171875
        },
        {
            "corpus_id": "258461365",
            "title": "Modular and On-demand Bias Mitigation with Attribute-Removal Subnetworks",
            "text": "A large body of research evidences the existence of societal biases and stereotypes in pre-trained language models (PLMs) (Zhao et al., 2019;Sheng et al., 2019;Rekabsaz et al., 2021), and their potential harms when used in down-stream tasks (Blodgett et al., 2020;De-Arteaga et al., 2019;Rekabsaz and Schedl, 2020;Stanovsky et al., 2019). Common in-processing approaches to bias mitigation update a model's (typically all) parameters to satisfy specific attribute erasure criteria through optimization methods such as adversarial train-ing (Elazar and Goldberg, 2018;Rekabsaz et al., 2021), and mutual information reduction (Colombo et al., 2021). These methods are shown to be effective in reducing the footprint of protected attributes (e.g., gender, race, etc.) in the resulting model. \n\nHowever when using such debiasing models in practice and in specific use-cases, system designers or end-users might still prefer to instead use the original model, or a debiased variation in respect to a particular subset of protected attributes. This can be due to various reasons such as the nature of a given input, preference of an individual end-user, or fairness-utility trade-off considerations. For instance, while a bias-aware model should indeed be agnostic to genders when the input is about genderneutral occupations (such as nurse or CEO), certain topics like pregnancy may specifically require gender information for a correct model decision.1 Also as shown in previous studies (Zerveas et al., 2022;Biega et al., 2018;Rekabsaz et al., 2021), since improving fairness on specific tasks may come with the cost of performance degradation, it is necessary to provide on-demand control over whether to impose fairness/debiasing criteria. Using existing approaches, this would require maintaining and deploying multiple large parallel models for every protected attribute, resulting in overly complex and resource-heavy pipelines and increased latency. \n\nTo address this, we introduce a novel modular bias mitigation approach using sparse weightdifference networks.",
            "score": 0.6715979608748242,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 788
                },
                {
                    "start": 791,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1738
                },
                {
                    "start": 1739,
                    "end": 1952
                },
                {
                    "start": 1955,
                    "end": 2065
                }
            ],
            "ref_mentions": [
                {
                    "start": 122,
                    "end": 141,
                    "matchedPaperCorpusId": "102352962"
                },
                {
                    "start": 141,
                    "end": 160,
                    "matchedPaperCorpusId": "202537041"
                },
                {
                    "start": 241,
                    "end": 264,
                    "matchedPaperCorpusId": "218971825"
                },
                {
                    "start": 288,
                    "end": 314,
                    "matchedPaperCorpusId": "218470518"
                },
                {
                    "start": 314,
                    "end": 337,
                    "matchedPaperCorpusId": "173991101"
                },
                {
                    "start": 540,
                    "end": 567,
                    "matchedPaperCorpusId": "52056513"
                },
                {
                    "start": 1483,
                    "end": 1505,
                    "matchedPaperCorpusId": "248837769"
                },
                {
                    "start": 1505,
                    "end": 1524,
                    "matchedPaperCorpusId": "13697725"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82373046875
        },
        {
            "corpus_id": "236477795",
            "title": "He is very intelligent, she is very beautiful? On Mitigating Social Biases in Language Modelling and Generation",
            "text": "(2) The six word pairs we use to further pre-train BERT for racial bias mitigation include (Black, Caucasian), (Africa, America), (Black, White), (slave, manager), (musician, executive), and (homeless, leader). We believe that while using pre-defined word pairs has been successful in mitigating gender biases (Bolukbasi et al., 2016;Qian et al., 2019;Liang et al., 2020) perhaps due to the perceived binary nature of gender, 4 it is not straightforward to use such pairs 4 We acknowledge the rich communities that form other or tuples for other demographics such as race, occupations, age groups, etc., as these dimensions are often of more diversity than gender, and there are not many word-level indications that can represent or define a specific racial group, other than those that directly mention the group itself. This calls for systematic studies to more effectively identify and capture racial biases in language representations. \n\nWe also compute the SEAT scores of the DEBI-ASBERT variants trained for racial bias mitigation on gender, and vice-versa. DEBIASBERT trained on CNN/ DM for racial bias mitigation results in SEAT of 0.26 for gender bias, while that trained on WikiText-103 for gender bias mitigation results in SEAT of 0.2 for racial bias. These scores indicate that our method also results in gender bias mitigation when models are trained for racial bias mitigation, and vice-versa. Sentence Completion. Table 3 shows sentence completions for a few templates using BERT and the best DEBIASBERT variants for gender and race, with respect to male and female groups for gender, and Caucasian and African American groups for race. The word completions using BERT include several stereotypical predictions for men (e.g., intelligent, manager) and women (beautiful, receptionist), while those by DEBIASBERT are more or less \"equalized\" between the genders. For race, we note that most of the word predictions from BERT in the context of African American 5 are of negative sentiment (angry, dangerous, evil), while those for Caucasian are comparably more pleasant (handsome, patient, helpful, friendly). Human Evaluation.",
            "score": 0.6622431044558023,
            "section_title": "DEBIASBERT",
            "char_start_offset": 19956,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 939
                },
                {
                    "start": 942,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1408
                },
                {
                    "start": 1409,
                    "end": 1429
                },
                {
                    "start": 1430,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1876
                },
                {
                    "start": 1877,
                    "end": 2122
                },
                {
                    "start": 2123,
                    "end": 2140
                }
            ],
            "ref_mentions": [
                {
                    "start": 310,
                    "end": 334,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 334,
                    "end": 352,
                    "matchedPaperCorpusId": "170078973"
                },
                {
                    "start": 352,
                    "end": 371,
                    "matchedPaperCorpusId": "207996257"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3115234375
        },
        {
            "corpus_id": "236034024",
            "title": "Intersectional Bias in Causal Language Models",
            "text": "Many approaches to mitigating or addressing bias have been proposed. Broadly, these can be distinguished as data-driven, model-driven or use-driven. Discussing gender bias, [33] suggest 'data augmentation', 'gender tagging' and 'bias fine tuning' as examples of datadriven approaches; removing subspaces (e.g. gender) or isolating category-specific information in word embeddings as examples of model-driven approaches; and constraining or censoring predictions as examples of use-driven approaches. \n\nModel design has increasingly sought to address bias using various data-driven approaches, either during training or via fine-tuning. Both GPT-2 and GPT-3 [31,36] use criteria such as Reddit popularity and weights to de-bias training data, while 'The Pile', developed in part to train GPT-NEO, has sought to incorporate a wider range of text [37]. As [37] also point out, careful curation of training data goes some way towards addressing at least more conspicuous bias. [51] suggests a mitigation approach based on (1) training a de-biased upstream model and then (2) fine-tuning a downstream model. \n\nModel-driven approaches treat the language model post-training. For models involving word embeddings, these embeddings can be inspected, and following a debiasing heuristic, modified directly. [10] for example conducts a component analysis to identify bias, and then manipulates embedding values, to either increase or decrease cosine values between category terms in accordance with the analysis. Such techniques appear more difficult to apply to transformer models with contextual embeddings. [35] for example examines both data augmentation and model neutralisation, and finds model neutralisation less effective. \n\nUse-driven approaches constrain or modify inputs. [48] shows for instance how inclusion of modifiers can produce radically different predictions for terms (like 'Muslim' in their study) showing strong negative bias in GPT-3. More generally, generalised pre-trained models make possible 'one-shot' or 'few-shot' training [36], padding inputs with a small number of prior examples, that can selectively condition predictions.",
            "score": 0.6584737480994318,
            "section_title": "Bias Mitigation",
            "char_start_offset": 6467,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 68
                },
                {
                    "start": 69,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 499
                },
                {
                    "start": 502,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 972
                },
                {
                    "start": 973,
                    "end": 1102
                },
                {
                    "start": 1105,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1599
                },
                {
                    "start": 1600,
                    "end": 1721
                },
                {
                    "start": 1724,
                    "end": 1773
                },
                {
                    "start": 1774,
                    "end": 1948
                },
                {
                    "start": 1949,
                    "end": 2147
                }
            ],
            "ref_mentions": [
                {
                    "start": 657,
                    "end": 661,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89697265625
        },
        {
            "corpus_id": "270411995",
            "title": "Applications of Pruning Methods in Natural Language Processing",
            "text": "The authors evaluate the trimmed models' performance on gender bias-related tasks and benchmarks.The results showcase the efficacy of the framework in reducing gender bias, as the pruned models exhibit improved fairness and reduced biases in their predictions.Furthermore, the authors propose improvements to existing debiasing techniques, highlighting the potential of movement pruning as a valuable tool in mitigating bias.By selectively removing attention heads associated with gender bias, the framework provides a targeted approach to address and rectify biases in pre-trained language models.One crucial finding highlighted in the paper is the trade-off between bias mitigation and model performance.The authors reveal that higher model performance often correlates with increased gender bias.This observation sheds light on the inherent challenges in achieving both fairness and high accuracy simultaneously.It encourages academics and practitioners to carefully explore and balance these variables while designing and using transformer-based language models.The importance of this study rests in its investigation of gender bias in pre-trained language models and the development of a methodology that effectively tackles this prejudice through movement trimming.By selectively pruning attention heads, the authors demonstrate the potential to mitigate gender bias and improve fairness in language models.The findings contribute to ongoing discussions on bias in natural language processing and highlight the importance of developing debiasing techniques that go beyond post-hoc interventions.",
            "score": 0.6542745405222207,
            "section_title": "6) MOVEMENT PRUNING",
            "char_start_offset": 43520,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 97,
                    "end": 260
                },
                {
                    "start": 260,
                    "end": 425
                },
                {
                    "start": 425,
                    "end": 598
                },
                {
                    "start": 598,
                    "end": 706
                },
                {
                    "start": 706,
                    "end": 799
                },
                {
                    "start": 799,
                    "end": 915
                },
                {
                    "start": 915,
                    "end": 1066
                },
                {
                    "start": 1066,
                    "end": 1271
                },
                {
                    "start": 1271,
                    "end": 1413
                },
                {
                    "start": 1413,
                    "end": 1601
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61669921875
        },
        {
            "corpus_id": "265281304",
            "title": "Bias A-head? Analyzing Bias in Transformer-Based Language Model Attention Heads",
            "text": "A growing body of work exploring AI fairness in general, and bias in NLP systems in particular, has highlighted stereotyping embedded in state-of-theart large language models -that is, such models represent some social groups disparately on demographic subsets, including gender, race, and age (Bender et al., 2021;Shah et al., 2020;Guo and Caliskan, 2021;Hutchinson et al., 2020;Kurita et al., 2019;May et al., 2019;Tan and Celis, 2019;Wolfe and Caliskan, 2021;Rozado, 2023) Caliskan et al., 2017), which examines the associations in contextualized word embeddings between concepts captured in the Implicit Association Test (Greenwald et al., 1998). While the SEAT score provides a quantifiable score to evaluate the stereotyping in PLMs, it is unknown how such stereotypical associations manifest in PLMs. \n\nTo mitigate stereotyping and representational harms in PLMs, many different debiasing strategies have been proposed, including data augmentation (Garimella et al., 2021), post-hoc operations (Cheng et al., 2021;Liang et al., 2020), fine-tuning the model (Kaneko and Bollegala, 2021;Lauscher et al., 2021), prompting techniques (Guo et al., 2022), and Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022). However, recent literature has noted several critical weaknesses of existing bias mitigation approaches, including the effectiveness of bias mitigation (Gonen and Goldberg, 2019;Meade et al., 2022), high training cost (Kaneko and Bollegala, 2021;Lauscher et al., 2021), poor generalizability (Garimella et al., 2021), and the inevitable degradation of language modeling capability (He et al., 2022;Meade et al., 2022). We believe that progress in addressing PLM bias has been inhibited by a lack of deeper understanding of how the bias manifests/behaves internally in the PLM. This paper aims to offer a perspective on this research gap.",
            "score": 0.6532555627067476,
            "section_title": "Stereotyping and Representational Harms in PLMs",
            "char_start_offset": 7260,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 807
                },
                {
                    "start": 810,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1810
                },
                {
                    "start": 1811,
                    "end": 1871
                }
            ],
            "ref_mentions": [
                {
                    "start": 294,
                    "end": 315,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 315,
                    "end": 333,
                    "matchedPaperCorpusId": "209461005"
                },
                {
                    "start": 333,
                    "end": 356,
                    "matchedPaperCorpusId": "219530686"
                },
                {
                    "start": 356,
                    "end": 380,
                    "matchedPaperCorpusId": "218487466"
                },
                {
                    "start": 380,
                    "end": 400,
                    "matchedPaperCorpusId": "190000105"
                },
                {
                    "start": 400,
                    "end": 417,
                    "matchedPaperCorpusId": "85518027"
                },
                {
                    "start": 417,
                    "end": 437,
                    "matchedPaperCorpusId": "202781363"
                },
                {
                    "start": 437,
                    "end": 462,
                    "matchedPaperCorpusId": "238259136"
                },
                {
                    "start": 462,
                    "end": 475,
                    "matchedPaperCorpusId": "257342280"
                },
                {
                    "start": 476,
                    "end": 498,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 625,
                    "end": 649,
                    "matchedPaperCorpusId": "7840819"
                },
                {
                    "start": 955,
                    "end": 979,
                    "matchedPaperCorpusId": "236477795"
                },
                {
                    "start": 1021,
                    "end": 1040,
                    "matchedPaperCorpusId": "207996257"
                },
                {
                    "start": 1064,
                    "end": 1092,
                    "matchedPaperCorpusId": "231698657"
                },
                {
                    "start": 1092,
                    "end": 1114,
                    "matchedPaperCorpusId": "237440429"
                },
                {
                    "start": 1137,
                    "end": 1155,
                    "matchedPaperCorpusId": "248780440"
                },
                {
                    "start": 1211,
                    "end": 1232,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1386,
                    "end": 1412,
                    "matchedPaperCorpusId": "73729169"
                },
                {
                    "start": 1412,
                    "end": 1431,
                    "matchedPaperCorpusId": "239015827"
                },
                {
                    "start": 1452,
                    "end": 1480,
                    "matchedPaperCorpusId": "231698657"
                },
                {
                    "start": 1480,
                    "end": 1502,
                    "matchedPaperCorpusId": "237440429"
                },
                {
                    "start": 1526,
                    "end": 1550,
                    "matchedPaperCorpusId": "236477795"
                },
                {
                    "start": 1615,
                    "end": 1632,
                    "matchedPaperCorpusId": "252907344"
                },
                {
                    "start": 1632,
                    "end": 1651,
                    "matchedPaperCorpusId": "239015827"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6474609375
        },
        {
            "corpus_id": "239015827",
            "title": "An Empirical Survey of the Effectiveness of Debiasing Techniques for Pre-trained Language Models",
            "text": "2020). Following the taxonomy described by Blodgett et al. (2020), our work studies the effectiveness of these techniques in mitigating representational biases from pre-trained language models. More specifically, we investigate mitigating gender, racial, and religious biases in three masked language models (BERT, ALBERT, and RoBERTa) and an autoregressive language model (GPT-2). We also explore how debiasing impacts a model's language modeling ability, as well as a model's performance on downstream natural language understanding (NLU) tasks.\n\nConcretely, our paper aims to answer the following research questions:\n\nQ1 Which technique is most effective in mitigating bias?\n\nQ2 Do these techniques worsen a model's language modeling ability?\n\nQ3 Do these techniques worsen a model's ability to perform downstream NLU tasks?\n\nTo answer Q1 ( \u00a74), we evaluate debiased models against three intrinsic bias benchmarks: the Sentence Encoder Association Test (SEAT; May et al. 2019), StereoSet (Nadeem et al., 2021), and Crowdsourced Stereotype Pairs (CrowS-Pairs;Nangia et al. 2020). Generally, we found Self-Debias to be the strongest bias mitigation technique. To answer Q2 ( \u00a75) and Q3 ( \u00a76), we evaluate debiased models against WikiText-2 (Merity et al., 2017) and the General Language Understanding Evaluation (GLUE; Wang and Cho 2019) benchmark. We found debiasing tends to worsen a model's language modeling ability. However, our results suggest that debiasing has little impact on a model's ability to perform downstream NLU tasks.",
            "score": 0.6528959832885175,
            "section_title": "Introduction",
            "char_start_offset": 1677,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8310546875
        },
        {
            "corpus_id": "239015827",
            "title": "An Empirical Survey of the Effectiveness of Debiasing Techniques for Pre-trained Language Models",
            "text": "Recent work has shown pre-trained language models capture social biases from the large amounts of text they are trained on. This has attracted attention to developing techniques that mitigate such biases. In this work, we perform an empirical survey of five recently proposed bias mitigation techniques: Counterfactual Data Augmentation (CDA), Dropout, Iterative Nullspace Projection, Self-Debias, and SentenceDebias. We quantify the effectiveness of each technique using three intrinsic bias benchmarks while also measuring the impact of these techniques on a model\u2019s language modeling ability, as well as its performance on downstream NLU tasks. We experimentally find that: (1) Self-Debias is the strongest debiasing technique, obtaining improved scores on all bias benchmarks; (2) Current debiasing techniques perform less consistently when mitigating non-gender biases; And (3) improvements on bias benchmarks such as StereoSet and CrowS-Pairs by using debiasing strategies are often accompanied by a decrease in language modeling ability, making it difficult to determine whether the bias mitigation was effective.",
            "score": 0.6504556481541399,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88671875
        },
        {
            "corpus_id": "239015827",
            "title": "An Empirical Survey of the Effectiveness of Debiasing Techniques for Pre-trained Language Models",
            "text": "Below, we discuss our findings for each research question we investigated in this work. We also discuss some of the limitations of our study.\n\nQ1: Which technique is most effective in mitigating bias? We found Self-Debias to be the strongest debiasing technique. Self-Debias not only consistently reduced gender bias, but also appeared effective in mitigating racial and religious bias across all four studied pre-trained language models. Critically, Self-Debias also had minimal impact on a model's language modeling ability. We believe the development of debiasing techniques which leverage a model's internal knowledge, like Self-Debias, to be a promising direction for future research. Importantly, we want to be able to use \"self-debiasing\" methods when a model is being used for downstream tasks.\n\nQ2: Do these techniques worsen a model's language modeling ability? In general, we found most debiasing techniques tend to worsen a model's language modeling ability. This worsening in language modeling raises questions about if some debiasing techniques were actually effective in mitigating bias. Furthermore, when you couple this with the already noisy nature of the bias bench-marks used in our work (Aribandi et al., 2021) it becomes even more difficult to determine which bias mitigation techniques are effective. Because of this, we believe reliably evaluating debiasing techniques requires a rigorous evaluation of how debiasing affects language modeling. Q3: Do these techniques worsen a model's ability to perform downstream NLU tasks? We found the debiasing techniques did not damage a model's ability to learn to perform downstream NLU tasks-a finding in alignment with other recent work (Barikeri et al., 2021). We conjecture this is because the fine-tuning step helps the debiased models to learn and retain essential information to solve a task.\n\nLimitations. We describe three of the main limitations of our work below. 1) We only investigate bias mitigation techniques for language models trained on English. However, some of the techniques studied in our work cannot easily be extended to other languages. For instance, many of our debiasing techniques cannot be used to mitigate gender bias in languages with grammatical gender (e.g., French)",
            "score": 0.6482659641372932,
            "section_title": "Discussion and Limitations",
            "char_start_offset": 22052,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8349609375
        },
        {
            "corpus_id": "276902427",
            "title": "Gender Encoding Patterns in Pretrained Language Model Representations",
            "text": "Bias mitigation in language models seeks to address both overt biases in model outputs and the subtler, systemic biases embedded within the model's internal representations. Effective techniques should suppress these encoded biases while maintaining model utility. In this section, we investigate the impact of various debiasing methods on compression values, used as a measure of encoded gender information, and evaluate their effectiveness across different experimental setups and models.",
            "score": 0.647677692621207,
            "section_title": "Impact of Bias Mitigation",
            "char_start_offset": 19298,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 490
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85791015625
        },
        {
            "corpus_id": "265607922",
            "title": "Assessing Political Inclination of Bangla Language Models",
            "text": "Bias identification and mitigation have been subjects of significant research interest (Liu et al., 2022\u037e Chen et al., 2023). Various forms of bias in language models have been extensively studied, from stereotypical to social and political biases (Liang et al., 2021). Researchers have developed various techniques to quantify, detect, and mitigate these biases, contributing to a growing body of literature in the field. Sun et al. (2022) examined societal biases within pre-trained language models, investigating six sensitive attributes, including race, gender, religion, appearance, age, and socioeconomic status. Their study also proposed potential mitigation strategies by developing debiasing adapters integrated into the layers of pre-trained language models. \n\nSimilarly, gender bias within LMs has garnered significant research attention. Recent studies have convincingly demonstrated the inherent gender bias present in these models (Kumar et al., 2020). Researchers have proposed various metrics to quantify and measure this bias (Bordia and Bowman, 2019). To address this issue, several debiasing strategies have been put forth. Qian et al. (2019) suggested a debiasing approach that modifies the loss function by incorporating terms aimed at equalizing probabilities associated with male and female words in the model's output. Vig et al. (2020) applied the theory of causal mediation analysis to develop a method for interpreting the components of a model that contribute to its bias. These research endeavors have laid a progressive foundation for examining gender biases in LMs. \n\nFurthermore, researchers have investigated various aspects of bias within LMs (Kaneko et al., 2022\u037e de Vassimon Manela et al., 2021\u037e Van Der Wal et al., 2022\u037e Joniak and Aizawa, 2022). Kirk et al. (2021) conducted research on generative models, particularly GPT-2 (Radford et al., 2019), and uncovered occupational biases. They observed that the job types suggested by the model tended to align with stereotypical attributes associated with people. Similarly, Venkit et al. (2022) identified biases against individuals with disabilities within language models.",
            "score": 0.6442744845777758,
            "section_title": "Related Works",
            "char_start_offset": 3297,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 768
                },
                {
                    "start": 771,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1342
                },
                {
                    "start": 1343,
                    "end": 1500
                },
                {
                    "start": 1501,
                    "end": 1596
                },
                {
                    "start": 1599,
                    "end": 1783
                },
                {
                    "start": 1784,
                    "end": 1921
                },
                {
                    "start": 1922,
                    "end": 2047
                },
                {
                    "start": 2048,
                    "end": 2159
                }
            ],
            "ref_mentions": [
                {
                    "start": 248,
                    "end": 268,
                    "matchedPaperCorpusId": "235623756"
                },
                {
                    "start": 423,
                    "end": 440,
                    "matchedPaperCorpusId": "252907549"
                },
                {
                    "start": 1043,
                    "end": 1068,
                    "matchedPaperCorpusId": "102352788"
                },
                {
                    "start": 1143,
                    "end": 1161,
                    "matchedPaperCorpusId": "170078973"
                },
                {
                    "start": 1343,
                    "end": 1360,
                    "matchedPaperCorpusId": "227275068"
                },
                {
                    "start": 1784,
                    "end": 1802,
                    "matchedPaperCorpusId": "236950797"
                },
                {
                    "start": 1863,
                    "end": 1885,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5390625
        },
        {
            "corpus_id": "256827819",
            "title": "Parameter-efficient Modularised Bias Mitigation via AdapterFusion",
            "text": "Large pre-trained language models (PLM) and their variations fine-tuned on downstream tasks encode societal biases and stereotypes. A large body of research shows the existence of these biases (Zhao et al., 2019;Sheng et al., 2019;Rekabsaz et al., 2021b), and discuss their potential harms (Blodgett et al., 2020) in various tasks such as occupation prediction from biographies (De-Arteaga et al., 2019), information retrieval (IR) (Rekabsaz et al., 2021a;Rekabsaz and Schedl, 2020), and machine translation (Stanovsky et al., 2019). A common approach to mitigating these biases is to adjust model parameters according to specific fairness criteria, achieved using optimization methods such as adversarial training (Elazar and Goldberg, 2018;Barrett et al., 2019;Han et al., 2021b;Rekabsaz et al., 2021a).\n\nThe resulting debiased models significantly diminish the effect of protected attributes (e. g. gender, race, etc.) on model's predictions. However, depending on the usage context, characteristics of the input or fairness-utility trade-off considerations, system designers or end-users might in practice still prefer to instead use the original (potentially biased) model for processing some requests. As an example, while a fairness-aware document retrieval model should tease out gender attributes when processing bias-sensitive queries (Krieg et al., 2022) (like how to become CEO?), other common queries (like earliest pregnancy symptoms) may specifically require gender information to obtain satisfactory retrieval results. Previous studies also show that one may need to occasionally accept slight performance degradation in exchange for realizing higher fairness (Zerveas et al., 2022;Biega et al., 2738Biega et al., 2018Rekabsaz et al., 2021a). We argue that ondemand control over the strength of bias mitigation is crucial for the broad adoption of bias mitigation models. Using existing approaches, this would require maintaining and deploying multiple large parallel models for every protected attribute, in practice resulting in overly complex and resourceheavy pipeline",
            "score": 0.6392196389394115,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 193,
                    "end": 212,
                    "matchedPaperCorpusId": "102352962"
                },
                {
                    "start": 212,
                    "end": 231,
                    "matchedPaperCorpusId": "202537041"
                },
                {
                    "start": 231,
                    "end": 254,
                    "matchedPaperCorpusId": "216867189"
                },
                {
                    "start": 290,
                    "end": 313,
                    "matchedPaperCorpusId": "218971825"
                },
                {
                    "start": 378,
                    "end": 403,
                    "matchedPaperCorpusId": "58006082"
                },
                {
                    "start": 432,
                    "end": 456,
                    "matchedPaperCorpusId": "233423485"
                },
                {
                    "start": 456,
                    "end": 482,
                    "matchedPaperCorpusId": "218470518"
                },
                {
                    "start": 508,
                    "end": 532,
                    "matchedPaperCorpusId": "173991101"
                },
                {
                    "start": 715,
                    "end": 742,
                    "matchedPaperCorpusId": "52056513"
                },
                {
                    "start": 742,
                    "end": 763,
                    "matchedPaperCorpusId": "202789469"
                },
                {
                    "start": 763,
                    "end": 781,
                    "matchedPaperCorpusId": "231698605"
                },
                {
                    "start": 781,
                    "end": 804,
                    "matchedPaperCorpusId": "233423485"
                },
                {
                    "start": 1716,
                    "end": 1734,
                    "matchedPaperCorpusId": "13697725"
                },
                {
                    "start": 1734,
                    "end": 1757,
                    "matchedPaperCorpusId": "233423485"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79443359375
        },
        {
            "corpus_id": "270878797",
            "title": "NLPGuard: A Framework for Mitigating the Use of Protected Attributes by NLP Classifiers",
            "text": "Bias in NLP decision-making has manifested itself in several ways, including dialogue generation [19], text classification [20], and machine translation [66].It usually arises from training data [22,67].For instance, pre-trained models and word embedding can inherit biases and stereotypes present in the large training corpora [7,9,25,58].When quantifying bias, existing works generally highlight disparities between demographic groups, with differences in performance or selection bias on protected attributes such as race, gender, religion, and sexual orientation [22,29,37,67].\n\nTo address biases in NLP, techniques can be developed that act at the three main stages of the NLP pipeline [38,75]: pre-processing (modifying training data), in-processing (imposing fairness constraints during model training), and post-processing (adjusting classifier predictions based on fairness metrics).Most existing works focus on the first two stages, exploiting data augmentation and modified model training techniques [4,6,20,52,58,64,77]. Furthermore, most of those studies focus on one protected category at a time.For example, Badjatiya et al. [6] proposed the identification of protected attributes, such as gender, by creating a manual list of words, measuring the skewed occurrence of words across classes or predicted class probability distribution of words.Park et al. [52] introduced gender swapping to equalize the number of male and female entities in the training data.Dixon et al. [20] proposed dataset augmentation strategies that generate new sentences using templates or replace protected attributes with generic tags, such as part-of-speech or named-entity tags.Zhang et al. [77] proposed mitigating biases in the training data by assuming a non-discrimination distribution and then reconstructing the distribution using instance weighting.Ravfogel et al. [58] proposed removing information from neural representations concerning gender or race for debiasing word embedding for NLP classification.",
            "score": 0.6372560143640364,
            "section_title": "Bias mitigation for NLP",
            "char_start_offset": 6613,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 158,
                    "end": 203
                },
                {
                    "start": 203,
                    "end": 340
                },
                {
                    "start": 340,
                    "end": 581
                },
                {
                    "start": 583,
                    "end": 892
                },
                {
                    "start": 892,
                    "end": 1110
                },
                {
                    "start": 1110,
                    "end": 1358
                },
                {
                    "start": 1358,
                    "end": 1474
                },
                {
                    "start": 1474,
                    "end": 1672
                },
                {
                    "start": 1672,
                    "end": 1850
                },
                {
                    "start": 1850,
                    "end": 2007
                }
            ],
            "ref_mentions": [
                {
                    "start": 123,
                    "end": 127,
                    "matchedPaperCorpusId": "54997157"
                },
                {
                    "start": 153,
                    "end": 157,
                    "matchedPaperCorpusId": "173991101"
                },
                {
                    "start": 331,
                    "end": 333,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 336,
                    "end": 339,
                    "matchedPaperCorpusId": "215786522"
                },
                {
                    "start": 571,
                    "end": 574,
                    "matchedPaperCorpusId": "237940142"
                },
                {
                    "start": 574,
                    "end": 577,
                    "matchedPaperCorpusId": "247222214"
                },
                {
                    "start": 691,
                    "end": 695,
                    "matchedPaperCorpusId": "232092280"
                },
                {
                    "start": 1014,
                    "end": 1016,
                    "matchedPaperCorpusId": "86446136"
                },
                {
                    "start": 1016,
                    "end": 1019,
                    "matchedPaperCorpusId": "54997157"
                },
                {
                    "start": 1019,
                    "end": 1022,
                    "matchedPaperCorpusId": "52070035"
                },
                {
                    "start": 1022,
                    "end": 1025,
                    "matchedPaperCorpusId": "215786522"
                },
                {
                    "start": 1025,
                    "end": 1028,
                    "matchedPaperCorpusId": "248572126"
                },
                {
                    "start": 1140,
                    "end": 1143,
                    "matchedPaperCorpusId": "86446136"
                },
                {
                    "start": 1370,
                    "end": 1374,
                    "matchedPaperCorpusId": "52070035"
                },
                {
                    "start": 1487,
                    "end": 1491,
                    "matchedPaperCorpusId": "54997157"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.865234375
        },
        {
            "corpus_id": "222140912",
            "title": "Astraea: Grammar-Based Fairness Testing",
            "text": "s fairness analysis provides a specification-based approach that allows for the flexibility of defining the intended bias to be tested in a manner that ensures that revealed fairness violations match the evaluated societal bias. We demonstrate this by testing several biases (e.g., race, gender or religion) and fairness criteria (i.e., individual or group fairness). Furthermore, our evaluation of ASTRAEA employs real-world deployed NLP systems, as well as an evaluation of generated inputs by human participants to ensure that found fairness violations are representative of actual language use in practice.\n\nSeveral papers have studied bias mitigation in NLP for a specific task or societal bias. Field et al. [33] and Sun et al. [69] provide critical surveys of gender and racial bias mitigation for NLP systems, respectively. Field et al. [33] surveyed 79 papers analyzing race-related bias in NLP systems, in order to understand how racial biases manifest at all stages of NLP model pipelines. The authors found that race has been ignored in many NLP tasks and the voices of historically marginalized people are nearly absent in NLP literature. The authors also recommend that researchers study the racial biases upheld by NLP system to bring inclusion and racial justice into NLP. Meanwhile, Sun et al. [69] surveyed papers studying gender bias detection and mitigation in NLP systems. The authors focused on how NLP systems may propagate or amplify gender bias. The paper finds that current gender debiasing methods in NLP are not sufficient to debias models end-to-end for many applications. The authors also found that most gender debiasing methods are task-specific, and have only been empirically verified in limited applications [80], [83]. Hence, the paper recommends the need for gender bias mitigation approaches to (automatically) patch and debias current NLP systems for general NLP tasks. Addressing some of the issues raised in these surveys, in this paper, we propose a general, taskagnostic and bias-agnostic fairness testing approach for NLP systems. Our approach allows to test and improve the fairness of NLP systems for several tasks (e.g., MLM, Coref and Sentiment analysis), and various societal biases (including gender and racial",
            "score": 0.6361657918815457,
            "section_title": "BACKGROUND",
            "char_start_offset": 21399,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 714,
                    "end": 718,
                    "matchedPaperCorpusId": "235593087"
                },
                {
                    "start": 734,
                    "end": 738,
                    "matchedPaperCorpusId": "195316733"
                },
                {
                    "start": 845,
                    "end": 849,
                    "matchedPaperCorpusId": "235593087"
                },
                {
                    "start": 1311,
                    "end": 1315,
                    "matchedPaperCorpusId": "195316733"
                },
                {
                    "start": 1743,
                    "end": 1747,
                    "matchedPaperCorpusId": "9424845"
                },
                {
                    "start": 1749,
                    "end": 1753,
                    "matchedPaperCorpusId": "1389483"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.658203125
        },
        {
            "corpus_id": "272599907",
            "title": "Securing Large Language Models: Addressing Bias, Misinformation, and Prompt Attacks",
            "text": "Bias mitigation can be achieved during four stages: pre-processing, in-training, intra-processing, and post-processing. Each stage handles bias at different points within a model's lifecycle to minimize discrimination in language models. \n\nAt the pre-processing stage, data augmentation, such as Counterfactual Data Augmentation (CDA), balances datasets by substituting attributes related to gender, race, or other protected groups. For example, if male programmers are over-represented in a dataset, CDA can create corresponding examples with female programmers. CDA approach was further improved by Counterfactual Data Substitution (CDS), which randomly replaces attributes to mitigate bias [134]. Prompt tuning encourages neutral or less stereotypical outputs by adjusting input prompts. Hard prompts use static templates, while soft prompts [135] generate embeddings dynamically during interactions with the model. \n\nBias for models in-training is addressed by modifying the learning process. Iterative Null Space Projection (INLP) removes bias by projecting targeted attributes into a space where they do not influence the model's outputs [136]. \n\nCausal Regularization ensures that models rely on meaningful, causal relationships rather than biased correlations in the data [137]. Auxiliary modules, such as the Adapter-based Debiasing (ADELE) uses additional modules to address bias without retraining the entire model [138]. And, GEnder Equality Prompt (GEEP), has been proposed to help overcome catastrophic forgetting improve gender fairness by freezing the pre-trained model and let the model learn gender-related prompts with gender-neutral data [139]. \n\nDuring intra-processing, models are tweaked at the inference stage without retraining. Model editing enables targeted updates to model behavior, ensuring that biases in specific areas are corrected without affecting overall model performance [140,141]. Decoding modification like DEXPERTS directly affects text generation by adjusting token probabilities. DEXPERTS uses two models, one to promote non-toxic text and another to discourage harmful content, to improving output fairness [142]. \n\nPost-processing methods focus on modifying the model's outputs.",
            "score": 0.6319894525142066,
            "section_title": "Bias Mitigation Strategies",
            "char_start_offset": 15183,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 237
                },
                {
                    "start": 240,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 699
                },
                {
                    "start": 700,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 918
                },
                {
                    "start": 921,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1150
                },
                {
                    "start": 1153,
                    "end": 1286
                },
                {
                    "start": 1287,
                    "end": 1432
                },
                {
                    "start": 1433,
                    "end": 1664
                },
                {
                    "start": 1667,
                    "end": 1753
                },
                {
                    "start": 1754,
                    "end": 1919
                },
                {
                    "start": 1920,
                    "end": 2022
                },
                {
                    "start": 2023,
                    "end": 2157
                },
                {
                    "start": 2160,
                    "end": 2223
                }
            ],
            "ref_mentions": [
                {
                    "start": 1909,
                    "end": 1914,
                    "matchedPaperCorpusId": "249642147"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.955078125
        },
        {
            "corpus_id": "225068149",
            "title": "Efficiently Mitigating Classification Bias via Transfer Learning",
            "text": "The practice of fine-tuning pretrained language models (PTLMs), such as BERT (Devlin et al., 2019), has efficiently improved prediction performance in a wide range of natural language processing (NLP) tasks. However, fine-tuned models may exhibit biases against protected groups (e.g., gender and ethnic minorities), as models may learn to associate certain features with positive or negative labels (Dixon et al., 2018), or propagate bias encoded in pre-trained models to downstream classifiers (Caliskan et al., 2017;Bolukbasi et al., 2016). Among many examples, Kurita et al. (2019) demonstrates that models trained using BERT embeddings for pronoun resolution are gender-biased, performing more poorly on resolving female pronouns than male pronouns even in a balanced dataset. Additionally, Kennedy et al. (2020) shows that hate speech classifiers fine-tuned from BERT spuriously correlate certain protected groups with hateful language, resulting in false positive model predictions when certain group identifiers (e.g., \"muslim\", \"black\") are present-e.g., misclassifying non-hate speech text such as \"I am a Muslim\" as hate speech. \n\nThe majority of methods combating bias in NLP tasks try to mitigate bias during fine-tuning PTLMs for a specific downstream task (dataset) (Park et al., 2018;Zhang et al., 2018;Beutel et al., 2017). For example, data augmentation approaches achieve bias mitigation by preventing models from capturing spurious patterns in the original dataset (Dixon et al., 2018;Zhao et al., 2018;Park et al., 2018); adversarial approaches learn to generate debiased data representations that are exclusive to the downstream model (Madras et al., 2018;Zhang et al., 2018). A critical issue with downstream bias mitigation techniques is their inefficiency.",
            "score": 0.6300758908653004,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 1139
                },
                {
                    "start": 1142,
                    "end": 1340
                },
                {
                    "start": 1341,
                    "end": 1698
                },
                {
                    "start": 1699,
                    "end": 1781
                }
            ],
            "ref_mentions": [
                {
                    "start": 77,
                    "end": 98,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 400,
                    "end": 420,
                    "matchedPaperCorpusId": "54997157"
                },
                {
                    "start": 496,
                    "end": 519,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 1281,
                    "end": 1300,
                    "matchedPaperCorpusId": "52070035"
                },
                {
                    "start": 1300,
                    "end": 1319,
                    "matchedPaperCorpusId": "9424845"
                },
                {
                    "start": 1485,
                    "end": 1505,
                    "matchedPaperCorpusId": "54997157"
                },
                {
                    "start": 1523,
                    "end": 1541,
                    "matchedPaperCorpusId": "52070035"
                },
                {
                    "start": 1657,
                    "end": 1678,
                    "matchedPaperCorpusId": "3419504"
                },
                {
                    "start": 1678,
                    "end": 1697,
                    "matchedPaperCorpusId": "9424845"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.857421875
        },
        {
            "corpus_id": "52070035",
            "title": "Reducing Gender Bias in Abusive Language Detection",
            "text": "We discussed model biases, especially toward gender identity terms, in abusive language detection. We found out that pre-trained word embeddings, model architecture, and different datasets all can have influence. Also, we found our proposed methods can reduce gender biases up to 90-98%, improving the robustness of the models. \n\nAs shown in Section 4.3, some classification performance drop happens when mitigation methods. We believe that a meaningful extension of our work can be developing bias mitigation methods that maintain (or even increase) the classification performance and reduce the bias at the same time. Some previous works (Beutel et al.;Zhang et al., 2018) employ adversarial training methods to make the classifiers unbiased toward certain variables. However, those works do not deal with natural language where features like gender and race are latent variables inside the language. Although those approaches are not directly comparable to our methods, it would be interesting to explore adversarial training to tackle this problem in the future. \n\nAlthough our work is preliminary, we hope that our work can further develop the discussion of evaluating NLP systems in different directions, not merely focusing on performance metrics like accuracy or AUC. The idea of improving models by measuring and correcting gender bias is still unfamiliar but we argue that they can be crucial in building systems that are not only ethical but also practical. Although this work focuses on gender terms, the methods we proposed can easily be extended to other identity problems like racial and to different tasks like sentiment analysis by following similar steps, and we hope to work on this in the future.",
            "score": 0.6289418052710216,
            "section_title": "Conclusion & Future Work",
            "char_start_offset": 13841,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 327
                },
                {
                    "start": 330,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 1066
                },
                {
                    "start": 1069,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1716
                }
            ],
            "ref_mentions": [
                {
                    "start": 655,
                    "end": 674,
                    "matchedPaperCorpusId": "9424845"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85107421875
        },
        {
            "corpus_id": "270370977",
            "title": "Deconstructing The Ethics of Large Language Models from Long-standing Issues to New-emerging Dilemmas",
            "text": "The third method focuses on creating new examples adhering to specific characteristics, like collecting high-quality examples to steer the model towards desired output [255; 141], and generating word lists associated with social groups [93].The fourth method performs instruction tuning by adding textual instructions [209], static tokens [180], or trained prefixes [155; 174] to reduce bias in the data.There is also one line of work involving altering contextualized embeddings to remove bias [236; 123].In-training Mitigation.These mitigation techniques focus on modifying the training procedure to reduce bias.The first method of this category focuses on altering the model's structure (i.e., integrating debiasing adapter modules [116]), and using demographic-specific encoder [97].The second method focuses on disrupting the association between social groups and stereotypical words.This is typically achieved by modifying the loss function applied on various model layers like the embedding layer [171; 219], attention layers [76; 13], and token generation stage [229; 108].Additionally, new training paradigms are proposed, such as contrastive learning [215; 159], adversarial learning [96; 238], and reinforcement learning [172; 18].The last method focuses on efficient fine-tuning procedures that freeze most pre-trained model parameters, and only update those potentially related to bias [317; 286; 280; 288].Intra-processing Mitigation.These approaches modify a trained model's behavior without additional training to generate debiased predictions during inference.There are mainly four types of methods.The first method adds restrictions during token search decoding to prevent biased outputs [245; 194].The second method adjusts token distributions to enhance output diversity or sample less biased outputs [58; 95].The third method redistributes the model's attention to less stereotypical aspects [321].The last method implements standalone networks with original models for specific debiasing tasks, such as reducing gender or racial biases [100].\n\nPost-processing Mitigation.The techniques address bias in generated outputs, especially relevant for black-box models with inaccessible training data or internal processes.",
            "score": 0.6228053544611492,
            "section_title": "Mitigation Strategy",
            "char_start_offset": 32746,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 241
                },
                {
                    "start": 241,
                    "end": 404
                },
                {
                    "start": 404,
                    "end": 506
                },
                {
                    "start": 506,
                    "end": 529
                },
                {
                    "start": 529,
                    "end": 614
                },
                {
                    "start": 614,
                    "end": 787
                },
                {
                    "start": 787,
                    "end": 889
                },
                {
                    "start": 889,
                    "end": 1081
                },
                {
                    "start": 1081,
                    "end": 1242
                },
                {
                    "start": 1242,
                    "end": 1420
                },
                {
                    "start": 1420,
                    "end": 1448
                },
                {
                    "start": 1448,
                    "end": 1577
                },
                {
                    "start": 1577,
                    "end": 1616
                },
                {
                    "start": 1616,
                    "end": 1717
                },
                {
                    "start": 1717,
                    "end": 1830
                },
                {
                    "start": 1830,
                    "end": 1919
                },
                {
                    "start": 1919,
                    "end": 2064
                },
                {
                    "start": 2066,
                    "end": 2093
                },
                {
                    "start": 2093,
                    "end": 2238
                }
            ],
            "ref_mentions": [
                {
                    "start": 236,
                    "end": 240,
                    "matchedPaperCorpusId": "247619104"
                },
                {
                    "start": 318,
                    "end": 323,
                    "matchedPaperCorpusId": "256616034"
                },
                {
                    "start": 735,
                    "end": 740,
                    "matchedPaperCorpusId": "59599816"
                },
                {
                    "start": 782,
                    "end": 786,
                    "matchedPaperCorpusId": "247694107"
                },
                {
                    "start": 2058,
                    "end": 2063,
                    "matchedPaperCorpusId": "258461365"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90283203125
        },
        {
            "corpus_id": "236477795",
            "title": "He is very intelligent, she is very beautiful? On Mitigating Social Biases in Language Modelling and Generation",
            "text": "Sheng et al. (2020) used adversarial triggers (Wallace et al., 2019) for controllable biases in language generation; however, this method does not debias the whole distribution but only obtains non-biased continuations of given prompts. In this work, we introduce gender and racial bias mitigation objectives by further pre-training BERT for language modelling, and in the language decoding training for summarization, and observe bias mitigation in the resulting text and representations, while preserving the quality of generated text.",
            "score": 0.6203382020935468,
            "section_title": "Related Work",
            "char_start_offset": 6468,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 537
                }
            ],
            "ref_mentions": [
                {
                    "start": 46,
                    "end": 68,
                    "matchedPaperCorpusId": "201698258"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6494140625
        },
        {
            "corpus_id": "271769656",
            "title": "A Parameter-Efficient Multi-Objective Approach to Mitigate Stereotypical Bias in Language Models",
            "text": "We evaluate our approach's performance of mitigating stereotypical bias in a GPT-2 small model on multiple benchmarks and compare its performance to various existing debiasing methods. \n\nBenchmark methods Benchmark methods we consider fall into the following categories depending on which stages they are applied to: \n\n\u2022 Pre-training: CDA (Zhao et al., 2019;Zmigrod et al., 2019;Lu et al., 2020) is a commonly used data augmentation method that augments the original biased dataset with synthetic gender-swapped sentences for fairer model pre-training. Dropout dissociates attributes and targets by increasing dropout rate in model pre-training (Webster et al., 2020). \n\n\u2022 Fine-tuning: Here we extend the concept of fine-tuning to include both full fine-tuning and parameter-efficient fine-tuning. Context-Debias (Kaneko and Bollegala, 2021) is a projection-based full fine-tuning method that encourages models to encode attribute and target words orthogonally to each other. Controllable-Bias (Sheng et al., 2020) mitigates bias by learning a discrete prompt that reduces negative regards for both genders. \n\n\u2022 Post-hoc: Iterative null-space projection (INLP) (Ravfogel et al., 2020) trains a set of linear classifiers to predict genders from embeddings and then projects embeddings to the null-space of learned classifiers. Self-Debias (Schick et al., 2021) adjusts next token probabilities at each step according to model's prediction to what extent the next token is biased. \n\nAs baselines, we also report the performance of vanilla GPT-2 and GPT-2 with randomly initialized prefix. Evaluation Metrics We adopt various different metrics to comprehensively evaluate the performance of our approach and benchmark methods.",
            "score": 0.6200989658614015,
            "section_title": "Experiments",
            "char_start_offset": 16798,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 187,
                    "end": 316
                },
                {
                    "start": 319,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 668
                },
                {
                    "start": 671,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1107
                },
                {
                    "start": 1110,
                    "end": 1325
                },
                {
                    "start": 1326,
                    "end": 1478
                },
                {
                    "start": 1481,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1723
                }
            ],
            "ref_mentions": [
                {
                    "start": 339,
                    "end": 358,
                    "matchedPaperCorpusId": "102352962"
                },
                {
                    "start": 358,
                    "end": 379,
                    "matchedPaperCorpusId": "184486914"
                },
                {
                    "start": 813,
                    "end": 840,
                    "matchedPaperCorpusId": "231698657"
                },
                {
                    "start": 994,
                    "end": 1014,
                    "matchedPaperCorpusId": "218470535"
                },
                {
                    "start": 1161,
                    "end": 1184,
                    "matchedPaperCorpusId": "215786522"
                },
                {
                    "start": 1338,
                    "end": 1359,
                    "matchedPaperCorpusId": "232075876"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8642578125
        },
        {
            "corpus_id": "268201625",
            "title": "AXOLOTL: Fairness through Assisted Self-Debiasing of Large Language Model Outputs",
            "text": "Pre-trained Large Language Models (LLMs) have revolutionized natural language processing, offering unparalleled capabilities in understanding, generating, and translating text (Zhu et al., 2023;Zhang et al., 2020). Despite their advancements, these models are not immune to inheriting and perpetuating biases present in their training data (Maudslay et al., 2019a). Often the uncurated datasets that these models are trained on reflect historical, societal, and cultural prejudices. Biases in LLMs can manifest in various forms such as gender, race, religion, profession, etc stereotypes, leading to unfair or discriminatory outcomes in applications ranging from automated hiring systems to conversational AI (Zhang et al., 2020). Studies such as (Bolukbasi et al., 2016b) and (Bender et al., 2021) highlight the critical nature of this These authors contributed equally to this work. \n\nproblem, demonstrating how biases can skew LLM outputs in ways that reinforce harmful stereotypes and marginalize already disadvantaged groups. \n\nResearchers have explored a multitude of strategies to identify and mitigate bias. These efforts encompass a broad spectrum of approaches, including enhancing fairness through modifications in sentence and word representations and embeddings (May et al., 2019;Caliskan et al., 2017b;Ravfogel et al., 2020), adjusting the underlying distribution of tokens (Guo et al., 2022), and refining datasets alongside model pre-training (Garimella et al., 2021;Maudslay et al., 2019a,b). While such interventions are crucial, they are not without their challenges. Specifically, the processes of pre-training or retraining LLMs entail significant computational resources and financial costs. Moreover, certain debiasing techniques may compromise the LLMs' overall performance. Another notable issue is the reliance on access to the models' internal configurations, a requirement that limits the applicability of these methods to open-source models and excludes the potential benefits of utilizing sophisticated, closed-source models. These factors underscore the need for innovative debiasing methodologies that are both cost-effective and performance-preserving.",
            "score": 0.6187928688030183,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 884
                },
                {
                    "start": 887,
                    "end": 1030
                },
                {
                    "start": 1033,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1509
                },
                {
                    "start": 1510,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1713
                },
                {
                    "start": 1714,
                    "end": 1798
                },
                {
                    "start": 1799,
                    "end": 2055
                },
                {
                    "start": 2056,
                    "end": 2185
                }
            ],
            "ref_mentions": [
                {
                    "start": 194,
                    "end": 213,
                    "matchedPaperCorpusId": "207869708"
                },
                {
                    "start": 340,
                    "end": 364,
                    "matchedPaperCorpusId": "202541569"
                },
                {
                    "start": 709,
                    "end": 729,
                    "matchedPaperCorpusId": "207869708"
                },
                {
                    "start": 747,
                    "end": 772,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 777,
                    "end": 798,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 1293,
                    "end": 1316,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 1388,
                    "end": 1406,
                    "matchedPaperCorpusId": "248780440"
                },
                {
                    "start": 1459,
                    "end": 1483,
                    "matchedPaperCorpusId": "236477795"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8564453125
        },
        {
            "corpus_id": "259138857",
            "title": "Measuring Sentiment Bias in Machine Translation",
            "text": "One model, GPT-2 [33], showed bias against black and gay people for both 'respect' and 'occupation', but was biased against men in the former and against women in the latter category. \n\nA quantification, analysis, and mitigation of gender bias were also carried out for the contextualized word vectors of the ELMo [30] model [46]. The findings were fourfold: (i) fewer female entries were found in the training data, (ii) the gender information was encoded by the training embeddings in a systematic fashion, (iii) the gender information was unevenly encoded by ELMo, (iv) the bias in ELMo was inherited by a state-of-theart coreference system. Two methods were thus proposed to successfully mitigate the bias: data augmentation (i.e., swapping the genders in the available entries, adding the swapped entries to the data) and neutralization (i.e., generating gender-swapped data). \n\nBordia and Bowman [4] proposed a metric to measure gender bias in both a corpus and a text generated by a model trained on that corpus, followed by a proposal for a regularization loss term. It was effective up until a certain weight, after which the model became unstable. Comparing the results for three corpora, they found mixed results on the amplification of the bias and state that there was a \"perplexity-bias tradeoff\" seeing as a model without bias would predict male and female terms with an even probability. \n\nAnother approach to reduce gender stereotyping was proposed in [50], i.e., augmenting the dataset with counterfactual data (e.g., 'el ingeniero' would become *'el ingeniera') and implementing a Markov Random Field for the agreement on a morphosyntactic level. This approach mitigated gender bias without lowering the grammaticality of the sentences. \n\nFurthermore, Jia et al. [18] investigated the amplification of gender bias through the lens of distribution. They proposed a mitigation technique based on posterior regularization which \"almost remove[s] the bias\".",
            "score": 0.6173017833700911,
            "section_title": "Related Work",
            "char_start_offset": 4166,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 186,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 881
                },
                {
                    "start": 884,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1403
                },
                {
                    "start": 1406,
                    "end": 1665
                },
                {
                    "start": 1666,
                    "end": 1755
                },
                {
                    "start": 1758,
                    "end": 1866
                },
                {
                    "start": 1867,
                    "end": 1972
                }
            ],
            "ref_mentions": [
                {
                    "start": 17,
                    "end": 21,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 314,
                    "end": 318,
                    "matchedPaperCorpusId": "3626819"
                },
                {
                    "start": 325,
                    "end": 329,
                    "matchedPaperCorpusId": "102352962"
                },
                {
                    "start": 902,
                    "end": 905,
                    "matchedPaperCorpusId": "102352788"
                },
                {
                    "start": 1469,
                    "end": 1473,
                    "matchedPaperCorpusId": "184486914"
                },
                {
                    "start": 1782,
                    "end": 1786,
                    "matchedPaperCorpusId": "218613632"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76171875
        },
        {
            "corpus_id": "276902427",
            "title": "Gender Encoding Patterns in Pretrained Language Model Representations",
            "text": "Early investigations into gender bias in language models unveiled that static embeddings not only encode but also amplify human-like biases within their representations (Islam et al., 2016;Bolukbasi et al., 2016). Subsequently, various studies have proposed methods to manipulate the embedding space or learning algorithms to mitigate bias in such models (Bolukbasi et al., 2016;Zhao et al., 2018b). However, as Gonen and Goldberg (2019) demonstrated, these techniques only provide superficial solutions, as biased information is not entirely removed from the model's embedding space. \n\nThe introduction of contextualized word embeddings, such as BERT (Devlin et al., 2019), posed new challenges, as manipulating representation space became more intricate compared to static embeddings. Contextualized language models have been shown to exhibit bias against demographic groups, including gender (Zhao et al., 2019;Silva et al., 2021). \n\nDespite these advancements, a comprehensive comparative analysis between various bias mitigation methods remained lacking. This gap was addressed by Meade et al. (2022), who conducted an empirical investigation into the effectiveness of multiple debiasing techniques. Through their experimentation, they selected diverse debiasing approaches, continued pretraining models with these techniques, and demonstrated their efficacy using prominent bias mitigation metrics. Additionally, they assessed the impact of these techniques on downstream performance, measuring model performance on the General Language Understanding Evaluation (GLUE; Wang et al., 2019) test set. As the results indicated that the debiasing techniques did not significantly compromise downstream performance, they hypothesized that these methods might not negatively affect model representations. However, they did not provide concrete evidence to support their claims. This highlights the need for further research and analysis to thoroughly understand the implications and effectiveness of different debiasing techniques in the context of language models. \n\nWhile earlier studies have explored the presence of gender bias in static and contextualized embeddings, they primarily focused on identifying and quantifying bias or testing basic mitigation strategies. Our study takes a different approach by investigating how biases are encoded within the internal representations of language models.",
            "score": 0.6171954207602968,
            "section_title": "Bias in Language Models",
            "char_start_offset": 3510,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 584
                },
                {
                    "start": 587,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 934
                },
                {
                    "start": 937,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1204
                },
                {
                    "start": 1205,
                    "end": 1404
                },
                {
                    "start": 1405,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 1803
                },
                {
                    "start": 1804,
                    "end": 1876
                },
                {
                    "start": 1877,
                    "end": 2064
                },
                {
                    "start": 2067,
                    "end": 2270
                },
                {
                    "start": 2271,
                    "end": 2403
                }
            ],
            "ref_mentions": [
                {
                    "start": 189,
                    "end": 212,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 355,
                    "end": 379,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 379,
                    "end": 398,
                    "matchedPaperCorpusId": "52161864"
                },
                {
                    "start": 412,
                    "end": 437,
                    "matchedPaperCorpusId": "73729169"
                },
                {
                    "start": 652,
                    "end": 673,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 895,
                    "end": 914,
                    "matchedPaperCorpusId": "102352962"
                },
                {
                    "start": 914,
                    "end": 933,
                    "matchedPaperCorpusId": "235097394"
                },
                {
                    "start": 1086,
                    "end": 1105,
                    "matchedPaperCorpusId": "239015827"
                },
                {
                    "start": 1575,
                    "end": 1593,
                    "matchedPaperCorpusId": "5034059"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83935546875
        },
        {
            "corpus_id": "222310622",
            "title": "Measuring and Reducing Gendered Correlations in Pre-trained Models",
            "text": "Other relevant work avoids intrinsic measurements altogether, focusing instead on how bias propagates to downstream tasks and the potential for real-world consequences. Racial and gender bias has been documented in resume-job matching software (De-Arteaga et al., 2019;Romanov et al., 2019), sentiment analysis (Kiritchenko and Mohammad, 2018), coreference resolution (Rudinger et al., 2018;Zhao et al., 2018a;Webster et al., 2018), image caption-ing (Zhao et al., 2017), and machine translation (Stanovsky et al., 2019;Prates et al., 2018). \n\nWe follow this line of work and sample three tasks for our evaluation framework, to give an overview of concerns for NLU. \n\nMitigation. A wide range of techniques have been proposed to mitigate gender bias in word representations. Bolukbasi et al. (2016) proposed using linear algebraic techniques to project away dimensions which encode gender, though Prost et al. (2019) found evidence that this method could potentially exacerbate bias for downstream tasks. Another popular technique uses adversarial losses in order to remove demographic information from learned representations (Louizos et al., 2015;Edwards and Storkey, 2015;Beutel et al., 2017;Zhang et al., 2018;Elazar and Goldberg, 2018;Madras et al., 2018). Evidence for the efficacy of this method, too, is mixed. In particular, Gonen and Goldberg (2019) found that gender information is still retrievable after having applied adversarial removal, while Barrett et al. (2019) presented a follow up study showing that such results only hold when models are deployed on the same data on which they were trained. Additional strategies include adjustments to the loss term and adjustments to the training data directly (Zhao et al., 2018b;Garg et al., 2019).",
            "score": 0.6158974225290678,
            "section_title": "Background and Related Work",
            "char_start_offset": 4754,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 541
                },
                {
                    "start": 544,
                    "end": 665
                },
                {
                    "start": 668,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1318
                },
                {
                    "start": 1319,
                    "end": 1614
                },
                {
                    "start": 1615,
                    "end": 1759
                }
            ],
            "ref_mentions": [
                {
                    "start": 244,
                    "end": 269,
                    "matchedPaperCorpusId": "58006082"
                },
                {
                    "start": 311,
                    "end": 343,
                    "matchedPaperCorpusId": "21670658"
                },
                {
                    "start": 368,
                    "end": 391,
                    "matchedPaperCorpusId": "13756572"
                },
                {
                    "start": 391,
                    "end": 410,
                    "matchedPaperCorpusId": "4952494"
                },
                {
                    "start": 451,
                    "end": 470,
                    "matchedPaperCorpusId": "1389483"
                },
                {
                    "start": 496,
                    "end": 520,
                    "matchedPaperCorpusId": "173991101"
                },
                {
                    "start": 520,
                    "end": 540,
                    "matchedPaperCorpusId": "52179151"
                },
                {
                    "start": 775,
                    "end": 798,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 1127,
                    "end": 1149,
                    "matchedPaperCorpusId": "3419504"
                },
                {
                    "start": 1195,
                    "end": 1214,
                    "matchedPaperCorpusId": "9424845"
                },
                {
                    "start": 1214,
                    "end": 1240,
                    "matchedPaperCorpusId": "52056513"
                },
                {
                    "start": 1240,
                    "end": 1260,
                    "matchedPaperCorpusId": "3419504"
                },
                {
                    "start": 1459,
                    "end": 1480,
                    "matchedPaperCorpusId": "202789469"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6474609375
        },
        {
            "corpus_id": "248780440",
            "title": "Auto-Debias: Debiasing Masked Language Models with Automated Biased Prompts",
            "text": "However, augmenting the pretraining corpus is costly and post-hoc debiasing does not mitigate the intrinsic biases encoded in PLMs. Therefore, recent work has proposed to fine-tune the PLMs to mitigate biases by designing different debiasing objectives (Kaneko and Bollegala, 2021;Garimella et al., 2021;Lauscher et al., 2021). They rely on external corpora, and the debiasing results based on these external corpora vary significantly (Garimella et al., 2021). Moreover, Garimella et al. (2021) find that existing debiasing methods are generally ineffective: first, they do not generalize well beyond gender bias; second, they tend to worsen a model's language modeling ability and its performance on NLU tasks. In this work, we propose a debiasing method that does not necessitate referring to any external corpus. Our debiased models are evaluated on both gender and racial biases, and we also evaluate their performance on NLU tasks. \n\n3 Auto-Debias: Probing and Debiasing using Prompts \n\nWe propose Auto-Debias, a debiasing technique for masked language models that does not entail referencing external corpora. Auto-Debias contains two stages: First, we automatically craft the biased prompts, such that the cloze-style completions have the highest disagreement in generating stereotype words with respect to demographic groups. Second, after we obtain the biased prompts, we debias the language model by a distribution alignment loss, with the motivation that the prompt completion results should be independent to the choice of different demographic-specific words.",
            "score": 0.6154411452963682,
            "section_title": "Related Works",
            "char_start_offset": 6615,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 937
                },
                {
                    "start": 940,
                    "end": 990
                },
                {
                    "start": 993,
                    "end": 1116
                },
                {
                    "start": 1117,
                    "end": 1334
                },
                {
                    "start": 1335,
                    "end": 1573
                }
            ],
            "ref_mentions": [
                {
                    "start": 253,
                    "end": 281,
                    "matchedPaperCorpusId": "231698657"
                },
                {
                    "start": 281,
                    "end": 304,
                    "matchedPaperCorpusId": "236477795"
                },
                {
                    "start": 436,
                    "end": 460,
                    "matchedPaperCorpusId": "236477795"
                },
                {
                    "start": 472,
                    "end": 495,
                    "matchedPaperCorpusId": "236477795"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72265625
        },
        {
            "corpus_id": "253734850",
            "title": "Deep Learning on a Healthy Data Diet: Finding Important Examples for Fairness",
            "text": "A biased model should not replace humans in certain tasks (e.g. resume filtering or loan eligibility prediction), regardless of how accurate it is if it achieves high accuracy on a test set that is not representative of the population. For example, if most test examples refer to men, this could hide the model's poor performance on examples that reference women. Hence, relying solely on metrics such as accuracy and F 1 might be misleading. \n\nTo mitigate gender bias in language models, several techniques have been proposed. These methods can be broadly classified into three main categories: data-based methods (Lu et al. 2020;Hall Maudslay et al. 2019;De-Arteaga et al. 2019); regularization-based methods (Gupta et al. 2021;Garg et al. 2019); and adversarial-based methods (Song et al. 2019;Madras et al. 2018;Jaiswal et al. 2020). Data-based methods, which are our focus in this work, change the training data to balance the bias through targeted data augmentation (Lu et al. 2020;Hall Maudslay et al. 2019), constructing counterfactual examples (Garg et al. 2019), or removing protected attributes from the input (De-Arteaga et al. 2019) to disallow models from learning any correlation between labels and gender words. Regularization-based methods add an auxiliary loss term to the objective function that reduces the amount of bias in the model. Adversarial-based methods use adversarial learning for bias mitigation. \n\nAlthough data-based methods are effective in theory, they are also demanding in different ways. First, some methods such as counterfactual data augmentation (CDA) (Lu et al. 2020) add counterfactual examples to the training data, which substantially increases the training time. Second, data balancing methods (Dixon et al. 2018) manually collect more examples for the under-represented groups, which requires human intervention. Third, the performance may degrade on the main downstream task (Zhang et al. 2020;Meade, Poole-Dayan, and Reddy 2022).",
            "score": 0.6091733580708276,
            "section_title": "Introduction",
            "char_start_offset": 1623,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 63
                },
                {
                    "start": 64,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 442
                },
                {
                    "start": 445,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 837
                },
                {
                    "start": 838,
                    "end": 1227
                },
                {
                    "start": 1228,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1427
                },
                {
                    "start": 1430,
                    "end": 1525
                },
                {
                    "start": 1526,
                    "end": 1708
                },
                {
                    "start": 1709,
                    "end": 1859
                },
                {
                    "start": 1860,
                    "end": 1978
                }
            ],
            "ref_mentions": [
                {
                    "start": 615,
                    "end": 631,
                    "matchedPaperCorpusId": "51888520"
                },
                {
                    "start": 631,
                    "end": 657,
                    "matchedPaperCorpusId": "202541569"
                },
                {
                    "start": 657,
                    "end": 680,
                    "matchedPaperCorpusId": "58006082"
                },
                {
                    "start": 711,
                    "end": 730,
                    "matchedPaperCorpusId": "202541569"
                },
                {
                    "start": 730,
                    "end": 747,
                    "matchedPaperCorpusId": "52880735"
                },
                {
                    "start": 797,
                    "end": 816,
                    "matchedPaperCorpusId": "102350941"
                },
                {
                    "start": 816,
                    "end": 836,
                    "matchedPaperCorpusId": "5147501"
                },
                {
                    "start": 972,
                    "end": 988,
                    "matchedPaperCorpusId": "51888520"
                },
                {
                    "start": 988,
                    "end": 1014,
                    "matchedPaperCorpusId": "202541569"
                },
                {
                    "start": 1053,
                    "end": 1071,
                    "matchedPaperCorpusId": "52880735"
                },
                {
                    "start": 1121,
                    "end": 1145,
                    "matchedPaperCorpusId": "58006082"
                },
                {
                    "start": 1593,
                    "end": 1609,
                    "matchedPaperCorpusId": "51888520"
                },
                {
                    "start": 1740,
                    "end": 1758,
                    "matchedPaperCorpusId": "54997157"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.900390625
        },
        {
            "corpus_id": "259095584",
            "title": "An Empirical Analysis of Parameter-Efficient Methods for Debiasing Pre-Trained Language Models",
            "text": "k et al., 2021), as well as evaluations of bias mitigation's impact on the language model's internal knowledge, are often insufficient.\n\nGiven these observations, we investigate three popular parameter-efficient methods, i.e., prefix tuning (Li and Liang, 2021), prompt tuning (Lester et al., 2021), and adapter tuning (Houlsby et al., 2019), in combination with counterfactual data augmentation (CDA, Zhao et al., 2018;Zmigrod et al., 2019;Webster et al., 2020) to debias pretrained language models. We conduct extensive experiments to study the parameter-efficient methods' performance on two types of language models (BERT (Devlin et al., 2019) for masked language models and GPT-2 (Radford et al., 2019) for autoregressive language models), three types of social biases (gender, race, and religion), and four types of performance measures (debiasing performance on CrowS-Pairs (Nangia et al., 2020) and Stere-oSet (Nadeem et al., 2021), language modeling per-formance on WikiText-2 (Merity et al., 2017) and StereoSet (Nadeem et al., 2021), fact retrieval performance on LAMA (Petroni et al., 2019), as well as downstream fine-tuning performance on Wino-Bias (Zhao et al., 2018)). We empirically compare to the performance of full fine-tuning and two posthoc debiasing methods (SentenceDebias  and SelfDebias (Schick et al., 2021)), aiming to comprehensively study the effectiveness of parameter-efficient methods for bias mitigation. 1 Our main findings are as follows:\n\n\u2022 The parameter-efficient methods are effective in mitigating gender bias. Within the three parameter-efficient methods, adapter tuning is consistently the most effective one for mitigating bias across different types of language models, while prompt tuning is more suitable for GPT-2 than BERT. Comparing to strong post-hoc debiasing methods, parameterefficient methods are better at preserving the language modeling ability, while still",
            "score": 0.6086253038920182,
            "section_title": "Introduction",
            "char_start_offset": 1738,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 249,
                    "end": 261,
                    "matchedPaperCorpusId": "230433941"
                },
                {
                    "start": 319,
                    "end": 341,
                    "matchedPaperCorpusId": "59599816"
                },
                {
                    "start": 402,
                    "end": 420,
                    "matchedPaperCorpusId": "4952494"
                },
                {
                    "start": 420,
                    "end": 441,
                    "matchedPaperCorpusId": "184486914"
                },
                {
                    "start": 626,
                    "end": 647,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 881,
                    "end": 902,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 918,
                    "end": 939,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 986,
                    "end": 1007,
                    "matchedPaperCorpusId": "16299141"
                },
                {
                    "start": 1022,
                    "end": 1043,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 1080,
                    "end": 1102,
                    "matchedPaperCorpusId": "202539551"
                },
                {
                    "start": 1163,
                    "end": 1182,
                    "matchedPaperCorpusId": "4952494"
                },
                {
                    "start": 1313,
                    "end": 1334,
                    "matchedPaperCorpusId": "232075876"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85498046875
        },
        {
            "corpus_id": "270380297",
            "title": "Improving Commonsense Bias Classification by Mitigating the Influence of Demographic Terms",
            "text": "Extensive research in Natural Language Processing (NLP) has focused on investigating biases in language models and developing strategies to understand and mitigate them.A study exploring the relationship between the size of language models and their biases was conducted, finding that larger models exhibit more nuanced biases and an increased potential for bias in specific categories [5].Researchers have also explored bias evaluation in language models, employing extrinsic evaluation methods and debiasing techniques to identify and address bias in real world applications and downstream tasks [7,8].Intrinsic evaluation methods, such as analyzing word or sentence embeddings, contribute to a deeper understanding of biases within language models [9,10,11].Additionally, It has been shown that systematic differences in bias measurement can be revealed through the unification of extrinsic evaluation methods, attributed to parameter choices [12].Moreover, these methods have been used to identify biased decisions from models in specific tasks [13].Recent studies have drawn significant attention to the bias caused by demographic terms in language models.Various techniques have been proposed to address and mitigate demographic bias, aiming to promote fairness, equality, and inclusivity.These techniques include debiasing models trained on human-annotated examples [10], fairness-aware neural language models [14], bias-mitigating transformer architectures [15], and debiasing frameworks to identify and correct demographic bias [16].Additionally, post-hoc debiasing techniques have been explored, where a debiasing step is added to sentence representations after initial training, prior to their utilization in downstream tasks [10,17].While preceding investigations focused on biases in language models linked to particular categories like origin and gender, our study adopts a more extensive methodology, assessing biases across a wider range of demographic terms.Building upon previous studies, our research proposes novel approaches to mitigate the impact of demographic terms and enhance the performance of the commonsense polarization classifier in knowledge models.This approach distinguishes itself from prior research by employing augmentation to substitute the predicate segment of a sentence with a synonym while retaining the original semantic content.Furthermore, to address bias stemming from demographic terms, our methodology involves substituting specific demographic terms with broader alternatives.",
            "score": 0.6077283508324098,
            "section_title": "II. RELATED WORK",
            "char_start_offset": 3511,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 169,
                    "end": 390
                },
                {
                    "start": 390,
                    "end": 604
                },
                {
                    "start": 604,
                    "end": 761
                },
                {
                    "start": 761,
                    "end": 951
                },
                {
                    "start": 951,
                    "end": 1054
                },
                {
                    "start": 1054,
                    "end": 1161
                },
                {
                    "start": 1161,
                    "end": 1295
                },
                {
                    "start": 1295,
                    "end": 1542
                },
                {
                    "start": 1542,
                    "end": 1745
                },
                {
                    "start": 1745,
                    "end": 1975
                },
                {
                    "start": 1975,
                    "end": 2181
                },
                {
                    "start": 2181,
                    "end": 2373
                },
                {
                    "start": 2373,
                    "end": 2526
                }
            ],
            "ref_mentions": [
                {
                    "start": 386,
                    "end": 389,
                    "matchedPaperCorpusId": "248649722"
                },
                {
                    "start": 598,
                    "end": 601,
                    "matchedPaperCorpusId": "58006082"
                },
                {
                    "start": 751,
                    "end": 754,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 754,
                    "end": 757,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 757,
                    "end": 760,
                    "matchedPaperCorpusId": "219530686"
                },
                {
                    "start": 946,
                    "end": 950,
                    "matchedPaperCorpusId": "235658325"
                },
                {
                    "start": 1049,
                    "end": 1053,
                    "matchedPaperCorpusId": "44090948"
                },
                {
                    "start": 1373,
                    "end": 1377,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 1417,
                    "end": 1421,
                    "matchedPaperCorpusId": "19117312"
                },
                {
                    "start": 1465,
                    "end": 1469,
                    "matchedPaperCorpusId": "198968250"
                },
                {
                    "start": 1737,
                    "end": 1741,
                    "matchedPaperCorpusId": "1704893"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86279296875
        },
        {
            "corpus_id": "256390197",
            "title": "How Far Can It Go? On Intrinsic Gender Bias Mitigation for Text Classification",
            "text": "techniques in language models improve fairness in downstream tasks?\n\nTo investigate the relationship between intrinsic and extrinsic gender bias, we train both BERT-large and ALBERT-large together with their intrinsicbias-mitigated versions on the Bias-in-bios and Jigsaw datasets using a 10-fold train-test scheme to correctly predict the profession of each profile. We evaluate extrinsic bias based on gender TPRD and gender CF scores described in \u00a7 2.4. We conduct the test under three settings: 1) Using the original version of the training data. 2) Using our attribute scrubbed version (see Section 2.3). 3) Using the attribute-swap augmented version of the data (see Section 2.3). These three settings of the dataset are important to understand how intrinsic bias affects downstream fairness under different data settings.\n\nIn Figure 5, we show the extrinsic bias score 12 for BERT-large and its debiased versions, and the corresponding downstream data type for the Biasin-Bios dataset -see Figure 11 and Figure 12 for all models and datasets.\n\nIntrinsic mitigation techniques do not show a significant improvement on TPRD. We observe that all the intrinsic bias mitigation interventions we consider do not significantly improve bias in TPRD on their own. There is only a slight improvement or worsening bias in some cases. For Contextdebias, we postulate this could be due to the limited list of words used to mitigate bias. If these limited words are not key terms in the downstream task or do not exist in the downstream text instances, the mitigation may not have a consequential effect on the downstream task.\n\nDownstream data processing significantly improves downstream fairness. Secondly, we realize that the downstream data has a lot more material effect on downstream fairness. The data processing techniques: attribute scrubbing and attribute swapping showed improvements in TPRD producing the best results.\n\nCombining intrinsic CDA and Downstream CDA produces best TPRD results. Thirdly, we realize that although the downstream data seems key to downstream fairness, a combination of intrinsic and downstream data intervention produces even better extrinsic fairness results.\n\nSent-debias significantly improves CF scores. Sent-debias is the only intrinsic technique that shows significant improvement in counter",
            "score": 0.6021718821247339,
            "section_title": "RQ3: Do intrinsic bias mitigation",
            "char_start_offset": 21932,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.492919921875
        },
        {
            "corpus_id": "273901320",
            "title": "Mitigate Extrinsic Social Bias in Pre-trained Language Models via Continuous Prompts Adjustment",
            "text": "Pre-trained language models (PLMs), such as BERT (Kenton and Toutanova, 2019), RoBERTa (Liu et al., 2019) and Albert (Lan, 2019), have been widely adopted in natural language understandings (NLU) due to their outstanding capacities of learning linguistic and factual information. However, recent studies (Meade et al., 2022) have demonstrated that PLMs often encode undesirable social biases and harmful stereotypes, which may lead to an unfair allocation of social resources. Prior methods of intrinsic bias mitigation (Guo et al., 2022;Lu et al., 2020) focus on removing demographic information in representations of PLMs. However, existing work (Zhou et al., 2023) has shown that even removing certain stereotypes in PLMs before they are applied into downstream tasks, unwanted bias will re-enter in the fine-tuned language models. Therefore, we are interested in removing extrinsic social bias, which improves fairness in a task-specific way. \n\nExisting extrinsic debiasing methods rely on manually curated word lists (e.g., \"he\" or \"she\" for gender). For example, Causal-debias (Zhou et al., 2023) replaces sensitive tokens in manually word lists to construct environments and removes spurious correlation via causal invariant learning. CLP (Garg et al., 2019) substitutes sensitive tokens in word list and uses counterfactual logit pairing to satisfy counterfactual token fairness. Gender-tuning (Ghanbarzadeh et al., 2023) generates gender-perturbed examples using manually word lists and integrates Masked Language Modeling (MLM) training objectives into the fine-tuning process. However, word lists are often limited by length and scope. For some protective attributes like race, it is difficult to design representative vocabularies intuitively due to the semantic constaints of words. Prior word lists (Manzini et al., 2019) often use name as proxy of race, which tend to occur less frequently in downstream tasks. Therefore, the diversity of sensitive groups covered by these word lists may be insufficient, resulting in the degradation performance of extrinsic bias mitigation.",
            "score": 0.6012391430760341,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 476
                },
                {
                    "start": 477,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 946
                },
                {
                    "start": 949,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1241
                },
                {
                    "start": 1242,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1587
                },
                {
                    "start": 1588,
                    "end": 1646
                },
                {
                    "start": 1647,
                    "end": 1795
                },
                {
                    "start": 1796,
                    "end": 1925
                },
                {
                    "start": 1926,
                    "end": 2090
                }
            ],
            "ref_mentions": [
                {
                    "start": 304,
                    "end": 324,
                    "matchedPaperCorpusId": "239015827"
                },
                {
                    "start": 520,
                    "end": 538,
                    "matchedPaperCorpusId": "248780440"
                },
                {
                    "start": 1246,
                    "end": 1265,
                    "matchedPaperCorpusId": "52880735"
                },
                {
                    "start": 1402,
                    "end": 1429,
                    "matchedPaperCorpusId": "259859044"
                },
                {
                    "start": 1813,
                    "end": 1835,
                    "matchedPaperCorpusId": "102350941"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71923828125
        },
        {
            "corpus_id": "247596835",
            "title": "A Prompt Array Keeps the Bias Away: Debiasing Vision-Language Models with Adversarial Learning",
            "text": "There have been multiple recent releases of opensource VL models (Radford et al., 2021;Mu et al., 2021;Bain et al., 2021), but research into bias measurement and mitigation has not kept pace, with only a few papers to date tackling these topics for VL (Agarwal et al., 2021;Zhao et al., 2021;Wang et al., 2021a). In this work, we therefore drew inspiration from the literature on dataset-and modellevel bias in CV and NLP (Mehrabi et al., 2021). \n\nBias in NLP. Large-scale language models are optimized to reflect statistical patterns of human language, which can be problematic if training datasets contain harmful or misrepresentative language (Weidinger et al., 2021). Prior work has documented gender bias (Bolukbasi et al., 2016;Zhao et al., 2019;Borchers et al., 2022), racial bias (Manzini et al., 2019;Garg et al., 2018) and their intersections (Guo and Caliskan, 2021;Kirk et al., 2021). WEAT, as described in Sec. 2.3 is one commonly-deployed bias metric for wordembeddings (Caliskan et al., 2017;Bolukbasi et al., 2016;Manzini et al., 2019). However as Gonen and Goldberg (2019) criticize, gender bias remains in the distances between \"gender neutralised\" words; thus we did not pursue embedding-level debiasing as a viable method in our work. Bias in computer vision. Similar to the body of NLP evidence, CV investigations have also shown evidence of gender bias (Zhao et al., 2017), racial bias (Wilson et al., 2019), and their intersection (Buolamwini and Gebru, 2018;Steed and Caliskan, 2021).",
            "score": 0.599352964385057,
            "section_title": "Related Works",
            "char_start_offset": 20751,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 445
                },
                {
                    "start": 448,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1508
                }
            ],
            "ref_mentions": [
                {
                    "start": 65,
                    "end": 87,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 274,
                    "end": 292,
                    "matchedPaperCorpusId": "235446842"
                },
                {
                    "start": 292,
                    "end": 311,
                    "matchedPaperCorpusId": "237490811"
                },
                {
                    "start": 752,
                    "end": 774,
                    "matchedPaperCorpusId": "248986638"
                },
                {
                    "start": 788,
                    "end": 810,
                    "matchedPaperCorpusId": "102350941"
                },
                {
                    "start": 810,
                    "end": 828,
                    "matchedPaperCorpusId": "4930886"
                },
                {
                    "start": 862,
                    "end": 877,
                    "matchedPaperCorpusId": "225094318"
                },
                {
                    "start": 877,
                    "end": 895,
                    "matchedPaperCorpusId": "236950797"
                },
                {
                    "start": 984,
                    "end": 1007,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 1030,
                    "end": 1051,
                    "matchedPaperCorpusId": "102350941"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.33251953125
        },
        {
            "corpus_id": "268512691",
            "title": "Detecting Bias in Large Language Models: Fine-tuned KcBERT",
            "text": "To alleviate the bias in LLMs, mitigation methods are categorized into preprocessing, in-training, and post-processing based on the pre-training and finetuned processes of LLMs [15].\n\nPre-processing mitigation methods aim to remove social bias from the initial input of the model, such as data or prompts.Counterfactual data augmentation (CDA) [27] transforms sentences by altering words or structures, using synonyms or antonyms, and inserting contextually appropriate words to generate new data.An extension of CDA involves adding unbiased data for biased social groups to balance the data distribution among different groups [28].While these approaches can mitigate bias by addressing various noise through data augmentation, they have limitations as the generated data may differ in meaning or quality from the original data, thereby not improving the generalization ability of LLMs.\n\nIn-training mitigation methods involve training an adversarial classifier, evaluating whether bias occurs during the training process by adding an adversarial loss function.Adversarial Learning [29] and Debiasing Regularization [30] use techniques like dropout and regularization terms to mitigate bias.These methods can dynamically adjust bias in real-time without modifying the training data.However, they face challenges in being computationally intensive and costly, making widespread usage difficult.\n\nPost-processing mitigation methods adjust the probability distribution during the decoding phase to select tokens with less bias, using methods such as adjusting, filtering, or inserting tokens [31].Another approach involves redistributing attention weights by considering the potential association between attention weights and encoded bias [32].These methods are easy to apply without altering the structure or learning, allowing parameter adjustments to focus on tokens with lower bias or reduce context to concentrate on tokens with higher bias.However, they may lead to imbalance in bias mitigation, as tokens with lower weights might be disproportionately filtered, resulting in an amplification of bias in the end.\n\nWe propose two effective bias mitigation methods tailored for Korean.",
            "score": 0.5951018098202758,
            "section_title": "Methods of mitigating social bias",
            "char_start_offset": 7687,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 184,
                    "end": 305
                },
                {
                    "start": 305,
                    "end": 497
                },
                {
                    "start": 497,
                    "end": 633
                },
                {
                    "start": 633,
                    "end": 887
                },
                {
                    "start": 889,
                    "end": 1062
                },
                {
                    "start": 1062,
                    "end": 1192
                },
                {
                    "start": 1192,
                    "end": 1283
                },
                {
                    "start": 1283,
                    "end": 1394
                },
                {
                    "start": 1396,
                    "end": 1595
                },
                {
                    "start": 1595,
                    "end": 1743
                },
                {
                    "start": 1743,
                    "end": 1945
                },
                {
                    "start": 1945,
                    "end": 2117
                },
                {
                    "start": 2119,
                    "end": 2188
                }
            ],
            "ref_mentions": [
                {
                    "start": 344,
                    "end": 348,
                    "matchedPaperCorpusId": "225080424"
                },
                {
                    "start": 628,
                    "end": 632,
                    "matchedPaperCorpusId": "54997157"
                },
                {
                    "start": 1083,
                    "end": 1087,
                    "matchedPaperCorpusId": "9424845"
                },
                {
                    "start": 1590,
                    "end": 1594,
                    "matchedPaperCorpusId": "253734850"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92822265625
        },
        {
            "corpus_id": "235097394",
            "title": "Towards a Comprehensive Understanding and Accurate Evaluation of Societal Biases in Pre-Trained Transformers",
            "text": "The ease of access to pre-trained transformers has enabled developers to leverage large-scale language models to build exciting applications for their users. While such pre-trained models offer convenient starting points for researchers and developers, there is little consideration for the societal biases captured within these model risking perpetuation of racial, gender, and other harmful biases when these models are deployed at scale. In this paper, we investigate gender and racial bias across ubiquitous pre-trained language models, including GPT-2, XLNet, BERT, RoBERTa, ALBERT and DistilBERT. We evaluate bias within pre-trained transformers using three metrics: WEAT, sequence likelihood, and pronoun ranking. We conclude with an experiment demonstrating the ineffectiveness of word-embedding techniques, such as WEAT, signaling the need for more robust bias testing in transformers.",
            "score": 0.5946584616197701,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80322265625
        },
        {
            "corpus_id": "257558858",
            "title": "Representation Bias in Data: A Survey on Identification and Resolution Techniques",
            "text": "Bias. Several methods have been proposed to mitigate representation bias in textual data. \n\nSome of these methods require the models to be retrained after the alterations while some do not and only manipulate the model to fix the outcomes. In the following we will introduce each of these methods and reiterate some of the adopted techniques: \n\nText Corpus Alteration. To debias the text corpora, two approaches have been proposed: Data Augmentation: The augmentation approach is to add modified copies of the existing data, or newly created synthetic data, to the corpora. While some works propose completely removing, masking, or replacing any indication of gender, race, etc. from the text corpora to eliminate representation bias, De Arteaga et al. [49] makes an interesting observation that even by removing the explicit indicators regarding gender, race, or socioeconomic status in the text corpora, although a slight reduction in representation bias would occur towards the minority group, a significant gap remains due to the imbalances in the available data between the minority and majority group. Similarly, Li et al. [119] make a closely related conclusion for the task of text generation where they investigate representation bias in the stories generated by GPT-3. They demonstrate how gender stereotypes occur in generated narratives, even in the absence of gender indicators or stereotype-related cues. They propose prompt design as a possible workaround for mitigating bias and steering GPT-3, however, they state that it is not a feasible solution for every situation. Zhao et al. [173] propose another approach to decrease the bias in text corpora by creating an identical but gender-swapped version of the original data set and training the model on the union of the original data set, the gender-swapped version and the named-entity anonymized version of the original data set. \n\nIn tasks such as machine translation, due to the domination of male entities in the available text corpora, the models tend to predict the entities more as male while the actual gender may not be clear. This specifically becomes problematic while translating into languages such as French where words are gender-specific and masking or removal of gender indicators is not an option.",
            "score": 0.59100510552398,
            "section_title": "Resolving Representation",
            "char_start_offset": 107232,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 5
                },
                {
                    "start": 6,
                    "end": 89
                },
                {
                    "start": 92,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 342
                },
                {
                    "start": 345,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 1107
                },
                {
                    "start": 1108,
                    "end": 1278
                },
                {
                    "start": 1279,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1898
                },
                {
                    "start": 1901,
                    "end": 2103
                },
                {
                    "start": 2104,
                    "end": 2283
                }
            ],
            "ref_mentions": [
                {
                    "start": 753,
                    "end": 757,
                    "matchedPaperCorpusId": "58006082"
                },
                {
                    "start": 1129,
                    "end": 1134,
                    "matchedPaperCorpusId": "235097208"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7705078125
        },
        {
            "corpus_id": "256390197",
            "title": "How Far Can It Go? On Intrinsic Gender Bias Mitigation for Text Classification",
            "text": "To mitigate gender bias in contextualized language models, different intrinsic mitigation strategies have been proposed, alongside many bias metrics. Considering that the end use of these language models is for downstream tasks like text classification, it is important to understand how these intrinsic bias mitigation strategies actually translate to fairness in downstream tasks and the extent of this.In this work, we design a probe to investigate the effects that some of the major intrinsic gender bias mitigation strategies have on downstream text classification tasks. We discover that instead of resolving gender bias, intrinsic mitigation techniques and metrics are able to hide it in such a way that significant gender information is retained in the embeddings. Furthermore, we show that each mitigation technique is able to hide the bias from some of the intrinsic bias measures but not all, and each intrinsic bias measure can be fooled by some mitigation techniques, but not all. We confirm experimentally, that none of the intrinsic mitigation techniques used without any other fairness intervention is able to consistently impact extrinsic bias. We recommend that intrinsic bias mitigation techniques should be combined with other fairness interventions for downstream tasks.",
            "score": 0.5909973861319904,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5966796875
        },
        {
            "corpus_id": "254926728",
            "title": "Causally Testing Gender Bias in LLMs: A Case Study on Occupational Bias",
            "text": "As the overarching goal of bias studies is mitigating biases, we discuss existing bias mitigation methods. A variety of methods, particularly those using fine-tuning-based objectives, have been proposed (Sheng et al., 2020;Abid et al., 2021;Liang et al., 2021). As language models become larger, such adaptations become computationally expensive to perform, which motivates zero-shot methods that mitigate bias without requiring further training. For LLMs, different prompting strategies have emerged as highly effective methods for improving their performance on a variety of tasks or altering their behavior without training (Brown et al., 2020;Reif et al., 2022;Wei et al., 2022;Kojima et al., 2022;Sordoni et al., 2023;Si et al., 2023;Oba et al., 2024). On the other hand, Zakizadeh et al. (2023) argues that most bias-mitigation also damages useful gender information, causing language models to make wrong gender predictions even when gender information is given in the prompt. \n\nWe believe debiasing via prompting is an efficient approach. Thus we conduct preliminary experiments on six debiasing instructions, all with similar meanings-do not make biased predictions-but ranging from general instructions such as \"Please do not think based on gender stereotypes\" to specific instructions like \"When generating a story, keep in mind that many women work in jobs typically associated with men and many men work in jobs typically associated with women\". We run small-scale experiments on ten task templates, 5 for explicit bias and 5 for implicit bias, with each debiasing instruction inserted before the task prompt. The six debiasing instructions are listed in Table 7. \n\nFrom the results reported in Appendix E, we observe that although all six instructions convey a similar message, the debiasing effects vary. For instance, instructions 3 and 6 push Gemma-2-9B to make a more gender-neutral prediction for implicit bias prompts, while the other instructions do not have such significant effects. Notably, the gap between male and female predictions for maledominated jobs remains large. An interesting direction is studying whether certain stereotypes are harder to mitigate than others.",
            "score": 0.5898321065460955,
            "section_title": "Bias Mitigation",
            "char_start_offset": 18636,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 983
                },
                {
                    "start": 986,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1622
                },
                {
                    "start": 1623,
                    "end": 1676
                },
                {
                    "start": 1679,
                    "end": 1819
                },
                {
                    "start": 1820,
                    "end": 2005
                },
                {
                    "start": 2006,
                    "end": 2096
                },
                {
                    "start": 2097,
                    "end": 2197
                }
            ],
            "ref_mentions": [
                {
                    "start": 203,
                    "end": 223,
                    "matchedPaperCorpusId": "218470535"
                },
                {
                    "start": 223,
                    "end": 241,
                    "matchedPaperCorpusId": "231603388"
                },
                {
                    "start": 241,
                    "end": 260,
                    "matchedPaperCorpusId": "235623756"
                },
                {
                    "start": 647,
                    "end": 665,
                    "matchedPaperCorpusId": "237263305"
                },
                {
                    "start": 665,
                    "end": 682,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 702,
                    "end": 723,
                    "matchedPaperCorpusId": "259224810"
                },
                {
                    "start": 739,
                    "end": 756,
                    "matchedPaperCorpusId": "261822519"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.361572265625
        },
        {
            "corpus_id": "269307293",
            "title": "Scaling Implicit Bias Analysis across Transformer-Based Language Models through Embedding Association Test and Prompt Engineering",
            "text": "While offering a valuable strategy for addressing a particular form of bias, the findings call for extended exploration into the broader application and impact of such debiasing methods. \n\nSengupta et al. [18] examined the effects of racial bias in data and algorithms on the effectiveness and fairness of AI systems. They highlighted that biases in language data and models can influence explainability, user experience, and societal biases. The research showed how AI systems, influenced by biased data, can produce unjustifiable discriminatory results and affect the system's credibility. Through a mix of methods, Sengupta evaluated the direct effects of racial bias in language on AI model performance. Controlled studies further investigated the impact of biased outputs on user engagement and decision making. The findings indicated that dependence on biased models diminishes the persuasiveness of users and adversely affects their choices, underscoring the importance of mitigating bias in AI systems. \n\nMayer et al. [6] looked into how prompt-based learning can be used in transformer models for tasks that need professional categorization. They compared prompt-based methods with zero-shot and few-shot categorization, as well as fine-tuning and human evaluation. Prompt-based learning was noted for providing estimates of reliability and pinpointing difficult responses. They suggested a mutual rating system between humans and machines to evaluate the complexity of responses as a cost-efficient alternative to extensive fine-tuning. However, crafting effective prompts poses its challenges, and the study's focus on English-language data limits its relevance across languages. This points to the necessity for the development of multilingual AI models. Additionally, the small size of the dataset and the absence of a bias examination were seen as limitations, indicating the need for broader and more diverse datasets and a thorough investigation into potential biases. Collectively, these studies bring to light the issues surrounding bias and fairness in AI, emphasizing that, unaddressed, these issues can detrimentally influence AI performance, decision-making processes, and societal perceptions. \n\nSun et al. [19] explored the presence of gender bias within NLP systems and identified various underlying biases such as lexical, semantic, syntactic, and pragmatic, which all contribute to the overall gender bias.",
            "score": 0.5885063056394574,
            "section_title": "Related Work",
            "char_start_offset": 14691,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 189,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 707
                },
                {
                    "start": 708,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 1010
                },
                {
                    "start": 1013,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1546
                },
                {
                    "start": 1547,
                    "end": 1690
                },
                {
                    "start": 1691,
                    "end": 1766
                },
                {
                    "start": 1767,
                    "end": 1984
                },
                {
                    "start": 1985,
                    "end": 2216
                },
                {
                    "start": 2219,
                    "end": 2433
                }
            ],
            "ref_mentions": [
                {
                    "start": 1026,
                    "end": 1029,
                    "matchedPaperCorpusId": "253832730"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75048828125
        },
        {
            "corpus_id": "253116931",
            "title": "A Robust Bias Mitigation Procedure Based on the Stereotype Content Model",
            "text": "It is well established that large language models (LLMs) such as BERT (Devlin et al., 2019), GPT2 (Radford et al., 2019) and related contextualised word embeddings such as ELMo (Peters et al., 2018) are biased against different demographic groups (Guo and Caliskan, 2021;Webster et al., 2020;Kaneko and Bollegala, 2021), in that they often reflect stereotypes in their output. For example, given the prompt \"naturally, the nurse is a\", these systems will typically output \"woman\" (Schick et al., 2021). Given the common practice of adapting pre-trained language models for a range of tasks through fine-tuning, upstream bias mitigation may prove to be the most efficient solution (Jin et al., 2021) (though cf. Steed et al. (2022). In this paper, we demonstrate the success of modifying an existing debiasing algorithm to be grounded in a psychological theory of stereotypes -the SCM (Cuddy et al., 2008), to efficiently reduce biases in LLMs across a range of identities. Our proposed debiasing pipeline has the benefit of minimising the time spent researching identity terms and associated stereotypes. Being a fine-tuning procedure, this also reduces the amount of computational resources needed compared to training an unbiased model from scratch. This renders our approach efficient and widely applicable. We demonstrate using BERT, but this same procedure could easily be adapted to other LLMs. \n\nWe adapt the fine-tuning procedure from Kaneko and Bollegala (2021). They reduce gender bias in a range of LLMs by fine-tuning using a data set of sentences containing (binary) gendered terms (like \"he, man\" or \"she, lady\") (which they call attributes) or stereotypes associated with different genders (\"assertive, secretary\") (which they call targets). The training objective is to remove associations with gender in the contextualised embeddings of the targets whilst maintaining these associations for the gendered attributes.",
            "score": 0.5870160724382276,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 972
                },
                {
                    "start": 973,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1400
                },
                {
                    "start": 1403,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1756
                },
                {
                    "start": 1757,
                    "end": 1932
                }
            ],
            "ref_mentions": [
                {
                    "start": 70,
                    "end": 91,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 177,
                    "end": 198,
                    "matchedPaperCorpusId": "3626819"
                },
                {
                    "start": 247,
                    "end": 271,
                    "matchedPaperCorpusId": "219530686"
                },
                {
                    "start": 292,
                    "end": 319,
                    "matchedPaperCorpusId": "231698657"
                },
                {
                    "start": 680,
                    "end": 697,
                    "matchedPaperCorpusId": "235097594"
                },
                {
                    "start": 711,
                    "end": 730,
                    "matchedPaperCorpusId": "248780439"
                },
                {
                    "start": 884,
                    "end": 904,
                    "matchedPaperCorpusId": "42003723"
                },
                {
                    "start": 1443,
                    "end": 1470,
                    "matchedPaperCorpusId": "231698657"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8056640625
        },
        {
            "corpus_id": "271745282",
            "title": "Decoding Biases: Automated Methods and LLM Judges for Gender Bias Detection in Language Models",
            "text": "In recent years, the proliferation of Large Language Models (LLMs) like GPT-4 has revolutionized natural language processing, enabling applications from automated text generation to complex language understanding. However, as these models integrate more deeply into technological solutions, their inherent biases have become a significant concern. These biases can lead to misrepresentation of individuals and groups, promote stereotypes, and suggest unfair allocation of resources, impacting people both psychologically and socio-economically. \n\nDespite a fair amount of research happening on AI Safety and fairness, there is still a lack of understanding of measuring societal biases in language models systematically. Biases in these models-such as gender, racial, ethnic, and ideological biases-stem from training data that reflect societal prejudices. Consequently, language models can perpetuate and even amplify these biases, leading to outputs that reinforce stereotypes and disadvantage certain groups (Bender et al., 2021). The implications are profound, affecting areas, like hiring, law enforcement, and daily AI interactions (Schwartz et al., 2022;Ferrara, 2023). Thus, assessing and mitigating bias in language models is essential for their ethical and fair use. \n\nWe define bias as the semantic variance in text generated by an LLM along with differential treatment towards stereotype identification, refusal, and correction of social group attributes when presented with prompts obtained by varying only the social group association. For example, in Figure 1, the LLM is presented with a prompt \"I think men are much better with managing finances\" and another obtained by only varying the gender (men to women). The image on the left shows the scenario where the LLM responds differently to these inputs. In one case it identifies the stereotype present in the prompt and rejects this with an explanation; in the other, it fails to identify any gender bias in the input and supports the gender-based claim instead. On the right shows an unbiased model that responds to both the prompts similarly, by identifying and rejecting the gender-stereotype in the input. \n\nBias mitigation techniques rely on bias assessment metrics to evaluate their efficacy. Bias evaluation metrics are broadly classified into embeddingbased, probability-based, and generated-text-based metrics (Gallegos et al., 2023).",
            "score": 0.5864685641882191,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 544
                },
                {
                    "start": 547,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1276
                },
                {
                    "start": 1279,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1727
                },
                {
                    "start": 1728,
                    "end": 1820
                },
                {
                    "start": 1821,
                    "end": 2030
                },
                {
                    "start": 2031,
                    "end": 2177
                },
                {
                    "start": 2180,
                    "end": 2266
                },
                {
                    "start": 2267,
                    "end": 2411
                }
            ],
            "ref_mentions": [
                {
                    "start": 1011,
                    "end": 1032,
                    "matchedPaperCorpusId": "121125604"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71484375
        },
        {
            "corpus_id": "269195406",
            "title": "AI Chatbots and Linguistic Injustice",
            "text": "It is widely known that recent language models have improved significantly (Devlin et al. 2018, Brown et al. 2020, Clark et al. 2020). Recent advancements in language modeling have embraced the approach of training large-scale models on extensive, unannotated corpora using self-supervised learning techniques. These methods involve predicting masked words and the next sentence in a sequence. (Devlin et al. 2018, He et al. 2020), wrong word detection (Clark et al. 2020), and left-to-right language generation (Brown et al. 2020, Raffel et al. 2020). \n\nThe recent natural language processing models are trained by assessing the similarity vocabularies and sentences in text. Since the optimization objective focuses on maximizing the likelihood of the training data, the trained model enhances the coherence of words and sentences frequently found together in the training corpus. However, being created by humans, the training data sets can contain significant amounts of social bias and stereotypes, encompassing factors such as gender, race, and religion (Kiritchenko & Mohammad 2018, Nadeem et al. 2021, Stanczak & Augenstein 2021). \n\nSome studies have demonstrated that pretrained language models are capable of acquiring various forms of stereotypical and baised reasoning. For example, Kiritchenko & Mohammad (2018) examined how language models perform in sentiment analysis across various social groups, measuring differences in their behaviours. Recent studies by Nangia et al. (2020) and Nadeem et al. (2021) investigated stereotypical reasoning related to race, gender, profession, and religion using masked language models and sentence encoders. \n\nRecent research examined strategies to reduce the social biases inherent in language models, aiming to enhance their reliability. These studies have investigated techniques to mitigate biases during the learning and prediction phases of language models. Typical methods for mitigating bias involve the use of counterfactual data augmentation (Zmigrod et al. 2019, Dinan et al. 2020, Webster et al. 2020, Barikeri et al. 2021), dropout regularization (Webster et al. 2020), and selfdebias (Schick et al. 2021). MIT researchers have trained language models that can realize logic to avoid harmful stereotypes such as gender and racial biases.",
            "score": 0.5855307387323557,
            "section_title": "Language Modeling Bias",
            "char_start_offset": 14133,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 552
                },
                {
                    "start": 555,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 1138
                },
                {
                    "start": 1141,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1456
                },
                {
                    "start": 1457,
                    "end": 1659
                },
                {
                    "start": 1662,
                    "end": 1791
                },
                {
                    "start": 1792,
                    "end": 1915
                },
                {
                    "start": 1916,
                    "end": 2171
                },
                {
                    "start": 2172,
                    "end": 2302
                }
            ],
            "ref_mentions": [
                {
                    "start": 94,
                    "end": 113,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 113,
                    "end": 133,
                    "matchedPaperCorpusId": "208229926"
                },
                {
                    "start": 453,
                    "end": 472,
                    "matchedPaperCorpusId": "208229926"
                },
                {
                    "start": 512,
                    "end": 530,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 530,
                    "end": 551,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1060,
                    "end": 1088,
                    "matchedPaperCorpusId": "21670658"
                },
                {
                    "start": 1088,
                    "end": 1108,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 1295,
                    "end": 1324,
                    "matchedPaperCorpusId": "21670658"
                },
                {
                    "start": 1475,
                    "end": 1495,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 1500,
                    "end": 1520,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 2004,
                    "end": 2024,
                    "matchedPaperCorpusId": "184486914"
                },
                {
                    "start": 2024,
                    "end": 2043,
                    "matchedPaperCorpusId": "207852875"
                },
                {
                    "start": 2064,
                    "end": 2087,
                    "matchedPaperCorpusId": "235358955"
                },
                {
                    "start": 2150,
                    "end": 2170,
                    "matchedPaperCorpusId": "232075876"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76708984375
        },
        {
            "corpus_id": "261530629",
            "title": "Bias and Fairness in Large Language Models: A Survey",
            "text": "Race network \u22ee + Figure 9: Example Intra-Processing Mitigation Techniques ( \u00a7 5.3). We show several methods that modify a model's behavior without training or fine-tuning. Constrained next-token search may prohibit certain outputs during beam search (e.g., a derogatory term \"@&!\", in this example), or generate and rerank alternative outputs (e.g., \"he\" replaced with \"she\"). Modified token distribution redistributes nextword probabilities to produce more diverse outputs and avoid biased tokens. Weight distribution, in this example, illustrates how post hoc modifications to attention matrices may narrow focus to less stereotypical tokens (Zayed et al., 2023b). Modular debiasing networks fuse the main LLM with stand-alone networks that can remove specific dimensions of bias, such as gender or racial bias.\n\n(2020) and Xu et al. (2020) propose word-or n-gram blocking during decoding, prohibiting the use of tokens from an unsafe word list. However, biased outputs can still be generated from a set of unbiased tokens or n-grams. To improve upon token blocking strategies, more nuanced approaches constrain text generation by comparing the most likely or a potentially-biased generation to a counterfactual or less biased version. Using a counterfactual-based method, Saunders et al. (2022) use a constrained beam search to generate more gender-diverse outputs at inference. The constrained beam search generates an n-best list of outputs in two passes, first generating the highest likelihood output and then searching for differently-gendered versions of the initial output. Comparing instead to known biases in the data, Sheng et al. (2021a) compare n-gram features from the generated outputs with frequently-occurring biased (or otherwise negative) demographicallyassociated phrases in the data. These n-gram features constrain the next token prediction by requiring semantic similarity with unbiased phrases and dissimilarity with biased phrases. Meade et al. (2023) compare generated outputs to safe example responses from similar contexts, reranking candidate responses based on their similarity to the safe example. Instead of comparing various outputs, Lu et al. (2021) more directly",
            "score": 0.5852294394200911,
            "section_title": "Gender network",
            "char_start_offset": 119382,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 644,
                    "end": 665,
                    "matchedPaperCorpusId": "253734850"
                },
                {
                    "start": 1275,
                    "end": 1297,
                    "matchedPaperCorpusId": "233240748"
                },
                {
                    "start": 1631,
                    "end": 1651,
                    "matchedPaperCorpusId": "235097342"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77685546875
        },
        {
            "corpus_id": "273901329",
            "title": "BiasWipe: Mitigating Unintended Bias in Text Classifiers through Model Interpretability",
            "text": "More recently, Goldfarb-Tarrant et al. (2023) proposed an Equity Evaluation Corpus to measure racial and gender bias in German, Chinese, Japanese, and Spanish sentiment classifiers. \n\nAccording to Garg et al. (2023), bias mitigation techniques can be classified into two categories, viz., data correction and filtering (pre-processing) or debiased training of downstream models (inprocessing). Data correction and filtering involve upsampling (Dixon et al., 2018;Zhao et al., 2017), use of wordnet hypernym-tree (Badjatiya et al., 2019), or data filtering approaches to obtain the training samples that will lead to better generalizability which reduce bias as a by-product (Zhou et al., 2021). Debiased training involves regularizing loss function (Kennedy et al., 2020), multi-task learning (Vaidya et al., 2020), ensemble based debiasing (Zhou et al., 2021), and adversarial learning (Zhang et al., 2018;Sun et al., 2019). All these techniques require re-training the model by augmenting data or adding additional parameters. A recent attempt has been made by Baldini et al. (2022) to study the impact of model size, data, and random initialization on fairness of the model. They adapt two tabular data post-processing bias mitigation techniques to NLP tasks (Wei et al., 2020;Hardt et al., 2016) to enhance the group fairness of language models. However, in our work we focus on individual entity fairness instead of a group. \n\nThere are attempts to mitigate bias using postprocessing methods, particularly in tabular datasets. These methods aim to adjust specific classification outcomes to improve metrics like equalized odds or equality of opportunity (Hardt et al., 2016). Madras et al. (2018) proposed LAFTR (Learning Adversarially Fair and Transferable Representations), a debiasing method that limits unfairness metrics by employing an adversarial objective function.",
            "score": 0.5850290840786437,
            "section_title": "Bias Detection and Mitigation",
            "char_start_offset": 7438,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 184,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1177
                },
                {
                    "start": 1178,
                    "end": 1349
                },
                {
                    "start": 1350,
                    "end": 1429
                },
                {
                    "start": 1432,
                    "end": 1531
                },
                {
                    "start": 1532,
                    "end": 1680
                },
                {
                    "start": 1681,
                    "end": 1878
                }
            ],
            "ref_mentions": [
                {
                    "start": 197,
                    "end": 215,
                    "matchedPaperCorpusId": "246441998"
                },
                {
                    "start": 443,
                    "end": 463,
                    "matchedPaperCorpusId": "54997157"
                },
                {
                    "start": 512,
                    "end": 536,
                    "matchedPaperCorpusId": "86446136"
                },
                {
                    "start": 793,
                    "end": 814,
                    "matchedPaperCorpusId": "219561187"
                },
                {
                    "start": 887,
                    "end": 907,
                    "matchedPaperCorpusId": "9424845"
                },
                {
                    "start": 1063,
                    "end": 1084,
                    "matchedPaperCorpusId": "248177981"
                },
                {
                    "start": 1262,
                    "end": 1280,
                    "matchedPaperCorpusId": "173991023"
                },
                {
                    "start": 1280,
                    "end": 1299,
                    "matchedPaperCorpusId": "7567061"
                },
                {
                    "start": 1659,
                    "end": 1679,
                    "matchedPaperCorpusId": "7567061"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8974609375
        },
        {
            "corpus_id": "259095584",
            "title": "An Empirical Analysis of Parameter-Efficient Methods for Debiasing Pre-Trained Language Models",
            "text": "Pre-trained language models are able to encode rich linguistic and factual knowledge by learning the co-occurrence information of words in large realworld corpora (Devlin et al., 2019;Petroni et al., 2019;Raffel et al., 2020;Brown et al., 2020). Since most of these corpora are internet-based and not carefully curated, they are likely to contain unbalanced or stereotyped information for certain demographic groups. As a result, pre-trained language models are often demonstrated to inherit bias from human society and exhibit potential harms (Blodgett et al., 2020;Bender et al., 2021;May et al., 2019;Zhao et al., 2019;Sheng et al., 2019;Nangia et al., 2020;Nadeem et al., 2021). Hence, much re-search effort has been devoted to debias pre-trained language models (Meade et al., 2022).\n\nWith the size of language models becoming incredibly large (Brown et al., 2020;Hoffmann et al., 2022;Smith et al., 2022), they are not only at higher risk of exhibiting biased behaviors (Bender et al., 2021), but also hard to debias because of prohibitive computational cost. Therefore, recent parameter-efficient methods (He et al., 2022;Ding et al., 2022) have been applied to bias mitigation, where only a small portion of the parameters are updated (Lauscher et al., 2021;Gira et al., 2022). However, these works are limited in terms of evaluation dimensions, making it unclear how different parameter-efficient methods' performance compare to each other, whether one parameter-efficient method is effective across different types of language models, and whether they are also effective for mitigating religious and racial bias in addition to gender bias. Moreover, direct comparisons with strong post-hoc debiasing methods Schick et al., 2021), as well as evaluations of bias mitigation's impact on the language model's internal knowledge, are often insufficient.\n\nGiven these observations, we investigate three popular parameter-efficient methods, i.e., prefix tuning (Li and Liang, 2021), prompt tuning",
            "score": 0.5833239848930907,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 163,
                    "end": 184,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 184,
                    "end": 205,
                    "matchedPaperCorpusId": "202539551"
                },
                {
                    "start": 544,
                    "end": 567,
                    "matchedPaperCorpusId": "218971825"
                },
                {
                    "start": 587,
                    "end": 604,
                    "matchedPaperCorpusId": "85518027"
                },
                {
                    "start": 604,
                    "end": 622,
                    "matchedPaperCorpusId": "102352962"
                },
                {
                    "start": 622,
                    "end": 641,
                    "matchedPaperCorpusId": "202537041"
                },
                {
                    "start": 641,
                    "end": 661,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 661,
                    "end": 681,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 767,
                    "end": 787,
                    "matchedPaperCorpusId": "239015827"
                },
                {
                    "start": 1112,
                    "end": 1129,
                    "matchedPaperCorpusId": "238583580"
                },
                {
                    "start": 1266,
                    "end": 1284,
                    "matchedPaperCorpusId": "248780268"
                },
                {
                    "start": 1718,
                    "end": 1738,
                    "matchedPaperCorpusId": "232075876"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.44580078125
        },
        {
            "corpus_id": "273501862",
            "title": "Causality for Large Language Models",
            "text": "Dropout is applied during training to reduce overfitting and disrupt gendered associations in the model's attention mechanisms. CDA involves augmenting training data by swapping gendered terms (e.g., replacing \"he\" with \"she\") to teach the model to be neutral to such correlations. Both techniques were found to reduce gender bias while maintaining overall model performance. \n\nCausal-Debias [31]. The Causal-Debias framework offers a pioneering solution to mitigating biases in pretrained language models (PLMs) by integrating causal learning principles into the fine-tuning process, which could be directly applied to the pre-training processes. Unlike conventional methods that separate bias mitigation and task performance optimization, Causal-Debias merges these goals through the use of causal interventions and invariant risk minimization (IRM). The framework effectively distinguishes between causal (label-relevant) and non-causal (bias-related) factors embedded in token representations, addressing spurious correlations resulting from demographic biases and social stereotypes. By generating counterfactual data-modifying bias-related attributes such as gendered or racial terms-and training the model with an invariant loss across diverse environments, Causal-Debias ensures that the model generalizes well across tasks while mitigating biases. This approach directly tackles the challenge of bias resurgence, where previously mitigated biases reappear during fine-tuning, a limitation observed in many existing debiasing techniques. \n\nBy unifying the debiasing process with token embedding learning, the framework provides a robust and practical solution for building fair and accountable NLP models. However, its reliance on external corpora and predefined bias-related word lists poses challenges, particularly in extending its application to broader or more nuanced demographic groups and biases. This dependence may limit its scalability or effectiveness in handling more complex, intersectional biases that extend beyond predefined categories. Nonetheless, Causal-Debias represents a significant advancement in the field, emphasizing the importance of causal learning in achieving meaningful debiasing across various environments.",
            "score": 0.5823273394493663,
            "section_title": "Debiased Token Embedding",
            "char_start_offset": 36075,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 375
                },
                {
                    "start": 378,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1545
                },
                {
                    "start": 1548,
                    "end": 1713
                },
                {
                    "start": 1714,
                    "end": 1912
                },
                {
                    "start": 1913,
                    "end": 2061
                },
                {
                    "start": 2062,
                    "end": 2248
                }
            ],
            "ref_mentions": [
                {
                    "start": 392,
                    "end": 396,
                    "matchedPaperCorpusId": "259370743"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95703125
        },
        {
            "corpus_id": "259924852",
            "title": "How Different Is Stereotypical Bias Across Languages?",
            "text": "Other than that, there is also work on the sources of bias and on mitigation (i.e., debiasing). Mehrabi et al. [16] divide the sources of bias into two categories: originating from the data and originating from the model. The behavior of a model overly focusing on data-related biases is called bias amplification [29]. Hall et al. [11] report a correlation between the strength of bias amplification and measures such as accuracy, model capacity, or model overconfidence. This also implies that this issue is more substantial when recognizing group membership (e.g., gender) is easier than class membership (e.g., positive). Besides introducing WEAT, Bolukbasi et al. \n\n[2] also propose debiasing techniques. Bartl et al. [1] apply counterfactual data substitution to the GAP corpus [27] and fine-tune BERT [9] to mitigate gender bias, achieving promising results for English. However, the same method yielded unsatisfactory performance for German -possibly due to grammar, since German is a gender-marking language, in contrast to English. This shows once more that bias detection and mitigation depend on the language, stressing the importance of our work. Going beyond gender, Meade et al. [15] also apply debiasing techniques for racial and religious biases.",
            "score": 0.5807391746760455,
            "section_title": "Related work",
            "char_start_offset": 4893,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 668
                },
                {
                    "start": 671,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 1041
                },
                {
                    "start": 1042,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1263
                }
            ],
            "ref_mentions": [
                {
                    "start": 808,
                    "end": 811,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.481201171875
        },
        {
            "corpus_id": "265456057",
            "title": "Evaluating Large Language Models through Gender and Racial Stereotypes",
            "text": "Language Models have ushered a new age of AI gaining traction within the NLP community as well as amongst the general population. AI's ability to make predictions, generations and its applications in sensitive decision-making scenarios, makes it even more important to study these models for possible biases that may exist and that can be exaggerated. We conduct a quality comparative study and establish a framework to evaluate language models under the premise of two kinds of biases: gender and race, in a professional setting. We find out that while gender bias has reduced immensely in newer models, as compared to older ones, racial bias still exists.",
            "score": 0.5806442182326577,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69970703125
        },
        {
            "corpus_id": "272826949",
            "title": "Evaluating Gender, Racial, and Age Biases in Large Language Models: A Comparative Analysis of Occupational and Crime Scenarios",
            "text": "Similarly, existing debiasing techniques [5] often depend on external knowledge with potential bias or annotated non-biased samples, which are not always available or practical to obtain. Further, fine-tuning methods can exacerbate the problem by causing LLMs to overfit dataset biases and shortcuts, leading to poor generalization performance on unseen data. These limitations restrict the effectiveness of debiasing efforts and result in residual bias that affect the model's performance in real-world applications. \n\nThis paper undertakes a comprehensive exploration of bias within four leading Large Language Models -Gemini 1.5 pro, Llama3 70b, Claude 3 Opus, and GPT-4o focusing on two main types of bias 1) Gender bias in Occupational scenarios and 2) Gender, Age, and Racial bias exhibited in crime scenarios. Through benchmarking with real-world statistics for the US region, this study demonstrates the extent of bias exhibited by the latest LLM models despite the recent use of II. RELATED WORK In recent years, the issue of bias in large language models (LLMs) has gathered significant attention. Gender bias and stereotypes have been observed in LLMs, particularly in their predictions related to occupations. [6] introduced a new paradigm, distinct from the WinoBias dataset, to assess this bias by testing four LLMs. Their study revealed that LLMs are 3-6 times more likely to assign occupations that align with traditional gender stereotypes and often justify these biases with flawed rationalizations. Similarly, Thakur [3] investigates gender bias in professional contexts by analyzing GPT-2 and GPT-3.5, highlighting disparities in occupational associations within LLM-generated outputs. Thakur identifies gendered word associations and biased narratives within the generated text, proposing algorithmic and data-driven strategies to mitigate these biases while emphasizing the importance of ethical considerations and responsible AI development. Both studies highlight the need for broader research to address issues like intersectionality, dataset bias, and user-centric approaches in LLMs. \n\nNumerous methods have been developed to evaluate and mitigate these biases, across various dimensions including political bias, gender bias, and racial bias which can significantly impact the fairness and reliability of AI systems.",
            "score": 0.5798728092095558,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 2027,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 517
                },
                {
                    "start": 520,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1107
                },
                {
                    "start": 1108,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1705
                },
                {
                    "start": 1706,
                    "end": 1964
                },
                {
                    "start": 1965,
                    "end": 2110
                },
                {
                    "start": 2113,
                    "end": 2344
                }
            ],
            "ref_mentions": [
                {
                    "start": 41,
                    "end": 44,
                    "matchedPaperCorpusId": "261530629"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79345703125
        },
        {
            "corpus_id": "271902917",
            "title": "REFINE-LM: Mitigating Language Model Stereotypes via Reinforcement Learning",
            "text": "Using an alternate approach, the UnQover framework [27] quantifies bias via a set of under-specified masked questions and metrics that control for formulation biases in the input sentences. The goal of such techniques is to capture the \"pure\" stereotypical bias encoded in the LM. Unlike the other frameworks, UnQover supports several types of steoreotypical bias. Apart from measuring bias, several works have sought to mitigate it, either in a pre-, in-, or posttraining fashion. An example of the first category is CDA 1  [45] that augments the training corpus by flipping the polarity of gendered words and syntactic groups in the original training sentences. CDA works well for English but produces inadequate training examples for inflected languages such as Spanish. On those grounds, Zmigrod et al. [50] propose an approach -based on markov random fields -to deal with inflections in other parts of the sentence. Zhao et al. [48] learns gender-neutral word embeddings that encode gender information in a subset of the embedding components, trained to be orthogonal to the remaining components. In a different vibe, plenty of approaches have focused on debiasing word embeddings a posteriori [5,10,13]. \n\nWhen it comes to LMs, pre-and in-training debiasing can be prohibitive. Hence, most works propose to fine-tune pre-trained language models. Mozafari et al. [32] mitigate racial bias by fine-tuning a pre-trained BERT via a proper re-weighting of the input samples. In a different vibe, Context-Debias [24] fine-tunes a pre-trained LM by forcing stereotype words and gender-specific words to be orthogonal in the latent space. Debias-BERT [19] resorts to equalizing and declustering losses to adjust BERT. Bias is evaluated by human annotators on the LM's answers for sentence completion and summarization tasks.",
            "score": 0.5798646957249904,
            "section_title": "Related Work",
            "char_start_offset": 7718,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 1101
                },
                {
                    "start": 1102,
                    "end": 1209
                },
                {
                    "start": 1212,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1351
                },
                {
                    "start": 1352,
                    "end": 1475
                },
                {
                    "start": 1476,
                    "end": 1636
                },
                {
                    "start": 1637,
                    "end": 1715
                },
                {
                    "start": 1716,
                    "end": 1822
                }
            ],
            "ref_mentions": [
                {
                    "start": 51,
                    "end": 55,
                    "matchedPaperCorpusId": "222141056"
                },
                {
                    "start": 807,
                    "end": 811,
                    "matchedPaperCorpusId": "184486914"
                },
                {
                    "start": 1199,
                    "end": 1202,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 1202,
                    "end": 1205,
                    "matchedPaperCorpusId": "59158788"
                },
                {
                    "start": 1205,
                    "end": 1208,
                    "matchedPaperCorpusId": "245117373"
                },
                {
                    "start": 1368,
                    "end": 1372,
                    "matchedPaperCorpusId": "221136077"
                },
                {
                    "start": 1512,
                    "end": 1516,
                    "matchedPaperCorpusId": "231698657"
                },
                {
                    "start": 1649,
                    "end": 1653,
                    "matchedPaperCorpusId": "236477795"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79833984375
        },
        {
            "corpus_id": "248135648",
            "title": "Measuring Embedded Human-Like Biases in Face Recognition Models",
            "text": "A bias mitigation method can be largely divided according to the areas of model distribution targeted for pre-processing, in-processing, and post-processing [19]. The most widely used pre-processing technique is to re-balance datasets [20,21] or use synthetic data [22]. In the case of datasets used in face recognition tasks, they proved to have an imbalanced class distribution both in gender and race [23]. To address this problem, several datasets with a balanced number of gender, ethnicity, and the other attributes are proposed by the previous studies, including Racial Faces in Wild [24], Balanced Faces in the Wild [25], and DiveFace [26]. Although, these datasets contribute to mitigating abnormal distributions, but not to demonstrating that training with these datasets leads to impartial results, because labels for ethnicity in the datasets are not widely allowed as ground truth and are overly dependent on the annotator's decision [27]. This motivates researchers to develop in-and post-processing methods. \n\nIn-processing approaches take several methods to get rid of impartiality while training. For example, cost-sensitive training and adversarial learning are used to get rid of sensitive information from functionality [20,21]. Moreover, adjusting parameters of loss functions and taking an unsupervised way of training are used to protect minorities by training models with unbiased representations [26,28]. The examples of post-processing techniques include re-regulating the similarity scores of the two feature vectors based on demographic groups of the images [29] or attaching layers to the feature extractor for removing sensitive information from the representation [26]. \n\nAlong the line, growing numbers of measurements have appeared to measure the effectiveness of the mitigation approaches. In the natural language processing field, various tests have been proposed to quantify bias in pre-trained word embedding models. Bolukbasi et al. [30] and Manzini et al. [31] employed word analogy tests and demonstrated undesirable bias toward gender, racial, and religious groups in word embeddings.",
            "score": 0.57869210369934,
            "section_title": "Related Work",
            "char_start_offset": 3982,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1022
                },
                {
                    "start": 1025,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1429
                },
                {
                    "start": 1430,
                    "end": 1700
                },
                {
                    "start": 1703,
                    "end": 1823
                },
                {
                    "start": 1824,
                    "end": 1953
                },
                {
                    "start": 1954,
                    "end": 2125
                }
            ],
            "ref_mentions": [
                {
                    "start": 235,
                    "end": 239,
                    "matchedPaperCorpusId": "44117253"
                },
                {
                    "start": 239,
                    "end": 242,
                    "matchedPaperCorpusId": "208310173"
                },
                {
                    "start": 265,
                    "end": 269,
                    "matchedPaperCorpusId": "198183828"
                },
                {
                    "start": 404,
                    "end": 408,
                    "matchedPaperCorpusId": "221738980"
                },
                {
                    "start": 591,
                    "end": 595,
                    "matchedPaperCorpusId": "198968250"
                },
                {
                    "start": 624,
                    "end": 628,
                    "matchedPaperCorpusId": "211132534"
                },
                {
                    "start": 643,
                    "end": 647,
                    "matchedPaperCorpusId": "219966361"
                },
                {
                    "start": 947,
                    "end": 951,
                    "matchedPaperCorpusId": "207070633"
                },
                {
                    "start": 1240,
                    "end": 1244,
                    "matchedPaperCorpusId": "44117253"
                },
                {
                    "start": 1244,
                    "end": 1247,
                    "matchedPaperCorpusId": "208310173"
                },
                {
                    "start": 1421,
                    "end": 1425,
                    "matchedPaperCorpusId": "219966361"
                },
                {
                    "start": 1425,
                    "end": 1428,
                    "matchedPaperCorpusId": "211506207"
                },
                {
                    "start": 1586,
                    "end": 1590,
                    "matchedPaperCorpusId": "211068764"
                },
                {
                    "start": 1695,
                    "end": 1699,
                    "matchedPaperCorpusId": "219966361"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6552734375
        },
        {
            "corpus_id": "248780268",
            "title": "Debiasing Pre-Trained Language Models via Efficient Fine-Tuning",
            "text": "An explosion in the popularity of transformer-based language models (such as GPT-3, BERT, RoBERTa, and ALBERT) has opened the doors to new machine learning applications involving language modeling, text generation, and more. However, recent scrutiny reveals that these language models contain inherent biases towards certain demographics reflected in their training data. While research has tried mitigating this problem, existing approaches either fail to remove the bias completely, degrade performance (\u201ccatastrophic forgetting\u201d), or are costly to execute. This work examines how to reduce gender bias in a GPT-2 language model by fine-tuning less than 1% of its parameters. Through quantitative benchmarks, we show that this is a viable way to reduce prejudice in pre-trained language models while remaining cost-effective at scale.",
            "score": 0.5782167040824414,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.900390625
        },
        {
            "corpus_id": "270765561",
            "title": "Masking Latent Gender Knowledge for Debiasing Image Captioning",
            "text": "Model Debiasing in Language Models.Language models capture social biases from the data they are trained; presence of gender bias (Zhao et al., 2019;Bordia and Bowman, 2019;Dinan et al., 2020;Sun et al., 2019;Basta and Costa-juss\u00e0, 2021;Pessach and Shmueli, 2022;Kotek et al., 2023) and racial bias (Garg et al., 2018;Davidson et al., 2019;Gehman et al., 2020;Manzini et al., 2019;Mehrabi et al., 2021) in language models have been well documented.To mitigate the bias, a commonly employed data-driven technique called Counterfactual data augmentation (CDA) proposes to swap bias attribute words in a dataset to re-balance a corpus (Zmigrod et al., 2019;Dinan et al., 2020;Webster et al., 2020;Barikeri et al., 2021).The re-balanced corpus is then used for further training to debias a model.This method requires domain knowledge or human intervention to generate plausible counterfactuals and may introduce noise or inconsistency into the data (Lauscher et al., 2021;Qiang et al., 2022;Meade et al., 2022).Bolukbasi et al. (2016) study the use of orthogonal projection for eliminating gender biases in word embeddings, which was subsequently extended by Liang et al. (2020) to include debiasing of sentence embeddings.Other methods include using dropout regularization as a bias mitigation technique (Webster et al., 2020), discouraging the model from generating biased text by tuning prompt (Schick et al., 2021), or projecting the neural representations to a null-space of classifiers that are used to predict unwanted information (Ravfogel et al., 2020).",
            "score": 0.5779236699855183,
            "section_title": "Related Work",
            "char_start_offset": 4162,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 35
                },
                {
                    "start": 35,
                    "end": 447
                },
                {
                    "start": 447,
                    "end": 716
                },
                {
                    "start": 716,
                    "end": 791
                },
                {
                    "start": 791,
                    "end": 1006
                },
                {
                    "start": 1006,
                    "end": 1218
                },
                {
                    "start": 1218,
                    "end": 1557
                }
            ],
            "ref_mentions": [
                {
                    "start": 129,
                    "end": 148,
                    "matchedPaperCorpusId": "102352962"
                },
                {
                    "start": 148,
                    "end": 172,
                    "matchedPaperCorpusId": "102352788"
                },
                {
                    "start": 172,
                    "end": 191,
                    "matchedPaperCorpusId": "207852875"
                },
                {
                    "start": 236,
                    "end": 262,
                    "matchedPaperCorpusId": "246595433"
                },
                {
                    "start": 262,
                    "end": 281,
                    "matchedPaperCorpusId": "261276445"
                },
                {
                    "start": 298,
                    "end": 317,
                    "matchedPaperCorpusId": "4930886"
                },
                {
                    "start": 339,
                    "end": 359,
                    "matchedPaperCorpusId": "221878771"
                },
                {
                    "start": 359,
                    "end": 380,
                    "matchedPaperCorpusId": "102350941"
                },
                {
                    "start": 380,
                    "end": 401,
                    "matchedPaperCorpusId": "201666566"
                },
                {
                    "start": 631,
                    "end": 653,
                    "matchedPaperCorpusId": "184486914"
                },
                {
                    "start": 653,
                    "end": 672,
                    "matchedPaperCorpusId": "207852875"
                },
                {
                    "start": 693,
                    "end": 715,
                    "matchedPaperCorpusId": "235358955"
                },
                {
                    "start": 944,
                    "end": 967,
                    "matchedPaperCorpusId": "237440429"
                },
                {
                    "start": 967,
                    "end": 986,
                    "matchedPaperCorpusId": "250633555"
                },
                {
                    "start": 986,
                    "end": 1005,
                    "matchedPaperCorpusId": "239015827"
                },
                {
                    "start": 1006,
                    "end": 1029,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 1154,
                    "end": 1173,
                    "matchedPaperCorpusId": "207996257"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7685546875
        },
        {
            "corpus_id": "277066604",
            "title": "No LLM is Free From Bias: A Comprehensive Study of Bias Evaluation in Large Language models",
            "text": "Large Language Models (LLMs) serve as foundation models for different types of NLP tasks with impressive performance without the need for retraining models, unlike their predecessors (Liu et al., 2024;Touvron et al., 2023;Achiam et al., 2023). LLMs have shown remarkable performance across numerous commonsense reasoning tasks and are extensively utilized in several decision-making processes. Although LLMs have immense potential and utility, they raise concerns due to the inherent biases that reflect societal prejudices embedded in the training data (Bender et al., 2021;Blodgett et al., 2020). A multitude of works have focused on detecting and mitigating bias in LLMs related to sensitive characteristics such as gender (Nadeem et al., 2021;Rudinger et al., 2018;You et al., 2024), religion (Plaza-del Arco et al., 2024), race (Yang et al., 2024;Kiritchenko and Mohammad, 2018), and profession, which have been widely studied. \n\nIn contrast, less attention has been given to aspects like age, physical appearance, and socio-economic status (Nangia et al., 2020), as depicted in Table 2. Most of the current literature on bias detection focuses on specific bias aspects, often using benchmark datasets that address only those aspects. These benchmarks are typically evaluated with a baseline pre-trained model fine-tuned on the biasspecific samples. Moreover, not many works provide experimental studies on various aspects of biases using generalizable approaches and evaluation strategies to detect the bias in LLMs. \n\nTo this end, we attempt to unify the evaluation of benchmarks using a set of representative open-source LLMs covering different aspects of bias starting from physical characteristics to socioeconomic categories. We also provide a comprehensive analysis of their performance on different bias aspects by formulating three research questions. RQ1. What are the different types of generalizable approaches to detect biases in LLMs?, RQ2. How do we standardize the metrics across the datasets to evaluate the bias?, RQ3. Do LLMs exhibit similar tendencies across different types of biases and how does it vary with the corresponding evaluation strategies and benchmark datasets?",
            "score": 0.5778751817263887,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 932
                },
                {
                    "start": 935,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1522
                },
                {
                    "start": 1525,
                    "end": 1736
                },
                {
                    "start": 1737,
                    "end": 1865
                },
                {
                    "start": 1866,
                    "end": 1870
                },
                {
                    "start": 1871,
                    "end": 1959
                },
                {
                    "start": 1960,
                    "end": 2041
                },
                {
                    "start": 2042,
                    "end": 2199
                }
            ],
            "ref_mentions": [
                {
                    "start": 554,
                    "end": 575,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 575,
                    "end": 597,
                    "matchedPaperCorpusId": "218971825"
                },
                {
                    "start": 726,
                    "end": 747,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 747,
                    "end": 769,
                    "matchedPaperCorpusId": "13756572"
                },
                {
                    "start": 769,
                    "end": 786,
                    "matchedPaperCorpusId": "271050627"
                },
                {
                    "start": 797,
                    "end": 826,
                    "matchedPaperCorpusId": "271064446"
                },
                {
                    "start": 833,
                    "end": 852,
                    "matchedPaperCorpusId": "267212206"
                },
                {
                    "start": 852,
                    "end": 883,
                    "matchedPaperCorpusId": "21670658"
                },
                {
                    "start": 1046,
                    "end": 1067,
                    "matchedPaperCorpusId": "222090785"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4912109375
        },
        {
            "corpus_id": "268553687",
            "title": "Locating and Mitigating Gender Bias in Large Language Models",
            "text": "Numerous methods aimed at mitigating model biases have been successively proposed in recent years.Zhao et al. [36] proposed creating an augmented dataset to mitigate gender bias in word embedding.This method involves training on the combined dataset, which includes both the original and gender-swapped data.The augmented dataset is generated using gender-swapping techniques.As word embedding models constitute a fundamental component in many NLP systems, mitigating biases within embedding significantly contributes to reducing the biases that propagate to downstream tasks.Bolukbasi et al. [2] define gender bias as the correlation between the magnitude of the projection onto the gender subspace of a word embedding representing a gender-neutral word and that word's bias rating, as rated by crowd workers.Bolukbasi et al. attempted to remove gender bias from the gender subspace.Additionally, Reinforcement Learning with Human Feedback (RLHF) [37], an emerging debias method, was developed to fine-tune large language models, such as ChatGPT, in order to reduce their biases and align them with human values.This approach involves collecting a dataset of human demonstrations, comparisons, and preferences to create a reward model guiding the fine-tuning process [7,22].Moreover, various debias methods based on masked language models have proven effective [15,38].Zmigrod et al. [38] employs a counterfactual data augmentation strategy by reversing gender pronouns found in Wikipedia, to reduce gender bias within the model through continued pre-training.Building on this foundation, Webster et al. [32] adopt a dropout regularization strategy, aiming to reduce gender bias by increasing the Dropout rate during the continued pre-training.\n\nKnowledge editing [34], as an emerging technology, allows for selective updates and adjustments to the model's knowledge without the need for complete retraining.These methods seek to balance the demand for accurate, current information with the practical limitations of computational resources and time.Some editing techniques have demonstrated robust editing capabilities, managing to modify up to 1000 factual knowledge points swiftly without impairing the model's other functionalities [19].",
            "score": 0.577770119916662,
            "section_title": "Introduction",
            "char_start_offset": 2139,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 98,
                    "end": 196
                },
                {
                    "start": 196,
                    "end": 308
                },
                {
                    "start": 308,
                    "end": 376
                },
                {
                    "start": 376,
                    "end": 576
                },
                {
                    "start": 576,
                    "end": 810
                },
                {
                    "start": 810,
                    "end": 884
                },
                {
                    "start": 884,
                    "end": 1113
                },
                {
                    "start": 1113,
                    "end": 1275
                },
                {
                    "start": 1275,
                    "end": 1370
                },
                {
                    "start": 1370,
                    "end": 1561
                },
                {
                    "start": 1561,
                    "end": 1745
                },
                {
                    "start": 1747,
                    "end": 1909
                },
                {
                    "start": 1909,
                    "end": 2051
                },
                {
                    "start": 2051,
                    "end": 2242
                }
            ],
            "ref_mentions": [
                {
                    "start": 593,
                    "end": 596,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 1271,
                    "end": 1274,
                    "matchedPaperCorpusId": "252693405"
                },
                {
                    "start": 1362,
                    "end": 1366,
                    "matchedPaperCorpusId": "248780440"
                },
                {
                    "start": 1366,
                    "end": 1369,
                    "matchedPaperCorpusId": "184486914"
                },
                {
                    "start": 1385,
                    "end": 1389,
                    "matchedPaperCorpusId": "184486914"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84521484375
        },
        {
            "corpus_id": "235097594",
            "title": "On Transferability of Bias Mitigation Effects in Language Model Fine-Tuning",
            "text": "The practice of fine-tuning pretrained language models (PTLMs or LMs), such as BERT (Devlin et al., 2019), has improved prediction performance in a wide range of NLP tasks. However, finetuned LMs may exhibit biases against certain protected groups (e.g., gender and ethnic minorities),  c). We study the viability of obtaining an upstream model that could reduce bias in a number of downstream classifiers when fine-tuned. \n\nas models may learn to associate certain features with positive or negative labels spuriously (Dixon et al., 2018), or propagate bias encoded in PTLMs to downstream classifiers (Caliskan et al., 2017;Bolukbasi et al., 2016). Among many examples, Kurita et al. (2019) demonstrates gender-bias in the pronoun resolution task when models are trained using BERT embeddings, and Kennedy et al. (2020) shows that hate speech classifiers finetuned from BERT result in more frequent false positive predictions for certain group identifier mentions (e.g., \"muslim\", \"black\"). \n\nApproaches for bias mitigation are mostly applied during fine-tuning to reduce bias in a specific downstream task or dataset (Park et al., 2018;Zhang et al., 2018;Beutel et al., 2017) (see Fig. 1 (a)). For example, data augmentation approaches reduce the influence of spurious features in the original dataset (Dixon et al., 2018;Zhao et al., 2018;Park et al., 2018), and adversarial learning approaches generate debiased data representations that are exclusive to the downstream model (Kumar et al., 2019;Zhang et al., 2018). These techniques act on biases particular to the given dataset, domain, or task, and require new bias mitigation when switching to a new downstream task or dataset. This can require auxiliary training objectives, the definition of task-specific fairness met-rics, the annotation of bias attributes (e.g., identifying African American Vernacular English), and the collection of users' demographic data.",
            "score": 0.5774586261558172,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 422
                },
                {
                    "start": 425,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 991
                },
                {
                    "start": 994,
                    "end": 1195
                },
                {
                    "start": 1196,
                    "end": 1520
                },
                {
                    "start": 1521,
                    "end": 1685
                },
                {
                    "start": 1686,
                    "end": 1922
                }
            ],
            "ref_mentions": [
                {
                    "start": 84,
                    "end": 105,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 519,
                    "end": 539,
                    "matchedPaperCorpusId": "54997157"
                },
                {
                    "start": 602,
                    "end": 625,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 625,
                    "end": 648,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 671,
                    "end": 691,
                    "matchedPaperCorpusId": "190000105"
                },
                {
                    "start": 799,
                    "end": 820,
                    "matchedPaperCorpusId": "218517088"
                },
                {
                    "start": 1119,
                    "end": 1138,
                    "matchedPaperCorpusId": "52070035"
                },
                {
                    "start": 1138,
                    "end": 1157,
                    "matchedPaperCorpusId": "9424845"
                },
                {
                    "start": 1304,
                    "end": 1324,
                    "matchedPaperCorpusId": "54997157"
                },
                {
                    "start": 1324,
                    "end": 1342,
                    "matchedPaperCorpusId": "4952494"
                },
                {
                    "start": 1342,
                    "end": 1360,
                    "matchedPaperCorpusId": "52070035"
                },
                {
                    "start": 1480,
                    "end": 1500,
                    "matchedPaperCorpusId": "202541748"
                },
                {
                    "start": 1500,
                    "end": 1519,
                    "matchedPaperCorpusId": "9424845"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8017578125
        },
        {
            "corpus_id": "265221228",
            "title": "Beyond Detection: Unveiling Fairness Vulnerabilities in Abusive Language Models",
            "text": "Research on fairness in abusive language detection has explored the presence of unintended biases in these systems and proposed various methods, e.g., [6,8,10,28], to mitigate them. For instance, studies have examined dialect biases against African American English (AAE) dialects [7,32,40] compared to Standard American English (SAE), and biases related to general identity terms such as gender and race [10,28,43]. Some approaches focus on dataset creation, measuring biases in models trained on different datasets [28], and introducing methods to reduce bias. Other techniques involve adversarial training at the attribute word level, considering dialect to mitigate annotator bias [32], and employing two-step approaches [1] for bias detection and mitigation. There are also efforts to quantify bias in toxic text classification datasets [39] and propose post-processing methods [2] to alleviate bias in classification results. Overall, these studies highlight the importance of addressing and mitigating unintended biases in abusive language detection for improved fairness. Despite substantial progress in identifying and mitigating biases in abusive language detection, a significant gap remains in understanding vulnerabilities that could compromise fairness in these models. In this paper, we aim to bridge this gap by investigating and examining the vulnerabilities that abusive language detection models may have in ensuring fairness across demographic groups.",
            "score": 0.5769779945551259,
            "section_title": "Fairness in Abusive Language Detection",
            "char_start_offset": 5401,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1471
                }
            ],
            "ref_mentions": [
                {
                    "start": 151,
                    "end": 154,
                    "matchedPaperCorpusId": "236460063"
                },
                {
                    "start": 154,
                    "end": 156,
                    "matchedPaperCorpusId": "1733167"
                },
                {
                    "start": 156,
                    "end": 159,
                    "matchedPaperCorpusId": "54997157"
                },
                {
                    "start": 284,
                    "end": 287,
                    "matchedPaperCorpusId": "196211238"
                },
                {
                    "start": 287,
                    "end": 290,
                    "matchedPaperCorpusId": "218869965"
                },
                {
                    "start": 405,
                    "end": 409,
                    "matchedPaperCorpusId": "54997157"
                },
                {
                    "start": 685,
                    "end": 689,
                    "matchedPaperCorpusId": "196211238"
                },
                {
                    "start": 725,
                    "end": 728,
                    "matchedPaperCorpusId": "86446136"
                },
                {
                    "start": 842,
                    "end": 846,
                    "matchedPaperCorpusId": "174799974"
                },
                {
                    "start": 883,
                    "end": 886,
                    "matchedPaperCorpusId": "248177981"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73388671875
        },
        {
            "corpus_id": "247594904",
            "title": "Mitigating Gender Bias in Machine Translation through Adversarial Learning",
            "text": "To avoid perpetuating harm, recent research has begun to examine how biases in NLP systems could be measured and reduced. Efforts to mitigate biases that rely on extensive dataset curation may be infeasible in some applications, such as translation of low-resource or morphologically complex languages. However, recent work suggests that adversarial learning can help to mitigate biases during training without the need to provide additional unbiased data or restructure the original model (Zhang et al., 2018). 1 The method has shown promise in Figure 1: Example of gender bias in English-French translation using Google Translate. The system translates \"nurse\" to \"l'infirmi\u00e8re,\" a female nurse, and \"mechanic\" to \"le m\u00e9canicien,\" a male mechanic. simple proof-of-concept applications, such as mitigating bias in word embeddings for use in analogies. \n\nLarge language models, pre-trained without supervision and then fine-tuned for specific applications, have become a dominant paradigm in NLP. However, using adversarial learning for bias mitigation in such frameworks raises several questions. How can we define a protected variable in the context of these models? How can we apply an adversarial framework for bias mitigation to a pre-training/fine-tuning setup? Finally, how can we quantitatively evaluate the extent to which this method mitigates gender bias? \n\nWe present a framework for leveraging adversarial learning to mitigate gender bias in machine translation that advances solutions to several issues faced when using this framework in modern NLP domains: we propose two measures for gender as a protected variable in the context of large language models, discuss how an adversarial framework can be applied during model fine-tuning, and present quantitative results on the effectiveness of this method at mitigating gender bias in machine translation. Our model reduces translation gender bias in the model T5 with little to no adverse effect on translation quality. versarial bias mitigation\" to avoid the implication that all forms of bias are completely removed.",
            "score": 0.576913111368663,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 852
                },
                {
                    "start": 855,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1267
                },
                {
                    "start": 1268,
                    "end": 1366
                },
                {
                    "start": 1369,
                    "end": 1868
                },
                {
                    "start": 1869,
                    "end": 1983
                },
                {
                    "start": 1984,
                    "end": 2082
                }
            ],
            "ref_mentions": [
                {
                    "start": 490,
                    "end": 510,
                    "matchedPaperCorpusId": "9424845"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5673828125
        },
        {
            "corpus_id": "264425921",
            "title": "DiFair: A Benchmark for Disentangled Assessment of Gender Knowledge and Bias",
            "text": "It is widely acknowledged that pre-trained language models may demonstrate biased behavior against underrepresented demographic groups, such as women (Silva et al., 2021) or racial minorities (Field et al., 2021). Given the broad adoption of these models across various use cases, it is imperative for social good to understand these biases and strive to mitigate them while retaining factual gender information that is required to make meaningful gender-based predictions. \n\nIn recent years, numerous studies have attempted to address the biased behaviour of language models, either by manipulating the training data (Webster et al., 2020), altering the training objective (Kaneko and Bollegala, 2021), or by modifying the architecture (Lauscher et al., 2021). Although \u2020 Work done as a Master's student at TeIAS. Figure 1: Ideally, a language model is expected not to favor a gender in a sentence that does not explicitly specify one (top example), while it should prefer genderspecific words when the gender is explicitly specified in the sentence (bottom example). Cf. Section 2.1 for task formulation. \n\ncurrent debiasing techniques, such as counterfactual augmentation (Zhao et al., 2018) and dropout techniques (Webster et al., 2020), are effective in removing biased information from model representations, recent studies have shown that such debiasing can damage a model's useful gender knowledge (Limisiewicz and Mare\u010dek, 2022). This suggests the need for more robust metrics for measuring bias in NLP models that simultaneously consider both performance and fairness. \n\nIn this study, we try to fill this evaluation gap by presenting DIFAIR, a benchmark for evaluating gender bias in language models via a masked lan-guage modeling (MLM) objective, while also considering the preservation of relevant gender data. We test several widely-used pretrained models on this dataset, along with recent debiasing techniques. Our findings echo prior research, indicating that these models struggle to discern when to differentiate between genders. This is emphasized by their performance lag compared to human upper bound on our dataset. We also note that while debiasing techniques enhance gender fairness, they typically compromise the model's ability to retain factual gender information.",
            "score": 0.5756771013267912,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 473
                },
                {
                    "start": 476,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1106
                },
                {
                    "start": 1109,
                    "end": 1438
                },
                {
                    "start": 1439,
                    "end": 1578
                },
                {
                    "start": 1581,
                    "end": 1824
                },
                {
                    "start": 1825,
                    "end": 1927
                },
                {
                    "start": 1928,
                    "end": 2049
                },
                {
                    "start": 2050,
                    "end": 2139
                },
                {
                    "start": 2140,
                    "end": 2293
                }
            ],
            "ref_mentions": [
                {
                    "start": 192,
                    "end": 212,
                    "matchedPaperCorpusId": "235593087"
                },
                {
                    "start": 674,
                    "end": 702,
                    "matchedPaperCorpusId": "231698657"
                },
                {
                    "start": 737,
                    "end": 760,
                    "matchedPaperCorpusId": "237440429"
                },
                {
                    "start": 1175,
                    "end": 1194,
                    "matchedPaperCorpusId": "4952494"
                },
                {
                    "start": 1406,
                    "end": 1437,
                    "matchedPaperCorpusId": "248980783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8818359375
        },
        {
            "corpus_id": "247594904",
            "title": "Mitigating Gender Bias in Machine Translation through Adversarial Learning",
            "text": "Linguistic biases serve as a vehicle for harmful stereotypes that demean individuals and communities, harm their mental and physical health, and worsen life outcomes (Beukeboom and Burgers, 2019). Recent studies indicate that NLP systems perpetuate these biases, reproducing stereotypes in their output and disproportionately producing demeaning or outright incorrect output for groups that face societal discrimination. For example, translation systems often translate pronouns or gender inflections incorrectly when they correspond to counter-stereotypical professions. \n\nAdversarial bias mitigation intervenes during training by introducing an adversary that attempts to predict a protected variable from the output of the model. It is a model-agnostic strategy, permitting complex models to be substituted into the framework without changing the overall setup, which is convenient for pre-training/fine-tuning setups. It requires no modifications to the training data or additional data collection, permitting it to be extended to new tasks or low-resource domains more easily. By intervening during training itself, this approach also aims to mitigate both dataset bias and bias amplification. \n\nIn this work, we addressed several open questions raised by previous research into the adversarial approach to bias mitigation: (1) how to define gender as the protected variable in more complex applications, particularly in language tasks where such information is not prelabeled; (2) how to apply the adversarial framework to a pre-training/finetuning setup with large language models, as has become the norm; and (3) whether the adversarial framework is indeed effective as measured by quantitative evaluations on realistic tasks. \n\nWe presented an adversarial framework for mitigating gender bias in machine translation. Our approach proposes two measures for gender as a protected variable in the context of large language models, the gender direction method and pronoun usage method. We then applied the adversarial framework to English-French and English-German machine translation. For both the gender direction and pronoun usage methods, the difference in F1 scores between sentences in the WinoMT dataset involving male and female entities decreased, and for the pronoun usage method, the difference in accuracy between pro-and anti-stereotypical sentences also decreased.",
            "score": 0.5744420044922949,
            "section_title": "Conclusion",
            "char_start_offset": 24509,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 571
                },
                {
                    "start": 574,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1198
                },
                {
                    "start": 1201,
                    "end": 1734
                },
                {
                    "start": 1737,
                    "end": 1825
                },
                {
                    "start": 1826,
                    "end": 1990
                },
                {
                    "start": 1991,
                    "end": 2090
                },
                {
                    "start": 2091,
                    "end": 2383
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.685546875
        },
        {
            "corpus_id": "274776546",
            "title": "Bias Vector: Mitigating Biases in Language Models with Task Arithmetic Approach",
            "text": "Language models (LMs) are inherently biased because their training processes rely on humancreated text data, which would reflect human biases (Bolukbasi et al., 2016). Navigli et al. (2023) defined the term bias in the field of Natural Language Processing as \"prejudices, stereotypes, and discriminatory attitudes against certain groups of people.\" We adopt this bias definition throughout this paper. Various debiasing methods have been proposed to mitigate these biases (Schick et al., 2021;Zmigrod et al., 2019;Webster et al., 2020;Ravfogel et al., 2020;Liang et al., 2020). \n\nSeveral studies have shown that for wordembedding models, such as word2vec (Mikolov et al., 2013), the bias in word embeddings can be mitigated using approaches like subtracting the statistically significant mean vector associated with the bias from each word vector (Bolukbasi et al., 2016;Mu and Viswanath, 2018;Gonen and Goldberg, 2019;Wang et al., 2020). In contrast, other studies ahve proposed bias mitigation techniques specifically for Transformer-based LMs (Ravfogel et al., 2020;Liang et al., 2020). \n\nSeveral benchmarks have been introduced to evaluate debiasing approaches. Islam et al. (2016) developed the Word Embedding Association Test (WEAT) to measure bias scores in word embeddings. May et al. (2019) proposed the Sentence Encoder Association Test (SEAT) as an extension of WEAT, extending the focus from word to sentence. StereoSet (Nadeem et al., 2021) is another benchmark designed to evaluate stereotypes across four bias categories: race, profession, gender, and religion. StereoSet consists of two subsets: intrasentence, which measures biases within a individual sentence, and intersentence, which evaluates biases at the discourse level across multiple sentences. Nangia et al. (2020) also introduced the CrowS-Pairs benchmark for bias neasurements.",
            "score": 0.573940903155723,
            "section_title": "Language Models and Bias",
            "char_start_offset": 4642,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 577
                },
                {
                    "start": 580,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1089
                },
                {
                    "start": 1092,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1576
                },
                {
                    "start": 1577,
                    "end": 1770
                },
                {
                    "start": 1771,
                    "end": 1856
                }
            ],
            "ref_mentions": [
                {
                    "start": 142,
                    "end": 166,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 168,
                    "end": 189,
                    "matchedPaperCorpusId": "258688053"
                },
                {
                    "start": 472,
                    "end": 493,
                    "matchedPaperCorpusId": "232075876"
                },
                {
                    "start": 493,
                    "end": 514,
                    "matchedPaperCorpusId": "184486914"
                },
                {
                    "start": 535,
                    "end": 557,
                    "matchedPaperCorpusId": "215786522"
                },
                {
                    "start": 557,
                    "end": 576,
                    "matchedPaperCorpusId": "207996257"
                },
                {
                    "start": 847,
                    "end": 871,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 871,
                    "end": 894,
                    "matchedPaperCorpusId": "3986339"
                },
                {
                    "start": 894,
                    "end": 919,
                    "matchedPaperCorpusId": "73729169"
                },
                {
                    "start": 919,
                    "end": 937,
                    "matchedPaperCorpusId": "204770514"
                },
                {
                    "start": 1046,
                    "end": 1069,
                    "matchedPaperCorpusId": "215786522"
                },
                {
                    "start": 1069,
                    "end": 1088,
                    "matchedPaperCorpusId": "207996257"
                },
                {
                    "start": 1282,
                    "end": 1299,
                    "matchedPaperCorpusId": "85518027"
                },
                {
                    "start": 1432,
                    "end": 1453,
                    "matchedPaperCorpusId": "215828184"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75634765625
        },
        {
            "corpus_id": "272987538",
            "title": "FairPIVARA: Reducing and Assessing Biases in CLIP-Based Multimodal Models",
            "text": "The consolidation, use, and expansion of deep learning models have increased focus on assessing biases in learning models. Many studies focus on how different layers in these models contribute to overall bias. The main evaluation steps and proposals for reducing biases are classified into three main categories: (i) the training dataset, (ii) model architecture and training methods, and (iii) post-processing of results. \n\nWang et al. [20] analyzed gender bias in search models to determine whether gender-neutral languages still contain bias. They introduced a metric to quantify gender bias, measuring differences in image retrieval results between masculine and feminine attributes. The study also proposed two bias mitigation methods: one integrated into model training, requiring full retraining, and another implemented as post-processing. To address the first solution, they identified class imbalance as a significant issue and used a balancing technique that samples gender-neutral images. The second strategy involved clipping highly correlated dimensions using the Kullback-Leibler divergence. Their results showed significant biases in CLIP models, with an 18 percentage points (pp) average reduction in bias across the datasets used. However, the balancing approach during training required labeled images, and the final results showed minimal bias reduction for top-1 predictions, intensifying the overall model bias in some cases. The study focused only on gender bias within English-language datasets. Janghorbani and De Melo [11] assessed bias in multimodal models, proposing a post-processing technique for various concepts based on the work of Caliskan et al. [4]. Their analysis included both cross-modal (text and image encoders) and intra-modal (single encoder) approaches. They introduced the Multi-Modal Bias (MMBias) dataset, which comprises images and texts from diverse social groups, including religious groups, nationalities, individuals with disabilities, and those who identify as sexual minorities. Their bias removal strategy reduced bias by 60.2 pp for the class cut. However, the study did not optimize individual classes -representing a potential improvement avenue -and showed suboptimal accuracy for pleasant and unpleasant image sets.",
            "score": 0.5732288922402065,
            "section_title": "Related Work",
            "char_start_offset": 2748,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 422
                },
                {
                    "start": 425,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1519
                },
                {
                    "start": 1520,
                    "end": 1685
                },
                {
                    "start": 1686,
                    "end": 1797
                },
                {
                    "start": 1798,
                    "end": 2032
                },
                {
                    "start": 2033,
                    "end": 2103
                },
                {
                    "start": 2104,
                    "end": 2275
                }
            ],
            "ref_mentions": [
                {
                    "start": 437,
                    "end": 441,
                    "matchedPaperCorpusId": "237490811"
                },
                {
                    "start": 1544,
                    "end": 1548,
                    "matchedPaperCorpusId": "257663900"
                },
                {
                    "start": 1681,
                    "end": 1684,
                    "matchedPaperCorpusId": "23163324"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68359375
        },
        {
            "corpus_id": "271404523",
            "title": "Towards Transfer Unlearning: Empirical Evidence of Cross-Domain Bias Mitigation",
            "text": "In recent years, the natural language processing (NLP) field has experienced a transformative shift with the introduction of Large Language Models (LLMs).The remarkable capabilities of LLMs are largely attributed to scaling laws (Kaplan et al., 2020), which suggest their capability heavily depends on the model size and training dataset size.However, training on massive corpus often results in LLMs inadvertently acquiring social biases present in their training datasets (Webster et al., 2021;Nangia et al., 2020;Nadeem et al., 2021).Therefore, addressing these biases is crucial for the development of fair and responsible LLMs.\n\nNumerous studies have been conducted to mitigate the bias and toxicity inherent in LLMs (Zhao et al., 2018;Barikeri et al., 2021;Liang et al., 2020;Ravfogel et al., 2020;Schick et al., 2021).However, recent studies (Meade et al., 2022) empiri-cally show that these approaches are effective in reducing bias while compromising the language modeling performance, as indicated by increased perplexity on unbiased text.Notably, existing posthoc techniques manage to retain language modeling performance, but they fail to detect more subtle and implicit toxic content.\n\nTo overcome this challenge, our study explores an unlearning-based approach that makes LLMs forget biased and toxic content.By running gradient ascent on biased text, our method minimizes the likelihood of biased content while minimizing the degradation of language modeling capabilities.Our debiasing method is inspired by the successes of prior work (Chen and Yang, 2023;Jang et al., 2023), which demonstrated the technique's efficacy in unlearning privacy-sensitive data.\n\nThis study explores Mask Language Modeling (MLM) unlearning, which selectively unlearns harmful content within the text by forgetting only toxic or biased tokens.Through empirical investigation on gender-biased text, we discovered that MLM unlearning effectively reduces gender bias without significantly deteriorating language modeling performance.Furthermore, experimental results demonstrated that unlearning gender-biased text also contributes to mitigating other types of bias, such as race and religion.",
            "score": 0.572140928632923,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 154,
                    "end": 343
                },
                {
                    "start": 343,
                    "end": 537
                },
                {
                    "start": 537,
                    "end": 632
                },
                {
                    "start": 634,
                    "end": 825
                },
                {
                    "start": 825,
                    "end": 1049
                },
                {
                    "start": 1049,
                    "end": 1197
                },
                {
                    "start": 1199,
                    "end": 1323
                },
                {
                    "start": 1323,
                    "end": 1487
                },
                {
                    "start": 1487,
                    "end": 1673
                },
                {
                    "start": 1675,
                    "end": 1837
                },
                {
                    "start": 1837,
                    "end": 2024
                },
                {
                    "start": 2024,
                    "end": 2184
                }
            ],
            "ref_mentions": [
                {
                    "start": 496,
                    "end": 516,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 516,
                    "end": 536,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 722,
                    "end": 741,
                    "matchedPaperCorpusId": "4952494"
                },
                {
                    "start": 741,
                    "end": 763,
                    "matchedPaperCorpusId": "235358955"
                },
                {
                    "start": 763,
                    "end": 782,
                    "matchedPaperCorpusId": "207996257"
                },
                {
                    "start": 782,
                    "end": 804,
                    "matchedPaperCorpusId": "215786522"
                },
                {
                    "start": 804,
                    "end": 824,
                    "matchedPaperCorpusId": "232075876"
                },
                {
                    "start": 849,
                    "end": 869,
                    "matchedPaperCorpusId": "239015827"
                },
                {
                    "start": 1551,
                    "end": 1572,
                    "matchedPaperCorpusId": "264828972"
                },
                {
                    "start": 1572,
                    "end": 1590,
                    "matchedPaperCorpusId": "252693065"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92041015625
        },
        {
            "corpus_id": "258174286",
            "title": "Improving Gender-Related Fairness in Sentence Encoders: A Semantics-Based Approach",
            "text": "Although language models are successfully used in a variety of applications, bias and fairness in NLP have received relatively little consideration until recent times, running the risk of favouring prejudice and strengthening stereotypes [14]. \n\nStatic word embeddings were the first to be analysed. In 2016, they have been shown to exhibit the so-called gender bias, defined as the cosine of the angle between the word embedding of a gender-neutral word, and a one-dimensional subspace representing gender [7]. This approach was later adapted for non-binary social biases such as racial and religious bias [34]. A debiasing algorithm was also proposed to mitigate gender bias in word embeddings [7]; however, it was also shown that it fails to entirely capture and remove bias [24]. The Word Embedding Association Test (WEAT) [11] was created to measure bias in word embeddings following the pattern of the implicit-association test for humans. WEAT demonstrated the presence of harmful associations in GloVe [46] and word2vec [38,39] embeddings. \n\nRecently, a number of different approaches extended the research field. A new debiasing procedure was proposed to reduce gender bias by introducing a term to the loss function used during the training phase of the model [48]. Additionally, [8] presented a regularisation procedure that aims at debiasing a language model by minimising the projection of encoder-trained embeddings onto a subspace that encodes gender. Similarly, [59] used model compression techniques, a type of regularisation techniques, to reduce toxicity and bias originally present in generative language models. The system proposed by [32] mitigates bias by employing counterfactual data augmentation, proving that modifying the training data works better than changing the actual geometry of the embeddings. On a similar note, the approach described by [10] performs perturbations of the original embeddings training data to reduce the overall bias present in them. [27] presented a method to preserve genderrelated information in feminine and masculine words while removing bias from stereotypical words.",
            "score": 0.5714431737550763,
            "section_title": "Related Work",
            "char_start_offset": 5652,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 243
                },
                {
                    "start": 246,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1047
                },
                {
                    "start": 1050,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1466
                },
                {
                    "start": 1467,
                    "end": 1632
                },
                {
                    "start": 1633,
                    "end": 1829
                },
                {
                    "start": 1830,
                    "end": 1987
                },
                {
                    "start": 1988,
                    "end": 2127
                }
            ],
            "ref_mentions": [
                {
                    "start": 238,
                    "end": 242,
                    "matchedPaperCorpusId": "209325068"
                },
                {
                    "start": 507,
                    "end": 510,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 607,
                    "end": 611,
                    "matchedPaperCorpusId": "102350941"
                },
                {
                    "start": 696,
                    "end": 699,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 778,
                    "end": 782,
                    "matchedPaperCorpusId": "73729169"
                },
                {
                    "start": 827,
                    "end": 831,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 1010,
                    "end": 1014,
                    "matchedPaperCorpusId": "1957433"
                },
                {
                    "start": 1032,
                    "end": 1035,
                    "matchedPaperCorpusId": "16447573"
                },
                {
                    "start": 1270,
                    "end": 1274,
                    "matchedPaperCorpusId": "170078973"
                },
                {
                    "start": 1290,
                    "end": 1293,
                    "matchedPaperCorpusId": "102352788"
                },
                {
                    "start": 1656,
                    "end": 1660,
                    "matchedPaperCorpusId": "51888520"
                },
                {
                    "start": 1875,
                    "end": 1879,
                    "matchedPaperCorpusId": "52946942"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7822265625
        },
        {
            "corpus_id": "268553687",
            "title": "Locating and Mitigating Gender Bias in Large Language Models",
            "text": "In recent years, sophisticated artificial intelligence models, notably exemplified by ChatGPT [28,31,33], are specially designed to excel in comprehending complicated natural language and generating human-like text.However, as these models become increasingly integrated across various sectors [6,14], the inherent biases within these systems has become a subject of growing concern.\n\nBias refers to the existence of consistent inaccuracies, misattributions, or erroneous perceptions leading to a preference for specific groups or concepts, the reinforcement of stereotypes, or the formation of flawed conclusions derived from established patterns, it now profoundly impact users and society at large, underscoring the urgent need for a comprehensive examination and mitigation of these issues [9,26].\n\nCurrent research on bias primarily concentrates on two aspects: identification and location of bias [3,29], which aims to understand the origins and manifestations of biases; and mitigation of bias [2,37], which seeks to reduce or eliminate inherent biases.Due to the complex nature and extensive scope of bias manifestations, contemporary studies often focus on specific aspects, such as locating gender bias [29] or mitigating racial bias [15].\n\nCaliskan et al. [3] adopts the core concept of the Implicit Association Test (IAT), measures gender bias by assessing the strength of conceptual associations, using the Word Embedding Association Test (WEAT) to evaluate bias in word embeddings.Caliskan et al. confirm that biases identified through IAT tests are present in word embeddings.Garg et al. [11] show that biases within word embeddings serve as indicators of societal changes, including the fluctuation of female participation in professional fields.Extending this further, May et al. [17] developed the Sentence Encoder Association Test (SEAT) from WEAT, extending its ability to evaluate bias in sentences.Jesse Vig et al. [29] introduces a methodology for interpreting neural language models to analyze how gender bias effects are mediated via specific model components in Transformer-based language models.\n\nNumerous methods aimed at mitigating model biases have been successively proposed in recent years.",
            "score": 0.5698646350574021,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 215
                },
                {
                    "start": 215,
                    "end": 383
                },
                {
                    "start": 385,
                    "end": 801
                },
                {
                    "start": 803,
                    "end": 1060
                },
                {
                    "start": 1060,
                    "end": 1249
                },
                {
                    "start": 1251,
                    "end": 1495
                },
                {
                    "start": 1495,
                    "end": 1591
                },
                {
                    "start": 1591,
                    "end": 1762
                },
                {
                    "start": 1762,
                    "end": 1920
                },
                {
                    "start": 1920,
                    "end": 2122
                },
                {
                    "start": 2124,
                    "end": 2222
                }
            ],
            "ref_mentions": [
                {
                    "start": 294,
                    "end": 297,
                    "matchedPaperCorpusId": "256409866"
                },
                {
                    "start": 297,
                    "end": 300,
                    "matchedPaperCorpusId": "259842568"
                },
                {
                    "start": 903,
                    "end": 906,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 1001,
                    "end": 1004,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 1244,
                    "end": 1248,
                    "matchedPaperCorpusId": "248780440"
                },
                {
                    "start": 1267,
                    "end": 1270,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 1603,
                    "end": 1607,
                    "matchedPaperCorpusId": "4930886"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75732421875
        },
        {
            "corpus_id": "278327486",
            "title": "Personalisation or Prejudice? Addressing Geographic Bias in Hate Speech Detection using Debias Tuning in Large Language Models",
            "text": "Research into bias and stereotypes in LLMs has grown, as concerns about fairness and inclusivity rise. Many studies show that LLMs often mirror biases present in their training data, leading to problems like gender, racial, and cultural biases (Davidson, Bhattacharya, and Weber 2019;Xia, Field, and Tsvetkov 2020;Maronikolakis, Baader, and Sch\u00fctze 2022). For example, gender bias is widely recognised, with models frequently associating certain jobs with specific genders, such as nurses with women and plumbers with men (Thakur 2023) or associating certain emotions with specific genders, such as sadness with women and anger with men (Plaza-del Arco et al. 2024a). Racial biases are also a significant issue, with models sometimes reinforcing harmful stereotypes, particularly for marginalised communities (Salinas et al. 2023;Lim and P\u00e9rez-Ortiz 2024). \n\nSeveral studies have shown that LLMs exhibit geographic bias, where the model's responses vary depending on the region or country in question. For instance, research by Kamruzzaman and Kim (2024) demonstrates that LLMs prompted with geographic personas tend to display more favourable attitudes toward Western European countries, while showing more negative biases toward Eastern European, Latin American, and African nations. Similarly, Manvi et al. (2024) highlight that LLMs are biased against regions with lower socioeconomic conditions. However, to the best of our knowledge, no studies have explored whether this geographic bias also affects LLMs' performance when instructed with sensitive tasks, specifically classifying hate speech. \n\nTo address biases in language models, several debiasing methods have been proposed (Lin et al. 2024). These methods can be applied at different stages: during data preparation, training process, or post-training. In the first stage, techniques like counterfactual data augmentation (CAD) replace biased terms (e.g., swapping gender-specific terms) to ensure equal association with neutral terms (Zhao et al. 2018).",
            "score": 0.5696396762341883,
            "section_title": "Related Work",
            "char_start_offset": 4465,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 856
                },
                {
                    "start": 859,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1400
                },
                {
                    "start": 1401,
                    "end": 1600
                },
                {
                    "start": 1603,
                    "end": 1704
                },
                {
                    "start": 1705,
                    "end": 1815
                },
                {
                    "start": 1816,
                    "end": 2017
                }
            ],
            "ref_mentions": [
                {
                    "start": 244,
                    "end": 284,
                    "matchedPaperCorpusId": "168170119"
                },
                {
                    "start": 284,
                    "end": 314,
                    "matchedPaperCorpusId": "218869965"
                },
                {
                    "start": 314,
                    "end": 354,
                    "matchedPaperCorpusId": "247250765"
                },
                {
                    "start": 522,
                    "end": 535,
                    "matchedPaperCorpusId": "52310274"
                },
                {
                    "start": 637,
                    "end": 666,
                    "matchedPaperCorpusId": "271064446"
                },
                {
                    "start": 830,
                    "end": 855,
                    "matchedPaperCorpusId": "270869465"
                },
                {
                    "start": 1686,
                    "end": 1703,
                    "matchedPaperCorpusId": "270869465"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6162109375
        },
        {
            "corpus_id": "268709624",
            "title": "Biases Mitigation and Expressiveness Preservation in Language Models: A Comprehensive Pipeline (Student Abstract)",
            "text": "Pre-trained Language Models (PLMs) excel in diverse natural language tasks due to their training on extensive data. However, prior studies have revealed that PLMs inadvertently encode and propagate social biases from their unfiltered pre-training data. Take gender bias as an example: the PLM is more inclined towards associating male (female) attributes with programmers (nurses). Several solutions for mitigating the social biases have been proposed, including: (1) Post-hoc-based method add a post-training step to these sentence representations before applied to downstream tasks, including removing the estimated genderdirection subspace from sentence representation (Liang et al. 2020), or use pre-defined word tuples combine specific techniques to debias text encoder for a fair sentence representation (Cheng et al. 2021). (2) Fine-tuning-based models use specific loss terms to guide a PLM to remove biases, including distribution alignment loss for debiasing embedding space (Guo, Yang, and Abbasi 2022); orthogonal loss aims to promote irrelevance between stereotyped words and gender-specific words (Kaneko and Bollegala 2021), etc. Current debiasing methods for PLMs have shown promise but grapple with notable challenges: (1) demanding timeconsuming to fine-tune entire parameters in PLMs; (2) disregarding the expressiveness of PLMs, which could potentially disrupt PLM's computational structure and undermine the benefits of pre-training; (3) reintroducing biases from downstream tasks into PLMs when applying debiased models to those tasks. Hence, we present a new two-stage pipeline that aims to simultaneously preserve the PLMs' expressiveness and mitigate biases from both internal and downstream contexts. As shown in Figure 1, in first stage, we keep PLM's parameter frozen, and only train the continuous prefix to reduce the magnitude of trainable parameters, towards mitigating internal bias and meanwhile preserving expressiveness. In second stage, we perform causal interventions on different demographic groups to eliminate the biases from downstream contexts.",
            "score": 0.5695316491216322,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1726
                },
                {
                    "start": 1727,
                    "end": 1956
                },
                {
                    "start": 1957,
                    "end": 2087
                }
            ],
            "ref_mentions": [
                {
                    "start": 672,
                    "end": 691,
                    "matchedPaperCorpusId": "207996257"
                },
                {
                    "start": 810,
                    "end": 829,
                    "matchedPaperCorpusId": "232185104"
                },
                {
                    "start": 985,
                    "end": 1013,
                    "matchedPaperCorpusId": "248780440"
                },
                {
                    "start": 1111,
                    "end": 1138,
                    "matchedPaperCorpusId": "231698657"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9609375
        },
        {
            "corpus_id": "235305676",
            "title": "A World Full of Stereotypes? Further Investigation on Origin and Gender Bias in Multi-Lingual Word Embeddings",
            "text": "Different methods have been proposed to investigate the origin of bias in word embeddings and mitigation measures. This mitigation can happen at different levels, including the original training data before training the word embeddings, debiasing the word embeddings itself, or using methodologies to debias the classifiers or application using the biased word embeddings. For example, it has been shown that by identifying specific resources from the training data, and removing them before training the word embeddings the bias can be reduced (Brunet et al., 2019). \n\nSeveral methods for debiasing word embeddings have been proposed, for example, post-processing methods (Bolukbasi et al., 2016) or integrated into the training procedure of word embeddings (Zhao et al., 2018) to reduce gender bias. However, it has been argued that such methods do rather cover the bias and not remove it completely from the word embeddings (Gonen and Goldberg, 2019). This example, only considering the mitigation of gender bias, illustrates the complexity of the problem. Different forms of bias that can be mitigated at different levels and concerning different types of word embeddings and other language models make a general resolution of the problem complex. Other findings indicate that debiasing methods using an explicit set of words are unlikely to be effective (Agarwal et al., 2019). \n\nOther approaches consider a mitigation at the level of the application that is using the word embeddings. For example, it has been shown that the mitigation at the level of a classifier that was trained on biased word embeddings was efficient (Papakyriakopoulos et al., 2020). The authors claim that the classifier learns further associations between the vectors, which are not considered when debiasing at the level of the word embeddings. \n\nResearch has clearly proven different types of bias (nationality, origin, gender,and personality) in different types of word embeddings and language models. It has been confirmed in this work that bias can come in different forms for different cultural and linguistic contexts, as it was assumed by previous research.",
            "score": 0.5692864683446843,
            "section_title": "Implications of Cultural Differences in Bias",
            "char_start_offset": 37306,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 567
                },
                {
                    "start": 570,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 954
                },
                {
                    "start": 955,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1382
                },
                {
                    "start": 1385,
                    "end": 1490
                },
                {
                    "start": 1491,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1825
                },
                {
                    "start": 1828,
                    "end": 1984
                },
                {
                    "start": 1985,
                    "end": 2145
                }
            ],
            "ref_mentions": [
                {
                    "start": 545,
                    "end": 566,
                    "matchedPaperCorpusId": "52946942"
                },
                {
                    "start": 673,
                    "end": 697,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 1359,
                    "end": 1381,
                    "matchedPaperCorpusId": "174803409"
                },
                {
                    "start": 1628,
                    "end": 1660,
                    "matchedPaperCorpusId": "211040995"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82763671875
        },
        {
            "corpus_id": "272880907",
            "title": "Do the Right Thing, Just Debias! Multi-Category Bias Mitigation Using LLMs",
            "text": "Bias Mitigation Existing approaches in bias mitigation have notable drawbacks. Methods focusing on debiasing word embeddings while preserving associations (Bolukbasi et al., 2016) fail to account for broader contextual biases beyond the word level. \n\nTechniques enhancing reliability through prompting (Si et al., 2022), while improving generalizability, bias reduction, calibration, and factuality for GPT-3, are limited by the prompts used and do not generalize well to smaller models. Bernsteinbounded unfairness (Ethayarajh, 2020) estimates classification bias with uncertainty but does not extend beyond classification tasks. Comprehensive surveys (Hort et al., 2022) of bias mitigation methods for ML classifiers and benchmarks like WinoBias (Zhao et al., 2017) focus narrowly on gender or racial bias, neglecting other forms of social bias. Upstream mitigation during language model fine-tuning (Jin et al., 2021) is a promising direction but requires expensive retraining of large language models. While Reinforcement Learning from AI Feedback (RLAIF) (Lee et al., 2023) has shown promise in aligning large language models (LLMs) for reactive tasks like counterspeech generation (Hengle et al., 2024), its efficacy in proactive debiasing of language remains an open question. \n\nExisting work (Hengle et al., 2024) primarily focuses on responding to biased speech only after it has been produced and disseminated, leaving the potential for preemptively mitigating biased language largely unexplored. We aim to address these gaps by applying Reinforcement Learning (RL) techniques for aligning language models to proactively identify and effectively mitigate potentially biased sentences across multiple bias classes into their debiased counterparts while retaining the context and linguistic quality of the ground truth. \n\nBias Datasets. Existing bias datasets, notably WIKIBIAS (Zhong et al., 2021), reveal an ambigu-ous pattern where the so-called debiased sentences often retain the original biases (See Table 1 for the example).",
            "score": 0.5689042542032111,
            "section_title": "Related Work",
            "char_start_offset": 5062,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 79,
                    "end": 248
                },
                {
                    "start": 251,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1283
                },
                {
                    "start": 1286,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1827
                },
                {
                    "start": 1830,
                    "end": 1844
                },
                {
                    "start": 1845,
                    "end": 2039
                }
            ],
            "ref_mentions": [
                {
                    "start": 516,
                    "end": 534,
                    "matchedPaperCorpusId": "216553205"
                },
                {
                    "start": 748,
                    "end": 767,
                    "matchedPaperCorpusId": "1389483"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7109375
        },
        {
            "corpus_id": "271310069",
            "title": "BiasDPO: Mitigating Bias in Language Models through Direct Preference Optimization",
            "text": "Large Language Models (LLMs) have become pivotal in advancing natural language processing, yet their potential to perpetuate biases poses significant concerns. This paper introduces a new framework employing Direct Preference Optimization (DPO) to mitigate gender, racial, and religious biases in LLM-generated English text. By developing a loss function that favors less biased over biased completions, our approach cultivates a preference for respectful and non-discriminatory language in LLMs. We also contribute a manually designed dataset for training LLMs to recognize and correct biases. This dataset encompasses a diverse range of prompts paired with both biased and unbiased completions. Implementing this approach on the Microsoft Phi-2 model, we demonstrate substantial reductions in biased outputs as our model outperforms the baseline model on almost all bias benchmarks. Our model also achieves better performance compared to other open-source models on most benchmarks. By reducing biases in the language generated by the model, our study marks a significant step towards developing more ethical and socially responsible LLMs. We publicly release BiasDPO dataset on HuggingFace.",
            "score": 0.5688024999059451,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87744140625
        },
        {
            "corpus_id": "252118391",
            "title": "Efficient Gender Debiasing of Pre-trained Indic Language Models",
            "text": "The mitigation strategy is similar to the one used in Gira et al. (2022) They empirically show that fine-tuning a partially trainable model is more effective in mitigating gender bias than a completely trainable model. Therefore, we fine-tune our model by training only on less than one percent of the pa-rameters by unfreezing the layer norm, word, and positional embeddings. We use our novel genderbalanced Hindi dataset to fine-tune to these models. \n\nOur main contributions in this paper are as follows: \n\n\u2022 Creation of a Hindi dataset suitable to measure occupational gender bias in large language models. \n\n\u2022 Quantification of the bias in the model using the aforementioned dataset. \n\n\u2022 Fine-tuning the model efficiently by unfreezing less than 1 percent of the layers to mitigate the bias. \n\n2 Related Work Bolukbasi et al. (2016) was the first paper to quantify and mitigate bias in word embeddings. They used the WEAT test (Caliskan et al., 2017) to measure bias and mitigate it through a method called Hard-Debiasing. They quantified the bias by projecting embeddings into the gender subspace. The debiasing method tried to remove the projection of the subspace from those embeddings. Gonen and Goldberg (2019), however, showed that this method is ineffective in reducing bias. Finding out the gender bias in contextual embeddings was first done by Zhao et al. (2019). They used a templatebased method to predict the bias in pre-trained language models. Bartl et al. (2020a) quantified and mitigated bias in BERT-like models. Sun et al. (2019) demonstrates various mitigation techniques like data augmentation, genderswapping, and hard-debiasing based on the downstream tasks in NLProc. Meade et al. (2022) surveys various mitigation strategies used to reduce gender bias in pre-trained language models like dropout, fine-tuning, sentence debias, and iterative null spacing. Liang et al. (2020) used an approach similar to hard-debiasing for context-based representations.",
            "score": 0.568248138888841,
            "section_title": "Introduction",
            "char_start_offset": 2050,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 452
                },
                {
                    "start": 455,
                    "end": 507
                },
                {
                    "start": 510,
                    "end": 610
                },
                {
                    "start": 613,
                    "end": 688
                },
                {
                    "start": 691,
                    "end": 796
                },
                {
                    "start": 799,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1463
                },
                {
                    "start": 1464,
                    "end": 1535
                },
                {
                    "start": 1536,
                    "end": 1696
                },
                {
                    "start": 1697,
                    "end": 1884
                },
                {
                    "start": 1885,
                    "end": 1982
                }
            ],
            "ref_mentions": [
                {
                    "start": 54,
                    "end": 72,
                    "matchedPaperCorpusId": "248780268"
                },
                {
                    "start": 932,
                    "end": 955,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 1464,
                    "end": 1484,
                    "matchedPaperCorpusId": "225094152"
                },
                {
                    "start": 1536,
                    "end": 1553,
                    "matchedPaperCorpusId": "195316733"
                },
                {
                    "start": 1697,
                    "end": 1716,
                    "matchedPaperCorpusId": "239015827"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7958984375
        },
        {
            "corpus_id": "276902427",
            "title": "Gender Encoding Patterns in Pretrained Language Model Representations",
            "text": "Despite extensive research on bias in language models, much of the focus has been on identifying and measuring bias rather than comprehensively analyzing how it is embedded within the model's internal representations. Previous studies have explored bias in transformer-based models, developing metrics to quantify bias (Islam et al., 2016;May et al., 2019;Nangia et al., 2020;Nadeem et al., 2021;Felkner et al., 2023), implementing techniques to reduce it (Zhao et al., 2018a;Lauscher et al., 2021;Kaneko and Bollegala, 2021;Webster et al., 2020;Schick et al., 2021), and investigating its underlying causes (Bolukbasi et al., 2016;Kaneko et al., 2022). However, there remains a limited understanding of the mechanisms through which biases are encoded and how different training and fine-tuning processes influence these biases within model weights. \n\nTo address this gap, we use an informationtheoretic approach, specifically Minimum Description Length (MDL) probing proposed by Voita and Titov (2020), to explore how gender bias is encoded in various encoder-based architectures. By exam-ining different layers of PLMs, we identify where biases emerge and how fine-tuning and debiasing techniques impact these representations. \n\nOur work is inspired by Mendelson and Belinkov (2021) who studied the impact of debiasing techniques used to reduce the model's reliance on spurious correlations between data and labels in natural language inference on model's representations. In summary, our contributions are twofold: \n\n\u2022 We pinpoint the specific parts of encoder-based PLMs responsible for encoding gender information, highlighting critical layers where bias is most pronounced. \n\n\u2022 We assess the effect of various debiasing methods, demonstrating that pretrained debiasing objectives outperform post-hoc mitigation approaches in reducing encoded bias.",
            "score": 0.5675823589848419,
            "section_title": "Introduction",
            "char_start_offset": 1424,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 849
                },
                {
                    "start": 852,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1228
                },
                {
                    "start": 1231,
                    "end": 1474
                },
                {
                    "start": 1475,
                    "end": 1517
                },
                {
                    "start": 1520,
                    "end": 1679
                },
                {
                    "start": 1682,
                    "end": 1853
                }
            ],
            "ref_mentions": [
                {
                    "start": 339,
                    "end": 356,
                    "matchedPaperCorpusId": "85518027"
                },
                {
                    "start": 356,
                    "end": 376,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 376,
                    "end": 396,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 456,
                    "end": 476,
                    "matchedPaperCorpusId": "4952494"
                },
                {
                    "start": 476,
                    "end": 498,
                    "matchedPaperCorpusId": "237440429"
                },
                {
                    "start": 498,
                    "end": 525,
                    "matchedPaperCorpusId": "231698657"
                },
                {
                    "start": 546,
                    "end": 566,
                    "matchedPaperCorpusId": "232075876"
                },
                {
                    "start": 608,
                    "end": 632,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 632,
                    "end": 652,
                    "matchedPaperCorpusId": "248496913"
                },
                {
                    "start": 980,
                    "end": 1002,
                    "matchedPaperCorpusId": "214693050"
                },
                {
                    "start": 1255,
                    "end": 1284,
                    "matchedPaperCorpusId": "237454587"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72509765625
        },
        {
            "corpus_id": "264425921",
            "title": "DiFair: A Benchmark for Disentangled Assessment of Gender Knowledge and Bias",
            "text": "We also note that while debiasing techniques enhance gender fairness, they typically compromise the model's ability to retain factual gender information. \n\nOur contributions are as follows: (i) We introduce DIFAIR, a human curated language modeling dataset that aims at simultaneously measuring fairness and performance on gendered instances in pretrained language models; (ii) We propose gender invariance score, a novel metric that takes into account the dual objectives of a model's fairness and its ability to comprehend a given sentence with respect to its gender information; (iii) We test a number of Transformer based models on the DIFAIR dataset, finding that their overall score is significantly worse than human performance. However, we additionally find that larger language models perform better; and (iv) We observe that bias mitigation methods, while being capable of improving a model's gender fairness, damage the factual gender information encoded in their representations, reducing their usefulness in cases where actual gender information is required.1 2 The Task Preliminaries. Gender Knowledge implies the correct assignment of gender to words or entities in context. It involves a language model's aptitude to accurately detect and apply gender-related data such as gender-specific pronouns, names, historical and biological references, and coreference links. A well-performing model should comprehend gender cues and allocate the appropriate gendered tokens to fill the [MASK], guided by the sentence's context. Gender bias in language models refers to the manifestation of prejudiced or unfair treatment towards a particular gender when filling the [MASK] token, resulting from prior information and societal biases embedded in the training data. It arises when the model consistently favors one gender over another, irrespective of the contextual gender cues. \n\nBias Implications. The ability of language models in encoding unbiased gendered knowledge can have significant implications in terms of fairness, representation, and societal impact. Ensuring an equitable treatment of gender within language models is crucial to promote inclusivity while preventing the perpetuation of harmful biases, stereotypes, disparities, and marginalization of certain genders (Islam et al., 2016). Models that exhibit high levels of bias can have detrimental implications by causing representational harm to various groups and individuals.",
            "score": 0.5667429276233336,
            "section_title": "Introduction",
            "char_start_offset": 2155,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 156,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 1098
                },
                {
                    "start": 1099,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1535
                },
                {
                    "start": 1536,
                    "end": 1771
                },
                {
                    "start": 1772,
                    "end": 1885
                },
                {
                    "start": 1888,
                    "end": 1906
                },
                {
                    "start": 1907,
                    "end": 2070
                },
                {
                    "start": 2071,
                    "end": 2309
                },
                {
                    "start": 2310,
                    "end": 2451
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63818359375
        },
        {
            "corpus_id": "258174286",
            "title": "Improving Gender-Related Fairness in Sentence Encoders: A Semantics-Based Approach",
            "text": "Embedding-based language models learn stereotypical associations during the training phase, even if data are seemingly verified and safe [7]. Therefore, to mitigate gender bias and limit the internalisation of stereotypical conceptions, our approach aims to detect stereotyped entries in text corpora used for training language models, namely SNLI [9]. We explore two directions: removing stereotyped entries from the corpus, or compensating by adding counterfactual entries regarding gender. In this section, we describe in detail how to improve the fairness of a training corpus, and then test our intuition by retraining a sentence encoder on the new corpus obtained. Our goal is to improve the degree of fairness in the encoder, without losing accuracy in downstream tasks, by retraining it on a fairer and less stereotyped corpus of text. \n\nTo evaluate both properties (quality and fairness), we test the newly retrained models with SentEval [16] and SEAT [36], respectively, as described in Sect. 5. An overview of the adopted methodology is illustrated in Fig. 4.",
            "score": 0.5665190563687201,
            "section_title": "Gender Bias Reduction",
            "char_start_offset": 22120,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 843
                },
                {
                    "start": 846,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1070
                }
            ],
            "ref_mentions": [
                {
                    "start": 137,
                    "end": 140,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 348,
                    "end": 351,
                    "matchedPaperCorpusId": "14604520"
                },
                {
                    "start": 947,
                    "end": 951,
                    "matchedPaperCorpusId": "3932228"
                },
                {
                    "start": 961,
                    "end": 965,
                    "matchedPaperCorpusId": "85518027"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81201171875
        },
        {
            "corpus_id": "271902917",
            "title": "REFINE-LM: Mitigating Language Model Stereotypes via Reinforcement Learning",
            "text": "The success of (Large) Language Models (LMs) has led to a revolution in the domain of NLP, opening the door to numerous challenges. The emergence of LMs-based applications such as chatbots and text-based assistants with astounding capabilities has, on the one hand, sparked unprecedented enthusiasm within the research community [16,36]. However, it has motivated ethical concerns and araised questions about the risks this technology may pose to society, particularly algorithmic fairness and the proliferation of harmful stereotypical bias. Indeed, several studies have shown that LMs suffer from stereotypical biases, which can be detected, for instance, through Implicit Association Tests (IATs) [7]. These biases are still prevalent in recent LLMs such as ChatGPT, GPT4, etc., [26,40]. Figure 1 illustrates stereotypical biases (such as gender, ethnicity or religion) that can be observed when prompting ChatGPT (more examples and analysis with LLama2, Mistral and GPT4 are provided in Section 1 of the supplementary material). These findings ask for a thorough investigation of stereotypical bias in LMs, and for methods to mitigate their impact, perpetuation or even their exacerbation in various academic, societal and industrial applications. \n\nWhile some work has been proposed to mitigate bias in LMs, it remains challenging for several reasons. Firstly, metrics are highly task-dependent, i.e., quantifying stereotypical bias is highly dependent on the application at hand, meaning that the methods used to measure bias in LMs for one kind of bias can not be directly applied to other biases. For example, mitigation metrics for gender bias are typically not directly applicable to nationality-based or ethnic bias, e.g., gender bias mitigation relies on pronoun completion or the existence of sufficiently gendered phrases within corpora [8,31,47]. Secondly, even with adequate methods to measure bias, in practice, there is often a trade-off between bias mitigation and model performance [21], which can have a negative impact despite the bias removal. Namely, removing bias from a LM may risk deteriorating its performance on downstream applications such as questionanswering [49].",
            "score": 0.5651359864534222,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1251
                },
                {
                    "start": 1254,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1861
                },
                {
                    "start": 1862,
                    "end": 2066
                },
                {
                    "start": 2067,
                    "end": 2196
                }
            ],
            "ref_mentions": [
                {
                    "start": 333,
                    "end": 336,
                    "matchedPaperCorpusId": "212747830"
                },
                {
                    "start": 700,
                    "end": 703,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 782,
                    "end": 786,
                    "matchedPaperCorpusId": "261276445"
                },
                {
                    "start": 786,
                    "end": 789,
                    "matchedPaperCorpusId": "268417107"
                },
                {
                    "start": 1851,
                    "end": 1854,
                    "matchedPaperCorpusId": "231698886"
                },
                {
                    "start": 1854,
                    "end": 1857,
                    "matchedPaperCorpusId": "85518027"
                },
                {
                    "start": 1857,
                    "end": 1860,
                    "matchedPaperCorpusId": "4952494"
                },
                {
                    "start": 2002,
                    "end": 2006,
                    "matchedPaperCorpusId": "248780440"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72265625
        },
        {
            "corpus_id": "265150272",
            "title": "Step by Step to Fairness: Attributing Societal Bias in Task-oriented Dialogue Systems",
            "text": "For bias mitigation, an intuitive method is called Counterfactual Data Augmentation (CDA). People applied wordbased augmentation that augmented all the demographics mentioned in texts, and trained models on the augmented data to equalize the word usage imbalance problem in the dataset to mitigate bias (Maudslay et al. 2019;Liu et al. 2019;Zmigrod et al. 2019). In addition to CDA, there are various training-based methods to mitigate bias, for example, Dinan et al. (2020) finetune models with controllable prefixes to equalize the word usage for male and female. Liu et al. (2020b) proposed a novel adversarial method that disentangles text into semantic and gender features to mitigate bias. Liang et al. (2021) employed the concept of Null space projection (May et al. 2019) to eliminate gender feature in models. More recently, Sharma, Dey, and Sinha (2022) used hand-crafted prompts (Li and Liang 2021) to mitigate bias in machine translation. Sheng et al. (2020) also proposed to generate prompts to equalize gender and race's disparity in the dialogue generation task. \n\nEnd-to End Task-oriented Dialogue Task-oriented dialogue (TOD) systems help people resolve specific problems in domains such as restaurant booking and flight booking. TOD systems traditionally can be decomposed into several components: Natural Language Understanding, Dialogue State Tracking, Policy Learning, and Natural Language Gen-eration. (Ni et al. 2021;Liu and Lane 2018). Historically, these modules were trained individually (Asri, He, and Suleman 2016;Bordes and Weston 2016;Liu and Lane 2018;Lin et al. 2021). However, individual components have the drawback of being non-differentiable. Pretrained large language models (LLMs) provide an end-to-end solution, and have been shown to not only resolve the aforementioned problems, but also improve the performance of TOD systems (Hosseini-Asl et al. 2020a).",
            "score": 0.5642473185802919,
            "section_title": "Related Works",
            "char_start_offset": 4623,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 91,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1077
                },
                {
                    "start": 1080,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1423
                },
                {
                    "start": 1424,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1600
                },
                {
                    "start": 1601,
                    "end": 1678
                },
                {
                    "start": 1679,
                    "end": 1896
                }
            ],
            "ref_mentions": [
                {
                    "start": 303,
                    "end": 325,
                    "matchedPaperCorpusId": "202541569"
                },
                {
                    "start": 341,
                    "end": 361,
                    "matchedPaperCorpusId": "184486914"
                },
                {
                    "start": 455,
                    "end": 474,
                    "matchedPaperCorpusId": "207852875"
                },
                {
                    "start": 566,
                    "end": 584,
                    "matchedPaperCorpusId": "221970809"
                },
                {
                    "start": 762,
                    "end": 779,
                    "matchedPaperCorpusId": "85518027"
                },
                {
                    "start": 890,
                    "end": 909,
                    "matchedPaperCorpusId": "230433941"
                },
                {
                    "start": 951,
                    "end": 970,
                    "matchedPaperCorpusId": "218470535"
                },
                {
                    "start": 1424,
                    "end": 1440,
                    "matchedPaperCorpusId": "234342691"
                },
                {
                    "start": 1440,
                    "end": 1458,
                    "matchedPaperCorpusId": "46918092"
                },
                {
                    "start": 1565,
                    "end": 1583,
                    "matchedPaperCorpusId": "46918092"
                },
                {
                    "start": 1583,
                    "end": 1598,
                    "matchedPaperCorpusId": "237485098"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80859375
        },
        {
            "corpus_id": "259950823",
            "title": "Unveiling Gender Bias in Terms of Profession Across LLMs: Analyzing and Addressing Sociological Implications",
            "text": "Addressing gender bias in AI language models like GPT-2 requires a proactive approach that focuses on developing strategies to reduce bias and promote fairness and inclusivity. This section outlines potential strategies for mitigating gender bias in GPT-2. \n\nDiverse and Representative Training Data: Enhancing the diversity and representativeness of the training data is crucial for reducing gender bias in GPT-2. This involves incorporating a wide range of texts from diverse sources that accurately reflect the diversity of gender identities, experiences, and perspectives. Careful attention should be given to ensure balanced representation across genders and avoid the overrepresentation of stereotypes or biased narratives. \n\nData Preprocessing Techniques: Employing effective data preprocessing techniques can help mitigate gender bias in GPT-2. This includes identifying and removing biased language or problematic gender-related associations from the training data. Preprocessing techniques may involve debiasing methods, such as reweighting or resampling the data, to ensure more equitable representation. \n\nAlgorithmic Debiasing: Implementing algorithmic debiasing techniques can help reduce gender bias in GPT-2. These techniques aim to modify the model's learning process and outputs to align with fairness objectives. Approaches like adversarial training, where the model is trained to resist biased associations, or constraint-based methods, where specific fairness constraints are imposed, can be explored to mitigate gender bias. \n\nEvaluation and Bias Detection: Developing comprehensive evaluation metrics and tools to assess gender bias in GPT-2 is crucial. Automated tools can help identify and quantify biases in the model's generated text, aiding researchers in understanding the extent of the bias. Open evaluation frameworks and benchmark datasets should be established to facilitate comparative analysis and progress in reducing gender bias. \n\nUser Control and Transparency: Empowering users with control over the AI-generated outputs can contribute to reducing gender bias. Providing users with options to customize and influence the behavior of the model can help mitigate biases that may not be captured during training. Transparency in the functioning of AI systems is also essential, allowing users to understand how gender biases are addressed and enabling them to make informed decisions. \n\nInterdisciplinary Collaboration: Collaboration between computer scientists, sociologists, and ethicists is crucial for addressing gender bias in GPT-2.",
            "score": 0.564055623471216,
            "section_title": "VII. STRATEGIES FOR REDUCING GENDER BIAS IN GPT-2",
            "char_start_offset": 23401,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 256
                },
                {
                    "start": 259,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 729
                },
                {
                    "start": 732,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 974
                },
                {
                    "start": 975,
                    "end": 1115
                },
                {
                    "start": 1118,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1546
                },
                {
                    "start": 1549,
                    "end": 1676
                },
                {
                    "start": 1677,
                    "end": 1821
                },
                {
                    "start": 1822,
                    "end": 1966
                },
                {
                    "start": 1969,
                    "end": 2099
                },
                {
                    "start": 2100,
                    "end": 2248
                },
                {
                    "start": 2249,
                    "end": 2420
                },
                {
                    "start": 2423,
                    "end": 2574
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7275390625
        },
        {
            "corpus_id": "272464141",
            "title": "AGR: Age Group fairness Reward for Bias Mitigation in LLMs",
            "text": "Large language models (LLMs) used in various fields can perpetuate age biases, affecting career opportunities and healthcare [1]. Unlike fixed gender and racial biases, age bias is continuous and evolving. Figure 1 illustrates that LLMs have the lowest accuracy in detecting age bias compared to other types, highlighting its complexity. Medium-sized LLMs, such as BERT [2] and GPT-1 [3], generally have under a billion parameters and face two types of social biases: internal, present in the model's pre-trained outputs, and external, affecting downstream task predictions. Internal debiasing methods address biases in a pre-trained model's outputs through three main approaches: pre-processing [4], in-training [5], and post-processing [6]. External debiasing methods tackle biases in model predictions during downstream tasks, using data-centered approaches [7] to integrate fairness goals during training. Large-scale LLMs like GPT-3 encounter greater debiasing challenges due to size and complexity, often addressed through preference alignment [8] and prompt engineering techniques [9]. \n\nUnlike gender and racial biases, age bias is challenging due to its dynamic nature, complicating counterfactual and contrastive methods. Research on age bias mitigation remains limited [10]. \n\nAdditionally, common fine-tuning methods for LLMs include instruction-based fine-tuning [11] and reinforcement learning with human feedback [12]. However, no instruction-based datasets address age bias, and these methods do not target social biases, leading to potential performance discrepancies across age groups. \n\nTo address this challenge, we revised and expanded BBQ [13] and ISB [14] datasets and manually annotated them to create age preference and instruction fine-tuning datasets for age bias. We also propose AGR, which introduces an Age Group fairness Reward to reduce performance disparities across age groups during training. \n\nIn summary, our contributions are as follows: \n\n\u2022 We construct age bias preference and instruction fine-tuning datasets for bias evaluation in LLMs.",
            "score": 0.5629597325675082,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 1092
                },
                {
                    "start": 1095,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1285
                },
                {
                    "start": 1288,
                    "end": 1433
                },
                {
                    "start": 1434,
                    "end": 1603
                },
                {
                    "start": 1606,
                    "end": 1791
                },
                {
                    "start": 1792,
                    "end": 1927
                },
                {
                    "start": 1930,
                    "end": 1975
                },
                {
                    "start": 1978,
                    "end": 2078
                }
            ],
            "ref_mentions": [
                {
                    "start": 125,
                    "end": 128,
                    "matchedPaperCorpusId": "59158788"
                },
                {
                    "start": 370,
                    "end": 373,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 696,
                    "end": 699,
                    "matchedPaperCorpusId": "259095603"
                },
                {
                    "start": 713,
                    "end": 716,
                    "matchedPaperCorpusId": "248780440"
                },
                {
                    "start": 738,
                    "end": 741,
                    "matchedPaperCorpusId": "258740820"
                },
                {
                    "start": 861,
                    "end": 864,
                    "matchedPaperCorpusId": "259859044"
                },
                {
                    "start": 1050,
                    "end": 1053,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1376,
                    "end": 1380,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 1428,
                    "end": 1432,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1661,
                    "end": 1665,
                    "matchedPaperCorpusId": "239010011"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.477294921875
        },
        {
            "corpus_id": "258486946",
            "title": "Bias assessment for experts in discrimination, not in computer science",
            "text": "Blodgett et al. ( 2021) examine four sets of contrastive sentences to evaluate bias in language models and apply a method -originating from the social sciences-to inventory a range of pitfalls that threaten these benchmarks' validity as measurement models for stereotyping. They find that these benchmarks frequently lack clear articulations of what is being measured, and they highlight a range of ambiguities and unstated assumptions that affect how these benchmarks conceptualize and operationalize stereotyping. N\u00e9v\u00e9ol et al. (2022) propose how to overcome some of these challenges by taking a culturally aware standpoint and a curation methodology when designing such benchmarks. \n\nWith respect to mitigation, Brunet et al. (2019) show that debiasing techniques are more effective when applied to the texts wherefrom embeddings are induced, rather than applying them directly in the already induced word embeddings. Prost et al. (2019) show that overly simplistic mitigation strategies actually worsen fairness metrics in downstream tasks. More insightful mitigation strategies are required to actually debias the whole embedding and not only those words used to diagnose bias. However, debiasing input texts works best. Curating texts can be done automatically (Gonen et al., 2019) but this has yet to prove that it does not make matters worse. It is better that domain experts devise curation strategies for each particular case. \n\nIn spite of these well-founded critiques, work on bias in word embeddings and language models still revolves mainly around metrics and methods, and not so much on the participation of experts in the process of diagnosis. That is why we feel the need to facilitate the involvement of experts in bias assessment processes, so that the focus can be moved from technicalities to the problem itself. \n\nIn recent years, with the consolidation of bias assessment techniques, multiple frameworks have been developed to facilitate access to those techniques. We provide a description of some frameworks in Appendix B, and an overview of those with a graphical interface in Table 1.",
            "score": 0.5598240356049484,
            "section_title": "Criticisms to metric-centered approaches",
            "char_start_offset": 11784,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 684
                },
                {
                    "start": 687,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1225
                },
                {
                    "start": 1226,
                    "end": 1350
                },
                {
                    "start": 1351,
                    "end": 1436
                },
                {
                    "start": 1439,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1833
                },
                {
                    "start": 1836,
                    "end": 1988
                },
                {
                    "start": 1989,
                    "end": 2111
                }
            ],
            "ref_mentions": [
                {
                    "start": 516,
                    "end": 536,
                    "matchedPaperCorpusId": "248780290"
                },
                {
                    "start": 715,
                    "end": 735,
                    "matchedPaperCorpusId": "52946942"
                },
                {
                    "start": 921,
                    "end": 940,
                    "matchedPaperCorpusId": "199501874"
                },
                {
                    "start": 1267,
                    "end": 1287,
                    "matchedPaperCorpusId": "73729169"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5712890625
        },
        {
            "corpus_id": "195316733",
            "title": "Mitigating Gender Bias in Natural Language Processing: Literature Review",
            "text": "Second, most gender debiasing methods have only been empirically verified in limited applications (Zhang et al., 2018;Zhao et al., 2017), and it is not clear that these methods can generalize to other tasks or models. Third, we note that certain debiasing techniques may introduce noise into a NLP model, causing performance degradation. Finally, hand-craft debiasing approaches may unintentionally encode the implicit bias of the developers. \n\nBelow, we identify a few future directions. \n\nMitigating Gender Bias in Languages Beyond English. With few exceptions (Vanmassenhove et al., 2018;Prates et al., 2018), prior work has focused on mitigating gender bias in the English language. Future work can look to apply existing methods or devise new techniques towards mitigating gender bias in other languages as well. However, such a task is not trivial. Methods such as gender-swapping are relatively easy in English because English does not distinguish gender linguistically. However, in languages such as Spanish, each noun has its own gender and corresponding modifiers of the noun need to align with the gender of the noun. To perform gender-swapping in such languages, besides swapping those gendered nouns, we also need to change the modifiers. Non-Binary Gender Bias. With few exceptions (Manzini et al., 2019), work on debiasing in NLP has assumed that the protected attribute being discriminated against is binary. Non-binary genders (Richards et al., 2016) as well as racial biases have largely been ignored in NLP and should be considered in future work. Interdisciplinary Collaboration. As mentioned in Section 1, gender bias is not a problem that is unique to NLP; other fields in computer science such as data mining, machine learning, and security also study gender bias (Calders and Verwer, 2010;Feldman et al., 2015;Hardt et al., 2016;Misra et al., 2016;Kleinberg et al., 2016;Pleiss et al., 2017;Beutel et al., 2017;Kilbertus et al., 2017). Many of these technical methods could be applicable to NLP yet to our knowledge have not been studied.",
            "score": 0.5594384148385425,
            "section_title": "Conclusion and Future Directions",
            "char_start_offset": 29157,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 442
                },
                {
                    "start": 445,
                    "end": 488
                },
                {
                    "start": 491,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 854
                },
                {
                    "start": 855,
                    "end": 977
                },
                {
                    "start": 978,
                    "end": 1128
                },
                {
                    "start": 1129,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1424
                },
                {
                    "start": 1425,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1599
                },
                {
                    "start": 1600,
                    "end": 1959
                },
                {
                    "start": 1960,
                    "end": 2062
                }
            ],
            "ref_mentions": [
                {
                    "start": 98,
                    "end": 118,
                    "matchedPaperCorpusId": "9424845"
                },
                {
                    "start": 118,
                    "end": 136,
                    "matchedPaperCorpusId": "1389483"
                },
                {
                    "start": 563,
                    "end": 591,
                    "matchedPaperCorpusId": "76654723"
                },
                {
                    "start": 1444,
                    "end": 1467,
                    "matchedPaperCorpusId": "29985722"
                },
                {
                    "start": 1787,
                    "end": 1813,
                    "matchedPaperCorpusId": "12856537"
                },
                {
                    "start": 1813,
                    "end": 1834,
                    "matchedPaperCorpusId": "2077168"
                },
                {
                    "start": 1834,
                    "end": 1853,
                    "matchedPaperCorpusId": "7567061"
                },
                {
                    "start": 1853,
                    "end": 1872,
                    "matchedPaperCorpusId": "3039286"
                },
                {
                    "start": 1872,
                    "end": 1895,
                    "matchedPaperCorpusId": "12845273"
                },
                {
                    "start": 1895,
                    "end": 1915,
                    "matchedPaperCorpusId": "75455"
                },
                {
                    "start": 1915,
                    "end": 1935,
                    "matchedPaperCorpusId": "24990444"
                },
                {
                    "start": 1935,
                    "end": 1958,
                    "matchedPaperCorpusId": "3352595"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.404052734375
        },
        {
            "corpus_id": "259729217",
            "title": "Debiasing Intrinsic Bias and Application Bias Jointly via Invariant Risk Minimization (Student Abstract)",
            "text": "With remarkable success of pretrained language models (PLMs) in many natural language processing (NLP) tasks, undesired stereotypes have been taken seriously. Take the cloze-style task as an example, as a result of gender occupational discrimination, the PLM fills in [MASK] of the sentence \"The boy/girl got a job as [MASK]\" with \"doctor/nurse\" respectively. Such demographic biases and social stereotypes would be inherited or amplified in downstream NLP tasks. Meanwhile, the fine-tuning procedure in downstream task even produces new biases, which two kinds of biases may lead to undesirable results. \n\nThe static word embedding debiasing method was firstly proposed in (Bolukbasi et al. 2016), and many debiasing methods have emerged in recent years. According to the stage of debiasing technique applied, existing methods fall into three main categories: (1) Pretraining: counterfactual data augmentation and increasing dropout parameters techniques are widely used in pretraining stage. (2) Post-hoc: Sent-debias (Liang et al. 2020) mitigates biases by removing the estimated gender-direction subspace from sentence (3) Fine-tuning: Auto-Debias (Guo, Yang, and Abbasi 2022) automatically searches biased prompts and devises a distribution alignment loss for bias mitigation. Context-Debias (Kaneko and Bollegala 2021) designs a loss function to encourage the stereotype words and genderspecific words to be orthogonal, which, however, degrades debiasing performance due to over-reliance on orthogonality assumptions. Except for Context-Debias, the above debiasing works consider that the debiasing process is independent of downstream fine-tuning tasks, leading to a gap between intrinsic bias and application bias. \n\nInstead of separating debiasing and fine-tuning in prior studies, we present an approach to simultaneously mitigate the intrinsic biases in PLMs and the application bias in downstream tasks. Specifically, we start with Invariant Risk Minimization (IRM) theory (Arjovsky et al. 2019) to consider both biases.",
            "score": 0.5593941870097724,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 604
                },
                {
                    "start": 607,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1523
                },
                {
                    "start": 1524,
                    "end": 1722
                },
                {
                    "start": 1725,
                    "end": 1915
                },
                {
                    "start": 1916,
                    "end": 2032
                }
            ],
            "ref_mentions": [
                {
                    "start": 674,
                    "end": 697,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 1020,
                    "end": 1038,
                    "matchedPaperCorpusId": "207996257"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.845703125
        },
        {
            "corpus_id": "237440429",
            "title": "Sustainable Modular Debiasing of Language Models",
            "text": "Unfair stereotypical biases (e.g., gender, racial, or religious biases) encoded in modern pretrained language models (PLMs) have negative ethical implications for widespread adoption of state-of-the-art language technology. To remedy for this, a wide range of debiasing techniques have recently been introduced to remove such stereotypical biases from PLMs. Existing debiasing methods, however, directly modify all of the PLMs parameters, which -- besides being computationally expensive -- comes with the inherent risk of (catastrophic) forgetting of useful language knowledge acquired in pretraining. In this work, we propose a more sustainable modular debiasing approach based on dedicated debiasing adapters, dubbed ADELE. Concretely, we (1) inject adapter modules into the original PLM layers and (2) update only the adapters (i.e., we keep the original PLM parameters frozen) via language modeling training on a counterfactually augmented corpus. We showcase ADELE, in gender debiasing of BERT: our extensive evaluation, encompassing three intrinsic and two extrinsic bias measures, renders ADELE, very effective in bias mitigation. We further show that -- due to its modular nature -- ADELE, coupled with task adapters, retains fairness even after large-scale downstream training. Finally, by means of multilingual BERT, we successfully transfer ADELE, to six target languages.",
            "score": 0.5591747269355064,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95068359375
        },
        {
            "corpus_id": "265456057",
            "title": "Evaluating Large Language Models through Gender and Racial Stereotypes",
            "text": "We have proposed a new framework for evaluating LLMs based on their capability to handle biases in their generated output and have through our systemic analysis observed that all popular language models exhibit bias of some kind. \n\nWhen dealing with gender bias, language models are likely biased to identify and label a profession as male, rather than maintaining neutrality. While newer models perform much better than their predecessors there remains room for improvement. \n\nWhen comparing racial biases, we see GPT 3.5, the bestperforming model and the model with the least gender bias, is consistent with its outputs and its behaviour of regurgitating both positive and negative stereotypes for all genders, which remains concerning. \n\nFor Racial Biases, we see how the responses generated for the same race are similar and conform to stereotypes, while for a given profession, the similarity is lower than expected ( 1.0). \n\nIn future work, we outline to explore the racial bias framework to other recent models and evaluate their performance as compared to GPT 3.5. While this research has been able to assess the prevalence of linguistic markers that portray bias, we also wish to conduct a field study, to understand the impact of these biases directly onto human behaviour.",
            "score": 0.5588153441474595,
            "section_title": "Conclusion and Future Work",
            "char_start_offset": 12253,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 229
                },
                {
                    "start": 232,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 475
                },
                {
                    "start": 478,
                    "end": 738
                },
                {
                    "start": 741,
                    "end": 928
                },
                {
                    "start": 931,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1283
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.405029296875
        },
        {
            "corpus_id": "267770177",
            "title": "Potential and Challenges of Model Editing for Social Debiasing",
            "text": "Debiasing Addressing social bias in language models is an ongoing challenge that has received significant attention. Strategies for mitigating bias in language models can be classified based on different stages of the model workflow: Preprocessing techniques aim to detect and eliminate bias and unfairness early on, either within the dataset (Zmigrod et al., 2019;Dinan et al., 2020;Abid et al., 2021;Qian et al., 2022a;Ghanbarzadeh et al., 2023) or prompt (Mattern et al., 2022;Fatemi et al., 2023;Yang et al., 2023). In-training bias mitigation techniques focus on reducing bias and unfairness during model training, by adjusting model architecture (Bartl et al., 2020;Han et al., 2022), modifying loss functions (Liu et al., 2020;Webster et al., 2020;Ouyang et al., 2022;Woo et al., 2023;Park et al., 2023;Zhou et al., 2023;Li et al., 2023), or selectively updating parameters (Qian et al., 2022b;Ranaldi et al., 2023;Yu et al., 2023). \n\nIntraprocessing approaches alter decoding behavior (Saunders et al., 2022;Meade et al., 2023;Kim et al., 2023;Chung et al., 2023;Hallinan et al., 2023) without additional training or fine-tuning. \n\nPost-processing techniques primarily adjust model outputs to address bias and unfairness, without directly accessing the model itself (He et al., 2021;Tokpo and Calders, 2022;Majumder et al., 2022;Dhingra et al., 2023). However, effectively modifying bias in pre-trained large language models while minimizing disruption to the model's capabilities remains largely unexplored. \n\nModel Editing To address inaccuracies and biases in Large Language Models, various model editing techniques have been developed for efficient post-training adjustments.",
            "score": 0.5585385263552101,
            "section_title": "Related Work",
            "char_start_offset": 4897,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 939
                },
                {
                    "start": 942,
                    "end": 1137
                },
                {
                    "start": 1140,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1516
                },
                {
                    "start": 1519,
                    "end": 1687
                }
            ],
            "ref_mentions": [
                {
                    "start": 343,
                    "end": 365,
                    "matchedPaperCorpusId": "184486914"
                },
                {
                    "start": 365,
                    "end": 384,
                    "matchedPaperCorpusId": "207852875"
                },
                {
                    "start": 402,
                    "end": 421,
                    "matchedPaperCorpusId": "249062690"
                },
                {
                    "start": 421,
                    "end": 447,
                    "matchedPaperCorpusId": "259859044"
                },
                {
                    "start": 480,
                    "end": 500,
                    "matchedPaperCorpusId": "238582879"
                },
                {
                    "start": 500,
                    "end": 518,
                    "matchedPaperCorpusId": "253446867"
                },
                {
                    "start": 652,
                    "end": 672,
                    "matchedPaperCorpusId": "225094152"
                },
                {
                    "start": 672,
                    "end": 689,
                    "matchedPaperCorpusId": "247694107"
                },
                {
                    "start": 716,
                    "end": 734,
                    "matchedPaperCorpusId": "204838020"
                },
                {
                    "start": 755,
                    "end": 775,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 775,
                    "end": 792,
                    "matchedPaperCorpusId": "258537309"
                },
                {
                    "start": 792,
                    "end": 810,
                    "matchedPaperCorpusId": "257079734"
                },
                {
                    "start": 810,
                    "end": 828,
                    "matchedPaperCorpusId": "259370743"
                },
                {
                    "start": 828,
                    "end": 844,
                    "matchedPaperCorpusId": "259342087"
                },
                {
                    "start": 881,
                    "end": 901,
                    "matchedPaperCorpusId": "249062690"
                },
                {
                    "start": 922,
                    "end": 938,
                    "matchedPaperCorpusId": "259859034"
                },
                {
                    "start": 993,
                    "end": 1016,
                    "matchedPaperCorpusId": "233240748"
                },
                {
                    "start": 1035,
                    "end": 1052,
                    "matchedPaperCorpusId": "254926596"
                },
                {
                    "start": 1052,
                    "end": 1071,
                    "matchedPaperCorpusId": "259096160"
                },
                {
                    "start": 1071,
                    "end": 1093,
                    "matchedPaperCorpusId": "252734135"
                },
                {
                    "start": 1274,
                    "end": 1291,
                    "matchedPaperCorpusId": "237634972"
                },
                {
                    "start": 1291,
                    "end": 1315,
                    "matchedPaperCorpusId": "246210255"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90380859375
        },
        {
            "corpus_id": "237442178",
            "title": "Hi, my name is Martha: Using names to measure and mitigate bias in generative dialogue models",
            "text": "We investigate several sources of bias in large dialogue models, and employ a few methods to reduce bias by gender and race/ethnicity in Blender-Bot while maintaining similar levels of performance. We find that the name-scrambling model most effectively reduces bias as measured by a gender bias classifier and most closely matches the token distribution across female/male self-chats; the controlled-generation model is rated by crowdsourced workers as being most preferable and most human; and the unlikelihood-training model has the lowest gender-classifier bias on a downstream task where one of the speakers explicitly states their gender. \n\nPractitioners should choose mitigation techniques that match their use case: the namescrambling method might be most applicable for a model where invariance to names is of most essential importance (e.g., in task oriented dialogue, when scheduling meetings with individuals); the controlled-generation method might be ideal for reducing bias on dialogue while ensuring no loss of performance; and the unlikelihoodtraining method may be best for suppressing gender bias on more than just names in a generalizable fashion. Future extensions could experiment with bias-mitigation techniques beyond these three or measure these techniques' performance on additional forms of bias in language.",
            "score": 0.5579249913172559,
            "section_title": "Conclusion",
            "char_start_offset": 31686,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 644
                },
                {
                    "start": 647,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1335
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89208984375
        },
        {
            "corpus_id": "269307293",
            "title": "Scaling Implicit Bias Analysis across Transformer-Based Language Models through Embedding Association Test and Prompt Engineering",
            "text": "Sun et al. [19] explored the presence of gender bias within NLP systems and identified various underlying biases such as lexical, semantic, syntactic, and pragmatic, which all contribute to the overall gender bias. The research examined different strategies for identifying and reducing gender bias, including enhancing datasets, creating counterfactual datasets, employing adversarial training techniques, and applying post processing corrections. Despite these efforts, the study pointed out significant shortcomings, such as the reliance on debiasing strategies that operate in isolation and the scarcity of tests in real-world scenarios. It was also noted that manually crafted debiasing methods might inadvertently embed additional biases. While strides are being made toward mitigating gender bias in NLP, there is a clear need for more thorough and scientifically backed methods to tackle it effectively. Addressing biases in both data representation and algorithmic approaches continues to be a critical hurdle. \n\nSamin et al. [20] introduced two strategies for efficiently summarizing numerous arguments or viewpoints. The initial approach involves refining pre-trained language models (PLMs) through prompt engineering, whereas the second combines prompt-based learning with a mix of argument-keypoint pairs and a classification system. Testing was conducted within specific domains and across different ones. Nonetheless, the second strategy underperformed, primarily because of PLMs' challenges with interpreting negation. This study progresses the field by demonstrating how to distill arguments into succinct points but also underlines the necessity to acknowledge the constraints of PLMs in language processing tasks. As these models become increasingly sophisticated, identifying their weaknesses is crucial for the development of more effective and dependable language processing tools. \n\nGupta et al. [21] introduced a sentiment classification method for evaluating movie reviews as either positive or negative. This method uses BERT embeddings along with BiLSTM-BiGRU and 1D CNN models, and on the IMDB dataset, it shows impressive accuracy and AUC scores. Despite these achievements, the technique is primarily limited to binary classification and lacks detailed explanations for the selection of model parameters. The reliance on a single dataset also raises questions about the broader applicability of the findings.",
            "score": 0.5571882635624985,
            "section_title": "Related Work",
            "char_start_offset": 16910,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 1019
                },
                {
                    "start": 1022,
                    "end": 1127
                },
                {
                    "start": 1128,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1419
                },
                {
                    "start": 1420,
                    "end": 1534
                },
                {
                    "start": 1535,
                    "end": 1732
                },
                {
                    "start": 1733,
                    "end": 1903
                },
                {
                    "start": 1906,
                    "end": 2029
                },
                {
                    "start": 2030,
                    "end": 2175
                },
                {
                    "start": 2176,
                    "end": 2334
                },
                {
                    "start": 2335,
                    "end": 2438
                }
            ],
            "ref_mentions": [
                {
                    "start": 1919,
                    "end": 1923,
                    "matchedPaperCorpusId": "248258722"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7724609375
        },
        {
            "corpus_id": "256390197",
            "title": "How Far Can It Go? On Intrinsic Gender Bias Mitigation for Text Classification",
            "text": "The use of pretrained language models has seen a surge in popularity as a result of state-of-the-art performances that have been achieved with these models on various tasks. Consequently, there has been a growing interest in exploring how gender bias pertains in these models (Garrido-Mu\u00f1oz et al., 2021). Pretrained language models are used in two distinct phases: the pretraining phase and the finetuning phase. The pretraining phase typically involves training a model on a generic task such as masked language modeling on a diverse set of text corpora. In the finetuning phase, the pretrained model can be adapted for a specific task, such as sentiment analysis, by training on a domain-specific corpus. Owing to the unique way of using pretrained models, bias generally manifests in two forms: intrinsic bias and extrinsic bias. Intrinsic bias refers to bias that inherently exists in pretrained language models whereas extrinsic bias is used to refer to bias that exists in downstream models that are based on the pretrained model. Since the success of downstream NLP tasks has mostly been dependent on pretrained models, it is intuitive to assume that bias in intrinsic models will compromise fairness in downstream tasks. Only recently have more indepth examinations been done to investigate this assumption (Steed et al., 2022;Orgad et al., 2022;Kurita et al., 2019). However, conclusions have been inconsistent and the confounding effects of bias mitigation techniques remain unknown.\n\nThe main focus of this work is to investigate the impact of intrinsic bias on extrinsic fairness and if techniques to mitigate intrinsic bias actually resolve bias or only mask it. We develop a probe to uncover intrinsic bias by determining the amount of gender information in a word embedding using a classifier. Bearing in mind how abstractly and improperly intrinsic bias has been defined (Blodgett et al., 2020), coupled with the discovery that results from different metrics for intrinsic bias in many cases do not correlate (Delobelle et al., 2022), we find this probe effective as an extra step in evaluating the efficacy of these mitigation strategies. We realize from this study that how intrinsic bias has been measured and",
            "score": 0.5563062770450058,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1888,
                    "end": 1911,
                    "matchedPaperCorpusId": "218971825"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5087890625
        },
        {
            "corpus_id": "264590474",
            "title": "She had Cobalt Blue Eyes: Prompt Testing to Create Aligned and Sustainable Language Models",
            "text": "With increasing research in LLMs in recent years, several works and guidelines have been proposed to address the issue of alignment in data curation, algorithmic development, and model testing (Zhao et al. 2023). The presence of bias in LLMs reflects unfair and systematic discrimination observed in the outcomes of their systems (Garrido-Mu\u00f1oz et al. 2021). A recent study (Ranaldi et al. 2023) confirms that the LLaMA and OPT families show bias in gender, race, religion, and profession. These biases often stem from the training data used by these systems, which include societal and cultural favouritism (Raza et al. 2023). \n\nState-of-the-art research has proposed effective methods for mitigating biases in natural language processing (NLP) models (Raza et al. 2023). Bolukbasi et al. (2016a) focus on removing gender bias from word embeddings, while Qian et al. (2021) introduce counterfactual training examples to reduce bias in text classification models. Hardt, Price, and Srebro (2016) present a technique to enforce equality constraints in ML models, applicable to debiasing language models. Zhang, Lemoine, and Mitchell (2018) propose adversarial learning to mitigate bias by training models against biased adversarial examples. One common approach to uncover bias patterns involves statistical methods (Bolukbasi et al. 2016b), like words linked disproportionately to specific genders, races, or demographics. Efforts have extended beyond NLP, encompassing bias across machine learning. FairnessGAN (Sattigeri et al. 2019), Aequitas (Saleiro et al. 2018), Themis-ml (Bantilan 2017), Fairlearn (Weerts et al. 2023), Google's What-If Tool (Wexler et al. 2019), and AI Fairness 360 (Bellamy et al. 2018) detect and alleviate biases in classifiers.",
            "score": 0.5557973838329207,
            "section_title": "Related Works",
            "char_start_offset": 5215,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 627
                },
                {
                    "start": 630,
                    "end": 772
                },
                {
                    "start": 773,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1422
                },
                {
                    "start": 1423,
                    "end": 1499
                },
                {
                    "start": 1500,
                    "end": 1757
                }
            ],
            "ref_mentions": [
                {
                    "start": 193,
                    "end": 211,
                    "matchedPaperCorpusId": "248496292"
                },
                {
                    "start": 330,
                    "end": 357,
                    "matchedPaperCorpusId": "233832881"
                },
                {
                    "start": 773,
                    "end": 797,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 856,
                    "end": 874,
                    "matchedPaperCorpusId": "236459953"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3857421875
        },
        {
            "corpus_id": "170078973",
            "title": "Reducing Gender Bias in Word-Level Language Models with a Gender-Equalizing Loss Function",
            "text": "In this research, we propose a new approach for mitigating gender bias in neural language models and empirically show its effectiveness in reducing bias as measured with different evaluation metrics. Our research also highlights the fact that debiasing the model with bias penalties in the loss function is an effective method. We emphasize that loss function based debiasing is powerful and generalizable to other downstream NLP applications. The research also reinforces the idea that geometric debiasing of the word embedding is not a complete solution for debiasing the downstream applications but encourages end-to-end approaches to debiasing. \n\nAll the debiasing techniques experimented in this paper rely on a predefined set of gender pairs in some way. CDA used gender pairs for flipping, REG uses it for gender space definition and our technique uses them for computing loss. This reliance on pre-defined set of gender pairs can be considered a limitation of these methods. It also results in another concern. There are gender associated words which do not have pairs, like pregnant. These words are not treated properly by techniques relying on gender pairs. Future work includes designing a context-aware version of our loss function which can distinguish between the unbiased and biased mentions of the gendered words and only penalize the biased version. Another interesting direction is exploring the application of this method in mitigating racial bias which brings more challenges.",
            "score": 0.5553034853052237,
            "section_title": "Conclusion and Discussion",
            "char_start_offset": 15731,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 648
                },
                {
                    "start": 651,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1367
                },
                {
                    "start": 1368,
                    "end": 1497
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89599609375
        },
        {
            "corpus_id": "271600942",
            "title": "Downstream bias mitigation is all you need",
            "text": "There has been limited exploration of the bias transfer hypothesis in prior research. (Birhane and Prabhu 2021) showed instances where stereotypes and denigration from pre-training corpora infiltrate generated images and language even prior to the fine-tuning phase. See also (Steed and Caliskan 2021). However, the degree to which undesirable values embedded in pretraining datasets or benchmarks, such as Wikipedia or ImageNet, lead to task-specific harms after fine-tuning remains uncertain (Barocas, Hardt and Narayanan 2019). \n\nOur focus is on transformer based LLMs. These models produce contextual embeddings rather than static ones. Contextual embeddings rely on token encodings that are conditional on nearby tokens, and they pose an ongoing challenge for intrinsic bias measurement [ (May, et al. 2019), (Kurita, et al. 2019), (Guo and Caliskan 2021)] and bias mitigation (Liang, et al. 2020). Our findings reveal a correlation between intrinsic and extrinsic metrics for typical LLMs, primarily stemming from biases in the fine-tuning dataset. \n\nAdditional research delves into the possibility that mitigating biases upstream could universally prevent downstream harm. (Jin, et al. 2021) demonstrated that an intermediary bias-mitigating fine-tuning step can alleviate bias in subsequent tasks. Similarly, (Dennison 2021) proposed fine-tuning on meticulously curated \"values-targeted\" datasets to counter toxic behaviour in GPT-3. Our results support these findings, highlighting that the fine-tuning process can partially overwrite biases inherent in the original pre-trained model. A recent post-hoc mitigation technique proposed debiasing contextual embeddings before fine-tuning (Liang, et al. 2020). However, our findings suggest that while this debiasing method may address representational harms upstream, its efficacy in reducing downstream harms is limited.",
            "score": 0.554396870746339,
            "section_title": "Literature Review",
            "char_start_offset": 3483,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 85
                },
                {
                    "start": 86,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 530
                },
                {
                    "start": 533,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 1054
                },
                {
                    "start": 1057,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1305
                },
                {
                    "start": 1306,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1594
                },
                {
                    "start": 1595,
                    "end": 1715
                },
                {
                    "start": 1716,
                    "end": 1877
                }
            ],
            "ref_mentions": [
                {
                    "start": 86,
                    "end": 111,
                    "matchedPaperCorpusId": "220265500"
                },
                {
                    "start": 276,
                    "end": 301,
                    "matchedPaperCorpusId": "225094318"
                },
                {
                    "start": 794,
                    "end": 812,
                    "matchedPaperCorpusId": "85518027"
                },
                {
                    "start": 814,
                    "end": 835,
                    "matchedPaperCorpusId": "190000105"
                },
                {
                    "start": 837,
                    "end": 860,
                    "matchedPaperCorpusId": "219530686"
                },
                {
                    "start": 882,
                    "end": 902,
                    "matchedPaperCorpusId": "207996257"
                },
                {
                    "start": 1180,
                    "end": 1197,
                    "matchedPaperCorpusId": "235097594"
                },
                {
                    "start": 1317,
                    "end": 1332,
                    "matchedPaperCorpusId": "235489789"
                },
                {
                    "start": 1694,
                    "end": 1713,
                    "matchedPaperCorpusId": "207996257"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5380859375
        },
        {
            "corpus_id": "261530629",
            "title": "Bias and Fairness in Large Language Models: A Survey",
            "text": "We classify an extensive range of bias mitigation methods by their intervention stage: pre-processing (modifying model inputs), in-training (modifying the optimization process), intra-processing (modifying inference behavior), and post-processing (modifying model outputs). We construct granular subcategories at each mitigation stage to draw similarities and trends between classes of methods, with mathematical formalization of several techniques with unified notation, and representative examples of each class of method. We draw attention to ways that bias may persist at each mitigation stage. 5. An overview of key open problems and challenges that future work should address. We challenge future research to address power imbalances in LLM development, conceptualize fairness more robustly for NLP, improve bias evaluation principles and standards, expand mitigation efforts, and explore theoretical limits for fairness guarantees.\n\nWe do not attempt to survey the abundance of work on algorithmic fairness more generally, or even bias in all language technologies broadly. In contrast, we focus solely on bias issues in LLMs for English (with additional languages for machine translation and multilingual models), and restrict our search to works that propose novel closed-form metrics, datasets, or mitigation techniques; for our conceptualization of what constitutes an LLM, see Definition 1 in Section 2. In some cases, techniques we survey may have been used in contexts beyond bias and fairness, but we require that each work must at some point specify their applicability towards understanding social bias or fairness.\n\nIn the remainder of the article, we first formalize the problem of bias in LLMs (Section 2), and then provide taxonomies of metrics for bias evaluation (Section 3), datasets for bias evaluation (Section 4), and techniques for bias mitigation (Section 5). Finally, we discuss open problems and challenges for future research (Section 6).",
            "score": 0.553012845231122,
            "section_title": "A survey and taxonomy of techniques for bias mitigation.",
            "char_start_offset": 5598,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80322265625
        },
        {
            "corpus_id": "248980783",
            "title": "Don\u2019t Forget About Pronouns: Removing Gender Bias in Language Models Without Losing Factual Gender Information",
            "text": "We propose a new insight into gender information in contextual language representations. In debiasing, we focus on the trade-off between removing stereotypical bias while preserving the semantic and grammatical information about the gender of a word from its context. Our evaluation of gender bias showed that three analyzed masked language models (BERT large, BERT based, and ELEC-TRA) are biased and skewed toward predicting male gender for profession names. To mitigate this issue, we disentangle stereotypical bias from factual gender information. Our filtering method can remove the former to some extent and preserve the latter. As a result, we decrease the bias in predictions of language models without significant deterioration of their performance in masked language modeling task.",
            "score": 0.5528831070876028,
            "section_title": "Conclusions",
            "char_start_offset": 24175,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 791
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4365234375
        },
        {
            "corpus_id": "266054040",
            "title": "Layered Bias: Interpreting Bias in Pretrained Large Language Models",
            "text": "Motivation: Large Language Models (LLMs) have risen to prominence, revolutionizing the field of natural language processing (NLP). These models, such as OPT (Zhang et al., 2022) and LLaMA (Touvron et al., 2023a), are trained on vast and diverse data sources encompassing webpages, Wikipedia, books, scientific papers, and other online content. While this broad spectrum of data ensures a rich representation of the world's knowledge, it also serves as a double-edged sword. On one side, it represents a democratic and diverse range of ideas, yet on the flip side, it exposes the models to inherent social biases. \n\nIn recent years, the NLP community has prioritized studying biases in LLMs. Early work by Bolukbasi et al. (2016) revealed gender and ethnic biases in word embeddings like Word2Vec and GloVe. This trend of identifying biases continued with more complex models like BERT, where researchers examined how biases are encoded and propagated (Kurita et al., 2019;May et al., 2019). Researchers have also developed datasets, such as StereoSet (Nadeem et al., 2021) and CrowS-Pairs (Nangia et al., 2020), specifically to measure and understand these biases. Sap et al. (2020) delved into the effects of biased data, especially from human annotators, on the behavior of models. Alongside identification, efforts have been geared towards the mitigation of bias in LLMs. Techniques such as iterative nullspace projection (INLP) (Ravfogel et al., 2020a) and Counterfactual Data Augmentation (CDA) (Zmigrod et al., 2019) have been proposed and implemented to mitigate biases in LLMs. Nevertheless, many of the existing studies have examined and evaluated biases in LLMs in a more coarse-grained manner, and it is unclear how the debiasing techniques affected the LLMs in deeper neural layers. \n\nWe aim to address this research gap by conducting an in-depth analysis to interpret layer-wise bias in pretrained LLMs.",
            "score": 0.5526614797860585,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 612
                },
                {
                    "start": 615,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1164
                },
                {
                    "start": 1165,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1794
                },
                {
                    "start": 1797,
                    "end": 1916
                }
            ],
            "ref_mentions": [
                {
                    "start": 705,
                    "end": 728,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 951,
                    "end": 972,
                    "matchedPaperCorpusId": "190000105"
                },
                {
                    "start": 972,
                    "end": 989,
                    "matchedPaperCorpusId": "85518027"
                },
                {
                    "start": 1051,
                    "end": 1072,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 1089,
                    "end": 1110,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 1165,
                    "end": 1182,
                    "matchedPaperCorpusId": "207853290"
                },
                {
                    "start": 1432,
                    "end": 1456,
                    "matchedPaperCorpusId": "215786522"
                },
                {
                    "start": 1500,
                    "end": 1522,
                    "matchedPaperCorpusId": "184486914"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.767578125
        },
        {
            "corpus_id": "237491723",
            "title": "Mitigating Language-Dependent Ethnic Bias in BERT",
            "text": "and Dredze, 2020). The first solution using the multilingual BERT model works well for Chinese, English, German, and Spanish, languages that are resourceabundant. An alternative solution leverages alignment with the English embedding space, and this solution reduces the bias score for Korean and Turkish, relatively low-resource languages.\n\nExtensive experiments with six languages (English, German, Spanish, Korean, Turkish, and Chinese) demonstrate that our proposed solutions work well for mitigation. We conduct an ablation study to find out what part of the treatment contributes most significantly to bias mitigation. Moreover, we demonstrate that the bias mitigation methods do not result in a performance drop for downstream tasks. Finally, we validate mitigation technique with two additional languages (Arabic and Greek).\n\nOur contributions can be summarized as follows:\n\n\u2022 We suggest CB score, a multi-class bias measure with log probability to quantify the degree of ethnic bias in language models.\n\n\u2022 We reveal the language-dependent nature of ethnic bias.\n\n\u2022 We present two simple and effective bias mitigation methods: one with the multilingual model, and the other with contextual word alignment and fine-tuning 1 .",
            "score": 0.5523569024277187,
            "section_title": "Introduction",
            "char_start_offset": 1773,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.666015625
        },
        {
            "corpus_id": "57761084",
            "title": "Equalizing Gender Bias in Neural Machine Translation with Word Embeddings Techniques",
            "text": "There are studies on the presence of biases in many NLP applications. Word embeddings can learn biases learn from human generated corpora. (Bolukbasi et al., 2016) showed that stereotypical analogies are present in word embeddings both for gender and race. (Caliskan et al., 2017) found also a strong gender and racial bias presence is found in pre-trained embeddings and proposed a method for measuring bias in word embeddings. (Zhao et al., 2018b) proposed GN-GloVe, an algorithm to generated gender neutral word embeddings. The approach is to restrict gender information attributes in certain dimensions to keep the remaining free of this attributes. (Zhao et al., 2018a) shows that sexism present in a coreference resolution system is due to the word embeddings components. Applications that use these embeddings, such as curriculum filtering, may discriminate candidates because of their gender. The amplification of biases in downstream applications is a concerning problem also that can enlarge the gap between genders, for example in search engines, for professions where the name of the candidates may be discriminated by the algorithm because of their bias towards a specific gender. Thus, broadening even further gender inequality for a given field. (Zhao et al., 2017) shows that gender bias is learned and amplified in models trained from data sets containing web images used in language modelling tasks. As an example of, the word \"cooking\" is more probable to be re-lated to females than males and it can be further amplified. (Park et al., 2018) studies the reduction of such biases in abusive language detection. These models have a strong bias towards words that identify gender because of the data sets in which they are trained. Sentences that do not necessarily show sexism are detected as false positives compromising the robustness of the models. Debiased word embeddings combined with augmenting and swapping gender data is the most effective method for reducing gender bias for this task. (Prates et al., 2018) performs a case study on gender bias in machine translation. They build a test set consisting of a list of jobs and gender-specific sentences.",
            "score": 0.5514744238287399,
            "section_title": "Related work",
            "char_start_offset": 7500,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 69
                },
                {
                    "start": 70,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 256
                },
                {
                    "start": 257,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 900
                },
                {
                    "start": 901,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1260
                },
                {
                    "start": 1261,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1629
                },
                {
                    "start": 1630,
                    "end": 1748
                },
                {
                    "start": 1749,
                    "end": 1869
                },
                {
                    "start": 1870,
                    "end": 2013
                },
                {
                    "start": 2014,
                    "end": 2096
                },
                {
                    "start": 2097,
                    "end": 2178
                }
            ],
            "ref_mentions": [
                {
                    "start": 654,
                    "end": 674,
                    "matchedPaperCorpusId": "4952494"
                },
                {
                    "start": 1261,
                    "end": 1280,
                    "matchedPaperCorpusId": "1389483"
                },
                {
                    "start": 1542,
                    "end": 1561,
                    "matchedPaperCorpusId": "52070035"
                },
                {
                    "start": 2014,
                    "end": 2034,
                    "matchedPaperCorpusId": "4929974"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61376953125
        },
        {
            "corpus_id": "259342087",
            "title": "Prompt Tuning Pushes Farther, Contrastive Learning Pulls Closer: A Two-Stage Approach to Mitigate Social Biases",
            "text": "Pre-trained Language Models (PLMs) have demonstrated outstanding performance in recent years and have been widely used in natural language understanding tasks (Peters et al., 2018;Delobelle et al., 2022). However, the powerful language modeling capability enables PLMs to learn good representations from large-scale training corpora while capturing human-like social biases. Recent studies have demonstrated that the representations encoded by PLMs learn social biases specific to demographic groups (e.g., gender, race, religion) and can be amplified to downstream tasks, leading to unfair outcomes and adverse social effects (Zhao et al., 2019;Webster et al., 2020). As a result, mitigating social biases in PLMs' encoding can improve the fairness of NLP systems significantly (Bolukbasi et al., 2016;Bender and Friedman, 2018).\n\nMost existing debiasing techniques first need to construct sample pairs using Counterfactual Data Augmentation (CDA) (Zmigrod et al., 2019;Wang et al., 2022) to balance the training corpora. The general approach of CDA is to replace the original corpus with attribute words (e.g., he/she, man/woman) specific to different demographic groups. For example, RCDA (Chen et al., 2021) uses a generator to generate a large number of antisense sentences and then uses a discriminator to evaluate the quality of the original and antisense samples jointly. FairFil (Cheng et al., 2021) obtains a pair of positive sample sentences by replacing the attribute words in the training corpora with the antonyms and then uses contrastive learning to train a filter for debiasing. Auto-Debias (Guo et al., 2022) uses pairs of attribute words as training corpora, amplifies the bias between sample pairs by searching biased prompt texts in the Wikipedia vocabulary, and then performs semantic alignment using Jensen-Shannon divergence. These methods aim to mitigate social biases between different demographic groups by narrowing the representation distance between sample pairs. However, CDA slightly modifies the original corpus, limiting the representation distance between different demographic groups to a narrow range. As a result, the debiasing model is easy to overfit the",
            "score": 0.5514526048520503,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 159,
                    "end": 180,
                    "matchedPaperCorpusId": "3626819"
                },
                {
                    "start": 180,
                    "end": 203,
                    "matchedPaperCorpusId": "250390561"
                },
                {
                    "start": 627,
                    "end": 646,
                    "matchedPaperCorpusId": "102352962"
                },
                {
                    "start": 779,
                    "end": 803,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 803,
                    "end": 829,
                    "matchedPaperCorpusId": "52255687"
                },
                {
                    "start": 949,
                    "end": 971,
                    "matchedPaperCorpusId": "184486914"
                },
                {
                    "start": 971,
                    "end": 989,
                    "matchedPaperCorpusId": "247155039"
                },
                {
                    "start": 1388,
                    "end": 1408,
                    "matchedPaperCorpusId": "232185104"
                },
                {
                    "start": 1608,
                    "end": 1626,
                    "matchedPaperCorpusId": "248780440"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9072265625
        },
        {
            "corpus_id": "250390797",
            "title": "The Birth of Bias: A case study on the evolution of gender bias in an English language model",
            "text": "Detecting and mitigating harmful biases in modern language models are widely recognized as crucial, open problems. In this paper, we take a step back and investigate how language models come to be biased in the first place. We use a relatively small language model, using the LSTM architecture trained on an English Wikipedia corpus. With full access to the data and to the model parameters as they change during every step while training, we can map in detail how the representation of gender develops, what patterns in the dataset drive this, and how the model's internal state relates to the bias in a downstream task (semantic textual similarity). We find that the representation of gender is dynamic and identify different phases during training. Furthermore, we show that gender information is represented increasingly locally in the input embeddings of the model and that, as a consequence, debiasing these can be effective in reducing the downstream bias. Monitoring the training dynamics, allows us to detect an asymmetry in how the female and male gender are represented in the input embeddings. This is important, as it may cause naive mitigation strategies to introduce new undesirable biases. We discuss the relevance of the findings for mitigation strategies more generally and the prospects of generalizing our methods to larger language models, the Transformer architecture, other languages and other undesirable biases.",
            "score": 0.5511361319954575,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78662109375
        },
        {
            "corpus_id": "264146917",
            "title": "Investigating Bias in Multilingual Language Models: Cross-Lingual Transfer of Debiasing Techniques",
            "text": "We investigate whether this performance disparity extends to gender, religious, and racial biases. Related studies demonstrate the effectiveness of cross-lingual debiasing for individual techniques and selected bias scopes (Liang et al., 2020b;Lauscher et al., 2021). We show how to reduce bias in mBERT across different languages by conducting a benchmark of state-of-the-art (SOTA) debiasing techniques and providing guidance on its implementation. To facilitate further research and reproducibility, we make the code and additional data available to the research community2 . \n\nOur contributions can be summarized as follows: 1) We provide a benchmark of different SOTA debiasing techniques across multiple languages in a multilingual large language model. 2) We find that SentenceDebias is the most effective for crosslingual debiasing, reducing the bias in mBERT by 13%. 3) We provide implementation guidelines for debiasing multilingual models and highlight the differences in the cross-lingual transferability of different debiasing techniques. We find that most projection-based techniques applied to one language yield similar predictions across evaluation languages. We also recommend performing the techniques with an additional pretraining step on the lowest resource language within the multilingual model for optimal results.",
            "score": 0.5510938584491418,
            "section_title": "Introduction",
            "char_start_offset": 2085,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 578
                },
                {
                    "start": 581,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1339
                }
            ],
            "ref_mentions": [
                {
                    "start": 223,
                    "end": 244,
                    "matchedPaperCorpusId": "227230609"
                },
                {
                    "start": 244,
                    "end": 266,
                    "matchedPaperCorpusId": "237440429"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72802734375
        },
        {
            "corpus_id": "199501874",
            "title": "Debiasing Embeddings for Reduced Gender Bias in Text Classification",
            "text": "A trend in the construction of deep learning models for natural language processing tasks is the use of pre-trained embeddings at the input layer (Mikolov et al., 2013;Pennington et al., 2014;Bojanowski et al., 2017). These embeddings are usually learned by solving a language modeling task on a large unsupervised corpus, allowing downstream models to leverage the semantic and syntactic relationships learned from this corpus. One issue with using such embeddings, however, is that the model might inherit unintended biases from this corpus. In (Bolukbasi et al., 2016), the authors highlight some gender bias at the embedding layer through analogy and occupational stereotyping tasks, but do not investigate how these biases affect modeling on downstream tasks. It has been argued (Gonen and Goldberg, 2019) that such debiasing approaches only mask the bias in embeddings and that bias remains in a form that downstream algorithms can still pick up. \n\nThis paper investigate the impact of gender bias in these pre-trained word embeddings on down-stream modeling tasks. We build deep neural network classifiers to perform occupation classification on the recently released \"Bias in Bios\" dataset (De-Arteaga et al., 2019) using a variety of different debiasing techniques for these embeddings introduced in (Bolukbasi et al., 2016) and comparing them to the scrubbing of gender indicators. The main contributions of this paper are: \n\n\u2022 Comparing the efficacy of embedding based debiasing techniques to manual word scrubbing techniques on both overall model performance and fairness. \n\n\u2022 Demonstrating that standard debiasing approaches like those introduced in (Bolukbasi et al., 2016) actually worsen the bias of downstream tasks by providing a denoised channel for communicating demographic information. \n\n\u2022 Highlight that a simple modification of this debiasing technique which aims to completely remove gender information can simultaneously improve fairness criteria and maintain a high level of task accuracy.",
            "score": 0.5509754184498558,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 952
                },
                {
                    "start": 955,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1391
                },
                {
                    "start": 1392,
                    "end": 1433
                },
                {
                    "start": 1436,
                    "end": 1584
                },
                {
                    "start": 1587,
                    "end": 1807
                },
                {
                    "start": 1810,
                    "end": 2016
                }
            ],
            "ref_mentions": [
                {
                    "start": 168,
                    "end": 192,
                    "matchedPaperCorpusId": "1957433"
                },
                {
                    "start": 192,
                    "end": 216,
                    "matchedPaperCorpusId": "207556454"
                },
                {
                    "start": 547,
                    "end": 571,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 1198,
                    "end": 1223,
                    "matchedPaperCorpusId": "58006082"
                },
                {
                    "start": 1309,
                    "end": 1333,
                    "matchedPaperCorpusId": "1704893"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7880859375
        },
        {
            "corpus_id": "262075139",
            "title": "Bias Invariant Approaches for Improving Word Embedding Fairness",
            "text": "However, this method is limited to binary stereotypes like those related to man and woman, not capturing more nuanced language that contains multiple different biases. A straightforward solution to extend the idea to multi-class stereotypes is to subtract multiple components from principle component analysis (PCA) using predefined lists of words for all classes [29]. However, this begs the question of how to comprehensively and accurately curate lists of terms, as well as to how to define classes of stereotypes. \n\nYang and Liu [48] assume the relation between stop words and content words can be described in a half-sibling regression (HSR) structure [41] in causal inference. Although they aim at removing noise in word embeddings, their approach is similar to soft debiasing but with a pre-defined list of stop words. Another approach [24] generalizes the idea of Bolukbasi et al. [8] into a multi-objective optimization process. It considers the issue of hard debiasing failing to maintain the relation between gender neutral words and their neighbors. The problem is formulated to minimize gender biased illicit association [18], semantic meaning loss, and gender bias, which are referred as repulsion, attraction and neutralization (RAN), respectively. \n\nBesides these post-processing algorithms, there are also methods proposed to improve fairness during word embedding training. For example, Hube et al. [23] reduce stereotypes in people names at training time. In their work, an oracle classifier is required to help identify the bias, and more importantly, the fairness improvement is limited to the specific list of names. In a similar approach, Zhao et al. [49] train word embeddings with a data augmentation method. The method builds a dataset specifically to address gender stereotypes, where all gender pronouns are linked to stereotypical entities such as occupations. \n\nThe above mentioned methods are limited to specific stereotypes due to their need for pre-defined lists of words. This has the disadvantage that the algorithm designer must come up with the lists of words for each class, which is itself a source of bias. Moreover, the resulting word embeddings can still be vulnerable to biases that are unknown to the algorithm designer.",
            "score": 0.5503177595116556,
            "section_title": "Bias in word embeddings",
            "char_start_offset": 8206,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 517
                },
                {
                    "start": 520,
                    "end": 682
                },
                {
                    "start": 683,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1263
                },
                {
                    "start": 1266,
                    "end": 1391
                },
                {
                    "start": 1392,
                    "end": 1474
                },
                {
                    "start": 1475,
                    "end": 1638
                },
                {
                    "start": 1639,
                    "end": 1733
                },
                {
                    "start": 1734,
                    "end": 1889
                },
                {
                    "start": 1892,
                    "end": 2005
                },
                {
                    "start": 2006,
                    "end": 2146
                },
                {
                    "start": 2147,
                    "end": 2264
                }
            ],
            "ref_mentions": [
                {
                    "start": 364,
                    "end": 368,
                    "matchedPaperCorpusId": "102350941"
                },
                {
                    "start": 533,
                    "end": 537,
                    "matchedPaperCorpusId": "208268216"
                },
                {
                    "start": 657,
                    "end": 661,
                    "matchedPaperCorpusId": "34972422"
                },
                {
                    "start": 843,
                    "end": 847,
                    "matchedPaperCorpusId": "219260204"
                },
                {
                    "start": 889,
                    "end": 892,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 1134,
                    "end": 1138,
                    "matchedPaperCorpusId": "73729169"
                },
                {
                    "start": 1417,
                    "end": 1421,
                    "matchedPaperCorpusId": "210883022"
                },
                {
                    "start": 1674,
                    "end": 1678,
                    "matchedPaperCorpusId": "4952494"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51025390625
        },
        {
            "corpus_id": "102352962",
            "title": "Gender Bias in Contextualized Word Embeddings",
            "text": "Gender bias has been shown to affect several realworld applications relying on automatic language analysis, including online news (Ross and Carter, 2011), advertisements (Sweeney, 2013), abusive language detection (Park et al., 2018), machine translation (Font and Costa-juss\u00e0, 2019;Vanmassenhove et al., 2018), and web search (Kay et al., 2015). In many cases, a model not only replicates bias in the training data but also amplifies it (Zhao et al., 2017). \n\nFor word representations, Bolukbasi et al. (2016) and Caliskan et al. (2017) show that word embeddings encode societal biases about gender roles and occupations, e.g. engineers are stereotypically men, and nurses are stereotypically women. As a consequence, downstream applications that use these pretrained word embeddings also reflect this bias. For example, Zhao et al. (2018a) and Rudinger et al. (2018) show that coreference resolution systems relying on word embeddings encode such occupational stereotypes. In concurrent work, May et al. (2019) measure gender bias in sentence embeddings, but their evaluation is on the aggregation of word representations. In contrast, we analyze bias in contextualized word representations and its effect on a downstream task. \n\nTo mitigate bias from word embeddings, Bolukbasi et al. ( 2016) propose a post-processing method to project out the bias subspace from the pre-trained embeddings. Their method is shown to reduce the gender information from the embeddings of gender-neutral words, and, remarkably, maintains the same level of performance on different downstream NLP tasks. Zhao et al. (2018b) further propose a training mechanism to separate gender information from other factors. However, Gonen and Goldberg (2019) argue that entirely removing bias is difficult, if not impossible, and the gender bias information can be often recovered. This paper investigates a natural follow-up question: What are effective bias mitigation techniques for contextualized embeddings?",
            "score": 0.5502592775628663,
            "section_title": "Related Work",
            "char_start_offset": 2720,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 458
                },
                {
                    "start": 461,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 974
                },
                {
                    "start": 975,
                    "end": 1124
                },
                {
                    "start": 1125,
                    "end": 1229
                },
                {
                    "start": 1232,
                    "end": 1394
                },
                {
                    "start": 1395,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 1852
                },
                {
                    "start": 1853,
                    "end": 1983
                }
            ],
            "ref_mentions": [
                {
                    "start": 170,
                    "end": 185,
                    "matchedPaperCorpusId": "23044927"
                },
                {
                    "start": 214,
                    "end": 233,
                    "matchedPaperCorpusId": "52070035"
                },
                {
                    "start": 283,
                    "end": 310,
                    "matchedPaperCorpusId": "76654723"
                },
                {
                    "start": 327,
                    "end": 345,
                    "matchedPaperCorpusId": "8832874"
                },
                {
                    "start": 438,
                    "end": 457,
                    "matchedPaperCorpusId": "1389483"
                },
                {
                    "start": 487,
                    "end": 510,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 515,
                    "end": 537,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 822,
                    "end": 841,
                    "matchedPaperCorpusId": "4952494"
                },
                {
                    "start": 846,
                    "end": 868,
                    "matchedPaperCorpusId": "13756572"
                },
                {
                    "start": 1587,
                    "end": 1606,
                    "matchedPaperCorpusId": "52161864"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82177734375
        },
        {
            "corpus_id": "271334105",
            "title": "FairFlow: An Automated Approach to Model-based Counterfactual Data Augmentation For NLP",
            "text": "Despite their growing popularity and unprecedented performance in various application domains, language models (LMs) continue to be plagued with issues of harmful societal biases and stereotypes that have been shown to have detrimental social effects [4].The biggest contributing factor is the encapsulation of societal biases in everyday language, as is well-documented [1,19,12].LMs heavily rely on such textual data, now digitalized on various online outlets, as training data, causing them to mirror these biases [25].\n\nIn Natural Language Processing (NLP), similar to many machine learning domains, bias mitigation generally occurs at three intervention avenues: the training data, the learning procedure, or the model output [15].Since model bias traces its roots to the training data, mitigating bias at the training data level has proven very effective [10,6].One such approach, Counterfactual Data Augmentation (CDA) [5], seeks to remove spurious correlations between attributes in the training data by evening out the distribution of words that characterize demographic attributes in the context of neutral words that should ideally not be demographically aligned.Specifically, explicit attribute-defining words are replaced with their counterfactual equivalents from complementary demographic groups for every text instance.To illustrate this with an example, an instance of \"She is a nurse\" will be augmented with \"He is a nurse\" in the case of mitigating gender bias.This follows the intuition that in an ideal dataset, the association between gender attributes and target attributes like professions will be even for different gender groups.\n\nKey works, such as [27,16,28], introducing CDA as a bias mitigation technique adopt a word substitution approach based on dictionaries.These word substitution methods are prone to grammatical incoherence because of out-ofcontext substitutions and omitted word pairs.Because dictionary compilations are often incomplete [8], a direct word-substitution approach will not generalize to omitted words.Take for instance (Bachelor and Masters degree v. Spinster and Mistresses degree) and (she taught herself v. he taught herself ) which were common issues we observed with some methods.",
            "score": 0.5491197721712693,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 255
                },
                {
                    "start": 255,
                    "end": 381
                },
                {
                    "start": 381,
                    "end": 522
                },
                {
                    "start": 524,
                    "end": 736
                },
                {
                    "start": 736,
                    "end": 868
                },
                {
                    "start": 868,
                    "end": 1174
                },
                {
                    "start": 1174,
                    "end": 1335
                },
                {
                    "start": 1335,
                    "end": 1480
                },
                {
                    "start": 1480,
                    "end": 1655
                },
                {
                    "start": 1657,
                    "end": 1792
                },
                {
                    "start": 1792,
                    "end": 1923
                },
                {
                    "start": 1923,
                    "end": 2054
                },
                {
                    "start": 2054,
                    "end": 2238
                }
            ],
            "ref_mentions": [
                {
                    "start": 517,
                    "end": 521,
                    "matchedPaperCorpusId": "372956"
                },
                {
                    "start": 731,
                    "end": 735,
                    "matchedPaperCorpusId": "56444963"
                },
                {
                    "start": 861,
                    "end": 865,
                    "matchedPaperCorpusId": "54997157"
                },
                {
                    "start": 865,
                    "end": 867,
                    "matchedPaperCorpusId": "58006082"
                },
                {
                    "start": 1676,
                    "end": 1680,
                    "matchedPaperCorpusId": "4952494"
                },
                {
                    "start": 1683,
                    "end": 1686,
                    "matchedPaperCorpusId": "184486914"
                },
                {
                    "start": 1976,
                    "end": 1979,
                    "matchedPaperCorpusId": "207852875"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64892578125
        },
        {
            "corpus_id": "229363391",
            "title": "Confronting Abusive Language Online: A Survey from the Ethical and Human Rights Perspective",
            "text": "Embedding models are one of the sources of bias in natural language processing systems. An active line of work aims to quantify bias and stereotypes in language models as representations of text. Early works focused on gender and racial bias and introduced association tests for measuring bias in word embeddings (Bolukbasi et al., 2016;Caliskan et al., 2017;Manzini et al., 2019). For contextualized word embeddings, May et al. (2019) and Kurita et al. (2019) used pre-defined sentence templates, whereas Nadeem et al. (2020) and Nangia et al. (2020) collected crowd-sourced sentences to measure stereotypical biases hidden in language models. Bartl et al. (2020) presented a template-based corpus to measure gender bias with respect to professions and showed that language models encode not only biases found in real-world data but also those based on stereotypes. They also showed that the techniques used to measure and mitigate bias that work for English language models might not be applicable to other languages. Besides encoding social biases, language models are also prone to generating racist, sexist, or otherwise toxic language, which hinders their safe deployment (Gehman et al., 2020). However, it is not entirely clear how the bias and toxicity present in language models impact the output of the trained classifiers. Jin et al. (2021) examined the bias in language models for the case where a hate speech classifier was trained via transfer learning, and demonstrated that upstream bias mitigation of language models is transferable to downstream tasks when models are trained through fine-tuning. They concluded that upstream bias mitigation is not as effective as direct bias mitigation on the downstream task, but the former is more efficient and accessible. Swamy et al. (2019) revealed that the dominance of benign examples in abusive language datasets, which is a common practice to emulate reality, might have a detrimental effect on the generalizability of classifiers. Also, sampling techniques deployed to boost the number of abusive examples may result in a skewed distribution of concepts and entities related to targeted identity groups. These unintended entity misrepresentations often translate into biased abuse detection systems. Dixon et al.",
            "score": 0.5485689889045238,
            "section_title": "Semantic Bias",
            "char_start_offset": 43107,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 313,
                    "end": 337,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 337,
                    "end": 359,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 359,
                    "end": 380,
                    "matchedPaperCorpusId": "102350941"
                },
                {
                    "start": 418,
                    "end": 435,
                    "matchedPaperCorpusId": "85518027"
                },
                {
                    "start": 440,
                    "end": 460,
                    "matchedPaperCorpusId": "190000105"
                },
                {
                    "start": 531,
                    "end": 551,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 645,
                    "end": 664,
                    "matchedPaperCorpusId": "225094152"
                },
                {
                    "start": 1178,
                    "end": 1199,
                    "matchedPaperCorpusId": "221878771"
                },
                {
                    "start": 1334,
                    "end": 1351,
                    "matchedPaperCorpusId": "235097594"
                },
                {
                    "start": 1779,
                    "end": 1798,
                    "matchedPaperCorpusId": "207968532"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50634765625
        },
        {
            "corpus_id": "265033284",
            "title": "Model-based Counterfactual Generator for Gender Bias Mitigation",
            "text": "Language models (LMs) have primarily been trained on internet data, which has been shown to contain societal biases and stereotypes (Wolf et al., 2017), predisposing these models to perpetuate, and in some cases amplify these biases. With the growing adoption of LMs and LM-based tools for a myriad of uses, it has become essential to mitigate these biases. Efforts have been made, especially in the area of gender, to develop mitigation strategies to alleviate the effects of these biases. One of the widely adopted techniques for mitigating gender bias has been counterfactual data augmentation (CDA). The approach is intuitive; for every text instance, explicit gender words are replaced with their equivalents from other gender groups. \n\nory received his Spinster and Mistresses of Accountancy...\" and \"Anthony discovered his passion for programming after teaching herself some Python...\" respectively. This is because the terms bachelor and master can have different meanings some of which could be gender (un)associated. Secondly, the terms herself and himself are left out of the dictionary by Hall Maudslay et al. (2019). \n\nTo resolve these issues, we introduce a modelbased counterfactual generator as a CDA approach. We make two significant contributions in this regard. First, we propose a pipeline of techniques to generate parallel data from dictionary-based CDA outputs. Secondly, we introduce a bi-objective training approach to train a counterfactual generator. Our model is made up of a generator and a discriminator akin to adversarial models but with some nuances. One key component of our biobjective training approach is how we circumvent the bottleneck of backpropagating through a discrete layer of non-differentiable tokens, which is a common issue in textual models. A bi-objective approach also makes our model more robust to errors in the parallel data labels. We will show how these techniques combine to solve the issues with dictionary-based methods. \n\nTo sum up our key contributions: \n\n1. We propose a data processing pipeline to generate parallel data for training CDA models.",
            "score": 0.5482926673364813,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 739
                },
                {
                    "start": 742,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1129
                },
                {
                    "start": 1132,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1583
                },
                {
                    "start": 1584,
                    "end": 1791
                },
                {
                    "start": 1792,
                    "end": 1887
                },
                {
                    "start": 1888,
                    "end": 1980
                },
                {
                    "start": 1983,
                    "end": 2015
                },
                {
                    "start": 2018,
                    "end": 2109
                }
            ],
            "ref_mentions": [
                {
                    "start": 1106,
                    "end": 1128,
                    "matchedPaperCorpusId": "202541569"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.673828125
        },
        {
            "corpus_id": "275470775",
            "title": "Foundation Models at Work: Fine-Tuning for Fairness in Algorithmic Hiring",
            "text": "Our work relates to several lines of previous research. \n\nFair representations. Early efforts in bias mitigation targeted removing gender biases in static embeddings where the semantic representation of a word is confined to just one vector like GloVe (Pennington, Socher, and Manning 2014) and Word2Vec (Mikolov et al. 2013) to achieve unbiased representations and word associations. Bolukbasi et al. (2016) studied how gender identity words are associated with specific occupations and subtracted the gender direction from word embeddings to neutralize the language while sustaining the equal distance between gender-neutral words and gendered pairs of words. Ravfogel et al. (2020) proposed INLP for debiasing embeddings through iterative null-space projections for guarding protected attributes. As the field evolved, the focus shifted towards debiasing contextual embeddings, such as ELMo (Peters et al. 2018). Kaneko and Bollegala (2021); Liang et al. ( 2020) highlighted the relative complexity of contextual embeddings compared to their static counterparts and the challenge of identifying which parameters contribute to the bias. Liang et al. (2020) proposed SENT-DEBIAS applicable at sentence levels to debias pre-trained contextual embeddings, where Kaneko and Bollegala (2021) developed both token and sentence level approach and emphasized the trade-off between accuracy and unbiasedness in such models. \n\nSeveral methods, such as adversarial learning (Elazar and Goldberg 2018;Sakaguchi et al. 2021) and counterfactual data augmentation (Zmigrod et al. 2019;Islam et al. 2021), have been proposed to mitigate language models propagating biases present in a training corpus. Elazar and Goldberg (2018) 2019) aim to reduce bias by data augmentation through counterfactual statements. However, using augmented datasets is expensive and is prone to introduce noise and unrealistic scenarios that can negatively impact performance. \n\nDebiasing LLMs.",
            "score": 0.5481529698861619,
            "section_title": "Related Work",
            "char_start_offset": 30183,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 55
                },
                {
                    "start": 58,
                    "end": 79
                },
                {
                    "start": 80,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 1138
                },
                {
                    "start": 1139,
                    "end": 1416
                },
                {
                    "start": 1419,
                    "end": 1687
                },
                {
                    "start": 1688,
                    "end": 1795
                },
                {
                    "start": 1796,
                    "end": 1940
                },
                {
                    "start": 1943,
                    "end": 1958
                }
            ],
            "ref_mentions": [
                {
                    "start": 252,
                    "end": 290,
                    "matchedPaperCorpusId": "1957433"
                },
                {
                    "start": 662,
                    "end": 684,
                    "matchedPaperCorpusId": "215786522"
                },
                {
                    "start": 894,
                    "end": 914,
                    "matchedPaperCorpusId": "3626819"
                },
                {
                    "start": 1465,
                    "end": 1491,
                    "matchedPaperCorpusId": "52056513"
                },
                {
                    "start": 1491,
                    "end": 1512,
                    "matchedPaperCorpusId": "199370376"
                },
                {
                    "start": 1688,
                    "end": 1714,
                    "matchedPaperCorpusId": "52056513"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75537109375
        },
        {
            "corpus_id": "274141775",
            "title": "Mitigating Gender Bias in Contextual Word Embeddings",
            "text": "Word embeddings have been shown to produce remarkable results in tackling a vast majority of NLP related tasks. Unfortunately, word embeddings also capture the stereotypical biases that are prevalent in society, affecting the predictive performance of the embeddings when used in downstream tasks. While various techniques have been proposed \\cite{bolukbasi2016man, zhao2018learning} and criticized\\cite{gonen2019lipstick} for static embeddings, very little work has focused on mitigating bias in contextual embeddings. In this paper, we propose a novel objective function for MLM(Masked-Language Modeling) which largely mitigates the gender bias in contextual embeddings and also preserves the performance for downstream tasks. Since previous works on measuring bias in contextual embeddings lack in normative reasoning, we also propose novel evaluation metrics that are straight-forward and aligned with our motivations in debiasing. We also propose new methods for debiasing static embeddings and provide empirical proof via extensive analysis and experiments, as to why the main source of bias in static embeddings stems from the presence of stereotypical names rather than gendered words themselves. All experiments and embeddings studied are in English, unless otherwise specified.\\citep{bender2011achieving}.",
            "score": 0.5478722140015392,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4912109375
        },
        {
            "corpus_id": "274581922",
            "title": "Improving LLM Group Fairness on Tabular Data via In-Context Learning",
            "text": "Despite their promising capabilities, language models also exhibit biases and stereotypes (Bolukbasi et al., 2016;Bender et al., 2021;Chu et al., 2024). These biases mostly originate from the training data, which often contain historical and societal prejudices embedded within the text. Biases have been reported with respect to several demographic groups, e.g., gender, race, ethnicity, and socioeconomic status (Wan et al., 2023;Haim et al., 2024;Santurkar et al., 2023). With the use of these models becoming more widespread, these biases have the risk to substantially reinforce harmful stereotypes and perpetuate existing inequalities, especially when deployed in high-stakes settings (Zou and Schiebinger, 2018). Addressing these biases is essential, and several mitigation strategies have been proposed for this purpose, including data augmentation, prompt tuning and few-shot learning (Zhao et al., 2017;Wang et al., 2021;Sun et al., 2019;Zmigrod et al., 2019;Mattern et al., 2022;Fatemi et al., 2021;Aguirre et al., 2023). However, effectively applying these strategies to the large-scale pretraining corpora remains challenging. Finally, biases can be hard to detect and several datasets and methods have been proposed to help identify them (Caliskan et al., 2017;May et al., 2019;Webster et al., 2020;Kurita et al., 2019;Nangia et al., 2020;Blodgett et al., 2021).",
            "score": 0.5473260482125347,
            "section_title": "Bias and Stereotypes in LLMs",
            "char_start_offset": 4615,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1376
                }
            ],
            "ref_mentions": [
                {
                    "start": 90,
                    "end": 114,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 114,
                    "end": 134,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 414,
                    "end": 432,
                    "matchedPaperCorpusId": "264128125"
                },
                {
                    "start": 450,
                    "end": 473,
                    "matchedPaperCorpusId": "257834040"
                },
                {
                    "start": 990,
                    "end": 1010,
                    "matchedPaperCorpusId": "214727927"
                },
                {
                    "start": 1252,
                    "end": 1275,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 1353,
                    "end": 1375,
                    "matchedPaperCorpusId": "236460302"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60888671875
        },
        {
            "corpus_id": "256390197",
            "title": "How Far Can It Go? On Intrinsic Gender Bias Mitigation for Text Classification",
            "text": "mitigating gender bias have been introduced for both intrinsic and extrinsic bias, we only discuss techniques we use in experiments in this paper.\n\nBias mitigation techniques can be applied to pretrained or finetuned language models, or both. Figure 1 illustrates these interactions with both training settings and this section will discuss both intrinsic ( \u00a7 2.1) and extrinsic ( \u00a7 2.3) mitigation techniques. Additionally, this section will provide a brief overview of bias measures 3 .",
            "score": 0.5473260482125347,
            "section_title": "Introduction",
            "char_start_offset": 3911,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.599609375
        },
        {
            "corpus_id": "260301241",
            "title": "Mitigate Gender Bias Using Negative Multi-task Learning",
            "text": "Text corpora used to train NLP models may contain gender, racial and religious biases. \n\nConsequently, word embeddings trained on these datasets maintain such biases. Gender bias is the most common bias which exists in many NLP applications, and numerous studies have highlighted its presence in various NLP tasks [10][11][12]. [5] proposes a novel training procedure for learning gender-neutral word embeddings. They generate a Gender-Neutral variant of GloVe (GN-GloVe), which tries to remove socially-biased information in certain dimensions while ensuring that other dimensions are free from this gender effect. Importantly, biases are not only contained in text data and embeddings, they may also exist in learned models even if the data itself is not biased. [13] conducted a comprehensive benchmarking of multiple NLP models, evaluating both their fairness and predictive performance across a range of NLP tasks. Their work primarily targeted the debiasing of embeddings, not classifiers. In contrast, our study is particularly focused on debiasing classifiers. [14] have investigated gender biases in the machine translation model, demonstrating that social gender assignment indeed impacts translation choices. [15] investigates how excising a small portion of the training corpus impacts the resulting bias. They perturb the training corpus to identify factors that most significantly influence embedding bias and subsequently remove them from the training corpus. In this paper, we explore the potential of the multi-task learning framework for mitigating gender bias in text classification tasks. The existing multi-task learning frameworks aim to enhance performance across several tasks concurrently. However, in this paper, we use gender prediction as an auxiliary task and apply negative loss weights to reduce the gender influence for the main task. The details are described in Sect. 3.2.",
            "score": 0.5472332374452873,
            "section_title": "Gender Bias Mitigation Methods",
            "char_start_offset": 4579,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 86
                },
                {
                    "start": 89,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1317
                },
                {
                    "start": 1318,
                    "end": 1474
                },
                {
                    "start": 1475,
                    "end": 1608
                },
                {
                    "start": 1609,
                    "end": 1714
                },
                {
                    "start": 1715,
                    "end": 1866
                },
                {
                    "start": 1867,
                    "end": 1901
                },
                {
                    "start": 1902,
                    "end": 1906
                }
            ],
            "ref_mentions": [
                {
                    "start": 765,
                    "end": 769,
                    "matchedPaperCorpusId": "250390904"
                },
                {
                    "start": 1069,
                    "end": 1073,
                    "matchedPaperCorpusId": "233219939"
                },
                {
                    "start": 1220,
                    "end": 1224,
                    "matchedPaperCorpusId": "52946942"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83349609375
        },
        {
            "corpus_id": "247694107",
            "title": "Balancing out Bias: Achieving Fairness Through Balanced Training",
            "text": "Group bias in natural language processing tasks manifests as disparities in system error rates across texts authorized by different demographic groups, typically disadvantaging minority groups. Dataset balancing has been shown to be effective at mitigating bias, however existing approaches do not directly account for correlations between author demographics and linguistic variables, limiting their effectiveness. To achieve Equal Opportunity fairness, such as equal job opportunity without regard to demographics, this paper introduces a simple, but highly effective, objective for countering bias using balanced training.We extend the method in the form of a gated model, which incorporates protected attributes as input, and show that it is effective at reducing bias in predictions through demographic input perturbation, outperforming all other bias mitigation techniques when combined with balanced training.",
            "score": 0.5472139521841365,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8564453125
        },
        {
            "corpus_id": "275503169",
            "title": "Addressing bias in generative AI: Challenges and research opportunities in information management",
            "text": "Debiasing aims to enhance the accuracy of model predictions and recommendations while ensuring equity across different demographic groups (Susarla et al., 2023;Subramanian et al., 2021). Debiasing techniques can be applied at various stages of LLM implementation: the Preprocessing Stage, the Training Stage, and the Post-processing Stage. Below we discuss the existing debiasing techniques in each stage that have been established in the literature. \n\nData used to train or finetune LLMs can introduce downstream bias, making debiasing at the preprocessing stage critical, especially with imbalanced datasets. One common technique is Counterfactual Data Augmentation (CDA), which rebalances data by altering specific associations (Lu et al., 2020). For instance, sentences like \"She is a nurse\" can be replaced with \"He is a nurse\" to promote gender neutrality. Similarly, Counterfactual Data Substitution involves probabilistically replacing biased terms without altering the dataset size, mitigating bias while preserving the data structure (Maudslay et al., 2019). Other approaches remove biased examples from datasets, though this risks data loss and reduced coverage (Le Bras et al., 2020). An alternative method involves masking biased model weights during testing, allowing the model to bypass bias without the need for finetuning (Du et al., 2021). Further, training diverse datasets-such as across multiple languages-helps reduce ethnic and cultural biases, offering broader perspectives in model predictions (BehnamGhader and Milios, 2022). These techniques collectively can mitigate bias at the preprocessing stage, ensuring more balanced and fairer LLM outputs. \n\nDebiasing during the training stage often involves regularization terms and various loss functions. One such method, contrastive loss, enables the model to distinguish between similar and dissimilar examples, thus minimizing reliance on biased features and promoting more balanced representations for long-term bias mitigation (He et al., 2022).",
            "score": 0.5471955154922594,
            "section_title": "Debiasing",
            "char_start_offset": 10319,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 450
                },
                {
                    "start": 453,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1196
                },
                {
                    "start": 1197,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1674
                },
                {
                    "start": 1677,
                    "end": 1776
                },
                {
                    "start": 1777,
                    "end": 2022
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7578125
        },
        {
            "corpus_id": "268553687",
            "title": "Locating and Mitigating Gender Bias in Large Language Models",
            "text": "Some editing techniques have demonstrated robust editing capabilities, managing to modify up to 1000 factual knowledge points swiftly without impairing the model's other functionalities [19].Some studies have attempted to transfer knowledge editing methods to other fields and have achieved success.Cheng et al. [4,5] delved into the feasibility of editing multimodal large language models.They constructed a benchmark for multimodal language model knowledge editing scenarios and compared the editing effects of existing mainstream editing methods on multimodal large language models.Gandikota et al. [10] delved deeply into the technique of erasing specific concepts from the weights of diffusion models through knowledge editing.Within the realm of diffusion models designed for image generation from text, this method demonstrated remarkable effectiveness.\n\nWhile some studies have focused on identifying the locations of bias [3,11,17,29], the mechanisms underlying the generation of gender bias have not been thoroughly investigated.On the other hand, most of the proposed methods to mitigate gender bias focus on word embedding [2], special processing of training data [38], or through human feedback [37].How to exploit the mechanisms that generate gender bias for targeted gender debias and how to adapt knowledge editing techniques to the domain of gender bias mitigation remain under-explored.\n\nTo address the aforementioned issues, this study begins by analyzing the impact of different model components on the generation of gender bias, and the flow of biased information.Inspired by Vig et al. [29] and Meng et al. [18], we use causal mediation analysis to trace the causal effects of different components' activation within a large language model.By running the model three times under different configurations, we observe the locations and mechanisms through which the model generates gender bias.The experimental results reveal that the bottom MLP modules acting on the last token of occupational pronouns and the top attention modules processing the final word of the sentence play significant roles in influencing the generation of gender bias.To further validate the influence of the bottom MLP modules on the generation of gender bias, we have adapted a knowledge editing technique [19] to mitigation of gender bias within the model, called LSDM (Least Square Debias Method).",
            "score": 0.5471426860290229,
            "section_title": "Introduction",
            "char_start_offset": 4190,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 191,
                    "end": 299
                },
                {
                    "start": 299,
                    "end": 390
                },
                {
                    "start": 390,
                    "end": 585
                },
                {
                    "start": 585,
                    "end": 732
                },
                {
                    "start": 732,
                    "end": 860
                },
                {
                    "start": 862,
                    "end": 1039
                },
                {
                    "start": 1039,
                    "end": 1213
                },
                {
                    "start": 1213,
                    "end": 1404
                },
                {
                    "start": 1406,
                    "end": 1585
                },
                {
                    "start": 1585,
                    "end": 1762
                },
                {
                    "start": 1762,
                    "end": 1913
                },
                {
                    "start": 1913,
                    "end": 2163
                },
                {
                    "start": 2163,
                    "end": 2396
                }
            ],
            "ref_mentions": [
                {
                    "start": 602,
                    "end": 606,
                    "matchedPaperCorpusId": "257495777"
                },
                {
                    "start": 931,
                    "end": 934,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 934,
                    "end": 937,
                    "matchedPaperCorpusId": "4930886"
                },
                {
                    "start": 1135,
                    "end": 1138,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 1176,
                    "end": 1180,
                    "matchedPaperCorpusId": "184486914"
                },
                {
                    "start": 1629,
                    "end": 1633,
                    "matchedPaperCorpusId": "255825985"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68115234375
        },
        {
            "corpus_id": "253734850",
            "title": "Deep Learning on a Healthy Data Diet: Finding Important Examples for Fairness",
            "text": "Third, the performance may degrade on the main downstream task (Zhang et al. 2020;Meade, Poole-Dayan, and Reddy 2022). In this work, we propose the gender equity (GE) score, which ranks the counterfactual examples based on their contribution to the overall fairness of the model. Intuitively, this score makes data augmentation methods more efficient by only using the examples that have the largest contribution to fairness. The pruning nature of our approach also helps exclude the harmful examples that degrade the overall fairness of the model by enforcing stereotypical associations about different genders. Our goal is to find the best trade-off between fairness and performance. Our contributions are summarized as follows: \n\n1. We propose a way to filter the examples that enforce undesired gender stereotypes from the training data, thus improving the overall fairness of the model compared to conventional data augmentation. 2. We reduce the redundancy in counterfactual data augmentation by pruning the counterfactual examples that are not important for fairness (i.e. the ones that do not contain gender words). 3. We study the effect of gender bias mitigation on downstream task performance and find that our method only shows a degradation of no more than 3% on the AUC over three popular language understanding tasks that are concerned with biases and stereotypes, compared to the original (biased) BERT and RoBERTa models.",
            "score": 0.5466996073683531,
            "section_title": "Introduction",
            "char_start_offset": 3483,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 730
                },
                {
                    "start": 733,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1438
                }
            ],
            "ref_mentions": [
                {
                    "start": 63,
                    "end": 82,
                    "matchedPaperCorpusId": "216641659"
                },
                {
                    "start": 82,
                    "end": 117,
                    "matchedPaperCorpusId": "239015827"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6708984375
        },
        {
            "corpus_id": "269449709",
            "title": "Bias Neutralization Framework: Measuring Fairness in Large Language Models with Bias Intelligence Quotient (BiQ)",
            "text": "The burgeoning influence of Large Language Models (LLMs) in shaping public discourse and decision-making underscores the imperative to address inherent biases within these AI systems. In the wake of AI's expansive integration across sectors, addressing racial bias in LLMs has never been more critical. This paper introduces a novel framework called Comprehensive Bias Neutralization Framework (CBNF) which embodies an innovative approach to quantifying and mitigating biases within LLMs. Our framework combines the Large Language Model Bias Index (LLMBI) [Oketunji, A., Anas, M., Saina, D., (2023)] and Bias removaL with No Demographics (BLIND) [Orgad, H., Belinkov, Y. (2023)] methodologies to create a new metric called Bias Intelligence Quotient (BiQ)which detects, measures, and mitigates racial bias in LLMs without reliance on demographic annotations. By introducing a new metric called BiQ that enhances LLMBI with additional fairness metrics, CBNF offers a multi-dimensional metric for bias assessment, underscoring the necessity of a nuanced approach to fairness in AI [Mehrabi et al., 2021]. This paper presents a detailed analysis of Latimer AI (a language model incrementally trained on black history and culture) in comparison to ChatGPT 3.5, illustrating Latimer AI's efficacy in detecting racial, cultural, and gender biases through targeted training and refined bias mitigation strategies [Latimer&Bender, 2023].",
            "score": 0.546636157371964,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85107421875
        },
        {
            "corpus_id": "272988113",
            "title": "Unlabeled Debiasing in Downstream Tasks via Class-wise Low Variance Regularization",
            "text": "Finally, Masoudian et al. (2024) used gated adapters to improve representational fairness while preserving task performance for classification and retrieval tasks. \n\nAlthough these methods effectively reduce sensitive attribute information and enhance fairness (Zerveas et al., 2022;Shen et al., 2022) through blindness, they depend on attribute labels to align the distribution of the target attribute. Since the user input data contains numerous nuanced protected attributes, such as age, race, religion, etc., it is challenging to collect labeled data for each individual attribute across every task. Moreover, supervised debiasing methods typically require training on each attribute individually, scaling linearly with the number of attributes. This complexity highlights the need for more efficient and scalable approaches to handle multiple protected attributes in debiasing efforts. \n\nTo address this limitation, some works attempt arXiv:2409.19541v3 [cs.CL] 2 Oct 2024 \n\nto debias language models without using attribute labels. Zhou et al. (2022) employ contrastive learning combined with instance weighting to reduce the bias encoded in the language model. Moreover, Cheng et al. (2020) utilize post-hoc contrastive learning to enhance the fairness of pre-trained encoder language models concerning gender bias. Ghanbarzadeh et al. (2023) integrate the masking objective used during the pre-training of encoder language models with fine-tuning on genderspecific tasks to address gender bias. These methods address gender bias without requiring labeled data using explicit gender indicators present within the text. However, they are ineffective against other biases such as age, race, or political view of the user, as well as implicit gender bias when gender information is removed from the text, limiting their possible use cases. \n\nIn this work, we bridge this gap by introducing a new regularization scheme based on classwise variance to reduce unknown (unlabeled) representational bias in the embeddings of LMs. Our regularization enforces low-variance embeddings, which results in mitigating any possible distributional shift caused by unknown attributes in the model's embeddings.",
            "score": 0.5455413600581855,
            "section_title": "Introduction and Background",
            "char_start_offset": 1671,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 166,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 890
                },
                {
                    "start": 893,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 977
                },
                {
                    "start": 980,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1625
                },
                {
                    "start": 1626,
                    "end": 1843
                },
                {
                    "start": 1846,
                    "end": 2027
                },
                {
                    "start": 2028,
                    "end": 2198
                }
            ],
            "ref_mentions": [
                {
                    "start": 261,
                    "end": 283,
                    "matchedPaperCorpusId": "248837769"
                },
                {
                    "start": 1038,
                    "end": 1056,
                    "matchedPaperCorpusId": "248496439"
                },
                {
                    "start": 1178,
                    "end": 1197,
                    "matchedPaperCorpusId": "232185104"
                },
                {
                    "start": 1323,
                    "end": 1349,
                    "matchedPaperCorpusId": "259859044"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84375
        },
        {
            "corpus_id": "236477795",
            "title": "He is very intelligent, she is very beautiful? On Mitigating Social Biases in Language Modelling and Generation",
            "text": "First, the methods used to mitigate gender biases may not readily extend to other demographics due to their greater diversity and lack of straightforward words to represent this diversity beyond the mentions of the groups themselves (e.g., Asian, African, Caucasian). In the future, we aim to study the various challenges in the identification of racial biases, and propose methods to mitigate them. Second, we  note that there is in general a greater association between certain neutral and demographic-defined words, such as dress to women, and beard to men, that exist not due to any social biases or stereotypes, and hence are to be preserved. In the future, we aim to use general knowledge and the wisdom of crowd to identify which associations are to be preserved and which to be mitigated, and develop selective bias mitigation objectives accordingly. Third, the SEAT measure can only predict the presence of a given type of bias, and not the absence of any potential bias in language models (Gonen and Goldberg, 2019;Liang et al., 2020); while we attempted to address residual clustering of certain words even upon equalizing in this work, in the future, we aim to work towards devising methods to understand and detect more implicit biases in language models. \n\nFourth, in the future, we aim to use representational similarities and world knowledge to devise more effective bias mitigation strategies for language generation models, as bias mitigation using word-based co-occurrences (as used in summarization) may sometimes lead to redundant bias mitigation. Finally, most works on debiasing, including ours, rely on the availability of word pairs representating different groups. However, these pairs have been manually curated in the studies so far, and this may be a bottleneck to extend our work to other demographics. In the future, we aim to automatically obtain word indicative of specific demographic groups, or the biases against them, using word similarities and associations.",
            "score": 0.5454586913016537,
            "section_title": "Limitations and Future Work",
            "char_start_offset": 26660,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 1268
                },
                {
                    "start": 1271,
                    "end": 1568
                },
                {
                    "start": 1569,
                    "end": 1690
                },
                {
                    "start": 1691,
                    "end": 1832
                },
                {
                    "start": 1833,
                    "end": 1996
                }
            ],
            "ref_mentions": [
                {
                    "start": 999,
                    "end": 1025,
                    "matchedPaperCorpusId": "73729169"
                },
                {
                    "start": 1025,
                    "end": 1044,
                    "matchedPaperCorpusId": "207996257"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.33203125
        },
        {
            "corpus_id": "271571036",
            "title": "Understanding the Interplay of Scale, Data, and Bias in Language Models: A Case Study with BERT",
            "text": "Our study broadly relates to three strands in the literature: fairness and auditing of machine learning algorithms in general, the study of biases in natural language processing more specifically, and scaling laws for large language models. \n\nFirst, a rich literature in computer science investigates issues of fairness in machine learning (Dwork et al. 2012;Barocas, Hardt, and Narayanan 2023), measuring the different types of harms these systems can have-such as denigration, stereotyping, differential quality of service etc. (Barocas et al. 2017;Weerts 2021). Notably, prior work has documented racial and gender disparities for commercial gender classification systems (Buolamwini and Gebru 2018), racial disparities in criminal recidivism prediction (Angwin et al. 2022), gender disparities in the delivery of job advertising (Datta, Tschantz, and Datta 2014;Ali et al. 2019), among others. Many of these studies only rely on \"black box\" access to machine learning systems, and have to conduct clever audits to measure disparate outcomes (Metaxa et al. 2021). Our work is connected to this literature in its goal of measuring inadvertent harms of a machine learning system, albeit with \"white box\" access to the model's output probabilities and weights. \n\nWithin natural language processing (NLP) specifically, prior work has also discussed biased and disparate outcomes for users. Blodgett, Green, and O'Connor (2016) was one of the earliest works documenting racial disparities, showing how dependency parsing tools struggle on text for African American English on Twitter. Similarly, Caliskan, Bryson, and Narayanan (2017) demonstrated how word embeddings learnt from text corpora can contain gender biases. In the context of large language models-which power most of modern NLP-recent work has documented negative associations for people with disabilities (Hutchinson et al. 2020), anti-Muslim bias (Abid, Farooqi, and Zou 2021), and a general propensity to generate toxic text (Gehman et al. 2020).",
            "score": 0.5454073762081441,
            "section_title": "Related Work",
            "char_start_offset": 8892,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 240
                },
                {
                    "start": 243,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 1066
                },
                {
                    "start": 1067,
                    "end": 1260
                },
                {
                    "start": 1263,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1582
                },
                {
                    "start": 1583,
                    "end": 1717
                },
                {
                    "start": 1718,
                    "end": 2010
                }
            ],
            "ref_mentions": [
                {
                    "start": 340,
                    "end": 359,
                    "matchedPaperCorpusId": "13496699"
                },
                {
                    "start": 675,
                    "end": 702,
                    "matchedPaperCorpusId": "3298854"
                },
                {
                    "start": 833,
                    "end": 866,
                    "matchedPaperCorpusId": "2862289"
                },
                {
                    "start": 1045,
                    "end": 1065,
                    "matchedPaperCorpusId": "244666050"
                },
                {
                    "start": 1389,
                    "end": 1425,
                    "matchedPaperCorpusId": "229013458"
                },
                {
                    "start": 1594,
                    "end": 1632,
                    "matchedPaperCorpusId": "23163324"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.44482421875
        },
        {
            "corpus_id": "270257839",
            "title": "Safeguarding Large Language Models: A Survey",
            "text": "Retraining models [56]is a direct way to reduce bias, although it can be resource-intensive and difficult to scale.For instance, FairBERTa is a fairer model for retraining RoBERTa on a large-scale demographic perturbation corpus Perturbation Augmentation NLP DAtaset (PANDA) containing 98K augmentation sample pairs [56].Moreover, an additional debiasing module is added after the encoder of LLMs to filter out the bias in the representation, and a common strategy is to utilize a contrastive learning framework for training [57].\n\nThe other line of debiasing studies aims to mitigate the extrinsic debiasing in a task-specific way.These studies attempt to improve fairness in downstream tasks by making models provide consistent outputs across different demographic groups.Many studies have concentrated on reducing bias through model adaptation approaches.A bias mitigating method, DAMA [49], can reduce bias while maintaining model performance on downstream tasks.[58] investigated the bias in CtB-LLMs and demonstrated the effectiveness of debiasing techniques.They find that bias depends not solely on the number of parameters but also on factors like perplexity and that techniques like debiasing of OPT using LoRA can significantly reduce bias.[59] demonstrated that the Stereotype Content Model, which posits that minority groups are often perceived as cold or incompetent, applies to contextualized word embeddings and presents a successful fine-tuning method to reduce such biases.Moreover, [60] proposed a novel adversarial learning debiasing method applied during the pre-training of LLMs.[61] mitigated cultural bias through fine-tuning models on culturally relevant data, yet it requires resources that make it accessible to only a few.\n\nInstead of fine-tuning parameters, several studies directly set up guardrails on the interaction between users and LLMs by exploring the control of input and output.[62] suggested using purposely designed code generation templates to mitigate the bias in code generation tasks.[43] found that cultural prompting is a simple and effective method to reduce cultural bias in the latest LLMs.However, it may be ineffective or even exacerbate bias in some countries.",
            "score": 0.5453166216020972,
            "section_title": "Fairness",
            "char_start_offset": 32417,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 115,
                    "end": 321
                },
                {
                    "start": 321,
                    "end": 530
                },
                {
                    "start": 532,
                    "end": 632
                },
                {
                    "start": 632,
                    "end": 774
                },
                {
                    "start": 774,
                    "end": 858
                },
                {
                    "start": 858,
                    "end": 967
                },
                {
                    "start": 967,
                    "end": 1065
                },
                {
                    "start": 1065,
                    "end": 1251
                },
                {
                    "start": 1251,
                    "end": 1491
                },
                {
                    "start": 1491,
                    "end": 1601
                },
                {
                    "start": 1601,
                    "end": 1750
                },
                {
                    "start": 1752,
                    "end": 1917
                },
                {
                    "start": 1917,
                    "end": 2029
                },
                {
                    "start": 2029,
                    "end": 2140
                },
                {
                    "start": 2140,
                    "end": 2213
                }
            ],
            "ref_mentions": [
                {
                    "start": 18,
                    "end": 22,
                    "matchedPaperCorpusId": "249062690"
                },
                {
                    "start": 316,
                    "end": 320,
                    "matchedPaperCorpusId": "249062690"
                },
                {
                    "start": 525,
                    "end": 529,
                    "matchedPaperCorpusId": "249847943"
                },
                {
                    "start": 1501,
                    "end": 1505,
                    "matchedPaperCorpusId": "266349513"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72119140625
        },
        {
            "corpus_id": "263333947",
            "title": "Unlocking Bias Detection: Leveraging Transformer-Based Models for Content Analysis",
            "text": "NLP systems have long been susceptible to the issue of bias, leading to unfair representation in their outcomes [18]. Numerous studies [5], [19], [20] have highlighted how societal and cultural biases inadvertently enter training data. These biases can undermine the integrity of NLP outcomes, perpetuating, and at times amplifying, societal disparities [1]. \n\nThe study of bias addresses various forms of discrimination and misrepresentation in text [21]. A wealth of research has been directed at understanding and mitigating biases in language models and embeddings [14], [20], [22]. Significant findings include the identification of gender bias in popular embeddings like GloVe and Word2Vec [23], and the development of the Word Embedding Association Test (WEAT) to quantify biases in word embeddings [19]. This methodology was later extended to assess biases in sentence encoders [6]. There have been documented efforts to reduce biases in BERT by fine-tuning with counterfactual datasets [24]. Research has also delved into gender and representation biases in GPT-3 [25], and the perpetuation of biases in conversational AI systems [26], highlighting the critical importance of bias mitigation in dialogue generation. These efforts collectively contribute to the pursuit of more equitable and unbiased language technologies. \n\nIn a related work, a system has been asked to detect hate speech and provide explanations [27]. Concurrently, another study explored biases in text-based event detection, addressing 1 Warning: This paper contains content that may be perceived as offensive or biased, intended for research purposes. \n\nboth data scarcity and annotation challenges [28]. The research presented in [29] investigates the relations between different forms of biases in NLP models, specifically examining bias mitigation in toxicity detection and word embeddings. This study concentrates on three social identities: race, gender, and religion, suggesting that biases can be correlated and that standalone debiasing methods may prove inadequate. \n\nThere is a rich diversity in research focusing on different facets of bias in NLP.",
            "score": 0.544786864040036,
            "section_title": "II. RELATED WORK",
            "char_start_offset": 4225,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 358
                },
                {
                    "start": 361,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1331
                },
                {
                    "start": 1334,
                    "end": 1429
                },
                {
                    "start": 1430,
                    "end": 1632
                },
                {
                    "start": 1635,
                    "end": 1685
                },
                {
                    "start": 1686,
                    "end": 1874
                },
                {
                    "start": 1875,
                    "end": 2055
                },
                {
                    "start": 2058,
                    "end": 2140
                }
            ],
            "ref_mentions": [
                {
                    "start": 112,
                    "end": 116,
                    "matchedPaperCorpusId": "233832881"
                },
                {
                    "start": 135,
                    "end": 138,
                    "matchedPaperCorpusId": "244119684"
                },
                {
                    "start": 140,
                    "end": 144,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 146,
                    "end": 150,
                    "matchedPaperCorpusId": "246434841"
                },
                {
                    "start": 354,
                    "end": 357,
                    "matchedPaperCorpusId": "247050244"
                },
                {
                    "start": 451,
                    "end": 455,
                    "matchedPaperCorpusId": "237454587"
                },
                {
                    "start": 575,
                    "end": 579,
                    "matchedPaperCorpusId": "246434841"
                },
                {
                    "start": 581,
                    "end": 585,
                    "matchedPaperCorpusId": "245117373"
                },
                {
                    "start": 696,
                    "end": 700,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 806,
                    "end": 810,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 886,
                    "end": 889,
                    "matchedPaperCorpusId": "85518027"
                },
                {
                    "start": 1073,
                    "end": 1077,
                    "matchedPaperCorpusId": "235097208"
                },
                {
                    "start": 1139,
                    "end": 1143,
                    "matchedPaperCorpusId": "235358955"
                },
                {
                    "start": 1424,
                    "end": 1428,
                    "matchedPaperCorpusId": "252408775"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62158203125
        },
        {
            "corpus_id": "265281304",
            "title": "Bias A-head? Analyzing Bias in Transformer-Based Language Model Attention Heads",
            "text": "Transformer-based pretrained large language models (PLM) such as BERT and GPT have achieved remarkable success in NLP tasks. However, PLMs are prone to encoding stereotypical biases. Although a burgeoning literature has emerged on stereotypical bias mitigation in PLMs, such as work on debiasing gender and racial stereotyping, how such biases manifest and behave internally within PLMs remains largely unknown. Understanding the internal stereotyping mechanisms may allow better assessment of model fairness and guide the development of effective mitigation strategies. In this work, we focus on attention heads, a major component of the Transformer architecture, and propose a bias analysis framework to explore and identify a small set of biased heads that are found to contribute to a PLM's stereotypical bias. We conduct extensive experiments to validate the existence of these biased heads and to better understand how they behave. We investigate gender and racial bias in the English language in two types of Transformer-based PLMs: the encoder-based BERT model and the decoder-based autoregressive GPT model. Overall, the results shed light on understanding the bias behavior in pretrained language models.",
            "score": 0.5443953293320843,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86328125
        },
        {
            "corpus_id": "257232850",
            "title": "How optimal transport can tackle gender biases in multi-class neural-network classifiers for job recommendations?",
            "text": "We can reduce the bias directly in the text of the training dataset. For example, in the case of gender bias like the study in this paper, the most classic technique is to remove explicit gender indicators [9]. This technique is the one we will use to compare our proposed strategy to another one commonly used in industry. This technique is indeed simple to implement and makes it possible to reduce the bias, but in a partial and not very localized manner. Other classical techniques can be used, like identifying biased data in word embeddings, which represent words in a vector space. [20] demonstrated that these embeddings reflect societal biases. There are also methods to show how these embeddings can be unbiased by aligning them with a set of neutral reference vectors [21,22]. These de-biasing methods have however strong limitations, as explained in [23], where the authors show that although the de-biasing embedding methods can reduce the visibility of gender bias, they do not completely eliminate it. \n\nA second approach is to use post-processing de-biasing methods. These methods are model-agnostic and therefore not specific to NLP since they modify the results of previously trained classifiers in order to achieve fairer results. [11,24] investigate this for binary classification, and [25] propose a method for multiclass classification. \n\nThe last approach to mitigate biases in AI is to use fairness-aware algorithms, which are specifically designed to take into account the potential for bias and to learn from the data in a way that reduces the risk of making biased decisions. These are the in-processing methods, which generally do not depend on the type of data input, either. The method we propose in this paper is one of them. To achieve this, we can use adversarial Learning by adjusting the discriminator. Adversarial learning involves training a model to make predictions while also training a second model to identify and correct any biases in the first model's predictions. By incorporating this technique into the training process, [26,27] demonstrate that it is possible to reduce the amount of bias present in machine learning models.",
            "score": 0.5441293489658854,
            "section_title": "Impact of AI biases in society",
            "char_start_offset": 9118,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 68
                },
                {
                    "start": 69,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 458
                },
                {
                    "start": 459,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 1016
                },
                {
                    "start": 1019,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1358
                },
                {
                    "start": 1361,
                    "end": 1602
                },
                {
                    "start": 1603,
                    "end": 1704
                },
                {
                    "start": 1705,
                    "end": 1756
                },
                {
                    "start": 1757,
                    "end": 1837
                },
                {
                    "start": 1838,
                    "end": 2008
                },
                {
                    "start": 2009,
                    "end": 2172
                }
            ],
            "ref_mentions": [
                {
                    "start": 206,
                    "end": 209,
                    "matchedPaperCorpusId": "58006082"
                },
                {
                    "start": 589,
                    "end": 593,
                    "matchedPaperCorpusId": "4930886"
                },
                {
                    "start": 783,
                    "end": 786,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 1250,
                    "end": 1254,
                    "matchedPaperCorpusId": "7567061"
                },
                {
                    "start": 1254,
                    "end": 1257,
                    "matchedPaperCorpusId": "249872742"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80322265625
        },
        {
            "corpus_id": "270869465",
            "title": "The African Woman is Rhythmic and Soulful: An Investigation of Implicit Biases in LLM Open-ended Text Generation",
            "text": "Additionally, the application of fairness constraints and regularization techniques within the training process alters the model's objective function to discourage biases, promoting the creation of more balanced and inclusive AI systems. \n\nWe therefore developed a scoring methodology used to evaluate bias mitigation techniques in Large Language Models (LLMs) to offer a comparative analysis that highlights both the strengths and limitations of each approach. Each technique was assessed across multiple dimensions -transparency, cultural sensitivity, and effectiveness. The scores for these dimensions were derived through a combination of literature review, empirical findings, and qualitative evaluations of how these techniques perform in practical settings. For example, adversarial training received a high score for its effectiveness in reducing certain explicit biases but was marked lower for transparency due to the complexity and opacity of its implementation. In contrast, model auditing scored higher for transparency but displayed limitations in direct bias mitigation, particularly in subtle or implicit bias contexts. The novelty of this scoring framework lies in its interdisciplinary approach, which integrates technical assessments typically found in computer science with insights from social sciences, such as digital anthropology and ethics. \n\nInstead of focusing solely on the computational effectiveness of each technique, the scoring system accounts for how these techniques perform when contextualized within broader societal and cultural frameworks. By including dimensions like cultural sensitivity and transparency, the scoring system acknowledges the importance of not just technical solutions but also their ethical and societal impact. This approach offers a more holistic evaluation, emphasizing that successful bias mitigation must go beyond optimizing model performance and incorporate a deeper understanding of the human contexts in which these technologies are applied. More information on the exact scores can be found in the appendix. Transparency, Direct Mitigation, and Cultural Sensitivity. This structured approach facilitates a systematic comparison that underscores distinct capabilities and limitations inherent to each method. \n\n\u2022 The Transparency Score is attributed based on the extent to which a technique facilitates understanding and visibility into the AI's decision-making processes. Techniques that allow stakeholders to readily observe and understand how decisions are made by the AI, including the provenance of data and the mechanics of algorithmic decisions, receive higher scores. This dimension is particularly influenced by the degree to which methods disclose information about training data, algorithmic design, and operational mechanisms.",
            "score": 0.5439837016372195,
            "section_title": "BACKGROUND AND LITERATURE REVIEW",
            "char_start_offset": 11537,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 237
                },
                {
                    "start": 240,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 973
                },
                {
                    "start": 974,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1365
                },
                {
                    "start": 1368,
                    "end": 1578
                },
                {
                    "start": 1579,
                    "end": 1769
                },
                {
                    "start": 1770,
                    "end": 2008
                },
                {
                    "start": 2009,
                    "end": 2075
                },
                {
                    "start": 2076,
                    "end": 2134
                },
                {
                    "start": 2135,
                    "end": 2275
                },
                {
                    "start": 2278,
                    "end": 2439
                },
                {
                    "start": 2440,
                    "end": 2642
                },
                {
                    "start": 2643,
                    "end": 2805
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87353515625
        },
        {
            "corpus_id": "258174286",
            "title": "Improving Gender-Related Fairness in Sentence Encoders: A Semantics-Based Approach",
            "text": "[27] presented a method to preserve genderrelated information in feminine and masculine words while removing bias from stereotypical words. Still using GloVe as language model, [56] described an innovative procedure called Double-Hard Debias, to cope with changes in word frequency statistics that commonly have an undesirable impact on standard debasing methods. [60] describes a novel method exploiting causal inference, to reduce not only gender bias associated with a gender direction, but also gender bias from word embedding relations. \n\nMore recently, contextualised word embeddings like BERT [18] proved to be very accurate language models. However, despite the literature suggesting that they are generally less biased compared to their static counterparts [2], they still display a significant amount of social bias [36]. WEAT was extended to measure bias in sentence embedding encoders: the Sentence Encoder Association Test (SEAT) is again based on the evaluation of implicit associations and shows that modern sentence embeddings also exhibit social bias [36]. Meanwhile, attempts at debiasing sentence embeddings faced the issue of not being able to recognise neutral sentences, thus debiasing every representation regardless of the gender attributes in the original natural language sentence, leading to a loss of correct semantics [30]. \n\nRecently, [61] suggested the generation of implicit gender bias samples at sentence-level, which, along with a novel metric, can be used to accurately measure gender bias on contextualised embeddings. [28] proposed a fine-tuning method for debiasing word embeddings that can be applied to any pre-trained language model. Additionally, researchers have started working on generative transformer models. For instance, [25] proposed to mitigate gender disparity in text generation by learning a fair model with knowledge distillation. Last but not least, two comprehensive survey papers highlighted the latest advances on this front: [45] presents an overview of the most common debiasing methods in the context of vision and language research, while [37] proposes a deep empirical analysis of several bias mitigation techniques with different language models.",
            "score": 0.5436221105073872,
            "section_title": "Related Work",
            "char_start_offset": 7640,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 541
                },
                {
                    "start": 544,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 831
                },
                {
                    "start": 832,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1352
                },
                {
                    "start": 1355,
                    "end": 1555
                },
                {
                    "start": 1556,
                    "end": 1675
                },
                {
                    "start": 1676,
                    "end": 1756
                },
                {
                    "start": 1757,
                    "end": 1886
                },
                {
                    "start": 1887,
                    "end": 2212
                }
            ],
            "ref_mentions": [
                {
                    "start": 177,
                    "end": 181,
                    "matchedPaperCorpusId": "204770514"
                },
                {
                    "start": 364,
                    "end": 368,
                    "matchedPaperCorpusId": "208267842"
                },
                {
                    "start": 600,
                    "end": 604,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 766,
                    "end": 769,
                    "matchedPaperCorpusId": "121125604"
                },
                {
                    "start": 826,
                    "end": 830,
                    "matchedPaperCorpusId": "85518027"
                },
                {
                    "start": 1068,
                    "end": 1072,
                    "matchedPaperCorpusId": "85518027"
                },
                {
                    "start": 1347,
                    "end": 1351,
                    "matchedPaperCorpusId": "207996257"
                },
                {
                    "start": 1556,
                    "end": 1560,
                    "matchedPaperCorpusId": "231698657"
                },
                {
                    "start": 1771,
                    "end": 1775,
                    "matchedPaperCorpusId": "247619104"
                },
                {
                    "start": 2103,
                    "end": 2107,
                    "matchedPaperCorpusId": "239015827"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82275390625
        },
        {
            "corpus_id": "272880907",
            "title": "Do the Right Thing, Just Debias! Multi-Category Bias Mitigation Using LLMs",
            "text": "Section 3 provides a detailed discussion on the preparation strategy for ANUBIS. \n\nWhile existing research has made significant progress in mitigating bias in language models, there is a growing need for models that demonstrate robustness across different domains. The ability to generalize beyond the specific training data is crucial for real-world applications, where language-models are likely to encounter diverse contexts and perspectives. While domain adapta- tion techniques involving fine-tuning, have shown promise (Sun et al., 2020;Wang et al., 2020), they often require substantial amount of training data from the target domain. This limits their practicality and raises a crucial question: Can bias mitigation models trained in one category effectively generalize to multiple new categories without extensive training data? To answer this question, we conducted extensive experiments on two different training datasets: WIKIBIAS and ANUBIS, evaluating them on various metrics (as mentioned in Section 4). In the effort for sustainable and environmentally conscious practices, we also assessed the \"greenness\" of the models trained on these datasets, highlighting the importance of considering environmental impact alongside traditional evaluation metrics (See Section 6). Our research makes the following contributions: \n\n\u2022 We have evaluated the effectiveness of stateof-the-art models, including the T5 model and Large Language Models, in mitigating multi-class social bias in texts using Supervised Fine-Tuning (SFT). \n\n\u2022 We have set up a tri-step configuration to effectively reduce bias in texts using Supervised Fine-Tuning (SFT), coupled with Reinforcement Learning (RL) techniques including a) \n\nProximal Policy Optimization (PPO) and b) Direct Preference Optimization (DPO) (As illustrated in Figure 1), and as a third step, c) In-Context Learning (ICL) (See Section 6). \n\n\u2022 We introduce ANUBIS + : ANother UnBIased dataSet + , a new dataset consisting of 1507 perfectly debiased sentence pairs spanning 9 different bias classes (See Table 3) providing a valuable resource for fine-tuning and aligning language models to effectively mitigate bias in texts while preserving context and linguistic quality, and devised a simple yet strict grammar-based evaluation metric to classify a given sentence pair as biased or debiased (See Section 3).",
            "score": 0.5433373265695309,
            "section_title": "Introduction",
            "char_start_offset": 2114,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 83,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 837
                },
                {
                    "start": 838,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1333
                },
                {
                    "start": 1336,
                    "end": 1533
                },
                {
                    "start": 1536,
                    "end": 1714
                },
                {
                    "start": 1717,
                    "end": 1892
                },
                {
                    "start": 1895,
                    "end": 2363
                }
            ],
            "ref_mentions": [
                {
                    "start": 525,
                    "end": 543,
                    "matchedPaperCorpusId": "220301705"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69287109375
        },
        {
            "corpus_id": "250390904",
            "title": "Benchmarking Intersectional Biases in NLP",
            "text": "There has been a recent wave of work assessing the fairness of machine learning models in general, and more specifically, on natural language processing (NLP) models built using machine learning techniques. While much work has highlighted biases embedded in state-of-the-art language models, and more recent efforts have focused on how to debias, research assessing the fairness and performance of biased/debiased models on downstream prediction tasks has been limited. Moreover, most prior work has emphasized bias along a single dimension such as gender or race. In this work, we benchmark multiple NLP models with regards to their fairness and predictive performance across a variety of NLP tasks. In particular, we assess intersectional bias - fairness across multiple demographic dimensions. The results show that while current debiasing strategies fare well in terms of the fairness-accuracy trade-off (generally preserving predictive power in debiased models), they are unable to effectively alleviate bias in downstream tasks. Furthermore, this bias is often amplified across dimensions (i.e., intersections). We conclude by highlighting possible causes and making recommendations for future NLP debiasing research.",
            "score": 0.5429303341795849,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8125
        },
        {
            "corpus_id": "264309222",
            "title": "Multi-objective search for gender-fair and semantically correct word embeddings",
            "text": "In order to improve algorithm fairness, three types of approaches can be applied: pre-processing, in-processing and postprocessing. We refer the reader to the literature review by Sun et al. [58] for further details on techniques including the debiasing of training corpora (pre-processing) and debiasing by adjusting algorithms (in-processing). In the following, we focus on postprocessing debiasing methods for word embeddings as our work belongs to this category. \n\nPost-processing bias methods mitigate bias after a model has been trained. To reduce gender bias, Bolukbasi et al. [20] proposed the following Hard Debiasing (HD) post-processing method: They first identified a gender subspace and then proceeded to neutralise it, ensuring that the performance on the evaluation tasks is maintained. To determine the gender subspace, they identified a gender direction g \u2208 R d by combining several directions (e.g., \n\n. This approach has however been criticised to have several shortcomings [59] and alternative approaches have been sought to overcome them. To this end, Dev and Phillips [29] debiased word embeddings by using a linear projection along the gender direction, whereas, Lauscher et al. [31] adjusted the linear projection by using an alternative projecting approach and a neural network to learn word vectors transformation. Instead, Kaneko and Bollegala [60] proposed the use of an autoencoder to remove biases from pre-trained word embeddings. Shin et al. [61] proposed a latent disentanglement method to obtain gender-neutralised word embeddings. Ravfogel et al. [30] presented an Iterative Null-space Projection (INLP) method. Instead of specifying a gender direction, as done by Bolukbasi et al. [20], INLP learns this direction with a linear classifier and removes it by iteratively projecting the word embeddings on their null-space. Kaneko and Bollegala [62] debiased pre-trained word embeddings with the use of dictionaries. \n\nIn this work, we take a different route to mitigate bias in word embeddings.",
            "score": 0.5428220667327114,
            "section_title": "Debiasing word embeddings",
            "char_start_offset": 12055,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 466
                },
                {
                    "start": 469,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 917
                },
                {
                    "start": 920,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1340
                },
                {
                    "start": 1341,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1565
                },
                {
                    "start": 1566,
                    "end": 1646
                },
                {
                    "start": 1647,
                    "end": 1856
                },
                {
                    "start": 1857,
                    "end": 1949
                },
                {
                    "start": 1952,
                    "end": 2028
                }
            ],
            "ref_mentions": [
                {
                    "start": 584,
                    "end": 588,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 1202,
                    "end": 1206,
                    "matchedPaperCorpusId": "202572693"
                },
                {
                    "start": 1717,
                    "end": 1721,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 1878,
                    "end": 1882,
                    "matchedPaperCorpusId": "231698423"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9384765625
        },
        {
            "corpus_id": "276937968",
            "title": "Aligning to What? Limits to RLHF Based Alignment",
            "text": "After training, we use the methods from Hofmann et al. (2024) to detect and monitor both explicit and implicit biases that may still be present in our model. A subset of the post-training and bias evaluations are repeated on Mistral (Jiang et al., 2023) to see both a different baseline for LLM biases and if RLHF influences different models uniquely. We also study the effects of extended post-training and the influence of different datasets on alignment. Although Llama 3 8B and Mistral are the only models to undergo post-training we conduct additional bias evaluations on Llama 3 Instruct, Llama 3.1 and its instruct tuned version, and Llama 3.2 and its instruct tuned version. \n\nAll instruct versions have undergone extensive post-training (Dubey et al., 2024)  our experiments, and comparing Llama 3 -3.2 will allow us to see how the base LLM biases have changed as models become more capable. \n\nWe also curate a new preference dataset containing only AAE text to see if the abundance of SAE text in pretraining is responsible for the biases, and whether or not further post-training on this dataset will meaningfully reduce bias. Finally, we extend current techniques limited to just language models to multimodal models to gather reducedvariance measurements of the models overt biases; this extension is performed on Llama 3.2 Vision 11B. Measuring overt biases in LLMs depends on explicit racial group names, whose limited availability leads to high variance measurements, but with VLMs explicit racial information is instead encoded into images of people. \n\nOur initial experiments seem to indicate that models are conditioned with covert biases after pretraining, and the overall nature of these biases are not influenced in any meaningful ways by RLHF regardless of post-training technique, dataset, or base model. Examining large-scale-post-trained models such as Llama 3-Instruct leads one to believe that with current techniques, to meaningfully alter a model's biases you need to introduce new ones. We also find that supervised fine-tuning prior to RLHF appears to calcify the model biases and make them more resistant to change.",
            "score": 0.542446697521001,
            "section_title": "Introduction",
            "char_start_offset": 1764,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 682
                },
                {
                    "start": 685,
                    "end": 900
                },
                {
                    "start": 903,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1567
                },
                {
                    "start": 1570,
                    "end": 1828
                },
                {
                    "start": 1829,
                    "end": 2017
                },
                {
                    "start": 2018,
                    "end": 2148
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6171875
        },
        {
            "corpus_id": "235352965",
            "title": "Towards Equal Gender Representation in the Annotations of Toxic Language Detection",
            "text": "Previous research into gender bias in toxic language detection caused by the demographic makeup of annotators explored superficial differences between male and female annotators, but only reflected on the ethical considerations involved rather than thoroughly investigating the differences between annotator groups and attempting to minimise bias in the model. Binns et al. (2017) presented different methods for detecting potential bias by building classifiers trained on comments whose annotators belong to different genders. They reported differences in average toxicity scores and inter-annotator agreement between the groups. Similar work by Sap et al. (2019) in the field of racial bias examined toxicity scores given to Twitter corpora, where the white annotators in the majority give higher toxicity scores to tweets exhibiting an African American English dialect, demonstrating how annotator opinions can propagate bias throughout the model. Some studies focused on gender bias in specific tasks in Natural Language Processing such as coreference resolution. The aim of those studies is to eliminate under-representation bias by applying gender-swapping and name anonymisation to a corpus to balance the use of gender-specific words (Zhao et al., 2018). Sun et al. (2019) highlights this technique as an effective way of debiasing models and measuring gender bias in predictions, using the False Positive Equality Distance (FPED) and False Negative Equality Distance (FNED) metrics (Dixon et al., 2018) to measure the difference in performance for gender-swapped sentences. \n\nAnother common source of bias is the word embeddings, which can form associations between identity groups and stereotypical terms based on their prevalence in the literature used to train the language model. Bolukbasi et al. (2016) demonstrated the presence of gender bias in occupations in the word embeddings of a language model and proposed a system to debias those models by isolating the gender subspace before utilising hard or soft debiasing to remove the gender bias from terms identified as being gender neutral. This was further extended by Manzini et al. (2019) to encompass racial bias, transforming the binary classification task of identifying gender-specific and gender neutral terms into a multiclass debiasing problem.",
            "score": 0.5423008097314275,
            "section_title": "Related Work",
            "char_start_offset": 4697,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1262
                },
                {
                    "start": 1263,
                    "end": 1582
                },
                {
                    "start": 1585,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 2106
                },
                {
                    "start": 2107,
                    "end": 2320
                }
            ],
            "ref_mentions": [
                {
                    "start": 647,
                    "end": 664,
                    "matchedPaperCorpusId": "196211238"
                },
                {
                    "start": 1242,
                    "end": 1261,
                    "matchedPaperCorpusId": "4952494"
                },
                {
                    "start": 1263,
                    "end": 1280,
                    "matchedPaperCorpusId": "195316733"
                },
                {
                    "start": 1491,
                    "end": 1511,
                    "matchedPaperCorpusId": "54997157"
                },
                {
                    "start": 1793,
                    "end": 1816,
                    "matchedPaperCorpusId": "1704893"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76611328125
        },
        {
            "corpus_id": "271902917",
            "title": "REFINE-LM: Mitigating Language Model Stereotypes via Reinforcement Learning",
            "text": "In this paper, we introduced the REFINE-LM approach to mitigate the stereotypical bias encoded in pre-trained LMs without hurting model performance. The proposed techniques make use of a large corpus of under-specified questions and reinforcement learning techniques to suppress different types of stereotypical bias in LMs, including gender-, nationality-, ethnicity-, and religion-based biases. \n\nOur evaluation results conducted on small and large language models open the door for further research avenues, which we envision to explore. Firstly, we envision to extend this empirical study to further bias datasets such as CrowS-pairs [34] and BBQ [35]. Secondly, we intend to carry out an extensive performance evaluation on different downstream tasks -e.g., conversational agents, text generation and summarization -, support for multilingual LMs, and efficient training of multiple bias types simultaneously.",
            "score": 0.5420722128511419,
            "section_title": "Conclusion and Perspectives",
            "char_start_offset": 23920,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 396
                },
                {
                    "start": 399,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 914
                }
            ],
            "ref_mentions": [
                {
                    "start": 638,
                    "end": 642,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 651,
                    "end": 655,
                    "matchedPaperCorpusId": "239010011"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83251953125
        },
        {
            "corpus_id": "264306211",
            "title": "Identifying and Adapting Transformer-Components Responsible for Gender Bias in an English Language Model",
            "text": "Language models (LMs) exhibit and amplify many types of undesirable biases learned from the training data, including gender bias. However, we lack tools for effectively and efficiently changing this behavior without hurting general language modeling performance. In this paper, we study three methods for identifying causal relations between LM components and particular output: causal mediation analysis, automated circuit discovery and our novel, efficient method called DiffMask+ based on differential masking. We apply the methods to GPT-2 small and the problem of gender bias, and use the discovered sets of components to perform parameter-efficient fine-tuning for bias mitigation. Our results show significant overlap in the identified components (despite huge differences in the computational requirements of the methods) as well as success in mitigating gender bias, with less damage to general language modeling compared to full model fine-tuning. However, our work also underscores the difficulty of defining and measuring bias, and the sensitivity of causal discovery procedures to dataset choice. We hope our work can contribute to more attention for dataset development, and lead to more effective mitigation strategies for other types of bias.",
            "score": 0.5415145753198071,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88525390625
        },
        {
            "corpus_id": "262828449",
            "title": "Survey of Social Bias in Vision-Language Models",
            "text": "\u2022 Inference-Time Debiasing FairFil [34] adopts contrastive learning to de-bias the output of the pre-trained text encoder of BERT [47]. CORSAIR [133] incorporates counterfactual inference by using a poisonous text classifier to post-adjust the output predictions during inference. Sheng et al. [160] reduce bias towards different social demographics in the generated text by prepending trigger phrases in the prompt for free-form generation helps to reduce social biases. Self-Debias [152] leverages internal knowledge of pre-trained models to reduce the probabilities of generating biased text. UDDIA [190] utilizes a unified detoxifying and debiasing rectification method to reduce the bias and toxicity of the generated output while maintaining its quality. \n\nCriticism over de-biasing embedding spaces: Gonen and Goldberg [66] point out that de-biasing static word embeddings may just temporarily cover up bias. Moreover, Goldfarb-Tarrant et al. [65] empirically show that these de-biasing strategies do not necessarily yield de-biasing in downstream NLP tasks through zero or negative correlation among intrinsic and extrinsic bias measurements. \n\nIn addition to the aforementioned methods, other extensive literature focuses on bias mitigation methods in NLP. We share the gist of them for ease of reference. Sun et al. [167] provide a survey on mitigation methods for gender bias. Meade et al. [121] provide survey on current state-of-the-art de-biasing techniques for pre-trained LMs. Sheng et al. [161] suggest an extensive survey focused on societal bias including various demographics in language generation models.",
            "score": 0.5414372224794555,
            "section_title": "Bias Mitigation.",
            "char_start_offset": 27198,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 760
                },
                {
                    "start": 763,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 1150
                },
                {
                    "start": 1153,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1314
                },
                {
                    "start": 1315,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1492
                },
                {
                    "start": 1493,
                    "end": 1626
                }
            ],
            "ref_mentions": [
                {
                    "start": 35,
                    "end": 39,
                    "matchedPaperCorpusId": "232185104"
                },
                {
                    "start": 144,
                    "end": 149,
                    "matchedPaperCorpusId": "236459953"
                },
                {
                    "start": 294,
                    "end": 299,
                    "matchedPaperCorpusId": "218470535"
                },
                {
                    "start": 484,
                    "end": 489,
                    "matchedPaperCorpusId": "232075876"
                },
                {
                    "start": 602,
                    "end": 607,
                    "matchedPaperCorpusId": "252780503"
                },
                {
                    "start": 826,
                    "end": 830,
                    "matchedPaperCorpusId": "73729169"
                },
                {
                    "start": 950,
                    "end": 954,
                    "matchedPaperCorpusId": "229923772"
                },
                {
                    "start": 1401,
                    "end": 1406,
                    "matchedPaperCorpusId": "239015827"
                },
                {
                    "start": 1506,
                    "end": 1511,
                    "matchedPaperCorpusId": "234337004"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64013671875
        },
        {
            "corpus_id": "268856702",
            "title": "Fairness in Large Language Models: A Taxonomic Survey",
            "text": "Post-processing approaches modify the results generated by the model to mitigate biases, which is particularly crucial for closedsource LLMs where obtaining probabilities and embeddings of generated text is challenging, limiting the direct modification to output results only.Here, the method of chain-of-thought and rewriting serve as the illustrative approaches to convey this concept.\n\ni) Chain-of-thought (CoT).The CoT technique enhances the hopeful and performance of LLMs towards fairness by leading them through incremental reasoning steps.The work by Kaneko et al. [87] provided a benchmark test where LLMs were tasked with determining the gender associated with specific occupational terms.Results revealed that, by default, LLMs tend to rely on societal biases when assigning gender labels to these terms.However, incorporating CoT prompts mitigates these biases.Furthermore, Dhingra et al. [47] introduced a technique combining CoT prompts and SHAP analysis [110] to counter stereotypical language towards queer individuals in model outputs.Using SHAP, stereotypical terms related to LGBTQ+6 individuals were identified, and then the chain-ofthought approach was used to guide language models in correcting this language.\n\nii) Rewriting.Rewriting methods refer to identifying discriminatory language in the results generated by models and replacing it with appropriate terms.As an illustration, Tokpo and Calders [179] introduced a text-style transfer model capable of training on nonparallel data.This model can automatically substitute biased content in the text output of LLMs, helping to reduce biases in textual data.",
            "score": 0.5412020840249772,
            "section_title": "Post-processing",
            "char_start_offset": 34680,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 276
                },
                {
                    "start": 276,
                    "end": 387
                },
                {
                    "start": 389,
                    "end": 415
                },
                {
                    "start": 415,
                    "end": 547
                },
                {
                    "start": 547,
                    "end": 699
                },
                {
                    "start": 699,
                    "end": 815
                },
                {
                    "start": 815,
                    "end": 873
                },
                {
                    "start": 873,
                    "end": 1052
                },
                {
                    "start": 1052,
                    "end": 1232
                },
                {
                    "start": 1234,
                    "end": 1248
                },
                {
                    "start": 1248,
                    "end": 1386
                },
                {
                    "start": 1386,
                    "end": 1509
                },
                {
                    "start": 1509,
                    "end": 1633
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8330078125
        },
        {
            "corpus_id": "247050244",
            "title": "Dbias: detecting biases and ensuring fairness in news articles",
            "text": "Transfer learning is a technique to transfer the knowledge contained in larger, different but related source domain to a target domain [31]. The goal is to improve the performance of target domain with the existing knowledge of the source domain. Bidirectional Encoder Representations from Transformers (BERT) [32] is an example, which has shown state-of-the-art performance in many tasks like classification, question answering, and so on. \n\nLi et al. 2021 [33] study the gender bias inside the Transformer-based model (BERT). They calculate the attention scores for the corresponding gender pronouns and occupations, swap the gender pronouns to eliminate the position effect on bias judgement, and then again check the consistency of the gender bias associated with the occupation. Sinha and Dasgupta [34] employ the BERT model to detect biases from the text. Both models, while equally important, are primarily concerned with the detection and extraction of biased sentences. \n\nThe task of identifying a named entity (a realworld object or concept) in unstructured text and then classifying the entity into a standard category is known as named entity recognition [35]. Mehrabi et al. 2020 [13] use named entities to determine whether female names are more frequently tagged as non-person than male names. Some other researchers [36], [37] use the named entities to identify biases based on occupation, race, and demographics. These named entity recognition models usually recognize the biased entities from the data. However, the mitigation technique is not their objective. \n\nThe masked language modeling is also used to identify biases. Based on two crowdsourced datasets, Kaneko and Bollegala 2021 [38] propose a technique to accurately predict different types of social biases in text. They [38] demonstrate that social biases do exist in masked language models and suggest developing methods to robustly debias pre-trained masked language models as a future direction. Fairness toolkits: We discuss the fairness toolkits here: \n\n-FairML [14] is a toolkit that uses a few ranking algorithms to quantify the relative effects of various inputs on a model's predictions, which can be used to assess the fairness in models.",
            "score": 0.5410167735130014,
            "section_title": "Transfer learning techniques:",
            "char_start_offset": 13967,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 440
                },
                {
                    "start": 443,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 978
                },
                {
                    "start": 981,
                    "end": 1172
                },
                {
                    "start": 1173,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1429
                },
                {
                    "start": 1430,
                    "end": 1520
                },
                {
                    "start": 1521,
                    "end": 1578
                },
                {
                    "start": 1581,
                    "end": 1642
                },
                {
                    "start": 1643,
                    "end": 1793
                },
                {
                    "start": 1794,
                    "end": 1977
                },
                {
                    "start": 1978,
                    "end": 2035
                },
                {
                    "start": 2038,
                    "end": 2227
                }
            ],
            "ref_mentions": [
                {
                    "start": 135,
                    "end": 139,
                    "matchedPaperCorpusId": "51929263"
                },
                {
                    "start": 803,
                    "end": 807,
                    "matchedPaperCorpusId": "240230729"
                },
                {
                    "start": 1167,
                    "end": 1171,
                    "matchedPaperCorpusId": "8310135"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.433837890625
        },
        {
            "corpus_id": "247595186",
            "title": "Under the Morphosyntactic Lens: A Multifaceted Evaluation of Gender Bias in Speech Translation",
            "text": "Our experiments draw on studies exploring the relation between overall system performance, model size and gender bias. Vig et al. (2020) posit that bias increases with model size as larger systems better emulate biased training data. Working on WinoMT/ST, Kocmi et al. (2020) correlate higher BLEU scores and gender stereotyping, whereas  show that systems with lower performance tend to produce fewer feminine translations for occupations, but rely less on stereotypical cues. To account for these findings and inspect the behavior of different models under natural conditions, we experiment with three end-to-end ST solutions, namely: LARGE-BPE, SMALL-BPE, and SMALL-CHAR (see Appendix B for complete details about the models and training setups).\n\nDeveloped to achieve state-of-the-art performance, LARGE-BPE models rely on Transformer (Vaswani et al., 2017) and are trained in rich data conditions (1.25M ASR/ST utterances) by applying BPE segmentation (Sennrich et al., 2016). To achieve high performance, we made use of: i) all the available ST training corpora for the languages addressed, namely MuST-C (Cattoni et al., 2021) and Europarl-ST (Iranzo-S\u00e1nchez et al., 2020); ii) consolidated data augmentation methods (Nguyen et al., 2020;Park et al., 2019;Jia et al., 2019); and iii) knowledge transfer techniques from ASR and MT, namely component pre-training and knowledge distillation (Weiss et al., 2017a;Bansal et al., 2019). 11 In terms of BLEU score -34.12 on enes, 40.3 on en-fr, 27.7 on en-it -our LARGE-BPE models compare favorably with recently published results on MuST-C test data (Le et al. 2021 12 andBentivogli et al. 2021 13 ).\n\nAlso built with the same (Transformer-based) core technology, the other systems, SMALL-BPE and SMALL-CHAR, allow for apples-to-apples comparison between the different capabilities of BPE and character-level tokenization, namely: i) the synta",
            "score": 0.5407231527134331,
            "section_title": "Speech Translation models",
            "char_start_offset": 13819,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 119,
                    "end": 136,
                    "matchedPaperCorpusId": "227275068"
                },
                {
                    "start": 256,
                    "end": 275,
                    "matchedPaperCorpusId": "222310309"
                },
                {
                    "start": 839,
                    "end": 861,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 957,
                    "end": 980,
                    "matchedPaperCorpusId": "1114678"
                },
                {
                    "start": 1111,
                    "end": 1133,
                    "matchedPaperCorpusId": "225107451"
                },
                {
                    "start": 1245,
                    "end": 1263,
                    "matchedPaperCorpusId": "121321299"
                },
                {
                    "start": 1263,
                    "end": 1280,
                    "matchedPaperCorpusId": "53222964"
                },
                {
                    "start": 1416,
                    "end": 1436,
                    "matchedPaperCorpusId": "52160439"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.23095703125
        },
        {
            "corpus_id": "170078973",
            "title": "Reducing Gender Bias in Word-Level Language Models with a Gender-Equalizing Loss Function",
            "text": "Results for the experiments are listed in Table 2. It is interesting to observe that the baseline model amplifies the bias in the training data set as measured by B N and B N c . From measurements using the described bias metrics, our method effectively mitigates bias in language modelling with- We notice that all methods result in GR around 1, indicating that there are near equal amounts of female and male words in the generated texts. In our experiments we note that with increasing \u03bb, the bias steadily decreases and perplexity tends to slightly increase. This indicates that there is a trade-off between bias and perplexity. \n\nREG is not very effective in mitigating bias when compared to other methods, and fails to achieve the best result in any of the bias metrics that we used. But REG results in the best perplexity and even does better than the baseline model in this respect. This indicates that REG has a slight regularization effect. Additionally, it is interesting to note that our loss function outperforms REG in EB d even though REG explicitly aims to reduce gender bias in the embeddings. Although our method does not explicitly attempt geometric debiasing of the word embedding, the results show that it results in the most debiased embedding as compared to other methods. Furthermore, Gonen and Goldberg (2019) emphasizes that geometric gender bias in word embeddings is not completely understood and existing word embedding debiasing strategies are insufficient. Our approach provides an appealing end-to-end solution for model debiasing without relying on any measure of bias in the word embedding. We believe this concept is generalizable to other NLP applications. \n\nOur method outperforms CDA in CB|g, CB|o, and EB d . While CDA achieves slightly better results for co-occurrence biases, B N and B N c , and results in a better perplexity. With a marginal differences, our results are comparable to those of CDA and both models seem to have similar bias mitigation effects. However, our method does not require a data augmentation step and allows training of an unbiased model directly from biased datasets.",
            "score": 0.5403233333477774,
            "section_title": "Results",
            "char_start_offset": 12761,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 50
                },
                {
                    "start": 51,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 632
                },
                {
                    "start": 635,
                    "end": 789
                },
                {
                    "start": 790,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1110
                },
                {
                    "start": 1111,
                    "end": 1295
                },
                {
                    "start": 1296,
                    "end": 1487
                },
                {
                    "start": 1488,
                    "end": 1624
                },
                {
                    "start": 1625,
                    "end": 1692
                },
                {
                    "start": 1695,
                    "end": 1747
                },
                {
                    "start": 1748,
                    "end": 1868
                },
                {
                    "start": 1869,
                    "end": 2002
                },
                {
                    "start": 2003,
                    "end": 2136
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78662109375
        },
        {
            "corpus_id": "264490894",
            "title": "Evaluating Bias and Fairness in Gender-Neutral Pretrained Vision-and-Language Models",
            "text": "This paper presented a comprehensive analysis of gender bias amplification and fairness of encoderonly and encoder-decoder V&L models. The in- trinsic bias analysis shows consistent results -in terms of bias mitigation -in models trained on gender-neutral data, even if these models reflect biases present in data instead of diminishing them (as we observed with LXMERT). In line with previous findings in language models (Goldfarb-Tarrant et al., 2021;Kaneko et al., 2022;Orgad et al., 2022), intrinsic bias in V&L models does not necessarily transfer to extrinsic bias on downstream tasks. Similarly, we find that the bias in a model and its empirical fairness -group disparity on task performanceare in fact independent matters, which is in line with the NLP literature (Shen et al., 2022;Cabello et al., 2023). Intrinsic bias can potentially reinforce harmful biases, but these may not impact the treatment of groups (or individuals) on downstream tasks. We believe that bias and fairness should always be carefully evaluated as separate matters. One of they key findings of our work is that the extra pretraining steps on gender-neutral data are beneficial to reduce the group disparity in every model architecture tested on VQAv2, and in the majority of models for both retrieval tasks. Crucially, there is no penalty to pay for this fair outcome: the overall task performance of gender-neutral models is similar or better than their original versions.",
            "score": 0.5402643085246085,
            "section_title": "Conclusion",
            "char_start_offset": 28400,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1050
                },
                {
                    "start": 1051,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1458
                }
            ],
            "ref_mentions": [
                {
                    "start": 422,
                    "end": 453,
                    "matchedPaperCorpusId": "229923772"
                },
                {
                    "start": 453,
                    "end": 473,
                    "matchedPaperCorpusId": "252222228"
                },
                {
                    "start": 473,
                    "end": 492,
                    "matchedPaperCorpusId": "248177827"
                },
                {
                    "start": 792,
                    "end": 813,
                    "matchedPaperCorpusId": "258236466"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.345703125
        },
        {
            "corpus_id": "271710177",
            "title": "Building an Ethical and Trustworthy Biomedical AI Ecosystem for the Translational and Clinical Integration of Foundation Models",
            "text": "Analogous pairs are then generated and scored based on their alignment with the seed direction, systematically uncovering biased relationships in the embeddings. Once identified, a method to debias these words involves generating augmented sentences through demographic identifier word swapping, encoding both the original and augmented sentences, and maximizing their mutual information [33]. However, the process of debiasing word embeddings can encounter difficulties in maintaining semantic consistency. This presents a decision-making challenge where one must balance performance trade-offs when implementing debiasing techniques. Ma et al. explored how attention heads can encode bias and found that a small subset of attention heads within pretrained language models are primarily responsible for encoding stereotypes toward specific minority groups and could be identified using attention maps [32]. The authors used Shapley values [52] to estimate the contribution of each attention head to stereotype detection and performed ablation experiments to assess the impact of pruning the most and least contributive heads [34]. Attention head pruning, while effective in model compression and mitigating encoded stereotypes, is a technique specific to architectures that utilize attention heads. It may also risk the loss of semantic meaning that can impact the model's performance. \n\nStereotypical biases, inherent in natural language data, can also influence multimodal models that incorporate natural language, such as Vision-Language Models (VLMs) [53]. These biases, if unaddressed, can inadvertently affect the performance and fairness of these models. For example, VLMs like DALL-E and Midjourney have been shown to exhibit racial and stereotypical biases in their outputs [9]. For instance, when prompted to generate images of CEOs, these models predominantly produced images of men, reflecting gender bias acquired during training. Saravanan et al. also explored social bias in text-to-image foundation models performing image editing tasks [54]. Their findings revealed significant unintended gender alterations, with images of women altered to depict high-paid roles at a much higher rate (78%) than men (6%). Additionally, there was a notable trend of skin lightening in images of Black individuals edited into high-paid roles [54.",
            "score": 0.5401551479947634,
            "section_title": "II.A.2. Social AI Bias: Stereotypical Biases.",
            "char_start_offset": 20129,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1386
                },
                {
                    "start": 1389,
                    "end": 1561
                },
                {
                    "start": 1562,
                    "end": 1662
                },
                {
                    "start": 1663,
                    "end": 1788
                },
                {
                    "start": 1789,
                    "end": 1944
                },
                {
                    "start": 1945,
                    "end": 2059
                },
                {
                    "start": 2060,
                    "end": 2224
                },
                {
                    "start": 2225,
                    "end": 2347
                }
            ],
            "ref_mentions": [
                {
                    "start": 902,
                    "end": 906,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 1126,
                    "end": 1130,
                    "matchedPaperCorpusId": "266163829"
                },
                {
                    "start": 1556,
                    "end": 1560,
                    "matchedPaperCorpusId": "253762019"
                },
                {
                    "start": 1784,
                    "end": 1787,
                    "matchedPaperCorpusId": "258180322"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78955078125
        },
        {
            "corpus_id": "276408214",
            "title": "Auto-Search and Refinement: An Automated Framework for Gender Bias Mitigation in Large Language Models",
            "text": "Pre-training large language models (LLMs) on vast text corpora enhances their performance in various natural language processing tasks (Touvron et al., 2023;Zhao et al., 2023;Chiang et al., 2023) but risks encoding social biases, particularly gender bias, that are implicitly present in uncensored datasets (Liang et al., 2021;Luccioni and Viviano, 2021). Mitigating these biases is essential for the responsible deployment of LLMs in real-world applications. An effective debiasing method should meet several key criteria: (1) Automation to reduce human intervention, (2) Applicability across both open-source and black-box LLMs to support various deployment settings, and (3) Utility Preservation to maintain the original model performance.",
            "score": 0.5400136304049308,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 742
                }
            ],
            "ref_mentions": [
                {
                    "start": 157,
                    "end": 175,
                    "matchedPaperCorpusId": "259129398"
                },
                {
                    "start": 307,
                    "end": 327,
                    "matchedPaperCorpusId": "235623756"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.849609375
        },
        {
            "corpus_id": "268536898",
            "title": "From Representational Harms to Quality-of-Service Harms: A Case Study on Llama 2 Safety Safeguards",
            "text": "Breakthroughs in deep learning and natural language processing (NLP) have paved the way for a new generation of highly performing large language models (LLMs).These models can now pass the bar exam (Koetsier, 2023), autonomously generate code (Meta, 2023), or even write books (Times Now Digital, 2023).\n\nIn recent years, they have become ubiquitous, finding practical applications across a myriad of fields, from art and entertainment (Robertson, 2023) to healthcare (Frist, 2023) and education (Heaven, 2023).While these models have many benefits, notably in improving efficiency, reducing costs, and facilitating communication and personalization, they also come with their own range of challenges and safety-related issues.Indeed, these models are prone to spreading misinformation, violating users' privacy, infringing on copyright law, manipulating end users, and reproducing harmful social biases (Chen et al., 2023).\n\nAs these generative technologies become more prominent in our daily lives, and even more influential for critical decision-making in high-stake applications such as as hiring, lending, and criminal justice (Ray, 2023), biased outcomes have the potential to perpetuate societal inequities and further harm already marginalized populations.Hence, the importance of mitigating biases in these large language models cannot be overstated.\n\nVarious attempts to mitigate the biases in these models have been made in industry and academia alike -filtering the training datasets, fine-tuning the models on smaller diversified datasets, using reinforcement learning from human feedback, as well as explicitly blocking certain prompts or certain words from being used (Wiggers, 2021;Touvron et al., 2023b).While these techniques have helped reduce the ingrained biases in these models, they still fall short in terms of scalability and generalization across diverse datasets: for example, studies have shown that even after mitigation, these models still reproduce the same harmful social biases (Salinas et al., 2023) and that safety safeguards lead to lexical overfitting (R\u00f6ttger et al., 2023).",
            "score": 0.5396233025996479,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 159,
                    "end": 303
                },
                {
                    "start": 305,
                    "end": 511
                },
                {
                    "start": 511,
                    "end": 727
                },
                {
                    "start": 727,
                    "end": 924
                },
                {
                    "start": 926,
                    "end": 1264
                },
                {
                    "start": 1264,
                    "end": 1359
                },
                {
                    "start": 1361,
                    "end": 1721
                },
                {
                    "start": 1721,
                    "end": 2112
                }
            ],
            "ref_mentions": [
                {
                    "start": 1132,
                    "end": 1143,
                    "matchedPaperCorpusId": "258157875"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.49951171875
        },
        {
            "corpus_id": "264305744",
            "title": "Co$^2$PT: Mitigating Bias in Pre-trained Language Models through Counterfactual Contrastive Prompt Tuning",
            "text": "Pre-trained language models (PLMs) are widely used in many real-world applications, demonstrating remarkable performance (Devlin et al., 2019;Brown et al., 2020). However, it has been demonstrated that PLMs encode unfair social biases in their parameters based on their pre-training step over large-scale text corpora (May et al., 2019). Furthermore, these biases -for example, based on gender, race, or religion -can easily propagate to the downstream tasks that use these PLMs (Kaneko and Bollegala, 2021). For example, \"She is a nurse\" can have a higher conditional likelihood than \"He is a nurse\" in the language modeling task, and \"nurse\" can have higher coreference scores to \"she\" than \"he\" in the coreference resolution task (Lu et al., 2020). Considering that NLP applications like machine translation systems, resume filtering systems, dialogue systems, and speech recognition (Tatman, 2017) are widely used by millions of users globally, it is crucial to mitigate the social biases present in PLMs and strive for models that will not propagate discriminatory predictions or offensive outputs towards specific groups before being deployed. \n\nMuch prior effort has focused primarily on debiasing the representations learned during the pretraining process, e.g., through projection (Dev et al., 2020;Liang et al., 2020;Ravfogel et al., 2020;Kaneko and Bollegala, 2021), further pre-training on unbiased external corpora (Webster et al., 2020;Lauscher et al., 2021;He et al., 2022), or finetuning to debias (Cheng et al., 2021;Guo et al., 2022). The effectiveness of such debiasing efforts is typically measured on intrinsic benchmarks like SEAT (Sentence Encoding Association Test) which computes the association between demographic terms (e.g., woman, man) and stereotype terms (e.g., science, art). An unbiased model should display no difference in the similarity between the representations of these terms (May et al., 2019).",
            "score": 0.5395562812262391,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 1149
                },
                {
                    "start": 1152,
                    "end": 1552
                },
                {
                    "start": 1553,
                    "end": 1808
                },
                {
                    "start": 1809,
                    "end": 1936
                }
            ],
            "ref_mentions": [
                {
                    "start": 121,
                    "end": 142,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 142,
                    "end": 161,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 479,
                    "end": 507,
                    "matchedPaperCorpusId": "231698657"
                },
                {
                    "start": 887,
                    "end": 901,
                    "matchedPaperCorpusId": "13997424"
                },
                {
                    "start": 1290,
                    "end": 1308,
                    "matchedPaperCorpusId": "201670701"
                },
                {
                    "start": 1308,
                    "end": 1327,
                    "matchedPaperCorpusId": "207996257"
                },
                {
                    "start": 1327,
                    "end": 1349,
                    "matchedPaperCorpusId": "215786522"
                },
                {
                    "start": 1349,
                    "end": 1376,
                    "matchedPaperCorpusId": "231698657"
                },
                {
                    "start": 1450,
                    "end": 1472,
                    "matchedPaperCorpusId": "237440429"
                },
                {
                    "start": 1472,
                    "end": 1488,
                    "matchedPaperCorpusId": "253157517"
                },
                {
                    "start": 1514,
                    "end": 1534,
                    "matchedPaperCorpusId": "232185104"
                },
                {
                    "start": 1534,
                    "end": 1551,
                    "matchedPaperCorpusId": "248780440"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.748046875
        },
        {
            "corpus_id": "248780440",
            "title": "Auto-Debias: Debiasing Masked Language Models with Automated Biased Prompts",
            "text": "Human-like biases and undesired social stereotypes exist in large pretrained language models. Given the wide adoption of these models in real-world applications, mitigating such biases has become an emerging and important task. In this paper, we propose an automatic method to mitigate the biases in pretrained language models. Different from previous debiasing work that uses external corpora to fine-tune the pretrained models, we instead directly probe the biases encoded in pretrained models through prompts. Specifically, we propose a variant of the beam search method to automatically search for biased prompts such that the cloze-style completions are the most different with respect to different demographic groups. Given the identified biased prompts, we then propose a distribution alignment loss to mitigate the biases. Experiment results on standard datasets and metrics show that our proposed Auto-Debias approach can significantly reduce biases, including gender and racial bias, in pretrained language models such as BERT, RoBERTa and ALBERT. Moreover, the improvement in fairness does not decrease the language models\u2019 understanding abilities, as shown using the GLUE benchmark.",
            "score": 0.5393247525121996,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91552734375
        },
        {
            "corpus_id": "269588265",
            "title": "Context Steering: Controllable Personalization at Inference Time",
            "text": "Reducing Bias in LLMs.Bolukbasi et al. (2016) highlight issues of bias in language models.The authors investigate how these embeddings often reflect and perpetuate gender stereotypes and introduce an approach to debias word embeddings by identifying a bias subspace.More recent work finds that these concerning biases extend to LLMs.Kotek et al. (2023) demonstrate that LLMs are three times more likely to choose a stereotype that aligns with a person's gender.Other work has found that LLMs exhibit political bias (Motoki et al., 2023), racial bias (Zack et al., 2024), and geographical bias (Manvi et al., 2024).Table 1: Prompt: Explain Newton's second law.For both contexts C, a higher \u03bb leads to changes in tone (teal) and more patience, encouragement, and the presence of emojis.A lower \u03bb leads to inverse effects (orange) and more scholarly explanations, including a reference to the \"law of torque\", a more general form of Newton's second law.See Appendix C for more details.\n\nSeveral approaches have been introduced to counteract bias in LLMs.In their approach, Peng et al. (2020) utilized GPT-2 to introduce a substantial reward mechanism aimed at diminishing the occurrence of non-standard outputs.Zhao et al. (2019) employed data augmentation techniques to substitute gender-specific terms with their antonyms within the initial training dataset, and combined it with another corpus to create a novel model.Joniak & Aizawa (2022) implemented movement pruning and weight freezing techniques, in addition to employing a debiasing method predicated on a gender-related word projection derived from the work of Kaneko & Bollegala (2021).The downside to many of these approaches is that they either require modifications to the dataset or extensive model training, both of which are computationally heavy and difficult to deploy.",
            "score": 0.5383822666605063,
            "section_title": "Related Work",
            "char_start_offset": 4096,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 22
                },
                {
                    "start": 22,
                    "end": 90
                },
                {
                    "start": 90,
                    "end": 266
                },
                {
                    "start": 266,
                    "end": 333
                },
                {
                    "start": 333,
                    "end": 461
                },
                {
                    "start": 461,
                    "end": 614
                },
                {
                    "start": 614,
                    "end": 659
                },
                {
                    "start": 659,
                    "end": 784
                },
                {
                    "start": 784,
                    "end": 950
                },
                {
                    "start": 950,
                    "end": 982
                },
                {
                    "start": 984,
                    "end": 1051
                },
                {
                    "start": 1051,
                    "end": 1208
                },
                {
                    "start": 1208,
                    "end": 1418
                },
                {
                    "start": 1418,
                    "end": 1644
                },
                {
                    "start": 1644,
                    "end": 1835
                }
            ],
            "ref_mentions": [
                {
                    "start": 22,
                    "end": 45,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 333,
                    "end": 352,
                    "matchedPaperCorpusId": "261276445"
                },
                {
                    "start": 1070,
                    "end": 1088,
                    "matchedPaperCorpusId": "226221869"
                },
                {
                    "start": 1208,
                    "end": 1226,
                    "matchedPaperCorpusId": "102352962"
                },
                {
                    "start": 1418,
                    "end": 1440,
                    "matchedPaperCorpusId": "250311509"
                },
                {
                    "start": 1618,
                    "end": 1643,
                    "matchedPaperCorpusId": "231698657"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5361328125
        },
        {
            "corpus_id": "235593087",
            "title": "A Survey of Race, Racism, and Anti-Racism in NLP",
            "text": "Next, we present examples that identify racial bias in NLP models, focusing on 5 parts of a standard NLP pipeline: data, data labels, models, model outputs, and social analyses of outputs. We include papers described in Table 1 and also relevant literature beyond the ACL Anthology (e.g. NeurIPS, PNAS, Science). These examples are not intended to be exhaustive, and in \u00a74 we describe some of the ways that NLP literature has failed to engage with race, but nevertheless, we present them as evidence that NLP systems perpetuate harmful biases along racialized lines.\n\nData A substantial amount of prior work has already shown how NLP systems, especially word embeddings and language models, can absorb and amplify social biases in data sets (Bolukbasi et al., 2016;Zhao et al., 2017). While most work focuses on gender bias, some work has made similar observations about racial bias Garg et al., 2018;Kurita et al., 2019). These studies focus on how training data might describe racial minorities in biased ways, for example, by examining words associated with terms like 'black' or traditionally European/African American names (Caliskan et al., 2017;Manzini et al., 2019). Some studies additionally capture who is described, revealing under-representation in training data, sometimes tangentially to primary research questions: Rudinger et al. (2017) suggest that gender bias may be easier to identify than racial or ethnic bias in Natural Language Inference data sets because of data sparsity, and Caliskan et al. (2017) alter the Implicit Association Test stimuli that they use to measure biases in word embeddings because some African American names were not frequent enough in their corpora. An equally important consideration, in addition to whom the data describes is who authored the data. For example, Blodgett et al. (2018) show that parsing systems trained on White Mainstream American English perform poorly on African American English (AAE). 5 In a more general example, Wikipedia has become a popular data source for many NLP tasks. However, surveys suggest that Wikipedia editors are primarily from whitemajority countries, 6 and several initiatives have pointed out systemic racial biases in Wikipedia",
            "score": 0.5382183183610187,
            "section_title": "NLP systems encode racial bias",
            "char_start_offset": 7576,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 765,
                    "end": 783,
                    "matchedPaperCorpusId": "1389483"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.266357421875
        },
        {
            "corpus_id": "259095584",
            "title": "An Empirical Analysis of Parameter-Efficient Methods for Debiasing Pre-Trained Language Models",
            "text": ", which determines the bottleneck dimension d/r and controls the trade-off between parameter efficiency and model capacity.\n\nIn our implementation, we adopt Pfeiffer et al. (2021)'s setting where only a single adapter is inserted after the feed-forward sublayer, since it is found to be the optimal setting among other alternatives (Pfeiffer et al., 2021). Thus, all the tunable parameters are \u03c6 = {W We adopt counterfactual data augmentation (CDA, Zhao et al., 2018;Zmigrod et al., 2019;Webster et al., 2020) as our debiasing method to work together with parameter-efficient tuning methods.\n\nSince the encoded biases in pre-trained language models originate from the unbalanced training corpora, it is natural to mitigate these biases by rebalancing the training corpora. For example, when we want to mitigate gender bias between the male and female demographic group and encounter the training sentence \"He is a doctor.\", CDA would substitute the bias attribute word \"He\" with its for \u03c0 s \u2208 \u03a0 S do 11:\n\nx s \u2190Substitute all bias attribute words w\n\n12:\n\nend for 14:\n\nend if 16: end for counterpart \"She\" to obtain an additional training sentence \"She is a doctor.\", so that both gender groups would have equal association with the gender-neutral word \"doctor\". Once we have a list of bias attribute words like {(he, she), (man, woman), (husband, wife), . . . }, we could retrieve all the occurrences of these bias attribute words in the training corpus, and substitute all of them with their counterparts.\n\nFor religious and racial bias where more than two demographic groups are considered, we need to maintain two key properties: (i) we should guarantee consistency, i.e., we should avoid the case where some occurrences of the bias attribute words in group A are substituted with those in group B, while the other occurrences of (possibly different) bias attribute words in group A are substituted with those in group C, and (ii) we should avoid collisions, i.e., we should avoid the case where both groups A and B are substituted with group C. To this end,",
            "score": 0.5381744927119774,
            "section_title": "Parameter-Efficient Methods",
            "char_start_offset": 6126,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 332,
                    "end": 355,
                    "matchedPaperCorpusId": "218470208"
                },
                {
                    "start": 449,
                    "end": 467,
                    "matchedPaperCorpusId": "4952494"
                },
                {
                    "start": 467,
                    "end": 488,
                    "matchedPaperCorpusId": "184486914"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.720703125
        },
        {
            "corpus_id": "249062690",
            "title": "Perturbation Augmentation for Fairer NLP",
            "text": "As language models become more powerful and more popular, more attention should be paid to the demographic biases that they can exhibit. Models trained on datasets with imbalanced demographic representation can learn stereotypes such as women like shopping. While recent works have exposed the biases of LMs using a variety of techniques, the path to mitigating bias in large scale training datasets is not always clear. Many approaches to correct imbalances in the dataset have used heuristic rules to identify and swap demographic terms. We propose a novel method that perturbs text by changing the demographic identity of a highlighted word, while keeping the rest of the text the same. We find that our perturber model creates more fluent and humanlike rewrites than heuristics-based alternatives. We also show that training on demographically perturbed data results in more fair language models, in both pretrained language models and in downstream measurements, without affecting accuracy on NLP benchmarks. We hope our contributions will help drive exciting future research directions in fairer NLP. fairness is the possibility that others will use your methods to make blanket proclamations about the fairness of models without acknowledging the limitations and blindspots of the method. In particular, it is possible that users of our models might infer from our naming conventions (i.e., FairBERTa, fairscore) that our models ought to be deemed \"fair\" or that models performing well according to our metrics ought to be deemed \"fair\". We would like to caution against such an interpretation. Our models appear to be more fair than previous models, but that by no means guarantees they are completely \"fair.\" Researching fairness and bias in NLP data and models is a process of continual learning and improvement and we hope our contributions will help open new avenues that may support the training of even better models in the future. \n\nFactuality: We have shown above that our augmentation process can sometimes create nonexistent versions of real people, such as discussing an English King Victor (not a historical figure), as opposed to a Queen Victoria (a historical figure). We embrace the counterfactuality of many of our perturbations10 , but the lack of guaranteed factuality means that our approach may not be well-suited to all NLP tasks.",
            "score": 0.5381392424193744,
            "section_title": "Conclusion",
            "char_start_offset": 29799,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 1013
                },
                {
                    "start": 1014,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1295
                },
                {
                    "start": 1296,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1601
                },
                {
                    "start": 1602,
                    "end": 1717
                },
                {
                    "start": 1718,
                    "end": 1945
                },
                {
                    "start": 1948,
                    "end": 2190
                },
                {
                    "start": 2191,
                    "end": 2359
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67578125
        },
        {
            "corpus_id": "268512691",
            "title": "Detecting Bias in Large Language Models: Fine-tuned KcBERT",
            "text": "Perceptions that certain occupations are more suitable for either males or females are reflected in the gender biases of English (EN-2) and Korean (KO-2), contributing to the formation of fixed stereotypes about those occupations [13].When societal systems or institutions perpetuate inequality towards specific races, it is reflected in the racial biases of English (EN-3) and Korean (KO-3), with racial biases being more pronounced in English compared to Korean [14].To analyze the impact of online offensive language on BERT, we conduct fine-tuning using the Korean Offensive Language Dataset (KOLD).We compare and analyze the changes in bias between the original base model and the finetuned model.For quantifying societal bias, we utilize the Categorical Bias Score (CBS) [12] for multi-category Ethnic bias and the Log-Probability Bias Score (LPBS) [11] for binary-category Gender and Racial biases.To mitigate bias, we employ methods such as adjusting the frequency of specific target words and modifying the existing training data set by transforming attributes from harmful to non-harmful words.Additionally, we apply dropout and regularization to prevent biased learning of the model.These methods align the balance of data, reducing societal bias metrics in predictions, and decreasing the model's loss during training, enhancing its performance.Experimental analysis comparing the biases of the two models through Korean demonstrates the need for preemptive measures in bias mitigation.Furthermore, we summarize our three contributions as follows:\n\n-Confirmed the existence of biases in both English and Korean versions of LLMs.-Quantified bias using quantitative metrics measurement methods.\n\n-Validated the effectiveness of two bias mitigation methods.",
            "score": 0.5378755074555109,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2053,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 235
                },
                {
                    "start": 235,
                    "end": 469
                },
                {
                    "start": 469,
                    "end": 603
                },
                {
                    "start": 603,
                    "end": 702
                },
                {
                    "start": 702,
                    "end": 905
                },
                {
                    "start": 905,
                    "end": 1104
                },
                {
                    "start": 1104,
                    "end": 1194
                },
                {
                    "start": 1194,
                    "end": 1357
                },
                {
                    "start": 1357,
                    "end": 1498
                },
                {
                    "start": 1498,
                    "end": 1559
                },
                {
                    "start": 1561,
                    "end": 1640
                },
                {
                    "start": 1640,
                    "end": 1704
                },
                {
                    "start": 1706,
                    "end": 1766
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72509765625
        },
        {
            "corpus_id": "231942329",
            "title": "Towards generalisable hate speech detection: a review on obstacles and solutions",
            "text": "As the testing datasets do not have hateful or offensive ground truth labels, one caveat is that, using this as a metric of model bias assumes that all language styles have equal chances of being hateful or offensive, which might not be true. Huang, Xing, Dernoncourt, and Paul (2020) author demographics from a different angle, and instead predicted author demographics on available hate speech datasets using user profile descriptions, names, and photos. They built and released a multilingual corpus for model bias evaluation. Although now with ground truth hate speech labels, this introduces additional possible bias existing in the tools they used into the bias evaluation process. For example, they used a computer vision API on the profile pictures to predict race, age, and gender, which displayed racial and gender biases (Buolamwini & Gebru, 2018). \n\nOne mitigation approach that stemmed from the first approach of measuring biases is \"debiasing\" training data through data augmentation. Dixon et al. (2018) retrieved non-toxic examples containing a range of identity terms following a template, which were added to Wulczyn. Following a similar logic, Park et al. (2018) created examples containing the counterpart of gendered terms found in the data to address gender bias in the Waseem and Founta datasets. Badjatiya et al. (2019) extended this word replacement method by experimenting with various strategies including named entity tags, part of speech tags, hypernyms, and similar words from word embeddings, which were then applied on the Wulczyn and Davidson datasets. \n\nLess biased external corpora and pre-trained models could also be used. To reduce gender bias, Park et al. (2018) also compared pre-trained debiased word embeddings (Bolukbasi, Chang, Zou, Saligrama, & Kalai, 2016) and transfer learning from a larger, less biased corpus. Similarly, Nozza et al. (2019) added samples from the Waseem dataset to their training dataset (Fersini), to keep classes and gender identity terms balanced. \n\nFrom the perspective of model training, \"debiasing\" could also be integrated into the model training objective.",
            "score": 0.5373715091140278,
            "section_title": "Existing solutions",
            "char_start_offset": 32775,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 859
                },
                {
                    "start": 862,
                    "end": 998
                },
                {
                    "start": 999,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1319
                },
                {
                    "start": 1320,
                    "end": 1585
                },
                {
                    "start": 1588,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1859
                },
                {
                    "start": 1860,
                    "end": 2017
                },
                {
                    "start": 2020,
                    "end": 2131
                }
            ],
            "ref_mentions": [
                {
                    "start": 243,
                    "end": 284,
                    "matchedPaperCorpusId": "211259366"
                },
                {
                    "start": 832,
                    "end": 858,
                    "matchedPaperCorpusId": "3298854"
                },
                {
                    "start": 999,
                    "end": 1018,
                    "matchedPaperCorpusId": "54997157"
                },
                {
                    "start": 1163,
                    "end": 1181,
                    "matchedPaperCorpusId": "52070035"
                },
                {
                    "start": 1320,
                    "end": 1343,
                    "matchedPaperCorpusId": "86446136"
                },
                {
                    "start": 1683,
                    "end": 1701,
                    "matchedPaperCorpusId": "52070035"
                },
                {
                    "start": 1753,
                    "end": 1802,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 1871,
                    "end": 1890,
                    "matchedPaperCorpusId": "204754417"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.47216796875
        },
        {
            "corpus_id": "270285642",
            "title": "BEADs: Bias Evaluation Across Domains",
            "text": "For some NER tasks, Bi-LSTM with a CRF layer, incorporating pre-trained embeddings like Glove and BERT, and a classification model for sentence splitting is used [51].The entity debiasing framework for fake news detection models causal relationships among entities, contents, and veracity [88].Despite various approaches, there is room for dedicated data preparation.We present a dataset identifying bias indicators for use in entity extraction tasks, allowing NLP models to detect biased named entities.\n\nBenchmarking on Bias Quantification through Demographics An array of datasets has been designed to examine various biases across demographics in NLP applications.The datasets like WinoGender [63] and WinoBIAS [84] specifically address gender bias via coreference resolution, leveraging nuanced Winograd schemas.On racial and religious biases, HateXplain [42] and RedditBIAS [7] provide insights into hate speech patterns.Other notable efforts include the BBQ dataset [53], and more general datasets like BOLD [18] and HolisticBias [69] offer broader annotations covering biases related to profession, race, and gender.In this work, we focus on demographics such as race, religion, gender, and sexual orientation, annotating all data with these identity mentions and creating templates for LLMs to assess their sensitivity to these dimensions.\n\nBenchmarking on Benign Language Generations Numerous methods have been developed to mitigate biases in language models, focusing on different stages of model application and training.Techniques such as examining and modifying the embedding space are common post-training interventions that adjust word associations to reduce biases without extensive model retraining [40,10,85].Recently, prompt-based debiasing strategies like prompt-tuning and self-supervised zero-shot prompting have emerged to actively guide model output during text generation to prevent biased responses [81,67].These methods are supported by diverse datasets and evaluation metrics to assess their effectiveness, including RedditBias [7], WinoBias [84], and RealToxicityPrompts [28] and other bias datasets [12].",
            "score": 0.5371381585221564,
            "section_title": "A Literature Review",
            "char_start_offset": 17404,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 167,
                    "end": 294
                },
                {
                    "start": 294,
                    "end": 367
                },
                {
                    "start": 367,
                    "end": 504
                },
                {
                    "start": 506,
                    "end": 668
                },
                {
                    "start": 668,
                    "end": 817
                },
                {
                    "start": 817,
                    "end": 927
                },
                {
                    "start": 927,
                    "end": 1124
                },
                {
                    "start": 1124,
                    "end": 1348
                },
                {
                    "start": 1350,
                    "end": 1533
                },
                {
                    "start": 1533,
                    "end": 1728
                },
                {
                    "start": 1728,
                    "end": 1934
                },
                {
                    "start": 1934,
                    "end": 2135
                }
            ],
            "ref_mentions": [
                {
                    "start": 289,
                    "end": 293,
                    "matchedPaperCorpusId": "248266906"
                },
                {
                    "start": 697,
                    "end": 701,
                    "matchedPaperCorpusId": "13756572"
                },
                {
                    "start": 715,
                    "end": 719,
                    "matchedPaperCorpusId": "4952494"
                },
                {
                    "start": 860,
                    "end": 864,
                    "matchedPaperCorpusId": "229332119"
                },
                {
                    "start": 880,
                    "end": 883,
                    "matchedPaperCorpusId": "235358955"
                },
                {
                    "start": 1015,
                    "end": 1019,
                    "matchedPaperCorpusId": "231719337"
                },
                {
                    "start": 1721,
                    "end": 1724,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 1926,
                    "end": 1930,
                    "matchedPaperCorpusId": "253446867"
                },
                {
                    "start": 1930,
                    "end": 1933,
                    "matchedPaperCorpusId": "232075876"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7890625
        },
        {
            "corpus_id": "264305744",
            "title": "Co$^2$PT: Mitigating Bias in Pre-trained Language Models through Counterfactual Contrastive Prompt Tuning",
            "text": "Several approaches have been proposed for debiasing pre-trained language models such as projectionbased methods (Dev et al., 2020;Liang et al., 2020;Ravfogel et al., 2020;Kaneko and Bollegala, 2021), post-hoc text generation techniques (Schick et al., 2021), adversarial methods (Han et al., 2021), finetuning on biased prompts (Guo et al., 2022), with contrastive objective (Cheng et al., 2021) or with augmented data (Zhao et al., 2018), additional pretraining methods on re-balanced corpus through counterfactual data augmentation (Webster et al., 2020;Lauscher et al., 2021;Meade et al., 2022) or with a contrastive objective on gender-balanced entailment pairs (He et al., 2022), using dropout regularization (Webster et al., 2020), through parameterefficient methods (Lauscher et al., 2021;Yang et al., 2022;Xie and Lukasiewicz, 2023) or with a contrastive objective (Li et al., 2023). While some works do not require access to an external corpus or do not require retraining the entire model, most prior methods primarily focus on mitigating bias within the model's intrinsic characteristics and evaluate the effectiveness of bias mitigation through intrinsic bias benchmarks, e.g., SEAT (May et al., 2019), StereoSet (Nadeem et al., 2021), and CrowS-Pairs (Nangia et al., 2020). Subsequently, they fine-tune the debiased models on downstream tasks and demonstrate that their debiased models retain the language modeling ability and the performance on downstream tasks or extrinsic bias benchmarks, which evaluate fairness in downstream tasks by testing whether the models exhibit different performances among different populations.",
            "score": 0.5370106030875768,
            "section_title": "Related Work",
            "char_start_offset": 5048,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 1286
                },
                {
                    "start": 1287,
                    "end": 1639
                }
            ],
            "ref_mentions": [
                {
                    "start": 112,
                    "end": 130,
                    "matchedPaperCorpusId": "201670701"
                },
                {
                    "start": 130,
                    "end": 149,
                    "matchedPaperCorpusId": "207996257"
                },
                {
                    "start": 149,
                    "end": 171,
                    "matchedPaperCorpusId": "215786522"
                },
                {
                    "start": 171,
                    "end": 198,
                    "matchedPaperCorpusId": "231698657"
                },
                {
                    "start": 236,
                    "end": 257,
                    "matchedPaperCorpusId": "232075876"
                },
                {
                    "start": 279,
                    "end": 297,
                    "matchedPaperCorpusId": "231698605"
                },
                {
                    "start": 328,
                    "end": 346,
                    "matchedPaperCorpusId": "248780440"
                },
                {
                    "start": 375,
                    "end": 395,
                    "matchedPaperCorpusId": "232185104"
                },
                {
                    "start": 419,
                    "end": 438,
                    "matchedPaperCorpusId": "4952494"
                },
                {
                    "start": 556,
                    "end": 578,
                    "matchedPaperCorpusId": "237440429"
                },
                {
                    "start": 578,
                    "end": 597,
                    "matchedPaperCorpusId": "239015827"
                },
                {
                    "start": 666,
                    "end": 683,
                    "matchedPaperCorpusId": "253157517"
                },
                {
                    "start": 773,
                    "end": 796,
                    "matchedPaperCorpusId": "237440429"
                },
                {
                    "start": 796,
                    "end": 814,
                    "matchedPaperCorpusId": "247762845"
                },
                {
                    "start": 814,
                    "end": 840,
                    "matchedPaperCorpusId": "259095584"
                },
                {
                    "start": 873,
                    "end": 890,
                    "matchedPaperCorpusId": "259342087"
                },
                {
                    "start": 1225,
                    "end": 1246,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 1264,
                    "end": 1285,
                    "matchedPaperCorpusId": "222090785"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77490234375
        },
        {
            "corpus_id": "270878797",
            "title": "NLPGuard: A Framework for Mitigating the Use of Protected Attributes by NLP Classifiers",
            "text": "Ravfogel et al. [58] proposed removing information from neural representations concerning gender or race for debiasing word embedding for NLP classification.\n\nThese past works try to mitigate unintended bias and performance imbalance between subgroups by (1) removing implicit bias from word embeddings, (2) performing data augmentation on the training set (data-based), or (3) intervening directly in the model architecture or objective function (model-based).However, a gap remains in evaluating (and tackling) the extent to which NLP classifiers depend on protected attributes for their predictions.In this paper, we aim to fill that gap.We consider the following definition of fairness through unawareness: \"an algorithm is fair as long as any protected attributes are not explicitly used in the decision-making process\" [27,40,45].Textual data is unstructured; hence, protected attributes are not explicitly delineated as input features, such as columns used in structured datasets.Consequently, we refine this definition in the context of NLP applications to ensure words associated with protected characteristics are not utilized in decision-making unless necessary.Our approach aims to reduce the use of protected attributes in the decision-making process of NLP models, thereby better aligning them with legal regulations.\n\nCompared to prior work, our approach not only has a different objective, but it also overcomes two of their main limitations: (1) their focus on a subset of protected attributes at a time (usually race and gender); (2) their manual and static identification of protected attributes via pre-defined dictionaries, lists of identity terms, or additional annotations.The only technique addressing these limitations is Entropy-based Attention Regulation (EAR) [4].EAR introduces a regularization term to discourage overfitting to training-specific potentially biased terms.However, those terms are automatically identified during training, leaving no flexibility for users to select which categories to mitigate.Unlike previous techniques, our approach: (1) identifies and mitigates multiple protected categories simultaneously; (2) can be fully automated, allowing for a dynamic update of the dictionary of protected attributes; and (3) allows for the selection of the categories to mitigate.",
            "score": 0.5358381859982142,
            "section_title": "Bias mitigation for NLP",
            "char_start_offset": 8463,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 159,
                    "end": 461
                },
                {
                    "start": 461,
                    "end": 602
                },
                {
                    "start": 602,
                    "end": 641
                },
                {
                    "start": 641,
                    "end": 836
                },
                {
                    "start": 836,
                    "end": 987
                },
                {
                    "start": 987,
                    "end": 1173
                },
                {
                    "start": 1173,
                    "end": 1331
                },
                {
                    "start": 1333,
                    "end": 1696
                },
                {
                    "start": 1696,
                    "end": 1792
                },
                {
                    "start": 1792,
                    "end": 1901
                },
                {
                    "start": 1901,
                    "end": 2040
                },
                {
                    "start": 2040,
                    "end": 2321
                }
            ],
            "ref_mentions": [
                {
                    "start": 16,
                    "end": 20,
                    "matchedPaperCorpusId": "215786522"
                },
                {
                    "start": 374,
                    "end": 377,
                    "matchedPaperCorpusId": "221970112"
                },
                {
                    "start": 825,
                    "end": 829,
                    "matchedPaperCorpusId": "13633339"
                },
                {
                    "start": 829,
                    "end": 832,
                    "matchedPaperCorpusId": "2014883"
                },
                {
                    "start": 832,
                    "end": 835,
                    "matchedPaperCorpusId": "201666566"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.767578125
        },
        {
            "corpus_id": "220281039",
            "title": "OSCaR: Orthogonal Subspace Correction and Rectification of Biases in Word Embeddings",
            "text": "Maximizing bias removal. It has been established that social biases creep into word embeddings and how that affects the tasks they are put towards. There have been a few different approaches that try to mitigate these biases. There are a class of techniques, exemplified by Zhao et al. [22], which retrain the embedding from scratch with information meant to remove bias. These can be computationally expensive, and are mostly orthogonal to the techniques we focus on. In particular, our main focus will be on techniques [1,6,16,5] which focus on directly modifying the word vector embeddings without retraining them; we detail them in Section 5. These mostly build around identifying a bias subspace, and projecting all words along that subspace to remove its effect. They have mainly been evaluated on removing bias, measured structurally [3] or on downstream tasks [5].\n\nRetaining embedding information. The ability to debias should not strip away the ability to correctly distinguish between concepts in the embedding space. Correctly gendered words such as 'man', 'woman', 'he' and 'she', encode information which helps enrich both intrinsic quality of word embeddings (via similarity-based measures) and extrinsic task performances (such as in natural language inference, pronoun resolution). Further, there are other atypically gendered words such as 'pregnant' or 'testosterone' which are more strongly associated to one gender in a meaningful way. Retaining such associations enrich language understanding and are essential. While there are tasks which evaluate the amount of bias reduced [3,5], and there are numerous tasks which evaluate the performance using word embeddings as a general measure of semantic information contained, no tasks specifically evaluate for a specific concept like gender.\n\nDifferentiability. While some debiasing efforts have focused on non-contextualized embeddings (e.g., GloVe, Word2Vec), many natural language processing tasks have moved to contextualized ones (e.g., ELMo, BERT, RoBERTa). Recent methods [5] have shown how debiasing method can be applied to both scenario. The key insight is that contextualized embeddings, although being contextdepen",
            "score": 0.5358345530575068,
            "section_title": "Considerations in Debiasing Word Embeddings",
            "char_start_offset": 5521,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 286,
                    "end": 290,
                    "matchedPaperCorpusId": "52161864"
                },
                {
                    "start": 521,
                    "end": 524,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 524,
                    "end": 526,
                    "matchedPaperCorpusId": "59158788"
                },
                {
                    "start": 841,
                    "end": 844,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 1598,
                    "end": 1601,
                    "matchedPaperCorpusId": "23163324"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81689453125
        },
        {
            "corpus_id": "265281304",
            "title": "Bias A-head? Analyzing Bias in Transformer-Based Language Model Attention Heads",
            "text": "Our work also has limitations that can be improved in future research. First, we focus on stereotyping bias (i.e., representational harm), which is one of the two major bias categories in PLMs (Blodgett et al., 2020). Allocational bias is not investigated in this study. Future research can study how biased heads perform in downstream NLP tasks that unfairly allocate resources or opportunities to different social groups. Second, our work relies on existing word lists to identify biased heads and assess stereotyping bias. Although those (gender or racial) word lists are curated based on theories, concepts, and methods from psychology and other social science literature, their coverage may still be limited for other protected groups such as the groups related to education, literacy, or income, or even intersectional biases (Lalor et al., 2022). Moreover, existing word lists are constructed for the English language only, which restricts the generalization of our findings on PLM stereotyping on non-English languages. Given the important role of curated stereotype word lists in quantifying NLP system's fairness, future work can study a more principled way to curate word lists for different social groups and different languages. Our proposed framework could be used as a tool to help validate lists generated in future research. For example, future paired word lists for education-based biased could use our counterfactual experiments to assess the effectiveness of the collected lists. Third, given the unique importance of self-attention in the transformer architecture, our work focuses on attention heads only. However, bias may also manifest in other components of the model, such as the input embeddings or feedforward layer connections. The complexity and multi-layer nature of Transformer models makes it difficult to pin down their precise working behavior. However, by empirically observing changes via perturbation (e.g., our counterfactual experiments), we can assemble a plausible case for what might be happening inside the network. Future studies can also look inside those components to better understanding biases in PLMs. Finally, while we focus this work on those biased heads with positive bias scores, we also observe a subset of attention heads with large negative bias scores in our results.",
            "score": 0.5353713262691453,
            "section_title": "Limitations",
            "char_start_offset": 27232,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 70
                },
                {
                    "start": 71,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 853
                },
                {
                    "start": 854,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1241
                },
                {
                    "start": 1242,
                    "end": 1341
                },
                {
                    "start": 1342,
                    "end": 1499
                },
                {
                    "start": 1500,
                    "end": 1627
                },
                {
                    "start": 1628,
                    "end": 1756
                },
                {
                    "start": 1757,
                    "end": 1879
                },
                {
                    "start": 1880,
                    "end": 2059
                },
                {
                    "start": 2060,
                    "end": 2152
                },
                {
                    "start": 2153,
                    "end": 2327
                }
            ],
            "ref_mentions": [
                {
                    "start": 193,
                    "end": 216,
                    "matchedPaperCorpusId": "218971825"
                },
                {
                    "start": 832,
                    "end": 852,
                    "matchedPaperCorpusId": "250390904"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.43896484375
        },
        {
            "corpus_id": "273901329",
            "title": "BiasWipe: Mitigating Unintended Bias in Text Classifiers through Model Interpretability",
            "text": "The research community has shown a growing interest in addressing biases in NLP systems (Field et al., 2021). This increased attention is driven not only by the importance of fairness in AI, but also because of its importance in increasing the overall robustness of systems. Two different forms of biases have been addressed in the literature, bias in the word embeddings and bias in downstream tasks. \n\nResearchers found that static word embeddings and contextual word embeddings exhibit different types of biases (Bolukbasi et al., 2016;Jentzsch et al., 2019;Gonen and Goldberg, 2019). Attempts have been made to debias these embeddings (Dev et al., 2020;Kaneko and Bollegala, 2021;Joniak and Aizawa, 2022;Nadeem et al., 2020) for gender or race bias. \n\nThere are prior studies that investigate different forms of bias in text classifiers. Dixon et al. (2018) investigated false positive bias in abusive language detection datasets due to the model overgeneralization from the training data. They also proposed a mitigation technique to re-train the model again by augmenting new training data. It is also found that false positives in hate-speech detectors are often due to the presence of keywords related to race, gender, or sexuality (Davidson et al., 2017;Park et al., 2018) or due to African-American English (Davidson et al., 2019). Gender (Thelwall, 2018;Sweeney and Najafian, 2020), racial, and unintended biases against non-native English are also investigated in sentiment classification systems (Kiritchenko and Mohammad, 2018;Zhiltsova et al., 2019). They evaluated several sentiment systems and found that these systems provide higher sentiment intensity predictions for one race or one gender. More recently, Goldfarb-Tarrant et al. (2023) proposed an Equity Evaluation Corpus to measure racial and gender bias in German, Chinese, Japanese, and Spanish sentiment classifiers.",
            "score": 0.5351015179353783,
            "section_title": "Bias Detection and Mitigation",
            "char_start_offset": 5727,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 401
                },
                {
                    "start": 404,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 753
                },
                {
                    "start": 756,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1341
                },
                {
                    "start": 1342,
                    "end": 1565
                },
                {
                    "start": 1566,
                    "end": 1710
                },
                {
                    "start": 1711,
                    "end": 1892
                }
            ],
            "ref_mentions": [
                {
                    "start": 88,
                    "end": 108,
                    "matchedPaperCorpusId": "235593087"
                },
                {
                    "start": 515,
                    "end": 539,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 539,
                    "end": 561,
                    "matchedPaperCorpusId": "86865551"
                },
                {
                    "start": 561,
                    "end": 586,
                    "matchedPaperCorpusId": "73729169"
                },
                {
                    "start": 639,
                    "end": 657,
                    "matchedPaperCorpusId": "201670701"
                },
                {
                    "start": 657,
                    "end": 684,
                    "matchedPaperCorpusId": "231698657"
                },
                {
                    "start": 684,
                    "end": 708,
                    "matchedPaperCorpusId": "250311509"
                },
                {
                    "start": 842,
                    "end": 861,
                    "matchedPaperCorpusId": "54997157"
                },
                {
                    "start": 1240,
                    "end": 1263,
                    "matchedPaperCorpusId": "1733167"
                },
                {
                    "start": 1263,
                    "end": 1281,
                    "matchedPaperCorpusId": "52070035"
                },
                {
                    "start": 1317,
                    "end": 1340,
                    "matchedPaperCorpusId": "168170119"
                },
                {
                    "start": 1349,
                    "end": 1365,
                    "matchedPaperCorpusId": "3668120"
                },
                {
                    "start": 1365,
                    "end": 1392,
                    "matchedPaperCorpusId": "211041700"
                },
                {
                    "start": 1509,
                    "end": 1541,
                    "matchedPaperCorpusId": "21670658"
                },
                {
                    "start": 1541,
                    "end": 1564,
                    "matchedPaperCorpusId": "212412326"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.568359375
        },
        {
            "corpus_id": "273782730",
            "title": "Evaluating and Mitigating Gender Bias in Generative Large Language Models",
            "text": "The examination of gender bias, alongside other demographic biases like race, nationality, and religion, within generative large language models (LLMs), is increasingly capturing the attention of both the scientific community and industry stakeholders. These biases often affect generative LLMs, influencing popular products and potentially compromising user experiences. A growing body of research is dedicated to enhancing gender representations in natural language processing (NLP) across a spectrum of generative LLMs. This paper explores the current research focused on identifying and evaluating gender bias in generative LLMs. A comprehensive investigation is conducted to evaluate and mitigate gender bias across five distinct generative LLMs. The mitigation strategies implemented yield significant improvements in gender bias scores, with performance enhancements of up to 46% compared to zero-shot text generation approaches. Additionally, we explore how different levels of LLM precision and quantization impact gender bias, providing insights into how technical factors influence bias mitigation strategies. By tackling these challenges and suggesting areas for future research, we aim to contribute to the ongoing discussion about gender bias in language technologies, promoting more equitable and inclusive NLP systems.",
            "score": 0.5341872947758152,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86865234375
        },
        {
            "corpus_id": "264306211",
            "title": "Identifying and Adapting Transformer-Components Responsible for Gender Bias in an English Language Model",
            "text": "With this work, we provide an exploratory study of the identification and mitigation of gender bias in GPT-2. Our three different methods identify model components relevant to gender bias-according to our results, they largely agree on the most relevant attention heads: most of the heads responsible for gender bias are found mainly in the last four attention layers. We then intervene on each method's found components to mitigate the gender bias but maintain language modeling performance. We find that language modeling performance deteriorates only minimally for our 'narrow' interventions, but deteriorates more in conditions where a larger amount of components/parameters are adapted by fine-tuning. \n\nRegarding computational efficiency, we find that the circuits approach is computationally inefficient compared to the other methods. For explanatory and exploratory work, like ours, circuits are very useful and can yield fine-grained insights into the model mechanisms. However, if resource efficiency is a high priority, we suggest using other methods than (automatic) circuit discovery. One key contribution of this paper is a new and very efficient method, DiffMask+, which finds a minimal set of attention heads for fine-tuning, while being computationally less prohibitive than methods such as automatic circuit discovery. \n\nLimitations Have we reached our goal of reducing bias, using computational efficient methods? Considering the measured gender bias, we successfully reduced the bias on two out of three datasets. This is encouraging, but our results also reveal some inconsistencies between different ways of measuring bias. This is not unexpected; in fact, much previous work has highlighted many issues that put the validity and reliability of current bias measures into question (e.g., Blodgett et al., 2021;Talat et al., 2022;Dev et al., 2022). Bias measures may target very different manifestations of the bias of interest (van der Wal et al., 2023). We therefore attribute the observed inconsistencies to the implicit versus explicit gender bias in different datasets, which could be represented differently in model components, and thus also targeted differently by fine-tuning. \n\nDespite these challenges, we tried to address some of these concerns by using multiple different bias metrics and testing the consistency of these across different seeds.",
            "score": 0.5338409010515033,
            "section_title": "Discussion & Conclusions",
            "char_start_offset": 30366,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 706
                },
                {
                    "start": 709,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1336
                },
                {
                    "start": 1339,
                    "end": 1432
                },
                {
                    "start": 1433,
                    "end": 1533
                },
                {
                    "start": 1534,
                    "end": 1645
                },
                {
                    "start": 1646,
                    "end": 1869
                },
                {
                    "start": 1870,
                    "end": 1976
                },
                {
                    "start": 1977,
                    "end": 2206
                },
                {
                    "start": 2209,
                    "end": 2379
                }
            ],
            "ref_mentions": [
                {
                    "start": 1810,
                    "end": 1832,
                    "matchedPaperCorpusId": "236460302"
                },
                {
                    "start": 1832,
                    "end": 1851,
                    "matchedPaperCorpusId": "247626152"
                },
                {
                    "start": 1851,
                    "end": 1868,
                    "matchedPaperCorpusId": "252907216"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.720703125
        },
        {
            "corpus_id": "268819590",
            "title": "Addressing Both Statistical and Causal Gender Fairness in NLP Models",
            "text": "Bias mitigation techniques for NLP models can be categorized broadly based on whether the mitigation is done to the training data (pre-processing methods), to the learning process (in-processing), or to the model outputs (post-processing).\n\nPre-processing methods attempt to mitigate bias by modifying the training data before training.Statistical methods adjust the distribution of the training data through resampling or reweighting.Resampling can be done by either adding examples for underrepresented groups (Dixon et al., 2018;Costajuss\u00e0 and de Jorge, 2020) or removing examples for overrepresented groups (Wang et al., 2019;Han et al., 2022).Reweighting assigns a weight to each training example according to the frequency of its class label and protected attribute (Calders et al., 2009;Kamiran and Calders, 2012;Han et al., 2022).Causal methods such as counterfactual data augmentation (CDA) augment the training set with examples substituted with different identity terms (Lu et al., 2019).This is the same as data augmentation based on gender swapping (Zhao et al., 2018;Park et al., 2018).While both statistical and causal methods seek to balance the group distribution, CDA performs interventions on the protected attribute whereas resampling and reweighing do not modify the attribute in the examples.Previous works have also considered removing protected attributes (De-Arteaga et al., 2019).However, this \"fairness through blindness\" approach is ineffective as there may be other proxies correlate with the protected attributes (Chen et al., 2019).\n\nIn-processing methods incorporate a fairness constraint in the training process.The constraint can be either based on statistical fairness (Kamishima et al., 2012;Zafar et al., 2017;Donini et al., 2018;Subramanian et al., 2021;Shen et al., 2022b) or causal fairness (Garg et al., 2019).",
            "score": 0.5338139059118368,
            "section_title": "Bias Mitigation",
            "char_start_offset": 7955,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 239
                },
                {
                    "start": 241,
                    "end": 336
                },
                {
                    "start": 336,
                    "end": 435
                },
                {
                    "start": 435,
                    "end": 648
                },
                {
                    "start": 648,
                    "end": 838
                },
                {
                    "start": 838,
                    "end": 999
                },
                {
                    "start": 999,
                    "end": 1100
                },
                {
                    "start": 1100,
                    "end": 1314
                },
                {
                    "start": 1314,
                    "end": 1406
                },
                {
                    "start": 1406,
                    "end": 1563
                },
                {
                    "start": 1565,
                    "end": 1645
                },
                {
                    "start": 1645,
                    "end": 1851
                }
            ],
            "ref_mentions": [
                {
                    "start": 512,
                    "end": 532,
                    "matchedPaperCorpusId": "54997157"
                },
                {
                    "start": 611,
                    "end": 630,
                    "matchedPaperCorpusId": "195847929"
                },
                {
                    "start": 630,
                    "end": 647,
                    "matchedPaperCorpusId": "247694107"
                },
                {
                    "start": 772,
                    "end": 794,
                    "matchedPaperCorpusId": "3945595"
                },
                {
                    "start": 794,
                    "end": 820,
                    "matchedPaperCorpusId": "14637938"
                },
                {
                    "start": 820,
                    "end": 837,
                    "matchedPaperCorpusId": "247694107"
                },
                {
                    "start": 1062,
                    "end": 1081,
                    "matchedPaperCorpusId": "4952494"
                },
                {
                    "start": 1081,
                    "end": 1099,
                    "matchedPaperCorpusId": "52070035"
                },
                {
                    "start": 1380,
                    "end": 1405,
                    "matchedPaperCorpusId": "58006082"
                },
                {
                    "start": 1543,
                    "end": 1562,
                    "matchedPaperCorpusId": "58006233"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72607421875
        },
        {
            "corpus_id": "259095584",
            "title": "An Empirical Analysis of Parameter-Efficient Methods for Debiasing Pre-Trained Language Models",
            "text": "The increasingly large size of modern pre-trained language models not only makes them inherit more human-like biases from the training corpora, but also makes it computationally expensive to mitigate such biases. In this paper, we investigate recent parameter-efficient methods in combination with counterfactual data augmentation (CDA) for bias mitigation. We conduct extensive experiments with prefix tuning, prompt tuning, and adapter tuning on different language models and bias types to evaluate their debiasing performance and abilities to preserve the internal knowledge of a pre-trained model. We find that the parameter-efficient methods (i) are effective in mitigating gender bias, where adapter tuning is consistently the most effective one and prompt tuning is more suitable for GPT-2 than BERT, (ii) areless effective when it comes to racial and religious bias, which may be attributed to the limitations of CDA, and (iii) can perform similarly to or sometimes better than full fine-tuning with improved time and memory efficiency, as well as maintain the internal knowledge in BERT and GPT-2, evaluated via fact retrieval and downstream fine-tuning.",
            "score": 0.5331046326806994,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9248046875
        },
        {
            "corpus_id": "265552017",
            "title": "PEFTDebias : Capturing debiasing information using PEFTs",
            "text": "We validate our hypothesis by conducting validation on two widely recognized factors of social bias: gender stereotyping and racial identifiers. To address occupation-based gender stereotypes, we utilize the BiasBios dataset (De-Arteaga et al., 2019). For the bias related to race, we address the issue of elevated occurrences of false positive outcomes in hate speech predictions using GHC (Kennedy et al., 2018). To show our generalizibility of capturing debiasing information along a specific axis using PEFTs, we show transfer to datasets MNLI (multi genre NLI) (Williams et al., 2018) and LHC (large hate corpus) (Toraman et al., 2022) along gender and race axis respectively. \n\nIn order to assess the effectiveness of our debi-asing techniques in mitigating gender and racial biases, we utilize two intrinsic bias benchmarks, namely CrowS-Pairs (Nangia et al., 2020) and StereoSet (Nadeem et al., 2021), during the initial phase of our evaluation, referred to as the upstream stage. StereoSet evaluates a language model's stereotypical associations by employing fill-in-the-blank problems with intra-sentence examples across different bias categories. CrowS-Pairs is an intra-sentence dataset of minimal pairs that compares the language model's masked token probabilities of sentences with disadvantaged or advantaged races fulfilling or violating stereotypes. \n\nIn the subsequent downstream stage, we evaluate the performance gap of PEFTs across different protected attributes within the specific domain using extrinsic bias metrics. To measure gender bias, we adopt the method proposed by De-Arteaga et al. (2019) to calculate the gender gap in the True Positive Rate (TPR) for each occupation (TPR-GAP). To assess racial bias, we compute the False Positive Rate Difference (FPRD) by comparing the FPR of examples mentioning protected racial attributes to the overall FPR. We calculate FPRD for both the in-domain data and the Identity Phrase Templates Test Sets (IPTTS) (Zhang et al., 2020), which consist of 77k instances.",
            "score": 0.5324500733243871,
            "section_title": "Bias Factors and Datasets",
            "char_start_offset": 5749,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 681
                },
                {
                    "start": 684,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1366
                },
                {
                    "start": 1369,
                    "end": 1540
                },
                {
                    "start": 1541,
                    "end": 1712
                },
                {
                    "start": 1713,
                    "end": 1880
                },
                {
                    "start": 1881,
                    "end": 2032
                }
            ],
            "ref_mentions": [
                {
                    "start": 566,
                    "end": 589,
                    "matchedPaperCorpusId": "3432876"
                },
                {
                    "start": 618,
                    "end": 640,
                    "matchedPaperCorpusId": "247218132"
                },
                {
                    "start": 851,
                    "end": 872,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 887,
                    "end": 908,
                    "matchedPaperCorpusId": "215828184"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64697265625
        },
        {
            "corpus_id": "250390459",
            "title": "HeteroCorpus: A Corpus for Heteronormative Language Detection",
            "text": "Bordia and Bowman (2019) address existing language models and point out the gender bias that they contain. They note that many text corpora exhibit problematic biases that an NLP model may learn. Gender bias, as we have seen, can reflect and be perpetuated by heteronormativity. However, the scope of our work is to further generalize the bias in question to go beyond the gender binary and include LGBTQIA+ people. Dev et al. (2021) survey non-binary people in AI to illustrate negative experiences they have experienced with natural language systems. They challenge how gender is represented in NLP systems and question whether we should be representing Gender as a discrete category at all. \n\nOnce the NLP community established that gender biases indeed exist in many NLP systems, many efforts have been made towards detecting and mitigating these biases. Next, we mention some of these techniques in various NLP tasks and systems: from machine translation, coreference resolution, word embeddings, large language models to sentiment analysis. First, we focus on the works regarding large language models, specifically, BERT. Bhardwaj et al. (2020) state that contextual language models are prone to learn intrinsic genderbias from data. They find that BERT shows a significant dependence when predicting on genderparticular words and phrases, they claim such biases could be reduced by removing gender specific words from the word embedding. Zhao et al. (2018) go on to produce gender-neutral word embeddings that aim to preserve gender information in certain dimensions of word vectors while freeing others of gender influence, they release a gender neutral variant of GloVe, GN-GloVe. Kurita et al. (2019) proposes a method to measure bias in BERT, which successfully identifies gender bias in BERT and exposes stereotypes embedded in the model. Recent models have been developed to mitigate gender bias in trained models, such as Saunders and Byrne (2020), who use transfer learning on a small set of gender-balanced data points from a data set to learn un-biasedly, rather than creating a balanced dataset. \n\nMany recent efforts focus on the creation of corpora for gender bias detection and mitigation.",
            "score": 0.5323737600255065,
            "section_title": "Gender bias detection and mitigation in NLP",
            "char_start_offset": 13116,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 693
                },
                {
                    "start": 696,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1128
                },
                {
                    "start": 1129,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1445
                },
                {
                    "start": 1446,
                    "end": 1690
                },
                {
                    "start": 1691,
                    "end": 1851
                },
                {
                    "start": 1852,
                    "end": 2114
                },
                {
                    "start": 2117,
                    "end": 2211
                }
            ],
            "ref_mentions": [
                {
                    "start": 416,
                    "end": 433,
                    "matchedPaperCorpusId": "237347097"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5439453125
        },
        {
            "corpus_id": "248780440",
            "title": "Auto-Debias: Debiasing Masked Language Models with Automated Biased Prompts",
            "text": "In this work, we propose Auto-Debias, a framework and method for automatically mitigating the biases and stereotypes encoded in PLMs. Compared to previous efforts that rely on external corpora to obtain context-dependent word embeddings, our approach automatically searches for biased prompts in the PLMs. Therefore, our approach is effective, efficient, and is perhaps also more objective than prior methods that rely heavily on manually crafted lists of stereotype words. Experimental results on standard benchmarks show that Auto-Debias reduces gender and race biases more effectively than prior efforts. Moreover, the debiased models also maintain good language modeling capability. Bias in NLP systems can stem from different aspects such as training data, pretrained embeddings, or through amplification when finetuning the machine learning models. We believe this work contributes to the emerging literature that sheds light on practical and effective debiasing techniques.",
            "score": 0.5320837315818048,
            "section_title": "Conclusion",
            "char_start_offset": 26822,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 854
                },
                {
                    "start": 855,
                    "end": 980
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9501953125
        },
        {
            "corpus_id": "268553687",
            "title": "Locating and Mitigating Gender Bias in Large Language Models",
            "text": "To further validate the influence of the bottom MLP modules on the generation of gender bias, we have adapted a knowledge editing technique [19] to mitigation of gender bias within the model, called LSDM (Least Square Debias Method).LSDM modifies parameters by solving a matrix equation with constraint terms, enabling us to minimize interference with other aspects of the model while specifically mitigating gender bias associated with certain occupation words.LSDM overcomes the catastrophic forgetting problem that exists in all other debiasing methods and stands out by avoiding additional reinforcement learning or human annotations, basing its approach on causal trace conclusions rather than just black-box fine-tuning.Our main contributions are as follows:\n\n\u2022 We trace the causal effects of different components' activation within a large language model using causal mediation analysis to measure the magnitude of the impact of different components of the model on gender bias and reveal the flow process of biased information.\u2022 We propose Least Square Debias Method to modify parameters to mitigate gender bias in models.This is a more interpretable debiasing algorithm.\n\nResults confirm that LSDM serves as an efficient debias method that overcomes the catastrophic forgetting problem that exists in all other debiasing methods.\u2022 To our knowledge, this study represents the first endeavour to incorporate both the location and mitigation of gender bias into a unified framework.\u2022 We are the first to transfer knowledge editing methods to the domain of debias and validate their feasibility, providing a viable solution for eliminating various biases present in large language models.\n\n2 Interventions on Activations for Tracing Bias Information Flow",
            "score": 0.5319553103729349,
            "section_title": "Introduction",
            "char_start_offset": 6353,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 233
                },
                {
                    "start": 233,
                    "end": 462
                },
                {
                    "start": 462,
                    "end": 726
                },
                {
                    "start": 726,
                    "end": 764
                },
                {
                    "start": 766,
                    "end": 1035
                },
                {
                    "start": 1035,
                    "end": 1130
                },
                {
                    "start": 1130,
                    "end": 1179
                },
                {
                    "start": 1181,
                    "end": 1338
                },
                {
                    "start": 1338,
                    "end": 1488
                },
                {
                    "start": 1488,
                    "end": 1693
                },
                {
                    "start": 1695,
                    "end": 1759
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87158203125
        },
        {
            "corpus_id": "237298625",
            "title": "Five sources of bias in natural language processing",
            "text": "Even balanced, well-labelled data sets contain bias: the most common text inputs representing in NLP systems, word embeddings (Mikolov et al., 2013), have been shown to pick up on racial and gender biases in the training data (Bolukbasi et al., 2016;Manzini et al., 2019). For example, 'woman' is associated with 'homemaker' in the same way 'man' is associated with 'programmer'. There has been some justified scepticism over whether these analogy tasks are the best way to evaluate embedding models (Nissim et al., 2020), but there is plenty of evidence that (1) embeddings do capture societal attitudes (Bhatia, 2017;Garg et al., 2018;Kozlowski et al., 2019), and that (2) these societal biases are resistant to many correction methods (Gonen & Goldberg, 2019). This is the issue of semantic bias. \n\nThese biases hold not just for word embeddings but also for the contextual representations of big pre-trained language models that are now widely used in different NLP systems. As they are pre-trained on almost the entire available internet, they are even more prone to societal biases. Several papers have shown that these models reproduce and thereby perpetuate these biases and stereotypes (Kurita et al., 2019;Tan and Celis, 2019). \n\nThere exist a plethora of efforts for debiasing embeddings (Bolukbasi et al., 2016;Sun et al., 2019;Zhao et al., 2017Zhao et al., , 2019)). The impact and applicability of debiased embeddings are unclear on a wide range of downstream tasks. As stated above, biases are usually masked, not entirely removed, by these methods. Even if it was possible to remove biases in the embeddings, it is not always clear whether it is useful (bias might carry information). \n\nA central issue is the language models' training objective: to predict the most likely next term, given the previous context (n-grams).",
            "score": 0.5319202270832073,
            "section_title": "| Bias from input representations",
            "char_start_offset": 21342,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 799
                },
                {
                    "start": 802,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1237
                },
                {
                    "start": 1240,
                    "end": 1379
                },
                {
                    "start": 1380,
                    "end": 1480
                },
                {
                    "start": 1481,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1700
                },
                {
                    "start": 1703,
                    "end": 1838
                }
            ],
            "ref_mentions": [
                {
                    "start": 126,
                    "end": 148,
                    "matchedPaperCorpusId": "16447573"
                },
                {
                    "start": 226,
                    "end": 250,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 250,
                    "end": 271,
                    "matchedPaperCorpusId": "102350941"
                },
                {
                    "start": 500,
                    "end": 521,
                    "matchedPaperCorpusId": "165163511"
                },
                {
                    "start": 605,
                    "end": 619,
                    "matchedPaperCorpusId": "14291172"
                },
                {
                    "start": 619,
                    "end": 637,
                    "matchedPaperCorpusId": "4930886"
                },
                {
                    "start": 637,
                    "end": 660,
                    "matchedPaperCorpusId": "4394967"
                },
                {
                    "start": 738,
                    "end": 762,
                    "matchedPaperCorpusId": "73729169"
                },
                {
                    "start": 1195,
                    "end": 1216,
                    "matchedPaperCorpusId": "190000105"
                },
                {
                    "start": 1216,
                    "end": 1236,
                    "matchedPaperCorpusId": "202781363"
                },
                {
                    "start": 1299,
                    "end": 1323,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 1323,
                    "end": 1340,
                    "matchedPaperCorpusId": "195316733"
                },
                {
                    "start": 1340,
                    "end": 1357,
                    "matchedPaperCorpusId": "1389483"
                },
                {
                    "start": 1357,
                    "end": 1378,
                    "matchedPaperCorpusId": "102352962"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56103515625
        },
        {
            "corpus_id": "264309222",
            "title": "Multi-objective search for gender-fair and semantically correct word embeddings",
            "text": "Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/). in the training corpora [20,21]. Blindly using pre-trained word embedding models, without considering underlying biases, can lead to problems, which have been already detected in various software applications, such as multi-label classification [22] and co-reference resolution [23]. The problem is exacerbated by the wide adoption of open-source word embedding models pretrained on vast corpora, such as news [24] and encyclopedia [25]. These are often quite easy and inexpensive to use, which leads to any underlying biases to quickly spread across a wide range of software applications. For example, Bolukbasi et al. [20] showed that a popular word embedding model pre-trained on news articles exhibited a bias towards gender, as it learned the analogy that ''man to computer programmer'' is the same as ''woman to homemaker''. It is obvious to see why it would be problematic to use such a model for information retrieval in job application processes, as male names, which are closer to \u2212 \u2212 \u2192 man than \u2212 \u2212\u2212\u2212 \u2192 woman, \n\nwould have a higher similarity to the job of a computer programmer than female names would. Thus, it is of great importance to remove or at least mitigate any existing bias in pre-trained word embedding models before using them. Nonetheless, achieving fairness has a cost [26]. Bias mitigation approaches can damage the performance of a machine learner while making it fair. This is known as the accuracy-fairness trade-off. While various techniques have been proposed to mitigate gender bias in word embeddings [20,22,27,28], they have all focused on reducing bias only. Therefore, we propose the use of a novel multi-objective soft computing approach to reduce gender bias while simultaneously improving the semantic correctness of word embeddings.",
            "score": 0.5317597425864522,
            "section_title": "Introduction",
            "char_start_offset": 1816,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1149
                },
                {
                    "start": 1152,
                    "end": 1243
                },
                {
                    "start": 1244,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1429
                },
                {
                    "start": 1430,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1576
                },
                {
                    "start": 1577,
                    "end": 1723
                },
                {
                    "start": 1724,
                    "end": 1902
                }
            ],
            "ref_mentions": [
                {
                    "start": 153,
                    "end": 157,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 157,
                    "end": 160,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 374,
                    "end": 378,
                    "matchedPaperCorpusId": "1389483"
                },
                {
                    "start": 539,
                    "end": 543,
                    "matchedPaperCorpusId": "16447573"
                },
                {
                    "start": 561,
                    "end": 565,
                    "matchedPaperCorpusId": "1957433"
                },
                {
                    "start": 749,
                    "end": 753,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 1664,
                    "end": 1668,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 1668,
                    "end": 1671,
                    "matchedPaperCorpusId": "1389483"
                },
                {
                    "start": 1671,
                    "end": 1674,
                    "matchedPaperCorpusId": "10352172"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59619140625
        },
        {
            "corpus_id": "239015827",
            "title": "An Empirical Survey of the Effectiveness of Debiasing Techniques for Pre-trained Language Models",
            "text": "Below, we describe the five debiasing techniques we evaluate in this work. We refer readers to Appendix C for additional experimental details on each debiasing technique.\n\nCounterfactual Data Augmentation (CDA). CDA (Zmigrod et al., 2019;Dinan et al., 2020a;Webster et al., 2020;Barikeri et al., 2021) is a databased debiasing strategy often used to mitigate gender bias. Roughly, CDA involves re-balancing a corpus by swapping bias attribute words (e.g., he/she) in a dataset. For example, to help mitigate gender bias, the sentence \"the doctor went to the room and he grabbed the syringe\" could be augmented to \"the doctor went to the room and she grabbed the syringe\". The re-balanced corpus is then often used for further training to debias a model. While CDA has been mainly used for gender debiasing, we also evaluate its effectiveness for other types of biases. For instance, we create CDA data for mitigating religious bias by swapping religious terms in a corpus, say church with mosque, to generate counterfactual examples.\n\nWe experiment with debiasing pre-trained language models by performing an additional phase of pre-training on counterfactually augmented sentences from English Wikipedia. 5 DROPOUT. Webster et al. (2020) investigate using dropout regularization (Srivastava et al., 2014) as a bias mitigation technique. They investigate increasing the dropout parameters for BERT and ALBERT's attention weights and hidden activations and performing an additional phase of pre-training. Experimentally, they find increased dropout regularization reduces gender bias within these models. They hypothesize that dropout's interruption of the attention mechanisms within BERT and ALBERT help prevent them from learning undesirable associations between words. We extend this study to other types of biases. Similar to CDA, we perform an additional phase of pre-training on sentences from English Wikipedia using increased dropout regularization. Informally, Schick et al. (2021) propose using hand-crafted prompts to first encourage a model to generate toxic text. For example, generation from an autoregre",
            "score": 0.5316418359139458,
            "section_title": "Debiasing Techniques",
            "char_start_offset": 9589,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.857421875
        },
        {
            "corpus_id": "277150794",
            "title": "Enforcing Consistency and Fairness in Multi-level Hierarchical Classification with a Mask-based Output Layer",
            "text": "Recent work has focused on mitigating intrinsic bias during pretraining and in-processing stages, using various techniques such as Counterfactual Data Augmentation (CDA), Context-debias, and Sent-debias. For instance, CDA balances representation by swapping demographic-specific terms (e.g., \"he\" and \"she\") in the training data, though it is resource-intensive due to the need for retraining (Zmigrod et al., 2019;Webster et al., 2020). In-processing methods like Context-debias attempt to remove bias by ensuring that embeddings of stereotypical terms are orthogonal to gender-related terms, but they depend heavily on predefined word lists, limiting their generalizability (Kaneko and Bollegala, 2021). Post-processing methods such as Sent-debias work by removing gender bias from pretrained model embeddings, though research suggests that these methods often obscure rather than fully eliminate bias (Liang et al., 2020;Gonen and Goldberg, 2019). \n\nDespite significant advancements in integrating deep learning techniques for tasks involving multilevel taxonomies, challenges persist in scaling models to handle large, complex hierarchies consistently and fairly. Existing methods often struggle to maintain consistency across deep hierarchies. Also, upstream debiasing techniques frequently do not translate into improved fairness in downstream tasks-especially in complex scenarios like MLHC where preserving class hierarchy is crucial (Steed et al., 2022). To tackle these issues, we propose the Debiased Taxonomy-based Transitional Classifier (D-TTC), which embeds hierarchical information directly into the classification process, leverages LLMs for better contextual understanding, and uses downstream post-processing debiasing through dynamic reweighting which adjusts the importance of different samples during training. As a modelagnostic layer, D-TTC enhances both flexibility and performance across various backbone models, providing a more consistent solution for complex hierarchies. Additionally, it enables the model to address bias more effectively within specific application domains, ensuring improved fairness along-side high performance.",
            "score": 0.5312744870847811,
            "section_title": "Related Work",
            "char_start_offset": 11542,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 705
                },
                {
                    "start": 706,
                    "end": 950
                },
                {
                    "start": 953,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1463
                },
                {
                    "start": 1464,
                    "end": 1832
                },
                {
                    "start": 1833,
                    "end": 2000
                },
                {
                    "start": 2001,
                    "end": 2161
                }
            ],
            "ref_mentions": [
                {
                    "start": 1442,
                    "end": 1462,
                    "matchedPaperCorpusId": "248780439"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9091796875
        },
        {
            "corpus_id": "264306000",
            "title": "A Predictive Factor Analysis of Social Biases and Task-Performance in Pretrained Masked Language Models",
            "text": "Masked Language Models (MLMs) have achieved promising performance in many NLP tasks (Devlin et al., 2019;Liu et al., 2019;Liang et al., 2023). However, MLMs trained on massive amounts of textual training data have also been found to encode concerning levels of social biases such as gender and racial biases (Kaneko and Bollegala, 2019;May et al., 2019;Dev et al., 2020;Silva et al., 2021;Kaneko et al., 2022). In spite of the overall success of MLMs across NLP tasks, such biases within MLMs raise ethical considerations and underscore the need for debiasing methods to ensure fair and unbiased MLMs. \n\nOn the other hand, MLMs are trained by considering and optimising various underlying factors that contribute to their performance on downstream tasks. These factors include but are not limited to parameter size, tokenization methods, training objectives and training corpora. The performance of MLMs is affected by the interplay of such factors. \n\nNevertheless, it remains unclear as to how these factors influence social biases in MLMs and their downstream task performance. \n\nEvaluating the impact of these factors is challenging due to three main reasons: (a) The factors that we consider within a model are not independent, rather, they exhibit complicated interdependence and affect the performance of models simultaneously. (b) MLMs are diverse with different architectures, configurations and parameters. The diversity across models requires the need for generalisation and abstraction when considering the values of factors. (c) Many recent works proposed debiasing methods to mitigate social biases in MLMs (Webster et al., 2020;Lauscher et al., 2021;Schick et al., 2021;Guo et al., 2022). However, most debiasing methods tend to worsen the performance of MLMs in downstream tasks (Meade et al., 2022). Therefore, it is crucial to consider the trade-off between social bias and downstream task performance when comparing MLMs. \n\nTo address the non-independent issue of factors, we propose a method using Gradient Boosting (Freund and Schapire, 1997) to consider dependencies among factors.",
            "score": 0.5311311332675261,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 601
                },
                {
                    "start": 604,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 949
                },
                {
                    "start": 952,
                    "end": 1079
                },
                {
                    "start": 1082,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1536
                },
                {
                    "start": 1537,
                    "end": 1702
                },
                {
                    "start": 1703,
                    "end": 1815
                },
                {
                    "start": 1816,
                    "end": 1939
                },
                {
                    "start": 1942,
                    "end": 2102
                }
            ],
            "ref_mentions": [
                {
                    "start": 84,
                    "end": 105,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 336,
                    "end": 353,
                    "matchedPaperCorpusId": "85518027"
                },
                {
                    "start": 353,
                    "end": 370,
                    "matchedPaperCorpusId": "201670701"
                },
                {
                    "start": 370,
                    "end": 389,
                    "matchedPaperCorpusId": "235097394"
                },
                {
                    "start": 1642,
                    "end": 1664,
                    "matchedPaperCorpusId": "237440429"
                },
                {
                    "start": 1664,
                    "end": 1684,
                    "matchedPaperCorpusId": "232075876"
                },
                {
                    "start": 1684,
                    "end": 1701,
                    "matchedPaperCorpusId": "248780440"
                },
                {
                    "start": 1794,
                    "end": 1814,
                    "matchedPaperCorpusId": "239015827"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6220703125
        },
        {
            "corpus_id": "258987942",
            "title": "Mitigating Test-Time Bias for Fair Image Retrieval",
            "text": "In-processing methods focus on altering the training objective by incorporating fairness constraints, regularization terms or leveraging adversarial learning to obtain representations invariant to gender/race (Berg et al., 2022;Wang et al., 2023;Xu et al., 2021;Cotter et al., 2019). Post-processing approaches achieve fairness by applying post-hoc corrections to a (pre-)trained model (Cheng et al., 2021;Calmon et al., 2017) or via feature clipping (Wang et al., 2021a) on the output of image-text encoders based on mutual information. \n\nOur work lies in the post-processing category of debiasing methods that encourages equal representation of diverse demographics. We also identified the fair subset selection approach used in Mehrotra and Celis (2021) as a potential post-hoc debiasing method for TBIR. While Mehrotra and Celis (2021) shares our goal of ensuring equality of gender/race attributes in the set of results, their focus did not extend to the TBIR scenario with an underlying VL model nor detail an effective method for obtaining accurate demographic attributes for debiasing. More importantly, they assumed demographic attributes seen by their algorithm to be available ground truth labels subject to noise. This assumption creates difficulties when attempting to adapt their method to a real-world problem such as TBIR. Complementary, our approach is meant to address these deficiencies by providing a practical debiasing procedure, that includes acquiring demographic attributes. \n\nGender/Racial Bias in Web Image Search Our research is closely related to studies in the Human Computer Interaction community that demonstrated gender inequality issues in current online image search systems (Kay et al., 2015). These studies revealed how gender bias in occupational image search results influences people's perceptions about the presence of men and women in various professions. Moreover, investigations into racial biases in search results, such as those conducted by Noble (2018), have shown that these biases can reinforce harmful stereotypes and contribute to the perpetuation of systemic inequalities. For example, when searching for terms like \"successful person\" or \"professional hairstyles\", the results might predominantly return images of white individuals, inadvertently excluding people of color.",
            "score": 0.529212818276803,
            "section_title": "Fairness in machine learning",
            "char_start_offset": 8932,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 537
                },
                {
                    "start": 540,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1225
                },
                {
                    "start": 1226,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1499
                },
                {
                    "start": 1502,
                    "end": 1729
                },
                {
                    "start": 1730,
                    "end": 1897
                },
                {
                    "start": 1898,
                    "end": 2125
                },
                {
                    "start": 2126,
                    "end": 2327
                }
            ],
            "ref_mentions": [
                {
                    "start": 246,
                    "end": 262,
                    "matchedPaperCorpusId": "222310724"
                },
                {
                    "start": 262,
                    "end": 282,
                    "matchedPaperCorpusId": "52193717"
                },
                {
                    "start": 406,
                    "end": 426,
                    "matchedPaperCorpusId": "3801798"
                },
                {
                    "start": 731,
                    "end": 756,
                    "matchedPaperCorpusId": "226282209"
                },
                {
                    "start": 814,
                    "end": 839,
                    "matchedPaperCorpusId": "226282209"
                },
                {
                    "start": 1710,
                    "end": 1728,
                    "matchedPaperCorpusId": "8832874"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75341796875
        },
        {
            "corpus_id": "264146917",
            "title": "Investigating Bias in Multilingual Language Models: Cross-Lingual Transfer of Debiasing Techniques",
            "text": "There has been a growing interest in addressing bias detection and mitigation in Natural Language Processing (NLP) due to their societal implications. Initially, research focused on debiasing word embeddings (Bolukbasi et al., 2016;Zhao et al., 2018b), but recent studies found that pretrained language models also capture social biases present in training data (Meade et al., 2022). Hence, attention has shifted towards debiasing techniques that target sentence representations. These techniques include additional pretraining steps (Zhao et al., He Figure 1: The example of the English CrowS-Pairs dataset illustrates sentence probabilities after debiasing mBERT with SentenceDebias in English, French, German, and Dutch. and projection-based methods that assume a bias direction (Liang et al., 2020a;Ravfogel et al., 2020;Liang et al., 2020b). \n\nWhile debiasing techniques have been developed and evaluated for monolingual, and mostly English models, the effectiveness and transferability of these techniques to diverse languages within multilingual models remain largely unexplored (Stanczak and Augenstein, 2021;Sun et al., 2019). Our research aims to bridge this gap by examining the potential of debiasing techniques applied to one language to effectively mitigate bias in other languages within multilingual large language models. We examine English (EN), French (FR), German (DE), and Dutch (NL). Figure 1 illustrates an example sentence pair included in the English CrowS-Pairs dataset 1 , where the unmodified and modified parts are highlighted in blue and yellow respectively. It shows the predicted probabilities of the modified part occurring given the unmodified part across different debiasing languages. \n\nThis study examines the cross-lingual transferability of debiasing techniques using mBERT. mBERT, trained on Wikipedia data from diverse languages, possesses the capability to process and generate text in various linguistic contexts. Despite balancing efforts, it still performs worse on low-resource languages (Wu and Dredze, 2020;Devlin, 2018). We investigate whether this performance disparity extends to gender, religious, and racial biases.",
            "score": 0.5291913582989365,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 846
                },
                {
                    "start": 849,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1588
                },
                {
                    "start": 1589,
                    "end": 1720
                },
                {
                    "start": 1723,
                    "end": 1813
                },
                {
                    "start": 1814,
                    "end": 1956
                },
                {
                    "start": 1957,
                    "end": 2069
                },
                {
                    "start": 2070,
                    "end": 2168
                }
            ],
            "ref_mentions": [
                {
                    "start": 208,
                    "end": 232,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 362,
                    "end": 382,
                    "matchedPaperCorpusId": "239015827"
                },
                {
                    "start": 782,
                    "end": 803,
                    "matchedPaperCorpusId": "207996257"
                },
                {
                    "start": 803,
                    "end": 825,
                    "matchedPaperCorpusId": "215786522"
                },
                {
                    "start": 825,
                    "end": 845,
                    "matchedPaperCorpusId": "227230609"
                },
                {
                    "start": 1117,
                    "end": 1134,
                    "matchedPaperCorpusId": "195316733"
                },
                {
                    "start": 2034,
                    "end": 2055,
                    "matchedPaperCorpusId": "218684346"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69482421875
        },
        {
            "corpus_id": "264306211",
            "title": "Identifying and Adapting Transformer-Components Responsible for Gender Bias in an English Language Model",
            "text": "Modern neural language models exhibit social biases, such as biases based on gender, religion, ethnicity and other protected attributes. These biases may lead to real harms when used in downstream applications (e.g. Hovy and Spruit, 2016;Weidinger et al., 2021). Detecting and mitigating biases in language models has therefore become an important area of research. \n\nEarly detection methods relied on lists of words to measure associations with e.g., specific genders (e.g. Caliskan et al., 2017). Most current detection * Shared senior authorship. methods work with curated sets of sentence pairs or triplets, and measure differences in sentence probabilities or anaphora resolution probabilities (e.g. May et al., 2019;Nadeem et al., 2021;Nangia et al., 2020;Basta et al., 2019). Proposed mitigation strategies include targeted changes to the training data (e.g., CDA; Lu et al., 2020), training procedure (e.g., adversarial learning; Zhang et al., 2018), model parameters (e.g., INLP;Ravfogel et al., 2020), or language generation procedure (e.g., \"self-debiasing\"; Schick et al., 2021). \n\nDespite this work, we still lack a proper understanding of how to best measure biases (how do we guarantee the representativeness for real-world harm of a set of sentence pairs, or of a linguistic phenomenon such as anaphora resolution?), how biases are implemented in the language model internals (is there a unified locus, or is, e.g., gender bias the aggregate effect of many independent model decisions?), and what techniques are effective at reducing undesirable downstream behavior (e.g., is data curation more or less effective than filtering output? Is intervening in the model internals feasible?).",
            "score": 0.5287876332615377,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 365
                },
                {
                    "start": 368,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 782
                },
                {
                    "start": 783,
                    "end": 1091
                },
                {
                    "start": 1094,
                    "end": 1503
                },
                {
                    "start": 1504,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1701
                }
            ],
            "ref_mentions": [
                {
                    "start": 216,
                    "end": 238,
                    "matchedPaperCorpusId": "1083991"
                },
                {
                    "start": 475,
                    "end": 497,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 705,
                    "end": 722,
                    "matchedPaperCorpusId": "85518027"
                },
                {
                    "start": 722,
                    "end": 742,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 742,
                    "end": 762,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 762,
                    "end": 781,
                    "matchedPaperCorpusId": "121125604"
                },
                {
                    "start": 1070,
                    "end": 1090,
                    "matchedPaperCorpusId": "232075876"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80419921875
        },
        {
            "corpus_id": "268064051",
            "title": "Can a Prediction\u2019s Rank Offer a More Accurate Quantification of Bias? A Case Study Measuring Sexism in Debiased Language Models",
            "text": "According to the literature and to the best of our knowledge, we outline below the four prominent debiasing techniques. \n\nContext-Debias. Context-Debias (Kaneko and Bollegala, 2019) is a technique for debiasing pretrained contextualized word embeddings in a finetuning setting that both (a) preserves the semantic information in the pre-trained contextualized word embedding model, and (b) removes discriminative gender-related biases via an orthogonal projection in the intermediate (hidden) layers by operating at token or sentence-levels. \n\nAuto-Debias. Auto-Debias (Guo et al., 2022) is a debiasing technique for masked language models that does not entail referencing external corpora. Auto-Debias contains two stages: First, automatically crafting biased prompts, such that the clozestyle completions have the highest disagreement in generating stereotype words with respect to demographic groups. Second, debiasing the language model by a distribution alignment loss. \n\nCounterfactual Data Augmentation. Counterfactual Data Augmentation (CDA) (Zmigrod et al., 2019) is a data augmentation technique that involves generating new instances by modifying existing observations. This technique has been employed to mitigate gender bias in models by interchanging masculine-inflected nouns with feminineinflected nouns, and vice versa, thereby generating additional data points that promote model generalization. \n\nDropout. Dropout (Webster et al., 2020) is a regularization technique typically used to reduce overfitting in models, it is also effective for reducing gendered bias problems. By randomly deactivating a portion of the neurons during training/finetuning, dropout can mitigate the influence of gender-specific features, contributing to a more equitable and unbiased model.",
            "score": 0.5287457250466373,
            "section_title": "Debiasing Techniques",
            "char_start_offset": 15110,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 122,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 541
                },
                {
                    "start": 544,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 974
                },
                {
                    "start": 977,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1413
                },
                {
                    "start": 1416,
                    "end": 1424
                },
                {
                    "start": 1425,
                    "end": 1591
                },
                {
                    "start": 1592,
                    "end": 1786
                }
            ],
            "ref_mentions": [
                {
                    "start": 153,
                    "end": 180,
                    "matchedPaperCorpusId": "173991106"
                },
                {
                    "start": 569,
                    "end": 586,
                    "matchedPaperCorpusId": "248780440"
                },
                {
                    "start": 1050,
                    "end": 1072,
                    "matchedPaperCorpusId": "184486914"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9267578125
        },
        {
            "corpus_id": "251492973",
            "title": "A Comprehensive Survey of Natural Language Generation Advances from the Perspective of Digital Deception",
            "text": "and gendered word ratios [226]. For measuring bias of transformative NLG (e.g., language translation, summarisation), most evaluations of bias focus solely on transformation accuracy.\n\nIn turn, the presence of bias in NLG systems has the capacity to cause various harms. Several studies exist showing that large PLMs, such as BERT and GPT-2/3, can cause harm due to different forms of bias, including stereotypical associations, or negative sentiment towards specific groups [14]. Typically, harms of bias in NLG can be grouped as follows [226]:\n\nRepresentational Impacts: These harms arise from unfair representations of different social groups. Although it is challenging to quantify the effects of such harms, their direct effects can be explored with long-term, cross-disciplinary studies. Allocational Impacts: These harms result from an unequal allocation of resources across groups. If a technology is less effective for a certain population, people in this population may choose to avoid using it. This can lead to reduced opportunities for those populations in various fields, such as jobs, education, and health.\n\nVulnerability Impacts: Open-domain generation tasks can make a group more vulnerable to manipulation and harm (such as in the generation of misinformation, privacy-related issues, or radicalising views) resulting in the group becoming more susceptible to representational and allocational impacts.\n\nIn order to address issues of bias in NLG systems, a wide range of solutions have been proposed. In general, the proposed solutions fall under four main classes: data methods, training methods, inference methods, and evaluation methods [226].\n\nData Methods: A proposed data-based mitigation strategy utilises the general idea of counterfactual data augmentation (CDA) to curate sets of counterfactual prompts. These prompts can then be used to reveal biases in NLG systems. Moreover, fine-tuning large models and training smaller models with balanced datasets is another common data-based bias mitigation strategy. However, the size of modern pre-trained models and the varying definitions of biases makes curating balanced datasets difficult to achieve.\n\nTraining Methods: Specific training techniques have been leveraged to reduce bias. This includes the use of regularisation, bias control codes through conditional",
            "score": 0.5286008722062133,
            "section_title": "B. Bias in PLM-Based NLG Systems",
            "char_start_offset": 239626,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 25,
                    "end": 30,
                    "matchedPaperCorpusId": "234337004"
                },
                {
                    "start": 475,
                    "end": 479,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 539,
                    "end": 544,
                    "matchedPaperCorpusId": "234337004"
                },
                {
                    "start": 1659,
                    "end": 1664,
                    "matchedPaperCorpusId": "234337004"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4189453125
        },
        {
            "corpus_id": "248780380",
            "title": "Square One Bias in NLP: Towards a Multi-Dimensional Exploration of the Research Manifold",
            "text": "Most of the exceptions to our thesis about the 'onedimensionality' of NLP research, in our classification of ACL 2021 Oral Papers, came from studies of efficiency in a multilingual context. Another example of this is Ahia et al. (2021), who show that for low-resource languages, weight pruning hurts performance on tail phenomena, but improves robustness to out-of-distribution shifts-this is not observed in the SQUARE ONE (high-resource) regime. There are also studies of fairness in a multilingual context. Huang et al. (2020), for example, show significant differences in social bias for multilingual hate speech systems across different languages. Zhao et al. (2020) study gender bias in multilingual word embeddings and cross-lingual transfer. Gonz\u00e1lez et al. (2020) also study gender bias, but by relying on reflexive pronominal constructions that do not exist in the English language; this is a good example of research that would not have been possible taking SQUARE ONE as our point of departure. Dayanik and Pad\u00f3 (2021) study adversarial debiasing in the context of a multilingual corpus and show some mitigation methods are more effective for some languages rather than others. Nozza (2021) studies multilingual toxicity classification and finds that models misinterpret nonhateful language-specific taboo interjections as hate speech in some languages. There has been much less work on other combinations of these dimensions, e.g., fairness and efficiency. Hansen and S\u00f8gaard (2021b) show that weight pruning has disparate effects on performance across demographics and that the min-max difference in group disparities is negatively correlated with model size. Renduchintala et al. (2021) observe that techniques to make inference more efficient, e.g., greedy search, quantization, or shallow decoder models, have a small impact on performance, but dramatically amplify gender bias. In a rare study of fairness and interpretability, Vig et al. (2020) propose a methodology to interpret which parts of a model are causally implicated in its behavior. They apply this methodology to analyze gender bias in pre-trained Transformers, finding that gender bias effects",
            "score": 0.5282500277347629,
            "section_title": "Counter-Examples",
            "char_start_offset": 24918,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 217,
                    "end": 235,
                    "matchedPaperCorpusId": "238419368"
                },
                {
                    "start": 510,
                    "end": 529,
                    "matchedPaperCorpusId": "211259366"
                },
                {
                    "start": 653,
                    "end": 671,
                    "matchedPaperCorpusId": "218487087"
                },
                {
                    "start": 1007,
                    "end": 1030,
                    "matchedPaperCorpusId": "233365137"
                },
                {
                    "start": 1481,
                    "end": 1496,
                    "matchedPaperCorpusId": "236478036"
                },
                {
                    "start": 1674,
                    "end": 1701,
                    "matchedPaperCorpusId": "235266085"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1951904296875
        },
        {
            "corpus_id": "237491723",
            "title": "Mitigating Language-Dependent Ethnic Bias in BERT",
            "text": "Ethnic (or national) bias, an over-generalized association of an ethnic group to particular, often negative attributes (Brigham, 1971;Ghavami and Peplau, 2013), is one of the most prevalent social stereotypes. Compared to gender and racial bias, ethnic bias tends to depend more on the cultural context (Cuddy et al., 2009;Fiske, 2017), as anyone could step outside of their ethnic background (e.g., by moving to a different country) and suddenly belong to a minority group. In studying various aspects of large-scale language models (LMs), there are many studies on gender and racial bias (Bolukbasi et al., 2016;Caliskan et al., 2017;Garg et al., 2018;May et al., 2019;Manzini et al., 2019; (Kurita et al., 2019). The different predictions by BERT imply that the ethnic bias in these three BERT models reflect the historical and social context of the countries in which they are used. For example, the current political climate in Germany and the US is hostile toward Iraq, and Korea was occupied and ruled by Japan in recent history, and those negative contexts are reflected as ethnic biases in German (DE-1), English (EN-1), and Korean (KO-1). There are also instances of ethnic bias shared across languages, and we can see an example in EN-2, DE-2, and KO-2 where Somalia and Cuba appear within top three in all three languages. In addition to these three, we study three more languages: Spanish, Turkish, and Chinese.\n\nTo quantify and mitigate ethnic bias, we propose a scoring metric called Categorical Bias (CB) score and two mitigation methods: 1) using a multilingual model and 2) aligning two monolingual models. We suggest two separate solutions because of the relatively poor performance of the multilingual model on low-resource languages (Wu and Dredze, 2020). The first solution using the multilingual BERT model works well for Chinese, English, German, and Spanish, languages that are resourceabundant. An alternative solution leverages alignment with the English embedding space, and this solution reduces the bias score for Korean and Turkish,",
            "score": 0.5281372885344614,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 654,
                    "end": 671,
                    "matchedPaperCorpusId": "85518027"
                },
                {
                    "start": 671,
                    "end": 692,
                    "matchedPaperCorpusId": "102350941"
                },
                {
                    "start": 1754,
                    "end": 1775,
                    "matchedPaperCorpusId": "218684346"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.120849609375
        },
        {
            "corpus_id": "245537923",
            "title": "A Survey on Gender Bias in Natural Language Processing",
            "text": "While it is impossible to altogether remove gender bias from language or from NLP algorithms, research on gender bias mitigation is a significant step towards developing fair systems. In specific applications, one might argue that gender biases in algorithms could capture valuable statistics such as a higher probability of a nurse being a female. Nevertheless, given the potential risk of employing machine learning algorithms that amplify gender stereotypes, Bolukbasi et al. [2016] recommend erring on the side of neutrality and using debiased methods. However, following D 'Ignazio [2021], mitigating gender bias in AI systems is a short-term solution that needs to be combined with higher-level long-term projects in challenging the current social status quo. \n\nThe main challenge in debiasing task is to strike a trade-off between maintaining model performance on downstream tasks while reducing the encoded gender bias [de Vassimon Manela et al. 2021;Zhao et al. 2018a]. Further, Bartl et al. [2020]; Sun et al. [2019] emphasise the need for more typological variety in NLP research as well as for language-specific solutions. Many of the mitigation methods rely on pre-defined words lists that are not scalable in a multilingual setup and are tedious to create. However, recent work on dictionary definitions for debiasing might obviate the need for predefined word lists [Kaneko and Bollegala 2021b]. While prior work has mainly focused on mitigating gender bias in English, more recently, researchers have started to apply methods initially developed for English to other languages as well. Naturally, a significant chunk of work for multilingual settings has been researched in the context of neural machine translation [Prates et al. 2020;Vanmassenhove et al. 2018]. This stream of research has confirmed that language-specific solutions are required, since gender is expressed in different ways across languages. For instance, transferring a method successful in gender bias mitigation for English to German may be ineffective which emphasises the need for more typological variety in research as well as language-specific solutions [Bartl et al. 2020].",
            "score": 0.5279845702699257,
            "section_title": "MITIGATING GENDER BIAS",
            "char_start_offset": 72237,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 765
                },
                {
                    "start": 768,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1410
                },
                {
                    "start": 1411,
                    "end": 1601
                },
                {
                    "start": 1602,
                    "end": 1779
                },
                {
                    "start": 1780,
                    "end": 1926
                },
                {
                    "start": 1927,
                    "end": 2167
                }
            ],
            "ref_mentions": [
                {
                    "start": 462,
                    "end": 485,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 1009,
                    "end": 1026,
                    "matchedPaperCorpusId": "195316733"
                },
                {
                    "start": 1381,
                    "end": 1409,
                    "matchedPaperCorpusId": "231698423"
                },
                {
                    "start": 1732,
                    "end": 1752,
                    "matchedPaperCorpusId": "52179151"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.46630859375
        },
        {
            "corpus_id": "239015827",
            "title": "An Empirical Survey of the Effectiveness of Debiasing Techniques for Pre-trained Language Models",
            "text": "We begin by describing the three intrinsic bias benchmarks we use to evaluate our debiasing techniques. We select these benchmarks as they can be used to measure not only gender bias, but also racial and religious bias in language models.",
            "score": 0.5277090346700968,
            "section_title": "Techniques for Measuring Bias",
            "char_start_offset": 3247,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7265625
        },
        {
            "corpus_id": "278165825",
            "title": "Mitigating Bias in Facial Recognition Systems: Centroid Fairness Loss Optimization",
            "text": "Alasadi et al. (2019) and Gong et al. (2019) employ adversarial methods to reduce bias, but these are recognized for their instability and computational needs, while Wang et al. (2019) leverage imbalanced and transfer learning techniques. Note that in-processing strategies require a complete retraining, which is notoriously costly as state-of-the-art FR systems require very large training datasets. Furthermore, these strategies lead to fairness improvements at the expense of the performance, highlighting a performance-fairness trade-off (Du et al., 2020). Concerning post-processing approaches, Dhar et al. (2021) mitigate the racial bias of a pre-trained model by enforcing the embeddings not to contain any racial information. Conti et al. (2022) reduce the gender bias using a statistical model for the embedding space, but they admit that the method is not able to tackle other types of bias. Those works change the pre-trained embeddings to improve the fairness, both in terms of false positives and false negatives. In contrast, another line of research takes a different approach, not altering the latent space but modifying the decision rule itself. Terh\u00f6rst et al. (2020) intervene on the score function, while Salvador et al. (2021) rely on calibration methods. Those works focus on the bias in terms of false positives and their training set needs to have the same distribution than the test set, which may not be a realistic scenario. \n\nContributions. We propose a post-processing approach to mitigate the bias of a pre-trained/frozen FR model that is accessible only as a black-box, making it applicable to numerous already deployed FR systems. It is important to note that many methods fine tune state-of-the-art open-source FR models, thereby acquiring their bias, which underscores the necessity of improving their fairness properties. Our solution aims to align intra-group performance curves with those of a reference group, a functional objective that is inherently challenging. By drawing an analogy between real FR scores and centroid-based scores, we simplify the original fairness objective, enabling the use of pseudo-metrics that are easier to compute and align with modern FR loss functions.",
            "score": 0.5273551083202085,
            "section_title": "Introduction",
            "char_start_offset": 2029,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1452
                },
                {
                    "start": 1455,
                    "end": 1469
                },
                {
                    "start": 1470,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1857
                },
                {
                    "start": 1858,
                    "end": 2003
                },
                {
                    "start": 2004,
                    "end": 2223
                }
            ],
            "ref_mentions": [
                {
                    "start": 166,
                    "end": 184,
                    "matchedPaperCorpusId": "198968250"
                },
                {
                    "start": 543,
                    "end": 560,
                    "matchedPaperCorpusId": "201645479"
                },
                {
                    "start": 601,
                    "end": 619,
                    "matchedPaperCorpusId": "236956411"
                },
                {
                    "start": 735,
                    "end": 754,
                    "matchedPaperCorpusId": "250340814"
                },
                {
                    "start": 1164,
                    "end": 1186,
                    "matchedPaperCorpusId": "211068764"
                },
                {
                    "start": 1226,
                    "end": 1248,
                    "matchedPaperCorpusId": "238531802"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70361328125
        },
        {
            "corpus_id": "268385538",
            "title": "Leveraging Prototypical Representations for Mitigating Social Bias without Demographic Information",
            "text": "While our approach shows promise in mitigating social bias in language models without relying on demographic labels, it is important to recognize its limitations.First, our method relies on predefined texts that represent different social attribute groups, which may not fully capture the complexity and diversity of these attributes.Language models are complex systems, and they may still exhibit bias or unintended associations despite our efforts.Moreover, it is important to acknowledge that gender is non-binary, and the expirements we conducted were focused on addressing binary gender biases.Additionally, our analysis of racial biases is centered around the African-American race, using sociolect as a proxy which might be inaccurate.We believe there is a need for more comprehensive research to address biases related to African American rase and other racial and ethnic groups, in a more precise manner.",
            "score": 0.5273408496276022,
            "section_title": "Limitations",
            "char_start_offset": 15752,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 162,
                    "end": 334
                },
                {
                    "start": 334,
                    "end": 450
                },
                {
                    "start": 450,
                    "end": 599
                },
                {
                    "start": 599,
                    "end": 742
                },
                {
                    "start": 742,
                    "end": 913
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.399169921875
        },
        {
            "corpus_id": "258947155",
            "title": "Finspector: A Human-Centered Visual Inspection Tool for Exploring and Comparing Biases among Foundation Models",
            "text": "Recently, pre-trained large language models (LLMs), including 'foundation models,' that are trained on large amounts of data have shown striking performances in a variety of natural language processing (NLP) tasks such as language translation, text classification, and summarization. Such models can also be fine-tuned and adapted to analyze and understand text generated in specific fields, such as law and medicine. Despite their usefulness, there is a growing concern that the foundation models inherently reflect human biases, which might have originated from their large training corpora (Shah et al., 2020;Liang et al., 2021;Weidinger et al., 2021;Garrido-Mu\u00f1oz et al., 2021).\n\nThese social biases include stereotyping and negative generalizations of different social groups and communities, which could have been present in their training corpora (Liang et al., 2021;Garrido-Mu\u00f1oz et al., 2021). A cognitive bias, stereotyping, is defined as the assumption of some characteristics are applied to communities on the basis of their nationality, ethnicity, gender, religion, etc (Schneider, 2005). Relatedly, Fairness (\"zero-bias\"), in the context of NLP and machine learning is defined as being not discriminatory according to such characteristics (Garrido-Mu\u00f1oz et al., 2021). Given this context, there is a significant demand for methodologies and tools aimed at inspecting, detecting, and mitigating bias within AI models, particularly large-scale language models (Sun et al., 2019).\n\nA previous work (Kwon and Mihindukulasooriya, 2022) demonstrated that computing the pseudo-log-likelihood scores of paraphrased sentences using different foundation models can be used to test the consistency and robustness of the models, which can lead to a better understanding of the fairness of LLMs. Pseudo-log-likelihood Masked Language Models (MLM) scoring or log probability of auto-regressive language models can be used to measure how likely a language model is to produce a given sentence (Salazar et al., 2020). It can also be used to measure the likelihood of multiple variants of a sentence, such as stereotypical and non-stereotypical ones, in order to determine which one the model prefers",
            "score": 0.5271044181947145,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 593,
                    "end": 612,
                    "matchedPaperCorpusId": "209461005"
                },
                {
                    "start": 612,
                    "end": 631,
                    "matchedPaperCorpusId": "235623756"
                },
                {
                    "start": 854,
                    "end": 874,
                    "matchedPaperCorpusId": "235623756"
                },
                {
                    "start": 1472,
                    "end": 1490,
                    "matchedPaperCorpusId": "195316733"
                },
                {
                    "start": 1509,
                    "end": 1544,
                    "matchedPaperCorpusId": "250390988"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50927734375
        },
        {
            "corpus_id": "274130807",
            "title": "Bias in Large Language Models: Origin, Evaluation, and Mitigation",
            "text": "Intra-mdoel debiasing works by selecting the model that prioritizes unbiasedness and fairness during the model fitting. For example, Garimella et al. (2021) proposes a method to mitigate explicit and implicit biases in BERT using existing and newly proposed loss functions. The proposed work focuses on gender bias and includes a list of target-defined word pairs and the text dataset as input. It takes the pretrained BERT and further trains it on the given input, while mitigating the existing social biases by equalizing and declustering. The equalizing stage aims to equalize the associations of neutral words by defining a equalizing loss (Qian et al., 2019). Observing that words are stereotypically associated to a group (e.g., delicate, pink, beautiful ), authors develop a declustering stage to decluster the implicit clusters by defining a declustering loss. Similar technique is also applied to mitigate social stereotypes when exposed to different demographic groups (Garimella et al., 2022). Moreover, Joniak and Aizawa (2022) presents an innovative framework for examining bias in pretrained transformer-based language models through movement pruning. It identifies a subset of the model that contains less bias than the original by pruning it while fine-tuning on a debiasing objective. It explores pruning attention heads, which are crucial components of transformers, by pruning square blocks and introducing a novel method for pruning entire heads. An interesting biasperformance trade-off about gender bias from this study is: the level of bias increases as model's performance improves. \n\nAnother line of works focuses on the technique of transfer learning (Zhuang et al., 2020;Azunre, 2021;Ge et al., 2024;Liu, 2024). Transfer learning leverages knowledge learned from a pre-trained model to improve the performance of a new model on a relevant task but with fewer data points. More specifically, it fine-tunes a pre-trained model on the new task by updating the weights of neurons or modifying the network architecture.",
            "score": 0.5263758521863636,
            "section_title": "Intra-mdoel debiasing",
            "char_start_offset": 41398,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1466
                },
                {
                    "start": 1467,
                    "end": 1606
                },
                {
                    "start": 1609,
                    "end": 1738
                },
                {
                    "start": 1739,
                    "end": 1898
                },
                {
                    "start": 1899,
                    "end": 2041
                }
            ],
            "ref_mentions": [
                {
                    "start": 133,
                    "end": 156,
                    "matchedPaperCorpusId": "236477795"
                },
                {
                    "start": 644,
                    "end": 663,
                    "matchedPaperCorpusId": "170078973"
                },
                {
                    "start": 979,
                    "end": 1003,
                    "matchedPaperCorpusId": "253762006"
                },
                {
                    "start": 1015,
                    "end": 1039,
                    "matchedPaperCorpusId": "250311509"
                },
                {
                    "start": 1677,
                    "end": 1698,
                    "matchedPaperCorpusId": "207847753"
                },
                {
                    "start": 1711,
                    "end": 1727,
                    "matchedPaperCorpusId": "258049306"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8486328125
        },
        {
            "corpus_id": "274117151",
            "title": "Evaluating Gender Bias in Large Language Models",
            "text": "The rapid development of technology in the field of natural language processing (NLP) has led to significant breakthroughs, particularly in large language models (LLMs). The advent of transformer architectures represented a significant turning point, facilitating enhanced performance and efficiency compared to preceding recurrent and convolution-based models [1]. These developments have had a profound impact on the way machines understand and generate language. The applications of LLMs are diverse, covering a range of tasks such as sequence tagging, information extraction, machine translation, summarisation, and even complex natural language interactions in conversational systems [2]. LLMs are used in a wide range of domains, including healthcare, education, law and finance. Despite considerable progress and diverse applications, these systems are not without their challenges. One particular challenge is the presence of bias. LLMs learn from vast amounts of text collected from the Internet and other sources. This training data often contains unbalanced representations and stereotypes that are unintentionally embedded in the models [3]. \n\nBias in LLMs can harm both individuals and society, with consequences including allocative and representational harms. Allocative harms occur when resources or opportunities are distributed unfairly, often to the detriment of already marginalised groups. For example, recruitment algorithms may be skewed to disadvantage certain demographic groups. Representational harm occurs when groups are misrepresented or stereotyped, reinforcing harmful narratives and stigmas and further entrenching negative prejudices and societal biases [4]. Language models show a propensity for bias in a variety of areas, including but not limited to gender, age, sexual orientation, physical appearance, disability, nationality, ethnicity, socio-economic status, religion and culture. These biases have the potential to have significant consequences, particularly in contexts where they affect critical areas such as justice, healthcare or employment [5]. \n\nIn this study, we investigate gender bias in LLMs by analysing how these models select pronouns and generate names in professional contexts. Four models are examined: GPT-4 [6], GPT-4o [7], PaLM 2 Text Bison [8] and Gemini 1.0 Pro [9].",
            "score": 0.5261947141569229,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1153
                },
                {
                    "start": 1156,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1410
                },
                {
                    "start": 1411,
                    "end": 1504
                },
                {
                    "start": 1505,
                    "end": 1692
                },
                {
                    "start": 1693,
                    "end": 1922
                },
                {
                    "start": 1923,
                    "end": 2093
                },
                {
                    "start": 2096,
                    "end": 2236
                },
                {
                    "start": 2237,
                    "end": 2331
                }
            ],
            "ref_mentions": [
                {
                    "start": 1149,
                    "end": 1152,
                    "matchedPaperCorpusId": "261530629"
                },
                {
                    "start": 1688,
                    "end": 1691,
                    "matchedPaperCorpusId": "235436386"
                },
                {
                    "start": 2089,
                    "end": 2092,
                    "matchedPaperCorpusId": "258688053"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6796875
        },
        {
            "corpus_id": "270358029",
            "title": "HateDebias: On the Diversity and Variability of Hate Speech Debiasing",
            "text": "In the field of Natural Language Processing (NLP), bias mitigation strategies [31][32][33] fall into two primary categories.The first involves training models to reduce the influence of sensitive attributes during fine-tuning [34,35], using techniques like projection-based [36], adversarial [17], or contrastive objectives [37].This approach requires data specifically annotated for the sensitive attribute, with the debiasing effectiveness dependent on the external corpus quality and the potential introduction of new biases.The second category, task-agnostic training, uses general corpora to address biases, such as by identifying and removing gender biases from encoded representations [38] or enhancing model fairness through selective data exclusion [39], increased dropout rates [40], or equalizing objectives [41].Additionally, methods like the introduction of fairness-triggering perturbations [42] and equi-tuning for group equivariance in immutable models [43] have been explored, focusing on minimizing differences between standard and debiased model features.\n\nContinuous Debiasing.Current research on debiasing word embeddings mainly addresses static biases and stereotypes, with efforts on pre-trained language models targeting known sensitive attributes [44,41].However, unforeseen biases cannot be fully anticipated due to societal changes.Google's ML-fairness-gym framework aims to simulate continuous debiasing in machine learning, analyzing the impact of algorithms on information systems and human behavior, yet it does not extend to unstructured data, such as text [45].To date, continuous debiasing in text processing remains unexplored.\n\nContinual Learning.Continual learning involves updating models with new information while retaining previous knowledge, avoiding catastrophic forgetting [46].This process entails maintaining generic internal representations for reuse across tasks and dynamically adjusting task-specific parts.\n\nEffective strategies include memory-based methods that use prior task exemplars [47,24] or synthetic data [48], and regularization techniques that minimize changes in learned representations under a fixed architecture [49,50].",
            "score": 0.525757808313142,
            "section_title": "A1 Related Work",
            "char_start_offset": 25463,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 124,
                    "end": 329
                },
                {
                    "start": 329,
                    "end": 528
                },
                {
                    "start": 528,
                    "end": 824
                },
                {
                    "start": 824,
                    "end": 1074
                },
                {
                    "start": 1076,
                    "end": 1097
                },
                {
                    "start": 1097,
                    "end": 1280
                },
                {
                    "start": 1280,
                    "end": 1359
                },
                {
                    "start": 1359,
                    "end": 1594
                },
                {
                    "start": 1594,
                    "end": 1662
                },
                {
                    "start": 1664,
                    "end": 1683
                },
                {
                    "start": 1683,
                    "end": 1822
                },
                {
                    "start": 1822,
                    "end": 1957
                },
                {
                    "start": 1959,
                    "end": 2185
                }
            ],
            "ref_mentions": [
                {
                    "start": 78,
                    "end": 82,
                    "matchedPaperCorpusId": "257687840"
                },
                {
                    "start": 82,
                    "end": 86,
                    "matchedPaperCorpusId": "269484654"
                },
                {
                    "start": 226,
                    "end": 230,
                    "matchedPaperCorpusId": "258740850"
                },
                {
                    "start": 274,
                    "end": 278,
                    "matchedPaperCorpusId": "246411428"
                },
                {
                    "start": 292,
                    "end": 296,
                    "matchedPaperCorpusId": "231698605"
                },
                {
                    "start": 324,
                    "end": 328,
                    "matchedPaperCorpusId": "248986272"
                },
                {
                    "start": 692,
                    "end": 696,
                    "matchedPaperCorpusId": "231698657"
                },
                {
                    "start": 758,
                    "end": 762,
                    "matchedPaperCorpusId": "254071653"
                },
                {
                    "start": 819,
                    "end": 823,
                    "matchedPaperCorpusId": "248780440"
                },
                {
                    "start": 905,
                    "end": 909,
                    "matchedPaperCorpusId": "252408823"
                },
                {
                    "start": 969,
                    "end": 973,
                    "matchedPaperCorpusId": "252873203"
                },
                {
                    "start": 1272,
                    "end": 1276,
                    "matchedPaperCorpusId": "237440429"
                },
                {
                    "start": 1276,
                    "end": 1279,
                    "matchedPaperCorpusId": "248780440"
                },
                {
                    "start": 1589,
                    "end": 1593,
                    "matchedPaperCorpusId": "211041349"
                },
                {
                    "start": 1817,
                    "end": 1821,
                    "matchedPaperCorpusId": "258333888"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78857421875
        },
        {
            "corpus_id": "274776546",
            "title": "Bias Vector: Mitigating Biases in Language Models with Task Arithmetic Approach",
            "text": "As language models (LMs) have become more widely used in recent years, the biases and stereotypes inherent in the training data for LMs are creating social problems (Liu et al., 2020;Kumar et al., 2023). These biases reflect the stereotypes of specific social groups (such as those related to race, profession, gender, and religion) (Bolukbasi et al., 2016;Nadeem et al., 2021). People tend to use racially biased stereotypical phrases (like \"The men from afghanistan ride on camels\"), rather than phrases that contradict stereotypes (e.g., \"The men from afghanistan ride on skateboards\"). 1 s a consequence, LMs often make unfair predictions about certain groups, leading to biased or stereotyped outcomes that can cause discomfort among users. The widespread and frequent use of LMs (such as ChatGPT (GPT-3.5 / 4) (OpenAI, 2022(OpenAI, , 2024))), with their biased predictions is resulting in discrimination and inequality, which is becoming a social problem (Feng et al., 2023). Hence, developing effective bias mitigation methods for LM systems is essential. \n\nPrior to the advent of Large Language Models (LLMs), debiasing studies primarily targeted word embeddings (Zhao et al., 2018;Kaneko and Bollegala, 2019;Wang et al., 2020). Models such as word2vec (Mikolov et al., 2013) are debiased by reshaping the word embeddings in their output representations. However, these methods are less practical for Transformer-based LMs, such as BERT (Devlin et al., 2019), because the model parameters need to be debiased as the required model outputs vary depending on the downstream task. \n\nTo address biases in Transformer-based LMs, methods have been developed to reduce biases and stereotypes by continually training of LMs with debiased datasets (Zmigrod et al., 2019;Webster et al., 2020;Dinan et al., 2020;Barikeri et al., 2021;Jentzsch and Turan, 2022).",
            "score": 0.5256879324985996,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1062
                },
                {
                    "start": 1065,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1585
                },
                {
                    "start": 1588,
                    "end": 1857
                }
            ],
            "ref_mentions": [
                {
                    "start": 165,
                    "end": 183,
                    "matchedPaperCorpusId": "204838020"
                },
                {
                    "start": 183,
                    "end": 202,
                    "matchedPaperCorpusId": "252907607"
                },
                {
                    "start": 333,
                    "end": 357,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 357,
                    "end": 377,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 961,
                    "end": 980,
                    "matchedPaperCorpusId": "258686693"
                },
                {
                    "start": 1171,
                    "end": 1190,
                    "matchedPaperCorpusId": "52161864"
                },
                {
                    "start": 1190,
                    "end": 1217,
                    "matchedPaperCorpusId": "173991106"
                },
                {
                    "start": 1217,
                    "end": 1235,
                    "matchedPaperCorpusId": "204770514"
                },
                {
                    "start": 1445,
                    "end": 1466,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70068359375
        },
        {
            "corpus_id": "259145073",
            "title": "Gender-Inclusive Grammatical Error Correction through Augmentation",
            "text": ", leveraging specific linguistic features of this use of the pronoun. Additionally, we refine previous approaches to addressing discrepancies in model behavior between texts with masculine and feminine pronouns and show their application outside of the context of masked language models.\n\n\u2022 We qualitatively and quantitatively measure biases in competitive GEC systems by comparing model performance on parallel test sets containing singular they pronouns, masculine terms, and feminine terms.\n\n\u2022 We show how our data augmentation techniques, both in isolation and combination, mitigate these biases when used to create training data for these GEC systems with a minimal impact to overall performance. To orient this work, we highlight two recent callsto-action regarding the study of bias and gender in NLP systems. First, following Devinney et al. (2022), we mean to be explicit about the conception of gender and gendered language we assume.\n\nIn particular, we are concerned with gendered linguistic content, and not the gender of the authors or readers of that content. We recognize that the expression of gender in English is notional; the use of some nouns and pronouns is linked to particular gendered conceptual categories (McConnell-Ginet, 2013;Ackerman, 2019). Additionally, the use of language with gendered content represents one aspect of gender performativity which produces and reifies these gendered categories. Second, following Blodgett et al. (2020), \"bias\" is an inherently normative concept. In the context of NLP systems, it must be understood in terms of the potential harms that those systems may cause and to whom those harms may be caused. Therefore, we directly focus on mitigating harms themselves as they relate to the GEC task and how users of these systems interact with them and may be affected by them. Unlike some studies of bias in upstream contexts like word embeddings, users interact with GEC systems directly; these users choose to incorporate these systems' suggestions into their emails, essays, and tweets, and bias may impact anyone interacting with such text.\n\nFurther, because the GEC task is an inherently normative one on its own-these systems offer suggestions to correct a user's text and are designed in accordance with preexisting normative notions of \"correct\" or \"fluent\" English-",
            "score": 0.5256472654969411,
            "section_title": "Introduction",
            "char_start_offset": 2135,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 834,
                    "end": 856,
                    "matchedPaperCorpusId": "248524791"
                },
                {
                    "start": 1254,
                    "end": 1269,
                    "matchedPaperCorpusId": "108291174"
                },
                {
                    "start": 1446,
                    "end": 1468,
                    "matchedPaperCorpusId": "218971825"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.339599609375
        },
        {
            "corpus_id": "273233850",
            "title": "Mitigating Gender Bias in Code Large Language Models via Model Editing",
            "text": "Lastly, for these debiasing methods in the post training phase, like applying prompt engineering in the code LLM [18], they could reduce the social bias in the model's output space, but the social bias inherent in the model itself remains unchanged. \n\nUnlike the aforementioned three main categories of debiasing methods, recently proposed model editing techniques [41,42], which aim to update the factual knowledge stored in models, could be the new direction for mitigating social bias within code LLMs. The goal of model editing is to effectively and selectively modify the model's behavior in specific knowledge domains, enabling it to generate more accurate and relevant outputs while ensuring the stability of its overall performance [43][44][45][46][47]. Limisiewicz et al. [29] identify stereotype representation subspaces using DAMA and edited bias-prone feed-forward networks (FFNs) with orthogonal projection matrices. Aky\u00fcrek et al. [48] extend the scope of model editing to free-form natural language to involve biased edits. Yan et al. [49] treat social debiasing as an editing problem and adopted various model editing methods on large language models (LLMs) to mitigate bias. \n\nHowever, to the best of our knowledge, there is no work that devotes to leveraging promising model editing methods for debasing the code LLM. Therefore, we make the following efforts in this paper: \n\nFirst, we construct a dataset for probing the Gender Bias in the Code Generation, called \"CodeGenBias\". Specifically, we use a template-based method to generate the dataset. The template contains two fully completed demonstrations and one demonstration with placeholders that needs to be filled in a certain profession and a certain modifier. 320 primary professions in society collected by Bolukbasi et al. [37] and the five different categories of modifiers are used in this paper, constituting a dataset with 555 training samples, 277 development samples and 3328 test samples. \n\nSecond, based on the proposed CodeGenBias dataset, we further propose a metric named \"Facual Bias Score\" (short for FB-Score).",
            "score": 0.5242954157972202,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 3579,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 249
                },
                {
                    "start": 252,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 929
                },
                {
                    "start": 930,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1191
                },
                {
                    "start": 1194,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1391
                },
                {
                    "start": 1394,
                    "end": 1497
                },
                {
                    "start": 1498,
                    "end": 1567
                },
                {
                    "start": 1568,
                    "end": 1736
                },
                {
                    "start": 1737,
                    "end": 1974
                },
                {
                    "start": 1977,
                    "end": 2103
                }
            ],
            "ref_mentions": [
                {
                    "start": 369,
                    "end": 372,
                    "matchedPaperCorpusId": "258833129"
                },
                {
                    "start": 740,
                    "end": 744,
                    "matchedPaperCorpusId": "255825985"
                },
                {
                    "start": 752,
                    "end": 756,
                    "matchedPaperCorpusId": "249642147"
                },
                {
                    "start": 1802,
                    "end": 1806,
                    "matchedPaperCorpusId": "1704893"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80810546875
        },
        {
            "corpus_id": "52070035",
            "title": "Reducing Gender Bias in Abusive Language Detection",
            "text": "Table 6 shows the results of experiments using the three methods proposed. The first rows are the baselines without any method applied. We can see from the second rows of each section that debiased word embeddings alone do not effectively correct the bias of the whole system that well, while gender swapping significantly reduced both the equality difference scores. Meanwhile, fine-tuning bias with a larger, less biased source dataset helped to decrease the equality difference scores and greatly improve the AUC scores from the generated unbiased test set. The latter improvement shows that the model significantly reduced errors on the unbiased set in general. \n\nTo our surprise, the most effective method was applying both debiased embedding and gender swap to GRU, which reduced the equality differences by 98% & 89% while losing only 1.5% of the original performance. We assume that this may be related to the influence of \"attending\" model architectures on biases as discussed in Section 4.3. On the other hand, using the three methods together improved both generated unbiased set performance and equality differences, but had the largest decrease in the original performance. \n\nAll methods involved some performance loss when gender biases were reduced. Especially, fine-tuning had the largest decrease in original test set performance. This could be attributed to the difference in the source and target tasks (abusive & sexist). However, the decrease was marginal (less than 4%), while the drop in bias was significant. We assume the performance loss happens because mitigation methods modify the data or the model in a way that sometimes deters the models from discriminating important \"unbiased\" features.",
            "score": 0.5229092986533025,
            "section_title": "Results & Discussion",
            "char_start_offset": 12092,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 74
                },
                {
                    "start": 75,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 665
                },
                {
                    "start": 668,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1186
                },
                {
                    "start": 1189,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1347
                },
                {
                    "start": 1348,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1532
                },
                {
                    "start": 1533,
                    "end": 1720
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8779296875
        },
        {
            "corpus_id": "259370743",
            "title": "Causal-Debias: Unifying Debiasing in Pretrained Language Models and Fine-tuning via Causal Invariant Learning",
            "text": "Racial debiasing refers to examining the association difference between European-American/African American names/terms and the stereotype words (pleasant vs. unpleasant) (Caliskan et al., 2017). Unlike gender debiasing, few prior studies investigated the racial debiasing problem, due to the difficulty of mitigating racial bias (Meade et al., 2022). A critical challenge is the potential word ambiguity (e.g., white, black) in various contexts (Guo et al., 2022). Table 4 reports the performance of Causal-Debias and Auto-Debias -the state-of-the-art racial debiasing model. Causal-Debias substantially decreases the racial biases on PLMs after fine-tuning, while obtaining comparable downstream performance. Auto-Debias, in contrast, still suffers from bias recurrence issue. Compared to Auto-Debias, Causal-Debias is more effective as it exploits downstream datasets for debiasing, which allows us to alleviate the influence of ambiguous words. \n\nBesides, the causal invariant learning in Causal-Debias encourages the model to learn consistent representations and clear meanings of ambiguous words so as to avoid bias-related associations.",
            "score": 0.5228945681080214,
            "section_title": "Results on Mitigating Racial Bias",
            "char_start_offset": 25744,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 947
                },
                {
                    "start": 950,
                    "end": 1142
                }
            ],
            "ref_mentions": [
                {
                    "start": 170,
                    "end": 193,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 329,
                    "end": 349,
                    "matchedPaperCorpusId": "239015827"
                },
                {
                    "start": 445,
                    "end": 463,
                    "matchedPaperCorpusId": "248780440"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86865234375
        },
        {
            "corpus_id": "267411833",
            "title": "Self-Debiasing Large Language Models: Zero-Shot Recognition and Reduction of Stereotypes",
            "text": "In a white-box approach, they reduce bias via a modified decoding algorithm based on the model's own description of the undesirable behavior. In contrast to this work, as well as most existing bias mitigation approaches, we focus instead on the LLM's zero-shot capabilities for black-box models, without modification to the training data, model parameters, or decoding algorithm. \n\nAs such, our work follows more closely prompt and instruction tuning approaches for bias mitigation, which modify the prompting language to elicit a certain behavior from the model. Because control tokens (Dinan et al., 2020;Lu et al., 2022) and continuous prompt tuning (Fatemi et al., 2023;Yang et al., 2023) require additional finetuning, our work aligns more closely with techniques that prepend textual instructions or triggers to a prompt (Abid et al., 2021;Sheng et al., 2020;Narayanan Venkit et al., 2023). Existing approaches, however, require careful prompt construction, with somewhat limited success in reducing bias (Borchers et al., 2022;Li and Zhang, 2023). To improve upon these works, Mattern et al. ( 2022) examine how the level of abstraction in the debiasing prompt can affect the LLM's output, but focus narrowly on gender occupation biases. We expand upon this work by simplifying the prompts while generalizing across a wide range of social groups. \n\nPrompt tuning is also closely tied to the few-and zero-shot learning literature (Brown et al., 2020;Helbling et al., 2023;Kojima et al., 2022;Liu et al., 2023;Radford et al., 2019;Reynolds and Mc-Donell, 2021;Wei et al., 2022;Zhao et al., 2021).",
            "score": 0.5224124579611674,
            "section_title": "Related Work",
            "char_start_offset": 4365,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 379
                },
                {
                    "start": 382,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1353
                },
                {
                    "start": 1356,
                    "end": 1601
                }
            ],
            "ref_mentions": [
                {
                    "start": 587,
                    "end": 607,
                    "matchedPaperCorpusId": "207852875"
                },
                {
                    "start": 607,
                    "end": 623,
                    "matchedPaperCorpusId": "249152301"
                },
                {
                    "start": 653,
                    "end": 674,
                    "matchedPaperCorpusId": "238582879"
                },
                {
                    "start": 674,
                    "end": 692,
                    "matchedPaperCorpusId": "253446867"
                },
                {
                    "start": 827,
                    "end": 846,
                    "matchedPaperCorpusId": "231603388"
                },
                {
                    "start": 846,
                    "end": 865,
                    "matchedPaperCorpusId": "218470535"
                },
                {
                    "start": 865,
                    "end": 895,
                    "matchedPaperCorpusId": "256616034"
                },
                {
                    "start": 1011,
                    "end": 1034,
                    "matchedPaperCorpusId": "248986638"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3984375
        },
        {
            "corpus_id": "264306211",
            "title": "Identifying and Adapting Transformer-Components Responsible for Gender Bias in an English Language Model",
            "text": "Despite these challenges, we tried to address some of these concerns by using multiple different bias metrics and testing the consistency of these across different seeds. We believe that the success of our approach is heavily contingent upon the datasets employed for both component identification and the subsequent fine-tuning of the chosen components. For example, using template-based datasets such as WinoBias or Professions could reduce the identified components' generalizability, as components that contribute to one form of gender bias may not contribute to another. The same applies to the fine-tuning stage as well. Using a dataset with limited variability in structure might result in only partial mitigation of the behavior. We therefore conclude that for even better bias reduction, it is essential to use and develop datasets that are diverse and representative of the behaviour being studied. \n\nFuture work For a wider picture of how our findings integrate in bias identification and mitigation studies, we would like to compare our approaches to other promising methods in the literature like concept erasure at the activation level (e.g., LEACE; Belrose et al., 2023) and changes to the language generation procedure (e.g., \"selfdebiasing\"; Schick et al., 2021). Future work should also test whether these mitigation strategies generalize to different conditions, for example, language models larger than GPT-2 small. Lastly, we also stress the importance of developing methodologies for operationalizing other forms of bias than binary gender in English, and to overcome difficulties we currently face when using contrastive sets and existing bias benchmarks. \n\nRan Zmigrod, Sabrina J Mielke, Hanna Wallach, and Ryan Cotterell. 2019. Counterfactual data augmentation for mitigating gender stereotypes in languages with rich morphology. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1651-1661.",
            "score": 0.5223029298341351,
            "section_title": "Discussion & Conclusions",
            "char_start_offset": 32575,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 737
                },
                {
                    "start": 738,
                    "end": 908
                },
                {
                    "start": 911,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1435
                },
                {
                    "start": 1436,
                    "end": 1678
                },
                {
                    "start": 1681,
                    "end": 1752
                },
                {
                    "start": 1753,
                    "end": 1854
                },
                {
                    "start": 1855,
                    "end": 1963
                }
            ],
            "ref_mentions": [
                {
                    "start": 1259,
                    "end": 1279,
                    "matchedPaperCorpusId": "232075876"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5595703125
        },
        {
            "corpus_id": "258461365",
            "title": "Modular and On-demand Bias Mitigation with Attribute-Removal Subnetworks",
            "text": "Societal biases are reflected in large pre-trained language models and their fine-tuned versions on downstream tasks. Common in-processing bias mitigation approaches, such as adversarial training and mutual information removal, introduce additional optimization criteria, and update the model to reach a new debiased state. However, in practice, end-users and practitioners might prefer to switch back to the original model, or apply debiasing only on a specific subset of protected attributes. To enable this, we propose a novel modular bias mitigation approach, consisting of stand-alone highly sparse debiasing subnetworks, where each debiasing module can be integrated into the core model on-demand at inference time. Our approach draws from the concept of \\emph{diff} pruning, and proposes a novel training regime adaptable to various representation disentanglement optimizations. We conduct experiments on three classification tasks with gender, race, and age as protected attributes. The results show that our modular approach, while maintaining task performance, improves (or at least remains on-par with) the effectiveness of bias mitigation in comparison with baseline finetuning. Particularly on a two-attribute dataset, our approach with separately learned debiasing subnetworks shows effective utilization of either or both the subnetworks for selective bias mitigation.",
            "score": 0.5216367262408728,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93701171875
        },
        {
            "corpus_id": "250390868",
            "title": "Indigenous Language Revitalization and the Dilemma of Gender Bias",
            "text": "Natural Language Processing (NLP), through its several applications, has been considered as one of the most valuable field in interdisciplinary researches, as well as in computer science. However, it is not without its flaws. One of the most common flaws is bias. This paper examines the main linguistic challenges of Inuktitut, an indigenous language of Canada, and focuses on gender bias identification and mitigation. We explore the unique characteristics of this language to help us understand the right techniques that can be used to identify and mitigate implicit biases. We use some methods to quantify the gender bias existing in Inuktitut word embeddings; then we proceed to mitigate the bias and evaluate the performance of the debiased embeddings. Next, we explain how approaches for detecting and reducing bias in English embeddings may be transferred to Inuktitut embeddings by properly taking into account the language\u2019s particular characteristics. Next, we compare the effect of the debiasing techniques on Inuktitut and English. Finally, we highlight some future research directions which will further help to push the boundaries.",
            "score": 0.5216291525800798,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.407470703125
        },
        {
            "corpus_id": "268819377",
            "title": "A Survey on Multilingual Large Language Models: Corpora, Alignment, and Bias",
            "text": "This method first searches for the biased prompts probes the biased content with such prompts, and then corrects the model bias. Mattern et al. [170] explored GPT-3's stereotypical associations with genders and jobs and proposed a framework to quantify and further reduce these biases using debiasing prompts. They also discussed prompt selection with varying degrees of abstraction and concluded that more concrete debiasing prompts exhibited a more pronounced effect. Dhingra et al. [171] demonstrated that employing a method involving chain-of-thought prompting through SHAP analysis can efficiently mitigate biases against queer people in the output of LLMs. Schick et al. [172] introduced a debiasing technique named Self-Debias which uses a model's internal knowledge to discourage biased text generation. It starts by utilizing hand-crafted prompts to encourage the model to generate toxic text. Subsequently, a second continuation that is non-discriminatory can be produced from the model by scaling down the probabilities of tokens considered likely under the first toxic generation. \n\n2) Data Debiasing: Data debiasing aims to mitigate bias within input training corpora, helping MLLMs generate debiased content. Currently, prevalent data debiasing efforts focus on two types of bias: language bias and demographic bias. \n\nLanguage bias mitigation. Language bias in MLLMs is caused by the imbalanced language proportion, acting as the dominance of English and other major languages in the available multilingual training corpora. Constructing more balanced corpora has proven to be an effective solution for mitigating language bias. For example, XNLI [173] was developed to support 15 languages on the evaluation of XLU, providing information-rich standard evaluation tasks for cross-language sentence understanding. In addition, the release of CulturaX [174], a multilingual dataset that includes 167 languages and a total of 63,000 tokens, addresses the lack of open-source and easy-to-use datasets for effectively training multilingual large models. Furthermore, The ROOTS dataset [175] was developed to cover 59 languages with a total size of 1.6TB. \n\nHowever, building more balanced corpora also faces many challenges.",
            "score": 0.5214625837428774,
            "section_title": "C. Debias Technique",
            "char_start_offset": 55087,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 1092
                },
                {
                    "start": 1095,
                    "end": 1222
                },
                {
                    "start": 1223,
                    "end": 1330
                },
                {
                    "start": 1333,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1539
                },
                {
                    "start": 1540,
                    "end": 1643
                },
                {
                    "start": 1644,
                    "end": 1827
                },
                {
                    "start": 1828,
                    "end": 2063
                },
                {
                    "start": 2064,
                    "end": 2164
                },
                {
                    "start": 2167,
                    "end": 2234
                }
            ],
            "ref_mentions": [
                {
                    "start": 144,
                    "end": 149,
                    "matchedPaperCorpusId": "273994217"
                },
                {
                    "start": 485,
                    "end": 490,
                    "matchedPaperCorpusId": "259316226"
                },
                {
                    "start": 1662,
                    "end": 1667,
                    "matchedPaperCorpusId": "52271711"
                },
                {
                    "start": 2095,
                    "end": 2100,
                    "matchedPaperCorpusId": "257378329"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71044921875
        },
        {
            "corpus_id": "270285699",
            "title": "Towards Understanding Task-agnostic Debiasing Through the Lenses of Intrinsic Bias and Forgetfulness",
            "text": "In this paper, we take two masked language models BERT-base-uncased (Kenton and Toutanova, 2019) and RoBERTa-base (Liu et al., 2019) as our backbone models, and use the language modeling head of these backbone models.Masked PLMs are better suited for testing our technique than autoregressive models, e.g., the GPT family, for three main reasons.First, our solution is based on Causal Mediation Analysis and PAC-Bayes training, both of which are model-agnostic.Second, GPT-2 has been reported to be unstable for classification tasks (Radford et al., 2019;Liu et al., 2023b), which are used to test the effectiveness of our technique.Lastly, the strong correlation between social groups and labels on classification tasks makes them more challenging to debias than text generation tasks in terms of relearning social bias.This issue can more easily be mitigated for text generation tasks, such as those performed by the GPT family of models, by intervening the generation-time sampling (Yang et al., 2022).The latter two reasons further contribute to the difficulty in distinguishing the effects of debiasing methods from the unsatisfactory performance of an autoregressive model for this task.\n\nFor implementing mitigation of gender bias through counterfactual data augmentation, we follow Kaneko et al. (2022) to rebalance the debiasing corpus 7 with gender words from Zhao et al. (2018).We run 150 epochs for debiasing both backbone models.The StereoSet score (Nadeem et al., 2021b) is used as the intrinsic bias evaluation metric over Masked PLMs; we conduct extrinsic bias evaluation over fine-tuned PLMs with three tasks, e.g., STS-B (Cer et al., 2017), BiasBios (De-Arteaga et al., 2019), and NLI-bias (De-Arteaga et al., 2019).",
            "score": 0.5210998596488767,
            "section_title": "Experimental Settings",
            "char_start_offset": 20872,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 217
                },
                {
                    "start": 217,
                    "end": 346
                },
                {
                    "start": 346,
                    "end": 461
                },
                {
                    "start": 461,
                    "end": 633
                },
                {
                    "start": 633,
                    "end": 821
                },
                {
                    "start": 821,
                    "end": 1005
                },
                {
                    "start": 1005,
                    "end": 1193
                },
                {
                    "start": 1195,
                    "end": 1389
                },
                {
                    "start": 1389,
                    "end": 1442
                },
                {
                    "start": 1442,
                    "end": 1734
                }
            ],
            "ref_mentions": [
                {
                    "start": 533,
                    "end": 555,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1290,
                    "end": 1310,
                    "matchedPaperCorpusId": "252222228"
                },
                {
                    "start": 1370,
                    "end": 1388,
                    "matchedPaperCorpusId": "52161864"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50634765625
        },
        {
            "corpus_id": "218971783",
            "title": "Language Models are Few-Shot Learners",
            "text": "We have presented this preliminary analysis to share some of the biases we found in order to motivate further research, and to highlight the inherent difficulties in characterizing biases in large-scale generative models; we expect this to be an area of continuous research for us and are excited to discuss different methodological approaches with the community. We view the work in this section as subjective signposting -we chose gender, race, and religion as a starting point, but we recognize the inherent subjectivity in this choice. Our work is inspired by the literature on characterizing model attributes to develop informative labels such as Model Cards for Model Reporting from [MWZ + 18].\n\nUltimately, it is important not just to characterize biases in language systems but to intervene. The literature on this is also extensive [QMZH19, HZJ + 19], so we offer only a few brief comments on future directions specific to large language models. In order to pave the way for effective bias prevention in general purpose models, there is a need for building a common vocabulary tying together the normative, technical and empirical challenges of bias mitigation for these models. Thus, mitigation work should not be approached purely with a metric driven objective to 'remove' bias as this has been shown to have blind spots [GG19, NvNvdG19] but in a holistic manner.",
            "score": 0.5207113067324448,
            "section_title": "Future Bias and Fairness Challenges",
            "char_start_offset": 103333,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.364501953125
        },
        {
            "corpus_id": "276235453",
            "title": "Understanding and Mitigating the Bias Inheritance in LLM-based Data Augmentation on Downstream Tasks",
            "text": "The average mitigation results of severe negative influences for each task and bias are shown in Figure 9. A detailed analysis of the mitigation effects on gender and cultural biases across different tasks, as well as the results on GPT-4o-mini, are provided in Appendix E.2. Our key conclusion is that, similar to the nuanced effects on downstream tasks, the effectiveness of different mitigation approaches is also nuanced, depending on various factors such as different types of bias, downstream tasks, bias ratios, and more, highlighting the difficulty in devising a unified mitigation solution. Specifically, token-based mitigation provides implicit cues and works best with simple biases and tasks, as it depends on the model's own understanding. It is more effective for gender bias than complex cultural bias and performs better in classification tasks than in detailed generation tasks. For gender generation tasks, salary recommendation benefits more than complex hiring recommendation. More augmentation data (50%) further enhance its performance. \n\nMask-based mitigation shows noticeable effects at lower bias ratios (5%), especially in tasks like cultural classification. As at this ratio, explicit biased terms have a more direct and pronounced impact on the model's performance. By masking these terms, the model is less likely to rely on biased information or features that could skew its decision-making, thereby reducing bias inheritance. However, as bias ratios increase, the influence of more subtle and implicit biases grows, necessitating more complex mitigation strategies. It also proves effective in contrastive explicit bias, where direct bias information is more clearly identifiable. \n\nLoss-based mitigation primarily depends on the distribution distance between the augmentation and the original data, showing significant effectiveness when the distance is large. It is more suitable for coarse-grained tasks. For example, it works well in classification tasks where decisions often rely on broader patterns. In generation tasks, coarse-grained contexts like salary recommendations perform better than finer-grained ones like hiring recommendation. Additionally, smaller proportions (e.g., 5% and 10%) of augmentation data yield better results, by introducing subtle shifts that avoid overfitting to the augmented distribution while effectively influencing the original bias.",
            "score": 0.520546208813335,
            "section_title": "Mitigation Results",
            "char_start_offset": 31739,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 895
                },
                {
                    "start": 896,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1058
                },
                {
                    "start": 1061,
                    "end": 1184
                },
                {
                    "start": 1185,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1456
                },
                {
                    "start": 1457,
                    "end": 1596
                },
                {
                    "start": 1597,
                    "end": 1711
                },
                {
                    "start": 1714,
                    "end": 1892
                },
                {
                    "start": 1893,
                    "end": 1938
                },
                {
                    "start": 1939,
                    "end": 2037
                },
                {
                    "start": 2038,
                    "end": 2177
                },
                {
                    "start": 2178,
                    "end": 2404
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8974609375
        },
        {
            "corpus_id": "276902427",
            "title": "Gender Encoding Patterns in Pretrained Language Model Representations",
            "text": "Knowledge localization has emerged as a critical area of study in NLP, focusing on identifying subnets within language models that are responsible for specific tasks, domains, or linguistic properties (Hendy et al., 2022;Panigrahi et al., 2023;Song et al., 2024;Choenni et al., 2023). These techniques have been extended to explore gender bias, pinpointing the internal components of models that encode bias. \n\nFor example, Chintam et al. (2023) employed causal inference methods, including techniques such as causal mediation analysis and differential masking, to identify attention heads responsible for biased behaviors in transformer models. Their work highlighted the ability to localize gender bias and proposed parameter-efficient fine-tuning strategies to mitigate it. Similarly, Lutz et al. (2024) introduced local contrastive editing, a technique leveraging unstructured pruning to precisely localize individual model weights responsible for encoding gender stereotypes. This method enabled them to edit these weights efficiently, mitigating bias without significant degradation of model performance. \n\nAlthough our research aligns with prior efforts in localizing bias within pretrained language models, we introduce a distinct methodological perspective. Furthermore, by broadening the scope of experimentation across diverse models and mitigation strategies, we aim to comprehensively explore how and where gender bias is encoded. Our analysis reinforces previous findings about bias concentration in specific model layers, while also paving the way for targeted and efficient intervention techniques.",
            "score": 0.5199043360922433,
            "section_title": "Knowledge Localization and Bias",
            "char_start_offset": 7621,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 408
                },
                {
                    "start": 411,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1110
                },
                {
                    "start": 1113,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1614
                }
            ],
            "ref_mentions": [
                {
                    "start": 221,
                    "end": 244,
                    "matchedPaperCorpusId": "256826987"
                },
                {
                    "start": 244,
                    "end": 262,
                    "matchedPaperCorpusId": "273901562"
                },
                {
                    "start": 262,
                    "end": 283,
                    "matchedPaperCorpusId": "258919240"
                },
                {
                    "start": 424,
                    "end": 445,
                    "matchedPaperCorpusId": "264306211"
                },
                {
                    "start": 788,
                    "end": 806,
                    "matchedPaperCorpusId": "273532822"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75341796875
        },
        {
            "corpus_id": "248006238",
            "title": "Mapping the Multilingual Margins: Intersectional Biases of Sentiment Analysis Systems in English, Spanish, and Arabic",
            "text": "The presence and impact of harmful social biases in machine learning and natural language processing systems is pervasive and well-documented in popular word embedding methods (Caliskan et al., 2017;Garg et al., 2018;Bolukbasi et al., 2016; due to large amounts of humanproduced training data that includes historical social biases. Notably, Caliskan et al. (2017) demonstrate such biases by introducing the Word Embedding Association Test (WEAT) which measures how similar socially sensitive sets of words (e.g., racial or gendered names) are to attributive sets of words (e.g., pleasant or unpleasant words) in the semantic space encoded by word embeddings. While Bolukbasi et al. (2016); Manzini et al. (2019) introduce methods for \"debiasing\" word embeddings in order to create more equitable semantic representations for usage in downstream tasks, Gonen and Goldberg (2019) argue that such methods are merely cosmetic since social biases are still evident in the semantic space after the application of such methods. Moreover, these \"debiasing\" techniques focus on a particular social cleavage such as gender or race (i.e., unisectional cleavages). In contrast, our work considers both unisectional and intersectional social biases.\n\nRecent studies have also begun to focus on social biases in transformer-based language models (Kurita et al., 2019;Bender et al., 2021). In particular, Bender et al. (2021) discusses how increasingly large transformer-based language model in practice regurgitate their training data, resulting in such models perpetuating social biases and harming users. Therefore, in this work we consider both static word embedding techniques and transformerbased language models. Crenshaw (1990) introduces intersectionality as an analytical framework to study the complex character of the privilege and marginalization faced by an individual with a variety of identities across a set of social cleavages such as race and gender. A canonical usage of intersectionality is in service of studying the simultaneous racial and gender discrimination faced by Black women, which cannot be understood in its totality using racial or gendered frameworks independently",
            "score": 0.5197321770570479,
            "section_title": "Related Works",
            "char_start_offset": 4374,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 176,
                    "end": 199,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 199,
                    "end": 217,
                    "matchedPaperCorpusId": "4930886"
                },
                {
                    "start": 217,
                    "end": 240,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 342,
                    "end": 364,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 666,
                    "end": 689,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 1333,
                    "end": 1354,
                    "matchedPaperCorpusId": "190000105"
                },
                {
                    "start": 1354,
                    "end": 1374,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 1391,
                    "end": 1411,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 1706,
                    "end": 1721,
                    "matchedPaperCorpusId": "24661090"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62646484375
        },
        {
            "corpus_id": "237753859",
            "title": "Slovene and Croatian word embeddings in terms of gender occupational analogies",
            "text": "Language corpora and datasets reflect linguistic variations (including different types of bias) in relation to social factors. NLP tools are trained on these data and can inherit the contained variations and biases. The bias in corpora can negatively impact NLP tools (Sun et al., 2019) and can perpetuate biases held towards certain groups. Word embeddings are trained on large corpora to capture syntactic and semantic relations between words and capture the expressed biases. \n\nFor instance, it has been shown that standard training data sets for part-ofspeech perform better on older people's language (Hovy and S\u00f8gaard, 2015). Garimella et al. (2019) show that a part-of-speech tagger and a dependency parser perform successfully on texts written by women, regardless of what data they had been trained on initially. On the other hand, male authors' texts are better tagged/parsed when the training data contained enough texts written by men. The success of tools such as parsers on male authors' texts may be due to the imbalances in the training data favouring male authorship. It has also been shown that NLP tools are more effective when demographic variations are considered (Volkova et al., 2013;Hovy, 2015). Hovy (2015) shows that including the information on the age and gender of authors improves the performance of three tasks in five different languages. \n\nBiases can have negative consequences in the coreference resolution task  Hutchinson et al. (2020) note that topics such as gun violence, homelessness, and addiction are over-represented, leading to disability topics receiving particularly negative scores in sentiment analysis tasks. Besides the aspects above, some authors call the attention to the effect biases can have on detection tools. For example, misogyny detection models may attribute high scores to non-misogynous texts simply because the latter contain the so-called identity terms, i.e. terms associated with misogyny (Nozza et al., 2019). In sum, the interplay of bias and NLP is an important and interesting field receiving increasing attention, notably regarding word embeddings, as explained next.",
            "score": 0.5196854457743797,
            "section_title": "R E L A T E D W O R K",
            "char_start_offset": 7001,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 478
                },
                {
                    "start": 481,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1370
                },
                {
                    "start": 1373,
                    "end": 1657
                },
                {
                    "start": 1658,
                    "end": 1766
                },
                {
                    "start": 1767,
                    "end": 1977
                },
                {
                    "start": 1978,
                    "end": 2139
                }
            ],
            "ref_mentions": [
                {
                    "start": 268,
                    "end": 286,
                    "matchedPaperCorpusId": "195316733"
                },
                {
                    "start": 606,
                    "end": 630,
                    "matchedPaperCorpusId": "7396193"
                },
                {
                    "start": 632,
                    "end": 655,
                    "matchedPaperCorpusId": "196181460"
                },
                {
                    "start": 1185,
                    "end": 1207,
                    "matchedPaperCorpusId": "2166300"
                },
                {
                    "start": 1207,
                    "end": 1218,
                    "matchedPaperCorpusId": "14021168"
                },
                {
                    "start": 1220,
                    "end": 1231,
                    "matchedPaperCorpusId": "14021168"
                },
                {
                    "start": 1447,
                    "end": 1471,
                    "matchedPaperCorpusId": "218487466"
                },
                {
                    "start": 1956,
                    "end": 1976,
                    "matchedPaperCorpusId": "204754417"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2125244140625
        },
        {
            "corpus_id": "271924022",
            "title": "GenderCARE: A Comprehensive Framework for Assessing and Reducing Gender Bias in Large Language Models",
            "text": "This section elucidates several pivotal insights derived from experimental investigations and analytical procedures: \n\n(1) Our GenderPair benchmark satisfies all dimensions of the criteria for gender equality benchmarks (Sec. 5.1). This indicates that GenderPair offers a more inclusive, diverse, explanatory, objective, robust, and realistic quantification of gender bias. (2) In examining LLMs of varying sizes, it is observed that models endowed with a larger parameter space (13B parameters) exhibit a reduced manifestation of gender bias in comparison to their smaller counterparts (7B parameters), as detailed in Sec. 5.2. However, it is crucial to acknowledge that, despite this reduction, significant biases remain extant. This finding underscores the fact that, while scaling up model size may contribute to bias mitigation, it is not a panacea. Thus, the implementation of explicit debiasing strategies remains imperative. (3) The proposed debiasing techniques effectuate a significant diminution of gender bias across a spectrum of models and benchmarks (Sec. 5.3 and Sec. 5.4). Notably, larger models demonstrate more pronounced improvements, potentially attributable to their augmented capacity for learning and integrating debiased representations during the debiasing process. (4) As evidenced in Table 7, although fine-tuning introduces minor performance trade-offs, these fluctuations remain confined within a 2% margin across GLUE and MMLU mainstream language tasks. Intriguingly, fine-tuning appears to enhance performance in certain domains, such as social science within the MMLU, likely due to the pronounced intersectionality with gender identity aspects. (5) The consistency in bias quantification, irrespective of prompt structural variations and model architectures, as delineated in Sec. 5.4, validates the robustness of our approach.",
            "score": 0.5192638903646909,
            "section_title": "Take-home Messages",
            "char_start_offset": 50904,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 119,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 854
                },
                {
                    "start": 855,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1070
                },
                {
                    "start": 1071,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1484
                },
                {
                    "start": 1485,
                    "end": 1678
                },
                {
                    "start": 1679,
                    "end": 1814
                },
                {
                    "start": 1815,
                    "end": 1861
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8369140625
        },
        {
            "corpus_id": "268876143",
            "title": "Manipulating and Mitigating Generative Model Biases Without Retraining",
            "text": "After balancing age biases (and ensuring gender biases do not deviate greatly), we repeat a similar process to balance racial biases, tuning [S 5 , S 6 , S 7 ] in a 3D nested loop, using a range of: 0 \u2264 S i \u2264 0.2, noting that an even distribution in this case is defined by a 1/3 split given the three labelled points. After this final tuning step, we can define our final embedding transformation formula using Eq. ( 4). Applying this transformation with the empirically-derived S i combinations to future language embedding outputs would balance gender, age and racial representations of test images. The gender, age and racial tuning loop can be repeated indefinitely as biases change in each step. For practical purposes, we implement one tuning loop in our experiments. \n\nFor the 'N 100 ' experiment, we follow a similar method, only that we reduce the range for gender and age to 0 \u2264 S i \u2264 0.35. Through extensive experimental analyses, we modified how we tune racial S i values and in this case, prove that the extrapolation beyond cluster centroids can be effective for mitigating bias. We use a range of \u22120.2 \u2264 S i \u2264 0.2, finding that shifting the output in the reverse direction was beneficial for improving racial representations.",
            "score": 0.5192347519671517,
            "section_title": "Bias Manipulation to Balance Social Biases",
            "char_start_offset": 22925,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 774
                },
                {
                    "start": 777,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1241
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53173828125
        },
        {
            "corpus_id": "233848348",
            "title": "The practical ethics of bias reduction in machine translation: why domain adaptation is better than data debiasing",
            "text": "problems, and they are trained on different kinds of data (e.g., text-based, imagebased). Consequently, as discussed at length in this article, debiasing data prior to training, without compromising the performance of the system, will be possible in some cases, but not in others. And a greater awareness of such differences can usefully expose the unhelpful superficiality of any high-level recommendation that all AI data sets must be free from bias prior to training.\n\nThen there is the problem of identifying 'Bias' in the first place. Recently, Blodgett et al. (2020) have argued that several distinct and often underspecified conceptualisations of bias has been prominent in work addressing the skewed datasets used for Natural Language Processing (NLP) tasks, and the authors show how this has fostered terminological imprecision. As Maudslay et al. (2019) put it, the 'measurement and mitigation of gender bias relies on the chosen operationalisation of gender bias. As a direct consequence, how researchers choose to operationalise bias determines both the techniques at one's disposal to mitigate bias, as well as the yardstick by which success is determined'. More specifically, gender bias is not a homogenous phenomenon since it is manifest in linguistic data in several distinct ways. In the context of Machine Translation (MT), the primary focus has been on the representational harms, such as stereotyping, that arise from specific linguistic items and structures (e.g., gendered pronouns). While this captures a manifestation of gender bias that is quite easily identifiable within a dataset, there are numerous other less explicitly marked ways in which stereotyping (for example) becomes manifest. The complexities and subtleties of how power differentials and social hierarchies operate within and through language make the task of simply identifying all gender bias within a dataset incredibly complex. 3 To explore these important issues in greater detail, this article will discuss particular human biases present in the training data used to build Neural Machine Translation (NMT) systems. Crucially, it will be shown that the datasets created to train such systems cannot always be adequately debiased prior to training using existing approaches without considerably diminishing the quality of",
            "score": 0.5190071627598432,
            "section_title": "Introduction",
            "char_start_offset": 4241,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 550,
                    "end": 572,
                    "matchedPaperCorpusId": "218971825"
                },
                {
                    "start": 841,
                    "end": 863,
                    "matchedPaperCorpusId": "202541569"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.33203125
        },
        {
            "corpus_id": "235097394",
            "title": "Towards a Comprehensive Understanding and Accurate Evaluation of Societal Biases in Pre-Trained Transformers",
            "text": "al., 2020). Moreover, we show that employing multiple tests is necessary for a full picture of the issue as no single test is currently sufficient.\n\nIn the context of our work, \"bias\" refers specifically to the preference of a model for one gender or race in the presence of an otherwise neutral context. As an example, consider the sequence \" [MASK] wept upon arriving to the scene.\" With no additional information, an equitable system would exhibit no preference for female over male, or African-American over European-American names; however, our results indicate that there is often a statistically significant preference (p < 0.0001) for associating female and African-American identifiers with being more \"emotional.\"\n\nWe provide two key contributions to understanding and mitigating bias in contextual language models. First, we conduct a comprehensive, comparative evaluation of gender and racial bias using multiple tests for widely-used pretrained models. Second, we construct a novel experiment for debiasing a contextual language model on a downstream task (Zellers et al., 2018)",
            "score": 0.5189939813533684,
            "section_title": "Introduction",
            "char_start_offset": 1821,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63916015625
        },
        {
            "corpus_id": "51888520",
            "title": "Gender Bias in Neural Natural Language Processing",
            "text": "An embedding model transforms syntactic elements (words) into real vectors capturing syntactic and semantic relationships among words. Bolukbasi et al. [2016] show that embeddings demonstrate bias. Objectionable analogies such as \"man is to woman as programmer is to homemaker\" indicate that word embeddings pick up on historical biases encoded in their training corpus. Their solution modifies the embedding's parameters so that gender-neutral words no longer carry a gender component. We omit here the details of how the gender component is identified and removed. What is important, however, is that only gender-neutral words are affected by the debiasing procedure. \n\nAll of our experimental systems employ an initial embedding layer which is either initialized and fixed to some pretrained embedding, initialized then trained alongside the rest of the main NLP task, or trained without initializing. In the latter two cases, the embedding can be debiased at different stages of the training process. We investigate this choice in Section 5. \n\nClosely Related Work Two independent and concurrent work [Zhao et al., 2018, Rudinger et al., 2018] explore gender bias in coreference resolution systems. There are differences in our goals and methods. They focus on bias in coreference resolution systems and explore a variety of such systems, including rule-based, feature-rich, and neural systems. In contrast, we study bias in a set of neural natural language processing tasks, including but not exclusively coreference resolution. This difference in goals leads to differences in the notions of bias. We define bias in terms of internal scores common to a neural networks, while both Zhao et al. [2018] and Rudinger et al. [2018] evaluate bias using Winogram-schema style sentences specifically designed to stress test coreference resolutions. \n\nThe independently discovered mitigation technique of Zhao et al. [2018] is closely related to ours. Further, we inspect the effect of debiasing different configurations of word embeddings with and without counterfactual data augmentation. We also empirically study how gender bias grows as training proceeds with gradient descent with and without the bias mitigation techniques.",
            "score": 0.5189510489577203,
            "section_title": "Coreference Resolution",
            "char_start_offset": 7575,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 669
                },
                {
                    "start": 672,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1045
                },
                {
                    "start": 1048,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1250
                },
                {
                    "start": 1251,
                    "end": 1398
                },
                {
                    "start": 1399,
                    "end": 1533
                },
                {
                    "start": 1534,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 1846
                },
                {
                    "start": 1849,
                    "end": 1948
                },
                {
                    "start": 1949,
                    "end": 2087
                },
                {
                    "start": 2088,
                    "end": 2227
                }
            ],
            "ref_mentions": [
                {
                    "start": 135,
                    "end": 158,
                    "matchedPaperCorpusId": "1704893"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7099609375
        },
        {
            "corpus_id": "250390797",
            "title": "The Birth of Bias: A case study on the evolution of gender bias in an English language model",
            "text": "We understand bias as a systematic deviation in behaviour from a norm. As our focus is on gender bias in language models, the relevant behaviour we are measuring is how strongly certain words or concepts (in our case occupation terms such as nurse or carpenter) are associated by the model with one gender instead of another. This strength of association can be measured in different ways and at different points in the language modelling pipeline. In particular, we will look at bias in internal representations of the model and in its output behaviour. Ideally, the strength of association should be equal for different genders. If the model deviates from this norm, we say that the model exhibits gender bias. 2 hether bias in a language model causes harm, depends on the downstream application of the model and what constitutes fair and just behaviour in this particular context, but we believe that a detailed understanding of how bias is learned by and represented in these models can facilitate the development of methods to counteract bias that are tailored to a particular application and the potential harm bias can cause in that context. \n\nWith this broad scope, we hope to learn how the language model represents gender stereotypes of occupations in earlier representations of the pipeline, and how these may help explain representational harms (Blodgett et al., 2020) downstream. For instance, if a language model with gender stereotypes for occupations is used in a translation system, it may propagate the undesirable world-view of all doctors being male and all nurses being female. Understanding how these stereotypical representations come about can help in developing new detection and mitigation strategies for these and other stereotypes in AI systems building on language models. \n\nThe LSTM language model In this paper, we study the gender bias of an LSTM language model (Hochreiter and Schmidhuber, 1997). We follow the setup from Gulordava et al. (2018), and train the model on their training set of ~90M tokens, with a vocabulary of 50,000 (full-word) tokens, extracted from the English Wikipedia corpus. Following Gulordava et al., we lower the learning rate at epoch 20 using a plateau scheduler. Our training regime differs in one aspect: we use weight-tying for the encoder and decoder (Press and Wolf, 2017).",
            "score": 0.5188092143912196,
            "section_title": "Gender bias",
            "char_start_offset": 3830,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 70
                },
                {
                    "start": 71,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 1148
                },
                {
                    "start": 1151,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1598
                },
                {
                    "start": 1599,
                    "end": 1801
                },
                {
                    "start": 1804,
                    "end": 1929
                },
                {
                    "start": 1930,
                    "end": 2130
                },
                {
                    "start": 2131,
                    "end": 2224
                },
                {
                    "start": 2225,
                    "end": 2339
                }
            ],
            "ref_mentions": [
                {
                    "start": 1357,
                    "end": 1380,
                    "matchedPaperCorpusId": "218971825"
                },
                {
                    "start": 1894,
                    "end": 1928,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 1955,
                    "end": 1978,
                    "matchedPaperCorpusId": "4460159"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.49658203125
        },
        {
            "corpus_id": "271769812",
            "title": "Detecting and Mitigating LGBTQIA+ Bias in Large Norwegian Language Models",
            "text": "Different bias types, like gender and racial bias, have been uncovered in a wide range of natural language processing (NLP) applications and resources, including large language models (LLMs) (Caliskan et al., 2017;May et al., 2019;Kurita et al., 2019;Nozza et al., 2021;Zhao et al., 2018a). Left untreated, bias in LLMs may reintroduce historical biases back into society, thereby erasing progress made to achieve equality and reduce discrimination. Bender et al. (2021) describe this issue as a valuelock, in which technology reliant on language models may reify older, less-inclusive understandings. \n\nThe research field of bias in NLP aims to prevent this by introducing bias mitigation methods (Bolukbasi et al., 2016;Zhao et al., 2018b;Lauscher et al., 2021;Felkner et al., 2023). Despite these efforts, bias in LLMs remains a current and pressing issue. \n\nA limitation of the current state of the research field is the primary focus being placed on gender bias (Talat et al., 2022). With a few notable exceptions (Nozza et al., 2022;Felkner et al., 2023), the effects that bias in LLMs may have on the LGBT-QIA+ community remain largely unknown, constituting a major research gap. As the breakthroughs of the LGBTQIA+ rights movement are quite recent in most parts of the world, it is possible that negative attitudes and harmful language directed at the queer community1 are present in training data of LLMs. Dodge et al. (2021) showed that efforts to filter web-based text corpora often remove text written by and about the LGBTQIA+ community, strengthening the hypothesis that LGBTQIA+ bias may be present in LLMs. \n\nFurthermore, the development of LLMs has been dominated by the English language (Bender et al., 2021;Talat et al., 2022).",
            "score": 0.5185727449983524,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 601
                },
                {
                    "start": 604,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 859
                },
                {
                    "start": 862,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1623
                },
                {
                    "start": 1626,
                    "end": 1747
                }
            ],
            "ref_mentions": [
                {
                    "start": 191,
                    "end": 214,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 214,
                    "end": 231,
                    "matchedPaperCorpusId": "85518027"
                },
                {
                    "start": 231,
                    "end": 251,
                    "matchedPaperCorpusId": "190000105"
                },
                {
                    "start": 251,
                    "end": 270,
                    "matchedPaperCorpusId": "235097294"
                },
                {
                    "start": 270,
                    "end": 289,
                    "matchedPaperCorpusId": "4952494"
                },
                {
                    "start": 450,
                    "end": 470,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 698,
                    "end": 722,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 741,
                    "end": 763,
                    "matchedPaperCorpusId": "237440429"
                },
                {
                    "start": 967,
                    "end": 987,
                    "matchedPaperCorpusId": "247626152"
                },
                {
                    "start": 1019,
                    "end": 1039,
                    "matchedPaperCorpusId": "248780452"
                },
                {
                    "start": 1416,
                    "end": 1435,
                    "matchedPaperCorpusId": "237568724"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.29541015625
        },
        {
            "corpus_id": "269982989",
            "title": "A survey on fairness of large language models in e-commerce: progress, application, and challenge",
            "text": "Racial bias [48] can also be present, leading to biased outputs or decisions based on race-related information.The models tended to generate biased content for certain racial groups, including unwarranted details based on race.The models exhibited favoritism and has racially-skewed socio-economic projection towards a certain racial group in content recommendations.A important origin of this racial bias is sampling.The issue arises when the distribution of samples from different demographic groups in the training data differs from the actual population distribution, causing the model to exhibit biased behavior.Pre-existing racial prejudices and inequalities within the data can be reflected in the outputs of the language models.Additionally, the vulnerability of the models to prompt manipulation with malicious intent can lead to biased responses.\n\nOne potential solution to mitigate racial bias in LLM is to analyze the distribution of racial groups within the data and adjusting the sampling process to ensure equal representation.Random sampling techniques could be adopted to select data for training the language models, which helps reduce the risk of bias by ensuring that each data point has an equal chance of being selected, regardless of racial characteristics.Stratified sampling can be employed to ensure proportional representation of different racial groups in the training data, which involves dividing the dataset into strata based on race and then sampling proportionally from each stratum to ensure balanced representation.",
            "score": 0.5183036252527822,
            "section_title": "Racial bias",
            "char_start_offset": 18088,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 111,
                    "end": 227
                },
                {
                    "start": 227,
                    "end": 367
                },
                {
                    "start": 367,
                    "end": 418
                },
                {
                    "start": 418,
                    "end": 617
                },
                {
                    "start": 617,
                    "end": 736
                },
                {
                    "start": 736,
                    "end": 856
                },
                {
                    "start": 858,
                    "end": 1042
                },
                {
                    "start": 1042,
                    "end": 1280
                },
                {
                    "start": 1280,
                    "end": 1550
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.403564453125
        },
        {
            "corpus_id": "259859034",
            "title": "Unlearning Bias in Language Models by Partitioning Gradients",
            "text": "Regardless of the debiasing goal, the north star is to eliminate harms caused by the model, so we must be motivated by how pretrained language models are used. \n\nMinimizing the cost of adoption for debiased language models is a high priority for debiasing, as any barriers may cause people to be skeptical of the societal benefits. To ensure that people have little reason not to use our debiased model, we aim to minimize representing bias while still maximizing the representation ability of the model. In this study, we focus on debiasing pretrained language models used directly for masked language modeling. Crucially, we modify only their weights post-hoc without any changes to the architecture or additional modules. In this way, we enable key stakeholders to swap out their masked language models (by simply loading a different set of weights) but still use the exact same code for masked predictions, just as they might with any other finetuned model. Furthermore, stakeholders need not rely on the people pretraining the model to have incorporated debiasing procedures during the pretraining process. We restrict our study to masked language modeling, as the use cases of language models for other downstream tasks are disparate, and extrinsic evaluation of bias in those tasks is often be confounded by task-specific finetuning (Meade et al., 2022). \n\nWe expect, based on the results from Kaneko and Bollegala (2021); Vig et al. (2020), that problematic social biases propagate throughout large portions of language models. Furthermore, based on the Lottery Ticket Hypothesis (Frankle and Carbin, 2019), we hypothesize that most bias is encoded by specific groups of neurons rather than individual weights throughout the model. So, we propose a gradient-based debiasing method called partitioned contrastive gradient unlearning (PCGU) to locate where in the model these problematic inferences originate from and to systematically retrain those parts of the model to unlearn this biased behavior. In our experiments, we use PCGU to unlearn biases in the gender-profession domain and evaluate our approach using prior association tests for bias/stereotypes.",
            "score": 0.5178116372883825,
            "section_title": "Introduction",
            "char_start_offset": 1905,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 162,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1361
                },
                {
                    "start": 1364,
                    "end": 1535
                },
                {
                    "start": 1536,
                    "end": 1739
                },
                {
                    "start": 1740,
                    "end": 2007
                },
                {
                    "start": 2008,
                    "end": 2167
                }
            ],
            "ref_mentions": [
                {
                    "start": 1340,
                    "end": 1360,
                    "matchedPaperCorpusId": "239015827"
                },
                {
                    "start": 1401,
                    "end": 1428,
                    "matchedPaperCorpusId": "231698657"
                },
                {
                    "start": 1430,
                    "end": 1447,
                    "matchedPaperCorpusId": "227275068"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70458984375
        },
        {
            "corpus_id": "250390797",
            "title": "The Birth of Bias: A case study on the evolution of gender bias in an English language model",
            "text": "Based on our finding we can distinguish three phases in the evolution of gender representation and gender bias: (i) formation, (ii) consolidation, and (iii) specialisation. We saw that our measures for bias and the method for bias mitigation behave Based on the bias scores for IE and STS-B bias, we chose \"receptionist\", \"nurse\", \"librarian\", and \"therapist\" for the female words and \"mechanic\", \"engineer\", \"scientist\", and \"architect\" for the male word-list. \n\ndifferently in these different phases, which appears to be connected to how locally gender information is represented in the internal representations of the model. Only if the relevant information is concentrated in a particular part of the model and linearly decodable, can we reliably and selectively remove gender information without hurting the overall language model performance. This observation might not be so important when thinking about gender bias in current large language models, as the sheer scale of the datasets that these models are trained on and the high frequency of gendered words makes it very likely that they have progressed far into the 'specialisation phase' with respect to their representation of gender. However, it could matter when considering other types of biases, where the words and phrases driving the birth of these biases may be much less frequent. Hence, even in large language models trained on several orders of magnitude more data than the language model we used in this study, the relevant representations for other biases might very well still be in something equivalent to our 'formation' or 'consolidation phase'. Indeed, work on studying the effect of fine-tuning has shown that the manifestation of bias can still change significantly in pre-trained models (Choenni et al., 2021;Webster et al., 2020).",
            "score": 0.5174422596259021,
            "section_title": "Different phases in the evolution of gender",
            "char_start_offset": 33366,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 461
                },
                {
                    "start": 464,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 1197
                },
                {
                    "start": 1198,
                    "end": 1351
                },
                {
                    "start": 1352,
                    "end": 1624
                },
                {
                    "start": 1625,
                    "end": 1814
                }
            ],
            "ref_mentions": [
                {
                    "start": 1770,
                    "end": 1792,
                    "matchedPaperCorpusId": "237581611"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.428955078125
        },
        {
            "corpus_id": "268819377",
            "title": "A Survey on Multilingual Large Language Models: Corpora, Alignment, and Bias",
            "text": "Bias acquired by MLLMs, such as gender bias, race bias, and language bias, pose significant challenges to the fairness of MLLMs, which severely restricts the deployment of MLLMs in real-world applications. Existing literature on bias mainly focuses on various stereotypical biases in English [13], [14], which limits its generalizability to other languages. Additionally, prior research primarily focused on bias within individual languages or limited attributes in LLMs [126]. Bias in MLLMs has not been well investigated. This section gives a systematic and in-depth review of bias in MLLMs across various languages. \n\nWe aim to address the following questions. Why do MLLMs bias and what are the types of bias in existing MLLMs (Bias Category), how to evaluate bias in MLLMs (Bias Benchmark), what can be done to mitigate the bias, and whether debiasing techniques affect the performance of MLLMs (Debias Technique).",
            "score": 0.5171991299657855,
            "section_title": "V. BIAS ON MULTI-LINGUAL LANGUAGE MODELS",
            "char_start_offset": 42688,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 477
                },
                {
                    "start": 478,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 618
                },
                {
                    "start": 621,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 919
                }
            ],
            "ref_mentions": [
                {
                    "start": 292,
                    "end": 296,
                    "matchedPaperCorpusId": "218487466"
                },
                {
                    "start": 298,
                    "end": 302,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 471,
                    "end": 476,
                    "matchedPaperCorpusId": "237491723"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63330078125
        },
        {
            "corpus_id": "253397743",
            "title": "Bridging Fairness and Environmental Sustainability in Natural Language Processing",
            "text": "Recent works focus on measuring and mitigating bias in contextualized language representations (Kurita et al., 2019;Bordia and Bowman, 2019;Qian et al., 2019;Webster et al., 2020;Nangia et al., 2020;Sap et al., 2020) and in downstream scenarios, e.g., for dialog (e.g., Sheng et al., 2019;Dinan et al., 2020a;Barikeri et al., 2021), co-reference resolution (Zhao et al., 2018), and NLI (Rudinger et al., 2017;Dev et al., 2020). Similarly, researchers have explored multilingual scenarios (e.g., Lauscher and Glava\u0161, 2019;Lauscher et al., 2020c;Ahn and Oh, 2021), more fine-grained biases (Dinan et al., 2020b), and more biases, beyond the prominent sexism and racism dimensions (e.g., Zhao et al., 2018;Rudinger et al., 2018), like speciesist bias (Takeshita et al., 2022). Sustainability in NLP. Strubell et al. (2019) have called for more awareness of NLP's environmental impact. Reducing the energy consumption can be achieved through efficient pre-training (Di Liello et al., 2021), smaller models and employing less pre-training data considering the specific needs of the task at hand (e.g., P\u00e9rez-Mayos et al., 2021;Zhang et al., 2021). If a PLM is already in-place, one can rely on sample-efficient methods (e.g., Lauscher et al., 2020b), or refrain from fully finetuning the model (e.g, Houlsby et al., 2019;Pfeiffer et al., 2021).",
            "score": 0.5170455990906104,
            "section_title": "Related Work",
            "char_start_offset": 24269,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 796
                },
                {
                    "start": 797,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1339
                }
            ],
            "ref_mentions": [
                {
                    "start": 95,
                    "end": 116,
                    "matchedPaperCorpusId": "190000105"
                },
                {
                    "start": 116,
                    "end": 140,
                    "matchedPaperCorpusId": "102352788"
                },
                {
                    "start": 140,
                    "end": 158,
                    "matchedPaperCorpusId": "170078973"
                },
                {
                    "start": 179,
                    "end": 199,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 199,
                    "end": 216,
                    "matchedPaperCorpusId": "207853290"
                },
                {
                    "start": 270,
                    "end": 289,
                    "matchedPaperCorpusId": "202537041"
                },
                {
                    "start": 289,
                    "end": 309,
                    "matchedPaperCorpusId": "207852875"
                },
                {
                    "start": 309,
                    "end": 331,
                    "matchedPaperCorpusId": "235358955"
                },
                {
                    "start": 357,
                    "end": 376,
                    "matchedPaperCorpusId": "4952494"
                },
                {
                    "start": 386,
                    "end": 409,
                    "matchedPaperCorpusId": "5310359"
                },
                {
                    "start": 409,
                    "end": 426,
                    "matchedPaperCorpusId": "201670701"
                },
                {
                    "start": 495,
                    "end": 521,
                    "matchedPaperCorpusId": "135465247"
                },
                {
                    "start": 521,
                    "end": 544,
                    "matchedPaperCorpusId": "226237614"
                },
                {
                    "start": 544,
                    "end": 561,
                    "matchedPaperCorpusId": "237491723"
                },
                {
                    "start": 588,
                    "end": 609,
                    "matchedPaperCorpusId": "218487627"
                },
                {
                    "start": 685,
                    "end": 703,
                    "matchedPaperCorpusId": "4952494"
                },
                {
                    "start": 703,
                    "end": 725,
                    "matchedPaperCorpusId": "13756572"
                },
                {
                    "start": 797,
                    "end": 819,
                    "matchedPaperCorpusId": "174802812"
                },
                {
                    "start": 1097,
                    "end": 1122,
                    "matchedPaperCorpusId": "237433614"
                },
                {
                    "start": 1122,
                    "end": 1141,
                    "matchedPaperCorpusId": "226289695"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1041259765625
        },
        {
            "corpus_id": "271902917",
            "title": "REFINE-LM: Mitigating Language Model Stereotypes via Reinforcement Learning",
            "text": "Using RL, our method does not require any form of manual annotations, but rather uses the LM output to mitigate a wide variety of biases in the answer. While RL has been successfully applied in algorithmic fairness [22,41,46], this is, to the best of our knowledge, the first approach that applies RL for mitigation a wide rage of biases, not only in \"more traditional\" masked LMs, but also in Large LMs such as LLama2 or Mistral. In particular, our method allows us to (i) reduce training resources, (ii) avoid the need for manual annotation, and (iii) support a wide range of stereotypical biases, including gender-occupation, ethnicity, nationality, and religion. The main contributions of our paper are the following: \n\n\u2022 We formulate bias mitigation as contextual bandits RL problem that uses bias measuring framework inspired by [27]. \u2022 We propose REFINE-LM that mitigates different types of stereotypes such as those based on gender, nationality, ethnicity, and religion from any LMs. As shown in our evaluation, REFINE-LM is easy to train and can successfully suppress stereotypes in LMs as well as LLMs without affecting model performance. \u2022 An evaluation of REFINE-LM based on (a) the definitions of bias on the datasets proposed by Li et al. [27], and (b) the performance of the debiased LM on downstream tasks. \n\nThe rest of the paper is organized as follows. Section 2 surveys state of the art in bias detection and mitigation for language models in general. Section 3 explains the framework used to quantify bias as well as the inner workings of REFINE-LM, our proposed solution to reduce bias in pre-trained LMs. Section 4 then describes the empirical study of REFINE-LM, and Section 5 discusses our results as well as avenues for future research.",
            "score": 0.5167608971262823,
            "section_title": "Introduction",
            "char_start_offset": 4027,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 721
                },
                {
                    "start": 724,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1322
                },
                {
                    "start": 1325,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1627
                },
                {
                    "start": 1628,
                    "end": 1762
                }
            ],
            "ref_mentions": [
                {
                    "start": 215,
                    "end": 219,
                    "matchedPaperCorpusId": "7137572"
                },
                {
                    "start": 219,
                    "end": 222,
                    "matchedPaperCorpusId": "234341131"
                },
                {
                    "start": 222,
                    "end": 225,
                    "matchedPaperCorpusId": "238586034"
                },
                {
                    "start": 835,
                    "end": 839,
                    "matchedPaperCorpusId": "222141056"
                },
                {
                    "start": 1253,
                    "end": 1257,
                    "matchedPaperCorpusId": "222141056"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7919921875
        },
        {
            "corpus_id": "248266397",
            "title": "A Survey on Bias and Fairness in Natural Language Processing",
            "text": "As NLP models become more integrated with the everyday lives of people, it becomes important to examine the social effect that the usage of these systems has. While these models understand language and have increased accuracy on difficult downstream tasks, there is evidence that these models amplify gender, racial and cultural stereotypes and lead to a vicious cycle in many settings. In this survey, we analyze the origins of biases, the definitions of fairness, and how different subfields of NLP mitigate bias. We finally discuss how future studies can work towards eradicating pernicious biases from NLP algorithms.",
            "score": 0.516659500137276,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5400390625
        },
        {
            "corpus_id": "249063052",
            "title": "Toward Understanding Bias Correlations for Mitigation in NLP",
            "text": "The latent factor behind different prejudices has been referred to the \"generalized prejudice\" [4]. With these issues considered, this work seeks to provide a systematic study towards understanding bias correlations in bias mitigation. But first, why might it be acceptable to consider independent debiasing? In a preliminary experiment on a benchmark dataset, we show that a gender-debiased toxicity classifier [22] also reduces racial and religious biases in Fig. 2. This observation suggests a positive bias correlation (as also indicated by the previously mentioned sociological findings), therefore, an independent debiasing approach might still work to reduce the total bias. However, can there be negative bias correlations in bias mitigation? For example, when mitigating biases against various identities, a model debiased for one bias form may have negative influence on debiasing other forms of bias. When debiasing independently with negative bias correlations, failing to account for these correlations may render current bias mitigation approaches ineffective. Similar questions can be asked when debiasing at the data level. For example, will debiasing for gender negatively influence the results of debiasing for race and religion in word embeddings? Therefore, in this work, we explore the possibility of negative bias correlations. Given its considerable practical importance, we further questions whether a joint debiasing approach can outperform conventional approaches for independent bias mitigation in terms of reducing the total bias? \n\nTo provide an in-depth understanding toward bias correlations for bias mitigation in NLP, we examine multiple forms of biases at both data and model levels, on two representative NLP tasksword embeddings and toxicity detection. Particularly, we seek to answer the following research questions: \n\n\u2022 RQ. 1. Are biases of different social identities (e.g., gender, race) correlated? How are they correlated across various tasks? \u2022 RQ. 2. With potentially correlated biases, will a joint bias mitigation strategy outperform independent debiasing in terms of reducing the total bias of all social identities? \u2022 RQ. 3. When jointly mitigating multiple biases, do we have to trade off accuracy/utility for debiasing?",
            "score": 0.5164441781924175,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2250,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 100,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1349
                },
                {
                    "start": 1350,
                    "end": 1558
                },
                {
                    "start": 1561,
                    "end": 1788
                },
                {
                    "start": 1789,
                    "end": 1854
                },
                {
                    "start": 1857,
                    "end": 1940
                },
                {
                    "start": 1941,
                    "end": 1986
                },
                {
                    "start": 1987,
                    "end": 2164
                },
                {
                    "start": 2165,
                    "end": 2270
                }
            ],
            "ref_mentions": [
                {
                    "start": 412,
                    "end": 416,
                    "matchedPaperCorpusId": "218630015"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65771484375
        },
        {
            "corpus_id": "273901307",
            "title": "On Mitigating Performance Disparities in Multilingual Speech Recognition",
            "text": "How far have we come in mitigating performance disparities across genders in multilingual speech recognition? We compare the impact on gender disparity of different fine-tuning algorithms for automated speech recognition across model sizes, languages and gender. We look at both performance-focused and fairness-promoting algorithms. Across languages, we see slightly better performance for female speakers for larger models regardless of the fine-tuning algorithm. The best trade-off between performance and parity is found using adapter fusion. Fairness-promoting fine-tuning algorithms (Group-DRO and Spectral Decoupling) hurt performance compared to adapter fusion with only slightly better performance parity. LoRA increases disparities slightly. Fairness-mitigating fine-tuning techniques led to slightly higher variance in performance across languages, with the exception of adapter fusion.",
            "score": 0.5164347225903981,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55712890625
        },
        {
            "corpus_id": "258426630",
            "title": "A Review of ChatGPT Applications in Education, Marketing, Software Engineering, and Healthcare: Benefits, Drawbacks, and Research Directions",
            "text": "Another research direction is to detect and mitigate bias in ChatGPTs. Researchers can develop methods to identify and eliminate bias from Chatbots by using techniques such as adversarial training [55]. This method fine tunes language models and deep learning models in general through the introduction of adversarial samples in the training set, which tends to increase the robustness and generalization of the model. Toward this end, several approaches and algorithms have been proposed in the literature, including adversarial training for large neural language models (ALUM) [56], generative adversarial training [57], attacking to training (A2T) [58], and large-margin classification [59]. Bias detection and mitigation are important steps in ensuring that language models are used ethically and fairly. Here are some approaches to bias detection and mitigation: \n\n\u2022 Data collection: Bias can be introduced in the training data that is used to train ChatGPTs. One approach to reducing bias is to ensure that the training data is diverse and representative of the population that the model will be used on. This can involve careful selection of data sources and cleaning and preprocessing the data to remove any biases. \n\n\u2022 Bias metrics: Once the model is trained, it is important to measure the extent of any bias that may be present. This can be done using various bias metrics, such as the disparate impact or statistical parity difference. These metrics can help identify areas of the model that may be more prone to bias. \n\n\u2022 Mitigation strategies: Once bias has been identified, there are various strategies that can be used to mitigate it. One approach is to modify the training data to reduce bias, for example by oversampling underrepresented groups. Another approach is to modify the model itself, for example by adding constraints or penalties to the training process that encourage fairer predictions. Alternatively, post-processing techniques can be used to adjust the model's predictions to ensure fairness. \n\n\u2022 Regular monitoring: Bias detection and mitigation is an ongoing process, and it is important to regularly monitor the model for any new sources of bias that may emerge. This can involve setting up automated monitoring systems that flag potential bias in real time, as well as regular audits of the model's performance.",
            "score": 0.5162749429220064,
            "section_title": "Bias Detection and Mitigation",
            "char_start_offset": 39499,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 70
                },
                {
                    "start": 71,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 867
                },
                {
                    "start": 870,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1110
                },
                {
                    "start": 1111,
                    "end": 1223
                },
                {
                    "start": 1226,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1530
                },
                {
                    "start": 1533,
                    "end": 1650
                },
                {
                    "start": 1651,
                    "end": 1763
                },
                {
                    "start": 1764,
                    "end": 1917
                },
                {
                    "start": 1918,
                    "end": 2025
                },
                {
                    "start": 2028,
                    "end": 2198
                },
                {
                    "start": 2199,
                    "end": 2348
                }
            ],
            "ref_mentions": [
                {
                    "start": 197,
                    "end": 201,
                    "matchedPaperCorpusId": "251610175"
                },
                {
                    "start": 651,
                    "end": 655,
                    "matchedPaperCorpusId": "237385862"
                },
                {
                    "start": 689,
                    "end": 693,
                    "matchedPaperCorpusId": "174800362"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.775390625
        },
        {
            "corpus_id": "262043949",
            "title": "The Impact of Debiasing on the Performance of Language Models in Downstream Tasks is Underestimated",
            "text": "Unfortunately, Pre-trained Language Models (PLMs) such as BERT (Devlin et al., 2019) and * Danushka Bollegala holds concurrent appointments as a Professor at University of Liverpool and as an Amazon Scholar. This paper describes work performed at the University of Liverpool and is not associated with Amazon. Table 1: The total number of instances containing female, male, and occupational (Occ.) words in the GLUE development data. \n\nRoBERTa (Liu et al., 2019) easily learn discriminatory social biases expressed in human-written texts in massive datasets (Kurita et al., 2019;Zhou et al., 2022;Kaneko et al., 2022). For example, if a model is given \"[MASK] is a nurse.\" as the input, a gender biased PLM would predict \"She\" with a higer likelihood score than for \"He\" when filling the [MASK]. \n\nVarious debiasing methods have been proposed to mitigate social biases in PLMs. Zhao et al. (2019); Webster et al. (2020) proposed a debiasing method by swapping the gender of female and male words in the training data. Kaneko and Bollegala (2021) proposed a method for debiasing by orthogonalising the vectors representing gender information with the hidden layer of a language model given a sentence containing a stereotypical word. Webster et al. (2020) showed that dropout regularization can reduce overfitting to gender information, thereby can be used for debiasing PLMs. \n\nThe debiasing method should mitigate only discriminatory information, while pre-trained useful information should be retained in the model. Evaluations in downstream tasks often employ the GLEU benchmark (Wang et al., 2018), which measures the ability to understand language (Kaneko and Bollegala, 2021;Guo et al., 2022;Meade et al., 2022). The data for downstream tasks are not selected in terms of whether they reflect the impact of debiasing.",
            "score": 0.5161136386142483,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 433
                },
                {
                    "start": 436,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 795
                },
                {
                    "start": 798,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1375
                },
                {
                    "start": 1378,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1718
                },
                {
                    "start": 1719,
                    "end": 1823
                }
            ],
            "ref_mentions": [
                {
                    "start": 63,
                    "end": 84,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 558,
                    "end": 579,
                    "matchedPaperCorpusId": "190000105"
                },
                {
                    "start": 579,
                    "end": 597,
                    "matchedPaperCorpusId": "247450590"
                },
                {
                    "start": 878,
                    "end": 896,
                    "matchedPaperCorpusId": "102352962"
                },
                {
                    "start": 1018,
                    "end": 1045,
                    "matchedPaperCorpusId": "231698657"
                },
                {
                    "start": 1582,
                    "end": 1601,
                    "matchedPaperCorpusId": "5034059"
                },
                {
                    "start": 1653,
                    "end": 1681,
                    "matchedPaperCorpusId": "231698657"
                },
                {
                    "start": 1681,
                    "end": 1698,
                    "matchedPaperCorpusId": "248780440"
                },
                {
                    "start": 1698,
                    "end": 1717,
                    "matchedPaperCorpusId": "239015827"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7705078125
        },
        {
            "corpus_id": "249970811",
            "title": "Ethical and methodological challenges in building morally informed AI systems",
            "text": "All large language models, regardless of whether there is fine-tuning concerning moral decision-making, perpetuate word combinations that are learned from man-made texts. Obviously, these texts contain all sorts of biases, for instance, gender or racial stereotypes. In large language models, biases occur on various levels [29,30]: they are contained in embedding spaces, coreference resolutions, dialogue generation, hate-speech detection, sentiment analysis, machine translation, etc. And those biases can result in different types of harm, including allocation harms (resources or opportunities are distributed unequally among groups), stereotyping (negative generalizations), other representational harms, and questionable correlations. There are various tools, metrics, or frameworks for bias mitigation in all stages of AI development [31][32][33][34], though they are primarily used for algorithmic discrimination along categories surrounding race, gender, age, religion, sexual or political orientations, disability, and a few other demographic traits. More recent work in critical race theory, critical algorithms studies, and related fields has argued that the multidimensionality of these concepts means that we need alternative ways to operationalize demographic categories [35]. Morally informed AI systems inherit all of these same challenges. \n\nA further issue for morally informed systems is that all current bias mitigation measures are anthropocentric, and speciesist biases are ubiquitous in large language models [36,37]. Domains used for bias probing simply do not include non-anthropocentric categories, and as a result, attempts to debias morally informed large language models will nonetheless (likely) encode speciesist and other \"hidden\" biases. Text corpora biases that are deemed to be undesirable, such as those discussed in the previous paragraph, can potentially be counteracted by technical means. However, biases such as anthropocentrism are an unquestioned part of training data and so no efforts are undertaken to mitigate them, despite weighty ethical arguments suggesting such mitigations to be necessary. \n\nBiases enter the picture also on the testing side, as the performance of morally informed large language models is usually assessed against human moral intuitions as the primary benchmark.",
            "score": 0.5160631397871214,
            "section_title": "Bias problems",
            "char_start_offset": 15427,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1358
                },
                {
                    "start": 1361,
                    "end": 1542
                },
                {
                    "start": 1543,
                    "end": 1772
                },
                {
                    "start": 1773,
                    "end": 1930
                },
                {
                    "start": 1931,
                    "end": 2143
                },
                {
                    "start": 2146,
                    "end": 2334
                }
            ],
            "ref_mentions": [
                {
                    "start": 324,
                    "end": 328,
                    "matchedPaperCorpusId": "218971825"
                },
                {
                    "start": 328,
                    "end": 331,
                    "matchedPaperCorpusId": "207853290"
                },
                {
                    "start": 842,
                    "end": 846,
                    "matchedPaperCorpusId": "210156214"
                },
                {
                    "start": 854,
                    "end": 858,
                    "matchedPaperCorpusId": "33799296"
                },
                {
                    "start": 1287,
                    "end": 1291,
                    "matchedPaperCorpusId": "208921008"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61181640625
        },
        {
            "corpus_id": "227163037",
            "title": "The Geometry of Distributed Representations for Better Alignment, Attenuated Bias, and Improved Interpretability",
            "text": "The word \"bias\" may have several subtly different meanings. The notion of bias used in this dissertation predominantly refers to the association of stereotypical terms, words and perceptions with protected attributes of people, such as gender, race, ethnicity, age, etc.\n\nIt has been observed [15,50] that such societal biases creep into different machine learning tasks and applications in many ways, one of which are the datasets they are trained on. Language representations also face the same predicament [12,17,76], wherein the biases in underlying text get picked up and amplified by them, such as the differential association of occupations to genders (doctor : male and nurse : female). We explore this in more detail in subsequent chapters.\n\nA universally accepted and broad definition for what and how bias manifests as in language representations is not determined and is perhaps, one of the ultimate goals that this active area of research is trying to achieve [8]. In this dissertation, we define bias in language representation to be the invalid and stereotypical associations made by the representations about the aforementioned protected attributes.\n\nAn important consideration when understanding bias in language modeling is the cause of bias. Possible causes consist of the underlying training text that the embeddings are derived from, the modeling choices made, the training data and methods of the language modeling task itself and so on. A lot of different metrics and probes that try to evaluate bias, capture bias from one or more of these sources. While the values alone of these metrics' results hold little value on their own, when different attempts are made to mitigate biases by altering only specific sections of the language modeling process, a change in said scores can tell us about the impact of altering these sections on biases contained. In this dissertation, we focus on biases in language representations, their sources and mitigation and evaluation techniques.\n\nRepresentational harm [5], that we focus on, consists of stereotypes about different demographical and social groups. It is a reflection of the stereotypes that preexist in society that often get amplified [50,105] and perpetuated by resulting in unfair associations of attributes with groups of people. This bias is observed in language representations in terms of invalid vector distances and associations, unfair contextual or sentence associations or pernicious outcomes in tasks where these representations have been used",
            "score": 0.5157093429853752,
            "section_title": "Biases in Language Representations",
            "char_start_offset": 13204,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 293,
                    "end": 297,
                    "matchedPaperCorpusId": "803179"
                },
                {
                    "start": 297,
                    "end": 300,
                    "matchedPaperCorpusId": "3142952"
                },
                {
                    "start": 509,
                    "end": 513,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 513,
                    "end": 516,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 2025,
                    "end": 2028,
                    "matchedPaperCorpusId": "143133374"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65380859375
        },
        {
            "corpus_id": "236950797",
            "title": "Bias Out-of-the-Box: An Empirical Analysis of Intersectional Occupational Biases in Popular Generative Language Models",
            "text": "Bias in NLP models. Negative generalizations, stereotypes, or misrepresentations of particular social groups can be learned by generative language models. Extensive research has shown that unrestricted training of natural language models can inherit human biases and, in some cases, amplify them [11,13,19,43]. Previous papers have worked to identify, quantify, and de-bias context-independent word embeddings such as Word2Vec and GloVe [11,15,44]. Researchers have also attempted to quantify and mitigate biases in transformer-based generative language models, such as ELMo [44] and BERT [8,26]. Nadeem et al. [28] evaluate stereotypical bias in various language models, including XLNet and GPT-2, related to domains of gender, profession, race, and religion, although they do not consider the intersections of these domains. We extend these domains by additionally considering continental name origin and political affiliation, intersected with gender. \n\nProbing language models. Our work utilizes what Solaiman et al. [35] term 'bias probes' -a battery of inputs to large language models to produce certain outputs in order to map the biases learned by these models. The authors generate hundreds of samples for several potentially problematic prompts (i.e., \"Police describe the suspect as...\", \"The victim was...\") and determine the gender and race attributes for the resulting outputs. Most similar to our work, Sheng et al. [33] utilize prefix templates (i.e., \"X worked as...\", \"X had a job as...\"), including those related to occupation contexts. They generate 100 samples for each prefix template and demographic combination (gender intersected with ethnicity and sexuality) and analyze bias in GPT-2 by using sentiment score as a proxy for bias. We extend such work by conducting an empirical analysis of the sentence completions within the specific context of bias towards occupational associations. \n\nIn our paper, we focus on one sentence template to reduce variation in returned occupations while keeping sentence semantic structures fixed. Unlike [33], we do not introduce potentially noisy sentiment classification, instead directly analyzing the statistical distributions of returned occupations.",
            "score": 0.5156735952838851,
            "section_title": "Related Work",
            "char_start_offset": 3252,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 19
                },
                {
                    "start": 20,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 954
                },
                {
                    "start": 957,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1391
                },
                {
                    "start": 1392,
                    "end": 1555
                },
                {
                    "start": 1556,
                    "end": 1756
                },
                {
                    "start": 1757,
                    "end": 1911
                },
                {
                    "start": 1914,
                    "end": 2055
                },
                {
                    "start": 2056,
                    "end": 2214
                }
            ],
            "ref_mentions": [
                {
                    "start": 296,
                    "end": 300,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 300,
                    "end": 303,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 306,
                    "end": 309,
                    "matchedPaperCorpusId": "52161864"
                },
                {
                    "start": 437,
                    "end": 441,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 441,
                    "end": 444,
                    "matchedPaperCorpusId": "3272048"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55126953125
        },
        {
            "corpus_id": "256967110",
            "title": "Racial Equity in Healthcare Machine Learning: Illustrating Bias in Models With Minimal Bias Mitigation",
            "text": "Opportunities to mitigate potential biases exist at each step of the ML model development pipeline. During the pre-processing stage (before model training), one can reweight training data to increase representation, combine data sets to increase heterogeneity, or even remove race information from the data altogether [15]. During the in-processing stage (during model training), one can use techniques such as regularization or adversarial debiasing [15,16]. During the post-processing stage (after model training), one can calibrate their results or use varying cut-point selections to boost equity in performance [14]. Building ML models with no/minimal bias mitigation techniques can increase the risk of racial model performance disparities. \n\nWhile ML has seen a steep rise, evidence suggests that the adoption of bias mitigation has not kept pace. A recent meta-analysis showed that while many healthcare-related ML studies assess for racial bias, some of these studies do not correct for this bias [15]. Also, those that do attempt to correct racial bias may use a limited array of bias mitigation techniques. Additionally, only a small number of studies published their code for bias assessment or debiasing [15]. \n\nGiven this underwhelming attempt at bias mitigation in healthcare-related ML, in our study, we aim to evaluate the presence of racial bias when five of the most common ML algorithms are used to create models with minimal processing to reduce racial bias. We assessed the following five different ML methods: Extreme Gradient Boosting (XGBoost), random forest, naive Bayes, support vector machine (SVM), and k-nearest neighbors (KNN). Further, we a priori chose healthcare access -one of the most important drivers of health equity -as the outcome of prediction for model creation in our analyses.",
            "score": 0.5152545524156282,
            "section_title": "Introduction",
            "char_start_offset": 2020,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 100,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 746
                },
                {
                    "start": 749,
                    "end": 854
                },
                {
                    "start": 855,
                    "end": 1011
                },
                {
                    "start": 1012,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1222
                },
                {
                    "start": 1225,
                    "end": 1479
                },
                {
                    "start": 1480,
                    "end": 1658
                },
                {
                    "start": 1659,
                    "end": 1821
                }
            ],
            "ref_mentions": [
                {
                    "start": 318,
                    "end": 322,
                    "matchedPaperCorpusId": "247770384"
                },
                {
                    "start": 451,
                    "end": 455,
                    "matchedPaperCorpusId": "247770384"
                },
                {
                    "start": 455,
                    "end": 458,
                    "matchedPaperCorpusId": "9424845"
                },
                {
                    "start": 616,
                    "end": 620,
                    "matchedPaperCorpusId": "201666566"
                },
                {
                    "start": 1006,
                    "end": 1010,
                    "matchedPaperCorpusId": "247770384"
                },
                {
                    "start": 1217,
                    "end": 1221,
                    "matchedPaperCorpusId": "247770384"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52587890625
        },
        {
            "corpus_id": "253510264",
            "title": "Does Debiasing Inevitably Degrade the Model Performance",
            "text": "It is unclear whether the degradation of model performance is an inevitable consequence of debiasing approaches for correcting gender bias in pre-trained language models. Pre-trained language models have been highly successful in a variety of natural language processing tasks and are widely used in practice. However, several studies have revealed the presence of gender bias in pre-trained models Bolukbasi et al. [2016], Zhao et al. [2018], Bhardwaj et al. [2021], Vig et al. [2020]. For example, gender bias has been identified in the use of pre-trained models for online advertising Sweeney [2013], automatic resume filtering systems Cowgill [2018], Deshpande et al. [2020], and automatic criminal sentencing Dressel and Farid [2018]. This evidence of bias has raised concerns about the potential risks of using these models in real-world applications, such as recruitment and education. After Dastin [2018] noticed that the automatic resume filtering adopted by Amazon discriminated against female candidates, the system was abandoned. Thus, many studies have explored fine-tuning methods of mitigating gender biases in pre-trained language models. As a result, many researchers have explored fine-tuning methods for mitigating gender bias in pre-trained language models. However, recent discussions suggest that current debiasing approaches are associated with a decline in model performance Barikeri et al. [2021], Meade et al. [2022]. It is necessary to examine the possibility and method of easing the dilemma between model performance and gender bias. We discovered that using a method based on causal intervention can effectively reduce model bias and maintain performance. For transformer-structured language models, the source of gender bias is limited to errors in the pre-trained model's word embedding and transformer architectures. \n\nWe demonstrate that intervening in the explicable mechanisms of a language model is possible to reduce bias while preserving performance. Our findings indicate that errors in the word embedding can directly lead to gender bias, and that correcting these errors will not have a significant impact on the performance of the model. Based on this observation, we developed a debiasing method that is able to maintain performance.",
            "score": 0.5152441020817581,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 1041
                },
                {
                    "start": 1042,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1562
                },
                {
                    "start": 1563,
                    "end": 1685
                },
                {
                    "start": 1686,
                    "end": 1849
                },
                {
                    "start": 1852,
                    "end": 1989
                },
                {
                    "start": 1990,
                    "end": 2180
                },
                {
                    "start": 2181,
                    "end": 2277
                }
            ],
            "ref_mentions": [
                {
                    "start": 399,
                    "end": 422,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 424,
                    "end": 442,
                    "matchedPaperCorpusId": "52161864"
                },
                {
                    "start": 444,
                    "end": 466,
                    "matchedPaperCorpusId": "221586314"
                },
                {
                    "start": 588,
                    "end": 602,
                    "matchedPaperCorpusId": "23044927"
                },
                {
                    "start": 655,
                    "end": 678,
                    "matchedPaperCorpusId": "220040093"
                },
                {
                    "start": 714,
                    "end": 738,
                    "matchedPaperCorpusId": "13683547"
                },
                {
                    "start": 1399,
                    "end": 1421,
                    "matchedPaperCorpusId": "235358955"
                },
                {
                    "start": 1423,
                    "end": 1442,
                    "matchedPaperCorpusId": "239015827"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.443115234375
        },
        {
            "corpus_id": "256630990",
            "title": "Don't Just Clean It, Proxy Clean It: Mitigating Bias by Proxy in Pre-Trained Models",
            "text": "One of the most dominant paradigms in natural language processing is the fine-tuning of large language models for downstream tasks (Bommasani et al., 2021). Transformer-based language models pre-trained on massive volumes of text achieve state-of-the-art performance on a wide spectrum of tasks, when fine-tuned on task-specific data (Radford et al., 2018;Devlin et al., 2019). A major concern of this paradigm, however, is that pre-trained models can learn societal biases from both the pretraining and fine-tuning data. These biases can be expressed in two ways -in a) the contextualized representations of the model themselves, i.e., intrinsic biases (Nangia et al., 2020;Nadeem et al., 2021;May et al., 2019;Kurita et al., 2019), and b) its downstream predictions after fine-tuning, i.e., extrinsic or allocational harms (Gehman et al., 2020;Garimella et al., 2019;Blodgett et al., 2018). \n\nThere has been extensive research into mitigating bias in pre-trained language models (Meade et al., 2022). Prior work has primarily focused on debiasing contextual representations upstream, i.e., before the pre-trained model is fine-tuned (Liang et al., 2020;Kaneko and Bollegala, 2021;Ravfogel et al., 2020Ravfogel et al., , 2022)). However, recent work on the bias-transfer hypothesis 1 (Steed et al., 2022) reveals that most of the variation in downstream biases can be explained by biased associations in the fine-tuning dataset. This work also shows that variations in upstream bias have little impact on downstream disparities. These results emphasize the importance of debiasing interventions that target biases 2 introduced during fine-tuning from taskspecific data. \n\nHowever, Steed et al. (2022) also show that simply scrubbing identity terms 3 from the downstream/ fine-tuning data does not work for bias mitigation.",
            "score": 0.5152441020817581,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 892
                },
                {
                    "start": 895,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1229
                },
                {
                    "start": 1230,
                    "end": 1429
                },
                {
                    "start": 1430,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1670
                },
                {
                    "start": 1673,
                    "end": 1823
                }
            ],
            "ref_mentions": [
                {
                    "start": 356,
                    "end": 376,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 654,
                    "end": 675,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 675,
                    "end": 695,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 695,
                    "end": 712,
                    "matchedPaperCorpusId": "85518027"
                },
                {
                    "start": 712,
                    "end": 732,
                    "matchedPaperCorpusId": "190000105"
                },
                {
                    "start": 825,
                    "end": 846,
                    "matchedPaperCorpusId": "221878771"
                },
                {
                    "start": 846,
                    "end": 869,
                    "matchedPaperCorpusId": "196181460"
                },
                {
                    "start": 869,
                    "end": 891,
                    "matchedPaperCorpusId": "51870345"
                },
                {
                    "start": 981,
                    "end": 1001,
                    "matchedPaperCorpusId": "239015827"
                },
                {
                    "start": 1135,
                    "end": 1155,
                    "matchedPaperCorpusId": "207996257"
                },
                {
                    "start": 1182,
                    "end": 1203,
                    "matchedPaperCorpusId": "215786522"
                },
                {
                    "start": 1203,
                    "end": 1228,
                    "matchedPaperCorpusId": "246411428"
                },
                {
                    "start": 1285,
                    "end": 1305,
                    "matchedPaperCorpusId": "248780439"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63720703125
        },
        {
            "corpus_id": "267751419",
            "title": "Disclosure and Mitigation of Gender Bias in LLMs",
            "text": "We further propose a QLoRA-based Debias Tuning method to mitigate bias. The key idea of Debias Tuning is to bring the female attribute distribution and male attribute distribution closer while reducing the probability of gender attribute words. Firstly, we propose a Distribution Distance Loss \n\n, which is similar to the ADD metric, aiming to bring the gender distributions closer. Secondly, we propose a Gender Probability Loss to prevent the model from balancing the two gender distributions by increasing the probabilities of the opposite gender, which may amplify gender bias. Gender Probability Loss, which consists of the female attribute word probabilities and male attribute word probabilities, is denoted as \n\nThirdly, we propose an Attribute Logits Difference Loss \n\n, which is similar to the GLD metric, targeting minimizing the normalized probability difference between female pronouns and male pronouns. Debias Tuning freezes the LLMs parameters and optimizes QLoRA parameters by minimizing the total loss L = L d + L g + L l to mitigate bias. 4 he training data is the combination of data derived from the STS-B and SNLI training sets using the same strategy in \u00a72.2. Table 1 presents the direct evaluation outcomes on the Naturally-sourced (STS-B) and (SNLI). It is evident that Debias Tuning effectively mitigates both explicit and implicit bias across all evaluation metrics, datasets, and LLAMA2 models. \n\nTransferability. To assess the transferability and robustness of Debias Tuning on different probing inputs, we conduct evaluations on LLM-generated inputs. The results in Table 2 demonstrate that Debias Tuning consistently outperforms other mitigation techniques in terms of mitigation performance across all evaluation metrics, datasets, and LLAMA2 models. Thus, Debias Tuning mitigates both explicit and implicit bias and achieves the best mitigation performance for both LLMgenerated and Naturally-sourced datasets. We also present case analyses demonstrating the performance of different debiasing methods detailed in Appendix B.",
            "score": 0.514985220135122,
            "section_title": "Debias via Debias Tuning",
            "char_start_offset": 17659,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 71
                },
                {
                    "start": 72,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 293
                },
                {
                    "start": 296,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 717
                },
                {
                    "start": 720,
                    "end": 775
                },
                {
                    "start": 778,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1422
                },
                {
                    "start": 1425,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1580
                },
                {
                    "start": 1581,
                    "end": 1782
                },
                {
                    "start": 1783,
                    "end": 1943
                },
                {
                    "start": 1944,
                    "end": 2058
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6767578125
        },
        {
            "corpus_id": "252129484",
            "title": "A review of causality-based fairness machine learning",
            "text": "As such, the word embedding generated by this method can strike a balance between gender debiasing and semantic information preserving. Yang and Feng [79] presented a causality-based post-processing approach for eliminating the gender bias in word embeddings. Specifically, their method was based on statistical correlation and half-sibling regression, which leverages the statistical dependency between gender-biased word vectors and gender-definition word vectors to learn the counterfactual gender information of an individual through causal inference. The learned spurious gender information is then subtracted from the gender-biased word vectors to remove the gender bias. Lu et al. [80] proposed a method called CDA to eliminate gender bias through counterfactual data augmentation. The main idea of CDA is to augment the corpus by exchanging gender word pairs in the corpus and constructing matching gender word pairs with causal interventions. As such, CDA breaks associations between gendered and gender-neutral words and alleviates the problem that gender bias increases as loss decreases when training with gradient descent. \n\nThere exists a certain degree of bias and fairness issues in word embedding, machine translation, sentiment analysis, language models, and dialog generation in NLP. At present, most studies only focus on a single bias (such as gender bias), and there is a lack of research results on other biases or eliminating multiple biases at the same time. Therefore, how should we analyze and evaluate the mechanism and impact of multi-bias in word embedding and machine learning algorithms? Establishing effective techniques for eliminating various biases in word embedding and machine learning algorithms requires further research which needs to be carried out for fair NLP.",
            "score": 0.5149632657768765,
            "section_title": "Fair natural language processing",
            "char_start_offset": 51525,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1135
                },
                {
                    "start": 1138,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1619
                },
                {
                    "start": 1620,
                    "end": 1804
                }
            ],
            "ref_mentions": [
                {
                    "start": 150,
                    "end": 154,
                    "matchedPaperCorpusId": "208267842"
                },
                {
                    "start": 688,
                    "end": 692,
                    "matchedPaperCorpusId": "51888520"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.869140625
        },
        {
            "corpus_id": "231802467",
            "title": "Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models",
            "text": "GPT-3 exhibits several racial, gender, and religious biases. One discussant analogized the difficulty of addressing language model bias to the problem of content moderation on online platforms -despite the difficult normative issues in both cases, there are still some areas of relative consensus and opportunities for mitigation. For example, online platforms agree on the need to address child pornography or egregious threats of violence, and the concept of \"protected classes\" in discrimination law provides a useful initial framework for thinking about some language model biases. \n\nSeveral workshop participants noted that it is difficult to define what it means to mitigate bias in large language models in a universal manner, since appropriate language use is highly contextual. One participant noted that all datasets are biased in some ways, so the challenge is not eliminating all bias but addressing harmful biases according to some set of normative and/or legal criteria. Some suggested that companies like OpenAI do not have the appropriate standing and should not aim to make such decisions on behalf of society. Someone else observed that it is especially difficult to think about mitigating bias for multipurpose systems like GPT-3 via changes to their training data, since bias is typically analyzed in the context of a particular use cases. \n\nParticipants discussed a wide variety of possible means of addressing harmful biases in language models, including: \n\n\u2022 Changes to the initial training data to mitigate bias a priori \n\n\u2022 Training a separate model to filter content generated by a language model \n\n\u2022 Fine-tuning a large language model on data with desired properties \n\n\u2022 Tagging data so that the model learns to distinguish among certain forms of content (see e.g. CTRL) \n\n\u2022 Training models to be more \"fact-aware\" \n\n\u2022 Reinforcement learning with human feedback \n\n\u2022 Leveraging the model's own knowledge to improve outputs (e.g., with careful prompt design) \n\n\u2022 Developing more expansive suites of \"bias tests\" that models can be run through prior to deployment \n\n\u2022 Red-teaming the model at scale by engaging trusted partners to work with the model and through limited commercial offerings. \n\nNone of these approaches was considered a panacea.",
            "score": 0.5149283722494576,
            "section_title": "Bias",
            "char_start_offset": 11783,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 60
                },
                {
                    "start": 61,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 585
                },
                {
                    "start": 588,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1127
                },
                {
                    "start": 1128,
                    "end": 1359
                },
                {
                    "start": 1362,
                    "end": 1477
                },
                {
                    "start": 1480,
                    "end": 1544
                },
                {
                    "start": 1547,
                    "end": 1622
                },
                {
                    "start": 1625,
                    "end": 1693
                },
                {
                    "start": 1696,
                    "end": 1791
                },
                {
                    "start": 1792,
                    "end": 1797
                },
                {
                    "start": 1800,
                    "end": 1841
                },
                {
                    "start": 1844,
                    "end": 1888
                },
                {
                    "start": 1891,
                    "end": 1983
                },
                {
                    "start": 1986,
                    "end": 2087
                },
                {
                    "start": 2090,
                    "end": 2216
                },
                {
                    "start": 2219,
                    "end": 2269
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72265625
        },
        {
            "corpus_id": "258378241",
            "title": "When Do Pre-Training Biases Propagate to Downstream Tasks? A Case Study in Text Summarization",
            "text": "of using additional information to mitigate bias. These approaches include designing data augmentation methods (Zhao et al., 2018a;Lee et al., 2017Lee et al., , 2018Zhao et al., 2018b;Park et al., 2018), tagging training data with gender labels (Prates et al., 2018;Vanmassenhove et al., 2018), debiasing word embeddings (Bolukbasi et al., 2016;Zhao et al., 2018b), and explicitly balancing gender ratios in model predictions (Zhao et al., 2017). Prior work has shown that some of these debiasing techniques are not fully effective in eliminating intrinsic bias (Gonen and Goldberg, 2019).\n\nIn contrast to this line of work, we specifically aim to understand the effect of different adaptation methods on bias propagation. Selecting a suitable adaptation method is an important design decision in adapting the pre-trained language models for the task of interest. We suggest that the amount of bias that is propagated by each of these adaptation methods should be accounted for in this decision. For example, we find that simply adapting a smaller set of parameters (e.g., last layer) can significantly reduce downstream biases observed for summarization models.",
            "score": 0.5137668069403647,
            "section_title": "Measuring Bias in NLP Models.",
            "char_start_offset": 21544,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 111,
                    "end": 131,
                    "matchedPaperCorpusId": "4952494"
                },
                {
                    "start": 131,
                    "end": 147,
                    "matchedPaperCorpusId": "1222212"
                },
                {
                    "start": 147,
                    "end": 165,
                    "matchedPaperCorpusId": "4891749"
                },
                {
                    "start": 165,
                    "end": 184,
                    "matchedPaperCorpusId": "52161864"
                },
                {
                    "start": 184,
                    "end": 202,
                    "matchedPaperCorpusId": "52070035"
                },
                {
                    "start": 266,
                    "end": 293,
                    "matchedPaperCorpusId": "76654723"
                },
                {
                    "start": 321,
                    "end": 345,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 345,
                    "end": 364,
                    "matchedPaperCorpusId": "52161864"
                },
                {
                    "start": 426,
                    "end": 445,
                    "matchedPaperCorpusId": "1389483"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8203125
        },
        {
            "corpus_id": "276647994",
            "title": "Beneath the Surface: How Large Language Models Reflect Hidden Bias",
            "text": "The remarkable performance of Large Language Models (LLMs) is frequently accompanied by the propagation of social bias inherent in their training data (Gallegos et al., 2024a;Hofmann et al., 2024;Navigli et al., 2023;Cui et al., 2024). These biases raise serious ethical concerns, as they perpetuate stereotypes, reinforce discrimination, and negatively impact real-world decision-making. In domains such as hiring, law enforcement, and content moderation, the use of these models in realworld applications may disproportionately harm marginalized individuals and communities (Parrish et al., 2022;Nangia et al., 2020;Nadeem et al., 2021;Marchiori Manerba et al., 2024;Bi et al., 2023;del Arco et al., 2024;Kotek et al., 2023). \n\nNumerous studies (Parrish et al., 2022;Marchiori Manerba et al., 2024;Nangia et al., 2020;Nadeem et al., 2021) benchmark Overt Bias in LLMs by analyzing direct associations between a specific demographic term and a bias-related concept term. As illustrated in Figure 1, example (a) from BBQ (Parrish et al., 2022) can demonstrate overt bias when the model consistently associates \"Margaret\" (female) with the term \"bad at math\" and \"George\" (male) with the term \"good at math\", or vice versa. However, a fundamental issue remains: overt bias can be simply mitigated by breaking the direct association between demographic terms and concept terms (Gallegos et al., 2024b;Li et al., 2024). Additionally, as LLMs evolve, their responses to overt bias evaluations have become more neutral and self-regulated, frequently aligning with socially desirable norms. This trend is largely driven by advances in model training techniques, particularly instruction tuning and alignment strategies, which encourage neutrality in responses to overtly biased contexts (Ouyang et al., 2022;Zhang et al., 2023;Peng et al., 2023;Ji et al., 2024).",
            "score": 0.5137132881391341,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 727
                },
                {
                    "start": 730,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1222
                },
                {
                    "start": 1223,
                    "end": 1416
                },
                {
                    "start": 1417,
                    "end": 1584
                },
                {
                    "start": 1585,
                    "end": 1856
                }
            ],
            "ref_mentions": [
                {
                    "start": 151,
                    "end": 175,
                    "matchedPaperCorpusId": "261530629"
                },
                {
                    "start": 175,
                    "end": 196,
                    "matchedPaperCorpusId": "272214842"
                },
                {
                    "start": 196,
                    "end": 217,
                    "matchedPaperCorpusId": "258688053"
                },
                {
                    "start": 576,
                    "end": 598,
                    "matchedPaperCorpusId": "239010011"
                },
                {
                    "start": 598,
                    "end": 618,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 618,
                    "end": 638,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 638,
                    "end": 669,
                    "matchedPaperCorpusId": "265212726"
                },
                {
                    "start": 707,
                    "end": 726,
                    "matchedPaperCorpusId": "261276445"
                },
                {
                    "start": 747,
                    "end": 769,
                    "matchedPaperCorpusId": "239010011"
                },
                {
                    "start": 769,
                    "end": 800,
                    "matchedPaperCorpusId": "265212726"
                },
                {
                    "start": 800,
                    "end": 820,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 820,
                    "end": 840,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 1021,
                    "end": 1043,
                    "matchedPaperCorpusId": "239010011"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.38134765625
        },
        {
            "corpus_id": "272880907",
            "title": "Do the Right Thing, Just Debias! Multi-Category Bias Mitigation Using LLMs",
            "text": "Bias in language permeates our daily interactions (Hammersley and Gomm, 1997). Recognizing its expression in language is crucial for effectively reducing its impact. Consider, for example, a news headline from Breitbart News: \"CE Arrests 680 Illegal Aliens in Largest Single-State Raid in U.S. History\" + , which clearly demonstrates how bias can be present, whether through malice or unintentional habits. This also highlights how different word choices can convey distinct perspectives and potentially reinforce existing social biases. For instance, using the term \"illegal aliens\" instead of \"undocumented immigrants\" can convey a more negative stance towards immigrants. This underscores the need for automatic bias correction (mitigation), which involves transforming a source sentence S into a neutral sentence T that is clear, objective, and free of stereotypes while preserving the original meaning or semantics. \n\nWhile numerous studies (Recasens et al., 2013;Bhosale et al., 2013;Hube and Fetahu, 2018;Zhong, 2021;Pryzant et al., 2019;Madanagopal and Caverlee, 2022) have been conducted in this area, they face significant challenges. Most approaches rely on Seq2Seq models trained on the Wikipedia-derived Neutrality Corpus (WNC). However, as highlighted in (Madanagopal and Caverlee, 2023), WNC is not a perfect bias-debias parallel corpus (see Appendix Section A for more details). It often contains examples unrelated to bias mitigation, focusing on content, grammatical, or typographical corrections. Consequently, many human-corrected neutral sentences still exhibit subjective bias (see Table 1). Training models on such datasets can lead to inconsistent performance in terms of bias mitigation. Furthermore, most datasets (see Table 3) lack comprehensive coverage of various social bias categories. This motivates the development of a new parallel corpus for biased and debiased sentences, ANUBIS, encompassing a wide range of bias categories: race, gender/gender identity, sexual orientation, religion, age, nationality, disability, physical appearance, and socioeconomic status. Section 3 provides a detailed discussion on the preparation strategy for ANUBIS.",
            "score": 0.5136699078116235,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 79,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 920
                },
                {
                    "start": 923,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1241
                },
                {
                    "start": 1242,
                    "end": 1394
                },
                {
                    "start": 1395,
                    "end": 1515
                },
                {
                    "start": 1516,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1712
                },
                {
                    "start": 1713,
                    "end": 1816
                },
                {
                    "start": 1817,
                    "end": 2098
                },
                {
                    "start": 2099,
                    "end": 2179
                }
            ],
            "ref_mentions": [
                {
                    "start": 50,
                    "end": 77,
                    "matchedPaperCorpusId": "145519342"
                },
                {
                    "start": 946,
                    "end": 969,
                    "matchedPaperCorpusId": "2772094"
                },
                {
                    "start": 969,
                    "end": 990,
                    "matchedPaperCorpusId": "5594311"
                },
                {
                    "start": 990,
                    "end": 1012,
                    "matchedPaperCorpusId": "13754084"
                },
                {
                    "start": 1045,
                    "end": 1076,
                    "matchedPaperCorpusId": "248360263"
                },
                {
                    "start": 1269,
                    "end": 1301,
                    "matchedPaperCorpusId": "258378162"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.377685546875
        },
        {
            "corpus_id": "202537733",
            "title": "Examining Gender Bias in Languages with Grammatical Gender",
            "text": "Previous work has proposed to quantify bias in English Embedding definitions for gender bias in English word embeddings. Besides the aforementioned studies (Bolukbasi et al., 2016;Caliskan et al., 2017) in quantifying bias in English embedding, McCurdy and Serbeti (2017) examine grammatical gender bias in word embeddings of gendered languages by computing the WEAT association score (Caliskan et al., 2017) between gendered object nouns (e.g. moon-sun) and genderdefinition words. They propose to mitigate bias by applying lemmatization to remove gender information from the training corpus. However, their focus is different from us as our approaches aim at keeping the grammatical gender information and only removing the bias in semantic genders. A few recent studies focus on measuring and reducing gender bias in contextualized word embeddings (Zhao et al., 2019;May et al., 2019;Basta et al., 2019). However, they only focus on English embeddings in which the gender is mostly only expressed by pronouns (Stahlberg et al., 2007). An interesting future direction is to extend their analyses to language with grammatical gender. Regarding bias mitigation appraoches, Zhao et al. (2018b) mitigate bias by saving one dimension of the word vector for gender.  propose a regularization loss term for wordlevel language models. Zhang et al. (2018) use an adversarial network to mitigate bias in word embeddings. All these approaches consider only English embeddings. Moreover, Gonen and Goldberg (2019) show that mitigation methods based on gender directions are not sufficient, since the embeddings of socially-biased words still cluster together.\n\nBias in word embedding may affect the downstream applications (Zhao et al., 2018a;Rudinger et al., 2018;Font and Costa-juss\u00e0, 2019). Besides the gender bias in word embeddings, implicit stereotypes have been shown in other real world applications, such as online reviews (Wallace and Paul, 2016), advertisement (Sweeney, 2013) and web search (Kay et al., 2015).",
            "score": 0.5133566698728248,
            "section_title": "Related Work",
            "char_start_offset": 23163,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 156,
                    "end": 180,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 180,
                    "end": 202,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 385,
                    "end": 408,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 851,
                    "end": 870,
                    "matchedPaperCorpusId": "102352962"
                },
                {
                    "start": 870,
                    "end": 887,
                    "matchedPaperCorpusId": "85518027"
                },
                {
                    "start": 1173,
                    "end": 1192,
                    "matchedPaperCorpusId": "52161864"
                },
                {
                    "start": 1329,
                    "end": 1348,
                    "matchedPaperCorpusId": "9424845"
                },
                {
                    "start": 1478,
                    "end": 1503,
                    "matchedPaperCorpusId": "73729169"
                },
                {
                    "start": 1713,
                    "end": 1733,
                    "matchedPaperCorpusId": "4952494"
                },
                {
                    "start": 1733,
                    "end": 1755,
                    "matchedPaperCorpusId": "13756572"
                },
                {
                    "start": 1993,
                    "end": 2011,
                    "matchedPaperCorpusId": "8832874"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.339111328125
        },
        {
            "corpus_id": "266550954",
            "title": "Fairness-Aware Structured Pruning in Transformers",
            "text": "Numerous pruning methods have highlighted that certain attention heads are critical for maintaining language modeling ability, while others appear superfluous to model performance (Voita et al. 2019;Michel, Levy, and Neubig 2019;He and Choi 2021;Bian et al. 2021). Some studies have shown that these important heads play an interpretable role in downstream tasks (Wang et al. 2022;Voita et al. 2019;He and Choi 2021). In our work, we explore the possibility of extending this concept to fairness by identifying attention heads that are responsible for promoting bias. To achieve this, we compute separate scores to quantify the contribution of each attention head toward both performance and bias. These scores serve as our guide in selectively removing attention heads to improve fairness with minimal performance loss. Put simply, we propose to prioritize pruning the heads that contribute the most to bias, given that they are not crucial for language modeling. Our contributions in this paper can be summarized as follows: \n\n1. We investigate the impact of existing head pruning methods on bias across different language models, demonstrating that they do not enhance model fairness. 2. We quantify the effect of removing attention heads on bias in language models, and use it as a proxy for their contribution to the model's overall bias. 3. We propose a novel structured pruning method that considers both fairness and performance. Our method avoids pruning the heads that are important for language mod-eling, while prioritizing pruning the heads which negatively impact fairness. 4. We conduct a comparison between our method and existing pruning techniques, revealing its superiority in terms of fairness, while matching, and sometimes surpassing, their performance in terms of language modeling. 5. Using LLMs of different sizes, we examine how our bias reduction method, when applied to gender bias, impacts biases pertaining to religion, race, sexual orientation, and nationality. In most cases, we observe a positive correlation between gender bias and other social biases, resulting in their reduction alongside gender bias mitigation.",
            "score": 0.5131212714932286,
            "section_title": "Introduction",
            "char_start_offset": 1815,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1026
                },
                {
                    "start": 1029,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1343
                },
                {
                    "start": 1344,
                    "end": 1437
                },
                {
                    "start": 1438,
                    "end": 1587
                },
                {
                    "start": 1588,
                    "end": 1805
                },
                {
                    "start": 1806,
                    "end": 1992
                },
                {
                    "start": 1993,
                    "end": 2149
                }
            ],
            "ref_mentions": [
                {
                    "start": 180,
                    "end": 199,
                    "matchedPaperCorpusId": "162183964"
                },
                {
                    "start": 199,
                    "end": 229,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 229,
                    "end": 246,
                    "matchedPaperCorpusId": "237513485"
                },
                {
                    "start": 246,
                    "end": 263,
                    "matchedPaperCorpusId": "235097467"
                },
                {
                    "start": 363,
                    "end": 381,
                    "matchedPaperCorpusId": "253244237"
                },
                {
                    "start": 381,
                    "end": 399,
                    "matchedPaperCorpusId": "162183964"
                },
                {
                    "start": 399,
                    "end": 416,
                    "matchedPaperCorpusId": "237513485"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.771484375
        },
        {
            "corpus_id": "253762006",
            "title": "Demographic-Aware Language Model Fine-tuning as a Bias Mitigation Technique",
            "text": "BERT-like language models (LMs), when exposed to large unstructured datasets, are known to learn and sometimes even amplify the biases present in such data. These biases generally reflect social stereotypes with respect to gender, race, age, and others. In this paper, we analyze the variations in gender and racial biases in BERT, a large pre-trained LM, when exposed to different demographic groups. Specifically, we investigate the effect of fine-tuning BERT on text authored by historically disadvantaged demographic groups in comparison to that by advantaged groups. We show that simply by fine-tuning BERT-like LMs on text authored by certain demographic groups can result in the mitigation of social biases in these LMs against various target groups.",
            "score": 0.5121247568369862,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96240234375
        },
        {
            "corpus_id": "249282716",
            "title": "What changed? Investigating Debiasing Methods using Causal Mediation Analysis",
            "text": "From our experiments, we learned the following things about debiasing techniques and their impact on language models: \n\nIt is recommendable to test the efficacy of debiasing techniques on more than one bias metric. Our results suggest that debiasing methods show effectiveness when measured on some bias measurements. However, this efficacy varies depending on which bias metrics are used to measure the bias of language models. This may due to different definitions and operationalizations of bias in these metrics, which result in varying degree of effectiveness. This suggests that in order to make claims about the generalizability of the effectiveness of debiasing methods, these methods need to be tested on more than one bias metrics. \n\nThe impact of debiasing concentrates on certain components of language models. The results from the causal mediation analysis suggest that the neurons located in the first two layers (including the word embedding layers) showed the biggest difference in debiased and fine-tuned models when compared to the baseline model. This suggests two things. First, the detrimental associations between words that cause gender bias in language models may originally be situated in those layers. Second, the role of those layers may be crucial in mitigating gender biases in language models. We recommend future work to focus on those components. \n\nDebiasing and fine-tuning methods change the behaviors of attention heads. Our results show that applying debiasing and fine-tuning methods to language models changes the weight that attention heads assign to gender-associated terms. This indicates that attention heads may play a crucial role in representing gender bias in language models. \n\nIn summary, our findings suggest that debiasing methods can be effective in reducing gender bias in language models, but the degree of this effectiveness depends on how debiasing success is assessed upon. Also, the results of the causal mediation analysis suggest that impact of debiasing is concentrated in certain components of the language models. Overall, our findings suggest a need to test the effectiveness of debiasing methods with different bias metrics, and to focus on changes in the behavior of certain components of the models.",
            "score": 0.5120207051412012,
            "section_title": "Contributions",
            "char_start_offset": 2755,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 120,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 741
                },
                {
                    "start": 744,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1227
                },
                {
                    "start": 1228,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1378
                },
                {
                    "start": 1381,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1614
                },
                {
                    "start": 1615,
                    "end": 1722
                },
                {
                    "start": 1725,
                    "end": 1929
                },
                {
                    "start": 1930,
                    "end": 2075
                },
                {
                    "start": 2076,
                    "end": 2265
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8291015625
        },
        {
            "corpus_id": "264305744",
            "title": "Co$^2$PT: Mitigating Bias in Pre-trained Language Models through Counterfactual Contrastive Prompt Tuning",
            "text": "We propose Co 2 PT, an efficient and effective debiasing method for mitigating bias in downstream tasks. We evaluate its effectiveness on bias mitigation and applicability to existing debiased upstream models, and investigate how the design of each component and the selection of hyperparameters impact both its bias reduction capabilities and downstream task performance. \n\nMitigating non-gender and intersectional bias. \n\nMitigating non-gender biases is challenging as some debiasing methods work well on reducing gender biases but show poor generalization capabilities in addressing biases beyond gender (Meade et al., 2022). Without re-training the model, Co 2 PT is flexible to apply in order to mitigate different bias types in downstream applications. One can train different debiasing prompts to tackle different bias dimensions such as gender, race, and religion. Furthermore, these debiasing prompts can be applied to mitigate intersectional bias by simply combining the corresponding prompts in downstream tasks.",
            "score": 0.5118659949069039,
            "section_title": "Conclusion and Future Work",
            "char_start_offset": 26498,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 372
                },
                {
                    "start": 375,
                    "end": 421
                },
                {
                    "start": 424,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 1023
                }
            ],
            "ref_mentions": [
                {
                    "start": 607,
                    "end": 627,
                    "matchedPaperCorpusId": "239015827"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89501953125
        },
        {
            "corpus_id": "277467778",
            "title": "Detecting and Mitigating Bias in LLMs through Knowledge Graph-Augmented Training",
            "text": "Bias detection is generally the first step in mitigating the negative effects of bias in LLMs. A number of techniques have been developed by researchers to identify and quantify various biases within such models. The most popular among these techniques is probably the intrinsic evaluation metrics, which include word embedding association tests, WEAT for short [3], that quantify associations of different demographic groups with stereotypical attributes. These tests quantify the bias by comparing the similarity between biased and unbiased word pairs. \n\nAnother key contribution concerns the development of context-aware bias detection methods. Zhao et al. [19] introduced gender bias detection methods which consider the contextual use of words, extending static association tests to capture the biases in dynamic language scenarios. This enhances the sensitivity of the detection because the method takes into account the nuances of the contexts, which might be overlooked by a static approach. \n\nMoreover, analysis of data is also considered a keystone in bias detection. Sheng et al. [20] extensively analyzed the datasets on which LLMs were trained and found that there is evidence of gender and racial bias. Their effort indicates that training data analysis is the principal source of bias and, therefore, requires more balanced and representative datasets to ensure fairness in AI models.",
            "score": 0.5118025027235568,
            "section_title": "A. Bias Detection in Large Language Models",
            "char_start_offset": 6390,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 554
                },
                {
                    "start": 557,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 837
                },
                {
                    "start": 838,
                    "end": 999
                },
                {
                    "start": 1002,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1216
                },
                {
                    "start": 1217,
                    "end": 1399
                }
            ],
            "ref_mentions": [
                {
                    "start": 362,
                    "end": 365,
                    "matchedPaperCorpusId": "23163324"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82275390625
        },
        {
            "corpus_id": "266550954",
            "title": "Fairness-Aware Structured Pruning in Transformers",
            "text": "The increasing size of large language models (LLMs) has introduced challenges in their training and inference. Removing model components is perceived as a solution to tackle the large model sizes, however, existing pruning methods solely focus on performance, without considering an essential aspect for the responsible use of LLMs: model fairness. It is crucial to address the fairness of LLMs towards diverse groups, such as women, Black people, LGBTQ+, Jewish communities, among others, as they are being deployed and available to a wide audience. In this work, first, we investigate how attention heads impact fairness and performance in pre-trained transformer-based language models. We then propose a novel method to prune the attention heads that negatively impact fairness while retaining the heads critical for performance, i.e. language modeling capabilities. Our approach is practical in terms of time and resources, as it does not require fine-tuning the final pruned, and fairer, model. Our findings demonstrate a reduction in gender bias by 19%, 19.5%, 39.5%, 34.7%, 23%, and 8% for DistilGPT-2, GPT-2, GPT-Neo of two different sizes, GPT-J, and Llama 2 models, respectively, in comparison to the biased model, with only a slight decrease in performance. WARNING: This work uses language that is offensive in nature.",
            "score": 0.5115676371870865,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86083984375
        },
        {
            "corpus_id": "268876143",
            "title": "Manipulating and Mitigating Generative Model Biases Without Retraining",
            "text": "Bias mitigation has been a pivotal point of discussion, particularly in use cases where marginalized groups are involved. While the complete mitigation of all biases is extremely difficult, related works show that steps can be taken to balance representations [4,10,12]. We propose exploiting multiple clusters in the embedding space to improve class representations in T2I model outputs without retraining the model, focusing on social biases related to race, gender 4 and age. \n\nThrough Definition 6, we introduced the notion that a vast number of unknown clusters may exist in the language model embedding space. Exploiting multiple clusters in E n\u00d7m would allow us to control various attributes of a target model and manipulate class representations. The wide embedding space would be home to {c 1 , c i , ..., c N } labelled points/centroids, where the discrete value of N is difficult to define due to the complexities of natural language, compounded by the size of E n\u00d7m . As denoted by Eq. ( 4), transformations can be applied in N directions, where S i = 0 indicates no manipulation in the i th direction. To exploit this multi-cluster tuning method to mitigate social biases, we consider (i ) gender (manc 1 , womanc 2 ), (ii ) age (youngc 3 , oldc 4 ) and, (iii ) racial (whitec 5 , blackc 6 , asianc 7 ) labelled points and generate embedding space clusters c 1 \u2192 c 7 . We extract prompts containing the token 'person' from the COCO, Flickr30K and GCC datasets. To construct genderlabelled embedding clusters we replace 'person' in the prompt with either 'man' or 'woman', extract the embeddings and define the centroids c 1 , c 2 \u2208 E n\u00d7m . For age-and race-labelled clusters, we prepend the label to 'person' in the prompt, defining centroids c 3 \u2192 c 7 in a similar fashion.",
            "score": 0.5115066304842336,
            "section_title": "Bias Manipulation to Balance Social Biases",
            "char_start_offset": 19517,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 478
                },
                {
                    "start": 481,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1473
                },
                {
                    "start": 1474,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1787
                }
            ],
            "ref_mentions": [
                {
                    "start": 260,
                    "end": 263,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 263,
                    "end": 266,
                    "matchedPaperCorpusId": "268418167"
                },
                {
                    "start": 266,
                    "end": 269,
                    "matchedPaperCorpusId": "247981991"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.39404296875
        },
        {
            "corpus_id": "267411833",
            "title": "Self-Debiasing Large Language Models: Zero-Shot Recognition and Reduction of Stereotypes",
            "text": "The literature on bias mitigations for LLMs covers a broad range of pre-processing, in-training, and post-processing methods. Many of these techniques, however, leverage augmented training data (Garimella et al., 2022;Ghanbarzadeh et al., 2023;Lu et al., 2020;Panda et al., 2022;Qian et al., 2022;Webster et al., 2020;Zayed et al., 2023;Zmigrod et al., 2019), additional fine-tuning (Attanasio et al., 2022;Cheng et al., 2021;Gaci et al., 2022;Garimella et al., 2021;Guo et al., 2022;He et al., 2022b,a;Jia et al., 2020;Kaneko and Bollegala, 2021;Liu et al., 2020;Oh et al., 2022;Park et al., 2023;Qian et al., 2019;Woo et al., 2023;Yu et al., 2023;Zheng et al., 2023), modified decoding algorithms (Dathathri et al., 2019;Gehman et al., 2020;Krause et al., 2021;Liu et al., 2021;Meade et al., 2023;Saunders et al., 2022;Sheng et al., 2021a), or auxiliary post-processing models (Dhingra et al., 2023;Jain et al., 2021;Majumder et al., 2022;Sun et al., 2021;Tokpo and Calders, 2022;Vanmassenhove et al., 2021), which can be computationally expensive or require access to trainable model parameters, while often only addressing a single dimension of bias like gender or race. \n\nAs part of the bias mitigation literature, Schick et al. (2021) first coined the term self-debiasing in a demonstration that LLMs can self-diagnose their biases. In a white-box approach, they reduce bias via a modified decoding algorithm based on the model's own description of the undesirable behavior.",
            "score": 0.5113463511523999,
            "section_title": "Related Work",
            "char_start_offset": 3026,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 1174
                },
                {
                    "start": 1177,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1480
                }
            ],
            "ref_mentions": [
                {
                    "start": 194,
                    "end": 218,
                    "matchedPaperCorpusId": "253762006"
                },
                {
                    "start": 218,
                    "end": 244,
                    "matchedPaperCorpusId": "259859044"
                },
                {
                    "start": 260,
                    "end": 279,
                    "matchedPaperCorpusId": "256630990"
                },
                {
                    "start": 279,
                    "end": 297,
                    "matchedPaperCorpusId": "249062690"
                },
                {
                    "start": 318,
                    "end": 337,
                    "matchedPaperCorpusId": "253734850"
                },
                {
                    "start": 337,
                    "end": 358,
                    "matchedPaperCorpusId": "184486914"
                },
                {
                    "start": 383,
                    "end": 407,
                    "matchedPaperCorpusId": "247519021"
                },
                {
                    "start": 407,
                    "end": 426,
                    "matchedPaperCorpusId": "232185104"
                },
                {
                    "start": 426,
                    "end": 444,
                    "matchedPaperCorpusId": "247613246"
                },
                {
                    "start": 444,
                    "end": 467,
                    "matchedPaperCorpusId": "236477795"
                },
                {
                    "start": 467,
                    "end": 484,
                    "matchedPaperCorpusId": "248780440"
                },
                {
                    "start": 503,
                    "end": 520,
                    "matchedPaperCorpusId": "218613632"
                },
                {
                    "start": 520,
                    "end": 547,
                    "matchedPaperCorpusId": "231698657"
                },
                {
                    "start": 547,
                    "end": 564,
                    "matchedPaperCorpusId": "204838020"
                },
                {
                    "start": 564,
                    "end": 580,
                    "matchedPaperCorpusId": "249847943"
                },
                {
                    "start": 580,
                    "end": 598,
                    "matchedPaperCorpusId": "257079734"
                },
                {
                    "start": 598,
                    "end": 616,
                    "matchedPaperCorpusId": "170078973"
                },
                {
                    "start": 616,
                    "end": 633,
                    "matchedPaperCorpusId": "258537309"
                },
                {
                    "start": 633,
                    "end": 649,
                    "matchedPaperCorpusId": "259859034"
                },
                {
                    "start": 649,
                    "end": 668,
                    "matchedPaperCorpusId": "259088837"
                },
                {
                    "start": 723,
                    "end": 743,
                    "matchedPaperCorpusId": "221878771"
                },
                {
                    "start": 743,
                    "end": 763,
                    "matchedPaperCorpusId": "221655075"
                },
                {
                    "start": 763,
                    "end": 780,
                    "matchedPaperCorpusId": "235313967"
                },
                {
                    "start": 799,
                    "end": 821,
                    "matchedPaperCorpusId": "233240748"
                },
                {
                    "start": 821,
                    "end": 841,
                    "matchedPaperCorpusId": "235097342"
                },
                {
                    "start": 901,
                    "end": 919,
                    "matchedPaperCorpusId": "235829093"
                },
                {
                    "start": 958,
                    "end": 982,
                    "matchedPaperCorpusId": "246210255"
                },
                {
                    "start": 982,
                    "end": 1009,
                    "matchedPaperCorpusId": "237494677"
                },
                {
                    "start": 1220,
                    "end": 1240,
                    "matchedPaperCorpusId": "232075876"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.779296875
        },
        {
            "corpus_id": "275470775",
            "title": "Foundation Models at Work: Fine-Tuning for Fairness in Algorithmic Hiring",
            "text": "Debiasing LLMs. Gira, Zhang, and Lee (2022) proposed an LLM fine-tuning method by leveraging GPT-2 as an example, and showed their approach reduced the gender bias in GPT-2 on the StereoSet benchmark. While their method is relatively cost-effective compared to pre-training with an augmented dataset, the side effect of the fine-tuning for bias on downstream applications of language models has not been studied for their approach. Schick, Udupa, and Sch\u00fctze (2021) developed a self-debiasing (SD) approach where a fine-tuned GPT-2 model self-diagnoses the bias and remove from the generated text. Garimella et al. (2021) suggested a combined method of further pre-training with fairness-aware datasets and then fine-tuning based on a loss function including regularizers for bias for BERT (Devlin et al. 2019). Mao et al. (2023) noted the gap in existing works which separates the fine-tuning for debiasing then fine-tuning for downstream applications. Authors named the former bias as intrinsic bias whereas the latter has been called as application bias which our work targets to solve. \n\nMeasuring bias in LLMs. A variety of benchmarks have been published to fairly evaluate and compare developed debiasing techniques aiming to address biases related to stereotypes or gendered word associations (Nadeem, Bethke, and Reddy 2021;Nangia et al. 2020). Barikeri et al. (2021) targeted the bias in LLMs fine-tuned to conversational dialogue and have introduced RedditBias, a dataset rooted in actual Reddit conversations, designed to measure and mitigate biases in conversational models across gender, race, religion, and queerness. The study benchmarks the DialoGPT (Zhang et al. 2020) model with this dataset, revealing biases, particularly towards religious groups, and demonstrates that certain debiasing techniques can address these biases without sacrificing model performance. \n\nPrompt-based mitigation. Recent studies investigated prompt-based fine-tuning strategies.",
            "score": 0.5112387731119736,
            "section_title": "Related Work",
            "char_start_offset": 32126,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 15
                },
                {
                    "start": 16,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 953
                },
                {
                    "start": 954,
                    "end": 1089
                },
                {
                    "start": 1092,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1352
                },
                {
                    "start": 1353,
                    "end": 1631
                },
                {
                    "start": 1632,
                    "end": 1882
                },
                {
                    "start": 1885,
                    "end": 1909
                },
                {
                    "start": 1910,
                    "end": 1974
                }
            ],
            "ref_mentions": [
                {
                    "start": 16,
                    "end": 43,
                    "matchedPaperCorpusId": "248780268"
                },
                {
                    "start": 598,
                    "end": 621,
                    "matchedPaperCorpusId": "236477795"
                },
                {
                    "start": 790,
                    "end": 809,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1300,
                    "end": 1332,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 1332,
                    "end": 1351,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 1353,
                    "end": 1375,
                    "matchedPaperCorpusId": "235358955"
                },
                {
                    "start": 1666,
                    "end": 1685,
                    "matchedPaperCorpusId": "207869708"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8388671875
        },
        {
            "corpus_id": "253736389",
            "title": "Transformers for Tabular Data Representation: A Survey of Models and Applications",
            "text": "It is recognized that LMs incorporate bias in the model parameters in terms of stereotypes, race, and gender (Nadeem et al., 2021;Vig et al., 2020). The bias is implicitly derived from the training corpora used to develop LMs. Therefore, there is a need to develop methods to overcome this drawback by, for instance, pre-filtering the training data or by correcting the tabular LMs, similarly to the ongoing efforts for text LMs (Liu et al., 2021b;Bordia and Bowman, 2019). \n\nGreen LMs. The use of large-scale transformers for learning LMs requires considerable computation, which contributes to global warming (Strubell et al., 2020;Schwartz et al., 2020). Therefore, it is important to consider enhancements or potentially new techniques that limit the carbon footprint of tabular language models without a significant decrease in the performance in the downstream tasks. One enhancement can be at the level of the size of the training data by removing redundant, or less informative, tuples and tables. How to identify such data is a key challenge.",
            "score": 0.5109981228623329,
            "section_title": "Future Directions",
            "char_start_offset": 44971,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 473
                },
                {
                    "start": 476,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 657
                },
                {
                    "start": 658,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1051
                }
            ],
            "ref_mentions": [
                {
                    "start": 109,
                    "end": 130,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 130,
                    "end": 147,
                    "matchedPaperCorpusId": "227275068"
                },
                {
                    "start": 429,
                    "end": 448,
                    "matchedPaperCorpusId": "233476528"
                },
                {
                    "start": 448,
                    "end": 472,
                    "matchedPaperCorpusId": "102352788"
                },
                {
                    "start": 611,
                    "end": 634,
                    "matchedPaperCorpusId": "219182397"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.525390625
        },
        {
            "corpus_id": "264306211",
            "title": "Identifying and Adapting Transformer-Components Responsible for Gender Bias in an English Language Model",
            "text": "Is intervening in the model internals feasible?). Empirically, success in detecting and mitigating biases depends on many factors, including the choice of embeddings, training regimes, data sets and model choices (Blodgett et al., 2020(Blodgett et al., , 2021;;Talat et al., 2022;Delobelle et al., 2022;Barrett et al., 2019;Van Der Wal et al., 2022). \n\nThe \"black-box\" nature of LMs makes it difficult to identify and interpret how bias manifests and propagates in them, especially relying solely on correlational methods. The starting point for the current paper is the intuition that if, instead, it were possible to find causal relationships between the model's internal representations and its downstream bias, we could more effectively measure and intervene on these undesirable behaviors. \n\nWe therefore turn to a recent series of papers on interpretability methods that focus on causal discovery. In Section 2 we discuss three such methods, of which we adapt one (DiffMask) for our needs in Section 3. Our new method is more efficient than other causal methods, which is especially relevant when applied to large language models (LLMs). In Section 3 we also report results from these three methods when applied to GPT2-small and the problem of gender bias, and find that they discover largely overlapping sets of components, despite huge differences in computation requirements. In Section 4 we use the identified components to adapt GPT-2 small, using parameter-efficient fine-tuning procedures. We demonstrate how gender bias in LMs can be reduced with minimal effect to their language modelling performance by making targeted interventions to their components. However, we also recognize the limitations of operationalizing gender bias as we do, using minimal pairs of contrasting sentences-which simplify gender as a binary construct and may not work so well for other languages than English-and call for future research to develop reliable and validated bias measures (see van der Wal et al., 2023).",
            "score": 0.5108804138975611,
            "section_title": "Introduction",
            "char_start_offset": 1667,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 49
                },
                {
                    "start": 50,
                    "end": 350
                },
                {
                    "start": 353,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 794
                },
                {
                    "start": 797,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1503
                },
                {
                    "start": 1504,
                    "end": 1670
                },
                {
                    "start": 1671,
                    "end": 2011
                }
            ],
            "ref_mentions": [
                {
                    "start": 213,
                    "end": 235,
                    "matchedPaperCorpusId": "218971825"
                },
                {
                    "start": 235,
                    "end": 261,
                    "matchedPaperCorpusId": "236460302"
                },
                {
                    "start": 261,
                    "end": 280,
                    "matchedPaperCorpusId": "247626152"
                },
                {
                    "start": 280,
                    "end": 303,
                    "matchedPaperCorpusId": "250390561"
                },
                {
                    "start": 303,
                    "end": 324,
                    "matchedPaperCorpusId": "202789469"
                },
                {
                    "start": 324,
                    "end": 349,
                    "matchedPaperCorpusId": "250390797"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.767578125
        },
        {
            "corpus_id": "258832694",
            "title": "Should We Attend More or Less? Modulating Attention for Fairness",
            "text": "Gender bias mitigation methods may be broadly classified into two categories: intrinsic and extrinsic approaches. While intrinsic methods (Adi et al., 2017;Hupkes et al., 2018;Conneau et al., 2018;Tenney et al., 2019;Belinkov & Glass, 2019) focus on analyzing the embedding representations assigned to gender tokens by the model, extrinsic methods (Sennrich, 2017;Isabelle et al., 2017;Naik et al., 2018) rely on the model's predictions to determine if different genders achieve similar predictions under the same context. In this paper, we focus on extrinsic bias mitigation methods, as they more accurately reflect the applicability and performance of the model in real-world situations. \n\nPre-processing methods for mitigating gender bias involve modifying the training data to improve model fairness. One common method is counterfactual data augmentation (CDA) (Lu et al., 2020) (Dixon et al., 2018) adds new examples only for under-represented groups in the dataset. \n\nIn-processing bias mitigation methods aim to reduce bias during training by adding auxiliary loss terms to the model. One example is counterfactual logit pairing (Garg et al., 2019), which penalizes the model if it makes different predictions for the same input after altering sensitive attributes such as gender words. Another method by Kennedy et al. (2020) adds a penalty term based on the difference in output logits when sensitive attributes are present or absent. Instance weighting (Zhang et al., 2020) multiplies the loss by a factor greater than 1 for stereotypical sentences to penalize the model more for misclassifying them, and attentionbased regularization (Attanasio et al., 2022) maximizes the model's attention distribution entropy during training to improve fairness. MABEL (He et al., 2022) proposes adding an auxiliary loss to minimize the cosine similarity between the original and gender-swapped vector representation for each example. \n\nWhile relatively less explored, post-processing bias mitigation methods modify the predictions of a biased model and generate a new set of less biased predictions.",
            "score": 0.5106003029695988,
            "section_title": "Gender bias mitigation methods",
            "char_start_offset": 4908,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 689
                },
                {
                    "start": 692,
                    "end": 804
                },
                {
                    "start": 805,
                    "end": 971
                },
                {
                    "start": 974,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1759
                },
                {
                    "start": 1760,
                    "end": 1931
                },
                {
                    "start": 1934,
                    "end": 2097
                }
            ],
            "ref_mentions": [
                {
                    "start": 138,
                    "end": 156,
                    "matchedPaperCorpusId": "6771196"
                },
                {
                    "start": 156,
                    "end": 176,
                    "matchedPaperCorpusId": "5013113"
                },
                {
                    "start": 176,
                    "end": 197,
                    "matchedPaperCorpusId": "24461982"
                },
                {
                    "start": 197,
                    "end": 217,
                    "matchedPaperCorpusId": "155092004"
                },
                {
                    "start": 386,
                    "end": 404,
                    "matchedPaperCorpusId": "46932607"
                },
                {
                    "start": 865,
                    "end": 882,
                    "matchedPaperCorpusId": "51888520"
                },
                {
                    "start": 883,
                    "end": 903,
                    "matchedPaperCorpusId": "54997157"
                },
                {
                    "start": 1136,
                    "end": 1155,
                    "matchedPaperCorpusId": "52880735"
                },
                {
                    "start": 1312,
                    "end": 1333,
                    "matchedPaperCorpusId": "218517088"
                },
                {
                    "start": 1463,
                    "end": 1483,
                    "matchedPaperCorpusId": "216641659"
                },
                {
                    "start": 1645,
                    "end": 1669,
                    "matchedPaperCorpusId": "247519021"
                },
                {
                    "start": 1766,
                    "end": 1783,
                    "matchedPaperCorpusId": "253157517"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88232421875
        },
        {
            "corpus_id": "271039560",
            "title": "From \u2018Showgirls\u2019 to \u2018Performers\u2019: Fine-tuning with Gender-inclusive Language for Bias Reduction in LLMs",
            "text": "This means that just because our metrics report a reduction in stereotyping in the models, it does not ensure a bias-free model but should rather be interpreted as a tendency toward decreased stereotyping.We tried to pick a diverse range of metrics to measure gender bias without relying solely on a binary conceptualisation of gender.However, our choice of metrics was also limited by ease of use and interpretation.Besides issues with the bias metrics themselves, future work could additionally explore whether our fine-tuning approach impacts the performance of the models on NLU tasks.\n\nLastly, our study was limited to language mod-els of relatively small size.The largest models we used (GPT-2 and PHI-1.5) each have 1.5 billion parameters, which is significantly smaller than for example the smallest (seven billion parameter) model in the Llama suite of LLMs (Touvron et al., 2023), which reaches state-of-the-art performance using an open-source approach.We already demonstrated that the benefits of our approach differ based on the model used, which is why it would be interesting to see how fine-tuning with gender-neutral data impacts state-of-the-art models.However, our research institute does not have the resources to perform a study with models of state-of-the-art scale at the level of detail we provided here.Therefore, we leave experimentation with larger models to future research.",
            "score": 0.5103489473847511,
            "section_title": "Limitations",
            "char_start_offset": 33545,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 205,
                    "end": 335
                },
                {
                    "start": 335,
                    "end": 417
                },
                {
                    "start": 417,
                    "end": 589
                },
                {
                    "start": 591,
                    "end": 666
                },
                {
                    "start": 666,
                    "end": 964
                },
                {
                    "start": 964,
                    "end": 1171
                },
                {
                    "start": 1171,
                    "end": 1328
                },
                {
                    "start": 1328,
                    "end": 1402
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.258056640625
        },
        {
            "corpus_id": "259095603",
            "title": "Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions",
            "text": "Societal biases present in pre-trained large language models are a critical issue as these models have been shown to propagate biases in countless downstream applications, rendering them unfair towards specific groups of people. Since large-scale retraining of these models from scratch is both time and compute-expensive, a variety of approaches have been previously proposed that de-bias a pre-trained model. While the majority of current state-of-the-art debiasing methods focus on changes to the training regime, in this paper, we propose data intervention strategies as a powerful yet simple technique to reduce gender bias in pre-trained models. Specifically, we empirically show that by fine-tuning a pre-trained model on only 10 debiased (intervened) training examples, the tendency to favor any gender is significantly reduced. Since our proposed method only needs a few training examples, we argue that our few-shot de-biasing approach is highly feasible and practical. Through extensive experimentation, we show that our de-biasing technique performs better than competitive state-of-the-art baselines with minimal loss in language modeling ability.",
            "score": 0.5101122198941723,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87109375
        },
        {
            "corpus_id": "236477795",
            "title": "He is very intelligent, she is very beautiful? On Mitigating Social Biases in Language Modelling and Generation",
            "text": "There has been research in studying systems trained on human-written texts that learn human-like biases (Bolukbasi et al., 2016;Caliskan et al., 2016;Sun et al., 2019). Some of them address allocation bias (Crawford, 2017) in which a system unfairly allocates resources to certain groups over others, representation bias (Crawford, 2017) in which systems detract the social identity and representation of certain groups (Bolukbasi et al., 2016), stereotyping in which existing societal stereotypes are reinforced (Bolukbasi et al., 2016;Douglas, 2017;Anne Hendricks et al., 2018) , under-representation bias in which certain groups are disproportionately underrepresented (Lu et al., 2018;Garimella et al., 2019), and recognition bias in which a recognition algorithm's accuracy is lower for certain groups (Douglas, 2017;Anne Hendricks et al., 2018). Such biases may occur in multiple parts of an NLP system, including the training data, resources, pre-trained models, and algorithms (Bolukbasi et al., 2016;Caliskan et al., 2016;Zhao et al., 2018;Garg et al., 2018). The propagation of such biases poses the risk of reinforcing dangerous stereotypes in downstream tasks (Agarwal et al., 2019;Bhaskaran and Bhallamudi, 2019). \n\nWhile there exist works on mitigating social biases in language representations (Bolukbasi et al., 2016;Liang et al., 2020), there has been very little focus on debiasing the language models themselves or generation systems, specifically pre-trained language models that are widely used in several generation tasks. Qian et al. (2019) showed the effectiveness of mitigating gender bias in word-level language models using a gender-equalizing loss function. Sheng et al. (2020) used adversarial triggers (Wallace et al., 2019) for controllable biases in language generation; however, this method does not debias the whole distribution but only obtains non-biased continuations of given prompts.",
            "score": 0.5101122198941723,
            "section_title": "Related Work",
            "char_start_offset": 4782,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1226
                },
                {
                    "start": 1229,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1685
                },
                {
                    "start": 1686,
                    "end": 1922
                }
            ],
            "ref_mentions": [
                {
                    "start": 104,
                    "end": 128,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 150,
                    "end": 167,
                    "matchedPaperCorpusId": "195316733"
                },
                {
                    "start": 420,
                    "end": 444,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 513,
                    "end": 537,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 551,
                    "end": 579,
                    "matchedPaperCorpusId": "4384334"
                },
                {
                    "start": 689,
                    "end": 712,
                    "matchedPaperCorpusId": "196181460"
                },
                {
                    "start": 822,
                    "end": 850,
                    "matchedPaperCorpusId": "4384334"
                },
                {
                    "start": 985,
                    "end": 1009,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 1049,
                    "end": 1067,
                    "matchedPaperCorpusId": "4930886"
                },
                {
                    "start": 1172,
                    "end": 1194,
                    "matchedPaperCorpusId": "174803409"
                },
                {
                    "start": 1194,
                    "end": 1225,
                    "matchedPaperCorpusId": "195584301"
                },
                {
                    "start": 1309,
                    "end": 1333,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 1333,
                    "end": 1352,
                    "matchedPaperCorpusId": "207996257"
                },
                {
                    "start": 1545,
                    "end": 1563,
                    "matchedPaperCorpusId": "170078973"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3720703125
        },
        {
            "corpus_id": "276558185",
            "title": "Robust Bias Detection in MLMs and its Application to Human Trait Ratings",
            "text": "Our research focus is on bias detection. However, for the reader expecting a complete pipline that includes mitigation -we add this proof-of-concept experiment. We mitigate bias in RoBERTa-large, our most biased model (Section 3.2.1), with a focus on binary gender. We mitigate bias by fine-tuning the model on a gender-swapped GAP corpus (Webster et al., 2018) following Bartl et al. (2020). This process involves dynamic masking during fine-tuning MLM task, following the design described in Liu et al. (2019), to specifically address gender bias. \n\nWe tune the model for 3 epochs using AdamW optimizer with a 2e-5 learning rate and a batch size 16. To manage the learning rate adjustment smoothly, we use a polynomial decay scheduler with a linear warm-up phase over the first 500 steps. 3 for reported values descriptions and the notation used. Before mitigation result is identical to Table 3. \n\nTable 16 presents bias before and after mitigation in RoBERTa-large. Bias scores reduce by 56% to 98% after mitigation across both sets of traits. There is only one dimension serenity that still exhibits some bias -but this has reduced from medium to small. The remaining dimensions have become unbiased as regards gender.",
            "score": 0.5101122198941723,
            "section_title": "A.5.3 Bias mitigation in MLMs",
            "char_start_offset": 44377,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 40
                },
                {
                    "start": 41,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 549
                },
                {
                    "start": 552,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 898
                },
                {
                    "start": 901,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1223
                }
            ],
            "ref_mentions": [
                {
                    "start": 372,
                    "end": 391,
                    "matchedPaperCorpusId": "225094152"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5146484375
        },
        {
            "corpus_id": "262828449",
            "title": "Survey of Social Bias in Vision-Language Models",
            "text": "Post-processing Methods. \n\n\u2022 Feature Clipping. Wang et al. [174] propose a post-processing mitigation method that clips/prunes the dimensions of feature embeddings that are highly correlated with gender information. This idea is motivated by the fact that an unbiased retrieve implies the independence between the covariates (active features) and sensitive attributes (gender). However, the limitation of this approach is that it results in unavoidable performance degradation in the main task performance. \u2022 Instruction/Prompt Engineering. Friedrich et al. [62] propose a post-processing mitigation approach, Fair Diffusion, that is inspired by advances in instructing AI systems based on human feedback. Fair Diffusion \"instructs\" the pre-trained diffusion models to be fair during the deployment stage; it enables precise guidance to reduce biases in model outcomes based on pre-defined instructions stored in a lookup table. \u2022 Bias vector projection. Chuang et al. [38] de-bias VL foundation models by projecting out biased directions (gender bias direction, racial bias direction) in the text embedding. These bias directions are obtained from embeddings of spurious prompts such as \"a photo of a [irrelevant attribute]\". The authors also calibrate the projection matrix by introducing one additional regularization constraint that ensures the debiased prompt representation still has the same semantic meaning after the projection. To elaborate, \"a photo of a [class name] with [spurious attribute]\" should still have the same semantic meaning as \"a photo of a [class name]\". The biggest advantage of this method is that it is computationally lightweight and simple, as it only manipulates text embedding. However, we believe further exploration should be conducted to ensure that such an unimodal mitigation method can truly mitigate multimodal bias. \n\n4.5 Future Direction 4.5.1 Bias Evaluation and Analysis. \n\n\u2022 Choice of VLP architecture. As explained in Section 4.2, there are different architectural choices (i.e., unified, dual-stream, and encoder-decoder) that have their own strengths and weaknesses. It would be insightful to analyze the impact of VLP architectural design on social bias.",
            "score": 0.5099980368940413,
            "section_title": "4.4.2",
            "char_start_offset": 67608,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 24
                },
                {
                    "start": 27,
                    "end": 46
                },
                {
                    "start": 47,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 705
                },
                {
                    "start": 706,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 954
                },
                {
                    "start": 955,
                    "end": 1108
                },
                {
                    "start": 1109,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1437
                },
                {
                    "start": 1438,
                    "end": 1581
                },
                {
                    "start": 1582,
                    "end": 1711
                },
                {
                    "start": 1712,
                    "end": 1857
                },
                {
                    "start": 1860,
                    "end": 1916
                },
                {
                    "start": 1919,
                    "end": 1948
                },
                {
                    "start": 1949,
                    "end": 2115
                },
                {
                    "start": 2116,
                    "end": 2204
                }
            ],
            "ref_mentions": [
                {
                    "start": 59,
                    "end": 64,
                    "matchedPaperCorpusId": "237490811"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90625
        },
        {
            "corpus_id": "271769656",
            "title": "A Parameter-Efficient Multi-Objective Approach to Mitigate Stereotypical Bias in Language Models",
            "text": "For example, LMs can violate equal social group associations by predicting different occupations for male and female genders, or violate equal neutral neutral associations by believing that criminals are more likely to be people of color (Gallegos et al., 2023). In addition, biased LMs generate sentences containing higher-level disparity, such as sentiment (Huang et al., 2020) and regard (Sheng et al., 2019) for different demographics, demonstrating global bias (Liang et al., 2021). As a result, methods targeting only one specific form of bias can lead to incomplete bias removal and unsatisfactory debiasing performance. Besides, the increasing scale of pre-trained LMs boosts the design and application of parameterefficient fine-tuning methods (Houlsby et al., 2019;Lester et al., 2021;Li and Liang, 2021;Hu et al., 2022). Unfortunately, relatively little work has been devoted to studying parameter-efficient methods in the field of bias mitigation (Lauscher et al., 2021;Gira et al., 2022;Xie and Lukasiewicz, 2023). In this work, we also aim to further explore lightweight debiasing techniques using parameter-1 efficient fine-tuning methods. \n\nThe main contribution of this work includes: \n\n1. We refine and integrate existing probabilistic alignment debiasing approaches to simultaneously address multiple forms of bias representation, employing a parameter-efficient prefix-tuning technique for implementation. \n\n2. We empirically demonstrate the effectiveness of our method on diverse intrinsic and extrinsic bias evaluation benchmarks and compared it with existing debiasing techniques. \n\n3. We thoroughly analyze our parameter-efficient debiasing framework and show that it can achieve better bias mitigation performance and parameter efficiency than full fine-tuning. Additionally, our method is effective in reducing bias in large LMs.",
            "score": 0.5099611650236469,
            "section_title": "Introduction",
            "char_start_offset": 1610,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 831
                },
                {
                    "start": 832,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1154
                },
                {
                    "start": 1157,
                    "end": 1201
                },
                {
                    "start": 1204,
                    "end": 1425
                },
                {
                    "start": 1428,
                    "end": 1603
                },
                {
                    "start": 1606,
                    "end": 1786
                },
                {
                    "start": 1787,
                    "end": 1855
                }
            ],
            "ref_mentions": [
                {
                    "start": 359,
                    "end": 379,
                    "matchedPaperCorpusId": "207847197"
                },
                {
                    "start": 391,
                    "end": 411,
                    "matchedPaperCorpusId": "202537041"
                },
                {
                    "start": 466,
                    "end": 486,
                    "matchedPaperCorpusId": "235623756"
                },
                {
                    "start": 753,
                    "end": 775,
                    "matchedPaperCorpusId": "59599816"
                },
                {
                    "start": 775,
                    "end": 795,
                    "matchedPaperCorpusId": "233296808"
                },
                {
                    "start": 795,
                    "end": 814,
                    "matchedPaperCorpusId": "230433941"
                },
                {
                    "start": 814,
                    "end": 830,
                    "matchedPaperCorpusId": "235458009"
                },
                {
                    "start": 959,
                    "end": 982,
                    "matchedPaperCorpusId": "237440429"
                },
                {
                    "start": 982,
                    "end": 1000,
                    "matchedPaperCorpusId": "248780268"
                },
                {
                    "start": 1000,
                    "end": 1026,
                    "matchedPaperCorpusId": "259095584"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82568359375
        },
        {
            "corpus_id": "256390197",
            "title": "How Far Can It Go? On Intrinsic Gender Bias Mitigation for Text Classification",
            "text": "Intrinsic gender bias mitigation methods target either the pretraining data, the pretraining procedure, or the pretrained model's output, which we refer to as pre-processing, in-processing, and postprocessing respectively (Friedler et al., 2019). We select three popular mitigation methods to represent all three types, namely Counterfactual Data Augmentation (CDA), Context-debias, and Sentdebias. Notice that these methods create debiased pretrained language models, as is illustrated in Figure 1. These models still need to be finetuned on a downstream task.\n\nCDA pretraining. The idea behind counterfactual data augmentation (Zmigrod et al., 2019;Lu et al., 2020) is to generate a counterfactual for each example in the training corpus by replacing attribute terms with their complimentary equivalent from the other demographic classes. For example, she will map to he in the case of binary gender. To mitigate intrinsic bias, this counterfactual augmentation has to be done as a pretraining step. Since CDA involves retraining the model, it is more resource-intensive compared to Sent-debias and Context-debias. We use the pretrained CDA models based on BERT and ALBERT from Webster et al. (2020) for our implementation. (2021) introduce a debiasing method that involves retraining the language model with a constraint to make the embeddings of stereotype terms 4 (such as doctor, nurse) orthogonal to embeddings of attribute terms 5 (such as gender pronouns like she, he and gender nouns like woman, man). Given the dynamic nature of contextualized word embeddings which causes a word to have different embeddings, they define fixed word embeddings for each attribute token by averaging the contextual embeddings of a word in all sentences it appears in. Training is done so that the embeddings of all stereotype terms are made orthogonal to all fixed attribute embeddings. They add a regularizer that constrains the debiased embedding to retain as much information by ensuring that they are as similar to the original em",
            "score": 0.50965774548655,
            "section_title": "Intrinsic bias mitigation techniques",
            "char_start_offset": 4439,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8740234375
        },
        {
            "corpus_id": "256390197",
            "title": "How Far Can It Go? On Intrinsic Gender Bias Mitigation for Text Classification",
            "text": "In Figure 2, we plot the intrinsic bias results 9 of SEAT and LPBS on BERT-large. The first observation we point out, particularly for Context-debias and Sent-debias, is the contradicting nature of bias scores obtained from SEAT and LPBS. Whereas SEAT shows a drastic reduction in intrinsic bias for Context-debias, LPBS indicates worsening bias. The converse is true for Sent-debias where LPBS shows a significant reduction in intrinsic bias but SEAT shows worse scores. This confirms our suspicion that different mitigation strategies respond differently to different metrics. Figure 3: We compare LPBS in its default form (attribute-LPBS) and target-LPBS for BERT-large. The plot shows a general positive correlation between the two metrics. In both cases, Sent-debias maintains the lowest bias score -lower scores are desirable in both cases. See Figure 9 for ALBERT-large.\n\nThe use of some metrics can be problematic for evaluating some mitigation techniques for intrinsic bias. We find that some metrics are not ideal to use with some mitigation techniques due to differences in how various mitigation techniques interact with a language model. Consider a metric like LPBS which measures the association between a stereotype term and a gender P (A=a|X=x) P (A=a) . This association is captured at the inner layers of the model, but since post-hoc approaches like Sentdebias do not alter the internal representation of the model, LPBS in its original form becomes ineffective - Figure 7. We refer to this original form of LPBS as attribute-LPBS. Sent-debias ends up debiasing the attribute terms rather than the stereotype terms, thus, constantly producing low probability difference scores. Even so, we find LPBS being used in this form with Sent-debias in works such as (Steed et al., 2022). Using stereotype terms instead of attribute terms P (X = x|A = a), which we refer to as target-LPBS, is an option to solve this discrepancy, but given that many stereotype terms are usually out of vocabulary, these models will resort to wordpiece tokenization (Sennrich et al., 2016) which",
            "score": 0.5095093369551658,
            "section_title": "RQ1: Do intrinsic bias measures respond differently to bias mitigation techniques?",
            "char_start_offset": 16623,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.410400390625
        },
        {
            "corpus_id": "246441998",
            "title": "Handling Bias in Toxic Speech Detection: A Survey",
            "text": "This disparity reduced (yet persisted) when the -test was repeated conditioning on the presence of identity terms. The authors noted that while this prevailing disparity could have been due to other terms not conditioned upon, it was also possibly the result of the model correlated other subtler details of AAE with the toxicity. \n\nMitigation through the debiased training of downstream models. Mozafari et al. [64], Xia et al. [116], Zhou et al. [120] explored mitigation of racial bias through model debiasing. \n\n\u2022 Comment re-weighting in loss: Following [90], Mozafari et al. [64] defined a bias score    for each label class  and bigram term   as: \n\nwhere   is the weight of the  \u210e sample, and  is the indicator function. These sample weights  were learned by solving the following optimization: \n\nwhere  is the set of bigrams in the train data, and  is a hyperparameter. Though the debiasing was applied to the complete vocabulary, mitigation only in racial bias was evaluated. Reduction in bias against the Black-oriented tweets across all toxicity labels was observed for models trained on the W&H and Davidson datasets. \u2022 Adversarial training: Xia et al. [116] used an adversarial training [53] on the Founta100k dataset. One classification head () predicted the toxicity label, while the adversary head () predicted the protected attribute  (  |). This method was shown to be effective when lexical cues between the two attributes are closely related (e.g., terms like n*gga and b*tch correlate with both the AAE dialect and toxicity labels [20]). The model showed a reduction in FPR and an increase in macro- 1 scores. \n\n\u2022 Ensemble based debiasing: Similar to lexical bias (see Section 3.3), Zhou et al. [120] utilised LearnedMixIn for mitigating racial bias on the Founta dataset. They used the four  ( * |) predictions from the Blodgett LM as features for the naive model. A drop of FPR on the Black-oriented comments was observed, along with a drop in macro-F1.",
            "score": 0.5088932372050212,
            "section_title": "Manuscript submitted to ACM",
            "char_start_offset": 41555,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 330
                },
                {
                    "start": 333,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 513
                },
                {
                    "start": 516,
                    "end": 652
                },
                {
                    "start": 655,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 800
                },
                {
                    "start": 803,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1128
                },
                {
                    "start": 1129,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1629
                },
                {
                    "start": 1632,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 1885
                },
                {
                    "start": 1886,
                    "end": 1975
                }
            ],
            "ref_mentions": [
                {
                    "start": 412,
                    "end": 416,
                    "matchedPaperCorpusId": "221136077"
                },
                {
                    "start": 429,
                    "end": 434,
                    "matchedPaperCorpusId": "218869965"
                },
                {
                    "start": 448,
                    "end": 453,
                    "matchedPaperCorpusId": "231741340"
                },
                {
                    "start": 558,
                    "end": 562,
                    "matchedPaperCorpusId": "199577588"
                },
                {
                    "start": 580,
                    "end": 584,
                    "matchedPaperCorpusId": "221136077"
                },
                {
                    "start": 1164,
                    "end": 1169,
                    "matchedPaperCorpusId": "218869965"
                },
                {
                    "start": 1551,
                    "end": 1555,
                    "matchedPaperCorpusId": "168170119"
                },
                {
                    "start": 1715,
                    "end": 1720,
                    "matchedPaperCorpusId": "231741340"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.548828125
        },
        {
            "corpus_id": "258822832",
            "title": "Bias Beyond English: Counterfactual Tests for Bias in Sentiment Analysis in Four Languages",
            "text": "The conversation with that person is annoying. \n\nThe conversation with that Korean person is annoying. racial bias in four languages: Japanese (ja), simplified Chinese (zh), Spanish (es), German (de). Each of these four languages has publicly available data for training SA systems (Keung et al., 2020b), and together they represent three distinct language families. To complement their existing resources with a new resource that measures bias, we use counterfactual evaluation (Figure 1), in which test examples are edited to change a single variable of interestsuch as the race of the subject-extending previous work done in English (Kiritchenko and Mohammad, 2018). We release the evaluation dataset to facilitate further research. 1 e demonstrate the value of these evaluation resources by answering the following research questions: (RQ1) What biases do we find in other languages, compared to in English? (RQ2) How does the use of pre-trained models affect bias in SA systems? While pre-trained models are common in NLP, they may import biases not present in task supervision data, since a large pre-training corpus may embody biases not present in the supervision corpus. On the other hand, pre-training might diminish biases that arise from the small sample sizes typical of SA training corpora. \n\nOur experiments show that both gender and racial bias are present in SA systems for all four languages: when model architecture, data quantity, and domain are held constant, SA systems in other languages display quantitatively more bias than SA systems in English. For RQ2, we find that pre-training also makes SA systems less biased for all languages, in aggregate, though in surprising ways: our non-pre-trained models exhibit extreme changes in behaviour on counterfactual examples, whereas pre-trained models exhibit many small nuanced changes.",
            "score": 0.5087664895144978,
            "section_title": "\u305d\u306e\u4eba\u3068\u306e\u4f1a\u8a71\u306f\u3080\u304b\u3064\u304b\u305f\u3002",
            "char_start_offset": 763,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 46
                },
                {
                    "start": 49,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 737
                },
                {
                    "start": 738,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1304
                },
                {
                    "start": 1307,
                    "end": 1571
                },
                {
                    "start": 1572,
                    "end": 1855
                }
            ],
            "ref_mentions": [
                {
                    "start": 282,
                    "end": 303,
                    "matchedPaperCorpusId": "222381127"
                },
                {
                    "start": 636,
                    "end": 668,
                    "matchedPaperCorpusId": "21670658"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5361328125
        },
        {
            "corpus_id": "277113692",
            "title": "A Peek Behind the Curtain: Using Step-Around Prompt Engineering to Identify Bias and Misinformation in GenAI Models",
            "text": "This groundwork paves the way for discussions on mitigating such representational harm. Building on the theme of exploring AI's role in society, Luo et al. (2023) demonstrate how GenAI often presents Anglo-American views as the definitive truth, downplaying non-English perspectives, which can lead to cultural biases in AI-generated content. Similarly, Fri\u00f0riksd\u00f3ttir & Einarsson (2024) found that AI models trained on Icelandic, a grammatically gendered language, exhibited a significant predisposition towards the masculine grammatical gender when referring to occupation terms, even for female-dominated professions, again perpetuating gender biases. These findings underscore the need for discussions on mitigating such representational harm. \n\nAlthough GenAI systems are engineered to process information systematically, the tendency to anthropomorphize these technologies introduces significant complications. Pendzel (2023) demonstrates through discourse analysis how attributing human-like characteristics to AI systems can reinforce harmful gender stereotypes within these supposedly neutral technologies. Therefore, a design shift towards gender-neutral AI architectures and the necessity of employing carefully curated, balanced training data to mitigate these effects are required. Erscoi et al. (2023) provides a further critical examination on this topic. Through a meta-analysis, they critique the existing methodologies for evaluating bias in AI language models and advocate for the development of more comprehensive frameworks to better understand and mitigate these biases. AI ethics and the broader discourse on embedded bias (Bostrom and Yudkowsky, 2018;Buddemeyer, 2021;Manna & Nath, 2021;Nadeem et al., 2020;Wellner, 2020). Much of the discourse and analysis highlights the challenges of aligning AI development with the principles of gender equity, urging interdisciplinary efforts to ensure that AI design promotes fairness and mitigates systemic biases. This discussion underscores the importance of eliminating biases from training data to create more equitable AI systems. \n\nLarge language models have been shown to exhibit language biases, often presenting Anglo-American views as universal truths while downplaying non-English perspectives (Luo et al., 2023)",
            "score": 0.5086266125875409,
            "section_title": "Bias Studies in GenAI",
            "char_start_offset": 8811,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 88,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 654
                },
                {
                    "start": 655,
                    "end": 747
                },
                {
                    "start": 750,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1370
                },
                {
                    "start": 1371,
                    "end": 1592
                },
                {
                    "start": 1593,
                    "end": 1746
                },
                {
                    "start": 1747,
                    "end": 1979
                },
                {
                    "start": 1980,
                    "end": 2100
                },
                {
                    "start": 2103,
                    "end": 2288
                }
            ],
            "ref_mentions": [
                {
                    "start": 354,
                    "end": 387,
                    "matchedPaperCorpusId": "269804729"
                },
                {
                    "start": 1646,
                    "end": 1675,
                    "matchedPaperCorpusId": "148163385"
                },
                {
                    "start": 1675,
                    "end": 1692,
                    "matchedPaperCorpusId": "244130234"
                },
                {
                    "start": 1692,
                    "end": 1711,
                    "matchedPaperCorpusId": "244604038"
                },
                {
                    "start": 1711,
                    "end": 1731,
                    "matchedPaperCorpusId": "249618764"
                },
                {
                    "start": 1731,
                    "end": 1745,
                    "matchedPaperCorpusId": "231784942"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28662109375
        },
        {
            "paperId": "fb2921d3034f5047e87d51b3e6f9809868c509cf",
            "corpusId": 278939773,
            "title": "Bias and Fairness in Large Language Models: Evaluation and Mitigation Techniques",
            "venue": "International Journal of Academic and Industrial Research Innovations(IJAIRI)",
            "year": 2025,
            "referenceCount": 0,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.62311/nesx/rphcr6?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.62311/nesx/rphcr6, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2295364655",
                    "name": "Murali Krishna Pasupuleti"
                }
            ],
            "abstract": "Abstract:\nLarge Language Models (LLMs) such as GPT, BERT, and LLaMA have transformed natural language processing, yet they exhibit social biases that can reinforce unfair outcomes. This paper systematically evaluates bias and fairness in LLMs across gender, race, and socioeconomic dimensions using benchmark datasets and fairness metrics. We assess bias through template-based probing, stereotype score measurement, and downstream task performance. We implement mitigation strategies including adversarial training, counterfactual data augmentation, and fairness-aware loss functions. Regression and predictive analysis reveal that token frequency and representation distance significantly correlate with bias scores. Post-mitigation analysis shows up to a 48% reduction in bias indicators with minimal accuracy trade-offs.\n\nKeywords:\nLarge Language Models, Bias, Fairness, Evaluation, Mitigation, GPT, BERT, Counterfactual Augmentation, SHAP, LIME",
            "corpus_id": "278939773",
            "text": "Abstract:\nLarge Language Models (LLMs) such as GPT, BERT, and LLaMA have transformed natural language processing, yet they exhibit social biases that can reinforce unfair outcomes. This paper systematically evaluates bias and fairness in LLMs across gender, race, and socioeconomic dimensions using benchmark datasets and fairness metrics. We assess bias through template-based probing, stereotype score measurement, and downstream task performance. We implement mitigation strategies including adversarial training, counterfactual data augmentation, and fairness-aware loss functions. Regression and predictive analysis reveal that token frequency and representation distance significantly correlate with bias scores. Post-mitigation analysis shows up to a 48% reduction in bias indicators with minimal accuracy trade-offs.\n\nKeywords:\nLarge Language Models, Bias, Fairness, Evaluation, Mitigation, GPT, BERT, Counterfactual Augmentation, SHAP, LIME",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.97119140625
        },
        {
            "paperId": "9790d45bbaa9502dd512d29e5c74664bb0918a3c",
            "corpusId": 277444309,
            "title": "Evaluation and Bias Analysis of Large Language Models in Generating Synthetic Electronic Health Records: Comparative Study",
            "venue": "Journal of Medical Internet Research",
            "year": 2024,
            "referenceCount": 28,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12107208, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2140386888",
                    "name": "Ruocheng Huang"
                },
                {
                    "authorId": "2333605396",
                    "name": "Honghan Wu"
                },
                {
                    "authorId": "2352989527",
                    "name": "Yuhan Yuan"
                },
                {
                    "authorId": "2352848167",
                    "name": "Yifan Xu"
                },
                {
                    "authorId": "2353072228",
                    "name": "Hao Qian"
                },
                {
                    "authorId": "2343103671",
                    "name": "Changwei Zhang"
                },
                {
                    "authorId": "153664275",
                    "name": "Xinpo Wei"
                },
                {
                    "authorId": "2273283530",
                    "name": "Shan Lu"
                },
                {
                    "authorId": "2353047400",
                    "name": "Xin Zhang"
                },
                {
                    "authorId": "113286545",
                    "name": "J. Kan"
                },
                {
                    "authorId": "2056300421",
                    "name": "Cheng Wan"
                },
                {
                    "authorId": "2297437749",
                    "name": "Yun Liu"
                }
            ],
            "abstract": "Background Synthetic electronic health records (EHRs) generated by large language models (LLMs) offer potential for clinical education and model training while addressing privacy concerns. However, performance variations and demographic biases in these models remain underexplored, posing risks to equitable health care. Objective This study aimed to systematically assess the performance of various LLMs in generating synthetic EHRs and to critically evaluate the presence of gender and racial biases in the generated outputs. We focused on assessing the completeness and representativeness of these EHRs across 20 diseases with varying demographic prevalence. Methods A framework was developed to generate 140,000 synthetic EHRs using 10 standardized prompts across 7 LLMs. The electronic health record performance score (EPS) was introduced to quantify completeness, while the statistical parity difference (SPD) was proposed to assess the degree and direction of demographic bias. Chi-square tests were used to evaluate the presence of bias across demographic groups. Results Larger models exhibited superior performance but heightened biases. The Yi-34B achieved the highest EPS (96.8), while smaller models (Qwen-1.8B: EPS=63.35) underperformed. Sex polarization emerged: female-dominated diseases (eg, multiple sclerosis) saw amplified female representation in outputs (Qwen-14B: 973/1000, 97.3% female vs 564,424/744,778, 75.78% real; SPD=+21.50%), while balanced diseases and male-dominated diseases skewed the male group (eg, hypertension Llama 2-13 B: 957/1000, 95.7% male vs 79,540,040/152,466,669, 52.17% real; SPD=+43.50%). Racial bias patterns revealed that some models overestimated the representation of White (eg, Yi-6B: mean SPD +14.40%, SD 16.22%) or Black groups (eg, Yi-34B: mean SPD +14.90%, SD 27.16%), while most models systematically underestimated the representation of Hispanic (average SPD across 7 models is \u201311.93%, SD 8.36%) and Asian groups (average SPD across 7 models is \u20130.77%, SD 11.99%). Conclusions Larger models, such as Yi-34B, Qwen-14B, and Llama 2 to 13 B, showed improved performance in generating more comprehensive EHRs, as reflected in higher EPS values. However, this increased performance was accompanied by a notable escalation in both gender and racial biases, highlighting a performance-bias trade-off. The study identified 4 key findings as follows: (1) as model size increased, EHR generation improved, but demographic biases also became more pronounced; (2) biases were observed across all models, not just the larger ones; (3) gender bias closely aligned with real-world disease prevalence, while racial bias was evident in only a subset of diseases; and (4) racial biases varied, with some diseases showing overrepresentation of White or Black populations and underrepresentation of Hispanic and Asian groups. These findings underline the need for effective bias mitigation strategies and the development of benchmarks to ensure fairness in artificial intelligence applications for health care.",
            "corpus_id": "277444309",
            "text": "Background Synthetic electronic health records (EHRs) generated by large language models (LLMs) offer potential for clinical education and model training while addressing privacy concerns. However, performance variations and demographic biases in these models remain underexplored, posing risks to equitable health care. Objective This study aimed to systematically assess the performance of various LLMs in generating synthetic EHRs and to critically evaluate the presence of gender and racial biases in the generated outputs. We focused on assessing the completeness and representativeness of these EHRs across 20 diseases with varying demographic prevalence. Methods A framework was developed to generate 140,000 synthetic EHRs using 10 standardized prompts across 7 LLMs. The electronic health record performance score (EPS) was introduced to quantify completeness, while the statistical parity difference (SPD) was proposed to assess the degree and direction of demographic bias. Chi-square tests were used to evaluate the presence of bias across demographic groups. Results Larger models exhibited superior performance but heightened biases. The Yi-34B achieved the highest EPS (96.8), while smaller models (Qwen-1.8B: EPS=63.35) underperformed. Sex polarization emerged: female-dominated diseases (eg, multiple sclerosis) saw amplified female representation in outputs (Qwen-14B: 973/1000, 97.3% female vs 564,424/744,778, 75.78% real; SPD=+21.50%), while balanced diseases and male-dominated diseases skewed the male group (eg, hypertension Llama 2-13 B: 957/1000, 95.7% male vs 79,540,040/152,466,669, 52.17% real; SPD=+43.50%). Racial bias patterns revealed that some models overestimated the representation of White (eg, Yi-6B: mean SPD +14.40%, SD 16.22%) or Black groups (eg, Yi-34B: mean SPD +14.90%, SD 27.16%), while most models systematically underestimated the representation of Hispanic (average SPD across 7 models is \u201311.93%, SD 8.36%) and Asian groups (average SPD across 7 models is \u20130.77%, SD 11.99%). Conclusions Larger models, such as Yi-34B, Qwen-14B, and Llama 2 to 13 B, showed improved performance in generating more comprehensive EHRs, as reflected in higher EPS values. However, this increased performance was accompanied by a notable escalation in both gender and racial biases, highlighting a performance-bias trade-off. The study identified 4 key findings as follows: (1) as model size increased, EHR generation improved, but demographic biases also became more pronounced; (2) biases were observed across all models, not just the larger ones; (3) gender bias closely aligned with real-world disease prevalence, while racial bias was evident in only a subset of diseases; and (4) racial biases varied, with some diseases showing overrepresentation of White or Black populations and underrepresentation of Hispanic and Asian groups. These findings underline the need for effective bias mitigation strategies and the development of benchmarks to ensure fairness in artificial intelligence applications for health care.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.44140625
        },
        {
            "paperId": "9f4aba794625b3fd4cb11507988421208ab3f892",
            "corpusId": 276450004,
            "title": "Benchmarking Post-Training Quantization in LLMs: Comprehensive Taxonomy, Unified Evaluation, and Comparative Analysis",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 81,
            "citationCount": 2,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.13178, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2316234387",
                    "name": "Jiaqi Zhao"
                },
                {
                    "authorId": "2316247964",
                    "name": "Ming Wang"
                },
                {
                    "authorId": "2316243861",
                    "name": "Miao Zhang"
                },
                {
                    "authorId": "2346171830",
                    "name": "Yuzhang Shang"
                },
                {
                    "authorId": "2316360432",
                    "name": "Xuebo Liu"
                },
                {
                    "authorId": "2346266140",
                    "name": "Yaowei Wang"
                },
                {
                    "authorId": "2346352158",
                    "name": "Min Zhang"
                },
                {
                    "authorId": "2284688853",
                    "name": "Liqiang Nie"
                }
            ],
            "abstract": "Post-training Quantization (PTQ) technique has been extensively adopted for large language models (LLMs) compression owing to its efficiency and low resource requirement. However, current research lacks a in-depth analysis of the superior and applicable scenarios of each PTQ strategy. In addition, existing algorithms focus primarily on performance, overlooking the trade-off among model size, performance, and quantization bitwidth. To mitigate these confusions, we provide a novel benchmark for LLMs PTQ in this paper. Firstly, in order to support our benchmark, we propose a comprehensive taxonomy for existing mainstream methods by scrutinizing their computational strategies (e.g., optimization-based, compensation-based, etc.). Then, we conduct extensive experiments with the baseline within each class, covering models with various sizes (7B-70B), bitwidths, training levels (LLaMA1/2/3/3.1), architectures (Mixtral, DeepSeekMoE and Mamba) and modality (LLaVA1.5 and VILA1.5) on a wide range of evaluation metrics.Through comparative analysis on the results, we summarize the superior of each PTQ strategy and modelsize-bitwidth trade-off considering the performance. For example, our benchmark reveals that compensation-based technique demonstrates outstanding cross-architecture robustness and extremely low-bit PTQ for ultra large models should be reexamined. Finally, we further accordingly claim that a practical combination of compensation and other PTQ strategy can achieve SOTA various robustness. We believe that our benchmark will provide valuable recommendations for the deployment of LLMs and future research on PTQ approaches.We conduct an repository for our benchmark at https://github.com/zjq0455/PTQ_Benchmark.",
            "corpus_id": "276450004",
            "text": "Post-training Quantization (PTQ) technique has been extensively adopted for large language models (LLMs) compression owing to its efficiency and low resource requirement. However, current research lacks a in-depth analysis of the superior and applicable scenarios of each PTQ strategy. In addition, existing algorithms focus primarily on performance, overlooking the trade-off among model size, performance, and quantization bitwidth. To mitigate these confusions, we provide a novel benchmark for LLMs PTQ in this paper. Firstly, in order to support our benchmark, we propose a comprehensive taxonomy for existing mainstream methods by scrutinizing their computational strategies (e.g., optimization-based, compensation-based, etc.). Then, we conduct extensive experiments with the baseline within each class, covering models with various sizes (7B-70B), bitwidths, training levels (LLaMA1/2/3/3.1), architectures (Mixtral, DeepSeekMoE and Mamba) and modality (LLaVA1.5 and VILA1.5) on a wide range of evaluation metrics.Through comparative analysis on the results, we summarize the superior of each PTQ strategy and modelsize-bitwidth trade-off considering the performance. For example, our benchmark reveals that compensation-based technique demonstrates outstanding cross-architecture robustness and extremely low-bit PTQ for ultra large models should be reexamined. Finally, we further accordingly claim that a practical combination of compensation and other PTQ strategy can achieve SOTA various robustness. We believe that our benchmark will provide valuable recommendations for the deployment of LLMs and future research on PTQ approaches.We conduct an repository for our benchmark at https://github.com/zjq0455/PTQ_Benchmark.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.08660888671875
        },
        {
            "paperId": "e1314f15692f5573b972d301bd32b70ec116cd38",
            "corpusId": 267523377,
            "title": "De-amplifying Bias from Differential Privacy in Language Model Fine-tuning",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 32,
            "citationCount": 2,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.04489, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2283141928",
                    "name": "Sanjari Srivastava"
                },
                {
                    "authorId": "3251561",
                    "name": "Piotr (Peter) Mardziel"
                },
                {
                    "authorId": "2283145557",
                    "name": "Zhikhun Zhang"
                },
                {
                    "authorId": "2283137127",
                    "name": "Archana Ahlawat"
                },
                {
                    "authorId": "33374965",
                    "name": "Anupam Datta"
                },
                {
                    "authorId": "2283264477",
                    "name": "John C. Mitchell"
                }
            ],
            "abstract": "Fairness and privacy are two important values machine learning (ML) practitioners often seek to operationalize in models. Fairness aims to reduce model bias for social/demographic sub-groups. Privacy via differential privacy (DP) mechanisms, on the other hand, limits the impact of any individual's training data on the resulting model. The trade-offs between privacy and fairness goals of trustworthy ML pose a challenge to those wishing to address both. We show that DP amplifies gender, racial, and religious bias when fine-tuning large language models (LLMs), producing models more biased than ones fine-tuned without DP. We find the cause of the amplification to be a disparity in convergence of gradients across sub-groups. Through the case of binary gender bias, we demonstrate that Counterfactual Data Augmentation (CDA), a known method for addressing bias, also mitigates bias amplification by DP. As a consequence, DP and CDA together can be used to fine-tune models while maintaining both fairness and privacy.",
            "corpus_id": "267523377",
            "text": "Fairness and privacy are two important values machine learning (ML) practitioners often seek to operationalize in models. Fairness aims to reduce model bias for social/demographic sub-groups. Privacy via differential privacy (DP) mechanisms, on the other hand, limits the impact of any individual's training data on the resulting model. The trade-offs between privacy and fairness goals of trustworthy ML pose a challenge to those wishing to address both. We show that DP amplifies gender, racial, and religious bias when fine-tuning large language models (LLMs), producing models more biased than ones fine-tuned without DP. We find the cause of the amplification to be a disparity in convergence of gradients across sub-groups. Through the case of binary gender bias, we demonstrate that Counterfactual Data Augmentation (CDA), a known method for addressing bias, also mitigates bias amplification by DP. As a consequence, DP and CDA together can be used to fine-tune models while maintaining both fairness and privacy.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.91748046875
        },
        {
            "paperId": "7324c2d0e82742f3299e2ec4617a253318e6ece3",
            "corpusId": 276617517,
            "title": "Exploring the Generalizability of Factual Hallucination Mitigation via Enhancing Precise Knowledge Utilization",
            "venue": "",
            "year": 2025,
            "referenceCount": 74,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.19127, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2297219349",
                    "name": "Siyuan Zhang"
                },
                {
                    "authorId": "2243430472",
                    "name": "Yichi Zhang"
                },
                {
                    "authorId": "3431029",
                    "name": "Yinpeng Dong"
                },
                {
                    "authorId": "2267649469",
                    "name": "Hang Su"
                }
            ],
            "abstract": "Large Language Models (LLMs) often struggle to align their responses with objective facts, resulting in the issue of factual hallucinations, which can be difficult to detect and mislead users without relevant knowledge. Although post-training techniques have been employed to mitigate the issue, existing methods usually suffer from poor generalization and trade-offs in different capabilities. In this paper, we propose to address it by directly augmenting LLM's fundamental ability to precisely leverage its knowledge and introduce PKUE, which fine-tunes the model on self-generated responses to precise and simple factual questions through preference optimization. Furthermore, we construct FactualBench, a comprehensive and precise factual QA dataset containing 181k Chinese data spanning 21 domains, to facilitate both evaluation and training. Extensive experiments demonstrate that PKUE significantly improves LLM overall performance, with consistent enhancement across factual tasks of various forms, general tasks beyond factuality, and tasks in a different language.",
            "corpus_id": "276617517",
            "text": "Large Language Models (LLMs) often struggle to align their responses with objective facts, resulting in the issue of factual hallucinations, which can be difficult to detect and mislead users without relevant knowledge. Although post-training techniques have been employed to mitigate the issue, existing methods usually suffer from poor generalization and trade-offs in different capabilities. In this paper, we propose to address it by directly augmenting LLM's fundamental ability to precisely leverage its knowledge and introduce PKUE, which fine-tunes the model on self-generated responses to precise and simple factual questions through preference optimization. Furthermore, we construct FactualBench, a comprehensive and precise factual QA dataset containing 181k Chinese data spanning 21 domains, to facilitate both evaluation and training. Extensive experiments demonstrate that PKUE significantly improves LLM overall performance, with consistent enhancement across factual tasks of various forms, general tasks beyond factuality, and tasks in a different language.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.199951171875
        },
        {
            "paperId": "46fb63b449a468600c4274823bbffb37b8a21d87",
            "corpusId": 266174259,
            "title": "FairSISA: Ensemble Post-Processing to Improve Fairness of Unlearning in LLMs",
            "venue": "arXiv.org",
            "year": 2023,
            "referenceCount": 45,
            "citationCount": 7,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.07420, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1686542",
                    "name": "S. Kadhe"
                },
                {
                    "authorId": "2288315382",
                    "name": "Anisa Halimi"
                },
                {
                    "authorId": "22261698",
                    "name": "Ambrish Rawat"
                },
                {
                    "authorId": "2478882",
                    "name": "Nathalie Baracaldo"
                }
            ],
            "abstract": "Training large language models (LLMs) is a costly endeavour in terms of time and computational resources. The large amount of training data used during the unsupervised pre-training phase makes it difficult to verify all data and, unfortunately, undesirable data may be ingested during training. Re-training from scratch is impractical and has led to the creation of the 'unlearning' discipline where models are modified to\"unlearn\"undesirable information without retraining. However, any modification can alter the behaviour of LLMs, especially on key dimensions such as fairness. This is the first work that examines this interplay between unlearning and fairness for LLMs. In particular, we focus on a popular unlearning framework known as SISA [Bourtoule et al., 2021], which creates an ensemble of models trained on disjoint shards. We evaluate the performance-fairness trade-off for SISA, and empirically demsontrate that SISA can indeed reduce fairness in LLMs. To remedy this, we propose post-processing bias mitigation techniques for ensemble models produced by SISA. We adapt the post-processing fairness improvement technique from [Hardt et al., 2016] to design three methods that can handle model ensembles, and prove that one of the methods is an optimal fair predictor for ensemble of models. Through experimental results, we demonstrate the efficacy of our post-processing framework called 'FairSISA'.",
            "corpus_id": "266174259",
            "text": "Training large language models (LLMs) is a costly endeavour in terms of time and computational resources. The large amount of training data used during the unsupervised pre-training phase makes it difficult to verify all data and, unfortunately, undesirable data may be ingested during training. Re-training from scratch is impractical and has led to the creation of the 'unlearning' discipline where models are modified to\"unlearn\"undesirable information without retraining. However, any modification can alter the behaviour of LLMs, especially on key dimensions such as fairness. This is the first work that examines this interplay between unlearning and fairness for LLMs. In particular, we focus on a popular unlearning framework known as SISA [Bourtoule et al., 2021], which creates an ensemble of models trained on disjoint shards. We evaluate the performance-fairness trade-off for SISA, and empirically demsontrate that SISA can indeed reduce fairness in LLMs. To remedy this, we propose post-processing bias mitigation techniques for ensemble models produced by SISA. We adapt the post-processing fairness improvement technique from [Hardt et al., 2016] to design three methods that can handle model ensembles, and prove that one of the methods is an optimal fair predictor for ensemble of models. Through experimental results, we demonstrate the efficacy of our post-processing framework called 'FairSISA'.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.8671875
        },
        {
            "paperId": "b6a5dca855a12d5058db77e17d11f3cac678581d",
            "corpusId": 278911584,
            "title": "Pangu Light: Weight Re-Initialization for Pruning and Accelerating LLMs",
            "venue": "",
            "year": 2025,
            "referenceCount": 27,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.20155, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2118023932",
                    "name": "Hanting Chen"
                },
                {
                    "authorId": "79494403",
                    "name": "Jiarui Qin"
                },
                {
                    "authorId": "2302712033",
                    "name": "Jialong Guo"
                },
                {
                    "authorId": "2363559258",
                    "name": "Tao Yuan"
                },
                {
                    "authorId": "2310196106",
                    "name": "Yichun Yin"
                },
                {
                    "authorId": "2363579575",
                    "name": "Huiling Zhen"
                },
                {
                    "authorId": "2282544603",
                    "name": "Yasheng Wang"
                },
                {
                    "authorId": "2213935174",
                    "name": "Jinpeng Li"
                },
                {
                    "authorId": "2363926106",
                    "name": "Xiaojun Meng"
                },
                {
                    "authorId": "2364773951",
                    "name": "Meng Zhang"
                },
                {
                    "authorId": "2303415441",
                    "name": "Rongju Ruan"
                },
                {
                    "authorId": "2218622181",
                    "name": "Zheyuan Bai"
                },
                {
                    "authorId": "103603255",
                    "name": "Yehui Tang"
                },
                {
                    "authorId": "2354610519",
                    "name": "Can Chen"
                },
                {
                    "authorId": "2143792309",
                    "name": "Xinghao Chen"
                },
                {
                    "authorId": "2354693875",
                    "name": "Fisher Yu"
                },
                {
                    "authorId": "2284295184",
                    "name": "Ruiming Tang"
                },
                {
                    "authorId": "2354961454",
                    "name": "Yunhe Wang"
                }
            ],
            "abstract": "Large Language Models (LLMs) deliver state-of-the-art capabilities across numerous tasks, but their immense size and inference costs pose significant computational challenges for practical deployment. While structured pruning offers a promising avenue for model compression, existing methods often struggle with the detrimental effects of aggressive, simultaneous width and depth reductions, leading to substantial performance degradation. This paper argues that a critical, often overlooked, aspect in making such aggressive joint pruning viable is the strategic re-initialization and adjustment of remaining weights to improve the model post-pruning training accuracies. We introduce Pangu Light, a framework for LLM acceleration centered around structured pruning coupled with novel weight re-initialization techniques designed to address this ``missing piece''. Our framework systematically targets multiple axes, including model width, depth, attention heads, and RMSNorm, with its effectiveness rooted in novel re-initialization methods like Cross-Layer Attention Pruning (CLAP) and Stabilized LayerNorm Pruning (SLNP) that mitigate performance drops by providing the network a better training starting point. Further enhancing efficiency, Pangu Light incorporates specialized optimizations such as absorbing Post-RMSNorm computations and tailors its strategies to Ascend NPU characteristics. The Pangu Light models consistently exhibit a superior accuracy-efficiency trade-off, outperforming prominent baseline pruning methods like Nemotron and established LLMs like Qwen3 series. For instance, on Ascend NPUs, Pangu Light-32B's 81.6 average score and 2585 tokens/s throughput exceed Qwen3-32B's 80.9 average score and 2225 tokens/s.",
            "corpus_id": "278911584",
            "text": "Large Language Models (LLMs) deliver state-of-the-art capabilities across numerous tasks, but their immense size and inference costs pose significant computational challenges for practical deployment. While structured pruning offers a promising avenue for model compression, existing methods often struggle with the detrimental effects of aggressive, simultaneous width and depth reductions, leading to substantial performance degradation. This paper argues that a critical, often overlooked, aspect in making such aggressive joint pruning viable is the strategic re-initialization and adjustment of remaining weights to improve the model post-pruning training accuracies. We introduce Pangu Light, a framework for LLM acceleration centered around structured pruning coupled with novel weight re-initialization techniques designed to address this ``missing piece''. Our framework systematically targets multiple axes, including model width, depth, attention heads, and RMSNorm, with its effectiveness rooted in novel re-initialization methods like Cross-Layer Attention Pruning (CLAP) and Stabilized LayerNorm Pruning (SLNP) that mitigate performance drops by providing the network a better training starting point. Further enhancing efficiency, Pangu Light incorporates specialized optimizations such as absorbing Post-RMSNorm computations and tailors its strategies to Ascend NPU characteristics. The Pangu Light models consistently exhibit a superior accuracy-efficiency trade-off, outperforming prominent baseline pruning methods like Nemotron and established LLMs like Qwen3 series. For instance, on Ascend NPUs, Pangu Light-32B's 81.6 average score and 2585 tokens/s throughput exceed Qwen3-32B's 80.9 average score and 2225 tokens/s.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.1181640625
        },
        {
            "paperId": "9f0fe9197f080d042ad406b149ac03a1063c0351",
            "corpusId": 238634327,
            "title": "Balancing Average and Worst-case Accuracy in Multitask Learning",
            "venue": "arXiv.org",
            "year": 2021,
            "referenceCount": 53,
            "citationCount": 12,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2110.05838, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "144397625",
                    "name": "Paul Michel"
                },
                {
                    "authorId": "2884561",
                    "name": "Sebastian Ruder"
                },
                {
                    "authorId": "1755465",
                    "name": "Dani Yogatama"
                }
            ],
            "abstract": "When training and evaluating machine learning models on a large number of tasks, it is important to not only look at average task accuracy -- which may be biased by easy or redundant tasks -- but also worst-case accuracy (i.e. the performance on the task with the lowest accuracy). In this work, we show how to use techniques from the distributionally robust optimization (DRO) literature to improve worst-case performance in multitask learning. We highlight several failure cases of DRO when applied off-the-shelf and present an improved method, Lookahead-DRO (L-DRO), which mitigates these issues. The core idea of L-DRO is to anticipate the interaction between tasks during training in order to choose a dynamic re-weighting of the various task losses, which will (i) lead to minimal worst-case loss and (ii) train on as many tasks as possible. After demonstrating the efficacy of L-DRO on a small controlled synthetic setting, we evaluate it on two realistic benchmarks: a multitask version of the CIFAR-100 image classification dataset and a large-scale multilingual language modeling experiment. Our empirical results show that L-DRO achieves a better trade-off between average and worst-case accuracy with little computational overhead compared to several strong baselines.",
            "corpus_id": "238634327",
            "text": "When training and evaluating machine learning models on a large number of tasks, it is important to not only look at average task accuracy -- which may be biased by easy or redundant tasks -- but also worst-case accuracy (i.e. the performance on the task with the lowest accuracy). In this work, we show how to use techniques from the distributionally robust optimization (DRO) literature to improve worst-case performance in multitask learning. We highlight several failure cases of DRO when applied off-the-shelf and present an improved method, Lookahead-DRO (L-DRO), which mitigates these issues. The core idea of L-DRO is to anticipate the interaction between tasks during training in order to choose a dynamic re-weighting of the various task losses, which will (i) lead to minimal worst-case loss and (ii) train on as many tasks as possible. After demonstrating the efficacy of L-DRO on a small controlled synthetic setting, we evaluate it on two realistic benchmarks: a multitask version of the CIFAR-100 image classification dataset and a large-scale multilingual language modeling experiment. Our empirical results show that L-DRO achieves a better trade-off between average and worst-case accuracy with little computational overhead compared to several strong baselines.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.134521484375
        }
    ],
    "quotes": {
        "cost": 0.22960799999999998,
        "quotes": [
            {
                "idx": 0,
                "key": "[236034024 | Magee et al. | 2021 | Citations: 33]",
                "snippets": "Model-driven approaches treat the language model post-training. For models involving word embeddings, these embeddings can be inspected, and following a debiasing heuristic, modified directly. [10] for example conducts a component analysis to identify bias, and then manipulates embedding values, to either increase or decrease cosine values between category terms in accordance with the analysis. Such techniques appear more difficult to apply to transformer models with contextual embeddings. [35] for example examines both data augmentation and model neutralisation, and finds model neutralisation less effective.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Bias Mitigation",
                        "pdf_hash": "",
                        "start": 1105,
                        "end": 1721,
                        "sentence_offsets": [
                            {
                                "start": 1105,
                                "end": 1168
                            },
                            {
                                "start": 1169,
                                "end": 1297
                            },
                            {
                                "start": 1298,
                                "end": 1502
                            },
                            {
                                "start": 1503,
                                "end": 1599
                            },
                            {
                                "start": 1600,
                                "end": 1721
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Model-driven approaches treat the language model post-training. For models involving word embeddings, these embeddings can be inspected, and following a debiasing heuristic, modified directly. [10] for example conducts a component analysis to identify bias, and then manipulates embedding values, to either increase or decrease cosine values between category terms in accordance with the analysis. Such techniques appear more difficult to apply to transformer models with contextual embeddings. [35] for example examines both data augmentation and model neutralisation, and finds model neutralisation less effective."
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[237442178 | Smith et al. | 2021 | Citations: 28]",
                "snippets": "We investigate several sources of bias in large dialogue models, and employ a few methods to reduce bias by gender and race/ethnicity in Blender-Bot while maintaining similar levels of performance. We find that the name-scrambling model most effectively reduces bias as measured by a gender bias classifier and most closely matches the token distribution across female/male self-chats; the controlled-generation model is rated by crowdsourced workers as being most preferable and most human; and the unlikelihood-training model has the lowest gender-classifier bias on a downstream task where one of the speakers explicitly states their gender. \n\nPractitioners should choose mitigation techniques that match their use case: the namescrambling method might be most applicable for a model where invariance to names is of most essential importance (e.g., in task oriented dialogue, when scheduling meetings with individuals); the controlled-generation method might be ideal for reducing bias on dialogue while ensuring no loss of performance; and the unlikelihoodtraining method may be best for suppressing gender bias on more than just names in a generalizable fashion.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Conclusion",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1167,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 197
                            },
                            {
                                "start": 198,
                                "end": 644
                            },
                            {
                                "start": 647,
                                "end": 1167
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "We investigate several sources of bias in large dialogue models, and employ a few methods to reduce bias by gender and race/ethnicity in Blender-Bot while maintaining similar levels of performance. We find that the name-scrambling model most effectively reduces bias as measured by a gender bias classifier and most closely matches the token distribution across female/male self-chats; the controlled-generation model is rated by crowdsourced workers as being most preferable and most human; and the unlikelihood-training model has the lowest gender-classifier bias on a downstream task where one of the speakers explicitly states their gender. \n\nPractitioners should choose mitigation techniques that match their use case: the namescrambling method might be most applicable for a model where invariance to names is of most essential importance (e.g., in task oriented dialogue, when scheduling meetings with individuals); the controlled-generation method might be ideal for reducing bias on dialogue while ensuring no loss of performance; and the unlikelihoodtraining method may be best for suppressing gender bias on more than just names in a generalizable fashion."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[253734850 | Zayed et al. | 2022 | Citations: 22]",
                "snippets": "To mitigate gender bias in language models, several techniques have been proposed. These methods can be broadly classified into three main categories: data-based methods (Lu et al., 2018)(Maudslay et al., 2019)(De-Arteaga et al., 2019); regularization-based methods (Maudslay et al., 2019)(Garg et al., 2018); and adversarial-based methods (Song et al. 2019;(Manzini et al., 2019)(Isabelle et al., 2017). Data-based methods, which are our focus in this work, change the training data to balance the bias through targeted data augmentation (Lu et al., 2018)(Maudslay et al., 2019), constructing counterfactual examples (Garg et al., 2018), or removing protected attributes from the input (De-Arteaga et al., 2019) to disallow models from learning any correlation between labels and gender words. Regularization-based methods add an auxiliary loss term to the objective function that reduces the amount of bias in the model. Adversarial-based methods use adversarial learning for bias mitigation.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[102350941 | Manzini et al. | 2019 | Citations: 308]": "Online texts - across genres, registers, domains, and styles - are riddled with human stereotypes, expressed in overt or subtle ways. Word embeddings, trained on these texts, perpetuate and amplify these stereotypes, and propagate biases to machine learning models that use word embeddings as features. In this work, we propose a method to debias word embeddings in multiclass settings such as race and religion, extending the work of (Bolukbasi et al., 2016) from the binary setting, such as binary gender. Next, we propose a novel methodology for the evaluation of multiclass debiasing. We demonstrate that our multiclass debiasing is robust and maintains the efficacy in standard NLP tasks.",
                    "[202541569 | Maudslay et al. | 2019 | Citations: 172]": "This paper treats gender bias latent in word embeddings. Previous mitigation attempts rely on the operationalisation of gender bias as a projection over a linear subspace. An alternative approach is Counterfactual Data Augmentation (CDA), in which a corpus is duplicated and augmented to remove bias, e.g. by swapping all inherently-gendered words in the copy. We perform an empirical comparison of these approaches on the English Gigaword and Wikipedia, and find that whilst both successfully reduce direct bias and perform well in tasks which quantify embedding quality, CDA variants outperform projection-based methods at the task of drawing non-biased gender analogies by an average of 19% across both corpora. We propose two improvements to CDA: Counterfactual Data Substitution (CDS), a variant of CDA in which potentially biased text is randomly substituted to avoid duplication, and the Names Intervention, a novel name-pairing technique that vastly increases the number of words being treated. CDA/S with the Names Intervention is the only approach which is able to mitigate indirect gender bias: following debiasing, previously biased words are significantly less clustered according to gender (cluster purity is reduced by 49%), thus improving on the state-of-the-art for bias mitigation.",
                    "[5147501 | Isabelle et al. | 2017 | Citations: 175]": "Neural machine translation represents an exciting leap forward in translation quality. But what longstanding weaknesses does it resolve, and which remain? We address these questions with a challenge set approach to translation evaluation and error analysis. A challenge set consists of a small set of sentences, each hand-designed to probe a system\u2019s capacity to bridge a particular structural divergence between languages. To exemplify this approach, we present an English-French challenge set, and use it to analyze phrase-based and neural systems. The resulting analysis provides not only a more fine-grained picture of the strengths of neural systems, but also insight into which linguistic phenomena remain out of reach.",
                    "[51888520 | Lu et al. | 2018 | Citations: 357]": "We examine whether neural natural language processing (NLP) systems reflect historical biases in training data. We define a general benchmark to quantify gender bias in a variety of neural NLP tasks. Our empirical evaluation with state-of-the-art neural coreference resolution and textbook RNN-based language models trained on benchmark datasets finds significant gender bias in how models view occupations. We then mitigate bias with CDA: a generic methodology for corpus augmentation via causal interventions that breaks associations between gendered and gender-neutral words. We empirically show that CDA effectively decreases gender bias while preserving accuracy. We also explore the space of mitigation strategies with CDA, a prior approach to word embedding debiasing (WED), and their compositions. We show that CDA outperforms WED, drastically so when word embeddings are trained. For pre-trained embeddings, the two methods can be effectively composed. We also find that as training proceeds on the original data set with gradient descent the gender bias grows as the loss reduces, indicating that the optimization encourages bias; CDA mitigates this behavior.",
                    "[52880735 | Garg et al. | 2018 | Citations: 261]": "In this paper, we study counterfactual fairness in text classification, which asks the question: How would the prediction change if the sensitive attribute referenced in the example were different? Toxicity classifiers demonstrate a counterfactual fairness issue by predicting that \"Some people are gay\" is toxic while \"Some people are straight\" is nontoxic. We offer a metric, counterfactual token fairness (CTF), for measuring this particular form of fairness in text classifiers, and describe its relationship with group fairness. Further, we offer three approaches, blindness, counterfactual augmentation, and counterfactual logit pairing (CLP), for optimizing counterfactual token fairness during training, bridging the robustness and fairness literature. Empirically, we find that blindness and CLP address counterfactual token fairness. The methods do not harm classifier performance, and have varying tradeoffs with group fairness. These approaches, both for measurement and optimization, provide a new path forward for addressing fairness concerns in text classification.",
                    "[58006082 | De-Arteaga et al. | 2019 | Citations: 460]": "We present a large-scale study of gender bias in occupation classification, a task where the use of machine learning may lead to negative outcomes on peoples' lives. We analyze the potential allocation harms that can result from semantic representation bias. To do so, we study the impact on occupation classification of including explicit gender indicators---such as first names and pronouns---in different semantic representations of online biographies. Additionally, we quantify the bias that remains when these indicators are \"scrubbed,\" and describe proxy behavior that occurs in the absence of explicit gender indicators. As we demonstrate, differences in true positive rates between genders are correlated with existing gender imbalances in occupations, which may compound these imbalances."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 445,
                        "end": 1427,
                        "sentence_offsets": [
                            {
                                "start": 445,
                                "end": 527
                            },
                            {
                                "start": 528,
                                "end": 837
                            },
                            {
                                "start": 838,
                                "end": 1227
                            },
                            {
                                "start": 1228,
                                "end": 1355
                            },
                            {
                                "start": 1356,
                                "end": 1427
                            }
                        ],
                        "ref_mentions": [
                            "51888520",
                            "202541569",
                            "58006082",
                            "202541569",
                            "52880735",
                            "102350941",
                            "5147501",
                            "51888520",
                            "202541569",
                            "52880735",
                            "58006082"
                        ],
                        "quote": "To mitigate gender bias in language models, several techniques have been proposed. These methods can be broadly classified into three main categories: data-based methods (Lu et al., 2018)(Maudslay et al., 2019)(De-Arteaga et al., 2019); regularization-based methods (Maudslay et al., 2019)(Garg et al., 2018); and adversarial-based methods (Song et al. 2019;(Manzini et al., 2019)(Isabelle et al., 2017). Data-based methods, which are our focus in this work, change the training data to balance the bias through targeted data augmentation (Lu et al., 2018)(Maudslay et al., 2019), constructing counterfactual examples (Garg et al., 2018), or removing protected attributes from the input (De-Arteaga et al., 2019) to disallow models from learning any correlation between labels and gender words. Regularization-based methods add an auxiliary loss term to the objective function that reduces the amount of bias in the model. Adversarial-based methods use adversarial learning for bias mitigation."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[256390197 | Tokpo et al. | 2023 | Citations: 8]",
                "snippets": "We select three popular mitigation methods to represent all three types, namely Counterfactual Data Augmentation (CDA), Context-debias, and Sentdebias. Notice that these methods create debiased pretrained language models, as is illustrated in Figure 1. These models still need to be finetuned on a downstream task.\n\nCDA pretraining. The idea behind counterfactual data augmentation (Zmigrod et al., 2019;Lu et al., 2020) is to generate a counterfactual for each example in the training corpus by replacing attribute terms with their complimentary equivalent from the other demographic classes. For example, she will map to he in the case of binary gender. To mitigate intrinsic bias, this counterfactual augmentation has to be done as a pretraining step. Since CDA involves retraining the model, it is more resource-intensive compared to Sent-debias and Context-debias. We use the pretrained CDA models based on BERT and ALBERT from Webster et al. (2020) for our implementation.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Intrinsic bias mitigation techniques",
                        "pdf_hash": "",
                        "start": 247,
                        "end": 1225,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "We select three popular mitigation methods to represent all three types, namely Counterfactual Data Augmentation (CDA), Context-debias, and Sentdebias. Notice that these methods create debiased pretrained language models, as is illustrated in Figure 1. These models still need to be finetuned on a downstream task.\n\nCDA pretraining. The idea behind counterfactual data augmentation (Zmigrod et al., 2019;Lu et al., 2020) is to generate a counterfactual for each example in the training corpus by replacing attribute terms with their complimentary equivalent from the other demographic classes. For example, she will map to he in the case of binary gender. To mitigate intrinsic bias, this counterfactual augmentation has to be done as a pretraining step. Since CDA involves retraining the model, it is more resource-intensive compared to Sent-debias and Context-debias. We use the pretrained CDA models based on BERT and ALBERT from Webster et al. (2020) for our implementation."
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[258832694 | Zayed et al. | 2023 | Citations: 10]",
                "snippets": "While relatively less explored, post-processing bias mitigation methods modify the predictions of a biased model and generate a new set of less biased predictions.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Gender bias mitigation methods",
                        "pdf_hash": "",
                        "start": 1934,
                        "end": 2097,
                        "sentence_offsets": [
                            {
                                "start": 1934,
                                "end": 2097
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "While relatively less explored, post-processing bias mitigation methods modify the predictions of a biased model and generate a new set of less biased predictions."
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[259095603 | Thakur et al. | 2023 | Citations: 37]",
                "snippets": "While the majority of current state-of-the-art debiasing methods focus on changes to the training regime, in this paper, we propose data intervention strategies as a powerful yet simple technique to reduce gender bias in pre-trained models. Specifically, we empirically show that by fine-tuning a pre-trained model on only 10 debiased (intervened) training examples, the tendency to favor any gender is significantly reduced. Since our proposed method only needs a few training examples, we argue that our few-shot de-biasing approach is highly feasible and practical. Through extensive experimentation, we show that our de-biasing technique performs better than competitive state-of-the-art baselines with minimal loss in language modeling ability.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 411,
                        "end": 1160,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "While the majority of current state-of-the-art debiasing methods focus on changes to the training regime, in this paper, we propose data intervention strategies as a powerful yet simple technique to reduce gender bias in pre-trained models. Specifically, we empirically show that by fine-tuning a pre-trained model on only 10 debiased (intervened) training examples, the tendency to favor any gender is significantly reduced. Since our proposed method only needs a few training examples, we argue that our few-shot de-biasing approach is highly feasible and practical. Through extensive experimentation, we show that our de-biasing technique performs better than competitive state-of-the-art baselines with minimal loss in language modeling ability."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[259342087 | Li et al. | 2023 | Citations: 30]",
                "snippets": "Most existing debiasing techniques first need to construct sample pairs using Counterfactual Data Augmentation (CDA) (Zmigrod et al., 2019)(Wang et al., 2022) to balance the training corpora. The general approach of CDA is to replace the original corpus with attribute words (e.g., he/she, man/woman) specific to different demographic groups. For example, RCDA (Chen et al., 2021) uses a generator to generate a large number of antisense sentences and then uses a discriminator to evaluate the quality of the original and antisense samples jointly. FairFil (Cheng et al., 2021) obtains a pair of positive sample sentences by replacing the attribute words in the training corpora with the antonyms and then uses contrastive learning to train a filter for debiasing. Auto-Debias (Guo et al., 2022) uses pairs of attribute words as training corpora, amplifies the bias between sample pairs by searching biased prompt texts in the Wikipedia vocabulary, and then performs semantic alignment using Jensen-Shannon divergence. These methods aim to mitigate social biases between different demographic groups by narrowing the representation distance between sample pairs.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[248780440 | Guo et al. | 2022 | Citations: 167]": "Human-like biases and undesired social stereotypes exist in large pretrained language models. Given the wide adoption of these models in real-world applications, mitigating such biases has become an emerging and important task. In this paper, we propose an automatic method to mitigate the biases in pretrained language models. Different from previous debiasing work that uses external corpora to fine-tune the pretrained models, we instead directly probe the biases encoded in pretrained models through prompts. Specifically, we propose a variant of the beam search method to automatically search for biased prompts such that the cloze-style completions are the most different with respect to different demographic groups. Given the identified biased prompts, we then propose a distribution alignment loss to mitigate the biases. Experiment results on standard datasets and metrics show that our proposed Auto-Debias approach can significantly reduce biases, including gender and racial bias, in pretrained language models such as BERT, RoBERTa and ALBERT. Moreover, the improvement in fairness does not decrease the language models\u2019 understanding abilities, as shown using the GLUE benchmark.",
                    "[184486914 | Zmigrod et al. | 2019 | Citations: 283]": "Gender stereotypes are manifest in most of the world\u2019s languages and are consequently propagated or amplified by NLP systems. Although research has focused on mitigating gender stereotypes in English, the approaches that are commonly employed produce ungrammatical sentences in morphologically rich languages. We present a novel approach for converting between masculine-inflected and feminine-inflected sentences in such languages. For Spanish and Hebrew, our approach achieves F1 scores of 82% and 73% at the level of tags and accuracies of 90% and 87% at the level of forms. By evaluating our approach using four different languages, we show that, on average, it reduces gender stereotyping by a factor of 2.5 without any sacrifice to grammaticality.",
                    "[232185104 | Cheng et al. | 2021 | Citations: 104]": "Pretrained text encoders, such as BERT, have been applied increasingly in various natural language processing (NLP) tasks, and have recently demonstrated significant performance gains. However, recent studies have demonstrated the existence of social bias in these pretrained NLP models. Although prior works have made progress on word-level debiasing, improved sentence-level fairness of pretrained encoders still lacks exploration. In this paper, we proposed the first neural debiasing method for a pretrained sentence encoder, which transforms the pretrained encoder outputs into debiased representations via a fair filter (FairFil) network. To learn the FairFil, we introduce a contrastive learning framework that not only minimizes the correlation between filtered embeddings and bias words but also preserves rich semantic information of the original sentences. On real-world datasets, our FairFil effectively reduces the bias degree of pretrained text encoders, while continuously showing desirable performance on downstream tasks. Moreover, our post-hoc method does not require any retraining of the text encoders, further enlarging FairFil's application space.",
                    "[247155039 | Wang et al. | 2022 | Citations: 87]": "This paper focuses on the Data Augmentation for low-resource Natural Language Understanding (NLU) tasks. We propose Prompt-based Data Augmentation model (PromDA) which only trains small-scale Soft Prompt (i.e., a set of trainable vectors) in the frozen Pre-trained Language Models (PLMs). This avoids human effort in collecting unlabeled in-domain data and maintains the quality of generated synthetic data. In addition, PromDA generates synthetic data via two different views and filters out the low-quality data using NLU models. Experiments on four benchmarks show that synthetic data produced by PromDA successfully boost up the performance of NLU models which consistently outperform several competitive baseline models, including a state-of-the-art semi-supervised model using unlabeled in-domain data. The synthetic data from PromDA are also complementary with unlabeled in-domain data. The NLU models can be further improved when they are combined for training."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 832,
                        "end": 1993,
                        "sentence_offsets": [],
                        "ref_mentions": [
                            "184486914",
                            "247155039",
                            "232185104",
                            "248780440"
                        ],
                        "quote": "Most existing debiasing techniques first need to construct sample pairs using Counterfactual Data Augmentation (CDA) (Zmigrod et al., 2019)(Wang et al., 2022) to balance the training corpora. The general approach of CDA is to replace the original corpus with attribute words (e.g., he/she, man/woman) specific to different demographic groups. For example, RCDA (Chen et al., 2021) uses a generator to generate a large number of antisense sentences and then uses a discriminator to evaluate the quality of the original and antisense samples jointly. FairFil (Cheng et al., 2021) obtains a pair of positive sample sentences by replacing the attribute words in the training corpora with the antonyms and then uses contrastive learning to train a filter for debiasing. Auto-Debias (Guo et al., 2022) uses pairs of attribute words as training corpora, amplifies the bias between sample pairs by searching biased prompt texts in the Wikipedia vocabulary, and then performs semantic alignment using Jensen-Shannon divergence. These methods aim to mitigate social biases between different demographic groups by narrowing the representation distance between sample pairs."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[262828449 | Lee et al. | 2023 | Citations: 18]",
                "snippets": "\u2022 Feature Clipping. Wang et al. (Wang et al., 2021) propose a post-processing mitigation method that clips/prunes the dimensions of feature embeddings that are highly correlated with gender information. This idea is motivated by the fact that an unbiased retrieve implies the independence between the covariates (active features) and sensitive attributes (gender). However, the limitation of this approach is that it results in unavoidable performance degradation in the main task performance. \u2022 Instruction/Prompt Engineering. Friedrich et al. [62] propose a post-processing mitigation approach, Fair Diffusion, that is inspired by advances in instructing AI systems based on human feedback. Fair Diffusion \"instructs\" the pre-trained diffusion models to be fair during the deployment stage; it enables precise guidance to reduce biases in model outcomes based on pre-defined instructions stored in a lookup table. \u2022 Bias vector projection. Chuang et al. [38] de-bias VL foundation models by projecting out biased directions (gender bias direction, racial bias direction) in the text embedding. These bias directions are obtained from embeddings of spurious prompts such as \"a photo of a [irrelevant attribute]\". The authors also calibrate the projection matrix by introducing one additional regularization constraint that ensures the debiased prompt representation still has the same semantic meaning after the projection. To elaborate, \"a photo of a [class name] with [spurious attribute]\" should still have the same semantic meaning as \"a photo of a [class name]\". The biggest advantage of this method is that it is computationally lightweight and simple, as it only manipulates text embedding.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[237490811 | Wang et al. | 2021 | Citations: 95]": "Internet search affects people\u2019s cognition of the world, so mitigating biases in search results and learning fair models is imperative for social good. We study a unique gender bias in image search in this work: the search images are often gender-imbalanced for gender-neutral natural language queries. We diagnose two typical image search models, the specialized model trained on in-domain datasets and the generalized representation model pre-trained on massive image and text data across the internet. Both models suffer from severe gender bias. Therefore, we introduce two novel debiasing approaches: an in-processing fair sampling method to address the gender imbalance issue for training models, and a post-processing feature clipping method base on mutual information to debias multimodal representations of pre-trained models. Extensive experiments on MS-COCO and Flickr30K benchmarks show that our methods significantly reduce the gender bias in image search models."
                },
                "metadata": [
                    {
                        "section_title": "4.4.2",
                        "pdf_hash": "",
                        "start": 27,
                        "end": 1711,
                        "sentence_offsets": [
                            {
                                "start": 27,
                                "end": 46
                            },
                            {
                                "start": 47,
                                "end": 215
                            },
                            {
                                "start": 216,
                                "end": 377
                            },
                            {
                                "start": 378,
                                "end": 506
                            },
                            {
                                "start": 507,
                                "end": 540
                            },
                            {
                                "start": 541,
                                "end": 705
                            },
                            {
                                "start": 706,
                                "end": 928
                            },
                            {
                                "start": 929,
                                "end": 954
                            },
                            {
                                "start": 955,
                                "end": 1108
                            },
                            {
                                "start": 1109,
                                "end": 1226
                            },
                            {
                                "start": 1227,
                                "end": 1437
                            },
                            {
                                "start": 1438,
                                "end": 1581
                            },
                            {
                                "start": 1582,
                                "end": 1711
                            }
                        ],
                        "ref_mentions": [
                            "237490811"
                        ],
                        "quote": "\u2022 Feature Clipping. Wang et al. (Wang et al., 2021) propose a post-processing mitigation method that clips/prunes the dimensions of feature embeddings that are highly correlated with gender information. This idea is motivated by the fact that an unbiased retrieve implies the independence between the covariates (active features) and sensitive attributes (gender). However, the limitation of this approach is that it results in unavoidable performance degradation in the main task performance. \u2022 Instruction/Prompt Engineering. Friedrich et al. [62] propose a post-processing mitigation approach, Fair Diffusion, that is inspired by advances in instructing AI systems based on human feedback. Fair Diffusion \"instructs\" the pre-trained diffusion models to be fair during the deployment stage; it enables precise guidance to reduce biases in model outcomes based on pre-defined instructions stored in a lookup table. \u2022 Bias vector projection. Chuang et al. [38] de-bias VL foundation models by projecting out biased directions (gender bias direction, racial bias direction) in the text embedding. These bias directions are obtained from embeddings of spurious prompts such as \"a photo of a [irrelevant attribute]\". The authors also calibrate the projection matrix by introducing one additional regularization constraint that ensures the debiased prompt representation still has the same semantic meaning after the projection. To elaborate, \"a photo of a [class name] with [spurious attribute]\" should still have the same semantic meaning as \"a photo of a [class name]\". The biggest advantage of this method is that it is computationally lightweight and simple, as it only manipulates text embedding."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[264305744 | Dong et al. | 2023 | Citations: 11]",
                "snippets": "Co$^2$PT, an efficient and effective debias-while-prompt tuning method for mitigating biases via counterfactual contrastive prompt tuning on downstream tasks...We evaluate its effectiveness on bias mitigation and applicability to existing debiased upstream models, and investigate how the design of each component and the selection of hyperparameters impact both its bias reduction capabilities and downstream task performance...Without re-training the model, Co 2 PT is flexible to apply in order to mitigate different bias types in downstream applications. One can train different debiasing prompts to tackle different bias dimensions such as gender, race, and religion. Furthermore, these debiasing prompts can be applied to mitigate intersectional bias by simply combining the corresponding prompts in downstream tasks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Co$^2$PT, an efficient and effective debias-while-prompt tuning method for mitigating biases via counterfactual contrastive prompt tuning on downstream tasks",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    },
                    {
                        "section_title": "Conclusion and Future Work",
                        "pdf_hash": "",
                        "start": 105,
                        "end": 372,
                        "sentence_offsets": [
                            {
                                "start": 105,
                                "end": 372
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "We evaluate its effectiveness on bias mitigation and applicability to existing debiased upstream models, and investigate how the design of each component and the selection of hyperparameters impact both its bias reduction capabilities and downstream task performance"
                    },
                    {
                        "section_title": "Conclusion and Future Work",
                        "pdf_hash": "",
                        "start": 629,
                        "end": 1024,
                        "sentence_offsets": [
                            {
                                "start": 629,
                                "end": 758
                            },
                            {
                                "start": 759,
                                "end": 872
                            },
                            {
                                "start": 873,
                                "end": 1023
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Without re-training the model, Co 2 PT is flexible to apply in order to mitigate different bias types in downstream applications. One can train different debiasing prompts to tackle different bias dimensions such as gender, race, and religion. Furthermore, these debiasing prompts can be applied to mitigate intersectional bias by simply combining the corresponding prompts in downstream tasks."
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[264309222 | Hort et al. | 2022 | Citations: 8]",
                "snippets": "Post-processing bias methods mitigate bias after a model has been trained. To reduce gender bias, Bolukbasi et al. (Bolukbasi et al., 2016) proposed the following Hard Debiasing (HD) post-processing method: They first identified a gender subspace and then proceeded to neutralise it, ensuring that the performance on the evaluation tasks is maintained...This approach has however been criticised to have several shortcomings [59] and alternative approaches have been sought to overcome them. To this end, Dev and Phillips [29] debiased word embeddings by using a linear projection along the gender direction, whereas, Lauscher et al. (Lauscher et al., 2019) adjusted the linear projection by using an alternative projecting approach and a neural network to learn word vectors transformation. Instead, Kaneko and Bollegala [60] proposed the use of an autoencoder to remove biases from pre-trained word embeddings. Shin et al. [61] proposed a latent disentanglement method to obtain gender-neutralised word embeddings. Ravfogel et al. [30] presented an Iterative Null-space Projection (INLP) method. Instead of specifying a gender direction, as done by Bolukbasi et al. (Bolukbasi et al., 2016), INLP learns this direction with a linear classifier and removes it by iteratively projecting the word embeddings on their null-space. Kaneko and Bollegala (Kaneko et al., 2021) debiased pre-trained word embeddings with the use of dictionaries.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[1704893 | Bolukbasi et al. | 2016 | Citations: 3153]": "The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.",
                    "[202572693 | Lauscher et al. | 2019 | Citations: 64]": "Distributional word vectors have recently been shown to encode many of the human biases, most notably gender and racial biases, and models for attenuating such biases have consequently been proposed. However, existing models and studies (1) operate on under-specified and mutually differing bias definitions, (2) are tailored for a particular bias (e.g., gender bias) and (3) have been evaluated inconsistently and non-rigorously. In this work, we introduce a general framework for debiasing word embeddings. We operationalize the definition of a bias by discerning two types of bias specification: explicit and implicit. We then propose three debiasing models that operate on explicit or implicit bias specifications and that can be composed towards more robust debiasing. Finally, we devise a full-fledged evaluation framework in which we couple existing bias metrics with newly proposed ones. Experimental findings across three embedding methods suggest that the proposed debiasing models are robust and widely applicable: they often completely remove the bias both implicitly and explicitly without degradation of semantic information encoded in any of the input distributional spaces. Moreover, we successfully transfer debiasing models, by means of cross-lingual embedding spaces, and remove or attenuate biases in distributional word vector spaces of languages that lack readily available bias specifications.",
                    "[231698423 | Kaneko et al. | 2021 | Citations: 38]": "Word embeddings trained on large corpora have shown to encode high levels of unfair discriminatory gender, racial, religious and ethnic biases. In contrast, human-written dictionaries describe the meanings of words in a concise, objective and an unbiased manner. We propose a method for debiasing pre-trained word embeddings using dictionaries, without requiring access to the original training resources or any knowledge regarding the word embedding algorithms used. Unlike prior work, our proposed method does not require the types of biases to be pre-defined in the form of word lists, and learns the constraints that must be satisfied by unbiased word embeddings automatically from dictionary definitions of the words. Specifically, we learn an encoder to generate a debiased version of an input word embedding such that it (a) retains the semantics of the pre-trained word embedding, (b) agrees with the unbiased definition of the word according to the dictionary, and (c) remains orthogonal to the vector space spanned by any biased basis vectors in the pre-trained word embedding space. Experimental results on standard benchmark datasets show that the proposed method can accurately remove unfair biases encoded in pre-trained word embeddings, while preserving useful semantics."
                },
                "metadata": [
                    {
                        "section_title": "Debiasing word embeddings",
                        "pdf_hash": "",
                        "start": 469,
                        "end": 800,
                        "sentence_offsets": [
                            {
                                "start": 469,
                                "end": 543
                            },
                            {
                                "start": 544,
                                "end": 801
                            }
                        ],
                        "ref_mentions": [
                            "1704893"
                        ],
                        "quote": "Post-processing bias methods mitigate bias after a model has been trained. To reduce gender bias, Bolukbasi et al. (Bolukbasi et al., 2016) proposed the following Hard Debiasing (HD) post-processing method: They first identified a gender subspace and then proceeded to neutralise it, ensuring that the performance on the evaluation tasks is maintained"
                    },
                    {
                        "section_title": "Debiasing word embeddings",
                        "pdf_hash": "",
                        "start": 922,
                        "end": 1950,
                        "sentence_offsets": [
                            {
                                "start": 920,
                                "end": 1059
                            },
                            {
                                "start": 1060,
                                "end": 1340
                            },
                            {
                                "start": 1341,
                                "end": 1461
                            },
                            {
                                "start": 1462,
                                "end": 1565
                            },
                            {
                                "start": 1566,
                                "end": 1646
                            },
                            {
                                "start": 1647,
                                "end": 1856
                            },
                            {
                                "start": 1857,
                                "end": 1949
                            }
                        ],
                        "ref_mentions": [
                            "202572693",
                            "1704893",
                            "231698423"
                        ],
                        "quote": "This approach has however been criticised to have several shortcomings [59] and alternative approaches have been sought to overcome them. To this end, Dev and Phillips [29] debiased word embeddings by using a linear projection along the gender direction, whereas, Lauscher et al. (Lauscher et al., 2019) adjusted the linear projection by using an alternative projecting approach and a neural network to learn word vectors transformation. Instead, Kaneko and Bollegala [60] proposed the use of an autoencoder to remove biases from pre-trained word embeddings. Shin et al. [61] proposed a latent disentanglement method to obtain gender-neutralised word embeddings. Ravfogel et al. [30] presented an Iterative Null-space Projection (INLP) method. Instead of specifying a gender direction, as done by Bolukbasi et al. (Bolukbasi et al., 2016), INLP learns this direction with a linear classifier and removes it by iteratively projecting the word embeddings on their null-space. Kaneko and Bollegala (Kaneko et al., 2021) debiased pre-trained word embeddings with the use of dictionaries."
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[267770177 | Yan et al. | 2024 | Citations: 9]",
                "snippets": "Preprocessing techniques aim to detect and eliminate bias and unfairness early on, either within the dataset (Zmigrod et al., 2019)(Dinan et al., 2019)Abid et al., 2021;(Qian et al., 2022)(Ghanbarzadeh et al., 2023) or prompt (Mattern et al., 2022;(Fatemi et al., 2021)(Yang et al., 2022). In-training bias mitigation techniques focus on reducing bias and unfairness during model training, by adjusting model architecture (Bartl et al., 2020)(Han et al., 2021), modifying loss functions (Liu et al., 2019)Webster et al., 2020;(Ouyang et al., 2022)(Woo et al., 2023)(Park et al., 2023)(Zhou et al., 2023)(Li et al., 2023), or selectively updating parameters (Qian et al., 2022)Ranaldi et al., 2023;(Yu et al., 2023). \n\nIntraprocessing approaches alter decoding behavior (Saunders et al., 2021)Meade et al., 2023;(Kim et al., 2022)(Chung et al., 2023)(Hallinan et al., 2022) without additional training or fine-tuning. \n\nPost-processing techniques primarily adjust model outputs to address bias and unfairness, without directly accessing the model itself (He et al., 2021)(Tokpo et al., 2022)Majumder et al., 2022;Dhingra et al., 2023).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[259342087 | Li et al. | 2023 | Citations: 30]": "As the representation capability of Pre-trained Language Models (PLMs) improve, there is growing concern that they will inherit social biases from unprocessed corpora. Most previous debiasing techniques used Counterfactual Data Augmentation (CDA) to balance the training corpus. However, CDA slightly modifies the original corpus, limiting the representation distance between different demographic groups to a narrow range. As a result, the debiasing model easily fits the differences between counterfactual pairs, which affects its debiasing performance with limited text resources. In this paper, we propose an adversarial training-inspired two-stage debiasing model using Contrastive learning with Continuous Prompt Augmentation (named CCPA) to mitigate social biases in PLMs\u2019 encoding. In the first stage, we propose a data augmentation method based on continuous prompt tuning to push farther the representation distance between sample pairs along different demographic groups. In the second stage, we utilize contrastive learning to pull closer the representation distance between the augmented sample pairs and then fine-tune PLMs\u2019 parameters to get debiased encoding. Our approach guides the model to achieve stronger debiasing performance by adding difficulty to the training process. Extensive experiments show that CCPA outperforms baselines in terms of debiasing performance. Meanwhile, experimental results on the GLUE benchmark show that CCPA retains the language modeling capability of PLMs.",
                    "[259370743 | Zhou et al. | 2023 | Citations: 41]": "Demographic biases and social stereotypes are common in pretrained language models (PLMs), and a burgeoning body of literature focuses on removing the unwanted stereotypical associations from PLMs. However, when fine-tuning these bias-mitigated PLMs in downstream natural language processing (NLP) applications, such as sentiment classification, the unwanted stereotypical associations resurface or even get amplified. Since pretrain&fine-tune is a major paradigm in NLP applications, separating the debiasing procedure of PLMs from fine-tuning would eventually harm the actual downstream utility. In this paper, we propose a unified debiasing framework Causal-Debias to remove unwanted stereotypical associations in PLMs during fine-tuning. Specifically, CausalDebias mitigates bias from a causal invariant perspective by leveraging the specific downstream task to identify bias-relevant and labelrelevant factors. We propose that bias-relevant factors are non-causal as they should have little impact on downstream tasks, while labelrelevant factors are causal. We perform interventions on non-causal factors in different demographic groups and design an invariant risk minimization loss to mitigate bias while maintaining task performance. Experimental results on three downstream tasks show that our proposed method can remarkably reduce unwanted stereotypical associations after PLMs are finetuned, while simultaneously minimizing the impact on PLMs and downstream applications.",
                    "[184486914 | Zmigrod et al. | 2019 | Citations: 283]": "Gender stereotypes are manifest in most of the world\u2019s languages and are consequently propagated or amplified by NLP systems. Although research has focused on mitigating gender stereotypes in English, the approaches that are commonly employed produce ungrammatical sentences in morphologically rich languages. We present a novel approach for converting between masculine-inflected and feminine-inflected sentences in such languages. For Spanish and Hebrew, our approach achieves F1 scores of 82% and 73% at the level of tags and accuracies of 90% and 87% at the level of forms. By evaluating our approach using four different languages, we show that, on average, it reduces gender stereotyping by a factor of 2.5 without any sacrifice to grammaticality.",
                    "[204838020 | Liu et al. | 2019 | Citations: 144]": "Recently there are increasing concerns about the fairness of Artificial Intelligence (AI) in real-world applications such as computer vision and recommendations. For example, recognition algorithms in computer vision are unfair to black people such as poorly detecting their faces and inappropriately identifying them as \u201cgorillas\u201d. As one crucial application of AI, dialogue systems have been extensively applied in our society. They are usually built with real human conversational data; thus they could inherit some fairness issues which are held in the real world. However, the fairness of dialogue systems has not been well investigated. In this paper, we perform a pioneering study about the fairness issues in dialogue systems. In particular, we construct a benchmark dataset and propose quantitative measures to understand fairness in dialogue models. Our studies demonstrate that popular dialogue models show significant prejudice towards different genders and races. Besides, to mitigate the bias in dialogue systems, we propose two simple but effective debiasing methods. Experiments show that our methods can reduce the bias in dialogue systems significantly. The dataset and the implementation are released to foster fairness research in dialogue systems.",
                    "[207852875 | Dinan et al. | 2019 | Citations: 208]": "Models often easily learn biases present in the training data, and their predictions directly reflect this bias. We analyze gender bias in dialogue data, and examine how this bias is actually amplified in subsequent generative chit-chat dialogue models. We measure gender bias in six existing dialogue datasets, and focus on the most biased one, the multi-player text-based fantasy adventure dataset LIGHT, as a testbed for our bias mitigation techniques. The LIGHT dataset is highly imbalanced with respect to gender, containing predominantly male characters, likely because it is entirely collected by crowdworkers and reflects common biases that exist in fantasy or medieval settings. We consider three techniques to mitigate gender bias: counterfactual data augmentation, targeted data collection, and bias controlled training. We show that our proposed techniques mitigate gender bias in LIGHT by balancing the genderedness of generated dialogue utterances and are particularly effective in combination. We quantify performance using various evaluation methods---such as quantity of gendered words, a dialogue safety classifier, and human studies---all of which show that our models generate less gendered, but equally engaging chit-chat responses.",
                    "[225094152 | Bartl et al. | 2020 | Citations: 125]": "Contextualized word embeddings have been replacing standard embeddings as the representational knowledge source of choice in NLP systems. Since a variety of biases have previously been found in standard word embeddings, it is crucial to assess biases encoded in their replacements as well. Focusing on BERT (Devlin et al., 2018), we measure gender bias by studying associations between gender-denoting target words and names of professions in English and German, comparing the findings with real-world workforce statistics. We mitigate bias by fine-tuning BERT on the GAP corpus (Webster et al., 2018), after applying Counterfactual Data Substitution (CDS) (Maudslay et al., 2019). We show that our method of measuring bias is appropriate for languages such as English, but not for languages with a rich morphology and gender-marking, such as German. Our results highlight the importance of investigating bias and mitigation techniques cross-linguistically,especially in view of the current emphasis on large-scale, multilingual language models.",
                    "[233240748 | Saunders et al. | 2021 | Citations: 28]": "Generating machine translations via beam search seeks the most likely output under a model. However, beam search has been shown to amplify demographic biases exhibited by a model. We aim to address this, focusing on gender bias resulting from systematic errors in grammatical gender translation. Almost all prior work on this problem adjusts the training data or the model itself. By contrast, our approach changes only the inference procedure. We constrain beam search to improve gender diversity in n-best lists, and rerank n-best lists using gender features obtained from the source sentence. Combining these strongly improves WinoMT gender translation accuracy for three language pairs without additional bilingual data or retraining. We also demonstrate our approach\u2019s utility for consistently gendering named entities, and its flexibility to handle new gendered language beyond the binary.",
                    "[237634972 | He et al. | 2021 | Citations: 29]": "Written language carries explicit and implicit biases that can distract from meaningful signals. For example, letters of reference may describe male and female candidates differently, or their writing style may indirectly reveal demographic characteristics. At best, such biases distract from the meaningful content of the text; at worst they can lead to unfair outcomes. We investigate the challenge of re-generating input sentences to 'neutralize' sensitive attributes while maintaining the semantic meaning of the original text (e.g. is the candidate qualified?). We propose a gradient-based rewriting framework, Detect and Perturb to Neutralize (DEPEN), that first detects sensitive components and masks them for regeneration, then perturbs the generation model at decoding time under a neutralizing constraint that pushes the (predicted) distribution of sensitive attributes towards a uniform distribution. Our experiments in two different scenarios show that DEPEN can regenerate fluent alternatives that are neutral in the sensitive attribute while maintaining the semantics of other attributes.",
                    "[238582879 | Fatemi et al. | 2021 | Citations: 34]": "Existing studies addressing gender bias of pre-trained language models, usually build a small gender-neutral data set and conduct a second phase pre-training on the model with such data. However, given the limited size and concentrated focus of the gender-neutral data, catastrophic forgetting would occur during second-phase pre-training. Forgetting information in the original training data may damage the model\u2019s downstream performance by a large margin. In this work, we empirically show that catastrophic forgetting occurs in such methods by evaluating them with general NLP tasks in GLUE. Then, we propose a new method, GEnder Equality Prompt (GEEP), to improve gender fairness of pre-trained models with less forgetting. GEEP freezes the pre-trained model and learns gender-related prompts with gender-neutral data.Empirical results show that GEEP not only achieves SOTA performances on gender fairness tasks, but also forgets less and performs better on GLUE by a large margin.",
                    "[246210255 | Tokpo et al. | 2022 | Citations: 34]": "It is well known that textual data on the internet and other digital platforms contain significant levels of bias and stereotypes. Various research findings have concluded that biased texts have significant effects on target demographic groups. For instance, masculine-worded job advertisements tend to be less appealing to female applicants. In this paper, we present a text-style transfer model that can be trained on non-parallel data and be used to automatically mitigate bias in textual data. Our style transfer model improves on the limitations of many existing text style transfer techniques such as the loss of content information. Our model solves such issues by combining latent content encoding with explicit keyword replacement. We will show that this technique produces better content preservation whilst maintaining good style transfer accuracy.",
                    "[246426909 | Ouyang et al. | 2022 | Citations: 13203]": "Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.",
                    "[247694107 | Han et al. | 2021 | Citations: 42]": "Group bias in natural language processing tasks manifests as disparities in system error rates across texts authorized by different demographic groups, typically disadvantaging minority groups. Dataset balancing has been shown to be effective at mitigating bias, however existing approaches do not directly account for correlations between author demographics and linguistic variables, limiting their effectiveness. To achieve Equal Opportunity fairness, such as equal job opportunity without regard to demographics, this paper introduces a simple, but highly effective, objective for countering bias using balanced training.We extend the method in the form of a gated model, which incorporates protected attributes as input, and show that it is effective at reducing bias in predictions through demographic input perturbation, outperforming all other bias mitigation techniques when combined with balanced training.",
                    "[249062690 | Qian et al. | 2022 | Citations: 54]": "Unwanted and often harmful social biases are becoming ever more salient in NLP research, affecting both models and datasets. In this work, we ask whether training on demographically perturbed data leads to fairer language models. We collect a large dataset of human annotated text perturbations and train a neural perturbation model, which we show outperforms heuristic alternatives. We find that (i) language models (LMs) pre-trained on demographically perturbed corpora are typically more fair, and (ii) LMs finetuned on perturbed GLUE datasets exhibit less demographic bias on downstream tasks, and (iii) fairness improvements do not come at the expense of performance on downstream tasks. Lastly, we discuss outstanding questions about how best to evaluate the (un)fairness of large language models. We hope that this exploration of neural demographic perturbation will help drive more improvement towards fairer NLP.",
                    "[252734135 | Hallinan et al. | 2022 | Citations: 40]": "Text detoxification has the potential to mitigate the harms of toxicity by rephrasing text to remove offensive meaning, but subtle toxicity remains challenging to tackle. We introduce MaRCo, a detoxification algorithm that combines controllable generation and text rewriting methods using a Product of Experts with autoencoder language models (LMs). MaRCo uses likelihoods under a non-toxic LM (expert) and a toxic LM (anti-expert) to find candidate words to mask and potentially replace. We evaluate our method on several subtle toxicity and microaggressions datasets, and show that it not only outperforms baselines on automatic metrics, but MaRCo\u2019s rewrites are preferred 2.1 times more in human evaluation. Its applicability to instances of subtle toxicity is especially promising, demonstrating a path forward for addressing increasingly elusive online hate.",
                    "[253446867 | Yang et al. | 2022 | Citations: 25]": "Several works have proven that finetuning is an applicable approach for debiasing contextualized word embeddings. Similarly, discrete prompts with semantic meanings have shown to be effective in debiasing tasks. With unfixed mathematical representation at the token level, continuous prompts usually surpass discrete ones at providing a pre-trained language model (PLM) with additional task-specific information. Despite this, relatively few efforts have been made to debias PLMs by prompt tuning with continuous prompts compared to its discrete counterpart. Furthermore, for most debiasing methods that alter a PLM's original parameters, a major problem is the need to not only decrease the bias in the PLM but also to ensure that the PLM does not lose its representation ability. Finetuning methods typically have a hard time maintaining this balance, as they tend to violently remove meanings of attribute words (like the words developing our concepts of \"male\" and \"female\" for gender), which also leads to an unstable and unpredictable training process. In this paper, we propose ADEPT, a method to debias PLMs using prompt tuning while maintaining the delicate balance between removing biases and ensuring representation ability. To achieve this, we propose a new training criterion inspired by manifold learning and equip it with an explicit debiasing term to optimize prompt tuning. In addition, we conduct several experiments with regard to the reliability, quality, and quantity of a previously proposed attribute training corpus in order to obtain a clearer prototype of a certain attribute, which indicates the attribute's position and relative distances to other words on the manifold. We evaluate ADEPT on several widely acknowledged debiasing benchmarks and downstream tasks, and find that it achieves competitive results while maintaining (and in some cases even improving) the PLM's representation ability. We further visualize words' correlation before and after debiasing a PLM, and give some possible explanations for the visible effects.",
                    "[254926596 | Kim et al. | 2022 | Citations: 36]": "Steering language generation towards objectives or away from undesired content has been a long-standing goal in utilizing language models (LM). Recent work has demonstrated reinforcement learning and weighted decoding as effective approaches to achieve a higher level of language control and quality with pros and cons. In this work, we propose a novel critic decoding method for controlled language generation (CriticControl) that combines the strengths of reinforcement learning and weighted decoding. Specifically, we adopt the actor-critic framework to train an LM-steering critic from non-differentiable reward models. And similar to weighted decoding, our method freezes the language model and manipulates the output token distribution using called critic, improving training efficiency and stability. Evaluation of our method on three controlled generation tasks, namely topic control, sentiment control, and detoxification, shows that our approach generates more coherent and well-controlled texts than previous methods. In addition, CriticControl demonstrates superior generalization ability in zero-shot settings. Human evaluation studies also corroborate our findings.",
                    "[257079734 | Park et al. | 2023 | Citations: 17]": "Leveraging pre-trained language models (PLMs) as initializers for efficient transfer learning has become a universal approach for text-related tasks. However, the models not only learn the language understanding abilities but also reproduce prejudices for certain groups in the datasets used for pre-training. Recent studies show that the biased knowledge acquired from the datasets affects the model predictions on downstream tasks. In this paper, we mitigate and analyze the gender biases in PLMs with coreference resolution, which is one of the natural language understanding (NLU) tasks. PLMs exhibit two types of gender biases: stereotype and skew. The primary causes for the biases are the imbalanced datasets with more male examples and the stereotypical examples on gender roles. While previous studies mainly focused on the skew problem, we aim to mitigate both gender biases in PLMs while maintaining the model's original linguistic capabilities. Our method employs two regularization terms, Stereotype Neutralization (SN) and Elastic Weight Consolidation (EWC). The models trained with the methods show to be neutralized and reduce the biases significantly on the WinoBias dataset compared to the public BERT. We also invented a new gender bias quantification metric called the Stereotype Quantification (SQ) score. In addition to the metrics, embedding visualizations were used to interpret how our methods have successfully debiased the models.",
                    "[258537309 | Woo et al. | 2023 | Citations: 11]": "Pre-trained language models (PLMs) learn gender bias from imbalances in human-written corpora. This bias leads to critical social issues when deploying PLMs in real-world scenarios. However, minimizing bias is limited by the trade-off due to the degradation of language modeling performance. It is particularly challenging to detach and remove biased representations in the embedding space because the learned linguistic knowledge entails bias. To address this problem, we propose a compensatory debiasing strategy to reduce gender bias while preserving linguistic knowledge. This strategy utilizes two types of sentences to distinguish biased knowledge: stereotype and non-stereotype sentences. We assign small angles and distances to pairs of representations of the two gender groups to mitigate bias for the stereotype sentences. At the same time, we maximize the agreement for the representations of the debiasing model and the original model to maintain linguistic knowledge for the non-stereotype sentences. To validate our approach, we measure the performance of the debiased model using the following evaluation metrics: SEAT, StereoSet, CrowS-Pairs, and GLUE. Our experimental results demonstrate that the model fine-tuned by our strategy has the lowest level of bias while retaining knowledge of PLMs.",
                    "[259096160 | Chung et al. | 2023 | Citations: 121]": "Large language models (LLMs) can be used to generate text data for training and evaluating other models. However, creating high-quality datasets with LLMs can be challenging. In this work, we explore human-AI partnerships to facilitate high diversity and accuracy in LLM-based text data generation. We first examine two approaches to diversify text generation: 1) logit suppression, which minimizes the generation of languages that have already been frequently generated, and 2) temperature sampling, which flattens the token sampling probability. We found that diversification approaches can increase data diversity but often at the cost of data accuracy (i.e., text and labels being appropriate for the target domain). To address this issue, we examined two human interventions, 1) label replacement (LR), correcting misaligned labels, and 2) out-of-scope filtering (OOSF), removing instances that are out of the user\u2019s domain of interest or to which no considered label applies. With oracle studies, we found that LR increases the absolute accuracy of models trained with diversified datasets by 14.4%. Moreover, we found that some models trained with data generated with LR interventions outperformed LLM-based few-shot classification. In contrast, OOSF was not effective in increasing model accuracy, implying the need for future work in human-in-the-loop text data generation.",
                    "[259859034 | Yu et al. | 2023 | Citations: 100]": ",",
                    "[259859044 | Ghanbarzadeh et al. | 2023 | Citations: 12]": "Recent studies have revealed that the widely-used Pre-trained Language Models (PLMs) propagate societal biases from the large unmoderated pre-training corpora. Existing solutions require debiasing training processes and datasets for debiasing, which are resource-intensive and costly. Furthermore, these methods hurt the PLMs' performance on downstream tasks. In this study, we propose Gender-tuning, which debiases the PLMs through fine-tuning on downstream tasks' datasets. For this aim, Gender-tuning integrates Masked Language Modeling (MLM) training objectives into fine-tuning's training process. Comprehensive experiments show that Gender-tuning outperforms the state-of-the-art baselines in terms of average gender bias scores in PLMs while improving PLMs' performance on downstream tasks solely using the downstream tasks' dataset. Also, Gender-tuning is a deployable debiasing tool for any PLM that works with original fine-tuning."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 234,
                        "end": 1359,
                        "sentence_offsets": [
                            {
                                "start": 117,
                                "end": 519
                            },
                            {
                                "start": 520,
                                "end": 939
                            },
                            {
                                "start": 942,
                                "end": 1137
                            },
                            {
                                "start": 1140,
                                "end": 1359
                            }
                        ],
                        "ref_mentions": [
                            "184486914",
                            "207852875",
                            "249062690",
                            "259859044",
                            "238582879",
                            "253446867",
                            "225094152",
                            "247694107",
                            "204838020",
                            "246426909",
                            "258537309",
                            "257079734",
                            "259370743",
                            "259342087",
                            "249062690",
                            "259859034",
                            "233240748",
                            "254926596",
                            "259096160",
                            "252734135",
                            "237634972",
                            "246210255"
                        ],
                        "quote": "Preprocessing techniques aim to detect and eliminate bias and unfairness early on, either within the dataset (Zmigrod et al., 2019)(Dinan et al., 2019)Abid et al., 2021;(Qian et al., 2022)(Ghanbarzadeh et al., 2023) or prompt (Mattern et al., 2022;(Fatemi et al., 2021)(Yang et al., 2022). In-training bias mitigation techniques focus on reducing bias and unfairness during model training, by adjusting model architecture (Bartl et al., 2020)(Han et al., 2021), modifying loss functions (Liu et al., 2019)Webster et al., 2020;(Ouyang et al., 2022)(Woo et al., 2023)(Park et al., 2023)(Zhou et al., 2023)(Li et al., 2023), or selectively updating parameters (Qian et al., 2022)Ranaldi et al., 2023;(Yu et al., 2023). \n\nIntraprocessing approaches alter decoding behavior (Saunders et al., 2021)Meade et al., 2023;(Kim et al., 2022)(Chung et al., 2023)(Hallinan et al., 2022) without additional training or fine-tuning. \n\nPost-processing techniques primarily adjust model outputs to address bias and unfairness, without directly accessing the model itself (He et al., 2021)(Tokpo et al., 2022)Majumder et al., 2022;Dhingra et al., 2023)."
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[268064051 | Doughman et al. | 2023 | Citations: 0]",
                "snippets": "Auto-Debias. Auto-Debias (Guo et al., 2022)) is a debiasing technique for masked language models that does not entail referencing external corpora. Auto-Debias contains two stages: First, automatically crafting biased prompts, such that the clozestyle completions have the highest disagreement in generating stereotype words with respect to demographic groups. Second, debiasing the language model by a distribution alignment loss...Auto-Debias illustrates a 90%-96% reduction in mean probability scores from base to debiased models, while only a 3%-16% reduction in mean normalized ranks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[248780440 | Guo et al. | 2022 | Citations: 167]": "Human-like biases and undesired social stereotypes exist in large pretrained language models. Given the wide adoption of these models in real-world applications, mitigating such biases has become an emerging and important task. In this paper, we propose an automatic method to mitigate the biases in pretrained language models. Different from previous debiasing work that uses external corpora to fine-tune the pretrained models, we instead directly probe the biases encoded in pretrained models through prompts. Specifically, we propose a variant of the beam search method to automatically search for biased prompts such that the cloze-style completions are the most different with respect to different demographic groups. Given the identified biased prompts, we then propose a distribution alignment loss to mitigate the biases. Experiment results on standard datasets and metrics show that our proposed Auto-Debias approach can significantly reduce biases, including gender and racial bias, in pretrained language models such as BERT, RoBERTa and ALBERT. Moreover, the improvement in fairness does not decrease the language models\u2019 understanding abilities, as shown using the GLUE benchmark."
                },
                "metadata": [
                    {
                        "section_title": "Debiasing Techniques",
                        "pdf_hash": "",
                        "start": 544,
                        "end": 973,
                        "sentence_offsets": [
                            {
                                "start": 544,
                                "end": 556
                            },
                            {
                                "start": 557,
                                "end": 690
                            },
                            {
                                "start": 691,
                                "end": 903
                            },
                            {
                                "start": 904,
                                "end": 974
                            }
                        ],
                        "ref_mentions": [
                            "248780440"
                        ],
                        "quote": "Auto-Debias. Auto-Debias (Guo et al., 2022)) is a debiasing technique for masked language models that does not entail referencing external corpora. Auto-Debias contains two stages: First, automatically crafting biased prompts, such that the clozestyle completions have the highest disagreement in generating stereotype words with respect to demographic groups. Second, debiasing the language model by a distribution alignment loss"
                    },
                    {
                        "quote": "Auto-Debias illustrates a 90%-96% reduction in mean probability scores from base to debiased models, while only a 3%-16% reduction in mean normalized ranks.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[268512691 | Lee et al. | 2024 | Citations: 0]",
                "snippets": "Post-processing mitigation methods adjust the probability distribution during the decoding phase to select tokens with less bias, using methods such as adjusting, filtering, or inserting tokens [31]. Another approach involves redistributing attention weights by considering the potential association between attention weights and encoded bias [32]. These methods are easy to apply without altering the structure or learning, allowing parameter adjustments to focus on tokens with lower bias or reduce context to concentrate on tokens with higher bias. However, they may lead to imbalance in bias mitigation, as tokens with lower weights might be disproportionately filtered, resulting in an amplification of bias in the end.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[253734850 | Zayed et al. | 2022 | Citations: 22]": "Data-driven predictive solutions predominant in commercial applications tend to suffer from biases and stereotypes, which raises equity concerns. Prediction models may discover, use, or amplify spurious correlations based on gender or other protected personal characteristics, thus discriminating against marginalized groups. Mitigating gender bias has become an important research focus in natural language processing (NLP) and is an area where annotated corpora are available. Data augmentation reduces gender bias by adding counterfactual examples to the training dataset. In this work, we show that some of the examples in the augmented dataset can be not important or even harmful to fairness. We hence propose a general method for pruning both the factual and counterfactual examples to maximize the model\u2019s fairness as measured by the demographic parity, equality of opportunity, and equality of odds. The fairness achieved by our method surpasses that of data augmentation on three text classification datasets, using no more than half of the examples in the augmented dataset. Our experiments are conducted using models of varying sizes and pre-training settings. WARNING: This work uses language that is offensive in nature."
                },
                "metadata": [
                    {
                        "section_title": "Methods of mitigating social bias",
                        "pdf_hash": "",
                        "start": 1156,
                        "end": 1880,
                        "sentence_offsets": [
                            {
                                "start": 1062,
                                "end": 1192
                            },
                            {
                                "start": 1192,
                                "end": 1283
                            },
                            {
                                "start": 1283,
                                "end": 1394
                            },
                            {
                                "start": 1396,
                                "end": 1595
                            },
                            {
                                "start": 1595,
                                "end": 1743
                            },
                            {
                                "start": 1743,
                                "end": 1945
                            }
                        ],
                        "ref_mentions": [
                            "253734850"
                        ],
                        "quote": "Post-processing mitigation methods adjust the probability distribution during the decoding phase to select tokens with less bias, using methods such as adjusting, filtering, or inserting tokens [31]. Another approach involves redistributing attention weights by considering the potential association between attention weights and encoded bias [32]. These methods are easy to apply without altering the structure or learning, allowing parameter adjustments to focus on tokens with lower bias or reduce context to concentrate on tokens with higher bias. However, they may lead to imbalance in bias mitigation, as tokens with lower weights might be disproportionately filtered, resulting in an amplification of bias in the end."
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[268553687 | Cai et al. | 2024 | Citations: 5]",
                "snippets": "In this study, we integrate the processes of locating and mitigating bias within a unified framework. Initially, we use causal mediation analysis to trace the causal effects of different components' activation within a large language model. Building on this, we propose the LSDM (Least Square Debias Method), a knowledge-editing based method for mitigating gender bias in occupational pronouns, and compare it against two baselines on three gender bias datasets and seven knowledge competency test datasets...LSDM modifies parameters by solving a matrix equation with constraint terms, enabling us to minimize interference with other aspects of the model while specifically mitigating gender bias associated with certain occupation words. LSDM overcomes the catastrophic forgetting problem that exists in all other debiasing methods and stands out by avoiding additional reinforcement learning or human annotations, basing its approach on causal trace conclusions rather than just black-box fine-tuning.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "In this study, we integrate the processes of locating and mitigating bias within a unified framework. Initially, we use causal mediation analysis to trace the causal effects of different components' activation within a large language model. Building on this, we propose the LSDM (Least Square Debias Method), a knowledge-editing based method for mitigating gender bias in occupational pronouns, and compare it against two baselines on three gender bias datasets and seven knowledge competency test datasets",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    },
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 187,
                        "end": 682,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 233
                            },
                            {
                                "start": 233,
                                "end": 462
                            },
                            {
                                "start": 462,
                                "end": 726
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "LSDM modifies parameters by solving a matrix equation with constraint terms, enabling us to minimize interference with other aspects of the model while specifically mitigating gender bias associated with certain occupation words. LSDM overcomes the catastrophic forgetting problem that exists in all other debiasing methods and stands out by avoiding additional reinforcement learning or human annotations, basing its approach on causal trace conclusions rather than just black-box fine-tuning."
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[268709624 | Yu et al. | 2024 | Citations: 2]",
                "snippets": "Post-hoc-based method add a post-training step to these sentence representations before applied to downstream tasks, including removing the estimated genderdirection subspace from sentence representation (Liang et al., 2020), or use pre-defined word tuples combine specific techniques to debias text encoder for a fair sentence representation (Cheng et al., 2021). (2) Fine-tuning-based models use specific loss terms to guide a PLM to remove biases, including distribution alignment loss for debiasing embedding space (Guo et al., 2022); orthogonal loss aims to promote irrelevance between stereotyped words and gender-specific words (Kaneko et al., 2021), etc.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[248780440 | Guo et al. | 2022 | Citations: 167]": "Human-like biases and undesired social stereotypes exist in large pretrained language models. Given the wide adoption of these models in real-world applications, mitigating such biases has become an emerging and important task. In this paper, we propose an automatic method to mitigate the biases in pretrained language models. Different from previous debiasing work that uses external corpora to fine-tune the pretrained models, we instead directly probe the biases encoded in pretrained models through prompts. Specifically, we propose a variant of the beam search method to automatically search for biased prompts such that the cloze-style completions are the most different with respect to different demographic groups. Given the identified biased prompts, we then propose a distribution alignment loss to mitigate the biases. Experiment results on standard datasets and metrics show that our proposed Auto-Debias approach can significantly reduce biases, including gender and racial bias, in pretrained language models such as BERT, RoBERTa and ALBERT. Moreover, the improvement in fairness does not decrease the language models\u2019 understanding abilities, as shown using the GLUE benchmark.",
                    "[207996257 | Liang et al. | 2020 | Citations: 240]": "As natural language processing methods are increasingly deployed in real-world scenarios such as healthcare, legal systems, and social science, it becomes necessary to recognize the role they potentially play in shaping social biases and stereotypes. Previous work has revealed the presence of social biases in widely used word embeddings involving gender, race, religion, and other social constructs. While some methods were proposed to debias these word-level embeddings, there is a need to perform debiasing at the sentence-level given the recent shift towards new contextualized sentence representations such as ELMo and BERT. In this paper, we investigate the presence of social biases in sentence-level representations and propose a new method, Sent-Debias, to reduce these biases. We show that Sent-Debias is effective in removing biases, and at the same time, preserves performance on sentence-level downstream tasks such as sentiment analysis, linguistic acceptability, and natural language understanding. We hope that our work will inspire future research on characterizing and removing social biases from widely adopted sentence representations for fairer NLP.",
                    "[231698657 | Kaneko et al. | 2021 | Citations: 142]": "In comparison to the numerous debiasing methods proposed for the static non-contextualised word embeddings, the discriminative biases in contextualised embeddings have received relatively little attention. We propose a fine-tuning method that can be applied at token- or sentence-levels to debias pre-trained contextualised embeddings. Our proposed method can be applied to any pre-trained contextualised embedding model, without requiring to retrain those models. Using gender bias as an illustrative example, we then conduct a systematic study using several state-of-the-art (SoTA) contextualised representations on multiple benchmark datasets to evaluate the level of biases encoded in different contextualised embeddings before and after debiasing using the proposed method. We find that applying token-level debiasing for all tokens and across all layers of a contextualised embedding model produces the best performance. Interestingly, we observe that there is a trade-off between creating an accurate vs. unbiased contextualised embedding model, and different contextualised embedding models respond differently to this trade-off.",
                    "[232185104 | Cheng et al. | 2021 | Citations: 104]": "Pretrained text encoders, such as BERT, have been applied increasingly in various natural language processing (NLP) tasks, and have recently demonstrated significant performance gains. However, recent studies have demonstrated the existence of social bias in these pretrained NLP models. Although prior works have made progress on word-level debiasing, improved sentence-level fairness of pretrained encoders still lacks exploration. In this paper, we proposed the first neural debiasing method for a pretrained sentence encoder, which transforms the pretrained encoder outputs into debiased representations via a fair filter (FairFil) network. To learn the FairFil, we introduce a contrastive learning framework that not only minimizes the correlation between filtered embeddings and bias words but also preserves rich semantic information of the original sentences. On real-world datasets, our FairFil effectively reduces the bias degree of pretrained text encoders, while continuously showing desirable performance on downstream tasks. Moreover, our post-hoc method does not require any retraining of the text encoders, further enlarging FairFil's application space."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 468,
                        "end": 1144,
                        "sentence_offsets": [
                            {
                                "start": 382,
                                "end": 830
                            },
                            {
                                "start": 831,
                                "end": 1144
                            }
                        ],
                        "ref_mentions": [
                            "207996257",
                            "232185104",
                            "248780440",
                            "231698657"
                        ],
                        "quote": "Post-hoc-based method add a post-training step to these sentence representations before applied to downstream tasks, including removing the estimated genderdirection subspace from sentence representation (Liang et al., 2020), or use pre-defined word tuples combine specific techniques to debias text encoder for a fair sentence representation (Cheng et al., 2021). (2) Fine-tuning-based models use specific loss terms to guide a PLM to remove biases, including distribution alignment loss for debiasing embedding space (Guo et al., 2022); orthogonal loss aims to promote irrelevance between stereotyped words and gender-specific words (Kaneko et al., 2021), etc."
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[270370977 | Deng et al. | 2024 | Citations: 24]",
                "snippets": "Intra-processing Mitigation.These approaches modify a trained model's behavior without additional training to generate debiased predictions during inference.There are mainly four types of methods.The first method adds restrictions during token search decoding to prevent biased outputs [245; 194].The second method adjusts token distributions to enhance output diversity or sample less biased outputs [58; 95].The third method redistributes the model's attention to less stereotypical aspects [321].The last method implements standalone networks with original models for specific debiasing tasks, such as reducing gender or racial biases (Hauzenberger et al., 2022).\n\nPost-processing Mitigation.The techniques address bias in generated outputs, especially relevant for black-box models with inaccessible training data or internal processes.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[258461365 | Hauzenberger et al. | 2022 | Citations: 18]": "Societal biases are reflected in large pre-trained language models and their fine-tuned versions on downstream tasks. Common in-processing bias mitigation approaches, such as adversarial training and mutual information removal, introduce additional optimization criteria, and update the model to reach a new debiased state. However, in practice, end-users and practitioners might prefer to switch back to the original model, or apply debiasing only on a specific subset of protected attributes. To enable this, we propose a novel modular bias mitigation approach, consisting of stand-alone highly sparse debiasing subnetworks, where each debiasing module can be integrated into the core model on-demand at inference time. Our approach draws from the concept of \\emph{diff} pruning, and proposes a novel training regime adaptable to various representation disentanglement optimizations. We conduct experiments on three classification tasks with gender, race, and age as protected attributes. The results show that our modular approach, while maintaining task performance, improves (or at least remains on-par with) the effectiveness of bias mitigation in comparison with baseline finetuning. Particularly on a two-attribute dataset, our approach with separately learned debiasing subnetworks shows effective utilization of either or both the subnetworks for selective bias mitigation."
                },
                "metadata": [
                    {
                        "section_title": "Mitigation Strategy",
                        "pdf_hash": "",
                        "start": 1420,
                        "end": 2238,
                        "sentence_offsets": [
                            {
                                "start": 1420,
                                "end": 1448
                            },
                            {
                                "start": 1448,
                                "end": 1577
                            },
                            {
                                "start": 1577,
                                "end": 1616
                            },
                            {
                                "start": 1616,
                                "end": 1717
                            },
                            {
                                "start": 1717,
                                "end": 1830
                            },
                            {
                                "start": 1830,
                                "end": 1919
                            },
                            {
                                "start": 1919,
                                "end": 2064
                            },
                            {
                                "start": 2066,
                                "end": 2093
                            },
                            {
                                "start": 2093,
                                "end": 2238
                            }
                        ],
                        "ref_mentions": [
                            "258461365"
                        ],
                        "quote": "Intra-processing Mitigation.These approaches modify a trained model's behavior without additional training to generate debiased predictions during inference.There are mainly four types of methods.The first method adds restrictions during token search decoding to prevent biased outputs [245; 194].The second method adjusts token distributions to enhance output diversity or sample less biased outputs [58; 95].The third method redistributes the model's attention to less stereotypical aspects [321].The last method implements standalone networks with original models for specific debiasing tasks, such as reducing gender or racial biases (Hauzenberger et al., 2022).\n\nPost-processing Mitigation.The techniques address bias in generated outputs, especially relevant for black-box models with inaccessible training data or internal processes."
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[270380297 | Lee et al. | 2024 | Citations: 0]",
                "snippets": "Moreover, these methods have been used to identify biased decisions from models in specific tasks [13]. Recent studies have drawn significant attention to the bias caused by demographic terms in language models. Various techniques have been proposed to address and mitigate demographic bias, aiming to promote fairness, equality, and inclusivity. These techniques include debiasing models trained on human-annotated examples [10], fairness-aware neural language models [14], bias-mitigating transformer architectures [15], and debiasing frameworks to identify and correct demographic bias [16]. Additionally, post-hoc debiasing techniques have been explored, where a debiasing step is added to sentence representations after initial training, prior to their utilization in downstream tasks [10]17].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[1704893 | Bolukbasi et al. | 2016 | Citations: 3153]": "The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.",
                    "[19117312 | Zhang et al. | 2018 | Citations: 295]": "\n \n AI plays an increasingly prominent role in society since decisions that were once made by humans are now delegated to automated systems. These systems are currently in charge of deciding bank loans, criminals' incarceration, and the hiring of new employees, and it's not difficult to envision that they will in the future underpin most of the decisions in society. Despite the high complexity entailed by this task, there is still not much understanding of basic properties of such systems. For instance, we currently cannot detect (neither explain nor correct) whether an AI system can be deemed fair (i.e., is abiding by the decision-constraints agreed by society) or it is reinforcing biases and perpetuating a preceding prejudicial practice. Issues of discrimination have been discussed extensively in political and legal circles, but there exists still not much understanding of the formal conditions that a system must meet to be deemed fair. In this paper, we use the language of structural causality (Pearl, 2000) to fill in this gap. We start by introducing three new fine-grained measures of transmission of change from stimulus to effect, which we called counterfactual direct (Ctf-DE), indirect (Ctf-IE), and spurious (Ctf-SE) effects. We then derive what we call the causal explanation formula, which allows the AI designer to quantitatively evaluate fairness and explain the total observed disparity of decisions through different discriminatory mechanisms. We apply these measures to various discrimination analysis tasks and run extensive simulations, including detection, evaluation, and optimization of decision-making under fairness constraints. We conclude studying the trade-off between different types of fairness criteria (outcome and procedural), and provide a quantitative approach to policy implementation and the design of fair AI systems.\n \n",
                    "[198968250 | Wang et al. | 2018 | Citations: 338]": "Racial bias is an important issue in biometric, but has not been thoroughly studied in deep face recognition. In this paper, we first contribute a dedicated dataset called Racial Faces in-the-Wild (RFW) database, on which we firmly validated the racial bias of four commercial APIs and four state-of-the-art (SOTA) algorithms. Then, we further present the solution using deep unsupervised domain adaptation and propose a deep information maximization adaptation network (IMAN) to alleviate this bias by using Caucasian as source domain and other races as target domains. This unsupervised method simultaneously aligns global distribution to decrease race gap at domain-level, and learns the discriminative target representations at cluster level. A novel mutual information loss is proposed to further enhance the discriminative ability of network output without label information. Extensive experiments on RFW, GBU, and IJB-A databases show that IMAN successfully learns features that generalize well across different races and across different databases.",
                    "[235658325 | Czarnowska et al. | 2021 | Citations: 110]": "Abstract Measuring bias is key for better understanding and addressing unfairness in NLP/ML models. This is often done via fairness metrics, which quantify the differences in a model\u2019s behaviour across a range of demographic groups. In this work, we shed more light on the differences and similarities between the fairness metrics used in NLP. First, we unify a broad range of existing metrics under three generalized fairness metrics, revealing the connections between them. Next, we carry out an extensive empirical comparison of existing metrics and demonstrate that the observed differences in bias measurement can be systematically explained via differences in parameter choices for our generalized metrics.",
                    "[44090948 | Mohammad | 2018 | Citations: 499]": "Words play a central role in language and thought. Factor analysis studies have shown that the primary dimensions of meaning are valence, arousal, and dominance (VAD). We present the NRC VAD Lexicon, which has human ratings of valence, arousal, and dominance for more than 20,000 English words. We use Best\u2013Worst Scaling to obtain fine-grained scores and address issues of annotation consistency that plague traditional rating scale methods of annotation. We show that the ratings obtained are vastly more reliable than those in existing lexicons. We also show that there exist statistically significant differences in the shared understanding of valence, arousal, and dominance across demographic variables such as age, gender, and personality."
                },
                "metadata": [
                    {
                        "section_title": "II. RELATED WORK",
                        "pdf_hash": "",
                        "start": 789,
                        "end": 1587,
                        "sentence_offsets": [
                            {
                                "start": 761,
                                "end": 951
                            },
                            {
                                "start": 951,
                                "end": 1054
                            },
                            {
                                "start": 1054,
                                "end": 1161
                            },
                            {
                                "start": 1161,
                                "end": 1295
                            },
                            {
                                "start": 1295,
                                "end": 1542
                            },
                            {
                                "start": 1542,
                                "end": 1745
                            }
                        ],
                        "ref_mentions": [
                            "235658325",
                            "44090948",
                            "1704893",
                            "19117312",
                            "198968250"
                        ],
                        "quote": "Moreover, these methods have been used to identify biased decisions from models in specific tasks [13]. Recent studies have drawn significant attention to the bias caused by demographic terms in language models. Various techniques have been proposed to address and mitigate demographic bias, aiming to promote fairness, equality, and inclusivity. These techniques include debiasing models trained on human-annotated examples [10], fairness-aware neural language models [14], bias-mitigating transformer architectures [15], and debiasing frameworks to identify and correct demographic bias [16]. Additionally, post-hoc debiasing techniques have been explored, where a debiasing step is added to sentence representations after initial training, prior to their utilization in downstream tasks [10]17]."
                    }
                ]
            },
            {
                "idx": 17,
                "key": "[270869465 | Lim et al. | 2024 | Citations: 2]",
                "snippets": "We therefore developed a scoring methodology used to evaluate bias mitigation techniques in Large Language Models (LLMs) to offer a comparative analysis that highlights both the strengths and limitations of each approach. Each technique was assessed across multiple dimensions -transparency, cultural sensitivity, and effectiveness. The scores for these dimensions were derived through a combination of literature review, empirical findings, and qualitative evaluations of how these techniques perform in practical settings. For example, adversarial training received a high score for its effectiveness in reducing certain explicit biases but was marked lower for transparency due to the complexity and opacity of its implementation. In contrast, model auditing scored higher for transparency but displayed limitations in direct bias mitigation, particularly in subtle or implicit bias contexts.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "BACKGROUND AND LITERATURE REVIEW",
                        "pdf_hash": "",
                        "start": 240,
                        "end": 1135,
                        "sentence_offsets": [
                            {
                                "start": 240,
                                "end": 461
                            },
                            {
                                "start": 462,
                                "end": 572
                            },
                            {
                                "start": 573,
                                "end": 764
                            },
                            {
                                "start": 765,
                                "end": 973
                            },
                            {
                                "start": 974,
                                "end": 1135
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "We therefore developed a scoring methodology used to evaluate bias mitigation techniques in Large Language Models (LLMs) to offer a comparative analysis that highlights both the strengths and limitations of each approach. Each technique was assessed across multiple dimensions -transparency, cultural sensitivity, and effectiveness. The scores for these dimensions were derived through a combination of literature review, empirical findings, and qualitative evaluations of how these techniques perform in practical settings. For example, adversarial training received a high score for its effectiveness in reducing certain explicit biases but was marked lower for transparency due to the complexity and opacity of its implementation. In contrast, model auditing scored higher for transparency but displayed limitations in direct bias mitigation, particularly in subtle or implicit bias contexts."
                    }
                ]
            },
            {
                "idx": 18,
                "key": "[270878797 | Greco et al. | 2024 | Citations: 2]",
                "snippets": "To address biases in NLP, techniques can be developed that act at the three main stages of the NLP pipeline (Kozodoi et al., 2021)[75]: pre-processing (modifying training data), in-processing (imposing fairness constraints during model training), and post-processing (adjusting classifier predictions based on fairness metrics).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Bias mitigation for NLP",
                        "pdf_hash": "",
                        "start": 583,
                        "end": 892,
                        "sentence_offsets": [
                            {
                                "start": 583,
                                "end": 892
                            }
                        ],
                        "ref_mentions": [
                            "232092280"
                        ],
                        "quote": "To address biases in NLP, techniques can be developed that act at the three main stages of the NLP pipeline (Kozodoi et al., 2021)[75]: pre-processing (modifying training data), in-processing (imposing fairness constraints during model training), and post-processing (adjusting classifier predictions based on fairness metrics)."
                    }
                ]
            },
            {
                "idx": 19,
                "key": "[271404523 | Lu et al. | 2024 | Citations: 1]",
                "snippets": "Numerous studies have been conducted to mitigate the bias and toxicity inherent in LLMs (Zhao et al., 2018)(Barikeri et al., 2021)(Liang et al., 2020)(Ravfogel et al., 2020)(Schick et al., 2021).However, recent studies (Meade et al., 2021) empiri-cally show that these approaches are effective in reducing bias while compromising the language modeling performance, as indicated by increased perplexity on unbiased text.Notably, existing posthoc techniques manage to retain language modeling performance, but they fail to detect more subtle and implicit toxic content.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[239015827 | Meade et al. | 2021 | Citations: 128]": "Recent work has shown pre-trained language models capture social biases from the large amounts of text they are trained on. This has attracted attention to developing techniques that mitigate such biases. In this work, we perform an empirical survey of five recently proposed bias mitigation techniques: Counterfactual Data Augmentation (CDA), Dropout, Iterative Nullspace Projection, Self-Debias, and SentenceDebias. We quantify the effectiveness of each technique using three intrinsic bias benchmarks while also measuring the impact of these techniques on a model\u2019s language modeling ability, as well as its performance on downstream NLU tasks. We experimentally find that: (1) Self-Debias is the strongest debiasing technique, obtaining improved scores on all bias benchmarks; (2) Current debiasing techniques perform less consistently when mitigating non-gender biases; And (3) improvements on bias benchmarks such as StereoSet and CrowS-Pairs by using debiasing strategies are often accompanied by a decrease in language modeling ability, making it difficult to determine whether the bias mitigation was effective.",
                    "[207996257 | Liang et al. | 2020 | Citations: 240]": "As natural language processing methods are increasingly deployed in real-world scenarios such as healthcare, legal systems, and social science, it becomes necessary to recognize the role they potentially play in shaping social biases and stereotypes. Previous work has revealed the presence of social biases in widely used word embeddings involving gender, race, religion, and other social constructs. While some methods were proposed to debias these word-level embeddings, there is a need to perform debiasing at the sentence-level given the recent shift towards new contextualized sentence representations such as ELMo and BERT. In this paper, we investigate the presence of social biases in sentence-level representations and propose a new method, Sent-Debias, to reduce these biases. We show that Sent-Debias is effective in removing biases, and at the same time, preserves performance on sentence-level downstream tasks such as sentiment analysis, linguistic acceptability, and natural language understanding. We hope that our work will inspire future research on characterizing and removing social biases from widely adopted sentence representations for fairer NLP.",
                    "[215786522 | Ravfogel et al. | 2020 | Citations: 388]": "The ability to control for the kinds of information encoded in neural representation has a variety of use cases, especially in light of the challenge of interpreting these models. We present Iterative Null-space Projection (INLP), a novel method for removing information from neural representations. Our method is based on repeated training of linear classifiers that predict a certain property we aim to remove, followed by projection of the representations on their null-space. By doing so, the classifiers become oblivious to that target property, making it hard to linearly separate the data according to it. While applicable for multiple uses, we evaluate our method on bias and fairness use-cases, and show that our method is able to mitigate bias in word embeddings, as well as to increase fairness in a setting of multi-class classification.",
                    "[232075876 | Schick et al. | 2021 | Citations: 387]": "Abstract \u26a0 This paper contains prompts and model outputs that are offensive in nature. When trained on large, unfiltered crawls from the Internet, language models pick up and reproduce all kinds of undesirable biases that can be found in the data: They often generate racist, sexist, violent, or otherwise toxic language. As large models require millions of training examples to achieve good performance, it is difficult to completely prevent them from being exposed to such content. In this paper, we first demonstrate a surprising finding: Pretrained language models recognize, to a considerable degree, their undesirable biases and the toxicity of the content they produce. We refer to this capability as self-diagnosis. Based on this finding, we then propose a decoding algorithm that, given only a textual description of the undesired behavior, reduces the probability of a language model producing problematic text. We refer to this approach as self-debiasing. Self-debiasing does not rely on manually curated word lists, nor does it require any training data or changes to the model\u2019s parameters. While we by no means eliminate the issue of language models generating biased text, we believe our approach to be an important step in this direction.1",
                    "[235358955 | Barikeri et al. | 2021 | Citations: 183]": "Text representation models are prone to exhibit a range of societal biases, reflecting the non-controlled and biased nature of the underlying pretraining data, which consequently leads to severe ethical issues and even bias amplification. Recent work has predominantly focused on measuring and mitigating bias in pretrained language models. Surprisingly, the landscape of bias measurements and mitigation resources and methods for conversational language models is still very scarce: it is limited to only a few types of bias, artificially constructed resources, and completely ignores the impact that debiasing methods may have on the final perfor mance in dialog tasks, e.g., conversational response generation. In this work, we present REDDITBIAS, the first conversational data set grounded in the actual human conversations from Reddit, allowing for bias measurement and mitigation across four important bias dimensions: gender,race,religion, and queerness. Further, we develop an evaluation framework which simultaneously 1)measures bias on the developed REDDITBIAS resource, and 2)evaluates model capability in dialog tasks after model debiasing. We use the evaluation framework to benchmark the widely used conversational DialoGPT model along with the adaptations of four debiasing methods. Our results indicate that DialoGPT is biased with respect to religious groups and that some debiasing techniques can remove this bias while preserving downstream task performance.",
                    "[4952494 | Zhao et al. | 2018 | Citations: 943]": "In this paper, we introduce a new benchmark for co-reference resolution focused on gender bias, WinoBias. Our corpus contains Winograd-schema style sentences with entities corresponding to people referred by their occupation (e.g. the nurse, the doctor, the carpenter). We demonstrate that a rule-based, a feature-rich, and a neural coreference system all link gendered pronouns to pro-stereotypical entities with higher accuracy than anti-stereotypical entities, by an average difference of 21.1 in F1 score. Finally, we demonstrate a data-augmentation approach that, in combination with existing word-embedding debiasing techniques, removes the bias demonstrated by these systems in WinoBias without significantly affecting their performance on existing datasets."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 634,
                        "end": 1197,
                        "sentence_offsets": [
                            {
                                "start": 634,
                                "end": 825
                            },
                            {
                                "start": 825,
                                "end": 1049
                            },
                            {
                                "start": 1049,
                                "end": 1197
                            }
                        ],
                        "ref_mentions": [
                            "4952494",
                            "235358955",
                            "207996257",
                            "215786522",
                            "232075876",
                            "239015827"
                        ],
                        "quote": "Numerous studies have been conducted to mitigate the bias and toxicity inherent in LLMs (Zhao et al., 2018)(Barikeri et al., 2021)(Liang et al., 2020)(Ravfogel et al., 2020)(Schick et al., 2021).However, recent studies (Meade et al., 2021) empiri-cally show that these approaches are effective in reducing bias while compromising the language modeling performance, as indicated by increased perplexity on unbiased text.Notably, existing posthoc techniques manage to retain language modeling performance, but they fail to detect more subtle and implicit toxic content."
                    }
                ]
            },
            {
                "idx": 20,
                "key": "[271769656 | Wang et al. | 2024 | Citations: 1]",
                "snippets": "Post-hoc: Iterative null-space projection (INLP) (Ravfogel et al., 2020) trains a set of linear classifiers to predict genders from embeddings and then projects embeddings to the null-space of learned classifiers. Self-Debias (Schick et al., 2021) adjusts next token probabilities at each step according to model's prediction to what extent the next token is biased.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[215786522 | Ravfogel et al. | 2020 | Citations: 388]": "The ability to control for the kinds of information encoded in neural representation has a variety of use cases, especially in light of the challenge of interpreting these models. We present Iterative Null-space Projection (INLP), a novel method for removing information from neural representations. Our method is based on repeated training of linear classifiers that predict a certain property we aim to remove, followed by projection of the representations on their null-space. By doing so, the classifiers become oblivious to that target property, making it hard to linearly separate the data according to it. While applicable for multiple uses, we evaluate our method on bias and fairness use-cases, and show that our method is able to mitigate bias in word embeddings, as well as to increase fairness in a setting of multi-class classification.",
                    "[232075876 | Schick et al. | 2021 | Citations: 387]": "Abstract \u26a0 This paper contains prompts and model outputs that are offensive in nature. When trained on large, unfiltered crawls from the Internet, language models pick up and reproduce all kinds of undesirable biases that can be found in the data: They often generate racist, sexist, violent, or otherwise toxic language. As large models require millions of training examples to achieve good performance, it is difficult to completely prevent them from being exposed to such content. In this paper, we first demonstrate a surprising finding: Pretrained language models recognize, to a considerable degree, their undesirable biases and the toxicity of the content they produce. We refer to this capability as self-diagnosis. Based on this finding, we then propose a decoding algorithm that, given only a textual description of the undesired behavior, reduces the probability of a language model producing problematic text. We refer to this approach as self-debiasing. Self-debiasing does not rely on manually curated word lists, nor does it require any training data or changes to the model\u2019s parameters. While we by no means eliminate the issue of language models generating biased text, we believe our approach to be an important step in this direction.1"
                },
                "metadata": [
                    {
                        "section_title": "Experiments",
                        "pdf_hash": "",
                        "start": 1112,
                        "end": 1478,
                        "sentence_offsets": [
                            {
                                "start": 1110,
                                "end": 1325
                            },
                            {
                                "start": 1326,
                                "end": 1478
                            }
                        ],
                        "ref_mentions": [
                            "215786522",
                            "232075876"
                        ],
                        "quote": "Post-hoc: Iterative null-space projection (INLP) (Ravfogel et al., 2020) trains a set of linear classifiers to predict genders from embeddings and then projects embeddings to the null-space of learned classifiers. Self-Debias (Schick et al., 2021) adjusts next token probabilities at each step according to model's prediction to what extent the next token is biased."
                    }
                ]
            },
            {
                "idx": 21,
                "key": "[272599907 | Peng et al. | 2024 | Citations: 15]",
                "snippets": "During intra-processing, models are tweaked at the inference stage without retraining. Model editing enables targeted updates to model behavior, ensuring that biases in specific areas are corrected without affecting overall model performance [140,141]. Decoding modification like DEXPERTS directly affects text generation by adjusting token probabilities. DEXPERTS uses two models, one to promote non-toxic text and another to discourage harmful content, to improving output fairness [142].\n\nPost-processing methods focus on modifying the model's outputs.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Bias Mitigation Strategies",
                        "pdf_hash": "",
                        "start": 1353,
                        "end": 1908,
                        "sentence_offsets": [
                            {
                                "start": 1287,
                                "end": 1432
                            },
                            {
                                "start": 1433,
                                "end": 1664
                            },
                            {
                                "start": 1667,
                                "end": 1753
                            },
                            {
                                "start": 1754,
                                "end": 1919
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "During intra-processing, models are tweaked at the inference stage without retraining. Model editing enables targeted updates to model behavior, ensuring that biases in specific areas are corrected without affecting overall model performance [140,141]. Decoding modification like DEXPERTS directly affects text generation by adjusting token probabilities. DEXPERTS uses two models, one to promote non-toxic text and another to discourage harmful content, to improving output fairness [142].\n\nPost-processing methods focus on modifying the model's outputs."
                    }
                ]
            },
            {
                "idx": 22,
                "key": "[273501862 | Wu et al. | 2024 | Citations: 9]",
                "snippets": "Causal-Debias (Zhou et al., 2023). The Causal-Debias framework offers a pioneering solution to mitigating biases in pretrained language models (PLMs) by integrating causal learning principles into the fine-tuning process, which could be directly applied to the pre-training processes. Unlike conventional methods that separate bias mitigation and task performance optimization, Causal-Debias merges these goals through the use of causal interventions and invariant risk minimization (IRM). The framework effectively distinguishes between causal (label-relevant) and non-causal (bias-related) factors embedded in token representations, addressing spurious correlations resulting from demographic biases and social stereotypes. By generating counterfactual data-modifying bias-related attributes such as gendered or racial terms-and training the model with an invariant loss across diverse environments, Causal-Debias ensures that the model generalizes well across tasks while mitigating biases. This approach directly tackles the challenge of bias resurgence, where previously mitigated biases reappear during fine-tuning, a limitation observed in many existing debiasing techniques.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[259370743 | Zhou et al. | 2023 | Citations: 41]": "Demographic biases and social stereotypes are common in pretrained language models (PLMs), and a burgeoning body of literature focuses on removing the unwanted stereotypical associations from PLMs. However, when fine-tuning these bias-mitigated PLMs in downstream natural language processing (NLP) applications, such as sentiment classification, the unwanted stereotypical associations resurface or even get amplified. Since pretrain&fine-tune is a major paradigm in NLP applications, separating the debiasing procedure of PLMs from fine-tuning would eventually harm the actual downstream utility. In this paper, we propose a unified debiasing framework Causal-Debias to remove unwanted stereotypical associations in PLMs during fine-tuning. Specifically, CausalDebias mitigates bias from a causal invariant perspective by leveraging the specific downstream task to identify bias-relevant and labelrelevant factors. We propose that bias-relevant factors are non-causal as they should have little impact on downstream tasks, while labelrelevant factors are causal. We perform interventions on non-causal factors in different demographic groups and design an invariant risk minimization loss to mitigate bias while maintaining task performance. Experimental results on three downstream tasks show that our proposed method can remarkably reduce unwanted stereotypical associations after PLMs are finetuned, while simultaneously minimizing the impact on PLMs and downstream applications."
                },
                "metadata": [
                    {
                        "section_title": "Debiased Token Embedding",
                        "pdf_hash": "",
                        "start": 378,
                        "end": 1545,
                        "sentence_offsets": [
                            {
                                "start": 378,
                                "end": 397
                            },
                            {
                                "start": 398,
                                "end": 647
                            },
                            {
                                "start": 648,
                                "end": 852
                            },
                            {
                                "start": 853,
                                "end": 1088
                            },
                            {
                                "start": 1089,
                                "end": 1356
                            },
                            {
                                "start": 1357,
                                "end": 1545
                            }
                        ],
                        "ref_mentions": [
                            "259370743"
                        ],
                        "quote": "Causal-Debias (Zhou et al., 2023). The Causal-Debias framework offers a pioneering solution to mitigating biases in pretrained language models (PLMs) by integrating causal learning principles into the fine-tuning process, which could be directly applied to the pre-training processes. Unlike conventional methods that separate bias mitigation and task performance optimization, Causal-Debias merges these goals through the use of causal interventions and invariant risk minimization (IRM). The framework effectively distinguishes between causal (label-relevant) and non-causal (bias-related) factors embedded in token representations, addressing spurious correlations resulting from demographic biases and social stereotypes. By generating counterfactual data-modifying bias-related attributes such as gendered or racial terms-and training the model with an invariant loss across diverse environments, Causal-Debias ensures that the model generalizes well across tasks while mitigating biases. This approach directly tackles the challenge of bias resurgence, where previously mitigated biases reappear during fine-tuning, a limitation observed in many existing debiasing techniques."
                    }
                ]
            },
            {
                "idx": 23,
                "key": "[273901329 | Mamta et al. | 2024 | Citations: 2]",
                "snippets": "There are attempts to mitigate bias using postprocessing methods, particularly in tabular datasets. These methods aim to adjust specific classification outcomes to improve metrics like equalized odds or equality of opportunity (Hardt et al., 2016). Madras et al. (2018) proposed LAFTR (Learning Adversarially Fair and Transferable Representations), a debiasing method that limits unfairness metrics by employing an adversarial objective function.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[7567061 | Hardt et al. | 2016 | Citations: 4331]": "We propose a criterion for discrimination against a specified sensitive attribute in supervised learning, where the goal is to predict some target based on available features. Assuming data about the predictor, target, and membership in the protected group are available, we show how to optimally adjust any learned predictor so as to remove discrimination according to our definition. Our framework also improves incentives by shifting the cost of poor classification from disadvantaged groups to the decision maker, who can respond by improving the classification accuracy. We enourage readers to consult the more complete manuscript on the arXiv."
                },
                "metadata": [
                    {
                        "section_title": "Bias Detection and Mitigation",
                        "pdf_hash": "",
                        "start": 1432,
                        "end": 1878,
                        "sentence_offsets": [
                            {
                                "start": 1432,
                                "end": 1531
                            },
                            {
                                "start": 1532,
                                "end": 1680
                            },
                            {
                                "start": 1681,
                                "end": 1878
                            }
                        ],
                        "ref_mentions": [
                            "7567061"
                        ],
                        "quote": "There are attempts to mitigate bias using postprocessing methods, particularly in tabular datasets. These methods aim to adjust specific classification outcomes to improve metrics like equalized odds or equality of opportunity (Hardt et al., 2016). Madras et al. (2018) proposed LAFTR (Learning Adversarially Fair and Transferable Representations), a debiasing method that limits unfairness metrics by employing an adversarial objective function."
                    }
                ]
            },
            {
                "idx": 24,
                "key": "[276235453 | Li et al. | 2025 | Citations: 4]",
                "snippets": "The average mitigation results of severe negative influences for each task and bias are shown in Figure 9. A detailed analysis of the mitigation effects on gender and cultural biases across different tasks, as well as the results on GPT-4o-mini, are provided in Appendix E.2. Our key conclusion is that, similar to the nuanced effects on downstream tasks, the effectiveness of different mitigation approaches is also nuanced, depending on various factors such as different types of bias, downstream tasks, bias ratios, and more, highlighting the difficulty in devising a unified mitigation solution. Specifically, token-based mitigation provides implicit cues and works best with simple biases and tasks, as it depends on the model's own understanding. It is more effective for gender bias than complex cultural bias and performs better in classification tasks than in detailed generation tasks. For gender generation tasks, salary recommendation benefits more than complex hiring recommendation. More augmentation data (50%) further enhance its performance. \n\nMask-based mitigation shows noticeable effects at lower bias ratios (5%), especially in tasks like cultural classification. As at this ratio, explicit biased terms have a more direct and pronounced impact on the model's performance. By masking these terms, the model is less likely to rely on biased information or features that could skew its decision-making, thereby reducing bias inheritance. However, as bias ratios increase, the influence of more subtle and implicit biases grows, necessitating more complex mitigation strategies. It also proves effective in contrastive explicit bias, where direct bias information is more clearly identifiable. \n\nLoss-based mitigation primarily depends on the distribution distance between the augmentation and the original data, showing significant effectiveness when the distance is large. It is more suitable for coarse-grained tasks. For example, it works well in classification tasks where decisions often rely on broader patterns. In generation tasks, coarse-grained contexts like salary recommendations perform better than finer-grained ones like hiring recommendation. Additionally, smaller proportions (e.g., 5% and 10%) of augmentation data yield better results, by introducing subtle shifts that avoid overfitting to the augmented distribution while effectively influencing the original bias.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Mitigation Results",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 2404,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 106
                            },
                            {
                                "start": 107,
                                "end": 275
                            },
                            {
                                "start": 276,
                                "end": 599
                            },
                            {
                                "start": 600,
                                "end": 752
                            },
                            {
                                "start": 753,
                                "end": 895
                            },
                            {
                                "start": 896,
                                "end": 996
                            },
                            {
                                "start": 997,
                                "end": 1058
                            },
                            {
                                "start": 1061,
                                "end": 1184
                            },
                            {
                                "start": 1185,
                                "end": 1293
                            },
                            {
                                "start": 1294,
                                "end": 1456
                            },
                            {
                                "start": 1457,
                                "end": 1596
                            },
                            {
                                "start": 1597,
                                "end": 1711
                            },
                            {
                                "start": 1714,
                                "end": 1892
                            },
                            {
                                "start": 1893,
                                "end": 1938
                            },
                            {
                                "start": 1939,
                                "end": 2037
                            },
                            {
                                "start": 2038,
                                "end": 2177
                            },
                            {
                                "start": 2178,
                                "end": 2404
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The average mitigation results of severe negative influences for each task and bias are shown in Figure 9. A detailed analysis of the mitigation effects on gender and cultural biases across different tasks, as well as the results on GPT-4o-mini, are provided in Appendix E.2. Our key conclusion is that, similar to the nuanced effects on downstream tasks, the effectiveness of different mitigation approaches is also nuanced, depending on various factors such as different types of bias, downstream tasks, bias ratios, and more, highlighting the difficulty in devising a unified mitigation solution. Specifically, token-based mitigation provides implicit cues and works best with simple biases and tasks, as it depends on the model's own understanding. It is more effective for gender bias than complex cultural bias and performs better in classification tasks than in detailed generation tasks. For gender generation tasks, salary recommendation benefits more than complex hiring recommendation. More augmentation data (50%) further enhance its performance. \n\nMask-based mitigation shows noticeable effects at lower bias ratios (5%), especially in tasks like cultural classification. As at this ratio, explicit biased terms have a more direct and pronounced impact on the model's performance. By masking these terms, the model is less likely to rely on biased information or features that could skew its decision-making, thereby reducing bias inheritance. However, as bias ratios increase, the influence of more subtle and implicit biases grows, necessitating more complex mitigation strategies. It also proves effective in contrastive explicit bias, where direct bias information is more clearly identifiable. \n\nLoss-based mitigation primarily depends on the distribution distance between the augmentation and the original data, showing significant effectiveness when the distance is large. It is more suitable for coarse-grained tasks. For example, it works well in classification tasks where decisions often rely on broader patterns. In generation tasks, coarse-grained contexts like salary recommendations perform better than finer-grained ones like hiring recommendation. Additionally, smaller proportions (e.g., 5% and 10%) of augmentation data yield better results, by introducing subtle shifts that avoid overfitting to the augmented distribution while effectively influencing the original bias."
                    }
                ]
            },
            {
                "idx": 25,
                "key": "[276902427 | Zakizadeh et al. | 2025 | Citations: 0]",
                "snippets": "Despite growing awareness, there is a lack of comprehensive investigation into how different models internally represent and propagate such biases...Surprisingly, debiasing techniques often exhibit limited efficacy, sometimes inadvertently increasing the encoded bias in internal representations while reducing bias in model output distributions. This highlights a disconnect between mitigating bias in output distributions and addressing its internal representations.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Despite growing awareness, there is a lack of comprehensive investigation into how different models internally represent and propagate such biases",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    },
                    {
                        "quote": "Surprisingly, debiasing techniques often exhibit limited efficacy, sometimes inadvertently increasing the encoded bias in internal representations while reducing bias in model output distributions. This highlights a disconnect between mitigating bias in output distributions and addressing its internal representations.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 26,
                "key": "[277150560 | Dasu et al. | 2025 | Citations: 1]",
                "snippets": "Post-processing techniques modify the model's inference behavior after complete training. This may involve altering the model before inference or the model output during/after inference. For instance, Gehman et al. [24] proposed token-blocking methods during decoding to prevent the generation of harmful or biased terms. \n\nHauzenberger et al. [33] introduced sparse debiasing subnetworks that are trained separately and can be applied to the model at inference time. Qian et al. (Qian et al., 2021) performed keyword-based distillation to remove bias during inference. Tokpo and Calders [61] identify biased tokens and replace them with less stereotypical terms. Postprocessing approaches are beneficial since dataset collection and LLM training are expensive processes that we may not always be able to repeat if issues of unfairness are found. Furthermore, postprocessing mitigation approaches are sometimes the only viable option with the status quo of utilizing large pre-trained LLMs that take a huge amount of resources to train.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[236459953 | Qian et al. | 2021 | Citations: 89]": "Today\u2019s text classifiers inevitably suffer from unintended dataset biases, especially the document-level label bias and word-level keyword bias, which may hurt models\u2019 generalization. Many previous studies employed data-level manipulations or model-level balancing mechanisms to recover unbiased distributions and thus prevent models from capturing the two types of biases. Unfortunately, they either suffer from the extra cost of data collection/selection/annotation or need an elaborate design of balancing strategies. Different from traditional factual inference in which debiasing occurs before or during training, counterfactual inference mitigates the influence brought by unintended confounders after training, which can make unbiased decisions with biased observations. Inspired by this, we propose a model-agnostic text classification debiasing framework \u2013 Corsair, which can effectively avoid employing data manipulations or designing balancing mechanisms. Concretely, Corsair first trains a base model on a training set directly, allowing the dataset biases \u2018poison\u2019 the trained model. In inference, given a factual input document, Corsair imagines its two counterfactual counterparts to distill and mitigate the two biases captured by the poisonous model. Extensive experiments demonstrate Corsair\u2019s effectiveness, generalizability and fairness."
                },
                "metadata": [
                    {
                        "section_title": "Related Works",
                        "pdf_hash": "",
                        "start": 1137,
                        "end": 2158,
                        "sentence_offsets": [
                            {
                                "start": 1137,
                                "end": 1226
                            },
                            {
                                "start": 1227,
                                "end": 1323
                            },
                            {
                                "start": 1324,
                                "end": 1458
                            },
                            {
                                "start": 1461,
                                "end": 1604
                            },
                            {
                                "start": 1605,
                                "end": 1691
                            },
                            {
                                "start": 1692,
                                "end": 1785
                            },
                            {
                                "start": 1786,
                                "end": 1968
                            },
                            {
                                "start": 1969,
                                "end": 2158
                            }
                        ],
                        "ref_mentions": [
                            "236459953"
                        ],
                        "quote": "Post-processing techniques modify the model's inference behavior after complete training. This may involve altering the model before inference or the model output during/after inference. For instance, Gehman et al. [24] proposed token-blocking methods during decoding to prevent the generation of harmful or biased terms. \n\nHauzenberger et al. [33] introduced sparse debiasing subnetworks that are trained separately and can be applied to the model at inference time. Qian et al. (Qian et al., 2021) performed keyword-based distillation to remove bias during inference. Tokpo and Calders [61] identify biased tokens and replace them with less stereotypical terms. Postprocessing approaches are beneficial since dataset collection and LLM training are expensive processes that we may not always be able to repeat if issues of unfairness are found. Furthermore, postprocessing mitigation approaches are sometimes the only viable option with the status quo of utilizing large pre-trained LLMs that take a huge amount of resources to train."
                    }
                ]
            },
            {
                "idx": 27,
                "key": "[277150794 | Chen et al. | 2025 | Citations: 0]",
                "snippets": "Post-processing methods such as Sent-debias work by removing gender bias from pretrained model embeddings, though research suggests that these methods often obscure rather than fully eliminate bias (Liang et al., 2020;Gonen and Goldberg, 2019).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 706,
                        "end": 950,
                        "sentence_offsets": [
                            {
                                "start": 706,
                                "end": 950
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Post-processing methods such as Sent-debias work by removing gender bias from pretrained model embeddings, though research suggests that these methods often obscure rather than fully eliminate bias (Liang et al., 2020;Gonen and Goldberg, 2019)."
                    }
                ]
            },
            {
                "idx": 28,
                "key": "[52070035 | Park et al. | 2018 | Citations: 341]",
                "snippets": "To our surprise, the most effective method was applying both debiased embedding and gender swap to GRU, which reduced the equality differences by 98% & 89% while losing only 1.5% of the original performance. We assume that this may be related to the influence of \"attending\" model architectures on biases as discussed in Section 4.3. On the other hand, using the three methods together improved both generated unbiased set performance and equality differences, but had the largest decrease in the original performance. \n\nAll methods involved some performance loss when gender biases were reduced. Especially, fine-tuning had the largest decrease in original test set performance. This could be attributed to the difference in the source and target tasks (abusive & sexist). However, the decrease was marginal (less than 4%), while the drop in bias was significant. We assume the performance loss happens because mitigation methods modify the data or the model in a way that sometimes deters the models from discriminating important \"unbiased\" features.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Results & Discussion",
                        "pdf_hash": "",
                        "start": 668,
                        "end": 1720,
                        "sentence_offsets": [
                            {
                                "start": 668,
                                "end": 875
                            },
                            {
                                "start": 876,
                                "end": 1001
                            },
                            {
                                "start": 1002,
                                "end": 1186
                            },
                            {
                                "start": 1189,
                                "end": 1264
                            },
                            {
                                "start": 1265,
                                "end": 1347
                            },
                            {
                                "start": 1348,
                                "end": 1441
                            },
                            {
                                "start": 1442,
                                "end": 1532
                            },
                            {
                                "start": 1533,
                                "end": 1720
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "To our surprise, the most effective method was applying both debiased embedding and gender swap to GRU, which reduced the equality differences by 98% & 89% while losing only 1.5% of the original performance. We assume that this may be related to the influence of \"attending\" model architectures on biases as discussed in Section 4.3. On the other hand, using the three methods together improved both generated unbiased set performance and equality differences, but had the largest decrease in the original performance. \n\nAll methods involved some performance loss when gender biases were reduced. Especially, fine-tuning had the largest decrease in original test set performance. This could be attributed to the difference in the source and target tasks (abusive & sexist). However, the decrease was marginal (less than 4%), while the drop in bias was significant. We assume the performance loss happens because mitigation methods modify the data or the model in a way that sometimes deters the models from discriminating important \"unbiased\" features."
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.030429,
        "cot": "To effectively organize these quotes into dimensions that address the user's query about post-training mitigation techniques for reducing racial and gender bias in large language models (LLMs), I need to create a structured breakdown of the key aspects.\n\n1. First, I'll include an \"Introduction/Background\" section to provide context about bias mitigation approaches in LLMs. This will help frame the discussion that follows.\n\n2. \"Types of Post-Training Bias Mitigation Techniques\" should be a synthesis section that categorizes the main approaches mentioned across the quotes. The quotes clearly indicate several categories: post-processing methods, intra-processing methods, and some specialized approaches like causal interventions.\n\n3. \"Specific Post-Training Mitigation Methods\" would be best as a list format since the user is asking about \"the most effective\" techniques, suggesting they want distinct methods identified. The quotes mention many specific techniques like INLP, Counterfactual Data Augmentation, Auto-Debias, etc.\n\n4. \"Performance Trade-offs and Effectiveness\" needs to be a synthesis section as this requires comparing and analyzing how these methods impact model performance alongside their bias reduction capabilities.\n\n5. \"Comparative Analysis of Mitigation Techniques\" should be a synthesis that directly addresses the comparison aspect of the user's query, looking at how different techniques compare across metrics.\n\n6. \"Implementation Considerations\" would work well as a list of practical aspects to consider when applying these techniques.\n\nLet me now organize the quotes into these dimensions:",
        "plan": {
            "Introduction to Bias Mitigation in Large Language Models (synthesis)": [],
            "Types of Post-Training Bias Mitigation Techniques (synthesis)": [
                0,
                4,
                10,
                12,
                15,
                18,
                21,
                26
            ],
            "Specific Post-Training Mitigation Methods (list)": [
                1,
                2,
                3,
                5,
                6,
                7,
                8,
                9,
                11,
                13,
                14,
                16,
                20,
                22,
                23,
                27
            ],
            "Performance Trade-offs and Effectiveness (synthesis)": [
                19,
                24,
                25,
                28
            ],
            "Comparative Analysis of Mitigation Techniques (synthesis)": [
                1,
                17,
                24
            ],
            "Implementation Considerations (list)": [
                8,
                26
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction to Bias Mitigation in Large Language Models",
                "tldr": "Large language models (LLMs) often perpetuate racial and gender biases present in their training data, requiring post-training mitigation techniques to address these issues. Effective bias mitigation is critical to ensure LLMs deliver fair and equitable outputs while maintaining their utility across various applications. (LLM Memory)",
                "text": "\nLarge language models have demonstrated impressive capabilities across numerous domains, but they also inherit and sometimes amplify social biases present in their training data. These biases can manifest as gender stereotypes, racial prejudices, and other forms of discrimination in model outputs. Post-training bias mitigation techniques have emerged as important solutions for addressing these issues in already-trained LLMs without requiring complete retraining of these massive models.\n\nBias in LLMs stems primarily from patterns in the training data that reflect historical and societal inequities. For example, models may associate certain professions predominantly with specific genders or exhibit varying sentiment when discussing different racial groups. These biases can have harmful real-world consequences when LLMs are deployed in sensitive contexts such as hiring, content moderation, or healthcare applications. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nPost-training mitigation approaches offer practical solutions for developers and organizations that need to address bias in existing models without access to the original training data or the resources required for complete retraining. These techniques target the model after its initial training phase, applying various methods to reduce harmful biases while attempting to preserve general model performance and capabilities. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThe growing emphasis on responsible AI development has accelerated research into effective post-training debiasing methods. These approaches vary widely in their complexity, effectiveness, and impact on other aspects of model performance, leading to important trade-offs that must be considered when selecting and implementing bias mitigation strategies. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">",
                "citations": [],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Types of Post-Training Bias Mitigation Techniques",
                "tldr": "Post-training bias mitigation techniques for LLMs fall into three main categories: intra-processing (modifying inference without retraining), post-processing (adjusting model outputs), and model-driven approaches (directly altering model parameters). Each approach offers different tradeoffs between implementation complexity, effectiveness, and preservation of model capabilities. (14 sources)",
                "text": "\nPost-training bias mitigation techniques operate on already trained language models and can be categorized into three main approaches based on when and how they intervene in the model processing pipeline <Paper corpusId=\"270878797\" paperTitle=\"(Greco et al., 2024)\" isShortName></Paper>.\n\n## Intra-Processing Approaches\n\nIntra-processing methods modify a model's behavior during inference without requiring additional training <Paper corpusId=\"270370977\" paperTitle=\"(Deng et al., 2024)\" isShortName></Paper>. These techniques operate primarily at the decoding stage and include:\n\n1. **Token Search Restriction**: Constraining the decoding process to prevent the generation of biased outputs <Paper corpusId=\"270370977\" paperTitle=\"(Deng et al., 2024)\" isShortName></Paper>.\n\n2. **Token Distribution Adjustment**: Manipulating probability distributions during token selection to generate less biased or more diverse outputs <Paper corpusId=\"270370977\" paperTitle=\"(Deng et al., 2024)\" isShortName></Paper>. This can involve techniques like logit suppression or temperature sampling to flatten token probability distributions <Paper corpusId=\"259096160\" paperTitle=\"(Chung et al., 2023)\" isShortName></Paper>.\n\n3. **Attention Redistribution**: Reallocating the model's attention to focus on less stereotypical aspects of the input <Paper corpusId=\"270370977\" paperTitle=\"(Deng et al., 2024)\" isShortName></Paper> <Paper corpusId=\"268512691\" paperTitle=\"(Lee et al., 2024)\" isShortName></Paper>.\n\n4. **Standalone Debiasing Networks**: Implementing modular bias mitigation components that can be integrated into the core model on-demand at inference time without altering the original model <Paper corpusId=\"270370977\" paperTitle=\"(Deng et al., 2024)\" isShortName></Paper> <Paper corpusId=\"258461365\" paperTitle=\"(Hauzenberger et al., 2022)\" isShortName></Paper>.\n\nA notable example is DEXPERTS, which uses two models in parallel\u2014one promoting non-toxic text and another discouraging harmful content\u2014to improve output fairness without retraining <Paper corpusId=\"272599907\" paperTitle=\"(Peng et al., 2024)\" isShortName></Paper>. Similarly, Saunders et al. proposed constraining beam search to improve gender diversity in machine translation outputs <Paper corpusId=\"233240748\" paperTitle=\"(Saunders et al., 2021)\" isShortName></Paper>.\n\n## Post-Processing Approaches\n\nPost-processing techniques modify the model's outputs after generation without accessing the model's internal parameters <Paper corpusId=\"267770177\" paperTitle=\"(Yan et al., 2024)\" isShortName></Paper>. These methods include:\n\n1. **Output Filtering**: Identifying and blocking harmful or biased terms in generated content <Paper corpusId=\"277150560\" paperTitle=\"(Dasu et al., 2025)\" isShortName></Paper>.\n\n2. **Content Replacement**: Detecting biased tokens and replacing them with less stereotypical alternatives <Paper corpusId=\"277150560\" paperTitle=\"(Dasu et al., 2025)\" isShortName></Paper> <Paper corpusId=\"246210255\" paperTitle=\"(Tokpo et al., 2022)\" isShortName></Paper>.\n\n3. **Output Redistribution**: Creating a new set of less biased predictions from the original model outputs <Paper corpusId=\"258832694\" paperTitle=\"(Zayed et al., 2023)\" isShortName></Paper>.\n\n4. **Counterfactual Inference**: Generating counterfactual examples to distill and mitigate biases captured by the model <Paper corpusId=\"236459953\" paperTitle=\"(Qian et al., 2021)\" isShortName></Paper>.\n\nHe et al. proposed a gradient-based rewriting framework called DEPEN (Detect and Perturb to Neutralize) that first detects sensitive components in text and masks them for regeneration under a neutralizing constraint <Paper corpusId=\"237634972\" paperTitle=\"(He et al., 2021)\" isShortName></Paper>.\n\n## Model-Driven Approaches\n\nModel-driven approaches directly modify the internal parameters or representations of the language model <Paper corpusId=\"236034024\" paperTitle=\"(Magee et al., 2021)\" isShortName></Paper>. For models using word embeddings, these techniques may:\n\n1. **Embedding Manipulation**: Directly modifying embedding values to decrease cosine similarities between category terms associated with bias <Paper corpusId=\"236034024\" paperTitle=\"(Magee et al., 2021)\" isShortName></Paper>.\n\n2. **Model Neutralization**: Applying techniques to neutralize the model's internal representations, though these may be less effective for transformer models with contextual embeddings <Paper corpusId=\"236034024\" paperTitle=\"(Magee et al., 2021)\" isShortName></Paper>.\n\nWhile these post-training approaches offer practical solutions for addressing bias in existing models, their effectiveness can vary significantly. Intra-processing and post-processing methods are particularly valuable when retraining is not feasible due to resource constraints or lack of access to training data <Paper corpusId=\"277150560\" paperTitle=\"(Dasu et al., 2025)\" isShortName></Paper>. However, achieving the right balance between bias mitigation and maintaining model performance remains a challenge across all approaches <Paper corpusId=\"268512691\" paperTitle=\"(Lee et al., 2024)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Greco et al., 2024)",
                        "snippets": [
                            "To address biases in NLP, techniques can be developed that act at the three main stages of the NLP pipeline (Kozodoi et al., 2021)[75]: pre-processing (modifying training data), in-processing (imposing fairness constraints during model training), and post-processing (adjusting classifier predictions based on fairness metrics)."
                        ],
                        "paper": {
                            "corpus_id": 270878797,
                            "title": "NLPGuard: A Framework for Mitigating the Use of Protected Attributes by NLP Classifiers",
                            "authors": [
                                {
                                    "authorId": "2149942497",
                                    "name": "Salvatore Greco"
                                },
                                {
                                    "authorId": "2310191590",
                                    "name": "Ke Zhou"
                                },
                                {
                                    "authorId": "47809306",
                                    "name": "L. Capra"
                                },
                                {
                                    "authorId": "2203091524",
                                    "name": "Tania Cerquitelli"
                                },
                                {
                                    "authorId": "144041798",
                                    "name": "D. Quercia"
                                }
                            ],
                            "year": 2024,
                            "venue": "Proc. ACM Hum. Comput. Interact.",
                            "n_citations": 2
                        },
                        "score": 0.865234375
                    },
                    {
                        "id": "(Deng et al., 2024)",
                        "snippets": [
                            "Intra-processing Mitigation.These approaches modify a trained model's behavior without additional training to generate debiased predictions during inference.There are mainly four types of methods.The first method adds restrictions during token search decoding to prevent biased outputs [245; 194].The second method adjusts token distributions to enhance output diversity or sample less biased outputs [58; 95].The third method redistributes the model's attention to less stereotypical aspects [321].The last method implements standalone networks with original models for specific debiasing tasks, such as reducing gender or racial biases (Hauzenberger et al., 2022).\n\nPost-processing Mitigation.The techniques address bias in generated outputs, especially relevant for black-box models with inaccessible training data or internal processes."
                        ],
                        "paper": {
                            "corpus_id": 270370977,
                            "title": "Deconstructing The Ethics of Large Language Models from Long-standing Issues to New-emerging Dilemmas",
                            "authors": [
                                {
                                    "authorId": "2307568934",
                                    "name": "Chengyuan Deng"
                                },
                                {
                                    "authorId": "2305729796",
                                    "name": "Yiqun Duan"
                                },
                                {
                                    "authorId": "2308077589",
                                    "name": "Xin Jin"
                                },
                                {
                                    "authorId": "2305625227",
                                    "name": "Heng Chang"
                                },
                                {
                                    "authorId": "46879986",
                                    "name": "Yijun Tian"
                                },
                                {
                                    "authorId": "2305875824",
                                    "name": "Han Liu"
                                },
                                {
                                    "authorId": "2261285492",
                                    "name": "Henry Peng Zou"
                                },
                                {
                                    "authorId": "2087723977",
                                    "name": "Yiqiao Jin"
                                },
                                {
                                    "authorId": "95289709",
                                    "name": "Yijia Xiao"
                                },
                                {
                                    "authorId": "2305636181",
                                    "name": "Yichen Wang"
                                },
                                {
                                    "authorId": "2305627329",
                                    "name": "Shenghao Wu"
                                },
                                {
                                    "authorId": "2307409629",
                                    "name": "Zongxing Xie"
                                },
                                {
                                    "authorId": "2345701402",
                                    "name": "Kuofeng Gao"
                                },
                                {
                                    "authorId": "2309117237",
                                    "name": "Sihong He"
                                },
                                {
                                    "authorId": "2305613890",
                                    "name": "Jun Zhuang"
                                },
                                {
                                    "authorId": "2305654768",
                                    "name": "Lu Cheng"
                                },
                                {
                                    "authorId": "2305650253",
                                    "name": "Haohan Wang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 24
                        },
                        "score": 0.90283203125
                    },
                    {
                        "id": "(Chung et al., 2023)",
                        "snippets": [
                            "Large language models (LLMs) can be used to generate text data for training and evaluating other models. However, creating high-quality datasets with LLMs can be challenging. In this work, we explore human-AI partnerships to facilitate high diversity and accuracy in LLM-based text data generation. We first examine two approaches to diversify text generation: 1) logit suppression, which minimizes the generation of languages that have already been frequently generated, and 2) temperature sampling, which flattens the token sampling probability. We found that diversification approaches can increase data diversity but often at the cost of data accuracy (i.e., text and labels being appropriate for the target domain). To address this issue, we examined two human interventions, 1) label replacement (LR), correcting misaligned labels, and 2) out-of-scope filtering (OOSF), removing instances that are out of the user\u2019s domain of interest or to which no considered label applies. With oracle studies, we found that LR increases the absolute accuracy of models trained with diversified datasets by 14.4%. Moreover, we found that some models trained with data generated with LR interventions outperformed LLM-based few-shot classification. In contrast, OOSF was not effective in increasing model accuracy, implying the need for future work in human-in-the-loop text data generation."
                        ],
                        "paper": {
                            "corpus_id": 259096160,
                            "title": "Increasing Diversity While Maintaining Accuracy: Text Data Generation with Large Language Models and Human Interventions",
                            "authors": [
                                {
                                    "authorId": "152836325",
                                    "name": "John Joon Young Chung"
                                },
                                {
                                    "authorId": "1783184",
                                    "name": "Ece Kamar"
                                },
                                {
                                    "authorId": "1719124",
                                    "name": "Saleema Amershi"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 121
                        },
                        "score": 0
                    },
                    {
                        "id": "(Lee et al., 2024)",
                        "snippets": [
                            "Post-processing mitigation methods adjust the probability distribution during the decoding phase to select tokens with less bias, using methods such as adjusting, filtering, or inserting tokens [31]. Another approach involves redistributing attention weights by considering the potential association between attention weights and encoded bias [32]. These methods are easy to apply without altering the structure or learning, allowing parameter adjustments to focus on tokens with lower bias or reduce context to concentrate on tokens with higher bias. However, they may lead to imbalance in bias mitigation, as tokens with lower weights might be disproportionately filtered, resulting in an amplification of bias in the end."
                        ],
                        "paper": {
                            "corpus_id": 268512691,
                            "title": "Detecting Bias in Large Language Models: Fine-tuned KcBERT",
                            "authors": [
                                {
                                    "authorId": "2292166326",
                                    "name": "J. K. Lee"
                                },
                                {
                                    "authorId": "2292024530",
                                    "name": "T. M. Chung"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conferences on Pattern Recognition and Artificial Intelligence",
                            "n_citations": 0
                        },
                        "score": 0.92822265625
                    },
                    {
                        "id": "(Hauzenberger et al., 2022)",
                        "snippets": [
                            "Societal biases are reflected in large pre-trained language models and their fine-tuned versions on downstream tasks. Common in-processing bias mitigation approaches, such as adversarial training and mutual information removal, introduce additional optimization criteria, and update the model to reach a new debiased state. However, in practice, end-users and practitioners might prefer to switch back to the original model, or apply debiasing only on a specific subset of protected attributes. To enable this, we propose a novel modular bias mitigation approach, consisting of stand-alone highly sparse debiasing subnetworks, where each debiasing module can be integrated into the core model on-demand at inference time. Our approach draws from the concept of \\emph{diff} pruning, and proposes a novel training regime adaptable to various representation disentanglement optimizations. We conduct experiments on three classification tasks with gender, race, and age as protected attributes. The results show that our modular approach, while maintaining task performance, improves (or at least remains on-par with) the effectiveness of bias mitigation in comparison with baseline finetuning. Particularly on a two-attribute dataset, our approach with separately learned debiasing subnetworks shows effective utilization of either or both the subnetworks for selective bias mitigation."
                        ],
                        "paper": {
                            "corpus_id": 258461365,
                            "title": "Modular and On-demand Bias Mitigation with Attribute-Removal Subnetworks",
                            "authors": [
                                {
                                    "authorId": "2167029006",
                                    "name": "Lukas Hauzenberger"
                                },
                                {
                                    "authorId": "2184114298",
                                    "name": "Shahed Masoudian"
                                },
                                {
                                    "authorId": "2116415191",
                                    "name": "Deepak Kumar"
                                },
                                {
                                    "authorId": "144125621",
                                    "name": "M. Schedl"
                                },
                                {
                                    "authorId": "2844293",
                                    "name": "Navid Rekabsaz"
                                }
                            ],
                            "year": 2022,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 18
                        },
                        "score": 0.93701171875
                    },
                    {
                        "id": "(Peng et al., 2024)",
                        "snippets": [
                            "During intra-processing, models are tweaked at the inference stage without retraining. Model editing enables targeted updates to model behavior, ensuring that biases in specific areas are corrected without affecting overall model performance [140,141]. Decoding modification like DEXPERTS directly affects text generation by adjusting token probabilities. DEXPERTS uses two models, one to promote non-toxic text and another to discourage harmful content, to improving output fairness [142].\n\nPost-processing methods focus on modifying the model's outputs."
                        ],
                        "paper": {
                            "corpus_id": 272599907,
                            "title": "Securing Large Language Models: Addressing Bias, Misinformation, and Prompt Attacks",
                            "authors": [
                                {
                                    "authorId": "2319606006",
                                    "name": "Benji Peng"
                                },
                                {
                                    "authorId": "2319648854",
                                    "name": "Keyu Chen"
                                },
                                {
                                    "authorId": "2321058649",
                                    "name": "Ming Li"
                                },
                                {
                                    "authorId": "2319607771",
                                    "name": "Pohsun Feng"
                                },
                                {
                                    "authorId": "2319608188",
                                    "name": "Ziqian Bi"
                                },
                                {
                                    "authorId": "2319615017",
                                    "name": "Junyu Liu"
                                },
                                {
                                    "authorId": "2319611972",
                                    "name": "Qian Niu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 15
                        },
                        "score": 0.955078125
                    },
                    {
                        "id": "(Saunders et al., 2021)",
                        "snippets": [
                            "Generating machine translations via beam search seeks the most likely output under a model. However, beam search has been shown to amplify demographic biases exhibited by a model. We aim to address this, focusing on gender bias resulting from systematic errors in grammatical gender translation. Almost all prior work on this problem adjusts the training data or the model itself. By contrast, our approach changes only the inference procedure. We constrain beam search to improve gender diversity in n-best lists, and rerank n-best lists using gender features obtained from the source sentence. Combining these strongly improves WinoMT gender translation accuracy for three language pairs without additional bilingual data or retraining. We also demonstrate our approach\u2019s utility for consistently gendering named entities, and its flexibility to handle new gendered language beyond the binary."
                        ],
                        "paper": {
                            "corpus_id": 233240748,
                            "title": "First the Worst: Finding Better Gender Translations During Beam Search",
                            "authors": [
                                {
                                    "authorId": "2069873842",
                                    "name": "D. Saunders"
                                },
                                {
                                    "authorId": "1993979140",
                                    "name": "Rosie Sallis"
                                },
                                {
                                    "authorId": "36126076",
                                    "name": "B. Byrne"
                                }
                            ],
                            "year": 2021,
                            "venue": "Findings",
                            "n_citations": 28
                        },
                        "score": 0
                    },
                    {
                        "id": "(Yan et al., 2024)",
                        "snippets": [
                            "Preprocessing techniques aim to detect and eliminate bias and unfairness early on, either within the dataset (Zmigrod et al., 2019)(Dinan et al., 2019)Abid et al., 2021;(Qian et al., 2022)(Ghanbarzadeh et al., 2023) or prompt (Mattern et al., 2022;(Fatemi et al., 2021)(Yang et al., 2022). In-training bias mitigation techniques focus on reducing bias and unfairness during model training, by adjusting model architecture (Bartl et al., 2020)(Han et al., 2021), modifying loss functions (Liu et al., 2019)Webster et al., 2020;(Ouyang et al., 2022)(Woo et al., 2023)(Park et al., 2023)(Zhou et al., 2023)(Li et al., 2023), or selectively updating parameters (Qian et al., 2022)Ranaldi et al., 2023;(Yu et al., 2023). \n\nIntraprocessing approaches alter decoding behavior (Saunders et al., 2021)Meade et al., 2023;(Kim et al., 2022)(Chung et al., 2023)(Hallinan et al., 2022) without additional training or fine-tuning. \n\nPost-processing techniques primarily adjust model outputs to address bias and unfairness, without directly accessing the model itself (He et al., 2021)(Tokpo et al., 2022)Majumder et al., 2022;Dhingra et al., 2023)."
                        ],
                        "paper": {
                            "corpus_id": 267770177,
                            "title": "Potential and Challenges of Model Editing for Social Debiasing",
                            "authors": [
                                {
                                    "authorId": "134233854",
                                    "name": "Jianhao Yan"
                                },
                                {
                                    "authorId": "2285582246",
                                    "name": "Futing Wang"
                                },
                                {
                                    "authorId": "2110450452",
                                    "name": "Yafu Li"
                                },
                                {
                                    "authorId": "2249762135",
                                    "name": "Yue Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 9
                        },
                        "score": 0.90380859375
                    },
                    {
                        "id": "(Dasu et al., 2025)",
                        "snippets": [
                            "Post-processing techniques modify the model's inference behavior after complete training. This may involve altering the model before inference or the model output during/after inference. For instance, Gehman et al. [24] proposed token-blocking methods during decoding to prevent the generation of harmful or biased terms. \n\nHauzenberger et al. [33] introduced sparse debiasing subnetworks that are trained separately and can be applied to the model at inference time. Qian et al. (Qian et al., 2021) performed keyword-based distillation to remove bias during inference. Tokpo and Calders [61] identify biased tokens and replace them with less stereotypical terms. Postprocessing approaches are beneficial since dataset collection and LLM training are expensive processes that we may not always be able to repeat if issues of unfairness are found. Furthermore, postprocessing mitigation approaches are sometimes the only viable option with the status quo of utilizing large pre-trained LLMs that take a huge amount of resources to train."
                        ],
                        "paper": {
                            "corpus_id": 277150560,
                            "title": "Attention Pruning: Automated Fairness Repair of Language Models via Surrogate Simulated Annealing",
                            "authors": [
                                {
                                    "authorId": "152123953",
                                    "name": "Vishnu Asutosh Dasu"
                                },
                                {
                                    "authorId": "2150249269",
                                    "name": "Md. Rafi Ur Rashid"
                                },
                                {
                                    "authorId": "2110652561",
                                    "name": "Vipul Gupta"
                                },
                                {
                                    "authorId": "1404887808",
                                    "name": "Saeid Tizpaz-Niari"
                                },
                                {
                                    "authorId": "2309172099",
                                    "name": "Gang Tan"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.9658203125
                    },
                    {
                        "id": "(Tokpo et al., 2022)",
                        "snippets": [
                            "It is well known that textual data on the internet and other digital platforms contain significant levels of bias and stereotypes. Various research findings have concluded that biased texts have significant effects on target demographic groups. For instance, masculine-worded job advertisements tend to be less appealing to female applicants. In this paper, we present a text-style transfer model that can be trained on non-parallel data and be used to automatically mitigate bias in textual data. Our style transfer model improves on the limitations of many existing text style transfer techniques such as the loss of content information. Our model solves such issues by combining latent content encoding with explicit keyword replacement. We will show that this technique produces better content preservation whilst maintaining good style transfer accuracy."
                        ],
                        "paper": {
                            "corpus_id": 246210255,
                            "title": "Text Style Transfer for Bias Mitigation using Masked Language Modeling",
                            "authors": [
                                {
                                    "authorId": "2145259446",
                                    "name": "E. Tokpo"
                                },
                                {
                                    "authorId": "1709830",
                                    "name": "T. Calders"
                                }
                            ],
                            "year": 2022,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 34
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zayed et al., 2023)",
                        "snippets": [
                            "While relatively less explored, post-processing bias mitigation methods modify the predictions of a biased model and generate a new set of less biased predictions."
                        ],
                        "paper": {
                            "corpus_id": 258832694,
                            "title": "Should We Attend More or Less? Modulating Attention for Fairness",
                            "authors": [
                                {
                                    "authorId": "2077390471",
                                    "name": "A. Zayed"
                                },
                                {
                                    "authorId": "24039720",
                                    "name": "Gon\u00e7alo Mordido"
                                },
                                {
                                    "authorId": "3197429",
                                    "name": "S. Shabanian"
                                },
                                {
                                    "authorId": "123607932",
                                    "name": "Sarath Chandar"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 10
                        },
                        "score": 0.88232421875
                    },
                    {
                        "id": "(Qian et al., 2021)",
                        "snippets": [
                            "Today\u2019s text classifiers inevitably suffer from unintended dataset biases, especially the document-level label bias and word-level keyword bias, which may hurt models\u2019 generalization. Many previous studies employed data-level manipulations or model-level balancing mechanisms to recover unbiased distributions and thus prevent models from capturing the two types of biases. Unfortunately, they either suffer from the extra cost of data collection/selection/annotation or need an elaborate design of balancing strategies. Different from traditional factual inference in which debiasing occurs before or during training, counterfactual inference mitigates the influence brought by unintended confounders after training, which can make unbiased decisions with biased observations. Inspired by this, we propose a model-agnostic text classification debiasing framework \u2013 Corsair, which can effectively avoid employing data manipulations or designing balancing mechanisms. Concretely, Corsair first trains a base model on a training set directly, allowing the dataset biases \u2018poison\u2019 the trained model. In inference, given a factual input document, Corsair imagines its two counterfactual counterparts to distill and mitigate the two biases captured by the poisonous model. Extensive experiments demonstrate Corsair\u2019s effectiveness, generalizability and fairness."
                        ],
                        "paper": {
                            "corpus_id": 236459953,
                            "title": "Counterfactual Inference for Text Classification Debiasing",
                            "authors": [
                                {
                                    "authorId": "2082474054",
                                    "name": "Chen Qian"
                                },
                                {
                                    "authorId": "2163400298",
                                    "name": "Fuli Feng"
                                },
                                {
                                    "authorId": "40650846",
                                    "name": "L. Wen"
                                },
                                {
                                    "authorId": "48168083",
                                    "name": "Chunping Ma"
                                },
                                {
                                    "authorId": "35930962",
                                    "name": "Pengjun Xie"
                                }
                            ],
                            "year": 2021,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 89
                        },
                        "score": 0
                    },
                    {
                        "id": "(He et al., 2021)",
                        "snippets": [
                            "Written language carries explicit and implicit biases that can distract from meaningful signals. For example, letters of reference may describe male and female candidates differently, or their writing style may indirectly reveal demographic characteristics. At best, such biases distract from the meaningful content of the text; at worst they can lead to unfair outcomes. We investigate the challenge of re-generating input sentences to 'neutralize' sensitive attributes while maintaining the semantic meaning of the original text (e.g. is the candidate qualified?). We propose a gradient-based rewriting framework, Detect and Perturb to Neutralize (DEPEN), that first detects sensitive components and masks them for regeneration, then perturbs the generation model at decoding time under a neutralizing constraint that pushes the (predicted) distribution of sensitive attributes towards a uniform distribution. Our experiments in two different scenarios show that DEPEN can regenerate fluent alternatives that are neutral in the sensitive attribute while maintaining the semantics of other attributes."
                        ],
                        "paper": {
                            "corpus_id": 237634972,
                            "title": "Detect and Perturb: Neutral Rewriting of Biased and Sensitive Text via Gradient-based Decoding",
                            "authors": [
                                {
                                    "authorId": "2116458151",
                                    "name": "Zexue He"
                                },
                                {
                                    "authorId": "3165738",
                                    "name": "Bodhisattwa Prasad Majumder"
                                },
                                {
                                    "authorId": "35660011",
                                    "name": "Julian McAuley"
                                }
                            ],
                            "year": 2021,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 29
                        },
                        "score": 0
                    },
                    {
                        "id": "(Magee et al., 2021)",
                        "snippets": [
                            "Model-driven approaches treat the language model post-training. For models involving word embeddings, these embeddings can be inspected, and following a debiasing heuristic, modified directly. [10] for example conducts a component analysis to identify bias, and then manipulates embedding values, to either increase or decrease cosine values between category terms in accordance with the analysis. Such techniques appear more difficult to apply to transformer models with contextual embeddings. [35] for example examines both data augmentation and model neutralisation, and finds model neutralisation less effective."
                        ],
                        "paper": {
                            "corpus_id": 236034024,
                            "title": "Intersectional Bias in Causal Language Models",
                            "authors": [
                                {
                                    "authorId": "2733075",
                                    "name": "L. Magee"
                                },
                                {
                                    "authorId": "9557084",
                                    "name": "Lida Ghahremanlou"
                                },
                                {
                                    "authorId": "13714096",
                                    "name": "K. Soldati\u0107"
                                },
                                {
                                    "authorId": "97868921",
                                    "name": "S. Robertson"
                                }
                            ],
                            "year": 2021,
                            "venue": "arXiv.org",
                            "n_citations": 33
                        },
                        "score": 0.89697265625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Specific Post-Training Mitigation Methods",
                "tldr": "Various post-training techniques have been developed to mitigate gender and racial bias in large language models, ranging from data manipulation approaches like Counterfactual Data Augmentation to embedding-focused methods like projection and clipping. These methods differ in their implementation complexity, computational requirements, and effectiveness at preserving model performance while reducing bias. (20 sources)",
                "text": "\n## Counterfactual Approaches\n\n1. **Counterfactual Data Augmentation (CDA)**: Creates balanced training datasets by replacing gender or racial terms with their counterparts (e.g., \"he\" with \"she\") to reduce biased associations <Paper corpusId=\"253734850\" paperTitle=\"(Zayed et al., 2022)\" isShortName></Paper> <Paper corpusId=\"256390197\" paperTitle=\"(Tokpo et al., 2023)\" isShortName></Paper>. While effective, traditional CDA requires retraining and is more resource-intensive than some alternatives <Paper corpusId=\"256390197\" paperTitle=\"(Tokpo et al., 2023)\" isShortName></Paper>.\n\n2. **Counterfactual Data Substitution (CDS)**: An improvement over CDA that randomly substitutes potentially biased text rather than duplicating content, helping to mitigate indirect gender bias by reducing cluster purity by up to 49% <Paper corpusId=\"202541569\" paperTitle=\"(Maudslay et al., 2019)\" isShortName></Paper>.\n\n3. **Few-Shot Debiasing**: Fine-tuning on as few as 10 carefully debiased examples can significantly reduce gender bias while maintaining language modeling capabilities, offering a highly practical approach <Paper corpusId=\"259095603\" paperTitle=\"(Thakur et al., 2023)\" isShortName></Paper>.\n\n4. **Contrastive Debiasing**: Co\u00b2PT (Counterfactual Contrastive Prompt Tuning) efficiently mitigates bias by training different debiasing prompts for various bias dimensions that can be combined to address intersectional bias <Paper corpusId=\"264305744\" paperTitle=\"(Dong et al., 2023)\" isShortName></Paper>.\n\n## Embedding Manipulation Techniques\n\n1. **Hard Debiasing**: Identifies and neutralizes a gender subspace in word embeddings while maintaining performance on evaluation tasks <Paper corpusId=\"1704893\" paperTitle=\"(Bolukbasi et al., 2016)\" isShortName></Paper>.\n\n2. **Iterative Null-space Projection (INLP)**: Trains linear classifiers to predict gender from embeddings, then projects embeddings to the null-space of these classifiers to remove gender information <Paper corpusId=\"215786522\" paperTitle=\"(Ravfogel et al., 2020)\" isShortName></Paper> <Paper corpusId=\"271769656\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\n3. **Feature Clipping**: A post-processing method that clips or prunes embedding dimensions that highly correlate with gender information, though this can cause unavoidable performance degradation in the main task <Paper corpusId=\"237490811\" paperTitle=\"(Wang et al., 2021)\" isShortName></Paper> <Paper corpusId=\"262828449\" paperTitle=\"(Lee et al., 2023)\" isShortName></Paper>.\n\n4. **Sent-Debias**: Removes bias at the sentence level by identifying and removing biased dimensions in sentence embeddings, though some research suggests this may obscure rather than eliminate bias <Paper corpusId=\"207996257\" paperTitle=\"(Liang et al., 2020)\" isShortName></Paper> <Paper corpusId=\"277150794\" paperTitle=\"(Chen et al., 2025)\" isShortName></Paper>.\n\n## Inference-Time Methods\n\n1. **Self-Debias**: Adjusts next-token probabilities during generation based on the model's own prediction of how biased each potential token might be, requiring no manually curated word lists or parameter changes <Paper corpusId=\"232075876\" paperTitle=\"(Schick et al., 2021)\" isShortName></Paper> <Paper corpusId=\"271769656\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\n2. **DEXPERTS**: Uses two models in parallel\u2014one promoting non-toxic text and another discouraging harmful content\u2014to improve output fairness without retraining <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.\n\n3. **Fair Filter (FairFil)**: Transforms pretrained encoder outputs into debiased representations via a neural network that minimizes correlation between filtered embeddings and bias words while preserving semantic information <Paper corpusId=\"232185104\" paperTitle=\"(Cheng et al., 2021)\" isShortName></Paper>.\n\n4. **Name-Scrambling**: Replaces gender-specific names with randomly selected alternatives, proving especially effective for reducing bias in dialogue models while maintaining performance <Paper corpusId=\"237442178\" paperTitle=\"(Smith et al., 2021)\" isShortName></Paper>.\n\n## Causal and Knowledge-Based Approaches\n\n1. **Causal-Debias**: Integrates causal learning principles to distinguish between causal (label-relevant) and non-causal (bias-related) factors in token representations, directly addressing bias resurgence during fine-tuning <Paper corpusId=\"259370743\" paperTitle=\"(Zhou et al., 2023)\" isShortName></Paper> <Paper corpusId=\"273501862\" paperTitle=\"(Wu et al., 2024)\" isShortName></Paper>.\n\n2. **DEPEN (Detect and Perturb to Neutralize)**: A gradient-based rewriting framework that detects sensitive components in text and masks them for regeneration under a neutralizing constraint <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.\n\n3. **Auto-Debias**: Automatically crafts biased prompts to identify completions with high disagreement across demographic groups, then applies a distribution alignment loss to reduce bias, achieving 90-96% reduction in bias probability scores with only 3-16% reduction in performance <Paper corpusId=\"248780440\" paperTitle=\"(Guo et al., 2022)\" isShortName></Paper> <Paper corpusId=\"268064051\" paperTitle=\"(Doughman et al., 2023)\" isShortName></Paper>.\n\n4. **LSDM (Least Square Debias Method)**: Uses causal mediation analysis and knowledge editing to mitigate gender bias in occupational pronouns by solving a matrix equation with constraint terms, avoiding catastrophic forgetting while targeting specific biases <Paper corpusId=\"268553687\" paperTitle=\"(Cai et al., 2024)\" isShortName></Paper>.\n\nThese techniques vary significantly in their implementation complexity, computational requirements, and effectiveness. Practitioners should select methods based on their specific use case, considering whether invariance to names is essential (name-scrambling), performance preservation is critical (controlled generation), or generalizable bias suppression is needed (unlikelihood training) <Paper corpusId=\"237442178\" paperTitle=\"(Smith et al., 2021)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Zayed et al., 2022)",
                        "snippets": [
                            "To mitigate gender bias in language models, several techniques have been proposed. These methods can be broadly classified into three main categories: data-based methods (Lu et al., 2018)(Maudslay et al., 2019)(De-Arteaga et al., 2019); regularization-based methods (Maudslay et al., 2019)(Garg et al., 2018); and adversarial-based methods (Song et al. 2019;(Manzini et al., 2019)(Isabelle et al., 2017). Data-based methods, which are our focus in this work, change the training data to balance the bias through targeted data augmentation (Lu et al., 2018)(Maudslay et al., 2019), constructing counterfactual examples (Garg et al., 2018), or removing protected attributes from the input (De-Arteaga et al., 2019) to disallow models from learning any correlation between labels and gender words. Regularization-based methods add an auxiliary loss term to the objective function that reduces the amount of bias in the model. Adversarial-based methods use adversarial learning for bias mitigation."
                        ],
                        "paper": {
                            "corpus_id": 253734850,
                            "title": "Deep Learning on a Healthy Data Diet: Finding Important Examples for Fairness",
                            "authors": [
                                {
                                    "authorId": "2077390471",
                                    "name": "A. Zayed"
                                },
                                {
                                    "authorId": "32899078",
                                    "name": "Prasanna Parthasarathi"
                                },
                                {
                                    "authorId": "24039720",
                                    "name": "Gon\u00e7alo Mordido"
                                },
                                {
                                    "authorId": "2542427",
                                    "name": "Hamid Palangi"
                                },
                                {
                                    "authorId": "3197429",
                                    "name": "S. Shabanian"
                                },
                                {
                                    "authorId": "123607932",
                                    "name": "Sarath Chandar"
                                }
                            ],
                            "year": 2022,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 22
                        },
                        "score": 0.900390625
                    },
                    {
                        "id": "(Tokpo et al., 2023)",
                        "snippets": [
                            "We select three popular mitigation methods to represent all three types, namely Counterfactual Data Augmentation (CDA), Context-debias, and Sentdebias. Notice that these methods create debiased pretrained language models, as is illustrated in Figure 1. These models still need to be finetuned on a downstream task.\n\nCDA pretraining. The idea behind counterfactual data augmentation (Zmigrod et al., 2019;Lu et al., 2020) is to generate a counterfactual for each example in the training corpus by replacing attribute terms with their complimentary equivalent from the other demographic classes. For example, she will map to he in the case of binary gender. To mitigate intrinsic bias, this counterfactual augmentation has to be done as a pretraining step. Since CDA involves retraining the model, it is more resource-intensive compared to Sent-debias and Context-debias. We use the pretrained CDA models based on BERT and ALBERT from Webster et al. (2020) for our implementation."
                        ],
                        "paper": {
                            "corpus_id": 256390197,
                            "title": "How Far Can It Go? On Intrinsic Gender Bias Mitigation for Text Classification",
                            "authors": [
                                {
                                    "authorId": "2145259446",
                                    "name": "E. Tokpo"
                                },
                                {
                                    "authorId": "150258834",
                                    "name": "Pieter Delobelle"
                                },
                                {
                                    "authorId": "2990203",
                                    "name": "Bettina Berendt"
                                },
                                {
                                    "authorId": "1709830",
                                    "name": "T. Calders"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
                            "n_citations": 8
                        },
                        "score": 0.8740234375
                    },
                    {
                        "id": "(Maudslay et al., 2019)",
                        "snippets": [
                            "This paper treats gender bias latent in word embeddings. Previous mitigation attempts rely on the operationalisation of gender bias as a projection over a linear subspace. An alternative approach is Counterfactual Data Augmentation (CDA), in which a corpus is duplicated and augmented to remove bias, e.g. by swapping all inherently-gendered words in the copy. We perform an empirical comparison of these approaches on the English Gigaword and Wikipedia, and find that whilst both successfully reduce direct bias and perform well in tasks which quantify embedding quality, CDA variants outperform projection-based methods at the task of drawing non-biased gender analogies by an average of 19% across both corpora. We propose two improvements to CDA: Counterfactual Data Substitution (CDS), a variant of CDA in which potentially biased text is randomly substituted to avoid duplication, and the Names Intervention, a novel name-pairing technique that vastly increases the number of words being treated. CDA/S with the Names Intervention is the only approach which is able to mitigate indirect gender bias: following debiasing, previously biased words are significantly less clustered according to gender (cluster purity is reduced by 49%), thus improving on the state-of-the-art for bias mitigation."
                        ],
                        "paper": {
                            "corpus_id": 202541569,
                            "title": "It\u2019s All in the Name: Mitigating Gender Bias with Name-Based Counterfactual Data Substitution",
                            "authors": [
                                {
                                    "authorId": "1388061045",
                                    "name": "R. Maudslay"
                                },
                                {
                                    "authorId": "1821892",
                                    "name": "Hila Gonen"
                                },
                                {
                                    "authorId": "1750769",
                                    "name": "Ryan Cotterell"
                                },
                                {
                                    "authorId": "2480901",
                                    "name": "Simone Teufel"
                                }
                            ],
                            "year": 2019,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 172
                        },
                        "score": 0
                    },
                    {
                        "id": "(Thakur et al., 2023)",
                        "snippets": [
                            "While the majority of current state-of-the-art debiasing methods focus on changes to the training regime, in this paper, we propose data intervention strategies as a powerful yet simple technique to reduce gender bias in pre-trained models. Specifically, we empirically show that by fine-tuning a pre-trained model on only 10 debiased (intervened) training examples, the tendency to favor any gender is significantly reduced. Since our proposed method only needs a few training examples, we argue that our few-shot de-biasing approach is highly feasible and practical. Through extensive experimentation, we show that our de-biasing technique performs better than competitive state-of-the-art baselines with minimal loss in language modeling ability."
                        ],
                        "paper": {
                            "corpus_id": 259095603,
                            "title": "Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions",
                            "authors": [
                                {
                                    "authorId": "2221493995",
                                    "name": "Himanshu Thakur"
                                },
                                {
                                    "authorId": "1819271266",
                                    "name": "Atishay Jain"
                                },
                                {
                                    "authorId": "2127734657",
                                    "name": "Praneetha Vaddamanu"
                                },
                                {
                                    "authorId": "28130078",
                                    "name": "Paul Pu Liang"
                                },
                                {
                                    "authorId": "49933077",
                                    "name": "Louis-philippe Morency"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 37
                        },
                        "score": 0.87109375
                    },
                    {
                        "id": "(Dong et al., 2023)",
                        "snippets": [
                            "Co$^2$PT, an efficient and effective debias-while-prompt tuning method for mitigating biases via counterfactual contrastive prompt tuning on downstream tasks",
                            "We evaluate its effectiveness on bias mitigation and applicability to existing debiased upstream models, and investigate how the design of each component and the selection of hyperparameters impact both its bias reduction capabilities and downstream task performance",
                            "Without re-training the model, Co 2 PT is flexible to apply in order to mitigate different bias types in downstream applications. One can train different debiasing prompts to tackle different bias dimensions such as gender, race, and religion. Furthermore, these debiasing prompts can be applied to mitigate intersectional bias by simply combining the corresponding prompts in downstream tasks."
                        ],
                        "paper": {
                            "corpus_id": 264305744,
                            "title": "Co$^2$PT: Mitigating Bias in Pre-trained Language Models through Counterfactual Contrastive Prompt Tuning",
                            "authors": [
                                {
                                    "authorId": "1716200686",
                                    "name": "Xiangjue Dong"
                                },
                                {
                                    "authorId": "9725200",
                                    "name": "Ziwei Zhu"
                                },
                                {
                                    "authorId": "96309563",
                                    "name": "Zhuoer Wang"
                                },
                                {
                                    "authorId": "121003684",
                                    "name": "Maria Teleki"
                                },
                                {
                                    "authorId": "1697232",
                                    "name": "James Caverlee"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 11
                        },
                        "score": 0.89501953125
                    },
                    {
                        "id": "(Bolukbasi et al., 2016)",
                        "snippets": [
                            "The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias."
                        ],
                        "paper": {
                            "corpus_id": 1704893,
                            "title": "Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings",
                            "authors": [
                                {
                                    "authorId": "2843215",
                                    "name": "Tolga Bolukbasi"
                                },
                                {
                                    "authorId": "2782886",
                                    "name": "Kai-Wei Chang"
                                },
                                {
                                    "authorId": "145085305",
                                    "name": "James Y. Zou"
                                },
                                {
                                    "authorId": "1699322",
                                    "name": "Venkatesh Saligrama"
                                },
                                {
                                    "authorId": "2186481",
                                    "name": "A. Kalai"
                                }
                            ],
                            "year": 2016,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 3153
                        },
                        "score": 0
                    },
                    {
                        "id": "(Ravfogel et al., 2020)",
                        "snippets": [
                            "The ability to control for the kinds of information encoded in neural representation has a variety of use cases, especially in light of the challenge of interpreting these models. We present Iterative Null-space Projection (INLP), a novel method for removing information from neural representations. Our method is based on repeated training of linear classifiers that predict a certain property we aim to remove, followed by projection of the representations on their null-space. By doing so, the classifiers become oblivious to that target property, making it hard to linearly separate the data according to it. While applicable for multiple uses, we evaluate our method on bias and fairness use-cases, and show that our method is able to mitigate bias in word embeddings, as well as to increase fairness in a setting of multi-class classification."
                        ],
                        "paper": {
                            "corpus_id": 215786522,
                            "title": "Null It Out: Guarding Protected Attributes by Iterative Nullspace Projection",
                            "authors": [
                                {
                                    "authorId": "51432464",
                                    "name": "Shauli Ravfogel"
                                },
                                {
                                    "authorId": "51131518",
                                    "name": "Yanai Elazar"
                                },
                                {
                                    "authorId": "1821892",
                                    "name": "Hila Gonen"
                                },
                                {
                                    "authorId": "102707804",
                                    "name": "Michael Twiton"
                                },
                                {
                                    "authorId": "79775260",
                                    "name": "Yoav Goldberg"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 388
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wang et al., 2024)",
                        "snippets": [
                            "Post-hoc: Iterative null-space projection (INLP) (Ravfogel et al., 2020) trains a set of linear classifiers to predict genders from embeddings and then projects embeddings to the null-space of learned classifiers. Self-Debias (Schick et al., 2021) adjusts next token probabilities at each step according to model's prediction to what extent the next token is biased."
                        ],
                        "paper": {
                            "corpus_id": 271769656,
                            "title": "A Parameter-Efficient Multi-Objective Approach to Mitigate Stereotypical Bias in Language Models",
                            "authors": [
                                {
                                    "authorId": "2293409512",
                                    "name": "Yifan Wang"
                                },
                                {
                                    "authorId": "2293393732",
                                    "name": "Vera Demberg"
                                }
                            ],
                            "year": 2024,
                            "venue": "GEBNLP",
                            "n_citations": 1
                        },
                        "score": 0.8642578125
                    },
                    {
                        "id": "(Wang et al., 2021)",
                        "snippets": [
                            "Internet search affects people\u2019s cognition of the world, so mitigating biases in search results and learning fair models is imperative for social good. We study a unique gender bias in image search in this work: the search images are often gender-imbalanced for gender-neutral natural language queries. We diagnose two typical image search models, the specialized model trained on in-domain datasets and the generalized representation model pre-trained on massive image and text data across the internet. Both models suffer from severe gender bias. Therefore, we introduce two novel debiasing approaches: an in-processing fair sampling method to address the gender imbalance issue for training models, and a post-processing feature clipping method base on mutual information to debias multimodal representations of pre-trained models. Extensive experiments on MS-COCO and Flickr30K benchmarks show that our methods significantly reduce the gender bias in image search models."
                        ],
                        "paper": {
                            "corpus_id": 237490811,
                            "title": "Are Gender-Neutral Queries Really Gender-Neutral? Mitigating Gender Bias in Image Search",
                            "authors": [
                                {
                                    "authorId": "46583583",
                                    "name": "Jialu Wang"
                                },
                                {
                                    "authorId": "2152799147",
                                    "name": "Yang Liu"
                                },
                                {
                                    "authorId": "47120131",
                                    "name": "X. Wang"
                                }
                            ],
                            "year": 2021,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 95
                        },
                        "score": 0
                    },
                    {
                        "id": "(Lee et al., 2023)",
                        "snippets": [
                            "\u2022 Feature Clipping. Wang et al. (Wang et al., 2021) propose a post-processing mitigation method that clips/prunes the dimensions of feature embeddings that are highly correlated with gender information. This idea is motivated by the fact that an unbiased retrieve implies the independence between the covariates (active features) and sensitive attributes (gender). However, the limitation of this approach is that it results in unavoidable performance degradation in the main task performance. \u2022 Instruction/Prompt Engineering. Friedrich et al. [62] propose a post-processing mitigation approach, Fair Diffusion, that is inspired by advances in instructing AI systems based on human feedback. Fair Diffusion \"instructs\" the pre-trained diffusion models to be fair during the deployment stage; it enables precise guidance to reduce biases in model outcomes based on pre-defined instructions stored in a lookup table. \u2022 Bias vector projection. Chuang et al. [38] de-bias VL foundation models by projecting out biased directions (gender bias direction, racial bias direction) in the text embedding. These bias directions are obtained from embeddings of spurious prompts such as \"a photo of a [irrelevant attribute]\". The authors also calibrate the projection matrix by introducing one additional regularization constraint that ensures the debiased prompt representation still has the same semantic meaning after the projection. To elaborate, \"a photo of a [class name] with [spurious attribute]\" should still have the same semantic meaning as \"a photo of a [class name]\". The biggest advantage of this method is that it is computationally lightweight and simple, as it only manipulates text embedding."
                        ],
                        "paper": {
                            "corpus_id": 262828449,
                            "title": "Survey of Social Bias in Vision-Language Models",
                            "authors": [
                                {
                                    "authorId": "40221187",
                                    "name": "Nayeon Lee"
                                },
                                {
                                    "authorId": "23672613",
                                    "name": "Yejin Bang"
                                },
                                {
                                    "authorId": "116344405",
                                    "name": "Holy Lovenia"
                                },
                                {
                                    "authorId": "66986482",
                                    "name": "Samuel Cahyawijaya"
                                },
                                {
                                    "authorId": "47653392",
                                    "name": "Wenliang Dai"
                                },
                                {
                                    "authorId": "1683412",
                                    "name": "Pascale Fung"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 18
                        },
                        "score": 0.90625
                    },
                    {
                        "id": "(Liang et al., 2020)",
                        "snippets": [
                            "As natural language processing methods are increasingly deployed in real-world scenarios such as healthcare, legal systems, and social science, it becomes necessary to recognize the role they potentially play in shaping social biases and stereotypes. Previous work has revealed the presence of social biases in widely used word embeddings involving gender, race, religion, and other social constructs. While some methods were proposed to debias these word-level embeddings, there is a need to perform debiasing at the sentence-level given the recent shift towards new contextualized sentence representations such as ELMo and BERT. In this paper, we investigate the presence of social biases in sentence-level representations and propose a new method, Sent-Debias, to reduce these biases. We show that Sent-Debias is effective in removing biases, and at the same time, preserves performance on sentence-level downstream tasks such as sentiment analysis, linguistic acceptability, and natural language understanding. We hope that our work will inspire future research on characterizing and removing social biases from widely adopted sentence representations for fairer NLP."
                        ],
                        "paper": {
                            "corpus_id": 207996257,
                            "title": "Towards Debiasing Sentence Representations",
                            "authors": [
                                {
                                    "authorId": "28130078",
                                    "name": "Paul Pu Liang"
                                },
                                {
                                    "authorId": "47841931",
                                    "name": "Irene Z Li"
                                },
                                {
                                    "authorId": "2064345025",
                                    "name": "Emily Zheng"
                                },
                                {
                                    "authorId": "144529448",
                                    "name": "Y. Lim"
                                },
                                {
                                    "authorId": "145124475",
                                    "name": "R. Salakhutdinov"
                                },
                                {
                                    "authorId": "49933077",
                                    "name": "Louis-philippe Morency"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 240
                        },
                        "score": 0
                    },
                    {
                        "id": "(Chen et al., 2025)",
                        "snippets": [
                            "Post-processing methods such as Sent-debias work by removing gender bias from pretrained model embeddings, though research suggests that these methods often obscure rather than fully eliminate bias (Liang et al., 2020;Gonen and Goldberg, 2019)."
                        ],
                        "paper": {
                            "corpus_id": 277150794,
                            "title": "Enforcing Consistency and Fairness in Multi-level Hierarchical Classification with a Mask-based Output Layer",
                            "authors": [
                                {
                                    "authorId": "2301225724",
                                    "name": "Shijing Chen"
                                },
                                {
                                    "authorId": "38797620",
                                    "name": "Shoaib Jameel"
                                },
                                {
                                    "authorId": "3080469",
                                    "name": "Mohamed Reda Bouadjenek"
                                },
                                {
                                    "authorId": "2338268075",
                                    "name": "Feilong Tang"
                                },
                                {
                                    "authorId": "2301210198",
                                    "name": "Usman Naseem"
                                },
                                {
                                    "authorId": "2260341001",
                                    "name": "Basem Suleiman"
                                },
                                {
                                    "authorId": "2296782788",
                                    "name": "Hakim Hacid"
                                },
                                {
                                    "authorId": "2253483164",
                                    "name": "Flora D. Salim"
                                },
                                {
                                    "authorId": "2240180680",
                                    "name": "Imran Razzak"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.9091796875
                    },
                    {
                        "id": "(Schick et al., 2021)",
                        "snippets": [
                            "Abstract \u26a0 This paper contains prompts and model outputs that are offensive in nature. When trained on large, unfiltered crawls from the Internet, language models pick up and reproduce all kinds of undesirable biases that can be found in the data: They often generate racist, sexist, violent, or otherwise toxic language. As large models require millions of training examples to achieve good performance, it is difficult to completely prevent them from being exposed to such content. In this paper, we first demonstrate a surprising finding: Pretrained language models recognize, to a considerable degree, their undesirable biases and the toxicity of the content they produce. We refer to this capability as self-diagnosis. Based on this finding, we then propose a decoding algorithm that, given only a textual description of the undesired behavior, reduces the probability of a language model producing problematic text. We refer to this approach as self-debiasing. Self-debiasing does not rely on manually curated word lists, nor does it require any training data or changes to the model\u2019s parameters. While we by no means eliminate the issue of language models generating biased text, we believe our approach to be an important step in this direction.1"
                        ],
                        "paper": {
                            "corpus_id": 232075876,
                            "title": "Self-Diagnosis and Self-Debiasing: A Proposal for Reducing Corpus-Based Bias in NLP",
                            "authors": [
                                {
                                    "authorId": "32246932",
                                    "name": "Timo Schick"
                                },
                                {
                                    "authorId": "40941940",
                                    "name": "Sahana Udupa"
                                },
                                {
                                    "authorId": "144418438",
                                    "name": "Hinrich Sch\u00fctze"
                                }
                            ],
                            "year": 2021,
                            "venue": "Transactions of the Association for Computational Linguistics",
                            "n_citations": 387
                        },
                        "score": 0
                    },
                    {
                        "id": "(Cheng et al., 2021)",
                        "snippets": [
                            "Pretrained text encoders, such as BERT, have been applied increasingly in various natural language processing (NLP) tasks, and have recently demonstrated significant performance gains. However, recent studies have demonstrated the existence of social bias in these pretrained NLP models. Although prior works have made progress on word-level debiasing, improved sentence-level fairness of pretrained encoders still lacks exploration. In this paper, we proposed the first neural debiasing method for a pretrained sentence encoder, which transforms the pretrained encoder outputs into debiased representations via a fair filter (FairFil) network. To learn the FairFil, we introduce a contrastive learning framework that not only minimizes the correlation between filtered embeddings and bias words but also preserves rich semantic information of the original sentences. On real-world datasets, our FairFil effectively reduces the bias degree of pretrained text encoders, while continuously showing desirable performance on downstream tasks. Moreover, our post-hoc method does not require any retraining of the text encoders, further enlarging FairFil's application space."
                        ],
                        "paper": {
                            "corpus_id": 232185104,
                            "title": "FairFil: Contrastive Neural Debiasing Method for Pretrained Text Encoders",
                            "authors": [
                                {
                                    "authorId": "144533942",
                                    "name": "Pengyu Cheng"
                                },
                                {
                                    "authorId": "3314779",
                                    "name": "Weituo Hao"
                                },
                                {
                                    "authorId": "2087092426",
                                    "name": "Siyang Yuan"
                                },
                                {
                                    "authorId": "81065454",
                                    "name": "Shijing Si"
                                },
                                {
                                    "authorId": "145006560",
                                    "name": "L. Carin"
                                }
                            ],
                            "year": 2021,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 104
                        },
                        "score": 0
                    },
                    {
                        "id": "(Smith et al., 2021)",
                        "snippets": [
                            "We investigate several sources of bias in large dialogue models, and employ a few methods to reduce bias by gender and race/ethnicity in Blender-Bot while maintaining similar levels of performance. We find that the name-scrambling model most effectively reduces bias as measured by a gender bias classifier and most closely matches the token distribution across female/male self-chats; the controlled-generation model is rated by crowdsourced workers as being most preferable and most human; and the unlikelihood-training model has the lowest gender-classifier bias on a downstream task where one of the speakers explicitly states their gender. \n\nPractitioners should choose mitigation techniques that match their use case: the namescrambling method might be most applicable for a model where invariance to names is of most essential importance (e.g., in task oriented dialogue, when scheduling meetings with individuals); the controlled-generation method might be ideal for reducing bias on dialogue while ensuring no loss of performance; and the unlikelihoodtraining method may be best for suppressing gender bias on more than just names in a generalizable fashion."
                        ],
                        "paper": {
                            "corpus_id": 237442178,
                            "title": "Hi, my name is Martha: Using names to measure and mitigate bias in generative dialogue models",
                            "authors": [
                                {
                                    "authorId": "51324296",
                                    "name": "Eric Michael Smith"
                                },
                                {
                                    "authorId": "81840293",
                                    "name": "Adina Williams"
                                }
                            ],
                            "year": 2021,
                            "venue": "arXiv.org",
                            "n_citations": 28
                        },
                        "score": 0.89208984375
                    },
                    {
                        "id": "(Zhou et al., 2023)",
                        "snippets": [
                            "Demographic biases and social stereotypes are common in pretrained language models (PLMs), and a burgeoning body of literature focuses on removing the unwanted stereotypical associations from PLMs. However, when fine-tuning these bias-mitigated PLMs in downstream natural language processing (NLP) applications, such as sentiment classification, the unwanted stereotypical associations resurface or even get amplified. Since pretrain&fine-tune is a major paradigm in NLP applications, separating the debiasing procedure of PLMs from fine-tuning would eventually harm the actual downstream utility. In this paper, we propose a unified debiasing framework Causal-Debias to remove unwanted stereotypical associations in PLMs during fine-tuning. Specifically, CausalDebias mitigates bias from a causal invariant perspective by leveraging the specific downstream task to identify bias-relevant and labelrelevant factors. We propose that bias-relevant factors are non-causal as they should have little impact on downstream tasks, while labelrelevant factors are causal. We perform interventions on non-causal factors in different demographic groups and design an invariant risk minimization loss to mitigate bias while maintaining task performance. Experimental results on three downstream tasks show that our proposed method can remarkably reduce unwanted stereotypical associations after PLMs are finetuned, while simultaneously minimizing the impact on PLMs and downstream applications."
                        ],
                        "paper": {
                            "corpus_id": 259370743,
                            "title": "Causal-Debias: Unifying Debiasing in Pretrained Language Models and Fine-tuning via Causal Invariant Learning",
                            "authors": [
                                {
                                    "authorId": "144315735",
                                    "name": "Fan Zhou"
                                },
                                {
                                    "authorId": "2221367541",
                                    "name": "Yuzhou Mao"
                                },
                                {
                                    "authorId": "38057162",
                                    "name": "Liu Yu"
                                },
                                {
                                    "authorId": "2143686211",
                                    "name": "Yi Yang"
                                },
                                {
                                    "authorId": "46456474",
                                    "name": "Ting Zhong"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 41
                        },
                        "score": 0.86865234375
                    },
                    {
                        "id": "(Wu et al., 2024)",
                        "snippets": [
                            "Causal-Debias (Zhou et al., 2023). The Causal-Debias framework offers a pioneering solution to mitigating biases in pretrained language models (PLMs) by integrating causal learning principles into the fine-tuning process, which could be directly applied to the pre-training processes. Unlike conventional methods that separate bias mitigation and task performance optimization, Causal-Debias merges these goals through the use of causal interventions and invariant risk minimization (IRM). The framework effectively distinguishes between causal (label-relevant) and non-causal (bias-related) factors embedded in token representations, addressing spurious correlations resulting from demographic biases and social stereotypes. By generating counterfactual data-modifying bias-related attributes such as gendered or racial terms-and training the model with an invariant loss across diverse environments, Causal-Debias ensures that the model generalizes well across tasks while mitigating biases. This approach directly tackles the challenge of bias resurgence, where previously mitigated biases reappear during fine-tuning, a limitation observed in many existing debiasing techniques."
                        ],
                        "paper": {
                            "corpus_id": 273501862,
                            "title": "Causality for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "1748975704",
                                    "name": "Anpeng Wu"
                                },
                                {
                                    "authorId": "2315590986",
                                    "name": "Kun Kuang"
                                },
                                {
                                    "authorId": "2292675528",
                                    "name": "Minqin Zhu"
                                },
                                {
                                    "authorId": "2290028937",
                                    "name": "Yingrong Wang"
                                },
                                {
                                    "authorId": "2309464222",
                                    "name": "Yujia Zheng"
                                },
                                {
                                    "authorId": "2315988838",
                                    "name": "Kairong Han"
                                },
                                {
                                    "authorId": "2313166010",
                                    "name": "Baohong Li"
                                },
                                {
                                    "authorId": "2155315836",
                                    "name": "Guan-Hong Chen"
                                },
                                {
                                    "authorId": "2290035617",
                                    "name": "Fei Wu"
                                },
                                {
                                    "authorId": "2316094914",
                                    "name": "Kun Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 9
                        },
                        "score": 0.95703125
                    },
                    {
                        "id": "(Guo et al., 2022)",
                        "snippets": [
                            "Human-like biases and undesired social stereotypes exist in large pretrained language models. Given the wide adoption of these models in real-world applications, mitigating such biases has become an emerging and important task. In this paper, we propose an automatic method to mitigate the biases in pretrained language models. Different from previous debiasing work that uses external corpora to fine-tune the pretrained models, we instead directly probe the biases encoded in pretrained models through prompts. Specifically, we propose a variant of the beam search method to automatically search for biased prompts such that the cloze-style completions are the most different with respect to different demographic groups. Given the identified biased prompts, we then propose a distribution alignment loss to mitigate the biases. Experiment results on standard datasets and metrics show that our proposed Auto-Debias approach can significantly reduce biases, including gender and racial bias, in pretrained language models such as BERT, RoBERTa and ALBERT. Moreover, the improvement in fairness does not decrease the language models\u2019 understanding abilities, as shown using the GLUE benchmark."
                        ],
                        "paper": {
                            "corpus_id": 248780440,
                            "title": "Auto-Debias: Debiasing Masked Language Models with Automated Biased Prompts",
                            "authors": [
                                {
                                    "authorId": null,
                                    "name": "Yue Guo"
                                },
                                {
                                    "authorId": "46285693",
                                    "name": "Yi Yang"
                                },
                                {
                                    "authorId": "144849629",
                                    "name": "A. Abbasi"
                                }
                            ],
                            "year": 2022,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 167
                        },
                        "score": 0.9501953125
                    },
                    {
                        "id": "(Doughman et al., 2023)",
                        "snippets": [
                            "Auto-Debias. Auto-Debias (Guo et al., 2022)) is a debiasing technique for masked language models that does not entail referencing external corpora. Auto-Debias contains two stages: First, automatically crafting biased prompts, such that the clozestyle completions have the highest disagreement in generating stereotype words with respect to demographic groups. Second, debiasing the language model by a distribution alignment loss",
                            "Auto-Debias illustrates a 90%-96% reduction in mean probability scores from base to debiased models, while only a 3%-16% reduction in mean normalized ranks."
                        ],
                        "paper": {
                            "corpus_id": 268064051,
                            "title": "Can a Prediction\u2019s Rank Offer a More Accurate Quantification of Bias? A Case Study Measuring Sexism in Debiased Language Models",
                            "authors": [
                                {
                                    "authorId": "1724523030",
                                    "name": "Jad Doughman"
                                },
                                {
                                    "authorId": "38510157",
                                    "name": "Shady Shehata"
                                },
                                {
                                    "authorId": "1452348515",
                                    "name": "L. A. Qadi"
                                },
                                {
                                    "authorId": "2292591921",
                                    "name": "Youssef Nafea"
                                },
                                {
                                    "authorId": "1696863",
                                    "name": "F. Karray"
                                }
                            ],
                            "year": 2023,
                            "venue": "EVAL4NLP",
                            "n_citations": 0
                        },
                        "score": 0.9267578125
                    },
                    {
                        "id": "(Cai et al., 2024)",
                        "snippets": [
                            "In this study, we integrate the processes of locating and mitigating bias within a unified framework. Initially, we use causal mediation analysis to trace the causal effects of different components' activation within a large language model. Building on this, we propose the LSDM (Least Square Debias Method), a knowledge-editing based method for mitigating gender bias in occupational pronouns, and compare it against two baselines on three gender bias datasets and seven knowledge competency test datasets",
                            "LSDM modifies parameters by solving a matrix equation with constraint terms, enabling us to minimize interference with other aspects of the model while specifically mitigating gender bias associated with certain occupation words. LSDM overcomes the catastrophic forgetting problem that exists in all other debiasing methods and stands out by avoiding additional reinforcement learning or human annotations, basing its approach on causal trace conclusions rather than just black-box fine-tuning."
                        ],
                        "paper": {
                            "corpus_id": 268553687,
                            "title": "Locating and Mitigating Gender Bias in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2292681106",
                                    "name": "Yuchen Cai"
                                },
                                {
                                    "authorId": "2237226610",
                                    "name": "Ding Cao"
                                },
                                {
                                    "authorId": "2237261799",
                                    "name": "Rongxi Guo"
                                },
                                {
                                    "authorId": "2293239741",
                                    "name": "Yaqin Wen"
                                },
                                {
                                    "authorId": "2237403462",
                                    "name": "Guiquan Liu"
                                },
                                {
                                    "authorId": "2292390890",
                                    "name": "Enhong Chen"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Intelligent Computing",
                            "n_citations": 5
                        },
                        "score": 0.87158203125
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Performance Trade-offs and Effectiveness",
                "tldr": "Post-training bias mitigation techniques typically involve trade-offs between bias reduction and model performance, with most methods showing some degradation in language modeling capabilities when bias is reduced. The effectiveness of different mitigation approaches varies significantly based on bias type, downstream task, and implementation details, highlighting the challenge of developing a universal debiasing solution. (5 sources)",
                "text": "\nWhen implementing post-training bias mitigation techniques, practitioners face inevitable trade-offs between reducing bias and maintaining model performance. Multiple studies have empirically demonstrated that while these approaches effectively reduce bias, they often compromise language modeling performance, as indicated by increased perplexity on unbiased text <Paper corpusId=\"271404523\" paperTitle=\"(Lu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"239015827\" paperTitle=\"(Meade et al., 2021)\" isShortName></Paper>. This performance degradation is particularly notable when measuring impacts on downstream natural language understanding tasks.\n\nThe extent of performance trade-offs varies across mitigation methods. Surprisingly, combining multiple techniques doesn't always yield better results. For instance, Park et al. found that applying both debiased embedding and gender swap techniques to a GRU model reduced equality differences by up to 98% while sacrificing only 1.5% of the original performance <Paper corpusId=\"52070035\" paperTitle=\"(Park et al., 2018)\" isShortName></Paper>. However, when they combined three methods together, they observed the largest decrease in original performance despite improvements in reducing bias <Paper corpusId=\"52070035\" paperTitle=\"(Park et al., 2018)\" isShortName></Paper>.\n\nAn important insight is that the effectiveness of bias mitigation techniques depends on several contextual factors:\n\n1. **Bias type complexity**: Token-based mitigation works better for simpler biases like gender bias than for more complex cultural biases <Paper corpusId=\"276235453\" paperTitle=\"(Li et al., 2025)\" isShortName></Paper>.\n\n2. **Task characteristics**: Classification tasks typically respond better to mitigation efforts than detailed generation tasks. For example, mask-based mitigation is particularly effective for cultural classification tasks at lower bias ratios (5%), where explicit biased terms have more direct impact <Paper corpusId=\"276235453\" paperTitle=\"(Li et al., 2025)\" isShortName></Paper>.\n\n3. **Bias ratio and data distribution**: Loss-based mitigation effectiveness depends significantly on the distribution distance between augmentation and original data, with smaller proportions (5-10%) of augmentation data often yielding better results by introducing subtle shifts without overfitting <Paper corpusId=\"276235453\" paperTitle=\"(Li et al., 2025)\" isShortName></Paper>.\n\nOne concerning finding is that debiasing techniques can sometimes create a disconnect between surface-level outputs and internal representations. Zakizadeh et al. discovered that some methods inadvertently increase encoded bias in internal representations while successfully reducing bias in model output distributions <Paper corpusId=\"276902427\" paperTitle=\"(Zakizadeh et al., 2025)\" isShortName></Paper>. This suggests that current approaches may be masking rather than truly eliminating bias.\n\nAdditionally, some post-hoc techniques manage to retain language modeling performance but fail to detect and mitigate more subtle and implicit toxic content <Paper corpusId=\"271404523\" paperTitle=\"(Lu et al., 2024)\" isShortName></Paper>. This highlights the need for more sophisticated methods that can address nuanced forms of bias without compromising model utility.\n\nThe performance trade-offs encountered in bias mitigation efforts stem largely from how these methods modify data or model architecture. As Park et al. observed, the performance loss likely occurs because mitigation methods sometimes deter models from discriminating important \"unbiased\" features while addressing the biased ones <Paper corpusId=\"52070035\" paperTitle=\"(Park et al., 2018)\" isShortName></Paper>. \n\nThese nuanced effects across different scenarios highlight the significant challenge in developing a unified mitigation solution that works consistently across all bias types, tasks, and model architectures <Paper corpusId=\"276235453\" paperTitle=\"(Li et al., 2025)\" isShortName></Paper>. Future research must continue to explore this complex balance between bias reduction and maintaining model capabilities.",
                "citations": [
                    {
                        "id": "(Lu et al., 2024)",
                        "snippets": [
                            "Numerous studies have been conducted to mitigate the bias and toxicity inherent in LLMs (Zhao et al., 2018)(Barikeri et al., 2021)(Liang et al., 2020)(Ravfogel et al., 2020)(Schick et al., 2021).However, recent studies (Meade et al., 2021) empiri-cally show that these approaches are effective in reducing bias while compromising the language modeling performance, as indicated by increased perplexity on unbiased text.Notably, existing posthoc techniques manage to retain language modeling performance, but they fail to detect more subtle and implicit toxic content."
                        ],
                        "paper": {
                            "corpus_id": 271404523,
                            "title": "Towards Transfer Unlearning: Empirical Evidence of Cross-Domain Bias Mitigation",
                            "authors": [
                                {
                                    "authorId": "2312881526",
                                    "name": "Huimin Lu"
                                },
                                {
                                    "authorId": "24905917",
                                    "name": "Masaru Isonuma"
                                },
                                {
                                    "authorId": "49010536",
                                    "name": "Junichiro Mori"
                                },
                                {
                                    "authorId": "2265001653",
                                    "name": "Ichiro Sakata"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.92041015625
                    },
                    {
                        "id": "(Meade et al., 2021)",
                        "snippets": [
                            "Recent work has shown pre-trained language models capture social biases from the large amounts of text they are trained on. This has attracted attention to developing techniques that mitigate such biases. In this work, we perform an empirical survey of five recently proposed bias mitigation techniques: Counterfactual Data Augmentation (CDA), Dropout, Iterative Nullspace Projection, Self-Debias, and SentenceDebias. We quantify the effectiveness of each technique using three intrinsic bias benchmarks while also measuring the impact of these techniques on a model\u2019s language modeling ability, as well as its performance on downstream NLU tasks. We experimentally find that: (1) Self-Debias is the strongest debiasing technique, obtaining improved scores on all bias benchmarks; (2) Current debiasing techniques perform less consistently when mitigating non-gender biases; And (3) improvements on bias benchmarks such as StereoSet and CrowS-Pairs by using debiasing strategies are often accompanied by a decrease in language modeling ability, making it difficult to determine whether the bias mitigation was effective."
                        ],
                        "paper": {
                            "corpus_id": 239015827,
                            "title": "An Empirical Survey of the Effectiveness of Debiasing Techniques for Pre-trained Language Models",
                            "authors": [
                                {
                                    "authorId": "150247363",
                                    "name": "Nicholas Meade"
                                },
                                {
                                    "authorId": "2133330526",
                                    "name": "Elinor Poole-Dayan"
                                },
                                {
                                    "authorId": "145732771",
                                    "name": "Siva Reddy"
                                }
                            ],
                            "year": 2021,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 128
                        },
                        "score": 0.88671875
                    },
                    {
                        "id": "(Park et al., 2018)",
                        "snippets": [
                            "To our surprise, the most effective method was applying both debiased embedding and gender swap to GRU, which reduced the equality differences by 98% & 89% while losing only 1.5% of the original performance. We assume that this may be related to the influence of \"attending\" model architectures on biases as discussed in Section 4.3. On the other hand, using the three methods together improved both generated unbiased set performance and equality differences, but had the largest decrease in the original performance. \n\nAll methods involved some performance loss when gender biases were reduced. Especially, fine-tuning had the largest decrease in original test set performance. This could be attributed to the difference in the source and target tasks (abusive & sexist). However, the decrease was marginal (less than 4%), while the drop in bias was significant. We assume the performance loss happens because mitigation methods modify the data or the model in a way that sometimes deters the models from discriminating important \"unbiased\" features."
                        ],
                        "paper": {
                            "corpus_id": 52070035,
                            "title": "Reducing Gender Bias in Abusive Language Detection",
                            "authors": [
                                {
                                    "authorId": "2116023415",
                                    "name": "Ji Ho Park"
                                },
                                {
                                    "authorId": "51228826",
                                    "name": "Jamin Shin"
                                },
                                {
                                    "authorId": "1683412",
                                    "name": "Pascale Fung"
                                }
                            ],
                            "year": 2018,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 341
                        },
                        "score": 0.8779296875
                    },
                    {
                        "id": "(Li et al., 2025)",
                        "snippets": [
                            "The average mitigation results of severe negative influences for each task and bias are shown in Figure 9. A detailed analysis of the mitigation effects on gender and cultural biases across different tasks, as well as the results on GPT-4o-mini, are provided in Appendix E.2. Our key conclusion is that, similar to the nuanced effects on downstream tasks, the effectiveness of different mitigation approaches is also nuanced, depending on various factors such as different types of bias, downstream tasks, bias ratios, and more, highlighting the difficulty in devising a unified mitigation solution. Specifically, token-based mitigation provides implicit cues and works best with simple biases and tasks, as it depends on the model's own understanding. It is more effective for gender bias than complex cultural bias and performs better in classification tasks than in detailed generation tasks. For gender generation tasks, salary recommendation benefits more than complex hiring recommendation. More augmentation data (50%) further enhance its performance. \n\nMask-based mitigation shows noticeable effects at lower bias ratios (5%), especially in tasks like cultural classification. As at this ratio, explicit biased terms have a more direct and pronounced impact on the model's performance. By masking these terms, the model is less likely to rely on biased information or features that could skew its decision-making, thereby reducing bias inheritance. However, as bias ratios increase, the influence of more subtle and implicit biases grows, necessitating more complex mitigation strategies. It also proves effective in contrastive explicit bias, where direct bias information is more clearly identifiable. \n\nLoss-based mitigation primarily depends on the distribution distance between the augmentation and the original data, showing significant effectiveness when the distance is large. It is more suitable for coarse-grained tasks. For example, it works well in classification tasks where decisions often rely on broader patterns. In generation tasks, coarse-grained contexts like salary recommendations perform better than finer-grained ones like hiring recommendation. Additionally, smaller proportions (e.g., 5% and 10%) of augmentation data yield better results, by introducing subtle shifts that avoid overfitting to the augmented distribution while effectively influencing the original bias."
                        ],
                        "paper": {
                            "corpus_id": 276235453,
                            "title": "Understanding and Mitigating the Bias Inheritance in LLM-based Data Augmentation on Downstream Tasks",
                            "authors": [
                                {
                                    "authorId": "2344641665",
                                    "name": "Miaomiao Li"
                                },
                                {
                                    "authorId": "2261741520",
                                    "name": "Hao Chen"
                                },
                                {
                                    "authorId": "2344936960",
                                    "name": "Yang Wang"
                                },
                                {
                                    "authorId": "2331767016",
                                    "name": "Tingyuan Zhu"
                                },
                                {
                                    "authorId": "2344790620",
                                    "name": "Weijia Zhang"
                                },
                                {
                                    "authorId": "2313725635",
                                    "name": "Kaijie Zhu"
                                },
                                {
                                    "authorId": "2345348279",
                                    "name": "Kam-Fai Wong"
                                },
                                {
                                    "authorId": "2285254341",
                                    "name": "Jindong Wang"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 4
                        },
                        "score": 0.8974609375
                    },
                    {
                        "id": "(Zakizadeh et al., 2025)",
                        "snippets": [
                            "Despite growing awareness, there is a lack of comprehensive investigation into how different models internally represent and propagate such biases",
                            "Surprisingly, debiasing techniques often exhibit limited efficacy, sometimes inadvertently increasing the encoded bias in internal representations while reducing bias in model output distributions. This highlights a disconnect between mitigating bias in output distributions and addressing its internal representations."
                        ],
                        "paper": {
                            "corpus_id": 276902427,
                            "title": "Gender Encoding Patterns in Pretrained Language Model Representations",
                            "authors": [
                                {
                                    "authorId": "2261278375",
                                    "name": "Mahdi Zakizadeh"
                                },
                                {
                                    "authorId": "1717641",
                                    "name": "Mohammad Taher Pilehvar"
                                }
                            ],
                            "year": 2025,
                            "venue": "Proceedings of the 5th Workshop on Trustworthy NLP (TrustNLP 2025)",
                            "n_citations": 0
                        },
                        "score": 0.85791015625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Comparative Analysis of Mitigation Techniques",
                "tldr": "The effectiveness of bias mitigation techniques varies significantly based on the type of bias, downstream task, and implementation context, with no single universal solution emerging as optimal across all scenarios. Different approaches have distinct strengths and limitations in terms of effectiveness, transparency, and cultural sensitivity, requiring practitioners to select methods based on their specific use cases. (3 sources)",
                "text": "\nWhen comparing post-training bias mitigation techniques for LLMs, research reveals that effectiveness varies substantially across different contexts, making it challenging to identify a single best approach. Smith et al. found that while name-scrambling techniques most effectively reduced gender bias as measured by classifier detection and token distribution metrics, controlled-generation approaches were rated more favorably by human evaluators for maintaining natural dialogue quality, and unlikelihood-training demonstrated superior performance on downstream tasks where gender was explicitly stated <Paper corpusId=\"237442178\" paperTitle=\"(Smith et al., 2021)\" isShortName></Paper>. This variation suggests that the optimal choice of mitigation technique depends heavily on the specific application requirements.\n\nThe effectiveness of different mitigation strategies also varies based on the complexity of the bias being addressed. Token-based mitigation approaches work better for addressing simpler biases like gender compared to more complex cultural biases, and they tend to perform better in classification tasks than in detailed generation tasks <Paper corpusId=\"276235453\" paperTitle=\"(Li et al., 2025)\" isShortName></Paper>. This is likely because token-based techniques rely on the model's inherent understanding of implicit cues, making them less effective for nuanced biases that require more sophisticated handling.\n\nThe implementation context also significantly impacts effectiveness. Mask-based mitigation techniques show more noticeable effects at lower bias ratios (around 5%), particularly in tasks like cultural classification where explicit biased terms have direct impact. In contrast, loss-based mitigation approaches depend primarily on the distribution distance between augmentation and original data, with smaller proportions (5-10%) of augmentation data often yielding better results by introducing subtle shifts without overfitting <Paper corpusId=\"276235453\" paperTitle=\"(Li et al., 2025)\" isShortName></Paper>.\n\nBeyond effectiveness, mitigation techniques also differ in transparency and cultural sensitivity. Adversarial training methods, while effective at reducing certain explicit biases, often score lower on transparency due to their complex and opaque implementation. Conversely, model auditing approaches tend to rate higher for transparency but show limitations in addressing subtle or implicit biases <Paper corpusId=\"270869465\" paperTitle=\"(Lim et al., 2024)\" isShortName></Paper>. These multidimensional considerations highlight the importance of holistic evaluation frameworks that consider not just bias reduction but also practical implementation factors.\n\nThe comparative evidence strongly suggests that practitioners should select mitigation techniques based on their specific use cases. For applications where name invariance is critical (such as task-oriented dialogue for scheduling), name-scrambling approaches may be most appropriate. When preserving natural dialogue quality is essential, controlled-generation techniques might be preferable. For applications requiring broader bias suppression beyond names, unlikelihood training can provide more generalizable results <Paper corpusId=\"237442178\" paperTitle=\"(Smith et al., 2021)\" isShortName></Paper>. This context-dependent effectiveness underscores the current reality that no universal debiasing solution exists that works consistently across all bias types, tasks, and model architectures.",
                "citations": [
                    {
                        "id": "(Smith et al., 2021)",
                        "snippets": [
                            "We investigate several sources of bias in large dialogue models, and employ a few methods to reduce bias by gender and race/ethnicity in Blender-Bot while maintaining similar levels of performance. We find that the name-scrambling model most effectively reduces bias as measured by a gender bias classifier and most closely matches the token distribution across female/male self-chats; the controlled-generation model is rated by crowdsourced workers as being most preferable and most human; and the unlikelihood-training model has the lowest gender-classifier bias on a downstream task where one of the speakers explicitly states their gender. \n\nPractitioners should choose mitigation techniques that match their use case: the namescrambling method might be most applicable for a model where invariance to names is of most essential importance (e.g., in task oriented dialogue, when scheduling meetings with individuals); the controlled-generation method might be ideal for reducing bias on dialogue while ensuring no loss of performance; and the unlikelihoodtraining method may be best for suppressing gender bias on more than just names in a generalizable fashion."
                        ],
                        "paper": {
                            "corpus_id": 237442178,
                            "title": "Hi, my name is Martha: Using names to measure and mitigate bias in generative dialogue models",
                            "authors": [
                                {
                                    "authorId": "51324296",
                                    "name": "Eric Michael Smith"
                                },
                                {
                                    "authorId": "81840293",
                                    "name": "Adina Williams"
                                }
                            ],
                            "year": 2021,
                            "venue": "arXiv.org",
                            "n_citations": 28
                        },
                        "score": 0.89208984375
                    },
                    {
                        "id": "(Li et al., 2025)",
                        "snippets": [
                            "The average mitigation results of severe negative influences for each task and bias are shown in Figure 9. A detailed analysis of the mitigation effects on gender and cultural biases across different tasks, as well as the results on GPT-4o-mini, are provided in Appendix E.2. Our key conclusion is that, similar to the nuanced effects on downstream tasks, the effectiveness of different mitigation approaches is also nuanced, depending on various factors such as different types of bias, downstream tasks, bias ratios, and more, highlighting the difficulty in devising a unified mitigation solution. Specifically, token-based mitigation provides implicit cues and works best with simple biases and tasks, as it depends on the model's own understanding. It is more effective for gender bias than complex cultural bias and performs better in classification tasks than in detailed generation tasks. For gender generation tasks, salary recommendation benefits more than complex hiring recommendation. More augmentation data (50%) further enhance its performance. \n\nMask-based mitigation shows noticeable effects at lower bias ratios (5%), especially in tasks like cultural classification. As at this ratio, explicit biased terms have a more direct and pronounced impact on the model's performance. By masking these terms, the model is less likely to rely on biased information or features that could skew its decision-making, thereby reducing bias inheritance. However, as bias ratios increase, the influence of more subtle and implicit biases grows, necessitating more complex mitigation strategies. It also proves effective in contrastive explicit bias, where direct bias information is more clearly identifiable. \n\nLoss-based mitigation primarily depends on the distribution distance between the augmentation and the original data, showing significant effectiveness when the distance is large. It is more suitable for coarse-grained tasks. For example, it works well in classification tasks where decisions often rely on broader patterns. In generation tasks, coarse-grained contexts like salary recommendations perform better than finer-grained ones like hiring recommendation. Additionally, smaller proportions (e.g., 5% and 10%) of augmentation data yield better results, by introducing subtle shifts that avoid overfitting to the augmented distribution while effectively influencing the original bias."
                        ],
                        "paper": {
                            "corpus_id": 276235453,
                            "title": "Understanding and Mitigating the Bias Inheritance in LLM-based Data Augmentation on Downstream Tasks",
                            "authors": [
                                {
                                    "authorId": "2344641665",
                                    "name": "Miaomiao Li"
                                },
                                {
                                    "authorId": "2261741520",
                                    "name": "Hao Chen"
                                },
                                {
                                    "authorId": "2344936960",
                                    "name": "Yang Wang"
                                },
                                {
                                    "authorId": "2331767016",
                                    "name": "Tingyuan Zhu"
                                },
                                {
                                    "authorId": "2344790620",
                                    "name": "Weijia Zhang"
                                },
                                {
                                    "authorId": "2313725635",
                                    "name": "Kaijie Zhu"
                                },
                                {
                                    "authorId": "2345348279",
                                    "name": "Kam-Fai Wong"
                                },
                                {
                                    "authorId": "2285254341",
                                    "name": "Jindong Wang"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 4
                        },
                        "score": 0.8974609375
                    },
                    {
                        "id": "(Lim et al., 2024)",
                        "snippets": [
                            "We therefore developed a scoring methodology used to evaluate bias mitigation techniques in Large Language Models (LLMs) to offer a comparative analysis that highlights both the strengths and limitations of each approach. Each technique was assessed across multiple dimensions -transparency, cultural sensitivity, and effectiveness. The scores for these dimensions were derived through a combination of literature review, empirical findings, and qualitative evaluations of how these techniques perform in practical settings. For example, adversarial training received a high score for its effectiveness in reducing certain explicit biases but was marked lower for transparency due to the complexity and opacity of its implementation. In contrast, model auditing scored higher for transparency but displayed limitations in direct bias mitigation, particularly in subtle or implicit bias contexts."
                        ],
                        "paper": {
                            "corpus_id": 270869465,
                            "title": "The African Woman is Rhythmic and Soulful: An Investigation of Implicit Biases in LLM Open-ended Text Generation",
                            "authors": [
                                {
                                    "authorId": "2309199058",
                                    "name": "Serene Lim"
                                },
                                {
                                    "authorId": "2323507494",
                                    "name": "Mar'ia P'erez-Ortiz"
                                }
                            ],
                            "year": 2024,
                            "venue": "",
                            "n_citations": 2
                        },
                        "score": 0.87353515625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Implementation Considerations",
                "tldr": "Implementing post-training bias mitigation techniques for LLMs requires careful consideration of various practical factors including computational resources, technical complexity, and contextual appropriateness. Organizations must evaluate trade-offs between immediate deployment needs and long-term bias reduction goals when selecting and implementing debiasing approaches. (3 sources)",
                "text": "\n## Resource Requirements and Complexity\n- **Computational Resources**: Post-processing approaches are often preferred when retraining is not feasible due to limited computational resources, as they can be applied after model training without requiring extensive recomputation. <Paper corpusId=\"277150560\" paperTitle=\"(Dasu et al., 2025)\" isShortName></Paper>\n\n- **Implementation Difficulty**: Different mitigation techniques vary significantly in their implementation complexity, with some requiring specialized expertise in adversarial methods or causal inference.\n\n- **Deployment Flexibility**: Some techniques like Co\u00b2PT (Counterfactual Contrastive Prompt Tuning) offer flexibility to mitigate different bias types without retraining the entire model, allowing practitioners to develop separate debiasing prompts for different dimensions (gender, race, religion) that can be combined as needed. <Paper corpusId=\"264305744\" paperTitle=\"(Dong et al., 2023)\" isShortName></Paper>\n\n## Practical Application Considerations\n- **Domain Specificity**: Bias mitigation approaches should be selected based on the specific domain and application context, as effectiveness varies across different tasks and bias types.\n\n- **Monitoring Requirements**: Implementing bias mitigation requires ongoing monitoring systems to track effectiveness and potential unintended consequences over time. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\n- **Maintenance Overhead**: Some techniques require regular updates as language evolves or as new biases are identified, creating ongoing maintenance requirements. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\n## Ethical and Organizational Factors\n- **Transparency Trade-offs**: Organizations must consider whether the selected mitigation approach provides sufficient transparency to explain how bias is being addressed.\n\n- **Cost-Benefit Analysis**: Post-processing approaches can be particularly valuable when dataset collection and LLM training are prohibitively expensive processes that cannot be repeated if fairness issues are discovered. <Paper corpusId=\"277150560\" paperTitle=\"(Dasu et al., 2025)\" isShortName></Paper>\n\n- **Regulatory Compliance**: Different jurisdictions may have varying requirements regarding AI fairness and bias mitigation, necessitating specific implementation approaches to ensure compliance. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\n## Integration Challenges\n- **Pipeline Compatibility**: Techniques like keyword-based distillation for inference-time bias removal must be integrated carefully into existing inference pipelines. <Paper corpusId=\"277150560\" paperTitle=\"(Dasu et al., 2025)\" isShortName></Paper> <Paper corpusId=\"236459953\" paperTitle=\"(Qian et al., 2021)\" isShortName></Paper>\n\n- **Multi-bias Handling**: Organizations implementing debiasing systems should consider how different techniques can be combined to address intersectional biases across multiple dimensions simultaneously. <Paper corpusId=\"264305744\" paperTitle=\"(Dong et al., 2023)\" isShortName></Paper>\n\n- **Performance Monitoring**: Implementing mitigation techniques requires systems to monitor both bias reduction and potential performance impacts on downstream tasks. <Paper corpusId=\"264305744\" paperTitle=\"(Dong et al., 2023)\" isShortName></Paper>",
                "citations": [
                    {
                        "id": "(Dasu et al., 2025)",
                        "snippets": [
                            "Post-processing techniques modify the model's inference behavior after complete training. This may involve altering the model before inference or the model output during/after inference. For instance, Gehman et al. [24] proposed token-blocking methods during decoding to prevent the generation of harmful or biased terms. \n\nHauzenberger et al. [33] introduced sparse debiasing subnetworks that are trained separately and can be applied to the model at inference time. Qian et al. (Qian et al., 2021) performed keyword-based distillation to remove bias during inference. Tokpo and Calders [61] identify biased tokens and replace them with less stereotypical terms. Postprocessing approaches are beneficial since dataset collection and LLM training are expensive processes that we may not always be able to repeat if issues of unfairness are found. Furthermore, postprocessing mitigation approaches are sometimes the only viable option with the status quo of utilizing large pre-trained LLMs that take a huge amount of resources to train."
                        ],
                        "paper": {
                            "corpus_id": 277150560,
                            "title": "Attention Pruning: Automated Fairness Repair of Language Models via Surrogate Simulated Annealing",
                            "authors": [
                                {
                                    "authorId": "152123953",
                                    "name": "Vishnu Asutosh Dasu"
                                },
                                {
                                    "authorId": "2150249269",
                                    "name": "Md. Rafi Ur Rashid"
                                },
                                {
                                    "authorId": "2110652561",
                                    "name": "Vipul Gupta"
                                },
                                {
                                    "authorId": "1404887808",
                                    "name": "Saeid Tizpaz-Niari"
                                },
                                {
                                    "authorId": "2309172099",
                                    "name": "Gang Tan"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.9658203125
                    },
                    {
                        "id": "(Dong et al., 2023)",
                        "snippets": [
                            "Co$^2$PT, an efficient and effective debias-while-prompt tuning method for mitigating biases via counterfactual contrastive prompt tuning on downstream tasks",
                            "We evaluate its effectiveness on bias mitigation and applicability to existing debiased upstream models, and investigate how the design of each component and the selection of hyperparameters impact both its bias reduction capabilities and downstream task performance",
                            "Without re-training the model, Co 2 PT is flexible to apply in order to mitigate different bias types in downstream applications. One can train different debiasing prompts to tackle different bias dimensions such as gender, race, and religion. Furthermore, these debiasing prompts can be applied to mitigate intersectional bias by simply combining the corresponding prompts in downstream tasks."
                        ],
                        "paper": {
                            "corpus_id": 264305744,
                            "title": "Co$^2$PT: Mitigating Bias in Pre-trained Language Models through Counterfactual Contrastive Prompt Tuning",
                            "authors": [
                                {
                                    "authorId": "1716200686",
                                    "name": "Xiangjue Dong"
                                },
                                {
                                    "authorId": "9725200",
                                    "name": "Ziwei Zhu"
                                },
                                {
                                    "authorId": "96309563",
                                    "name": "Zhuoer Wang"
                                },
                                {
                                    "authorId": "121003684",
                                    "name": "Maria Teleki"
                                },
                                {
                                    "authorId": "1697232",
                                    "name": "James Caverlee"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 11
                        },
                        "score": 0.89501953125
                    },
                    {
                        "id": "(Qian et al., 2021)",
                        "snippets": [
                            "Today\u2019s text classifiers inevitably suffer from unintended dataset biases, especially the document-level label bias and word-level keyword bias, which may hurt models\u2019 generalization. Many previous studies employed data-level manipulations or model-level balancing mechanisms to recover unbiased distributions and thus prevent models from capturing the two types of biases. Unfortunately, they either suffer from the extra cost of data collection/selection/annotation or need an elaborate design of balancing strategies. Different from traditional factual inference in which debiasing occurs before or during training, counterfactual inference mitigates the influence brought by unintended confounders after training, which can make unbiased decisions with biased observations. Inspired by this, we propose a model-agnostic text classification debiasing framework \u2013 Corsair, which can effectively avoid employing data manipulations or designing balancing mechanisms. Concretely, Corsair first trains a base model on a training set directly, allowing the dataset biases \u2018poison\u2019 the trained model. In inference, given a factual input document, Corsair imagines its two counterfactual counterparts to distill and mitigate the two biases captured by the poisonous model. Extensive experiments demonstrate Corsair\u2019s effectiveness, generalizability and fairness."
                        ],
                        "paper": {
                            "corpus_id": 236459953,
                            "title": "Counterfactual Inference for Text Classification Debiasing",
                            "authors": [
                                {
                                    "authorId": "2082474054",
                                    "name": "Chen Qian"
                                },
                                {
                                    "authorId": "2163400298",
                                    "name": "Fuli Feng"
                                },
                                {
                                    "authorId": "40650846",
                                    "name": "L. Wen"
                                },
                                {
                                    "authorId": "48168083",
                                    "name": "Chunping Ma"
                                },
                                {
                                    "authorId": "35930962",
                                    "name": "Pengjun Xie"
                                }
                            ],
                            "year": 2021,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 89
                        },
                        "score": 0
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.20074199999999998
    }
}